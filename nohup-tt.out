WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-14 21:37:46.919244: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-14 21:37:47.014223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:85:00.0
2024-04-14 21:37:47.014975: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:37:47.018309: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:37:47.021051: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:37:47.022032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:37:47.026054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:37:47.028851: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:37:47.037991: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:37:47.046740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:37:47.047158: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-14 21:37:47.063022: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-14 21:37:47.067407: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47e8180 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:37:47.067466: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-14 21:37:47.367757: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2272330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:37:47.367843: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-14 21:37:47.375775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:85:00.0
2024-04-14 21:37:47.375880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:37:47.375919: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:37:47.375951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:37:47.375982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:37:47.376014: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:37:47.376045: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:37:47.376077: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:37:47.385795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:37:47.385881: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:37:47.395778: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-14 21:37:47.395810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-14 21:37:47.395821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-14 21:37:47.404510: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-14 21:37:50.767599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.5174 - val_loss: 0.0016
Epoch 2/512
448/448 - 0s - loss: 0.0923 - val_loss: 1.6040e-04
Epoch 3/512
448/448 - 0s - loss: 0.0088 - val_loss: 2.0444e-05
Epoch 4/512
448/448 - 0s - loss: 0.0016 - val_loss: 1.1452e-05
Epoch 5/512
448/448 - 0s - loss: 0.0010 - val_loss: 8.1753e-06
Epoch 6/512
448/448 - 0s - loss: 7.2496e-04 - val_loss: 5.4198e-06
Epoch 7/512
448/448 - 0s - loss: 4.6745e-04 - val_loss: 3.2506e-06
Epoch 8/512
448/448 - 0s - loss: 2.7098e-04 - val_loss: 1.7183e-06
Epoch 9/512
448/448 - 0s - loss: 1.3740e-04 - val_loss: 7.7363e-07
Epoch 10/512
448/448 - 0s - loss: 5.8810e-05 - val_loss: 2.8451e-07
Epoch 11/512
448/448 - 0s - loss: 2.0342e-05 - val_loss: 8.0811e-08
Epoch 12/512
448/448 - 0s - loss: 5.3768e-06 - val_loss: 1.6774e-08
Epoch 13/512
448/448 - 0s - loss: 1.2300e-06 - val_loss: 4.1948e-08
Epoch 14/512
448/448 - 0s - loss: 1.5527e-04 - val_loss: 4.1523e-05
Epoch 15/512
448/448 - 0s - loss: 0.0052 - val_loss: 6.3931e-06
Epoch 16/512
448/448 - 0s - loss: 3.1564e-04 - val_loss: 8.5077e-07
Epoch 17/512
448/448 - 0s - loss: 1.0199e-04 - val_loss: 2.7664e-06
Epoch 18/512
448/448 - 0s - loss: 8.6800e-04 - val_loss: 3.5266e-05
Epoch 19/512
448/448 - 0s - loss: 0.0029 - val_loss: 9.2530e-06
Epoch 20/512
448/448 - 0s - loss: 6.2598e-04 - val_loss: 4.3050e-06
Epoch 21/512
448/448 - 0s - loss: 5.7707e-04 - val_loss: 1.2886e-05
Epoch 22/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.8858e-05
Epoch 23/512
448/448 - 0s - loss: 0.0014 - val_loss: 7.9761e-06
Epoch 24/512
448/448 - 0s - loss: 7.5124e-04 - val_loss: 9.2608e-06
Epoch 25/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.5708e-05
Epoch 26/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.1317e-05
Epoch 27/512
448/448 - 0s - loss: 9.8966e-04 - val_loss: 9.2033e-06
Epoch 28/512
448/448 - 0s - loss: 9.7227e-04 - val_loss: 1.2374e-05
Epoch 29/512
448/448 - 0s - loss: 0.0013 - val_loss: 1.2145e-05
Epoch 30/512
448/448 - 0s - loss: 0.0011 - val_loss: 9.8441e-06
Epoch 31/512
448/448 - 0s - loss: 9.6730e-04 - val_loss: 1.0840e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1746e-05
Epoch 33/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.0225e-05
Epoch 34/512
448/448 - 0s - loss: 9.7753e-04 - val_loss: 1.0374e-05
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.1291e-05
Epoch 36/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.0381e-05
Epoch 37/512
448/448 - 0s - loss: 9.8766e-04 - val_loss: 9.8344e-06
Epoch 38/512
448/448 - 0s - loss: 9.8029e-04 - val_loss: 1.0485e-05
Epoch 39/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.0354e-05
Epoch 40/512
448/448 - 0s - loss: 9.8788e-04 - val_loss: 9.9995e-06
Epoch 41/512
448/448 - 0s - loss: 9.7219e-04 - val_loss: 9.9812e-06
Epoch 42/512
448/448 - 0s - loss: 9.7012e-04 - val_loss: 1.0032e-05
Epoch 43/512
448/448 - 0s - loss: 9.7434e-04 - val_loss: 9.6947e-06
Epoch 44/512
448/448 - 0s - loss: 9.4764e-04 - val_loss: 9.5091e-06
Epoch 45/512
448/448 - 0s - loss: 9.4644e-04 - val_loss: 9.4013e-06
Epoch 46/512
448/448 - 0s - loss: 9.2994e-04 - val_loss: 9.5738e-06
Epoch 47/512
448/448 - 0s - loss: 9.3849e-04 - val_loss: 9.6447e-06
Epoch 48/512
448/448 - 0s - loss: 9.3677e-04 - val_loss: 9.1132e-06
Epoch 49/512
448/448 - 0s - loss: 8.9347e-04 - val_loss: 9.2012e-06
Epoch 50/512
448/448 - 0s - loss: 9.1803e-04 - val_loss: 9.2432e-06
Epoch 51/512
448/448 - 0s - loss: 8.9804e-04 - val_loss: 9.3527e-06
Epoch 52/512
448/448 - 0s - loss: 9.0900e-04 - val_loss: 9.0988e-06
Epoch 53/512
448/448 - 0s - loss: 8.8273e-04 - val_loss: 8.8767e-06
Epoch 54/512
448/448 - 0s - loss: 8.7694e-04 - val_loss: 8.9266e-06
Epoch 55/512
448/448 - 0s - loss: 8.8163e-04 - val_loss: 8.7668e-06
Epoch 56/512
448/448 - 0s - loss: 8.6581e-04 - val_loss: 8.7022e-06
Epoch 57/512
448/448 - 0s - loss: 8.6274e-04 - val_loss: 8.6727e-06
Epoch 58/512
448/448 - 0s - loss: 8.5496e-04 - val_loss: 8.7121e-06
Epoch 59/512
448/448 - 0s - loss: 8.5794e-04 - val_loss: 8.5885e-06
Epoch 60/512
448/448 - 0s - loss: 8.4279e-04 - val_loss: 8.4348e-06
Epoch 61/512
448/448 - 0s - loss: 8.3929e-04 - val_loss: 8.2932e-06
Epoch 62/512
448/448 - 0s - loss: 8.2215e-04 - val_loss: 8.4849e-06
Epoch 63/512
448/448 - 0s - loss: 8.3960e-04 - val_loss: 8.3911e-06
Epoch 64/512
448/448 - 0s - loss: 8.2479e-04 - val_loss: 8.1155e-06
Epoch 65/512
448/448 - 0s - loss: 8.0733e-04 - val_loss: 8.1056e-06
Epoch 66/512
448/448 - 0s - loss: 8.0985e-04 - val_loss: 8.1833e-06
Epoch 67/512
448/448 - 0s - loss: 8.1076e-04 - val_loss: 8.1202e-06
Epoch 68/512
448/448 - 0s - loss: 8.0114e-04 - val_loss: 7.9312e-06
Epoch 69/512
448/448 - 0s - loss: 7.8791e-04 - val_loss: 8.0881e-06
Epoch 70/512
448/448 - 0s - loss: 8.0072e-04 - val_loss: 8.0025e-06
Epoch 71/512
448/448 - 0s - loss: 7.8461e-04 - val_loss: 7.8270e-06
Epoch 72/512
448/448 - 0s - loss: 7.7621e-04 - val_loss: 7.9039e-06
Epoch 73/512
448/448 - 0s - loss: 7.7831e-04 - val_loss: 7.9325e-06
Epoch 74/512
448/448 - 0s - loss: 7.8115e-04 - val_loss: 7.6737e-06
Epoch 75/512
448/448 - 0s - loss: 7.5771e-04 - val_loss: 7.6832e-06
Epoch 76/512
448/448 - 0s - loss: 7.6209e-04 - val_loss: 7.7839e-06
Epoch 77/512
448/448 - 0s - loss: 7.6914e-04 - val_loss: 7.5141e-06
Epoch 78/512
448/448 - 0s - loss: 7.4342e-04 - val_loss: 7.4339e-06
Epoch 79/512
448/448 - 0s - loss: 7.4319e-04 - val_loss: 7.6784e-06
Epoch 80/512
448/448 - 0s - loss: 7.6205e-04 - val_loss: 7.4690e-06
Epoch 81/512
448/448 - 0s - loss: 7.2869e-04 - val_loss: 7.4651e-06
Epoch 82/512
448/448 - 0s - loss: 7.4058e-04 - val_loss: 7.4331e-06
Epoch 83/512
448/448 - 0s - loss: 7.3026e-04 - val_loss: 7.4338e-06
Epoch 84/512
448/448 - 0s - loss: 7.3003e-04 - val_loss: 7.3264e-06
Epoch 85/512
448/448 - 0s - loss: 7.2372e-04 - val_loss: 7.2459e-06
Epoch 86/512
448/448 - 0s - loss: 7.2156e-04 - val_loss: 7.1224e-06
Epoch 87/512
448/448 - 0s - loss: 7.0321e-04 - val_loss: 7.3907e-06
Epoch 88/512
448/448 - 0s - loss: 7.2721e-04 - val_loss: 7.3880e-06
Epoch 89/512
448/448 - 0s - loss: 7.2086e-04 - val_loss: 6.8157e-06
Epoch 90/512
448/448 - 0s - loss: 6.7946e-04 - val_loss: 6.9710e-06
Epoch 91/512
448/448 - 0s - loss: 7.0429e-04 - val_loss: 7.2089e-06
Epoch 92/512
448/448 - 0s - loss: 7.0869e-04 - val_loss: 6.9567e-06
Epoch 93/512
448/448 - 0s - loss: 6.8533e-04 - val_loss: 6.8173e-06
Epoch 94/512
448/448 - 0s - loss: 6.7916e-04 - val_loss: 7.0262e-06
Epoch 95/512
448/448 - 0s - loss: 6.9816e-04 - val_loss: 6.7826e-06
Epoch 96/512
448/448 - 0s - loss: 6.6898e-04 - val_loss: 6.7767e-06
Epoch 97/512
448/448 - 0s - loss: 6.7352e-04 - val_loss: 7.0342e-06
Epoch 98/512
448/448 - 0s - loss: 6.9019e-04 - val_loss: 6.7379e-06
Epoch 99/512
448/448 - 0s - loss: 6.5909e-04 - val_loss: 6.5858e-06
Epoch 100/512
448/448 - 0s - loss: 6.6215e-04 - val_loss: 6.5721e-06
Epoch 101/512
448/448 - 0s - loss: 6.5962e-04 - val_loss: 6.5890e-06
Epoch 102/512
448/448 - 0s - loss: 6.6003e-04 - val_loss: 6.4838e-06
Epoch 103/512
448/448 - 0s - loss: 6.4532e-04 - val_loss: 6.6035e-06
Epoch 104/512
448/448 - 0s - loss: 6.5881e-04 - val_loss: 6.4315e-06
Epoch 105/512
448/448 - 0s - loss: 6.3380e-04 - val_loss: 6.5181e-06
Epoch 106/512
448/448 - 0s - loss: 6.5156e-04 - val_loss: 6.3647e-06
Epoch 107/512
448/448 - 0s - loss: 6.2680e-04 - val_loss: 6.4058e-06
Epoch 108/512
448/448 - 0s - loss: 6.4008e-04 - val_loss: 6.3355e-06
Epoch 109/512
448/448 - 0s - loss: 6.2419e-04 - val_loss: 6.3167e-06
Epoch 110/512
448/448 - 0s - loss: 6.2464e-04 - val_loss: 6.3816e-06
Epoch 111/512
448/448 - 0s - loss: 6.2733e-04 - val_loss: 6.1971e-06
Epoch 112/512
448/448 - 0s - loss: 6.1061e-04 - val_loss: 6.1382e-06
Epoch 113/512
448/448 - 0s - loss: 6.1061e-04 - val_loss: 6.1777e-06
Epoch 114/512
448/448 - 0s - loss: 6.1213e-04 - val_loss: 6.1011e-06
Epoch 115/512
448/448 - 0s - loss: 6.0299e-04 - val_loss: 5.9873e-06
Epoch 116/512
448/448 - 0s - loss: 5.9788e-04 - val_loss: 5.9237e-06
Epoch 117/512
448/448 - 0s - loss: 5.9642e-04 - val_loss: 5.8846e-06
Epoch 118/512
448/448 - 0s - loss: 5.9215e-04 - val_loss: 5.8269e-06
Epoch 119/512
448/448 - 0s - loss: 5.7960e-04 - val_loss: 5.9949e-06
Epoch 120/512
448/448 - 0s - loss: 5.9414e-04 - val_loss: 5.9103e-06
Epoch 121/512
448/448 - 0s - loss: 5.7875e-04 - val_loss: 5.7254e-06
Epoch 122/512
448/448 - 0s - loss: 5.6921e-04 - val_loss: 5.7050e-06
Epoch 123/512
448/448 - 0s - loss: 5.6927e-04 - val_loss: 5.7368e-06
Epoch 124/512
448/448 - 0s - loss: 5.7104e-04 - val_loss: 5.6652e-06
Epoch 125/512
448/448 - 0s - loss: 5.5787e-04 - val_loss: 5.5855e-06
Epoch 126/512
448/448 - 0s - loss: 5.5569e-04 - val_loss: 5.5926e-06
Epoch 127/512
448/448 - 0s - loss: 5.5187e-04 - val_loss: 5.5889e-06
Epoch 128/512
448/448 - 0s - loss: 5.5157e-04 - val_loss: 5.4478e-06
Epoch 129/512
448/448 - 0s - loss: 5.3754e-04 - val_loss: 5.4773e-06
Epoch 130/512
448/448 - 0s - loss: 5.4427e-04 - val_loss: 5.4282e-06
Epoch 131/512
448/448 - 0s - loss: 5.3484e-04 - val_loss: 5.2905e-06
Epoch 132/512
448/448 - 0s - loss: 5.2785e-04 - val_loss: 5.1848e-06
Epoch 133/512
448/448 - 0s - loss: 5.1595e-04 - val_loss: 5.3960e-06
Epoch 134/512
448/448 - 0s - loss: 5.3306e-04 - val_loss: 5.2849e-06
Epoch 135/512
448/448 - 0s - loss: 5.1939e-04 - val_loss: 4.9107e-06
Epoch 136/512
448/448 - 0s - loss: 4.9248e-04 - val_loss: 5.0635e-06
Epoch 137/512
448/448 - 0s - loss: 5.1435e-04 - val_loss: 5.1742e-06
Epoch 138/512
448/448 - 0s - loss: 5.0809e-04 - val_loss: 4.9920e-06
Epoch 139/512
448/448 - 0s - loss: 4.9341e-04 - val_loss: 4.8136e-06
Epoch 140/512
448/448 - 0s - loss: 4.8023e-04 - val_loss: 5.0076e-06
Epoch 141/512
448/448 - 0s - loss: 5.0239e-04 - val_loss: 4.8230e-06
Epoch 142/512
448/448 - 0s - loss: 4.7501e-04 - val_loss: 4.7167e-06
Epoch 143/512
448/448 - 0s - loss: 4.7597e-04 - val_loss: 4.7158e-06
Epoch 144/512
448/448 - 0s - loss: 4.7252e-04 - val_loss: 4.6657e-06
Epoch 145/512
448/448 - 0s - loss: 4.6760e-04 - val_loss: 4.6045e-06
Epoch 146/512
448/448 - 0s - loss: 4.6048e-04 - val_loss: 4.5383e-06
Epoch 147/512
448/448 - 0s - loss: 4.5744e-04 - val_loss: 4.4558e-06
Epoch 148/512
448/448 - 0s - loss: 4.4680e-04 - val_loss: 4.5299e-06
Epoch 149/512
448/448 - 0s - loss: 4.5406e-04 - val_loss: 4.4471e-06
Epoch 150/512
448/448 - 0s - loss: 4.4082e-04 - val_loss: 4.3199e-06
Epoch 151/512
448/448 - 0s - loss: 4.3558e-04 - val_loss: 4.2525e-06
Epoch 152/512
448/448 - 0s - loss: 4.2839e-04 - val_loss: 4.2763e-06
Epoch 153/512
448/448 - 0s - loss: 4.2918e-04 - val_loss: 4.2589e-06
Epoch 154/512
448/448 - 0s - loss: 4.2459e-04 - val_loss: 4.0987e-06
Epoch 155/512
448/448 - 0s - loss: 4.0931e-04 - val_loss: 4.0809e-06
Epoch 156/512
448/448 - 0s - loss: 4.1113e-04 - val_loss: 4.1340e-06
Epoch 157/512
448/448 - 0s - loss: 4.1101e-04 - val_loss: 4.0320e-06
Epoch 158/512
448/448 - 0s - loss: 3.9822e-04 - val_loss: 3.9051e-06
Epoch 159/512
448/448 - 0s - loss: 3.9118e-04 - val_loss: 3.9039e-06
Epoch 160/512
448/448 - 0s - loss: 3.9173e-04 - val_loss: 3.9074e-06
Epoch 161/512
448/448 - 0s - loss: 3.8519e-04 - val_loss: 3.8539e-06
Epoch 162/512
448/448 - 0s - loss: 3.8313e-04 - val_loss: 3.6579e-06
Epoch 163/512
448/448 - 0s - loss: 3.6441e-04 - val_loss: 3.7302e-06
Epoch 164/512
448/448 - 0s - loss: 3.7462e-04 - val_loss: 3.7004e-06
Epoch 165/512
448/448 - 0s - loss: 3.6639e-04 - val_loss: 3.4888e-06
Epoch 166/512
448/448 - 0s - loss: 3.4885e-04 - val_loss: 3.5277e-06
Epoch 167/512
448/448 - 0s - loss: 3.5568e-04 - val_loss: 3.5493e-06
Epoch 168/512
448/448 - 0s - loss: 3.5497e-04 - val_loss: 3.2985e-06
Epoch 169/512
448/448 - 0s - loss: 3.2979e-04 - val_loss: 3.3380e-06
Epoch 170/512
448/448 - 0s - loss: 3.4106e-04 - val_loss: 3.3919e-06
Epoch 171/512
448/448 - 0s - loss: 3.3765e-04 - val_loss: 3.1943e-06
Epoch 172/512
448/448 - 0s - loss: 3.1943e-04 - val_loss: 3.1301e-06
Epoch 173/512
448/448 - 0s - loss: 3.1756e-04 - val_loss: 3.2302e-06
Epoch 174/512
448/448 - 0s - loss: 3.2311e-04 - val_loss: 3.1081e-06
Epoch 175/512
448/448 - 0s - loss: 3.0734e-04 - val_loss: 2.9860e-06
Epoch 176/512
448/448 - 0s - loss: 3.0270e-04 - val_loss: 2.9863e-06
Epoch 177/512
448/448 - 0s - loss: 3.0101e-04 - val_loss: 2.9854e-06
Epoch 178/512
448/448 - 0s - loss: 2.9652e-04 - val_loss: 2.8981e-06
Epoch 179/512
448/448 - 0s - loss: 2.8939e-04 - val_loss: 2.7951e-06
Epoch 180/512
448/448 - 0s - loss: 2.7971e-04 - val_loss: 2.8287e-06
Epoch 181/512
448/448 - 0s - loss: 2.8384e-04 - val_loss: 2.7352e-06
Epoch 182/512
448/448 - 0s - loss: 2.7447e-04 - val_loss: 2.6033e-06
Epoch 183/512
448/448 - 0s - loss: 2.6247e-04 - val_loss: 2.6520e-06
Epoch 184/512
448/448 - 0s - loss: 2.6880e-04 - val_loss: 2.6095e-06
Epoch 185/512
448/448 - 0s - loss: 2.5875e-04 - val_loss: 2.5056e-06
Epoch 186/512
448/448 - 0s - loss: 2.5081e-04 - val_loss: 2.4883e-06
Epoch 187/512
448/448 - 0s - loss: 2.5184e-04 - val_loss: 2.4034e-06
Epoch 188/512
448/448 - 0s - loss: 2.4088e-04 - val_loss: 2.3342e-06
Epoch 189/512
448/448 - 0s - loss: 2.3758e-04 - val_loss: 2.3394e-06
Epoch 190/512
448/448 - 0s - loss: 2.3689e-04 - val_loss: 2.2705e-06
Epoch 191/512
448/448 - 0s - loss: 2.2917e-04 - val_loss: 2.1654e-06
Epoch 192/512
448/448 - 0s - loss: 2.2000e-04 - val_loss: 2.1965e-06
Epoch 193/512
448/448 - 0s - loss: 2.2305e-04 - val_loss: 2.1488e-06
Epoch 194/512
448/448 - 0s - loss: 2.1479e-04 - val_loss: 2.0771e-06
Epoch 195/512
448/448 - 0s - loss: 2.0872e-04 - val_loss: 2.0585e-06
Epoch 196/512
448/448 - 0s - loss: 2.0616e-04 - val_loss: 2.0207e-06
Epoch 197/512
448/448 - 0s - loss: 2.0286e-04 - val_loss: 1.9078e-06
Epoch 198/512
448/448 - 0s - loss: 1.9200e-04 - val_loss: 1.8997e-06
Epoch 199/512
448/448 - 0s - loss: 1.9386e-04 - val_loss: 1.8847e-06
Epoch 200/512
448/448 - 0s - loss: 1.8969e-04 - val_loss: 1.8032e-06
Epoch 201/512
448/448 - 0s - loss: 1.8092e-04 - val_loss: 1.7678e-06
Epoch 202/512
448/448 - 0s - loss: 1.7905e-04 - val_loss: 1.7606e-06
Epoch 203/512
448/448 - 0s - loss: 1.7616e-04 - val_loss: 1.7009e-06
Epoch 204/512
448/448 - 0s - loss: 1.7057e-04 - val_loss: 1.6217e-06
Epoch 205/512
448/448 - 0s - loss: 1.6427e-04 - val_loss: 1.5839e-06
Epoch 206/512
448/448 - 0s - loss: 1.6194e-04 - val_loss: 1.5630e-06
Epoch 207/512
448/448 - 0s - loss: 1.5919e-04 - val_loss: 1.5014e-06
Epoch 208/512
448/448 - 0s - loss: 1.5196e-04 - val_loss: 1.4928e-06
Epoch 209/512
448/448 - 0s - loss: 1.5165e-04 - val_loss: 1.4560e-06
Epoch 210/512
448/448 - 0s - loss: 1.4574e-04 - val_loss: 1.4135e-06
Epoch 211/512
448/448 - 0s - loss: 1.4314e-04 - val_loss: 1.3586e-06
Epoch 212/512
448/448 - 0s - loss: 1.3768e-04 - val_loss: 1.3122e-06
Epoch 213/512
448/448 - 0s - loss: 1.3423e-04 - val_loss: 1.3144e-06
Epoch 214/512
448/448 - 0s - loss: 1.3264e-04 - val_loss: 1.2900e-06
Epoch 215/512
448/448 - 0s - loss: 1.2813e-04 - val_loss: 1.2485e-06
Epoch 216/512
448/448 - 0s - loss: 1.2540e-04 - val_loss: 1.1829e-06
Epoch 217/512
448/448 - 0s - loss: 1.1957e-04 - val_loss: 1.1477e-06
Epoch 218/512
448/448 - 0s - loss: 1.1718e-04 - val_loss: 1.1450e-06
Epoch 219/512
448/448 - 0s - loss: 1.1592e-04 - val_loss: 1.0989e-06
Epoch 220/512
448/448 - 0s - loss: 1.1014e-04 - val_loss: 1.0607e-06
Epoch 221/512
448/448 - 0s - loss: 1.0788e-04 - val_loss: 1.0340e-06
Epoch 222/512
448/448 - 0s - loss: 1.0512e-04 - val_loss: 9.9364e-07
Epoch 223/512
448/448 - 0s - loss: 1.0128e-04 - val_loss: 9.5480e-07
Epoch 224/512
448/448 - 0s - loss: 9.7979e-05 - val_loss: 9.4539e-07
Epoch 225/512
448/448 - 0s - loss: 9.6378e-05 - val_loss: 9.2486e-07
Epoch 226/512
448/448 - 0s - loss: 9.3218e-05 - val_loss: 8.9928e-07
Epoch 227/512
448/448 - 0s - loss: 9.0805e-05 - val_loss: 8.5178e-07
Epoch 228/512
448/448 - 0s - loss: 8.7073e-05 - val_loss: 8.0354e-07
Epoch 229/512
448/448 - 0s - loss: 8.2460e-05 - val_loss: 8.1581e-07
Epoch 230/512
448/448 - 0s - loss: 8.4190e-05 - val_loss: 7.9217e-07
Epoch 231/512
448/448 - 0s - loss: 7.9575e-05 - val_loss: 7.4424e-07
Epoch 232/512
448/448 - 0s - loss: 7.5455e-05 - val_loss: 7.5103e-07
Epoch 233/512
448/448 - 0s - loss: 7.6451e-05 - val_loss: 7.1193e-07
Epoch 234/512
448/448 - 0s - loss: 7.1707e-05 - val_loss: 6.6121e-07
Epoch 235/512
448/448 - 0s - loss: 6.8323e-05 - val_loss: 6.5887e-07
Epoch 236/512
448/448 - 0s - loss: 6.8149e-05 - val_loss: 6.5423e-07
Epoch 237/512
448/448 - 0s - loss: 6.6326e-05 - val_loss: 6.0278e-07
Epoch 238/512
448/448 - 0s - loss: 6.1297e-05 - val_loss: 5.9481e-07
Epoch 239/512
448/448 - 0s - loss: 6.1248e-05 - val_loss: 6.0434e-07
Epoch 240/512
448/448 - 0s - loss: 6.0958e-05 - val_loss: 5.5706e-07
Epoch 241/512
448/448 - 0s - loss: 5.6106e-05 - val_loss: 5.1606e-07
Epoch 242/512
448/448 - 0s - loss: 5.3365e-05 - val_loss: 5.3133e-07
Epoch 243/512
448/448 - 0s - loss: 5.4774e-05 - val_loss: 5.2094e-07
Epoch 244/512
448/448 - 0s - loss: 5.2298e-05 - val_loss: 4.6615e-07
Epoch 245/512
448/448 - 0s - loss: 4.7205e-05 - val_loss: 4.7195e-07
Epoch 246/512
448/448 - 0s - loss: 4.9095e-05 - val_loss: 4.6868e-07
Epoch 247/512
448/448 - 0s - loss: 4.6899e-05 - val_loss: 4.2939e-07
Epoch 248/512
448/448 - 0s - loss: 4.3701e-05 - val_loss: 4.0745e-07
Epoch 249/512
448/448 - 0s - loss: 4.2493e-05 - val_loss: 4.1276e-07
Epoch 250/512
448/448 - 0s - loss: 4.2231e-05 - val_loss: 3.9947e-07
Epoch 251/512
448/448 - 0s - loss: 4.0037e-05 - val_loss: 3.7072e-07
Epoch 252/512
448/448 - 0s - loss: 3.7913e-05 - val_loss: 3.5831e-07
Epoch 253/512
448/448 - 0s - loss: 3.7110e-05 - val_loss: 3.4992e-07
Epoch 254/512
448/448 - 0s - loss: 3.5995e-05 - val_loss: 3.3017e-07
Epoch 255/512
448/448 - 0s - loss: 3.4047e-05 - val_loss: 3.1751e-07
Epoch 256/512
448/448 - 0s - loss: 3.2886e-05 - val_loss: 3.1402e-07
Epoch 257/512
448/448 - 0s - loss: 3.2335e-05 - val_loss: 3.0222e-07
Epoch 258/512
448/448 - 0s - loss: 3.0869e-05 - val_loss: 2.7687e-07
Epoch 259/512
448/448 - 0s - loss: 2.8488e-05 - val_loss: 2.7964e-07
Epoch 260/512
448/448 - 0s - loss: 2.9180e-05 - val_loss: 2.6970e-07
Epoch 261/512
448/448 - 0s - loss: 2.7396e-05 - val_loss: 2.4751e-07
Epoch 262/512
448/448 - 0s - loss: 2.5358e-05 - val_loss: 2.5123e-07
Epoch 263/512
448/448 - 0s - loss: 2.5896e-05 - val_loss: 2.4821e-07
Epoch 264/512
448/448 - 0s - loss: 2.4916e-05 - val_loss: 2.1735e-07
Epoch 265/512
448/448 - 0s - loss: 2.2110e-05 - val_loss: 2.1361e-07
Epoch 266/512
448/448 - 0s - loss: 2.2648e-05 - val_loss: 2.1635e-07
Epoch 267/512
448/448 - 0s - loss: 2.2023e-05 - val_loss: 2.0073e-07
Epoch 268/512
448/448 - 0s - loss: 2.0391e-05 - val_loss: 1.8931e-07
Epoch 269/512
448/448 - 0s - loss: 1.9619e-05 - val_loss: 1.8723e-07
Epoch 270/512
448/448 - 0s - loss: 1.9266e-05 - val_loss: 1.8177e-07
Epoch 271/512
448/448 - 0s - loss: 1.8529e-05 - val_loss: 1.6792e-07
Epoch 272/512
448/448 - 0s - loss: 1.7187e-05 - val_loss: 1.6258e-07
Epoch 273/512
448/448 - 0s - loss: 1.6958e-05 - val_loss: 1.5591e-07
Epoch 274/512
448/448 - 0s - loss: 1.6123e-05 - val_loss: 1.4973e-07
Epoch 275/512
448/448 - 0s - loss: 1.5511e-05 - val_loss: 1.4398e-07
Epoch 276/512
448/448 - 0s - loss: 1.4827e-05 - val_loss: 1.3855e-07
Epoch 277/512
448/448 - 0s - loss: 1.4284e-05 - val_loss: 1.3150e-07
Epoch 278/512
448/448 - 0s - loss: 1.3403e-05 - val_loss: 1.3180e-07
Epoch 279/512
448/448 - 0s - loss: 1.3555e-05 - val_loss: 1.2012e-07
Epoch 280/512
448/448 - 0s - loss: 1.2227e-05 - val_loss: 1.1295e-07
Epoch 281/512
448/448 - 0s - loss: 1.1832e-05 - val_loss: 1.1277e-07
Epoch 282/512
448/448 - 0s - loss: 1.1684e-05 - val_loss: 1.0887e-07
Epoch 283/512
448/448 - 0s - loss: 1.1041e-05 - val_loss: 1.0298e-07
Epoch 284/512
448/448 - 0s - loss: 1.0556e-05 - val_loss: 9.6454e-08
Epoch 285/512
448/448 - 0s - loss: 9.9690e-06 - val_loss: 9.2122e-08
Epoch 286/512
448/448 - 0s - loss: 9.5999e-06 - val_loss: 9.1431e-08
Epoch 287/512
448/448 - 0s - loss: 9.4046e-06 - val_loss: 8.6800e-08
Epoch 288/512
448/448 - 0s - loss: 8.8294e-06 - val_loss: 8.1561e-08
Epoch 289/512
448/448 - 0s - loss: 8.3798e-06 - val_loss: 7.8333e-08
Epoch 290/512
448/448 - 0s - loss: 8.1065e-06 - val_loss: 7.6454e-08
Epoch 291/512
448/448 - 0s - loss: 7.8394e-06 - val_loss: 7.2223e-08
Epoch 292/512
448/448 - 0s - loss: 7.3582e-06 - val_loss: 6.9616e-08
Epoch 293/512
448/448 - 0s - loss: 7.1478e-06 - val_loss: 6.5614e-08
Epoch 294/512
448/448 - 0s - loss: 6.8004e-06 - val_loss: 5.9379e-08
Epoch 295/512
448/448 - 0s - loss: 6.2400e-06 - val_loss: 5.9530e-08
Epoch 296/512
448/448 - 0s - loss: 6.2825e-06 - val_loss: 6.0198e-08
Epoch 297/512
448/448 - 0s - loss: 6.1039e-06 - val_loss: 5.4366e-08
Epoch 298/512
448/448 - 0s - loss: 5.5540e-06 - val_loss: 4.9615e-08
Epoch 299/512
448/448 - 0s - loss: 5.1892e-06 - val_loss: 5.1638e-08
Epoch 300/512
448/448 - 0s - loss: 5.3913e-06 - val_loss: 4.9095e-08
Epoch 301/512
448/448 - 0s - loss: 4.9724e-06 - val_loss: 4.2650e-08
Epoch 302/512
448/448 - 0s - loss: 4.4184e-06 - val_loss: 4.3800e-08
Epoch 303/512
448/448 - 0s - loss: 4.6733e-06 - val_loss: 4.2346e-08
Epoch 304/512
448/448 - 0s - loss: 4.2930e-06 - val_loss: 3.8590e-08
Epoch 305/512
448/448 - 0s - loss: 4.0055e-06 - val_loss: 3.7441e-08
Epoch 306/512
448/448 - 0s - loss: 3.9228e-06 - val_loss: 3.6662e-08
Epoch 307/512
448/448 - 0s - loss: 3.7763e-06 - val_loss: 3.4365e-08
Epoch 308/512
448/448 - 0s - loss: 3.5354e-06 - val_loss: 3.1910e-08
Epoch 309/512
448/448 - 0s - loss: 3.3188e-06 - val_loss: 3.1705e-08
Epoch 310/512
448/448 - 0s - loss: 3.3162e-06 - val_loss: 3.0038e-08
Epoch 311/512
448/448 - 0s - loss: 3.0650e-06 - val_loss: 2.8140e-08
Epoch 312/512
448/448 - 0s - loss: 2.9329e-06 - val_loss: 2.6522e-08
Epoch 313/512
448/448 - 0s - loss: 2.7694e-06 - val_loss: 2.5894e-08
Epoch 314/512
448/448 - 0s - loss: 2.7119e-06 - val_loss: 2.4834e-08
Epoch 315/512
448/448 - 0s - loss: 2.5459e-06 - val_loss: 2.3427e-08
Epoch 316/512
448/448 - 0s - loss: 2.4304e-06 - val_loss: 2.1958e-08
Epoch 317/512
448/448 - 0s - loss: 2.2741e-06 - val_loss: 2.1551e-08
Epoch 318/512
448/448 - 0s - loss: 2.2618e-06 - val_loss: 2.0179e-08
Epoch 319/512
448/448 - 0s - loss: 2.0852e-06 - val_loss: 1.8680e-08
Epoch 320/512
448/448 - 0s - loss: 1.9536e-06 - val_loss: 1.8562e-08
Epoch 321/512
448/448 - 0s - loss: 1.9446e-06 - val_loss: 1.8012e-08
Epoch 322/512
448/448 - 0s - loss: 1.8516e-06 - val_loss: 1.6005e-08
Epoch 323/512
448/448 - 0s - loss: 1.6672e-06 - val_loss: 1.5650e-08
Epoch 324/512
448/448 - 0s - loss: 1.6663e-06 - val_loss: 1.5455e-08
Epoch 325/512
448/448 - 0s - loss: 1.6080e-06 - val_loss: 1.4146e-08
Epoch 326/512
448/448 - 0s - loss: 1.4503e-06 - val_loss: 1.3603e-08
Epoch 327/512
448/448 - 0s - loss: 1.4275e-06 - val_loss: 1.3302e-08
Epoch 328/512
448/448 - 0s - loss: 1.3866e-06 - val_loss: 1.2225e-08
Epoch 329/512
448/448 - 0s - loss: 1.2634e-06 - val_loss: 1.1484e-08
Epoch 330/512
448/448 - 0s - loss: 1.2155e-06 - val_loss: 1.1480e-08
Epoch 331/512
448/448 - 0s - loss: 1.1936e-06 - val_loss: 1.0722e-08
Epoch 332/512
448/448 - 0s - loss: 1.1064e-06 - val_loss: 9.9667e-09
Epoch 333/512
448/448 - 0s - loss: 1.0402e-06 - val_loss: 9.8495e-09
Epoch 334/512
448/448 - 0s - loss: 1.0237e-06 - val_loss: 9.4783e-09
Epoch 335/512
448/448 - 0s - loss: 9.7502e-07 - val_loss: 8.5202e-09
Epoch 336/512
448/448 - 0s - loss: 8.8165e-07 - val_loss: 8.2239e-09
Epoch 337/512
448/448 - 0s - loss: 8.6874e-07 - val_loss: 8.0398e-09
Epoch 338/512
448/448 - 0s - loss: 8.3886e-07 - val_loss: 7.5246e-09
Epoch 339/512
448/448 - 0s - loss: 7.7853e-07 - val_loss: 7.0857e-09
Epoch 340/512
448/448 - 0s - loss: 7.4013e-07 - val_loss: 6.7515e-09
Epoch 341/512
448/448 - 0s - loss: 7.0952e-07 - val_loss: 6.5168e-09
Epoch 342/512
448/448 - 0s - loss: 6.8327e-07 - val_loss: 6.0883e-09
Epoch 343/512
448/448 - 0s - loss: 6.3229e-07 - val_loss: 5.8667e-09
Epoch 344/512
448/448 - 0s - loss: 6.1593e-07 - val_loss: 5.4514e-09
Epoch 345/512
448/448 - 0s - loss: 5.7253e-07 - val_loss: 5.1233e-09
Epoch 346/512
448/448 - 0s - loss: 5.4407e-07 - val_loss: 5.0132e-09
Epoch 347/512
448/448 - 0s - loss: 5.2457e-07 - val_loss: 4.9523e-09
Epoch 348/512
448/448 - 0s - loss: 5.1397e-07 - val_loss: 4.4280e-09
Epoch 349/512
448/448 - 0s - loss: 4.5771e-07 - val_loss: 4.1215e-09
Epoch 350/512
448/448 - 0s - loss: 4.4317e-07 - val_loss: 4.1864e-09
Epoch 351/512
448/448 - 0s - loss: 4.4261e-07 - val_loss: 3.9007e-09
Epoch 352/512
448/448 - 0s - loss: 4.0045e-07 - val_loss: 3.5709e-09
Epoch 353/512
448/448 - 0s - loss: 3.7580e-07 - val_loss: 3.6077e-09
Epoch 354/512
448/448 - 0s - loss: 3.7967e-07 - val_loss: 3.4111e-09
Epoch 355/512
448/448 - 0s - loss: 3.5219e-07 - val_loss: 2.9757e-09
Epoch 356/512
448/448 - 0s - loss: 3.1461e-07 - val_loss: 2.9194e-09
Epoch 357/512
448/448 - 0s - loss: 3.1439e-07 - val_loss: 3.0398e-09
Epoch 358/512
448/448 - 0s - loss: 3.1549e-07 - val_loss: 2.7291e-09
Epoch 359/512
448/448 - 0s - loss: 2.7646e-07 - val_loss: 2.4519e-09
Epoch 360/512
448/448 - 0s - loss: 2.6162e-07 - val_loss: 2.4677e-09
Epoch 361/512
448/448 - 0s - loss: 2.6253e-07 - val_loss: 2.3805e-09
Epoch 362/512
448/448 - 0s - loss: 2.4571e-07 - val_loss: 2.1578e-09
Epoch 363/512
448/448 - 0s - loss: 2.2534e-07 - val_loss: 2.0620e-09
Epoch 364/512
448/448 - 0s - loss: 2.1849e-07 - val_loss: 2.0087e-09
Epoch 365/512
448/448 - 0s - loss: 2.0997e-07 - val_loss: 1.9004e-09
Epoch 366/512
448/448 - 0s - loss: 1.9778e-07 - val_loss: 1.7785e-09
Epoch 367/512
448/448 - 0s - loss: 1.8651e-07 - val_loss: 1.7001e-09
Epoch 368/512
448/448 - 0s - loss: 1.7901e-07 - val_loss: 1.6035e-09
Epoch 369/512
448/448 - 0s - loss: 1.6739e-07 - val_loss: 1.5562e-09
Epoch 370/512
448/448 - 0s - loss: 1.6321e-07 - val_loss: 1.4634e-09
Epoch 371/512
448/448 - 0s - loss: 1.5293e-07 - val_loss: 1.3362e-09
Epoch 372/512
448/448 - 0s - loss: 1.4118e-07 - val_loss: 1.3104e-09
Epoch 373/512
448/448 - 0s - loss: 1.3982e-07 - val_loss: 1.2720e-09
Epoch 374/512
448/448 - 0s - loss: 1.3233e-07 - val_loss: 1.1827e-09
Epoch 375/512
448/448 - 0s - loss: 1.2317e-07 - val_loss: 1.0906e-09
Epoch 376/512
448/448 - 0s - loss: 1.1572e-07 - val_loss: 1.0608e-09
Epoch 377/512
448/448 - 0s - loss: 1.1222e-07 - val_loss: 1.0358e-09
Epoch 378/512
448/448 - 0s - loss: 1.0773e-07 - val_loss: 9.7297e-10
Epoch 379/512
448/448 - 0s - loss: 1.0124e-07 - val_loss: 8.8067e-10
Epoch 380/512
448/448 - 0s - loss: 9.2259e-08 - val_loss: 8.6226e-10
Epoch 381/512
448/448 - 0s - loss: 9.2294e-08 - val_loss: 8.4539e-10
Epoch 382/512
448/448 - 0s - loss: 8.8547e-08 - val_loss: 7.6156e-10
Epoch 383/512
448/448 - 0s - loss: 7.9430e-08 - val_loss: 7.4128e-10
Epoch 384/512
448/448 - 0s - loss: 7.8784e-08 - val_loss: 7.1021e-10
Epoch 385/512
448/448 - 0s - loss: 7.4285e-08 - val_loss: 6.6903e-10
Epoch 386/512
448/448 - 0s - loss: 7.0529e-08 - val_loss: 6.1754e-10
Epoch 387/512
448/448 - 0s - loss: 6.4552e-08 - val_loss: 6.1555e-10
Epoch 388/512
448/448 - 0s - loss: 6.5084e-08 - val_loss: 5.8569e-10
Epoch 389/512
448/448 - 0s - loss: 6.0916e-08 - val_loss: 5.2292e-10
Epoch 390/512
448/448 - 0s - loss: 5.5006e-08 - val_loss: 5.0947e-10
Epoch 391/512
448/448 - 0s - loss: 5.4202e-08 - val_loss: 5.1491e-10
Epoch 392/512
448/448 - 0s - loss: 5.3897e-08 - val_loss: 4.6814e-10
Epoch 393/512
448/448 - 0s - loss: 4.7695e-08 - val_loss: 4.2815e-10
Epoch 394/512
448/448 - 0s - loss: 4.5626e-08 - val_loss: 4.2324e-10
Epoch 395/512
448/448 - 0s - loss: 4.4668e-08 - val_loss: 4.0938e-10
Epoch 396/512
448/448 - 0s - loss: 4.2856e-08 - val_loss: 3.7035e-10
Epoch 397/512
448/448 - 0s - loss: 3.8692e-08 - val_loss: 3.6007e-10
Epoch 398/512
448/448 - 0s - loss: 3.8107e-08 - val_loss: 3.5733e-10
Epoch 399/512
448/448 - 0s - loss: 3.7237e-08 - val_loss: 3.2769e-10
Epoch 400/512
448/448 - 0s - loss: 3.3833e-08 - val_loss: 3.0182e-10
Epoch 401/512
448/448 - 0s - loss: 3.1655e-08 - val_loss: 2.9871e-10
Epoch 402/512
448/448 - 0s - loss: 3.1681e-08 - val_loss: 2.8579e-10
Epoch 403/512
448/448 - 0s - loss: 2.9617e-08 - val_loss: 2.6807e-10
Epoch 404/512
448/448 - 0s - loss: 2.7890e-08 - val_loss: 2.5224e-10
Epoch 405/512
448/448 - 0s - loss: 2.6499e-08 - val_loss: 2.4191e-10
Epoch 406/512
448/448 - 0s - loss: 2.5439e-08 - val_loss: 2.3320e-10
Epoch 407/512
448/448 - 0s - loss: 2.4423e-08 - val_loss: 2.1816e-10
Epoch 408/512
448/448 - 0s - loss: 2.2763e-08 - val_loss: 2.0562e-10
Epoch 409/512
448/448 - 0s - loss: 2.1723e-08 - val_loss: 1.9966e-10
Epoch 410/512
448/448 - 0s - loss: 2.1069e-08 - val_loss: 1.9381e-10
Epoch 411/512
448/448 - 0s - loss: 2.0271e-08 - val_loss: 1.7405e-10
Epoch 412/512
448/448 - 0s - loss: 1.8331e-08 - val_loss: 1.6584e-10
Epoch 413/512
448/448 - 0s - loss: 1.7598e-08 - val_loss: 1.6674e-10
Epoch 414/512
448/448 - 0s - loss: 1.7619e-08 - val_loss: 1.6473e-10
Epoch 415/512
448/448 - 0s - loss: 1.7013e-08 - val_loss: 1.4495e-10
Epoch 416/512
448/448 - 0s - loss: 1.5051e-08 - val_loss: 1.3437e-10
Epoch 417/512
448/448 - 0s - loss: 1.4282e-08 - val_loss: 1.3776e-10
Epoch 418/512
448/448 - 0s - loss: 1.4598e-08 - val_loss: 1.3600e-10
Epoch 419/512
448/448 - 0s - loss: 1.4046e-08 - val_loss: 1.2392e-10
Epoch 420/512
448/448 - 0s - loss: 1.2716e-08 - val_loss: 1.1479e-10
Epoch 421/512
448/448 - 0s - loss: 1.2015e-08 - val_loss: 1.1028e-10
Epoch 422/512
448/448 - 0s - loss: 1.1618e-08 - val_loss: 1.1115e-10
Epoch 423/512
448/448 - 0s - loss: 1.1679e-08 - val_loss: 1.0433e-10
Epoch 424/512
448/448 - 0s - loss: 1.0712e-08 - val_loss: 9.6309e-11
Epoch 425/512
448/448 - 0s - loss: 1.0093e-08 - val_loss: 9.2664e-11
Epoch 426/512
448/448 - 0s - loss: 9.7892e-09 - val_loss: 8.9582e-11
Epoch 427/512
448/448 - 0s - loss: 9.4734e-09 - val_loss: 8.5373e-11
Epoch 428/512
448/448 - 0s - loss: 8.9154e-09 - val_loss: 8.3084e-11
Epoch 429/512
448/448 - 0s - loss: 8.6926e-09 - val_loss: 7.9313e-11
Epoch 430/512
448/448 - 0s - loss: 8.2224e-09 - val_loss: 7.5850e-11
Epoch 431/512
448/448 - 0s - loss: 7.8826e-09 - val_loss: 7.1503e-11
Epoch 432/512
448/448 - 0s - loss: 7.4921e-09 - val_loss: 6.8169e-11
Epoch 433/512
448/448 - 0s - loss: 7.1680e-09 - val_loss: 6.6332e-11
Epoch 434/512
448/448 - 0s - loss: 6.9723e-09 - val_loss: 6.5880e-11
Epoch 435/512
448/448 - 0s - loss: 6.7881e-09 - val_loss: 6.2181e-11
Epoch 436/512
448/448 - 0s - loss: 6.3577e-09 - val_loss: 5.8823e-11
Epoch 437/512
448/448 - 0s - loss: 6.0880e-09 - val_loss: 5.6789e-11
Epoch 438/512
448/448 - 0s - loss: 5.8523e-09 - val_loss: 5.5085e-11
Epoch 439/512
448/448 - 0s - loss: 5.6865e-09 - val_loss: 5.2652e-11
Epoch 440/512
448/448 - 0s - loss: 5.4599e-09 - val_loss: 4.9254e-11
Epoch 441/512
448/448 - 0s - loss: 5.1361e-09 - val_loss: 4.7002e-11
Epoch 442/512
448/448 - 0s - loss: 4.8965e-09 - val_loss: 4.7393e-11
Epoch 443/512
448/448 - 0s - loss: 4.9511e-09 - val_loss: 4.5298e-11
Epoch 444/512
448/448 - 0s - loss: 4.6657e-09 - val_loss: 4.2476e-11
Epoch 445/512
448/448 - 0s - loss: 4.4101e-09 - val_loss: 4.0756e-11
Epoch 446/512
448/448 - 0s - loss: 4.2623e-09 - val_loss: 3.9109e-11
Epoch 447/512
448/448 - 0s - loss: 4.0873e-09 - val_loss: 3.8204e-11
Epoch 448/512
448/448 - 0s - loss: 3.9573e-09 - val_loss: 3.7604e-11
Epoch 449/512
448/448 - 0s - loss: 3.8841e-09 - val_loss: 3.5835e-11
Epoch 450/512
448/448 - 0s - loss: 3.7048e-09 - val_loss: 3.3992e-11
Epoch 451/512
448/448 - 0s - loss: 3.4882e-09 - val_loss: 3.2789e-11
Epoch 452/512
448/448 - 0s - loss: 3.3939e-09 - val_loss: 3.2476e-11
Epoch 453/512
448/448 - 0s - loss: 3.3966e-09 - val_loss: 3.1467e-11
Epoch 454/512
448/448 - 0s - loss: 3.2285e-09 - val_loss: 2.9688e-11
Epoch 455/512
448/448 - 0s - loss: 3.0632e-09 - val_loss: 2.8430e-11
Epoch 456/512
448/448 - 0s - loss: 2.9356e-09 - val_loss: 2.8338e-11
Epoch 457/512
448/448 - 0s - loss: 2.9266e-09 - val_loss: 2.7820e-11
Epoch 458/512
448/448 - 0s - loss: 2.8515e-09 - val_loss: 2.6093e-11
Epoch 459/512
448/448 - 0s - loss: 2.6478e-09 - val_loss: 2.4757e-11
Epoch 460/512
448/448 - 0s - loss: 2.5503e-09 - val_loss: 2.4322e-11
Epoch 461/512
448/448 - 0s - loss: 2.5222e-09 - val_loss: 2.3595e-11
Epoch 462/512
448/448 - 0s - loss: 2.4607e-09 - val_loss: 2.2988e-11
Epoch 463/512
448/448 - 0s - loss: 2.3581e-09 - val_loss: 2.2102e-11
Epoch 464/512
448/448 - 0s - loss: 2.2816e-09 - val_loss: 2.1553e-11
Epoch 465/512
448/448 - 0s - loss: 2.2118e-09 - val_loss: 2.0834e-11
Epoch 466/512
448/448 - 0s - loss: 2.1578e-09 - val_loss: 2.0179e-11
Epoch 467/512
448/448 - 0s - loss: 2.0848e-09 - val_loss: 1.9307e-11
Epoch 468/512
448/448 - 0s - loss: 1.9843e-09 - val_loss: 1.9007e-11
Epoch 469/512
448/448 - 0s - loss: 1.9677e-09 - val_loss: 1.8901e-11
Epoch 470/512
448/448 - 0s - loss: 1.9360e-09 - val_loss: 1.8153e-11
Epoch 471/512
448/448 - 0s - loss: 1.8744e-09 - val_loss: 1.7337e-11
Epoch 472/512
448/448 - 0s - loss: 1.7872e-09 - val_loss: 1.6610e-11
Epoch 473/512
448/448 - 0s - loss: 1.7184e-09 - val_loss: 1.6171e-11
Epoch 474/512
448/448 - 0s - loss: 1.6656e-09 - val_loss: 1.5762e-11
Epoch 475/512
448/448 - 0s - loss: 1.6205e-09 - val_loss: 1.5913e-11
Epoch 476/512
448/448 - 0s - loss: 1.6344e-09 - val_loss: 1.5549e-11
Epoch 477/512
448/448 - 0s - loss: 1.5896e-09 - val_loss: 1.4892e-11
Epoch 478/512
448/448 - 0s - loss: 1.5226e-09 - val_loss: 1.4202e-11
Epoch 479/512
448/448 - 0s - loss: 1.4351e-09 - val_loss: 1.3715e-11
Epoch 480/512
448/448 - 0s - loss: 1.4244e-09 - val_loss: 1.3411e-11
Epoch 481/512
448/448 - 0s - loss: 1.3726e-09 - val_loss: 1.3273e-11
Epoch 482/512
448/448 - 0s - loss: 1.3636e-09 - val_loss: 1.2940e-11
Epoch 483/512
448/448 - 0s - loss: 1.3062e-09 - val_loss: 1.2390e-11
Epoch 484/512
448/448 - 0s - loss: 1.2755e-09 - val_loss: 1.2071e-11
Epoch 485/512
448/448 - 0s - loss: 1.2338e-09 - val_loss: 1.1839e-11
Epoch 486/512
448/448 - 0s - loss: 1.2273e-09 - val_loss: 1.1567e-11
Epoch 487/512
448/448 - 0s - loss: 1.1684e-09 - val_loss: 1.1289e-11
Epoch 488/512
448/448 - 0s - loss: 1.1606e-09 - val_loss: 1.1184e-11
Epoch 489/512
448/448 - 0s - loss: 1.1302e-09 - val_loss: 1.0747e-11
Epoch 490/512
448/448 - 0s - loss: 1.1009e-09 - val_loss: 1.0500e-11
Epoch 491/512
448/448 - 0s - loss: 1.0837e-09 - val_loss: 1.0087e-11
Epoch 492/512
448/448 - 0s - loss: 1.0391e-09 - val_loss: 9.9451e-12
Epoch 493/512
448/448 - 0s - loss: 1.0270e-09 - val_loss: 9.6605e-12
Epoch 494/512
448/448 - 0s - loss: 9.8538e-10 - val_loss: 9.6124e-12
Epoch 495/512
448/448 - 0s - loss: 9.8767e-10 - val_loss: 9.2186e-12
Epoch 496/512
448/448 - 0s - loss: 9.3620e-10 - val_loss: 8.9609e-12
Epoch 497/512
448/448 - 0s - loss: 9.1409e-10 - val_loss: 8.9739e-12
Epoch 498/512
448/448 - 0s - loss: 9.1963e-10 - val_loss: 8.7537e-12
Epoch 499/512
448/448 - 0s - loss: 8.9624e-10 - val_loss: 8.6619e-12
Epoch 500/512
448/448 - 0s - loss: 8.8405e-10 - val_loss: 8.4470e-12
Epoch 501/512
448/448 - 0s - loss: 8.6155e-10 - val_loss: 8.0963e-12
Epoch 502/512
448/448 - 0s - loss: 8.2800e-10 - val_loss: 7.9063e-12
Epoch 503/512
448/448 - 0s - loss: 8.0549e-10 - val_loss: 7.6479e-12
Epoch 504/512
448/448 - 0s - loss: 7.7736e-10 - val_loss: 7.5502e-12
Epoch 505/512
448/448 - 0s - loss: 7.7093e-10 - val_loss: 7.3342e-12
Epoch 506/512
448/448 - 0s - loss: 7.5744e-10 - val_loss: 7.3043e-12
Epoch 507/512
448/448 - 0s - loss: 7.5188e-10 - val_loss: 7.2307e-12
Epoch 508/512
448/448 - 0s - loss: 7.3519e-10 - val_loss: 6.9868e-12
Epoch 509/512
448/448 - 0s - loss: 7.1476e-10 - val_loss: 6.8246e-12
Epoch 510/512
448/448 - 0s - loss: 7.0213e-10 - val_loss: 6.8458e-12
Epoch 511/512
448/448 - 0s - loss: 7.0208e-10 - val_loss: 6.5868e-12
Epoch 512/512
448/448 - 0s - loss: 6.6905e-10 - val_loss: 6.2996e-12
2024-04-14 21:38:05.665548: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.8487e-10 - val_loss: 1.5840e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7883e-09 - val_loss: 1.6519e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5005e-09 - val_loss: 1.0205e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.0555e-10 - val_loss: 6.6850e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.4137e-10 - val_loss: 5.8150e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1530e-10 - val_loss: 6.7219e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5557e-10 - val_loss: 8.7882e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5691e-10 - val_loss: 9.9625e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0240e-09 - val_loss: 9.3551e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1407e-10 - val_loss: 7.8641e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7694e-10 - val_loss: 6.9866e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0168e-10 - val_loss: 6.7470e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9663e-10 - val_loss: 7.0321e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4100e-10 - val_loss: 7.4634e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7137e-10 - val_loss: 7.4532e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6254e-10 - val_loss: 7.1903e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2525e-10 - val_loss: 6.8277e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8872e-10 - val_loss: 6.4824e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5787e-10 - val_loss: 6.3150e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4710e-10 - val_loss: 6.3116e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4717e-10 - val_loss: 6.3129e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4431e-10 - val_loss: 6.1044e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1634e-10 - val_loss: 5.7692e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9069e-10 - val_loss: 5.7175e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8379e-10 - val_loss: 5.7266e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8672e-10 - val_loss: 5.6570e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8010e-10 - val_loss: 5.6487e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7135e-10 - val_loss: 5.4594e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5275e-10 - val_loss: 5.2721e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3800e-10 - val_loss: 5.2054e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3096e-10 - val_loss: 5.2022e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3380e-10 - val_loss: 5.1787e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2585e-10 - val_loss: 4.9951e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0508e-10 - val_loss: 4.8544e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9266e-10 - val_loss: 4.7110e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7891e-10 - val_loss: 4.6999e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8053e-10 - val_loss: 4.6679e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7934e-10 - val_loss: 4.6127e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6903e-10 - val_loss: 4.5322e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6185e-10 - val_loss: 4.4597e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5068e-10 - val_loss: 4.2639e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3701e-10 - val_loss: 4.2499e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3132e-10 - val_loss: 4.1156e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1632e-10 - val_loss: 3.9749e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0204e-10 - val_loss: 3.9694e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0842e-10 - val_loss: 3.9881e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0449e-10 - val_loss: 3.9698e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0655e-10 - val_loss: 3.9576e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0089e-10 - val_loss: 3.8592e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9080e-10 - val_loss: 3.7042e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7861e-10 - val_loss: 3.7146e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7740e-10 - val_loss: 3.6251e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6939e-10 - val_loss: 3.5813e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6536e-10 - val_loss: 3.5843e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6578e-10 - val_loss: 3.5583e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6272e-10 - val_loss: 3.5584e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6268e-10 - val_loss: 3.4321e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4577e-10 - val_loss: 3.2853e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3195e-10 - val_loss: 3.2102e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2823e-10 - val_loss: 3.2035e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2484e-10 - val_loss: 3.1467e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1705e-10 - val_loss: 3.0377e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0752e-10 - val_loss: 2.9829e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0645e-10 - val_loss: 2.9838e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0316e-10 - val_loss: 3.0445e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1235e-10 - val_loss: 3.0532e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0841e-10 - val_loss: 2.9515e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9827e-10 - val_loss: 2.8600e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8774e-10 - val_loss: 2.7452e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7750e-10 - val_loss: 2.6991e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7340e-10 - val_loss: 2.7147e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7551e-10 - val_loss: 2.7208e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7546e-10 - val_loss: 2.6917e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7376e-10 - val_loss: 2.7001e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7230e-10 - val_loss: 2.5761e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5590e-10 - val_loss: 2.4697e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4925e-10 - val_loss: 2.4416e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4849e-10 - val_loss: 2.4595e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5240e-10 - val_loss: 2.5264e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5256e-10 - val_loss: 2.3912e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4511e-10 - val_loss: 2.4453e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4310e-10 - val_loss: 2.2673e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2910e-10 - val_loss: 2.2073e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2558e-10 - val_loss: 2.2625e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2908e-10 - val_loss: 2.1941e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2416e-10 - val_loss: 2.2338e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2868e-10 - val_loss: 2.2184e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2547e-10 - val_loss: 2.2460e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2803e-10 - val_loss: 2.1842e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2076e-10 - val_loss: 2.1064e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1080e-10 - val_loss: 2.0567e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0663e-10 - val_loss: 2.0097e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0270e-10 - val_loss: 2.0211e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0713e-10 - val_loss: 2.0661e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0774e-10 - val_loss: 1.9829e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0144e-10 - val_loss: 1.9532e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9971e-10 - val_loss: 2.0051e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0614e-10 - val_loss: 2.0178e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0073e-10 - val_loss: 1.9498e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9745e-10 - val_loss: 1.9489e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9719e-10 - val_loss: 1.9004e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8914e-10 - val_loss: 1.8298e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8586e-10 - val_loss: 1.8268e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8590e-10 - val_loss: 1.8584e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8877e-10 - val_loss: 1.8607e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8632e-10 - val_loss: 1.7215e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7294e-10 - val_loss: 1.6613e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6948e-10 - val_loss: 1.7037e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7385e-10 - val_loss: 1.7175e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7337e-10 - val_loss: 1.6797e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7024e-10 - val_loss: 1.6801e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7097e-10 - val_loss: 1.6543e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6527e-10 - val_loss: 1.6109e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6238e-10 - val_loss: 1.5922e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6030e-10 - val_loss: 1.5481e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5591e-10 - val_loss: 1.5199e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5502e-10 - val_loss: 1.5678e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6031e-10 - val_loss: 1.5456e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5488e-10 - val_loss: 1.5155e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5445e-10 - val_loss: 1.5180e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5173e-10 - val_loss: 1.4697e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4923e-10 - val_loss: 1.5137e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5390e-10 - val_loss: 1.4706e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4648e-10 - val_loss: 1.4311e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4402e-10 - val_loss: 1.3797e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4018e-10 - val_loss: 1.4222e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4504e-10 - val_loss: 1.4111e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3940e-10 - val_loss: 1.3122e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3130e-10 - val_loss: 1.2608e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2821e-10 - val_loss: 1.2710e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3031e-10 - val_loss: 1.3605e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4043e-10 - val_loss: 1.4306e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4553e-10 - val_loss: 1.3954e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3774e-10 - val_loss: 1.3007e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2989e-10 - val_loss: 1.2226e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2159e-10 - val_loss: 1.1618e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1994e-10 - val_loss: 1.2330e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2764e-10 - val_loss: 1.3017e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3504e-10 - val_loss: 1.2987e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2974e-10 - val_loss: 1.2287e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2331e-10 - val_loss: 1.2025e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2027e-10 - val_loss: 1.1820e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1864e-10 - val_loss: 1.1902e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2215e-10 - val_loss: 1.2645e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2618e-10 - val_loss: 1.2100e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1928e-10 - val_loss: 1.0984e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1030e-10 - val_loss: 1.1311e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1697e-10 - val_loss: 1.1459e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1558e-10 - val_loss: 1.1770e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1880e-10 - val_loss: 1.1580e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1823e-10 - val_loss: 1.1543e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1536e-10 - val_loss: 1.1382e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1414e-10 - val_loss: 1.0911e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0878e-10 - val_loss: 1.0614e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0656e-10 - val_loss: 1.0289e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0513e-10 - val_loss: 1.0585e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0649e-10 - val_loss: 1.0340e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0384e-10 - val_loss: 1.0338e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0461e-10 - val_loss: 1.0546e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0695e-10 - val_loss: 1.0711e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0754e-10 - val_loss: 1.0278e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0456e-10 - val_loss: 1.0258e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0146e-10 - val_loss: 9.9392e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.8644e-11 - val_loss: 9.4180e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.6122e-11 - val_loss: 9.2965e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1692e-11 - val_loss: 8.8962e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9415e-11 - val_loss: 9.1654e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7149e-11 - val_loss: 1.0453e-10
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0591e-10 - val_loss: 1.0508e-10
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0410e-10 - val_loss: 9.8595e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8392e-11 - val_loss: 9.2394e-11
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1920e-11 - val_loss: 8.7468e-11
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.7517e-11 - val_loss: 8.4438e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.4085e-11 - val_loss: 8.3595e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6095e-11 - val_loss: 8.5924e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8200e-11 - val_loss: 8.7280e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9015e-11 - val_loss: 8.8401e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2023e-11 - val_loss: 9.4136e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4643e-11 - val_loss: 9.4004e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2391e-11 - val_loss: 8.7206e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8046e-11 - val_loss: 8.5050e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.6100e-11 - val_loss: 8.0952e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0158e-11 - val_loss: 7.8851e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8053e-11 - val_loss: 7.7752e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8085e-11 - val_loss: 7.7515e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0311e-11 - val_loss: 8.2001e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3288e-11 - val_loss: 8.2335e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3822e-11 - val_loss: 8.3590e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3175e-11 - val_loss: 8.2293e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2573e-11 - val_loss: 8.2171e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1945e-11 - val_loss: 8.0312e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9651e-11 - val_loss: 7.8308e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.7919e-11 - val_loss: 7.6962e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.7304e-11 - val_loss: 7.6925e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.7308e-11 - val_loss: 7.4841e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4830e-11 - val_loss: 7.5856e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7703e-11 - val_loss: 7.6644e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8040e-11 - val_loss: 7.9746e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9944e-11 - val_loss: 8.0429e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9083e-11 - val_loss: 7.6624e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6749e-11 - val_loss: 7.6143e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.6439e-11 - val_loss: 7.4192e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.3983e-11 - val_loss: 7.3199e-11
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2036e-11 - val_loss: 7.1598e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3446e-11 - val_loss: 7.2610e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.3522e-11 - val_loss: 7.0393e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0299e-11 - val_loss: 6.8327e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.9270e-11 - val_loss: 6.5395e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6171e-11 - val_loss: 6.8135e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8762e-11 - val_loss: 6.8925e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0586e-11 - val_loss: 6.9636e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0032e-11 - val_loss: 6.6020e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.6396e-11 - val_loss: 6.4251e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5215e-11 - val_loss: 6.6269e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5956e-11 - val_loss: 6.6243e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7805e-11 - val_loss: 6.4257e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5602e-11 - val_loss: 6.7414e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5377e-11 - val_loss: 5.8179e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7560e-11 - val_loss: 5.3183e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4357e-11 - val_loss: 5.5697e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5217e-11 - val_loss: 5.5125e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7128e-11 - val_loss: 6.0957e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2757e-11 - val_loss: 6.2298e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4010e-11 - val_loss: 6.6529e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7468e-11 - val_loss: 6.4580e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5857e-11 - val_loss: 6.4420e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4900e-11 - val_loss: 6.3914e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4933e-11 - val_loss: 6.3596e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2909e-11 - val_loss: 6.1446e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1961e-11 - val_loss: 5.8646e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8867e-11 - val_loss: 5.6299e-11
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6717e-11 - val_loss: 5.2482e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2113e-11 - val_loss: 4.8532e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9175e-11 - val_loss: 5.0358e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1428e-11 - val_loss: 5.0833e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2658e-11 - val_loss: 5.6481e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8604e-11 - val_loss: 6.1758e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3333e-11 - val_loss: 5.9042e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0112e-11 - val_loss: 6.2302e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1844e-11 - val_loss: 6.0851e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1616e-11 - val_loss: 5.8302e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9407e-11 - val_loss: 5.7920e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6973e-11 - val_loss: 5.2807e-11
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1922e-11 - val_loss: 4.7444e-11
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7331e-11 - val_loss: 4.4154e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3895e-11 - val_loss: 4.4469e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6379e-11 - val_loss: 4.9551e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1041e-11 - val_loss: 4.9592e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0614e-11 - val_loss: 5.2079e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3844e-11 - val_loss: 5.6622e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6258e-11 - val_loss: 5.6434e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6783e-11 - val_loss: 5.3370e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3638e-11 - val_loss: 5.4121e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4384e-11 - val_loss: 5.2494e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3561e-11 - val_loss: 5.2548e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3191e-11 - val_loss: 5.1207e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9879e-11 - val_loss: 4.7247e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5931e-11 - val_loss: 4.4344e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5806e-11 - val_loss: 4.5774e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4716e-11 - val_loss: 4.4553e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6192e-11 - val_loss: 4.6715e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5385e-11 - val_loss: 4.4619e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6061e-11 - val_loss: 4.8870e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0557e-11 - val_loss: 4.8546e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8545e-11 - val_loss: 4.8947e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0611e-11 - val_loss: 4.9529e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0441e-11 - val_loss: 5.2163e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4545e-11 - val_loss: 5.7216e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6424e-11 - val_loss: 5.2990e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0898e-11 - val_loss: 4.9826e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0746e-11 - val_loss: 4.6940e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6272e-11 - val_loss: 4.3024e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2770e-11 - val_loss: 4.3347e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4611e-11 - val_loss: 4.3237e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3471e-11 - val_loss: 4.3891e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4888e-11 - val_loss: 4.2964e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3134e-11 - val_loss: 4.5080e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6271e-11 - val_loss: 4.8575e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9562e-11 - val_loss: 4.7423e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7127e-11 - val_loss: 4.2891e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2359e-11 - val_loss: 3.9390e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9434e-11 - val_loss: 3.9764e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1161e-11 - val_loss: 4.1986e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0507e-11 - val_loss: 3.9762e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1171e-11 - val_loss: 4.2349e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4656e-11 - val_loss: 4.7358e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8932e-11 - val_loss: 5.1659e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0675e-11 - val_loss: 4.6270e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4765e-11 - val_loss: 4.2494e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0298e-11 - val_loss: 3.7811e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9248e-11 - val_loss: 4.1199e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0397e-11 - val_loss: 3.8297e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7979e-11 - val_loss: 3.5117e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5802e-11 - val_loss: 3.6879e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7746e-11 - val_loss: 3.7721e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6663e-11 - val_loss: 3.6293e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7544e-11 - val_loss: 3.8305e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9551e-11 - val_loss: 4.0933e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1739e-11 - val_loss: 3.8576e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8184e-11 - val_loss: 3.5219e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6070e-11 - val_loss: 3.8087e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8259e-11 - val_loss: 3.9642e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0843e-11 - val_loss: 4.1368e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0487e-11 - val_loss: 3.7761e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8617e-11 - val_loss: 4.0558e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0177e-11 - val_loss: 3.7823e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7630e-11 - val_loss: 3.4868e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4631e-11 - val_loss: 3.4653e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5748e-11 - val_loss: 3.7284e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6291e-11 - val_loss: 3.4556e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5171e-11 - val_loss: 3.6666e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7141e-11 - val_loss: 3.4902e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4975e-11 - val_loss: 3.5731e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6874e-11 - val_loss: 3.7162e-11
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6132e-11 - val_loss: 3.3889e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4056e-11 - val_loss: 3.5195e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6004e-11 - val_loss: 3.4241e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4136e-11 - val_loss: 3.5474e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6898e-11 - val_loss: 3.7817e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8945e-11 - val_loss: 4.1440e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2042e-11 - val_loss: 3.8707e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8441e-11 - val_loss: 3.5460e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4777e-11 - val_loss: 3.1843e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0296e-11 - val_loss: 2.7539e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6369e-11 - val_loss: 2.4842e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5072e-11 - val_loss: 2.5336e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6606e-11 - val_loss: 2.8781e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9292e-11 - val_loss: 3.0131e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1729e-11 - val_loss: 3.3256e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2298e-11 - val_loss: 3.0153e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0904e-11 - val_loss: 3.2782e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3301e-11 - val_loss: 3.3425e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2559e-11 - val_loss: 3.1991e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2799e-11 - val_loss: 3.3894e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4324e-11 - val_loss: 3.6076e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7265e-11 - val_loss: 3.7435e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6104e-11 - val_loss: 3.4024e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4639e-11 - val_loss: 3.6101e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5263e-11 - val_loss: 3.3277e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3070e-11 - val_loss: 3.0462e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0563e-11 - val_loss: 2.9956e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0727e-11 - val_loss: 3.1896e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2600e-11 - val_loss: 3.0620e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0751e-11 - val_loss: 3.1231e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2155e-11 - val_loss: 3.3157e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3301e-11 - val_loss: 3.0604e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9615e-11 - val_loss: 2.8184e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7120e-11 - val_loss: 2.5711e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5746e-11 - val_loss: 2.6580e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7520e-11 - val_loss: 2.8309e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7614e-11 - val_loss: 2.5423e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5130e-11 - val_loss: 2.5102e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3700e-11 - val_loss: 2.2032e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1909e-11 - val_loss: 2.1503e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0614e-11 - val_loss: 1.9697e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9686e-11 - val_loss: 2.0731e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1978e-11 - val_loss: 2.2942e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2905e-11 - val_loss: 2.2304e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3242e-11 - val_loss: 2.4705e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5617e-11 - val_loss: 2.7862e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9419e-11 - val_loss: 3.0252e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1636e-11 - val_loss: 3.3734e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3655e-11 - val_loss: 3.2789e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1625e-11 - val_loss: 2.9806e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9443e-11 - val_loss: 2.6968e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6954e-11 - val_loss: 2.7265e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8158e-11 - val_loss: 2.9116e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9663e-11 - val_loss: 3.0959e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1752e-11 - val_loss: 3.0184e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9972e-11 - val_loss: 2.9089e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8610e-11 - val_loss: 2.5578e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5456e-11 - val_loss: 2.4668e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3031e-11 - val_loss: 2.1073e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1171e-11 - val_loss: 2.1292e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1383e-11 - val_loss: 1.9587e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9158e-11 - val_loss: 1.8159e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8228e-11 - val_loss: 1.9392e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0478e-11 - val_loss: 2.1890e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2550e-11 - val_loss: 2.3877e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4834e-11 - val_loss: 2.4777e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5017e-11 - val_loss: 2.6220e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7143e-11 - val_loss: 2.8205e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6989e-11 - val_loss: 2.4966e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5332e-11 - val_loss: 2.5408e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3900e-11 - val_loss: 2.1685e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1935e-11 - val_loss: 2.3257e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4147e-11 - val_loss: 2.4656e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4764e-11 - val_loss: 2.2916e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2485e-11 - val_loss: 2.1744e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1881e-11 - val_loss: 2.3131e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4072e-11 - val_loss: 2.4813e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5771e-11 - val_loss: 2.7743e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8911e-11 - val_loss: 3.0091e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9679e-11 - val_loss: 2.7245e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7084e-11 - val_loss: 2.7693e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8729e-11 - val_loss: 2.9202e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9211e-11 - val_loss: 2.6326e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6076e-11 - val_loss: 2.5732e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6734e-11 - val_loss: 2.8828e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9127e-11 - val_loss: 2.8783e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7708e-11 - val_loss: 2.5342e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4584e-11 - val_loss: 2.1938e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1546e-11 - val_loss: 2.1088e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1296e-11 - val_loss: 1.9664e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9146e-11 - val_loss: 1.8613e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8789e-11 - val_loss: 1.9775e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0562e-11 - val_loss: 2.1008e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1395e-11 - val_loss: 2.1942e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2898e-11 - val_loss: 2.4369e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4740e-11 - val_loss: 2.4611e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3588e-11 - val_loss: 2.1749e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1321e-11 - val_loss: 1.9226e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9059e-11 - val_loss: 1.8653e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8821e-11 - val_loss: 1.9796e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0688e-11 - val_loss: 2.1260e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1505e-11 - val_loss: 2.1879e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2667e-11 - val_loss: 2.4027e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4656e-11 - val_loss: 2.4735e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3465e-11 - val_loss: 2.1631e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1788e-11 - val_loss: 2.1832e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2350e-11 - val_loss: 2.3534e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3969e-11 - val_loss: 2.2478e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2277e-11 - val_loss: 2.1027e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1025e-11 - val_loss: 2.0714e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9825e-11 - val_loss: 1.8649e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8694e-11 - val_loss: 1.8194e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8130e-11 - val_loss: 1.6618e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5759e-11 - val_loss: 1.4688e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4766e-11 - val_loss: 1.4679e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4654e-11 - val_loss: 1.3046e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3020e-11 - val_loss: 1.2592e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2418e-11 - val_loss: 1.2104e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2193e-11 - val_loss: 1.2162e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2729e-11 - val_loss: 1.4162e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4497e-11 - val_loss: 1.5505e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6129e-11 - val_loss: 1.7180e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8011e-11 - val_loss: 1.8222e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8328e-11 - val_loss: 1.8452e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8537e-11 - val_loss: 2.0146e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1072e-11 - val_loss: 2.1201e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1698e-11 - val_loss: 2.1372e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0254e-11 - val_loss: 1.8808e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8972e-11 - val_loss: 2.0115e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0851e-11 - val_loss: 2.0927e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1047e-11 - val_loss: 2.1091e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0485e-11 - val_loss: 1.8607e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8545e-11 - val_loss: 1.8443e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8583e-11 - val_loss: 1.9161e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0078e-11 - val_loss: 2.0972e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1428e-11 - val_loss: 2.1418e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0532e-11 - val_loss: 1.9055e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8761e-11 - val_loss: 1.7984e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7096e-11 - val_loss: 1.6230e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6178e-11 - val_loss: 1.6034e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6541e-11 - val_loss: 1.7721e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8195e-11 - val_loss: 1.8621e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8774e-11 - val_loss: 1.8270e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8857e-11 - val_loss: 2.0279e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1095e-11 - val_loss: 2.1381e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1331e-11 - val_loss: 1.9186e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8949e-11 - val_loss: 1.8406e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8536e-11 - val_loss: 1.9423e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0165e-11 - val_loss: 2.0568e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0763e-11 - val_loss: 1.9435e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9119e-11 - val_loss: 1.7657e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7448e-11 - val_loss: 1.5811e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5777e-11 - val_loss: 1.5507e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4884e-11 - val_loss: 1.4369e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4118e-11 - val_loss: 1.2350e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2116e-11 - val_loss: 1.1794e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1870e-11 - val_loss: 1.1833e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1936e-11 - val_loss: 1.1932e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2239e-11 - val_loss: 1.2767e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3375e-11 - val_loss: 1.4581e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4851e-11 - val_loss: 1.5194e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5133e-11 - val_loss: 1.4818e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3956e-11 - val_loss: 1.2591e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2325e-11 - val_loss: 1.1920e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1981e-11 - val_loss: 1.1906e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1998e-11 - val_loss: 1.2160e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2911e-11 - val_loss: 1.4689e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5109e-11 - val_loss: 1.5525e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5999e-11 - val_loss: 1.7205e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7830e-11 - val_loss: 1.8060e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8126e-11 - val_loss: 1.7911e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8284e-11 - val_loss: 1.9920e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0695e-11 - val_loss: 2.0781e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0854e-11 - val_loss: 1.9101e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8835e-11 - val_loss: 1.8247e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8299e-11 - val_loss: 1.7437e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6700e-11 - val_loss: 1.5744e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5576e-11 - val_loss: 1.5225e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5342e-11 - val_loss: 1.5278e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4263e-11 - val_loss: 1.3073e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3060e-11 - val_loss: 1.2648e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2681e-11 - val_loss: 1.2593e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3438e-11 - val_loss: 1.4561e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4991e-11 - val_loss: 1.5332e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5549e-11 - val_loss: 1.5520e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6153e-11 - val_loss: 1.7891e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8142e-11 - val_loss: 1.8408e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8463e-11 - val_loss: 1.8393e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7368e-11 - val_loss: 1.5547e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5484e-11 - val_loss: 1.5206e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5431e-11 - val_loss: 1.6406e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7085e-11 - val_loss: 1.7712e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7908e-11 - val_loss: 1.7804e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6907e-11 - val_loss: 1.5437e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5409e-11 - val_loss: 1.5180e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5297e-11 - val_loss: 1.5221e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4400e-11 - val_loss: 1.3095e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3035e-11 - val_loss: 1.2580e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.3523 - val_loss: 0.0777
Epoch 2/512
448/448 - 0s - loss: 0.0477 - val_loss: 0.0343
Epoch 3/512
448/448 - 0s - loss: 0.0265 - val_loss: 0.0139
Epoch 4/512
448/448 - 0s - loss: 0.0137 - val_loss: 0.0039
Epoch 5/512
448/448 - 0s - loss: 0.0073 - val_loss: 8.1600e-04
Epoch 6/512
448/448 - 0s - loss: 0.0047 - val_loss: 5.0460e-04
Epoch 7/512
448/448 - 0s - loss: 0.0063 - val_loss: 4.7434e-04
Epoch 8/512
448/448 - 0s - loss: 0.0054 - val_loss: 0.0011
Epoch 9/512
448/448 - 0s - loss: 0.0040 - val_loss: 0.0012
Epoch 10/512
448/448 - 0s - loss: 0.0043 - val_loss: 0.0010
Epoch 11/512
448/448 - 0s - loss: 0.0047 - val_loss: 7.9733e-04
Epoch 12/512
448/448 - 0s - loss: 0.0058 - val_loss: 9.0737e-04
Epoch 13/512
448/448 - 0s - loss: 0.0042 - val_loss: 0.0014
Epoch 14/512
448/448 - 0s - loss: 0.0036 - val_loss: 9.2377e-04
Epoch 15/512
448/448 - 0s - loss: 0.0053 - val_loss: 6.7039e-04
Epoch 16/512
448/448 - 0s - loss: 0.0042 - val_loss: 0.0020
Epoch 17/512
448/448 - 0s - loss: 0.0029 - val_loss: 0.0012
Epoch 18/512
448/448 - 0s - loss: 0.0059 - val_loss: 7.9077e-04
Epoch 19/512
448/448 - 0s - loss: 0.0036 - val_loss: 0.0019
Epoch 20/512
448/448 - 0s - loss: 0.0026 - val_loss: 0.0023
Epoch 21/512
448/448 - 0s - loss: 0.0024 - val_loss: 0.0020
Epoch 22/512
448/448 - 0s - loss: 0.0045 - val_loss: 3.7069e-04
Epoch 23/512
448/448 - 0s - loss: 0.0039 - val_loss: 0.0026
Epoch 24/512
448/448 - 0s - loss: 0.0024 - val_loss: 0.0015
Epoch 25/512
448/448 - 0s - loss: 0.0022 - val_loss: 0.0018
Epoch 26/512
448/448 - 0s - loss: 0.0020 - val_loss: 0.0017
Epoch 27/512
448/448 - 0s - loss: 0.0021 - val_loss: 7.8784e-04
Epoch 28/512
448/448 - 0s - loss: 0.0034 - val_loss: 7.4935e-04
Epoch 29/512
448/448 - 0s - loss: 0.0020 - val_loss: 0.0017
Epoch 30/512
448/448 - 0s - loss: 0.0016 - val_loss: 0.0034
Epoch 31/512
448/448 - 0s - loss: 0.0016 - val_loss: 0.0046
Epoch 32/512
448/448 - 0s - loss: 0.0024 - val_loss: 0.0049
Epoch 33/512
448/448 - 0s - loss: 0.0017 - val_loss: 0.0038
Epoch 34/512
448/448 - 0s - loss: 0.0014 - val_loss: 0.0052
Epoch 35/512
448/448 - 0s - loss: 0.0015 - val_loss: 0.0061
Epoch 36/512
448/448 - 0s - loss: 0.0016 - val_loss: 0.0048
Epoch 37/512
448/448 - 0s - loss: 0.0013 - val_loss: 0.0048
Epoch 38/512
448/448 - 0s - loss: 0.0014 - val_loss: 0.0039
Epoch 39/512
448/448 - 0s - loss: 0.0014 - val_loss: 0.0029
Epoch 40/512
448/448 - 0s - loss: 0.0010 - val_loss: 0.0030
Epoch 41/512
448/448 - 0s - loss: 9.4090e-04 - val_loss: 0.0028
Epoch 42/512
448/448 - 0s - loss: 9.8255e-04 - val_loss: 3.9018e-04
Epoch 43/512
448/448 - 0s - loss: 0.0012 - val_loss: 7.0305e-04
Epoch 44/512
448/448 - 0s - loss: 6.7530e-04 - val_loss: 9.1936e-04
Epoch 45/512
448/448 - 0s - loss: 6.1758e-04 - val_loss: 4.7909e-04
Epoch 46/512
448/448 - 0s - loss: 6.3074e-04 - val_loss: 4.7350e-04
Epoch 47/512
448/448 - 0s - loss: 4.6973e-04 - val_loss: 5.0869e-04
Epoch 48/512
448/448 - 0s - loss: 4.2263e-04 - val_loss: 2.2139e-04
Epoch 49/512
448/448 - 0s - loss: 4.6487e-04 - val_loss: 5.0006e-04
Epoch 50/512
448/448 - 0s - loss: 2.6822e-04 - val_loss: 6.9028e-04
Epoch 51/512
448/448 - 0s - loss: 2.0950e-04 - val_loss: 4.8484e-04
Epoch 52/512
448/448 - 0s - loss: 2.2595e-04 - val_loss: 1.9135e-04
Epoch 53/512
448/448 - 0s - loss: 2.3130e-04 - val_loss: 3.0057e-04
Epoch 54/512
448/448 - 0s - loss: 1.3848e-04 - val_loss: 3.6725e-04
Epoch 55/512
448/448 - 0s - loss: 1.1645e-04 - val_loss: 2.5947e-04
Epoch 56/512
448/448 - 0s - loss: 1.2952e-04 - val_loss: 1.4289e-04
Epoch 57/512
448/448 - 0s - loss: 1.1060e-04 - val_loss: 1.7012e-04
Epoch 58/512
448/448 - 0s - loss: 7.5113e-05 - val_loss: 1.8447e-04
Epoch 59/512
448/448 - 0s - loss: 6.8587e-05 - val_loss: 1.4994e-04
Epoch 60/512
448/448 - 0s - loss: 7.4338e-05 - val_loss: 1.1245e-04
Epoch 61/512
448/448 - 0s - loss: 5.3486e-05 - val_loss: 9.1548e-05
Epoch 62/512
448/448 - 0s - loss: 4.5527e-05 - val_loss: 7.0221e-05
Epoch 63/512
448/448 - 0s - loss: 4.1931e-05 - val_loss: 5.7685e-05
Epoch 64/512
448/448 - 0s - loss: 5.7310e-05 - val_loss: 1.0095e-04
Epoch 65/512
448/448 - 0s - loss: 2.5257e-05 - val_loss: 1.0619e-04
Epoch 66/512
448/448 - 0s - loss: 1.9981e-05 - val_loss: 6.2604e-05
Epoch 67/512
448/448 - 0s - loss: 2.5325e-05 - val_loss: 3.0195e-05
Epoch 68/512
448/448 - 0s - loss: 2.7073e-05 - val_loss: 3.6357e-05
Epoch 69/512
448/448 - 0s - loss: 3.0058e-05 - val_loss: 8.4790e-05
Epoch 70/512
448/448 - 0s - loss: 1.1668e-05 - val_loss: 8.0659e-05
Epoch 71/512
448/448 - 0s - loss: 9.9737e-06 - val_loss: 8.9191e-05
Epoch 72/512
448/448 - 0s - loss: 1.3843e-05 - val_loss: 1.5603e-04
Epoch 73/512
448/448 - 0s - loss: 2.1365e-05 - val_loss: 8.1332e-05
Epoch 74/512
448/448 - 0s - loss: 7.6908e-06 - val_loss: 5.4979e-05
Epoch 75/512
448/448 - 0s - loss: 6.1866e-06 - val_loss: 4.7696e-05
Epoch 76/512
448/448 - 0s - loss: 6.5173e-06 - val_loss: 5.5500e-05
Epoch 77/512
448/448 - 0s - loss: 1.0768e-05 - val_loss: 7.3557e-05
Epoch 78/512
448/448 - 0s - loss: 6.1130e-06 - val_loss: 4.2991e-05
Epoch 79/512
448/448 - 0s - loss: 3.8754e-06 - val_loss: 3.4670e-05
Epoch 80/512
448/448 - 0s - loss: 3.5558e-06 - val_loss: 3.3624e-05
Epoch 81/512
448/448 - 0s - loss: 4.5222e-06 - val_loss: 4.4221e-05
Epoch 82/512
448/448 - 0s - loss: 3.8830e-06 - val_loss: 3.0329e-05
Epoch 83/512
448/448 - 0s - loss: 2.2315e-06 - val_loss: 2.3951e-05
Epoch 84/512
448/448 - 0s - loss: 1.8151e-06 - val_loss: 2.1785e-05
Epoch 85/512
448/448 - 0s - loss: 1.6996e-06 - val_loss: 2.1090e-05
Epoch 86/512
448/448 - 0s - loss: 1.9542e-06 - val_loss: 1.9061e-05
Epoch 87/512
448/448 - 0s - loss: 1.2944e-06 - val_loss: 1.3406e-05
Epoch 88/512
448/448 - 0s - loss: 8.3649e-07 - val_loss: 1.0578e-05
Epoch 89/512
448/448 - 0s - loss: 6.9051e-07 - val_loss: 8.8625e-06
Epoch 90/512
448/448 - 0s - loss: 7.0110e-07 - val_loss: 8.6057e-06
Epoch 91/512
448/448 - 0s - loss: 8.5937e-07 - val_loss: 6.4034e-06
Epoch 92/512
448/448 - 0s - loss: 5.0791e-07 - val_loss: 4.6887e-06
Epoch 93/512
448/448 - 0s - loss: 3.5419e-07 - val_loss: 3.7933e-06
Epoch 94/512
448/448 - 0s - loss: 3.2138e-07 - val_loss: 3.7942e-06
Epoch 95/512
448/448 - 0s - loss: 4.1365e-07 - val_loss: 3.6068e-06
Epoch 96/512
448/448 - 0s - loss: 3.5193e-07 - val_loss: 2.4100e-06
Epoch 97/512
448/448 - 0s - loss: 1.9668e-07 - val_loss: 1.9327e-06
Epoch 98/512
448/448 - 0s - loss: 1.6340e-07 - val_loss: 1.8248e-06
Epoch 99/512
448/448 - 0s - loss: 1.7959e-07 - val_loss: 1.8084e-06
Epoch 100/512
448/448 - 0s - loss: 2.0615e-07 - val_loss: 1.3858e-06
Epoch 101/512
448/448 - 0s - loss: 1.2482e-07 - val_loss: 1.0197e-06
Epoch 102/512
448/448 - 0s - loss: 9.0645e-08 - val_loss: 8.7940e-07
Epoch 103/512
448/448 - 0s - loss: 8.9717e-08 - val_loss: 8.8165e-07
Epoch 104/512
448/448 - 0s - loss: 1.0524e-07 - val_loss: 8.1128e-07
Epoch 105/512
448/448 - 0s - loss: 8.6742e-08 - val_loss: 6.0421e-07
Epoch 106/512
448/448 - 0s - loss: 5.6705e-08 - val_loss: 5.1012e-07
Epoch 107/512
448/448 - 0s - loss: 4.9762e-08 - val_loss: 4.4245e-07
Epoch 108/512
448/448 - 0s - loss: 5.6061e-08 - val_loss: 4.4309e-07
Epoch 109/512
448/448 - 0s - loss: 5.8231e-08 - val_loss: 3.6512e-07
Epoch 110/512
448/448 - 0s - loss: 4.1312e-08 - val_loss: 3.1329e-07
Epoch 111/512
448/448 - 0s - loss: 3.3356e-08 - val_loss: 2.6742e-07
Epoch 112/512
448/448 - 0s - loss: 3.3862e-08 - val_loss: 2.4313e-07
Epoch 113/512
448/448 - 0s - loss: 3.6331e-08 - val_loss: 2.3649e-07
Epoch 114/512
448/448 - 0s - loss: 3.1630e-08 - val_loss: 2.0790e-07
Epoch 115/512
448/448 - 0s - loss: 2.6044e-08 - val_loss: 1.7457e-07
Epoch 116/512
448/448 - 0s - loss: 2.3691e-08 - val_loss: 1.6510e-07
Epoch 117/512
448/448 - 0s - loss: 2.2790e-08 - val_loss: 1.4859e-07
Epoch 118/512
448/448 - 0s - loss: 2.2245e-08 - val_loss: 1.4168e-07
Epoch 119/512
448/448 - 0s - loss: 2.0579e-08 - val_loss: 1.3049e-07
Epoch 120/512
448/448 - 0s - loss: 1.8623e-08 - val_loss: 1.1542e-07
Epoch 121/512
448/448 - 0s - loss: 1.7309e-08 - val_loss: 1.0753e-07
Epoch 122/512
448/448 - 0s - loss: 1.6201e-08 - val_loss: 1.0354e-07
Epoch 123/512
448/448 - 0s - loss: 1.5497e-08 - val_loss: 9.8791e-08
Epoch 124/512
448/448 - 0s - loss: 1.4757e-08 - val_loss: 9.3522e-08
Epoch 125/512
448/448 - 0s - loss: 1.3820e-08 - val_loss: 8.9125e-08
Epoch 126/512
448/448 - 0s - loss: 1.2589e-08 - val_loss: 8.0365e-08
Epoch 127/512
448/448 - 0s - loss: 1.1830e-08 - val_loss: 7.6934e-08
Epoch 128/512
448/448 - 0s - loss: 1.1429e-08 - val_loss: 7.4868e-08
Epoch 129/512
448/448 - 0s - loss: 1.1166e-08 - val_loss: 7.0966e-08
Epoch 130/512
448/448 - 0s - loss: 1.0569e-08 - val_loss: 6.6804e-08
Epoch 131/512
448/448 - 0s - loss: 9.8755e-09 - val_loss: 6.4677e-08
Epoch 132/512
448/448 - 0s - loss: 9.3186e-09 - val_loss: 6.0838e-08
Epoch 133/512
448/448 - 0s - loss: 8.9717e-09 - val_loss: 5.8527e-08
Epoch 134/512
448/448 - 0s - loss: 8.8986e-09 - val_loss: 5.6836e-08
Epoch 135/512
448/448 - 0s - loss: 8.3473e-09 - val_loss: 5.3834e-08
Epoch 136/512
448/448 - 0s - loss: 7.9707e-09 - val_loss: 5.2841e-08
Epoch 137/512
448/448 - 0s - loss: 7.6764e-09 - val_loss: 5.0924e-08
Epoch 138/512
448/448 - 0s - loss: 7.3111e-09 - val_loss: 4.8581e-08
Epoch 139/512
448/448 - 0s - loss: 7.0869e-09 - val_loss: 4.6698e-08
Epoch 140/512
448/448 - 0s - loss: 6.7847e-09 - val_loss: 4.5173e-08
Epoch 141/512
448/448 - 0s - loss: 6.5816e-09 - val_loss: 4.3908e-08
Epoch 142/512
448/448 - 0s - loss: 6.3265e-09 - val_loss: 4.2568e-08
Epoch 143/512
448/448 - 0s - loss: 6.0810e-09 - val_loss: 4.0614e-08
Epoch 144/512
448/448 - 0s - loss: 5.9129e-09 - val_loss: 3.9836e-08
Epoch 145/512
448/448 - 0s - loss: 5.7850e-09 - val_loss: 3.8821e-08
Epoch 146/512
448/448 - 0s - loss: 5.5599e-09 - val_loss: 3.7653e-08
Epoch 147/512
448/448 - 0s - loss: 5.4590e-09 - val_loss: 3.6368e-08
Epoch 148/512
448/448 - 0s - loss: 5.2939e-09 - val_loss: 3.5270e-08
Epoch 149/512
448/448 - 0s - loss: 5.0617e-09 - val_loss: 3.4185e-08
Epoch 150/512
448/448 - 0s - loss: 4.8921e-09 - val_loss: 3.3837e-08
Epoch 151/512
448/448 - 0s - loss: 4.8554e-09 - val_loss: 3.3028e-08
Epoch 152/512
448/448 - 0s - loss: 4.6940e-09 - val_loss: 3.1773e-08
Epoch 153/512
448/448 - 0s - loss: 4.5804e-09 - val_loss: 3.1486e-08
Epoch 154/512
448/448 - 0s - loss: 4.4697e-09 - val_loss: 3.0336e-08
Epoch 155/512
448/448 - 0s - loss: 4.3502e-09 - val_loss: 2.9435e-08
Epoch 156/512
448/448 - 0s - loss: 4.2203e-09 - val_loss: 2.8895e-08
Epoch 157/512
448/448 - 0s - loss: 4.1284e-09 - val_loss: 2.8449e-08
Epoch 158/512
448/448 - 0s - loss: 4.0398e-09 - val_loss: 2.8082e-08
Epoch 159/512
448/448 - 0s - loss: 4.0035e-09 - val_loss: 2.7331e-08
Epoch 160/512
448/448 - 0s - loss: 3.8980e-09 - val_loss: 2.6858e-08
Epoch 161/512
448/448 - 0s - loss: 3.8015e-09 - val_loss: 2.5994e-08
Epoch 162/512
448/448 - 0s - loss: 3.6897e-09 - val_loss: 2.5292e-08
Epoch 163/512
448/448 - 0s - loss: 3.6108e-09 - val_loss: 2.4785e-08
Epoch 164/512
448/448 - 0s - loss: 3.5345e-09 - val_loss: 2.4046e-08
Epoch 165/512
448/448 - 0s - loss: 3.4952e-09 - val_loss: 2.3573e-08
Epoch 166/512
448/448 - 0s - loss: 3.4298e-09 - val_loss: 2.3014e-08
Epoch 167/512
448/448 - 0s - loss: 3.3466e-09 - val_loss: 2.2522e-08
Epoch 168/512
448/448 - 0s - loss: 3.2628e-09 - val_loss: 2.2290e-08
Epoch 169/512
448/448 - 0s - loss: 3.2204e-09 - val_loss: 2.1771e-08
Epoch 170/512
448/448 - 0s - loss: 3.1709e-09 - val_loss: 2.1303e-08
Epoch 171/512
448/448 - 0s - loss: 3.1418e-09 - val_loss: 2.0968e-08
Epoch 172/512
448/448 - 0s - loss: 3.1314e-09 - val_loss: 2.0678e-08
Epoch 173/512
448/448 - 0s - loss: 3.0516e-09 - val_loss: 2.0108e-08
Epoch 174/512
448/448 - 0s - loss: 2.9402e-09 - val_loss: 1.9677e-08
Epoch 175/512
448/448 - 0s - loss: 2.8631e-09 - val_loss: 1.9251e-08
Epoch 176/512
448/448 - 0s - loss: 2.8294e-09 - val_loss: 1.8883e-08
Epoch 177/512
448/448 - 0s - loss: 2.8114e-09 - val_loss: 1.8698e-08
Epoch 178/512
448/448 - 0s - loss: 2.8314e-09 - val_loss: 1.8412e-08
Epoch 179/512
448/448 - 0s - loss: 2.7856e-09 - val_loss: 1.8098e-08
Epoch 180/512
448/448 - 0s - loss: 2.7196e-09 - val_loss: 1.7877e-08
Epoch 181/512
448/448 - 0s - loss: 2.6517e-09 - val_loss: 1.7376e-08
Epoch 182/512
448/448 - 0s - loss: 2.5965e-09 - val_loss: 1.7095e-08
Epoch 183/512
448/448 - 0s - loss: 2.5642e-09 - val_loss: 1.6813e-08
Epoch 184/512
448/448 - 0s - loss: 2.5388e-09 - val_loss: 1.6667e-08
Epoch 185/512
448/448 - 0s - loss: 2.5106e-09 - val_loss: 1.6323e-08
Epoch 186/512
448/448 - 0s - loss: 2.4778e-09 - val_loss: 1.6165e-08
Epoch 187/512
448/448 - 0s - loss: 2.4508e-09 - val_loss: 1.5930e-08
Epoch 188/512
448/448 - 0s - loss: 2.4106e-09 - val_loss: 1.5589e-08
Epoch 189/512
448/448 - 0s - loss: 2.3555e-09 - val_loss: 1.5229e-08
Epoch 190/512
448/448 - 0s - loss: 2.3157e-09 - val_loss: 1.5067e-08
Epoch 191/512
448/448 - 0s - loss: 2.3126e-09 - val_loss: 1.4857e-08
Epoch 192/512
448/448 - 0s - loss: 2.3076e-09 - val_loss: 1.4711e-08
Epoch 193/512
448/448 - 0s - loss: 2.2998e-09 - val_loss: 1.4504e-08
Epoch 194/512
448/448 - 0s - loss: 2.2779e-09 - val_loss: 1.4346e-08
Epoch 195/512
448/448 - 0s - loss: 2.2461e-09 - val_loss: 1.4125e-08
Epoch 196/512
448/448 - 0s - loss: 2.2060e-09 - val_loss: 1.3909e-08
Epoch 197/512
448/448 - 0s - loss: 2.1685e-09 - val_loss: 1.3636e-08
Epoch 198/512
448/448 - 0s - loss: 2.1220e-09 - val_loss: 1.3435e-08
Epoch 199/512
448/448 - 0s - loss: 2.1028e-09 - val_loss: 1.3208e-08
Epoch 200/512
448/448 - 0s - loss: 2.0766e-09 - val_loss: 1.3021e-08
Epoch 201/512
448/448 - 0s - loss: 2.0633e-09 - val_loss: 1.2799e-08
Epoch 202/512
448/448 - 0s - loss: 2.0500e-09 - val_loss: 1.2676e-08
Epoch 203/512
448/448 - 0s - loss: 2.0273e-09 - val_loss: 1.2486e-08
Epoch 204/512
448/448 - 0s - loss: 2.0098e-09 - val_loss: 1.2394e-08
Epoch 205/512
448/448 - 0s - loss: 1.9847e-09 - val_loss: 1.2194e-08
Epoch 206/512
448/448 - 0s - loss: 1.9625e-09 - val_loss: 1.2024e-08
Epoch 207/512
448/448 - 0s - loss: 1.9340e-09 - val_loss: 1.1888e-08
Epoch 208/512
448/448 - 0s - loss: 1.8999e-09 - val_loss: 1.1701e-08
Epoch 209/512
448/448 - 0s - loss: 1.8840e-09 - val_loss: 1.1596e-08
Epoch 210/512
448/448 - 0s - loss: 1.8732e-09 - val_loss: 1.1471e-08
Epoch 211/512
448/448 - 0s - loss: 1.8586e-09 - val_loss: 1.1354e-08
Epoch 212/512
448/448 - 0s - loss: 1.8562e-09 - val_loss: 1.1239e-08
Epoch 213/512
448/448 - 0s - loss: 1.8414e-09 - val_loss: 1.1122e-08
Epoch 214/512
448/448 - 0s - loss: 1.8301e-09 - val_loss: 1.0989e-08
Epoch 215/512
448/448 - 0s - loss: 1.8074e-09 - val_loss: 1.0933e-08
Epoch 216/512
448/448 - 0s - loss: 1.8070e-09 - val_loss: 1.0763e-08
Epoch 217/512
448/448 - 0s - loss: 1.7855e-09 - val_loss: 1.0606e-08
Epoch 218/512
448/448 - 0s - loss: 1.7630e-09 - val_loss: 1.0531e-08
Epoch 219/512
448/448 - 0s - loss: 1.7446e-09 - val_loss: 1.0360e-08
Epoch 220/512
448/448 - 0s - loss: 1.7263e-09 - val_loss: 1.0255e-08
Epoch 221/512
448/448 - 0s - loss: 1.7057e-09 - val_loss: 1.0112e-08
Epoch 222/512
448/448 - 0s - loss: 1.6878e-09 - val_loss: 1.0022e-08
Epoch 223/512
448/448 - 0s - loss: 1.6730e-09 - val_loss: 9.9084e-09
Epoch 224/512
448/448 - 0s - loss: 1.6645e-09 - val_loss: 9.8074e-09
Epoch 225/512
448/448 - 0s - loss: 1.6504e-09 - val_loss: 9.6425e-09
Epoch 226/512
448/448 - 0s - loss: 1.6295e-09 - val_loss: 9.5769e-09
Epoch 227/512
448/448 - 0s - loss: 1.6199e-09 - val_loss: 9.4673e-09
Epoch 228/512
448/448 - 0s - loss: 1.6036e-09 - val_loss: 9.3774e-09
Epoch 229/512
448/448 - 0s - loss: 1.5865e-09 - val_loss: 9.2590e-09
Epoch 230/512
448/448 - 0s - loss: 1.5731e-09 - val_loss: 9.1488e-09
Epoch 231/512
448/448 - 0s - loss: 1.5672e-09 - val_loss: 9.0828e-09
Epoch 232/512
448/448 - 0s - loss: 1.5636e-09 - val_loss: 9.0140e-09
Epoch 233/512
448/448 - 0s - loss: 1.5538e-09 - val_loss: 8.9413e-09
Epoch 234/512
448/448 - 0s - loss: 1.5383e-09 - val_loss: 8.8392e-09
Epoch 235/512
448/448 - 0s - loss: 1.5243e-09 - val_loss: 8.7482e-09
Epoch 236/512
448/448 - 0s - loss: 1.5080e-09 - val_loss: 8.6636e-09
Epoch 237/512
448/448 - 0s - loss: 1.4944e-09 - val_loss: 8.5754e-09
Epoch 238/512
448/448 - 0s - loss: 1.4768e-09 - val_loss: 8.4902e-09
Epoch 239/512
448/448 - 0s - loss: 1.4730e-09 - val_loss: 8.4467e-09
Epoch 240/512
448/448 - 0s - loss: 1.4778e-09 - val_loss: 8.3789e-09
Epoch 241/512
448/448 - 0s - loss: 1.4811e-09 - val_loss: 8.3102e-09
Epoch 242/512
448/448 - 0s - loss: 1.4683e-09 - val_loss: 8.2227e-09
Epoch 243/512
448/448 - 0s - loss: 1.4419e-09 - val_loss: 8.1120e-09
Epoch 244/512
448/448 - 0s - loss: 1.4216e-09 - val_loss: 8.0559e-09
Epoch 245/512
448/448 - 0s - loss: 1.4100e-09 - val_loss: 7.9934e-09
Epoch 246/512
448/448 - 0s - loss: 1.4037e-09 - val_loss: 7.9217e-09
Epoch 247/512
448/448 - 0s - loss: 1.3905e-09 - val_loss: 7.8359e-09
Epoch 248/512
448/448 - 0s - loss: 1.3804e-09 - val_loss: 7.7602e-09
Epoch 249/512
448/448 - 0s - loss: 1.3741e-09 - val_loss: 7.6888e-09
Epoch 250/512
448/448 - 0s - loss: 1.3694e-09 - val_loss: 7.6135e-09
Epoch 251/512
448/448 - 0s - loss: 1.3654e-09 - val_loss: 7.5604e-09
Epoch 252/512
448/448 - 0s - loss: 1.3542e-09 - val_loss: 7.4899e-09
Epoch 253/512
448/448 - 0s - loss: 1.3441e-09 - val_loss: 7.4052e-09
Epoch 254/512
448/448 - 0s - loss: 1.3316e-09 - val_loss: 7.3447e-09
Epoch 255/512
448/448 - 0s - loss: 1.3226e-09 - val_loss: 7.2998e-09
Epoch 256/512
448/448 - 0s - loss: 1.3202e-09 - val_loss: 7.2614e-09
Epoch 257/512
448/448 - 0s - loss: 1.3160e-09 - val_loss: 7.2007e-09
Epoch 258/512
448/448 - 0s - loss: 1.3004e-09 - val_loss: 7.1452e-09
Epoch 259/512
448/448 - 0s - loss: 1.2920e-09 - val_loss: 7.0760e-09
Epoch 260/512
448/448 - 0s - loss: 1.2810e-09 - val_loss: 7.0165e-09
Epoch 261/512
448/448 - 0s - loss: 1.2739e-09 - val_loss: 6.9369e-09
Epoch 262/512
448/448 - 0s - loss: 1.2670e-09 - val_loss: 6.8734e-09
Epoch 263/512
448/448 - 0s - loss: 1.2566e-09 - val_loss: 6.8197e-09
Epoch 264/512
448/448 - 0s - loss: 1.2549e-09 - val_loss: 6.7748e-09
Epoch 265/512
448/448 - 0s - loss: 1.2479e-09 - val_loss: 6.7263e-09
Epoch 266/512
448/448 - 0s - loss: 1.2390e-09 - val_loss: 6.6889e-09
Epoch 267/512
448/448 - 0s - loss: 1.2306e-09 - val_loss: 6.6173e-09
Epoch 268/512
448/448 - 0s - loss: 1.2194e-09 - val_loss: 6.5578e-09
Epoch 269/512
448/448 - 0s - loss: 1.2083e-09 - val_loss: 6.4968e-09
Epoch 270/512
448/448 - 0s - loss: 1.2045e-09 - val_loss: 6.4688e-09
Epoch 271/512
448/448 - 0s - loss: 1.2069e-09 - val_loss: 6.4262e-09
Epoch 272/512
448/448 - 0s - loss: 1.2020e-09 - val_loss: 6.3833e-09
Epoch 273/512
448/448 - 0s - loss: 1.1975e-09 - val_loss: 6.3328e-09
Epoch 274/512
448/448 - 0s - loss: 1.1909e-09 - val_loss: 6.2953e-09
Epoch 275/512
448/448 - 0s - loss: 1.1823e-09 - val_loss: 6.2447e-09
Epoch 276/512
448/448 - 0s - loss: 1.1692e-09 - val_loss: 6.1809e-09
Epoch 277/512
448/448 - 0s - loss: 1.1579e-09 - val_loss: 6.1287e-09
Epoch 278/512
448/448 - 0s - loss: 1.1520e-09 - val_loss: 6.0930e-09
Epoch 279/512
448/448 - 0s - loss: 1.1458e-09 - val_loss: 6.0429e-09
Epoch 280/512
448/448 - 0s - loss: 1.1423e-09 - val_loss: 6.0212e-09
Epoch 281/512
448/448 - 0s - loss: 1.1423e-09 - val_loss: 5.9890e-09
Epoch 282/512
448/448 - 0s - loss: 1.1337e-09 - val_loss: 5.9346e-09
Epoch 283/512
448/448 - 0s - loss: 1.1304e-09 - val_loss: 5.9013e-09
Epoch 284/512
448/448 - 0s - loss: 1.1257e-09 - val_loss: 5.8490e-09
Epoch 285/512
448/448 - 0s - loss: 1.1167e-09 - val_loss: 5.8041e-09
Epoch 286/512
448/448 - 0s - loss: 1.1128e-09 - val_loss: 5.7648e-09
Epoch 287/512
448/448 - 0s - loss: 1.1038e-09 - val_loss: 5.7354e-09
Epoch 288/512
448/448 - 0s - loss: 1.0953e-09 - val_loss: 5.6851e-09
Epoch 289/512
448/448 - 0s - loss: 1.0861e-09 - val_loss: 5.6562e-09
Epoch 290/512
448/448 - 0s - loss: 1.0828e-09 - val_loss: 5.6138e-09
Epoch 291/512
448/448 - 0s - loss: 1.0803e-09 - val_loss: 5.5866e-09
Epoch 292/512
448/448 - 0s - loss: 1.0765e-09 - val_loss: 5.5507e-09
Epoch 293/512
448/448 - 0s - loss: 1.0712e-09 - val_loss: 5.5229e-09
Epoch 294/512
448/448 - 0s - loss: 1.0654e-09 - val_loss: 5.4921e-09
Epoch 295/512
448/448 - 0s - loss: 1.0596e-09 - val_loss: 5.4619e-09
Epoch 296/512
448/448 - 0s - loss: 1.0606e-09 - val_loss: 5.4183e-09
Epoch 297/512
448/448 - 0s - loss: 1.0535e-09 - val_loss: 5.3808e-09
Epoch 298/512
448/448 - 0s - loss: 1.0445e-09 - val_loss: 5.3431e-09
Epoch 299/512
448/448 - 0s - loss: 1.0386e-09 - val_loss: 5.3220e-09
Epoch 300/512
448/448 - 0s - loss: 1.0328e-09 - val_loss: 5.2991e-09
Epoch 301/512
448/448 - 0s - loss: 1.0298e-09 - val_loss: 5.2610e-09
Epoch 302/512
448/448 - 0s - loss: 1.0262e-09 - val_loss: 5.2353e-09
Epoch 303/512
448/448 - 0s - loss: 1.0216e-09 - val_loss: 5.2028e-09
Epoch 304/512
448/448 - 0s - loss: 1.0137e-09 - val_loss: 5.1604e-09
Epoch 305/512
448/448 - 0s - loss: 1.0074e-09 - val_loss: 5.1302e-09
Epoch 306/512
448/448 - 0s - loss: 1.0031e-09 - val_loss: 5.0977e-09
Epoch 307/512
448/448 - 0s - loss: 9.9844e-10 - val_loss: 5.0659e-09
Epoch 308/512
448/448 - 0s - loss: 9.9239e-10 - val_loss: 5.0340e-09
Epoch 309/512
448/448 - 0s - loss: 9.8522e-10 - val_loss: 5.0039e-09
Epoch 310/512
448/448 - 0s - loss: 9.8256e-10 - val_loss: 4.9794e-09
Epoch 311/512
448/448 - 0s - loss: 9.7882e-10 - val_loss: 4.9341e-09
Epoch 312/512
448/448 - 0s - loss: 9.7498e-10 - val_loss: 4.9117e-09
Epoch 313/512
448/448 - 0s - loss: 9.7206e-10 - val_loss: 4.8759e-09
Epoch 314/512
448/448 - 0s - loss: 9.6795e-10 - val_loss: 4.8504e-09
Epoch 315/512
448/448 - 0s - loss: 9.6717e-10 - val_loss: 4.8298e-09
Epoch 316/512
448/448 - 0s - loss: 9.6115e-10 - val_loss: 4.8007e-09
Epoch 317/512
448/448 - 0s - loss: 9.5684e-10 - val_loss: 4.7802e-09
Epoch 318/512
448/448 - 0s - loss: 9.5238e-10 - val_loss: 4.7520e-09
Epoch 319/512
448/448 - 0s - loss: 9.4794e-10 - val_loss: 4.7323e-09
Epoch 320/512
448/448 - 0s - loss: 9.4305e-10 - val_loss: 4.7059e-09
Epoch 321/512
448/448 - 0s - loss: 9.3583e-10 - val_loss: 4.6793e-09
Epoch 322/512
448/448 - 0s - loss: 9.3486e-10 - val_loss: 4.6602e-09
Epoch 323/512
448/448 - 0s - loss: 9.3183e-10 - val_loss: 4.6312e-09
Epoch 324/512
448/448 - 0s - loss: 9.2793e-10 - val_loss: 4.6037e-09
Epoch 325/512
448/448 - 0s - loss: 9.2157e-10 - val_loss: 4.5790e-09
Epoch 326/512
448/448 - 0s - loss: 9.1698e-10 - val_loss: 4.5493e-09
Epoch 327/512
448/448 - 0s - loss: 9.1367e-10 - val_loss: 4.5263e-09
Epoch 328/512
448/448 - 0s - loss: 9.0830e-10 - val_loss: 4.4982e-09
Epoch 329/512
448/448 - 0s - loss: 9.0539e-10 - val_loss: 4.4773e-09
Epoch 330/512
448/448 - 0s - loss: 9.0298e-10 - val_loss: 4.4563e-09
Epoch 331/512
448/448 - 0s - loss: 9.0260e-10 - val_loss: 4.4330e-09
Epoch 332/512
448/448 - 0s - loss: 8.9996e-10 - val_loss: 4.4059e-09
Epoch 333/512
448/448 - 0s - loss: 8.9587e-10 - val_loss: 4.3898e-09
Epoch 334/512
448/448 - 0s - loss: 8.9337e-10 - val_loss: 4.3649e-09
Epoch 335/512
448/448 - 0s - loss: 8.8596e-10 - val_loss: 4.3407e-09
Epoch 336/512
448/448 - 0s - loss: 8.8259e-10 - val_loss: 4.3188e-09
Epoch 337/512
448/448 - 0s - loss: 8.7953e-10 - val_loss: 4.2960e-09
Epoch 338/512
448/448 - 0s - loss: 8.7577e-10 - val_loss: 4.2791e-09
Epoch 339/512
448/448 - 0s - loss: 8.7294e-10 - val_loss: 4.2570e-09
Epoch 340/512
448/448 - 0s - loss: 8.6785e-10 - val_loss: 4.2407e-09
Epoch 341/512
448/448 - 0s - loss: 8.6409e-10 - val_loss: 4.2138e-09
Epoch 342/512
448/448 - 0s - loss: 8.5914e-10 - val_loss: 4.1861e-09
Epoch 343/512
448/448 - 0s - loss: 8.5553e-10 - val_loss: 4.1656e-09
Epoch 344/512
448/448 - 0s - loss: 8.5136e-10 - val_loss: 4.1438e-09
Epoch 345/512
448/448 - 0s - loss: 8.4785e-10 - val_loss: 4.1251e-09
Epoch 346/512
448/448 - 0s - loss: 8.4507e-10 - val_loss: 4.1026e-09
Epoch 347/512
448/448 - 0s - loss: 8.4186e-10 - val_loss: 4.0870e-09
Epoch 348/512
448/448 - 0s - loss: 8.4123e-10 - val_loss: 4.0702e-09
Epoch 349/512
448/448 - 0s - loss: 8.3901e-10 - val_loss: 4.0454e-09
Epoch 350/512
448/448 - 0s - loss: 8.3598e-10 - val_loss: 4.0256e-09
Epoch 351/512
448/448 - 0s - loss: 8.3056e-10 - val_loss: 4.0082e-09
Epoch 352/512
448/448 - 0s - loss: 8.2722e-10 - val_loss: 3.9824e-09
Epoch 353/512
448/448 - 0s - loss: 8.2630e-10 - val_loss: 3.9689e-09
Epoch 354/512
448/448 - 0s - loss: 8.2610e-10 - val_loss: 3.9525e-09
Epoch 355/512
448/448 - 0s - loss: 8.2600e-10 - val_loss: 3.9383e-09
Epoch 356/512
448/448 - 0s - loss: 8.2228e-10 - val_loss: 3.9179e-09
Epoch 357/512
448/448 - 0s - loss: 8.1621e-10 - val_loss: 3.8965e-09
Epoch 358/512
448/448 - 0s - loss: 8.1088e-10 - val_loss: 3.8788e-09
Epoch 359/512
448/448 - 0s - loss: 8.0630e-10 - val_loss: 3.8667e-09
Epoch 360/512
448/448 - 0s - loss: 8.0396e-10 - val_loss: 3.8459e-09
Epoch 361/512
448/448 - 0s - loss: 8.0301e-10 - val_loss: 3.8279e-09
Epoch 362/512
448/448 - 0s - loss: 7.9822e-10 - val_loss: 3.8128e-09
Epoch 363/512
448/448 - 0s - loss: 7.9485e-10 - val_loss: 3.7932e-09
Epoch 364/512
448/448 - 0s - loss: 7.9391e-10 - val_loss: 3.7798e-09
Epoch 365/512
448/448 - 0s - loss: 7.9208e-10 - val_loss: 3.7630e-09
Epoch 366/512
448/448 - 0s - loss: 7.8843e-10 - val_loss: 3.7483e-09
Epoch 367/512
448/448 - 0s - loss: 7.8816e-10 - val_loss: 3.7318e-09
Epoch 368/512
448/448 - 0s - loss: 7.8659e-10 - val_loss: 3.7170e-09
Epoch 369/512
448/448 - 0s - loss: 7.8322e-10 - val_loss: 3.6985e-09
Epoch 370/512
448/448 - 0s - loss: 7.7933e-10 - val_loss: 3.6809e-09
Epoch 371/512
448/448 - 0s - loss: 7.7683e-10 - val_loss: 3.6641e-09
Epoch 372/512
448/448 - 0s - loss: 7.7395e-10 - val_loss: 3.6492e-09
Epoch 373/512
448/448 - 0s - loss: 7.7195e-10 - val_loss: 3.6280e-09
Epoch 374/512
448/448 - 0s - loss: 7.6893e-10 - val_loss: 3.6179e-09
Epoch 375/512
448/448 - 0s - loss: 7.6579e-10 - val_loss: 3.6045e-09
Epoch 376/512
448/448 - 0s - loss: 7.6491e-10 - val_loss: 3.5864e-09
Epoch 377/512
448/448 - 0s - loss: 7.6286e-10 - val_loss: 3.5730e-09
Epoch 378/512
448/448 - 0s - loss: 7.6008e-10 - val_loss: 3.5560e-09
Epoch 379/512
448/448 - 0s - loss: 7.5652e-10 - val_loss: 3.5409e-09
Epoch 380/512
448/448 - 0s - loss: 7.5405e-10 - val_loss: 3.5254e-09
Epoch 381/512
448/448 - 0s - loss: 7.5073e-10 - val_loss: 3.5143e-09
Epoch 382/512
448/448 - 0s - loss: 7.4855e-10 - val_loss: 3.5028e-09
Epoch 383/512
448/448 - 0s - loss: 7.4693e-10 - val_loss: 3.4871e-09
Epoch 384/512
448/448 - 0s - loss: 7.4089e-10 - val_loss: 3.4651e-09
Epoch 385/512
448/448 - 0s - loss: 7.3616e-10 - val_loss: 3.4522e-09
Epoch 386/512
448/448 - 0s - loss: 7.3366e-10 - val_loss: 3.4396e-09
Epoch 387/512
448/448 - 0s - loss: 7.3087e-10 - val_loss: 3.4259e-09
Epoch 388/512
448/448 - 0s - loss: 7.2845e-10 - val_loss: 3.4112e-09
Epoch 389/512
448/448 - 0s - loss: 7.2819e-10 - val_loss: 3.4002e-09
Epoch 390/512
448/448 - 0s - loss: 7.2816e-10 - val_loss: 3.3854e-09
Epoch 391/512
448/448 - 0s - loss: 7.2620e-10 - val_loss: 3.3689e-09
Epoch 392/512
448/448 - 0s - loss: 7.2312e-10 - val_loss: 3.3543e-09
Epoch 393/512
448/448 - 0s - loss: 7.2078e-10 - val_loss: 3.3395e-09
Epoch 394/512
448/448 - 0s - loss: 7.1704e-10 - val_loss: 3.3268e-09
Epoch 395/512
448/448 - 0s - loss: 7.1369e-10 - val_loss: 3.3131e-09
Epoch 396/512
448/448 - 0s - loss: 7.1073e-10 - val_loss: 3.2999e-09
Epoch 397/512
448/448 - 0s - loss: 7.0891e-10 - val_loss: 3.2879e-09
Epoch 398/512
448/448 - 0s - loss: 7.0690e-10 - val_loss: 3.2755e-09
Epoch 399/512
448/448 - 0s - loss: 7.0639e-10 - val_loss: 3.2624e-09
Epoch 400/512
448/448 - 0s - loss: 7.0481e-10 - val_loss: 3.2523e-09
Epoch 401/512
448/448 - 0s - loss: 7.0357e-10 - val_loss: 3.2392e-09
Epoch 402/512
448/448 - 0s - loss: 7.0149e-10 - val_loss: 3.2304e-09
Epoch 403/512
448/448 - 0s - loss: 7.0161e-10 - val_loss: 3.2154e-09
Epoch 404/512
448/448 - 0s - loss: 6.9987e-10 - val_loss: 3.2047e-09
Epoch 405/512
448/448 - 0s - loss: 6.9838e-10 - val_loss: 3.1882e-09
Epoch 406/512
448/448 - 0s - loss: 6.9300e-10 - val_loss: 3.1734e-09
Epoch 407/512
448/448 - 0s - loss: 6.8995e-10 - val_loss: 3.1627e-09
Epoch 408/512
448/448 - 0s - loss: 6.8752e-10 - val_loss: 3.1511e-09
Epoch 409/512
448/448 - 0s - loss: 6.8558e-10 - val_loss: 3.1382e-09
Epoch 410/512
448/448 - 0s - loss: 6.8239e-10 - val_loss: 3.1247e-09
Epoch 411/512
448/448 - 0s - loss: 6.8062e-10 - val_loss: 3.1184e-09
Epoch 412/512
448/448 - 0s - loss: 6.7848e-10 - val_loss: 3.1079e-09
Epoch 413/512
448/448 - 0s - loss: 6.7518e-10 - val_loss: 3.0961e-09
Epoch 414/512
448/448 - 0s - loss: 6.7279e-10 - val_loss: 3.0847e-09
Epoch 415/512
448/448 - 0s - loss: 6.7138e-10 - val_loss: 3.0738e-09
Epoch 416/512
448/448 - 0s - loss: 6.7015e-10 - val_loss: 3.0630e-09
Epoch 417/512
448/448 - 0s - loss: 6.6971e-10 - val_loss: 3.0473e-09
Epoch 418/512
448/448 - 0s - loss: 6.6937e-10 - val_loss: 3.0346e-09
Epoch 419/512
448/448 - 0s - loss: 6.6744e-10 - val_loss: 3.0228e-09
Epoch 420/512
448/448 - 0s - loss: 6.6565e-10 - val_loss: 3.0133e-09
Epoch 421/512
448/448 - 0s - loss: 6.6396e-10 - val_loss: 3.0044e-09
Epoch 422/512
448/448 - 0s - loss: 6.6098e-10 - val_loss: 2.9967e-09
Epoch 423/512
448/448 - 0s - loss: 6.5953e-10 - val_loss: 2.9858e-09
Epoch 424/512
448/448 - 0s - loss: 6.5862e-10 - val_loss: 2.9752e-09
Epoch 425/512
448/448 - 0s - loss: 6.5690e-10 - val_loss: 2.9642e-09
Epoch 426/512
448/448 - 0s - loss: 6.5440e-10 - val_loss: 2.9532e-09
Epoch 427/512
448/448 - 0s - loss: 6.5162e-10 - val_loss: 2.9419e-09
Epoch 428/512
448/448 - 0s - loss: 6.4964e-10 - val_loss: 2.9298e-09
Epoch 429/512
448/448 - 0s - loss: 6.4704e-10 - val_loss: 2.9175e-09
Epoch 430/512
448/448 - 0s - loss: 6.4453e-10 - val_loss: 2.9058e-09
Epoch 431/512
448/448 - 0s - loss: 6.4213e-10 - val_loss: 2.8942e-09
Epoch 432/512
448/448 - 0s - loss: 6.3966e-10 - val_loss: 2.8845e-09
Epoch 433/512
448/448 - 0s - loss: 6.3739e-10 - val_loss: 2.8759e-09
Epoch 434/512
448/448 - 0s - loss: 6.3648e-10 - val_loss: 2.8654e-09
Epoch 435/512
448/448 - 0s - loss: 6.3484e-10 - val_loss: 2.8547e-09
Epoch 436/512
448/448 - 0s - loss: 6.3357e-10 - val_loss: 2.8492e-09
Epoch 437/512
448/448 - 0s - loss: 6.3394e-10 - val_loss: 2.8372e-09
Epoch 438/512
448/448 - 0s - loss: 6.3167e-10 - val_loss: 2.8287e-09
Epoch 439/512
448/448 - 0s - loss: 6.2918e-10 - val_loss: 2.8213e-09
Epoch 440/512
448/448 - 0s - loss: 6.2786e-10 - val_loss: 2.8090e-09
Epoch 441/512
448/448 - 0s - loss: 6.2624e-10 - val_loss: 2.7999e-09
Epoch 442/512
448/448 - 0s - loss: 6.2560e-10 - val_loss: 2.7932e-09
Epoch 443/512
448/448 - 0s - loss: 6.2396e-10 - val_loss: 2.7856e-09
Epoch 444/512
448/448 - 0s - loss: 6.2277e-10 - val_loss: 2.7777e-09
Epoch 445/512
448/448 - 0s - loss: 6.2048e-10 - val_loss: 2.7684e-09
Epoch 446/512
448/448 - 0s - loss: 6.1809e-10 - val_loss: 2.7589e-09
Epoch 447/512
448/448 - 0s - loss: 6.1554e-10 - val_loss: 2.7498e-09
Epoch 448/512
448/448 - 0s - loss: 6.1488e-10 - val_loss: 2.7395e-09
Epoch 449/512
448/448 - 0s - loss: 6.1429e-10 - val_loss: 2.7318e-09
Epoch 450/512
448/448 - 0s - loss: 6.1241e-10 - val_loss: 2.7204e-09
Epoch 451/512
448/448 - 0s - loss: 6.0980e-10 - val_loss: 2.7109e-09
Epoch 452/512
448/448 - 0s - loss: 6.0702e-10 - val_loss: 2.7034e-09
Epoch 453/512
448/448 - 0s - loss: 6.0502e-10 - val_loss: 2.6946e-09
Epoch 454/512
448/448 - 0s - loss: 6.0321e-10 - val_loss: 2.6849e-09
Epoch 455/512
448/448 - 0s - loss: 6.0349e-10 - val_loss: 2.6753e-09
Epoch 456/512
448/448 - 0s - loss: 6.0253e-10 - val_loss: 2.6659e-09
Epoch 457/512
448/448 - 0s - loss: 5.9969e-10 - val_loss: 2.6604e-09
Epoch 458/512
448/448 - 0s - loss: 5.9895e-10 - val_loss: 2.6505e-09
Epoch 459/512
448/448 - 0s - loss: 5.9750e-10 - val_loss: 2.6444e-09
Epoch 460/512
448/448 - 0s - loss: 5.9689e-10 - val_loss: 2.6380e-09
Epoch 461/512
448/448 - 0s - loss: 5.9566e-10 - val_loss: 2.6300e-09
Epoch 462/512
448/448 - 0s - loss: 5.9159e-10 - val_loss: 2.6229e-09
Epoch 463/512
448/448 - 0s - loss: 5.8979e-10 - val_loss: 2.6154e-09
Epoch 464/512
448/448 - 0s - loss: 5.8937e-10 - val_loss: 2.6077e-09
Epoch 465/512
448/448 - 0s - loss: 5.8847e-10 - val_loss: 2.5986e-09
Epoch 466/512
448/448 - 0s - loss: 5.8626e-10 - val_loss: 2.5910e-09
Epoch 467/512
448/448 - 0s - loss: 5.8391e-10 - val_loss: 2.5851e-09
Epoch 468/512
448/448 - 0s - loss: 5.8267e-10 - val_loss: 2.5773e-09
Epoch 469/512
448/448 - 0s - loss: 5.8340e-10 - val_loss: 2.5716e-09
Epoch 470/512
448/448 - 0s - loss: 5.8297e-10 - val_loss: 2.5632e-09
Epoch 471/512
448/448 - 0s - loss: 5.8152e-10 - val_loss: 2.5549e-09
Epoch 472/512
448/448 - 0s - loss: 5.7876e-10 - val_loss: 2.5487e-09
Epoch 473/512
448/448 - 0s - loss: 5.7695e-10 - val_loss: 2.5414e-09
Epoch 474/512
448/448 - 0s - loss: 5.7425e-10 - val_loss: 2.5331e-09
Epoch 475/512
448/448 - 0s - loss: 5.7320e-10 - val_loss: 2.5249e-09
Epoch 476/512
448/448 - 0s - loss: 5.7105e-10 - val_loss: 2.5194e-09
Epoch 477/512
448/448 - 0s - loss: 5.6849e-10 - val_loss: 2.5123e-09
Epoch 478/512
448/448 - 0s - loss: 5.6635e-10 - val_loss: 2.5037e-09
Epoch 479/512
448/448 - 0s - loss: 5.6469e-10 - val_loss: 2.4948e-09
Epoch 480/512
448/448 - 0s - loss: 5.6368e-10 - val_loss: 2.4869e-09
Epoch 481/512
448/448 - 0s - loss: 5.6227e-10 - val_loss: 2.4793e-09
Epoch 482/512
448/448 - 0s - loss: 5.6088e-10 - val_loss: 2.4714e-09
Epoch 483/512
448/448 - 0s - loss: 5.5957e-10 - val_loss: 2.4642e-09
Epoch 484/512
448/448 - 0s - loss: 5.5806e-10 - val_loss: 2.4565e-09
Epoch 485/512
448/448 - 0s - loss: 5.5754e-10 - val_loss: 2.4498e-09
Epoch 486/512
448/448 - 0s - loss: 5.5694e-10 - val_loss: 2.4432e-09
Epoch 487/512
448/448 - 0s - loss: 5.5596e-10 - val_loss: 2.4370e-09
Epoch 488/512
448/448 - 0s - loss: 5.5513e-10 - val_loss: 2.4297e-09
Epoch 489/512
448/448 - 0s - loss: 5.5387e-10 - val_loss: 2.4237e-09
Epoch 490/512
448/448 - 0s - loss: 5.5344e-10 - val_loss: 2.4169e-09
Epoch 491/512
448/448 - 0s - loss: 5.5402e-10 - val_loss: 2.4103e-09
Epoch 492/512
448/448 - 0s - loss: 5.5291e-10 - val_loss: 2.4019e-09
Epoch 493/512
448/448 - 0s - loss: 5.5125e-10 - val_loss: 2.3961e-09
Epoch 494/512
448/448 - 0s - loss: 5.4947e-10 - val_loss: 2.3906e-09
Epoch 495/512
448/448 - 0s - loss: 5.4649e-10 - val_loss: 2.3836e-09
Epoch 496/512
448/448 - 0s - loss: 5.4466e-10 - val_loss: 2.3759e-09
Epoch 497/512
448/448 - 0s - loss: 5.4339e-10 - val_loss: 2.3688e-09
Epoch 498/512
448/448 - 0s - loss: 5.4197e-10 - val_loss: 2.3606e-09
Epoch 499/512
448/448 - 0s - loss: 5.4013e-10 - val_loss: 2.3562e-09
Epoch 500/512
448/448 - 0s - loss: 5.3953e-10 - val_loss: 2.3481e-09
Epoch 501/512
448/448 - 0s - loss: 5.3992e-10 - val_loss: 2.3418e-09
Epoch 502/512
448/448 - 0s - loss: 5.3899e-10 - val_loss: 2.3354e-09
Epoch 503/512
448/448 - 0s - loss: 5.3781e-10 - val_loss: 2.3286e-09
Epoch 504/512
448/448 - 0s - loss: 5.3553e-10 - val_loss: 2.3236e-09
Epoch 505/512
448/448 - 0s - loss: 5.3441e-10 - val_loss: 2.3183e-09
Epoch 506/512
448/448 - 0s - loss: 5.3289e-10 - val_loss: 2.3111e-09
Epoch 507/512
448/448 - 0s - loss: 5.3138e-10 - val_loss: 2.3051e-09
Epoch 508/512
448/448 - 0s - loss: 5.3020e-10 - val_loss: 2.2986e-09
Epoch 509/512
448/448 - 0s - loss: 5.2900e-10 - val_loss: 2.2945e-09
Epoch 510/512
448/448 - 0s - loss: 5.2778e-10 - val_loss: 2.2873e-09
Epoch 511/512
448/448 - 0s - loss: 5.2669e-10 - val_loss: 2.2797e-09
Epoch 512/512
448/448 - 0s - loss: 5.2678e-10 - val_loss: 2.2733e-09
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6922e-09 - val_loss: 6.0969e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.3145e-09 - val_loss: 7.8912e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.5550e-10 - val_loss: 1.1866e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.0640e-11 - val_loss: 6.1593e-11
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4583e-11 - val_loss: 8.6778e-11
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2341e-10 - val_loss: 2.7033e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7295e-10 - val_loss: 1.1021e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4200e-09 - val_loss: 1.5022e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2554e-09 - val_loss: 6.8462e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4850e-10 - val_loss: 3.4471e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1803e-10 - val_loss: 2.9440e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2122e-10 - val_loss: 4.0347e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7751e-10 - val_loss: 6.3123e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0046e-10 - val_loss: 7.4793e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2662e-10 - val_loss: 6.1098e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6166e-10 - val_loss: 4.5047e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2664e-10 - val_loss: 3.8437e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8673e-10 - val_loss: 3.9346e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1484e-10 - val_loss: 4.4613e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6632e-10 - val_loss: 4.8127e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8585e-10 - val_loss: 4.6355e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5420e-10 - val_loss: 4.1618e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0680e-10 - val_loss: 3.7639e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7308e-10 - val_loss: 3.5952e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6157e-10 - val_loss: 3.5815e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6346e-10 - val_loss: 3.6219e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6640e-10 - val_loss: 3.6004e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6075e-10 - val_loss: 3.4804e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4619e-10 - val_loss: 3.3273e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2979e-10 - val_loss: 3.1582e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1507e-10 - val_loss: 3.0557e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0605e-10 - val_loss: 2.9877e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0049e-10 - val_loss: 2.9641e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9752e-10 - val_loss: 2.8901e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8937e-10 - val_loss: 2.8134e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8050e-10 - val_loss: 2.7081e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7043e-10 - val_loss: 2.6188e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6165e-10 - val_loss: 2.5549e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5649e-10 - val_loss: 2.5097e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5150e-10 - val_loss: 2.4641e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4722e-10 - val_loss: 2.4158e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4126e-10 - val_loss: 2.3528e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3535e-10 - val_loss: 2.2912e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2899e-10 - val_loss: 2.2263e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2190e-10 - val_loss: 2.1602e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1636e-10 - val_loss: 2.1062e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1073e-10 - val_loss: 2.0550e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0615e-10 - val_loss: 2.0230e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0341e-10 - val_loss: 2.0051e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0032e-10 - val_loss: 1.9502e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9446e-10 - val_loss: 1.8799e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8781e-10 - val_loss: 1.8301e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8246e-10 - val_loss: 1.7864e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7914e-10 - val_loss: 1.7569e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7601e-10 - val_loss: 1.7312e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7376e-10 - val_loss: 1.7124e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7099e-10 - val_loss: 1.6666e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6659e-10 - val_loss: 1.6295e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6245e-10 - val_loss: 1.5866e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5808e-10 - val_loss: 1.5333e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5271e-10 - val_loss: 1.4949e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5006e-10 - val_loss: 1.4832e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4856e-10 - val_loss: 1.4663e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4732e-10 - val_loss: 1.4538e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4560e-10 - val_loss: 1.4193e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4122e-10 - val_loss: 1.3648e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3693e-10 - val_loss: 1.3509e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3516e-10 - val_loss: 1.3331e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3349e-10 - val_loss: 1.3065e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3071e-10 - val_loss: 1.2708e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2630e-10 - val_loss: 1.2305e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2344e-10 - val_loss: 1.2174e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2240e-10 - val_loss: 1.2055e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2055e-10 - val_loss: 1.1872e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1915e-10 - val_loss: 1.1753e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1810e-10 - val_loss: 1.1653e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1664e-10 - val_loss: 1.1405e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1341e-10 - val_loss: 1.1012e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0992e-10 - val_loss: 1.0797e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0749e-10 - val_loss: 1.0515e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0529e-10 - val_loss: 1.0347e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0390e-10 - val_loss: 1.0304e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0350e-10 - val_loss: 1.0272e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0314e-10 - val_loss: 1.0184e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0171e-10 - val_loss: 1.0033e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0049e-10 - val_loss: 9.8645e-11
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7916e-11 - val_loss: 9.4690e-11
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4310e-11 - val_loss: 9.2490e-11
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2524e-11 - val_loss: 9.1080e-11
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1365e-11 - val_loss: 9.0690e-11
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0661e-11 - val_loss: 8.9185e-11
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9116e-11 - val_loss: 8.7697e-11
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7740e-11 - val_loss: 8.6939e-11
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7364e-11 - val_loss: 8.6294e-11
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6546e-11 - val_loss: 8.5405e-11
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5497e-11 - val_loss: 8.3727e-11
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3320e-11 - val_loss: 8.0818e-11
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0741e-11 - val_loss: 7.9439e-11
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9303e-11 - val_loss: 7.7261e-11
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7286e-11 - val_loss: 7.6645e-11
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7022e-11 - val_loss: 7.6190e-11
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6546e-11 - val_loss: 7.6063e-11
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6562e-11 - val_loss: 7.6193e-11
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6396e-11 - val_loss: 7.5344e-11
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5014e-11 - val_loss: 7.3604e-11
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3832e-11 - val_loss: 7.2835e-11
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2609e-11 - val_loss: 7.0849e-11
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0552e-11 - val_loss: 6.9418e-11
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9493e-11 - val_loss: 6.8112e-11
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8348e-11 - val_loss: 6.7832e-11
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8105e-11 - val_loss: 6.7428e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7625e-11 - val_loss: 6.7303e-11
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7453e-11 - val_loss: 6.6589e-11
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6341e-11 - val_loss: 6.4775e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4973e-11 - val_loss: 6.4099e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3854e-11 - val_loss: 6.2211e-11
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.1838e-11 - val_loss: 6.0491e-11
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.0473e-11 - val_loss: 6.0247e-11
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.0395e-11 - val_loss: 6.0047e-11
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.9867e-11 - val_loss: 5.8610e-11
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.8754e-11 - val_loss: 5.8101e-11
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.8173e-11 - val_loss: 5.7957e-11
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7909e-11 - val_loss: 5.7329e-11
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7465e-11 - val_loss: 5.7174e-11
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7475e-11 - val_loss: 5.6737e-11
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.6895e-11 - val_loss: 5.6192e-11
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.5894e-11 - val_loss: 5.4371e-11
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.4285e-11 - val_loss: 5.2922e-11
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.2714e-11 - val_loss: 5.2224e-11
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.2053e-11 - val_loss: 5.1557e-11
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.1822e-11 - val_loss: 5.1285e-11
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.1358e-11 - val_loss: 5.1270e-11
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.1481e-11 - val_loss: 5.1222e-11
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.1449e-11 - val_loss: 5.1067e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1191e-11 - val_loss: 5.1078e-11
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.0940e-11 - val_loss: 5.0510e-11
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.0197e-11 - val_loss: 4.9299e-11
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.9059e-11 - val_loss: 4.7757e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.7482e-11 - val_loss: 4.5898e-11
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.5987e-11 - val_loss: 4.5875e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6185e-11 - val_loss: 4.6290e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6811e-11 - val_loss: 4.7195e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7360e-11 - val_loss: 4.6651e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6517e-11 - val_loss: 4.6039e-11
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.6020e-11 - val_loss: 4.5221e-11
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.5252e-11 - val_loss: 4.4950e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.4791e-11 - val_loss: 4.4253e-11
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.4099e-11 - val_loss: 4.3146e-11
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.3062e-11 - val_loss: 4.2407e-11
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.2351e-11 - val_loss: 4.1938e-11
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.2000e-11 - val_loss: 4.1639e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1724e-11 - val_loss: 4.1747e-11
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.1874e-11 - val_loss: 4.1163e-11
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.1000e-11 - val_loss: 4.0187e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0264e-11 - val_loss: 4.0323e-11
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.0454e-11 - val_loss: 3.9855e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.9507e-11 - val_loss: 3.8538e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.8454e-11 - val_loss: 3.7819e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.7647e-11 - val_loss: 3.7330e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.7308e-11 - val_loss: 3.6818e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7227e-11 - val_loss: 3.7565e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7629e-11 - val_loss: 3.7207e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7413e-11 - val_loss: 3.7608e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7538e-11 - val_loss: 3.7047e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7165e-11 - val_loss: 3.6866e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.6757e-11 - val_loss: 3.5976e-11
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.5460e-11 - val_loss: 3.4269e-11
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.4093e-11 - val_loss: 3.3559e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3684e-11 - val_loss: 3.3773e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3842e-11 - val_loss: 3.3599e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3618e-11 - val_loss: 3.3562e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3775e-11 - val_loss: 3.3635e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3793e-11 - val_loss: 3.3852e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4201e-11 - val_loss: 3.4018e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3822e-11 - val_loss: 3.2756e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.2359e-11 - val_loss: 3.1369e-11
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.1145e-11 - val_loss: 3.0915e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1274e-11 - val_loss: 3.1649e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2049e-11 - val_loss: 3.2201e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2189e-11 - val_loss: 3.2346e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2576e-11 - val_loss: 3.2609e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2471e-11 - val_loss: 3.1964e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2011e-11 - val_loss: 3.1630e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.1529e-11 - val_loss: 3.0767e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.0446e-11 - val_loss: 2.9420e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.9170e-11 - val_loss: 2.8870e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8690e-11 - val_loss: 2.8331e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8352e-11 - val_loss: 2.8189e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8307e-11 - val_loss: 2.8387e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8397e-11 - val_loss: 2.8171e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8542e-11 - val_loss: 2.8792e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8874e-11 - val_loss: 2.8375e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8012e-11 - val_loss: 2.7029e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6761e-11 - val_loss: 2.6100e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5847e-11 - val_loss: 2.5412e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5671e-11 - val_loss: 2.5632e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5813e-11 - val_loss: 2.5851e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6034e-11 - val_loss: 2.6508e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7113e-11 - val_loss: 2.7896e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8238e-11 - val_loss: 2.8398e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8415e-11 - val_loss: 2.8088e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8034e-11 - val_loss: 2.7175e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6832e-11 - val_loss: 2.6007e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6106e-11 - val_loss: 2.6026e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6202e-11 - val_loss: 2.6235e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6365e-11 - val_loss: 2.6074e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5795e-11 - val_loss: 2.5199e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5226e-11 - val_loss: 2.5264e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5374e-11 - val_loss: 2.5214e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4978e-11 - val_loss: 2.4200e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4075e-11 - val_loss: 2.3905e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4008e-11 - val_loss: 2.3545e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3468e-11 - val_loss: 2.3282e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3189e-11 - val_loss: 2.2798e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2802e-11 - val_loss: 2.2696e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2763e-11 - val_loss: 2.3162e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3571e-11 - val_loss: 2.4053e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4145e-11 - val_loss: 2.3969e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3908e-11 - val_loss: 2.3274e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3113e-11 - val_loss: 2.2392e-11
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2190e-11 - val_loss: 2.1670e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1545e-11 - val_loss: 2.1461e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1649e-11 - val_loss: 2.2086e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2206e-11 - val_loss: 2.2158e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2223e-11 - val_loss: 2.2347e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2491e-11 - val_loss: 2.2193e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2001e-11 - val_loss: 2.1191e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0824e-11 - val_loss: 2.0178e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0153e-11 - val_loss: 1.9968e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0039e-11 - val_loss: 1.9913e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0367e-11 - val_loss: 2.0873e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1048e-11 - val_loss: 2.1038e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1213e-11 - val_loss: 2.1540e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1673e-11 - val_loss: 2.1843e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1877e-11 - val_loss: 2.1559e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1415e-11 - val_loss: 2.0571e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0378e-11 - val_loss: 1.9788e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9651e-11 - val_loss: 1.9350e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9288e-11 - val_loss: 1.9184e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9347e-11 - val_loss: 1.9221e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9253e-11 - val_loss: 1.9296e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9331e-11 - val_loss: 1.9505e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9797e-11 - val_loss: 2.0011e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0104e-11 - val_loss: 1.9643e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9478e-11 - val_loss: 1.9207e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9142e-11 - val_loss: 1.8773e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8671e-11 - val_loss: 1.8380e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8295e-11 - val_loss: 1.8299e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8444e-11 - val_loss: 1.8607e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8703e-11 - val_loss: 1.8735e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8636e-11 - val_loss: 1.7947e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7677e-11 - val_loss: 1.7223e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7208e-11 - val_loss: 1.7006e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7042e-11 - val_loss: 1.6826e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6843e-11 - val_loss: 1.6783e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7055e-11 - val_loss: 1.7657e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7808e-11 - val_loss: 1.8165e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8357e-11 - val_loss: 1.8519e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8647e-11 - val_loss: 1.8392e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8210e-11 - val_loss: 1.7694e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7436e-11 - val_loss: 1.6759e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6641e-11 - val_loss: 1.6264e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6217e-11 - val_loss: 1.6466e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6715e-11 - val_loss: 1.6975e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7111e-11 - val_loss: 1.7632e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7986e-11 - val_loss: 1.8101e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8091e-11 - val_loss: 1.7973e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7828e-11 - val_loss: 1.7330e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7185e-11 - val_loss: 1.6744e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6625e-11 - val_loss: 1.6107e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5987e-11 - val_loss: 1.5792e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5805e-11 - val_loss: 1.5655e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5655e-11 - val_loss: 1.5537e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5576e-11 - val_loss: 1.5524e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5606e-11 - val_loss: 1.5833e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5904e-11 - val_loss: 1.5642e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5356e-11 - val_loss: 1.4636e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4453e-11 - val_loss: 1.4109e-11
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3959e-11 - val_loss: 1.3639e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3735e-11 - val_loss: 1.3873e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4063e-11 - val_loss: 1.4530e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4637e-11 - val_loss: 1.4681e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4787e-11 - val_loss: 1.4876e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4755e-11 - val_loss: 1.4465e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4288e-11 - val_loss: 1.3980e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3981e-11 - val_loss: 1.3894e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3881e-11 - val_loss: 1.3805e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3736e-11 - val_loss: 1.3450e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3328e-11 - val_loss: 1.3110e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3099e-11 - val_loss: 1.3014e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3217e-11 - val_loss: 1.3642e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3839e-11 - val_loss: 1.4014e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4050e-11 - val_loss: 1.4116e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4188e-11 - val_loss: 1.4310e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4207e-11 - val_loss: 1.3852e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3948e-11 - val_loss: 1.3708e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3570e-11 - val_loss: 1.3325e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3345e-11 - val_loss: 1.3316e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3542e-11 - val_loss: 1.3797e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4017e-11 - val_loss: 1.4137e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4282e-11 - val_loss: 1.4078e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3881e-11 - val_loss: 1.3425e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3334e-11 - val_loss: 1.3267e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3398e-11 - val_loss: 1.3569e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3684e-11 - val_loss: 1.3577e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3423e-11 - val_loss: 1.2953e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2890e-11 - val_loss: 1.2583e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2534e-11 - val_loss: 1.2046e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1874e-11 - val_loss: 1.1703e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1676e-11 - val_loss: 1.1532e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1610e-11 - val_loss: 1.1660e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1734e-11 - val_loss: 1.2074e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2283e-11 - val_loss: 1.2627e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2811e-11 - val_loss: 1.3025e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3076e-11 - val_loss: 1.2964e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2864e-11 - val_loss: 1.2538e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2423e-11 - val_loss: 1.1993e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1857e-11 - val_loss: 1.1539e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1508e-11 - val_loss: 1.1405e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1343e-11 - val_loss: 1.1138e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1152e-11 - val_loss: 1.1382e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1358e-11 - val_loss: 1.1466e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1670e-11 - val_loss: 1.2039e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2258e-11 - val_loss: 1.2567e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2597e-11 - val_loss: 1.2576e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2589e-11 - val_loss: 1.2304e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2108e-11 - val_loss: 1.1561e-11
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1352e-11 - val_loss: 1.1032e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1001e-11 - val_loss: 1.0853e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0865e-11 - val_loss: 1.0755e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0615e-11 - val_loss: 1.0607e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0567e-11 - val_loss: 1.0259e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0087e-11 - val_loss: 9.9378e-12
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0022e-11 - val_loss: 1.0441e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0600e-11 - val_loss: 1.0920e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1039e-11 - val_loss: 1.1367e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1428e-11 - val_loss: 1.1472e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1592e-11 - val_loss: 1.1774e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1728e-11 - val_loss: 1.1495e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1489e-11 - val_loss: 1.1553e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1709e-11 - val_loss: 1.1832e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1796e-11 - val_loss: 1.1583e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1511e-11 - val_loss: 1.1239e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1092e-11 - val_loss: 1.0718e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0655e-11 - val_loss: 1.0528e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0546e-11 - val_loss: 1.0657e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0768e-11 - val_loss: 1.0919e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0984e-11 - val_loss: 1.0727e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0615e-11 - val_loss: 1.0558e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0645e-11 - val_loss: 1.0630e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0423e-11 - val_loss: 9.9401e-12
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.8049e-12 - val_loss: 9.5515e-12
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.4718e-12 - val_loss: 9.2786e-12
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2666e-12 - val_loss: 9.3454e-12
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4224e-12 - val_loss: 9.7362e-12
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8056e-12 - val_loss: 9.9365e-12
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0071e-11 - val_loss: 1.0286e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0409e-11 - val_loss: 1.0502e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0514e-11 - val_loss: 1.0604e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0685e-11 - val_loss: 1.0599e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0538e-11 - val_loss: 1.0359e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0368e-11 - val_loss: 1.0125e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0099e-11 - val_loss: 9.7335e-12
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.6014e-12 - val_loss: 9.2730e-12
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.1791e-12 - val_loss: 8.9822e-12
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0539e-12 - val_loss: 9.1933e-12
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2581e-12 - val_loss: 9.3101e-12
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3688e-12 - val_loss: 9.3007e-12
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.1133e-12 - val_loss: 8.9653e-12
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0750e-12 - val_loss: 9.2267e-12
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2456e-12 - val_loss: 9.1019e-12
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1267e-12 - val_loss: 9.1354e-12
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2819e-12 - val_loss: 9.4289e-12
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5928e-12 - val_loss: 1.0064e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0287e-11 - val_loss: 1.0568e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0667e-11 - val_loss: 1.0877e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0960e-11 - val_loss: 1.1022e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1071e-11 - val_loss: 1.1012e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0893e-11 - val_loss: 1.0526e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0419e-11 - val_loss: 1.0134e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8635e-12 - val_loss: 9.2575e-12
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.0516e-12 - val_loss: 8.5952e-12
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5690e-12 - val_loss: 8.6484e-12
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7221e-12 - val_loss: 8.7538e-12
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7994e-12 - val_loss: 8.8640e-12
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8991e-12 - val_loss: 8.8521e-12
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9521e-12 - val_loss: 9.1063e-12
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2205e-12 - val_loss: 9.4387e-12
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5564e-12 - val_loss: 9.8489e-12
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9503e-12 - val_loss: 9.9775e-12
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9696e-12 - val_loss: 9.7575e-12
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6427e-12 - val_loss: 9.1876e-12
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0925e-12 - val_loss: 8.7493e-12
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.7195e-12 - val_loss: 8.5919e-12
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.4654e-12 - val_loss: 8.3463e-12
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3594e-12 - val_loss: 8.3703e-12
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.3006e-12 - val_loss: 7.9597e-12
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.8890e-12 - val_loss: 7.7371e-12
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.6655e-12 - val_loss: 7.5238e-12
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.4818e-12 - val_loss: 7.4403e-12
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5953e-12 - val_loss: 7.8728e-12
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0661e-12 - val_loss: 8.3604e-12
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4614e-12 - val_loss: 8.6495e-12
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8183e-12 - val_loss: 8.9085e-12
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9776e-12 - val_loss: 8.9127e-12
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8947e-12 - val_loss: 8.7286e-12
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6840e-12 - val_loss: 8.7058e-12
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8725e-12 - val_loss: 9.0983e-12
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2846e-12 - val_loss: 9.4705e-12
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4691e-12 - val_loss: 9.4197e-12
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2723e-12 - val_loss: 8.9089e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7152e-12 - val_loss: 8.3927e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2666e-12 - val_loss: 7.9220e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8722e-12 - val_loss: 7.7244e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7487e-12 - val_loss: 7.8474e-12
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8454e-12 - val_loss: 7.8449e-12
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9987e-12 - val_loss: 8.1011e-12
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0466e-12 - val_loss: 7.7128e-12
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.6014e-12 - val_loss: 7.4224e-12
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.4076e-12 - val_loss: 7.3422e-12
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.3343e-12 - val_loss: 7.1816e-12
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.1061e-12 - val_loss: 6.9267e-12
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.9421e-12 - val_loss: 6.8580e-12
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8638e-12 - val_loss: 7.0601e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2372e-12 - val_loss: 7.4807e-12
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6503e-12 - val_loss: 7.8685e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9819e-12 - val_loss: 8.1259e-12
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2116e-12 - val_loss: 8.1927e-12
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1870e-12 - val_loss: 7.9565e-12
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8713e-12 - val_loss: 7.4469e-12
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4153e-12 - val_loss: 7.4077e-12
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3869e-12 - val_loss: 7.2290e-12
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1910e-12 - val_loss: 7.0984e-12
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1587e-12 - val_loss: 7.4231e-12
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5434e-12 - val_loss: 7.7430e-12
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8388e-12 - val_loss: 7.9316e-12
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9732e-12 - val_loss: 8.0731e-12
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0723e-12 - val_loss: 8.0940e-12
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2055e-12 - val_loss: 8.3432e-12
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3003e-12 - val_loss: 7.9465e-12
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7957e-12 - val_loss: 7.4175e-12
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3069e-12 - val_loss: 6.9823e-12
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.8780e-12 - val_loss: 6.7360e-12
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7414e-12 - val_loss: 6.7728e-12
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8517e-12 - val_loss: 6.9316e-12
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0297e-12 - val_loss: 7.4641e-12
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5874e-12 - val_loss: 7.7183e-12
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8326e-12 - val_loss: 7.9809e-12
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0366e-12 - val_loss: 8.0553e-12
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0729e-12 - val_loss: 8.0600e-12
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8364e-12 - val_loss: 7.4183e-12
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3470e-12 - val_loss: 7.1623e-12
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.9878e-12 - val_loss: 6.6258e-12
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.5749e-12 - val_loss: 6.4917e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5083e-12 - val_loss: 6.5382e-12
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5916e-12 - val_loss: 6.6000e-12
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5839e-12 - val_loss: 6.5675e-12
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.5913e-12 - val_loss: 6.4263e-12
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.2592e-12 - val_loss: 5.8809e-12
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.8029e-12 - val_loss: 5.6029e-12
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.5565e-12 - val_loss: 5.4718e-12
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.4549e-12 - val_loss: 5.4435e-12
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5821e-12 - val_loss: 5.8641e-12
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9378e-12 - val_loss: 6.0592e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1496e-12 - val_loss: 6.2902e-12
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4228e-12 - val_loss: 6.6637e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7646e-12 - val_loss: 6.9249e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9036e-12 - val_loss: 6.8726e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8945e-12 - val_loss: 7.1503e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3338e-12 - val_loss: 7.5564e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6480e-12 - val_loss: 7.7746e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8300e-12 - val_loss: 7.8660e-12
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8739e-12 - val_loss: 7.9150e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9812e-12 - val_loss: 7.9069e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7835e-12 - val_loss: 7.4814e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3362e-12 - val_loss: 7.1050e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0651e-12 - val_loss: 6.8769e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8583e-12 - val_loss: 6.7513e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6711e-12 - val_loss: 6.4803e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4026e-12 - val_loss: 6.2888e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1819e-12 - val_loss: 5.8474e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8046e-12 - val_loss: 5.6232e-12
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6394e-12 - val_loss: 5.6540e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6691e-12 - val_loss: 5.7495e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7567e-12 - val_loss: 5.7829e-12
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8912e-12 - val_loss: 6.0952e-12
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1109e-12 - val_loss: 6.1724e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2743e-12 - val_loss: 6.3214e-12
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3018e-12 - val_loss: 6.2710e-12
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3124e-12 - val_loss: 6.2748e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2981e-12 - val_loss: 6.2992e-12
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2902e-12 - val_loss: 6.2651e-12
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3511e-12 - val_loss: 6.4912e-12
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5059e-12 - val_loss: 6.4323e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3589e-12 - val_loss: 6.1996e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1881e-12 - val_loss: 6.0742e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9686e-12 - val_loss: 5.6884e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6140e-12 - val_loss: 5.4598e-12
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.4381e-12 - val_loss: 5.2431e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2606e-12 - val_loss: 5.3521e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5371e-12 - val_loss: 5.6602e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8297e-12 - val_loss: 6.0999e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2168e-12 - val_loss: 6.4212e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4779e-12 - val_loss: 6.5098e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5657e-12 - val_loss: 6.4684e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2961e-12 - val_loss: 5.7644e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6358e-12 - val_loss: 5.3883e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3788e-12 - val_loss: 5.2764e-12
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.2220e-12 - val_loss: 4.9984e-12
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.9486e-12 - val_loss: 4.9588e-12
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.9581e-12 - val_loss: 4.9375e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0060e-12 - val_loss: 5.0416e-12
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.929 | eve: 9.346 | bob: 9.737Epoch   0:   0% | abe: 9.885 | eve: 9.349 | bob: 9.700Epoch   0:   1% | abe: 9.870 | eve: 9.341 | bob: 9.692Epoch   0:   2% | abe: 9.816 | eve: 9.334 | bob: 9.644Epoch   0:   2% | abe: 9.800 | eve: 9.327 | bob: 9.634Epoch   0:   3% | abe: 9.781 | eve: 9.326 | bob: 9.620Epoch   0:   4% | abe: 9.763 | eve: 9.323 | bob: 9.607Epoch   0:   4% | abe: 9.745 | eve: 9.327 | bob: 9.593Epoch   0:   5% | abe: 9.718 | eve: 9.333 | bob: 9.570Epoch   0:   6% | abe: 9.684 | eve: 9.331 | bob: 9.540Epoch   0:   6% | abe: 9.662 | eve: 9.329 | bob: 9.521Epoch   0:   7% | abe: 9.645 | eve: 9.332 | bob: 9.508Epoch   0:   8% | abe: 9.628 | eve: 9.329 | bob: 9.494Epoch   0:   8% | abe: 9.609 | eve: 9.331 | bob: 9.477Epoch   0:   9% | abe: 9.593 | eve: 9.336 | bob: 9.465Epoch   0:  10% | abe: 9.571 | eve: 9.332 | bob: 9.446Epoch   0:  10% | abe: 9.555 | eve: 9.330 | bob: 9.433Epoch   0:  11% | abe: 9.539 | eve: 9.330 | bob: 9.419Epoch   0:  12% | abe: 9.519 | eve: 9.330 | bob: 9.402Epoch   0:  13% | abe: 9.503 | eve: 9.332 | bob: 9.389Epoch   0:  13% | abe: 9.490 | eve: 9.333 | bob: 9.379Epoch   0:  14% | abe: 9.475 | eve: 9.336 | bob: 9.368Epoch   0:  15% | abe: 9.460 | eve: 9.335 | bob: 9.355Epoch   0:  15% | abe: 9.445 | eve: 9.339 | bob: 9.343Epoch   0:  16% | abe: 9.431 | eve: 9.339 | bob: 9.332Epoch   0:  17% | abe: 9.416 | eve: 9.338 | bob: 9.319Epoch   0:  17% | abe: 9.405 | eve: 9.338 | bob: 9.311Epoch   0:  18% | abe: 9.396 | eve: 9.334 | bob: 9.304Epoch   0:  19% | abe: 9.389 | eve: 9.333 | bob: 9.299Epoch   0:  19% | abe: 9.382 | eve: 9.333 | bob: 9.294Epoch   0:  20% | abe: 9.371 | eve: 9.334 | bob: 9.286Epoch   0:  21% | abe: 9.362 | eve: 9.333 | bob: 9.279Epoch   0:  21% | abe: 9.356 | eve: 9.333 | bob: 9.274Epoch   0:  22% | abe: 9.346 | eve: 9.337 | bob: 9.266Epoch   0:  23% | abe: 9.339 | eve: 9.337 | bob: 9.260Epoch   0:  23% | abe: 9.332 | eve: 9.337 | bob: 9.254Epoch   0:  24% | abe: 9.326 | eve: 9.337 | bob: 9.250Epoch   0:  25% | abe: 9.320 | eve: 9.338 | bob: 9.245Epoch   0:  26% | abe: 9.312 | eve: 9.339 | bob: 9.239Epoch   0:  26% | abe: 9.309 | eve: 9.339 | bob: 9.237Epoch   0:  27% | abe: 9.304 | eve: 9.337 | bob: 9.232Epoch   0:  28% | abe: 9.299 | eve: 9.337 | bob: 9.228Epoch   0:  28% | abe: 9.293 | eve: 9.339 | bob: 9.224Epoch   0:  29% | abe: 9.289 | eve: 9.337 | bob: 9.221Epoch   0:  30% | abe: 9.284 | eve: 9.339 | bob: 9.217Epoch   0:  30% | abe: 9.279 | eve: 9.338 | bob: 9.213Epoch   0:  31% | abe: 9.275 | eve: 9.339 | bob: 9.210Epoch   0:  32% | abe: 9.272 | eve: 9.339 | bob: 9.208Epoch   0:  32% | abe: 9.267 | eve: 9.339 | bob: 9.203Epoch   0:  33% | abe: 9.263 | eve: 9.340 | bob: 9.200Epoch   0:  34% | abe: 9.258 | eve: 9.340 | bob: 9.196Epoch   0:  34% | abe: 9.256 | eve: 9.341 | bob: 9.195Epoch   0:  35% | abe: 9.252 | eve: 9.341 | bob: 9.192Epoch   0:  36% | abe: 9.249 | eve: 9.341 | bob: 9.189Epoch   0:  36% | abe: 9.245 | eve: 9.340 | bob: 9.186Epoch   0:  37% | abe: 9.241 | eve: 9.341 | bob: 9.183Epoch   0:  38% | abe: 9.239 | eve: 9.340 | bob: 9.182Epoch   0:  39% | abe: 9.237 | eve: 9.339 | bob: 9.180Epoch   0:  39% | abe: 9.234 | eve: 9.340 | bob: 9.178Epoch   0:  40% | abe: 9.231 | eve: 9.340 | bob: 9.176Epoch   0:  41% | abe: 9.229 | eve: 9.340 | bob: 9.175Epoch   0:  41% | abe: 9.227 | eve: 9.340 | bob: 9.173Epoch   0:  42% | abe: 9.224 | eve: 9.339 | bob: 9.171Epoch   0:  43% | abe: 9.221 | eve: 9.339 | bob: 9.169Epoch   0:  43% | abe: 9.220 | eve: 9.340 | bob: 9.168Epoch   0:  44% | abe: 9.217 | eve: 9.340 | bob: 9.166Epoch   0:  45% | abe: 9.215 | eve: 9.340 | bob: 9.164Epoch   0:  45% | abe: 9.212 | eve: 9.340 | bob: 9.162Epoch   0:  46% | abe: 9.210 | eve: 9.341 | bob: 9.161Epoch   0:  47% | abe: 9.207 | eve: 9.341 | bob: 9.159Epoch   0:  47% | abe: 9.205 | eve: 9.341 | bob: 9.157Epoch   0:  48% | abe: 9.203 | eve: 9.342 | bob: 9.155Epoch   0:  49% | abe: 9.201 | eve: 9.343 | bob: 9.154Epoch   0:  50% | abe: 9.199 | eve: 9.343 | bob: 9.152Epoch   0:  50% | abe: 9.197 | eve: 9.344 | bob: 9.151Epoch   0:  51% | abe: 9.196 | eve: 9.344 | bob: 9.150Epoch   0:  52% | abe: 9.194 | eve: 9.343 | bob: 9.149Epoch   0:  52% | abe: 9.192 | eve: 9.344 | bob: 9.147Epoch   0:  53% | abe: 9.191 | eve: 9.344 | bob: 9.146Epoch   0:  54% | abe: 9.188 | eve: 9.343 | bob: 9.145Epoch   0:  54% | abe: 9.188 | eve: 9.344 | bob: 9.144Epoch   0:  55% | abe: 9.186 | eve: 9.345 | bob: 9.143Epoch   0:  56% | abe: 9.185 | eve: 9.345 | bob: 9.143Epoch   0:  56% | abe: 9.184 | eve: 9.346 | bob: 9.142Epoch   0:  57% | abe: 9.182 | eve: 9.345 | bob: 9.141Epoch   0:  58% | abe: 9.181 | eve: 9.345 | bob: 9.140Epoch   0:  58% | abe: 9.181 | eve: 9.346 | bob: 9.140Epoch   0:  59% | abe: 9.178 | eve: 9.346 | bob: 9.138Epoch   0:  60% | abe: 9.177 | eve: 9.346 | bob: 9.137Epoch   0:  60% | abe: 9.177 | eve: 9.346 | bob: 9.137Epoch   0:  61% | abe: 9.174 | eve: 9.346 | bob: 9.135Epoch   0:  62% | abe: 9.173 | eve: 9.346 | bob: 9.134Epoch   0:  63% | abe: 9.172 | eve: 9.346 | bob: 9.134Epoch   0:  63% | abe: 9.171 | eve: 9.346 | bob: 9.133Epoch   0:  64% | abe: 9.169 | eve: 9.347 | bob: 9.131Epoch   0:  65% | abe: 9.168 | eve: 9.347 | bob: 9.130Epoch   0:  65% | abe: 9.167 | eve: 9.347 | bob: 9.130Epoch   0:  66% | abe: 9.166 | eve: 9.347 | bob: 9.129Epoch   0:  67% | abe: 9.165 | eve: 9.348 | bob: 9.129Epoch   0:  67% | abe: 9.164 | eve: 9.348 | bob: 9.129Epoch   0:  68% | abe: 9.163 | eve: 9.348 | bob: 9.128Epoch   0:  69% | abe: 9.162 | eve: 9.347 | bob: 9.127Epoch   0:  69% | abe: 9.160 | eve: 9.348 | bob: 9.126Epoch   0:  70% | abe: 9.159 | eve: 9.348 | bob: 9.125Epoch   0:  71% | abe: 9.159 | eve: 9.349 | bob: 9.125Epoch   0:  71% | abe: 9.158 | eve: 9.348 | bob: 9.124Epoch   0:  72% | abe: 9.157 | eve: 9.349 | bob: 9.123Epoch   0:  73% | abe: 9.155 | eve: 9.350 | bob: 9.122Epoch   0:  73% | abe: 9.154 | eve: 9.350 | bob: 9.121Epoch   0:  74% | abe: 9.153 | eve: 9.350 | bob: 9.121Epoch   0:  75% | abe: 9.153 | eve: 9.350 | bob: 9.121Epoch   0:  76% | abe: 9.152 | eve: 9.350 | bob: 9.120Epoch   0:  76% | abe: 9.152 | eve: 9.350 | bob: 9.120Epoch   0:  77% | abe: 9.150 | eve: 9.350 | bob: 9.118Epoch   0:  78% | abe: 9.150 | eve: 9.351 | bob: 9.118Epoch   0:  78% | abe: 9.149 | eve: 9.351 | bob: 9.118Epoch   0:  79% | abe: 9.148 | eve: 9.352 | bob: 9.117Epoch   0:  80% | abe: 9.147 | eve: 9.352 | bob: 9.117Epoch   0:  80% | abe: 9.146 | eve: 9.353 | bob: 9.116Epoch   0:  81% | abe: 9.146 | eve: 9.353 | bob: 9.116Epoch   0:  82% | abe: 9.145 | eve: 9.353 | bob: 9.115Epoch   0:  82% | abe: 9.144 | eve: 9.353 | bob: 9.114Epoch   0:  83% | abe: 9.143 | eve: 9.353 | bob: 9.113Epoch   0:  84% | abe: 9.142 | eve: 9.354 | bob: 9.113Epoch   0:  84% | abe: 9.141 | eve: 9.354 | bob: 9.113Epoch   0:  85% | abe: 9.141 | eve: 9.355 | bob: 9.112Epoch   0:  86% | abe: 9.140 | eve: 9.355 | bob: 9.112Epoch   0:  86% | abe: 9.140 | eve: 9.355 | bob: 9.112Epoch   0:  87% | abe: 9.140 | eve: 9.355 | bob: 9.112Epoch   0:  88% | abe: 9.139 | eve: 9.355 | bob: 9.112Epoch   0:  89% | abe: 9.139 | eve: 9.355 | bob: 9.112Epoch   0:  89% | abe: 9.138 | eve: 9.356 | bob: 9.111Epoch   0:  90% | abe: 9.137 | eve: 9.356 | bob: 9.111Epoch   0:  91% | abe: 9.137 | eve: 9.356 | bob: 9.110Epoch   0:  91% | abe: 9.136 | eve: 9.357 | bob: 9.109Epoch   0:  92% | abe: 9.135 | eve: 9.358 | bob: 9.109Epoch   0:  93% | abe: 9.134 | eve: 9.358 | bob: 9.108Epoch   0:  93% | abe: 9.134 | eve: 9.359 | bob: 9.108Epoch   0:  94% | abe: 9.134 | eve: 9.359 | bob: 9.108Epoch   0:  95% | abe: 9.133 | eve: 9.360 | bob: 9.108Epoch   0:  95% | abe: 9.133 | eve: 9.360 | bob: 9.108Epoch   0:  96% | abe: 9.132 | eve: 9.361 | bob: 9.107Epoch   0:  97% | abe: 9.131 | eve: 9.361 | bob: 9.107Epoch   0:  97% | abe: 9.131 | eve: 9.361 | bob: 9.107Epoch   0:  98% | abe: 9.131 | eve: 9.361 | bob: 9.107Epoch   0:  99% | abe: 9.130 | eve: 9.361 | bob: 9.107
New best Bob loss 9.106511548579553 at epoch 0
Epoch   1:   0% | abe: 9.069 | eve: 9.348 | bob: 9.077Epoch   1:   0% | abe: 9.047 | eve: 9.382 | bob: 9.055Epoch   1:   1% | abe: 9.076 | eve: 9.376 | bob: 9.084Epoch   1:   2% | abe: 9.071 | eve: 9.364 | bob: 9.080Epoch   1:   2% | abe: 9.068 | eve: 9.366 | bob: 9.077Epoch   1:   3% | abe: 9.063 | eve: 9.368 | bob: 9.072Epoch   1:   4% | abe: 9.058 | eve: 9.363 | bob: 9.067Epoch   1:   4% | abe: 9.045 | eve: 9.377 | bob: 9.055Epoch   1:   5% | abe: 9.044 | eve: 9.386 | bob: 9.055Epoch   1:   6% | abe: 9.048 | eve: 9.380 | bob: 9.059Epoch   1:   6% | abe: 9.047 | eve: 9.377 | bob: 9.058Epoch   1:   7% | abe: 9.045 | eve: 9.382 | bob: 9.055Epoch   1:   8% | abe: 9.043 | eve: 9.383 | bob: 9.054Epoch   1:   8% | abe: 9.048 | eve: 9.382 | bob: 9.060Epoch   1:   9% | abe: 9.051 | eve: 9.383 | bob: 9.063Epoch   1:  10% | abe: 9.053 | eve: 9.383 | bob: 9.065Epoch   1:  10% | abe: 9.051 | eve: 9.385 | bob: 9.063Epoch   1:  11% | abe: 9.052 | eve: 9.386 | bob: 9.064Epoch   1:  12% | abe: 9.052 | eve: 9.388 | bob: 9.064Epoch   1:  13% | abe: 9.050 | eve: 9.387 | bob: 9.062Epoch   1:  13% | abe: 9.048 | eve: 9.387 | bob: 9.060Epoch   1:  14% | abe: 9.045 | eve: 9.385 | bob: 9.057Epoch   1:  15% | abe: 9.043 | eve: 9.386 | bob: 9.055Epoch   1:  15% | abe: 9.042 | eve: 9.391 | bob: 9.054Epoch   1:  16% | abe: 9.042 | eve: 9.391 | bob: 9.055Epoch   1:  17% | abe: 9.040 | eve: 9.390 | bob: 9.053Epoch   1:  17% | abe: 9.040 | eve: 9.390 | bob: 9.053Epoch   1:  18% | abe: 9.041 | eve: 9.391 | bob: 9.054Epoch   1:  19% | abe: 9.040 | eve: 9.396 | bob: 9.053Epoch   1:  19% | abe: 9.040 | eve: 9.393 | bob: 9.053Epoch   1:  20% | abe: 9.039 | eve: 9.391 | bob: 9.053Epoch   1:  21% | abe: 9.041 | eve: 9.390 | bob: 9.056Epoch   1:  21% | abe: 9.042 | eve: 9.390 | bob: 9.057Epoch   1:  22% | abe: 9.042 | eve: 9.392 | bob: 9.057Epoch   1:  23% | abe: 9.042 | eve: 9.393 | bob: 9.056Epoch   1:  23% | abe: 9.042 | eve: 9.394 | bob: 9.056Epoch   1:  24% | abe: 9.042 | eve: 9.394 | bob: 9.057Epoch   1:  25% | abe: 9.040 | eve: 9.395 | bob: 9.055Epoch   1:  26% | abe: 9.040 | eve: 9.395 | bob: 9.055Epoch   1:  26% | abe: 9.039 | eve: 9.396 | bob: 9.054Epoch   1:  27% | abe: 9.039 | eve: 9.397 | bob: 9.054Epoch   1:  28% | abe: 9.039 | eve: 9.399 | bob: 9.054Epoch   1:  28% | abe: 9.039 | eve: 9.400 | bob: 9.055Epoch   1:  29% | abe: 9.040 | eve: 9.400 | bob: 9.056Epoch   1:  30% | abe: 9.041 | eve: 9.402 | bob: 9.057Epoch   1:  30% | abe: 9.040 | eve: 9.402 | bob: 9.056Epoch   1:  31% | abe: 9.040 | eve: 9.402 | bob: 9.056Epoch   1:  32% | abe: 9.042 | eve: 9.403 | bob: 9.058Epoch   1:  32% | abe: 9.043 | eve: 9.403 | bob: 9.060Epoch   1:  33% | abe: 9.043 | eve: 9.403 | bob: 9.060Epoch   1:  34% | abe: 9.044 | eve: 9.403 | bob: 9.062Epoch   1:  34% | abe: 9.045 | eve: 9.403 | bob: 9.062Epoch   1:  35% | abe: 9.045 | eve: 9.403 | bob: 9.063Epoch   1:  36% | abe: 9.045 | eve: 9.403 | bob: 9.063Epoch   1:  36% | abe: 9.045 | eve: 9.403 | bob: 9.063Epoch   1:  37% | abe: 9.045 | eve: 9.404 | bob: 9.063Epoch   1:  38% | abe: 9.045 | eve: 9.405 | bob: 9.063Epoch   1:  39% | abe: 9.044 | eve: 9.406 | bob: 9.063Epoch   1:  39% | abe: 9.045 | eve: 9.408 | bob: 9.064Epoch   1:  40% | abe: 9.045 | eve: 9.406 | bob: 9.064Epoch   1:  41% | abe: 9.045 | eve: 9.407 | bob: 9.064Epoch   1:  41% | abe: 9.045 | eve: 9.408 | bob: 9.064Epoch   1:  42% | abe: 9.044 | eve: 9.409 | bob: 9.064Epoch   1:  43% | abe: 9.044 | eve: 9.409 | bob: 9.063Epoch   1:  43% | abe: 9.043 | eve: 9.409 | bob: 9.063Epoch   1:  44% | abe: 9.043 | eve: 9.409 | bob: 9.063Epoch   1:  45% | abe: 9.043 | eve: 9.410 | bob: 9.063Epoch   1:  45% | abe: 9.042 | eve: 9.410 | bob: 9.063Epoch   1:  46% | abe: 9.041 | eve: 9.411 | bob: 9.061Epoch   1:  47% | abe: 9.040 | eve: 9.410 | bob: 9.061Epoch   1:  47% | abe: 9.040 | eve: 9.410 | bob: 9.061Epoch   1:  48% | abe: 9.039 | eve: 9.410 | bob: 9.060Epoch   1:  49% | abe: 9.039 | eve: 9.409 | bob: 9.060Epoch   1:  50% | abe: 9.039 | eve: 9.409 | bob: 9.060Epoch   1:  50% | abe: 9.039 | eve: 9.410 | bob: 9.060Epoch   1:  51% | abe: 9.039 | eve: 9.411 | bob: 9.060Epoch   1:  52% | abe: 9.038 | eve: 9.412 | bob: 9.060Epoch   1:  52% | abe: 9.039 | eve: 9.412 | bob: 9.060Epoch   1:  53% | abe: 9.039 | eve: 9.412 | bob: 9.060Epoch   1:  54% | abe: 9.038 | eve: 9.412 | bob: 9.060Epoch   1:  54% | abe: 9.038 | eve: 9.413 | bob: 9.061Epoch   1:  55% | abe: 9.038 | eve: 9.414 | bob: 9.060Epoch   1:  56% | abe: 9.037 | eve: 9.416 | bob: 9.060Epoch   1:  56% | abe: 9.038 | eve: 9.417 | bob: 9.060Epoch   1:  57% | abe: 9.038 | eve: 9.418 | bob: 9.061Epoch   1:  58% | abe: 9.038 | eve: 9.418 | bob: 9.061Epoch   1:  58% | abe: 9.038 | eve: 9.418 | bob: 9.061Epoch   1:  59% | abe: 9.037 | eve: 9.418 | bob: 9.061Epoch   1:  60% | abe: 9.037 | eve: 9.417 | bob: 9.060Epoch   1:  60% | abe: 9.036 | eve: 9.418 | bob: 9.060Epoch   1:  61% | abe: 9.036 | eve: 9.417 | bob: 9.060Epoch   1:  62% | abe: 9.037 | eve: 9.417 | bob: 9.061Epoch   1:  63% | abe: 9.036 | eve: 9.418 | bob: 9.060Epoch   1:  63% | abe: 9.036 | eve: 9.417 | bob: 9.060Epoch   1:  64% | abe: 9.036 | eve: 9.417 | bob: 9.061Epoch   1:  65% | abe: 9.036 | eve: 9.418 | bob: 9.060Epoch   1:  65% | abe: 9.035 | eve: 9.418 | bob: 9.060Epoch   1:  66% | abe: 9.035 | eve: 9.419 | bob: 9.059Epoch   1:  67% | abe: 9.035 | eve: 9.418 | bob: 9.060Epoch   1:  67% | abe: 9.034 | eve: 9.419 | bob: 9.060Epoch   1:  68% | abe: 9.035 | eve: 9.420 | bob: 9.060Epoch   1:  69% | abe: 9.035 | eve: 9.420 | bob: 9.061Epoch   1:  69% | abe: 9.035 | eve: 9.420 | bob: 9.061Epoch   1:  70% | abe: 9.035 | eve: 9.420 | bob: 9.061Epoch   1:  71% | abe: 9.035 | eve: 9.420 | bob: 9.061Epoch   1:  71% | abe: 9.035 | eve: 9.420 | bob: 9.061Epoch   1:  72% | abe: 9.035 | eve: 9.422 | bob: 9.062Epoch   1:  73% | abe: 9.035 | eve: 9.422 | bob: 9.061Epoch   1:  73% | abe: 9.035 | eve: 9.422 | bob: 9.061Epoch   1:  74% | abe: 9.035 | eve: 9.423 | bob: 9.062Epoch   1:  75% | abe: 9.035 | eve: 9.424 | bob: 9.062Epoch   1:  76% | abe: 9.036 | eve: 9.424 | bob: 9.063Epoch   1:  76% | abe: 9.035 | eve: 9.424 | bob: 9.062Epoch   1:  77% | abe: 9.035 | eve: 9.424 | bob: 9.063Epoch   1:  78% | abe: 9.035 | eve: 9.424 | bob: 9.063Epoch   1:  78% | abe: 9.035 | eve: 9.425 | bob: 9.062Epoch   1:  79% | abe: 9.034 | eve: 9.425 | bob: 9.062Epoch   1:  80% | abe: 9.034 | eve: 9.426 | bob: 9.062Epoch   1:  80% | abe: 9.034 | eve: 9.426 | bob: 9.062Epoch   1:  81% | abe: 9.034 | eve: 9.426 | bob: 9.062Epoch   1:  82% | abe: 9.034 | eve: 9.427 | bob: 9.063Epoch   1:  82% | abe: 9.034 | eve: 9.427 | bob: 9.062Epoch   1:  83% | abe: 9.033 | eve: 9.427 | bob: 9.062Epoch   1:  84% | abe: 9.033 | eve: 9.428 | bob: 9.062Epoch   1:  84% | abe: 9.033 | eve: 9.428 | bob: 9.062Epoch   1:  85% | abe: 9.034 | eve: 9.427 | bob: 9.063Epoch   1:  86% | abe: 9.034 | eve: 9.428 | bob: 9.063Epoch   1:  86% | abe: 9.033 | eve: 9.428 | bob: 9.062Epoch   1:  87% | abe: 9.033 | eve: 9.428 | bob: 9.062Epoch   1:  88% | abe: 9.032 | eve: 9.428 | bob: 9.062Epoch   1:  89% | abe: 9.032 | eve: 9.428 | bob: 9.062Epoch   1:  89% | abe: 9.032 | eve: 9.428 | bob: 9.062Epoch   1:  90% | abe: 9.032 | eve: 9.428 | bob: 9.063Epoch   1:  91% | abe: 9.032 | eve: 9.428 | bob: 9.062Epoch   1:  91% | abe: 9.031 | eve: 9.428 | bob: 9.062Epoch   1:  92% | abe: 9.031 | eve: 9.429 | bob: 9.062Epoch   1:  93% | abe: 9.031 | eve: 9.428 | bob: 9.062Epoch   1:  93% | abe: 9.031 | eve: 9.429 | bob: 9.063Epoch   1:  94% | abe: 9.031 | eve: 9.429 | bob: 9.062Epoch   1:  95% | abe: 9.031 | eve: 9.429 | bob: 9.063Epoch   1:  95% | abe: 9.031 | eve: 9.430 | bob: 9.063Epoch   1:  96% | abe: 9.031 | eve: 9.430 | bob: 9.063Epoch   1:  97% | abe: 9.031 | eve: 9.430 | bob: 9.063Epoch   1:  97% | abe: 9.032 | eve: 9.430 | bob: 9.064Epoch   1:  98% | abe: 9.032 | eve: 9.431 | bob: 9.064Epoch   1:  99% | abe: 9.032 | eve: 9.431 | bob: 9.064
New best Bob loss 9.064066805176248 at epoch 1
Training complete.
cipher1 + cipher2
[[0.8997116  0.90693426 0.97036165 ... 1.2199624  1.2277179  0.9504648 ]
 [0.91558146 0.82805884 0.9823843  ... 1.1924775  1.2180597  0.93536216]
 [0.9526912  0.86893237 0.99387634 ... 1.1758405  1.0991392  1.0094658 ]
 ...
 [0.9362845  0.88637614 0.890167   ... 1.143426   1.1361014  0.96961683]
 [0.8882118  0.88048923 0.9876418  ... 1.1945629  1.1838782  0.9879879 ]
 [0.87931025 0.84359974 0.94850063 ... 1.1998408  1.1717722  0.93294394]]
HO addition:
[[0.8996948  0.90691686 0.9703429  ... 1.219939   1.2276942  0.95044726]
 [0.9155642  0.8280433  0.98236525 ... 1.1924547  1.2180362  0.9353452 ]
 [0.95267355 0.868916   0.99385685 ... 1.1758182  1.0991181  1.0094465 ]
 ...
 [0.93626654 0.8863596  0.89015    ... 1.1434039  1.1360793  0.96959907]
 [0.8881949  0.8804727  0.9876229  ... 1.1945399  1.1838555  0.9879699 ]
 [0.8792937  0.843584   0.94848245 ... 1.1998179  1.1717492  0.93292665]]
cipher1 * cipher2
[[0.20186146 0.20491289 0.2329126  ... 0.37090507 0.37421396 0.22392134]
 [0.20955624 0.17116034 0.23889136 ... 0.3550482  0.3684499  0.2124532 ]
 [0.22553043 0.18871589 0.24215412 ... 0.345476   0.3006703  0.25370717]
 ...
 [0.21825418 0.19595216 0.19782521 ... 0.32387653 0.31981626 0.2307949 ]
 [0.19702248 0.1933976  0.2432314  ... 0.35608009 0.34931502 0.23974557]
 [0.19329627 0.17764418 0.22408272 ... 0.3590481  0.33586198 0.2162518 ]]
HO multiplication
[[0.20185989 0.20491147 0.23291036 ... 0.37091002 0.3742191  0.22391924]
 [0.20955446 0.17116001 0.23888895 ... 0.35505134 0.36845416 0.21245164]
 [0.22552823 0.18871497 0.2421518  ... 0.34547797 0.30066824 0.2537043 ]
 ...
 [0.21825227 0.19595084 0.197824   ... 0.3238758  0.3198153  0.23079292]
 [0.19702134 0.19339637 0.2432287  ... 0.35608315 0.34931713 0.23974347]
 [0.19329521 0.17764369 0.22408062 ... 0.35905147 0.3358621  0.2162498 ]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 100.00%
Bob decrypted addition: [[0.89177936 0.879907   0.936622   ... 0.93389463 0.96329194 0.9458307 ]
 [0.87651545 0.9343291  0.9420035  ... 0.86462015 0.92700356 0.9649181 ]
 [0.9336587  0.860731   0.9100106  ... 0.91348726 0.9394237  0.9566036 ]
 ...
 [0.9424175  0.86410534 0.93567026 ... 0.8862464  0.9223825  0.9680435 ]
 [0.9386363  0.8749891  0.9443369  ... 0.9383219  0.95831245 0.94874644]
 [0.82738847 0.9578951  0.9342433  ... 0.88889104 0.9250928  0.96587425]]
Bob decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits addition: 3551
Total number of bits addition: 7168
Decryption accuracy addition: 49.539620535714285%
Bob decrypted multiplication: [[1.0126762  0.917778   0.94266695 ... 0.9423743  0.9678873  0.93958426]
 [0.9760366  0.9580803  0.9485026  ... 0.90931565 0.9778484  0.93715894]
 [0.99424356 0.92749625 0.9906066  ... 0.9642935  0.96192336 0.9389473 ]
 ...
 [1.001886   0.9173984  0.96267354 ... 0.9329665  0.9804629  0.93995684]
 [1.000232   0.9095181  0.96436495 ... 0.9817531  0.98215085 0.9292536 ]
 [0.94676524 0.9947935  0.9735762  ... 0.95165336 0.9802051  0.9338171 ]]
Bob decrypted bits multiplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits multiplication: 1818
Total number of bits multiplication: 7168
Decryption accuracy multiplication: 25.362723214285715%
Eve decrypted addition: [[0.9532929  0.9893789  0.98608744 ... 1.1024476  1.0729436  0.9370759 ]
 [0.9838552  0.94078463 0.96886164 ... 1.1076614  1.0737257  0.95616794]
 [0.9988475  0.9604474  0.95636714 ... 1.108806   1.079541   0.9939802 ]
 ...
 [0.98756677 0.9361847  0.95083076 ... 1.101164   1.0844752  0.99067426]
 [0.94457287 0.96317035 0.96304435 ... 1.0854211  1.068398   0.94530165]
 [0.9646027  1.0572599  1.0318198  ... 1.1005166  1.0663275  0.9436226 ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 3551
Total number of bits addition: 7168
Decryption accuracy by Eve addition: 49.539620535714285%
Eve decrypted mulitplication: [[0.93774164 0.997083   0.9662839  ... 1.1027858  1.055053   0.9178308 ]
 [0.9563707  0.96886694 0.9649611  ... 1.0989934  1.037744   0.93188655]
 [0.9659275  1.0186809  0.9562287  ... 1.1049278  1.042229   0.9588117 ]
 ...
 [0.9838885  0.9917123  0.9435613  ... 1.1050984  1.068794   0.9669621 ]
 [0.9335488  1.000359   0.9547623  ... 1.0823735  1.0326899  0.92324746]
 [0.9634278  1.081423   1.0421798  ... 1.0864055  1.0276431  0.9230402 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 1818
Total number of bits mulitplication: 7168
Decryption accuracy by Eve mulitplication: 25.362723214285715%
Bob decrypted P1: [[0.9709421  0.90659523 0.94830954 ... 0.9408076  0.9695314  0.9406129 ]
 [0.9465833  0.9480565  0.9501627  ... 0.90454525 0.97238755 0.94160706]
 [0.9738898  0.91474783 0.96373093 ... 0.9533447  0.962216   0.9415094 ]
 ...
 [0.9790578  0.9051006  0.96257484 ... 0.9306095  0.97376686 0.9438706 ]
 [0.97798264 0.9024952  0.96187234 ... 0.9696551  0.98120046 0.9331332 ]
 [0.8993639  0.9818819  0.9649958  ... 0.9416697  0.9746591  0.9396038 ]]
Bob decrypted bits P1: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P1: 3593
Total number of bits P1: 7168
Decryption accuracy P1: 50.12555803571429%
Bob decrypted P2: [[0.96082574 0.9011667  0.948554   ... 0.94400877 0.97047085 0.9404406 ]
 [0.93209666 0.9488953  0.9494532  ... 0.90257525 0.9721146  0.9422906 ]
 [0.97175133 0.90541005 0.9626843  ... 0.9527752  0.9611287  0.94263595]
 ...
 [0.9779643  0.8999284  0.9589582  ... 0.9254962  0.9709099  0.9457828 ]
 [0.9757447  0.8987873  0.96257573 ... 0.9717283  0.983862   0.9316955 ]
 [0.8812464  0.9841281  0.9635571  ... 0.9379326  0.9776313  0.93890357]]
Bob decrypted bits P2: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P2: 3594
Total number of bits P2: 7168
Decryption accuracy P2: 50.13950892857143%
