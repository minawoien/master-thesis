WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-18 19:33:56.927438: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-18 19:33:57.055891: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-18 19:33:57.056685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:33:57.060034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-18 19:33:57.062759: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-18 19:33:57.063733: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-18 19:33:57.067556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-18 19:33:57.070128: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-18 19:33:57.078122: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-18 19:33:57.086988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-18 19:33:57.087436: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-18 19:33:57.101461: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-18 19:33:57.105411: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f66360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-18 19:33:57.105462: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-18 19:33:57.307152: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x492f990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-18 19:33:57.307234: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-18 19:33:57.315294: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-18 19:33:57.315418: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:33:57.315462: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-18 19:33:57.315498: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-18 19:33:57.315534: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-18 19:33:57.315570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-18 19:33:57.315605: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-18 19:33:57.315642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-18 19:33:57.321796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-18 19:33:57.321868: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:33:57.325273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-18 19:33:57.325296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-18 19:33:57.325305: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-18 19:33:57.333907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output bob_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_2.
WARNING:tensorflow:Output bob_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_3.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
WARNING:tensorflow:Output eve_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_2.
WARNING:tensorflow:Output eve_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_3.
2024-04-18 19:34:00.826454: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9214 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4577 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1462 - val_loss: 3.8877e-04
Epoch 4/512
448/448 - 0s - loss: 0.0222 - val_loss: 3.9878e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5595e-06
Epoch 6/512
448/448 - 0s - loss: 7.3945e-04 - val_loss: 5.8594e-06
Epoch 7/512
448/448 - 0s - loss: 5.3829e-04 - val_loss: 4.4139e-06
Epoch 8/512
448/448 - 0s - loss: 3.9839e-04 - val_loss: 3.1155e-06
Epoch 9/512
448/448 - 0s - loss: 2.7460e-04 - val_loss: 2.0206e-06
Epoch 10/512
448/448 - 0s - loss: 1.7294e-04 - val_loss: 1.1772e-06
Epoch 11/512
448/448 - 0s - loss: 9.7209e-05 - val_loss: 6.0017e-07
Epoch 12/512
448/448 - 0s - loss: 4.7428e-05 - val_loss: 2.5787e-07
Epoch 13/512
448/448 - 0s - loss: 1.9325e-05 - val_loss: 8.9498e-08
Epoch 14/512
448/448 - 0s - loss: 6.2910e-06 - val_loss: 2.3575e-08
Epoch 15/512
448/448 - 0s - loss: 1.5370e-06 - val_loss: 4.5712e-09
Epoch 16/512
448/448 - 0s - loss: 6.8089e-07 - val_loss: 1.2379e-07
Epoch 17/512
448/448 - 0s - loss: 8.3661e-04 - val_loss: 8.6116e-05
Epoch 18/512
448/448 - 0s - loss: 0.0041 - val_loss: 2.8924e-06
Epoch 19/512
448/448 - 0s - loss: 1.7087e-04 - val_loss: 9.9415e-07
Epoch 20/512
448/448 - 0s - loss: 1.6066e-04 - val_loss: 6.2171e-06
Epoch 21/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.0728e-05
Epoch 22/512
448/448 - 0s - loss: 0.0019 - val_loss: 5.5792e-06
Epoch 23/512
448/448 - 0s - loss: 4.5498e-04 - val_loss: 4.9162e-06
Epoch 24/512
448/448 - 0s - loss: 7.5211e-04 - val_loss: 1.6630e-05
Epoch 25/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.2969e-05
Epoch 26/512
448/448 - 0s - loss: 9.7275e-04 - val_loss: 6.5459e-06
Epoch 27/512
448/448 - 0s - loss: 6.9460e-04 - val_loss: 1.0169e-05
Epoch 28/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.4226e-05
Epoch 29/512
448/448 - 0s - loss: 0.0012 - val_loss: 8.7341e-06
Epoch 30/512
448/448 - 0s - loss: 8.0283e-04 - val_loss: 8.6869e-06
Epoch 31/512
448/448 - 0s - loss: 9.5817e-04 - val_loss: 1.1794e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.0081e-05
Epoch 33/512
448/448 - 0s - loss: 9.1208e-04 - val_loss: 8.7429e-06
Epoch 34/512
448/448 - 0s - loss: 8.8802e-04 - val_loss: 9.9789e-06
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.0034e-05
Epoch 36/512
448/448 - 0s - loss: 9.4109e-04 - val_loss: 9.0622e-06
Epoch 37/512
448/448 - 0s - loss: 8.8019e-04 - val_loss: 9.1786e-06
Epoch 38/512
448/448 - 0s - loss: 9.1036e-04 - val_loss: 9.5989e-06
Epoch 39/512
448/448 - 0s - loss: 9.2574e-04 - val_loss: 8.9531e-06
Epoch 40/512
448/448 - 0s - loss: 8.6326e-04 - val_loss: 8.6987e-06
Epoch 41/512
448/448 - 0s - loss: 8.6510e-04 - val_loss: 8.9302e-06
Epoch 42/512
448/448 - 0s - loss: 8.8109e-04 - val_loss: 8.6568e-06
Epoch 43/512
448/448 - 0s - loss: 8.4110e-04 - val_loss: 8.5314e-06
Epoch 44/512
448/448 - 0s - loss: 8.3688e-04 - val_loss: 8.6610e-06
Epoch 45/512
448/448 - 0s - loss: 8.4265e-04 - val_loss: 8.4993e-06
Epoch 46/512
448/448 - 0s - loss: 8.2166e-04 - val_loss: 8.2168e-06
Epoch 47/512
448/448 - 0s - loss: 8.0767e-04 - val_loss: 8.0772e-06
Epoch 48/512
448/448 - 0s - loss: 8.0017e-04 - val_loss: 8.0660e-06
Epoch 49/512
448/448 - 0s - loss: 7.8995e-04 - val_loss: 8.1082e-06
Epoch 50/512
448/448 - 0s - loss: 7.9025e-04 - val_loss: 7.9950e-06
Epoch 51/512
448/448 - 0s - loss: 7.7767e-04 - val_loss: 7.7112e-06
Epoch 52/512
448/448 - 0s - loss: 7.5837e-04 - val_loss: 7.6571e-06
Epoch 53/512
448/448 - 0s - loss: 7.6036e-04 - val_loss: 7.5420e-06
Epoch 54/512
448/448 - 0s - loss: 7.4546e-04 - val_loss: 7.5229e-06
Epoch 55/512
448/448 - 0s - loss: 7.4063e-04 - val_loss: 7.5211e-06
Epoch 56/512
448/448 - 0s - loss: 7.3972e-04 - val_loss: 7.2467e-06
Epoch 57/512
448/448 - 0s - loss: 7.1220e-04 - val_loss: 7.2378e-06
Epoch 58/512
448/448 - 0s - loss: 7.1647e-04 - val_loss: 7.3643e-06
Epoch 59/512
448/448 - 0s - loss: 7.1989e-04 - val_loss: 7.0561e-06
Epoch 60/512
448/448 - 0s - loss: 6.9251e-04 - val_loss: 6.9075e-06
Epoch 61/512
448/448 - 0s - loss: 6.8914e-04 - val_loss: 6.9769e-06
Epoch 62/512
448/448 - 0s - loss: 6.9223e-04 - val_loss: 6.8981e-06
Epoch 63/512
448/448 - 0s - loss: 6.8113e-04 - val_loss: 6.6594e-06
Epoch 64/512
448/448 - 0s - loss: 6.6051e-04 - val_loss: 6.7420e-06
Epoch 65/512
448/448 - 0s - loss: 6.7000e-04 - val_loss: 6.7887e-06
Epoch 66/512
448/448 - 0s - loss: 6.6540e-04 - val_loss: 6.5640e-06
Epoch 67/512
448/448 - 0s - loss: 6.4489e-04 - val_loss: 6.4448e-06
Epoch 68/512
448/448 - 0s - loss: 6.3906e-04 - val_loss: 6.5799e-06
Epoch 69/512
448/448 - 0s - loss: 6.5010e-04 - val_loss: 6.4651e-06
Epoch 70/512
448/448 - 0s - loss: 6.3279e-04 - val_loss: 6.2040e-06
Epoch 71/512
448/448 - 0s - loss: 6.1927e-04 - val_loss: 6.1176e-06
Epoch 72/512
448/448 - 0s - loss: 6.1056e-04 - val_loss: 6.3788e-06
Epoch 73/512
448/448 - 0s - loss: 6.3025e-04 - val_loss: 6.1897e-06
Epoch 74/512
448/448 - 0s - loss: 6.0406e-04 - val_loss: 5.9813e-06
Epoch 75/512
448/448 - 0s - loss: 5.9373e-04 - val_loss: 6.0737e-06
Epoch 76/512
448/448 - 0s - loss: 6.0315e-04 - val_loss: 5.9885e-06
Epoch 77/512
448/448 - 0s - loss: 5.9102e-04 - val_loss: 5.7997e-06
Epoch 78/512
448/448 - 0s - loss: 5.7756e-04 - val_loss: 5.8545e-06
Epoch 79/512
448/448 - 0s - loss: 5.8035e-04 - val_loss: 5.9827e-06
Epoch 80/512
448/448 - 0s - loss: 5.8602e-04 - val_loss: 5.6974e-06
Epoch 81/512
448/448 - 0s - loss: 5.5581e-04 - val_loss: 5.6578e-06
Epoch 82/512
448/448 - 0s - loss: 5.6473e-04 - val_loss: 5.6797e-06
Epoch 83/512
448/448 - 0s - loss: 5.6126e-04 - val_loss: 5.5935e-06
Epoch 84/512
448/448 - 0s - loss: 5.5116e-04 - val_loss: 5.5380e-06
Epoch 85/512
448/448 - 0s - loss: 5.4534e-04 - val_loss: 5.5836e-06
Epoch 86/512
448/448 - 0s - loss: 5.4950e-04 - val_loss: 5.3524e-06
Epoch 87/512
448/448 - 0s - loss: 5.2830e-04 - val_loss: 5.3538e-06
Epoch 88/512
448/448 - 0s - loss: 5.3423e-04 - val_loss: 5.3825e-06
Epoch 89/512
448/448 - 0s - loss: 5.3358e-04 - val_loss: 5.2121e-06
Epoch 90/512
448/448 - 0s - loss: 5.1908e-04 - val_loss: 5.0778e-06
Epoch 91/512
448/448 - 0s - loss: 5.0911e-04 - val_loss: 5.2748e-06
Epoch 92/512
448/448 - 0s - loss: 5.2446e-04 - val_loss: 5.1866e-06
Epoch 93/512
448/448 - 0s - loss: 5.0821e-04 - val_loss: 5.0209e-06
Epoch 94/512
448/448 - 0s - loss: 5.0165e-04 - val_loss: 4.9370e-06
Epoch 95/512
448/448 - 0s - loss: 4.9456e-04 - val_loss: 5.0298e-06
Epoch 96/512
448/448 - 0s - loss: 5.0143e-04 - val_loss: 4.9308e-06
Epoch 97/512
448/448 - 0s - loss: 4.8904e-04 - val_loss: 4.7834e-06
Epoch 98/512
448/448 - 0s - loss: 4.7971e-04 - val_loss: 4.8314e-06
Epoch 99/512
448/448 - 0s - loss: 4.8233e-04 - val_loss: 4.9305e-06
Epoch 100/512
448/448 - 0s - loss: 4.8791e-04 - val_loss: 4.6694e-06
Epoch 101/512
448/448 - 0s - loss: 4.6277e-04 - val_loss: 4.6508e-06
Epoch 102/512
448/448 - 0s - loss: 4.6558e-04 - val_loss: 4.8588e-06
Epoch 103/512
448/448 - 0s - loss: 4.7940e-04 - val_loss: 4.6213e-06
Epoch 104/512
448/448 - 0s - loss: 4.5192e-04 - val_loss: 4.5280e-06
Epoch 105/512
448/448 - 0s - loss: 4.5147e-04 - val_loss: 4.6693e-06
Epoch 106/512
448/448 - 0s - loss: 4.6347e-04 - val_loss: 4.5145e-06
Epoch 107/512
448/448 - 0s - loss: 4.4652e-04 - val_loss: 4.3135e-06
Epoch 108/512
448/448 - 0s - loss: 4.3342e-04 - val_loss: 4.4785e-06
Epoch 109/512
448/448 - 0s - loss: 4.4998e-04 - val_loss: 4.4449e-06
Epoch 110/512
448/448 - 0s - loss: 4.3593e-04 - val_loss: 4.3139e-06
Epoch 111/512
448/448 - 0s - loss: 4.2723e-04 - val_loss: 4.3535e-06
Epoch 112/512
448/448 - 0s - loss: 4.3319e-04 - val_loss: 4.2886e-06
Epoch 113/512
448/448 - 0s - loss: 4.2445e-04 - val_loss: 4.1759e-06
Epoch 114/512
448/448 - 0s - loss: 4.1790e-04 - val_loss: 4.1796e-06
Epoch 115/512
448/448 - 0s - loss: 4.1740e-04 - val_loss: 4.1926e-06
Epoch 116/512
448/448 - 0s - loss: 4.1761e-04 - val_loss: 4.0373e-06
Epoch 117/512
448/448 - 0s - loss: 4.0139e-04 - val_loss: 4.0986e-06
Epoch 118/512
448/448 - 0s - loss: 4.1065e-04 - val_loss: 4.0749e-06
Epoch 119/512
448/448 - 0s - loss: 4.0278e-04 - val_loss: 3.9853e-06
Epoch 120/512
448/448 - 0s - loss: 3.9480e-04 - val_loss: 4.0042e-06
Epoch 121/512
448/448 - 0s - loss: 3.9860e-04 - val_loss: 3.9268e-06
Epoch 122/512
448/448 - 0s - loss: 3.8726e-04 - val_loss: 3.9248e-06
Epoch 123/512
448/448 - 0s - loss: 3.9145e-04 - val_loss: 3.8456e-06
Epoch 124/512
448/448 - 0s - loss: 3.8380e-04 - val_loss: 3.7638e-06
Epoch 125/512
448/448 - 0s - loss: 3.7507e-04 - val_loss: 3.8454e-06
Epoch 126/512
448/448 - 0s - loss: 3.8175e-04 - val_loss: 3.8373e-06
Epoch 127/512
448/448 - 0s - loss: 3.7667e-04 - val_loss: 3.6776e-06
Epoch 128/512
448/448 - 0s - loss: 3.6247e-04 - val_loss: 3.7625e-06
Epoch 129/512
448/448 - 0s - loss: 3.7772e-04 - val_loss: 3.5805e-06
Epoch 130/512
448/448 - 0s - loss: 3.5539e-04 - val_loss: 3.5415e-06
Epoch 131/512
448/448 - 0s - loss: 3.5867e-04 - val_loss: 3.6399e-06
Epoch 132/512
448/448 - 0s - loss: 3.6371e-04 - val_loss: 3.5190e-06
Epoch 133/512
448/448 - 0s - loss: 3.4810e-04 - val_loss: 3.5045e-06
Epoch 134/512
448/448 - 0s - loss: 3.5110e-04 - val_loss: 3.5343e-06
Epoch 135/512
448/448 - 0s - loss: 3.5204e-04 - val_loss: 3.3685e-06
Epoch 136/512
448/448 - 0s - loss: 3.3420e-04 - val_loss: 3.4488e-06
Epoch 137/512
448/448 - 0s - loss: 3.4669e-04 - val_loss: 3.4186e-06
Epoch 138/512
448/448 - 0s - loss: 3.3676e-04 - val_loss: 3.3423e-06
Epoch 139/512
448/448 - 0s - loss: 3.3386e-04 - val_loss: 3.2390e-06
Epoch 140/512
448/448 - 0s - loss: 3.2480e-04 - val_loss: 3.3422e-06
Epoch 141/512
448/448 - 0s - loss: 3.3567e-04 - val_loss: 3.2469e-06
Epoch 142/512
448/448 - 0s - loss: 3.2233e-04 - val_loss: 3.1412e-06
Epoch 143/512
448/448 - 0s - loss: 3.1678e-04 - val_loss: 3.1957e-06
Epoch 144/512
448/448 - 0s - loss: 3.2126e-04 - val_loss: 3.2006e-06
Epoch 145/512
448/448 - 0s - loss: 3.1836e-04 - val_loss: 3.0746e-06
Epoch 146/512
448/448 - 0s - loss: 3.0576e-04 - val_loss: 3.1300e-06
Epoch 147/512
448/448 - 0s - loss: 3.1486e-04 - val_loss: 3.0759e-06
Epoch 148/512
448/448 - 0s - loss: 3.0594e-04 - val_loss: 2.9497e-06
Epoch 149/512
448/448 - 0s - loss: 2.9651e-04 - val_loss: 3.0165e-06
Epoch 150/512
448/448 - 0s - loss: 3.0418e-04 - val_loss: 3.0071e-06
Epoch 151/512
448/448 - 0s - loss: 2.9819e-04 - val_loss: 2.9221e-06
Epoch 152/512
448/448 - 0s - loss: 2.9181e-04 - val_loss: 2.9064e-06
Epoch 153/512
448/448 - 0s - loss: 2.8997e-04 - val_loss: 2.9520e-06
Epoch 154/512
448/448 - 0s - loss: 2.9229e-04 - val_loss: 2.8873e-06
Epoch 155/512
448/448 - 0s - loss: 2.8448e-04 - val_loss: 2.8116e-06
Epoch 156/512
448/448 - 0s - loss: 2.8036e-04 - val_loss: 2.8255e-06
Epoch 157/512
448/448 - 0s - loss: 2.8289e-04 - val_loss: 2.7910e-06
Epoch 158/512
448/448 - 0s - loss: 2.7757e-04 - val_loss: 2.6902e-06
Epoch 159/512
448/448 - 0s - loss: 2.7105e-04 - val_loss: 2.6867e-06
Epoch 160/512
448/448 - 0s - loss: 2.7124e-04 - val_loss: 2.7162e-06
Epoch 161/512
448/448 - 0s - loss: 2.7122e-04 - val_loss: 2.6564e-06
Epoch 162/512
448/448 - 0s - loss: 2.6449e-04 - val_loss: 2.6171e-06
Epoch 163/512
448/448 - 0s - loss: 2.6308e-04 - val_loss: 2.5814e-06
Epoch 164/512
448/448 - 0s - loss: 2.5855e-04 - val_loss: 2.6023e-06
Epoch 165/512
448/448 - 0s - loss: 2.6018e-04 - val_loss: 2.5850e-06
Epoch 166/512
448/448 - 0s - loss: 2.5680e-04 - val_loss: 2.4781e-06
Epoch 167/512
448/448 - 0s - loss: 2.4919e-04 - val_loss: 2.4590e-06
Epoch 168/512
448/448 - 0s - loss: 2.4892e-04 - val_loss: 2.4861e-06
Epoch 169/512
448/448 - 0s - loss: 2.4853e-04 - val_loss: 2.4605e-06
Epoch 170/512
448/448 - 0s - loss: 2.4568e-04 - val_loss: 2.3833e-06
Epoch 171/512
448/448 - 0s - loss: 2.3879e-04 - val_loss: 2.3829e-06
Epoch 172/512
448/448 - 0s - loss: 2.4048e-04 - val_loss: 2.3679e-06
Epoch 173/512
448/448 - 0s - loss: 2.3652e-04 - val_loss: 2.3424e-06
Epoch 174/512
448/448 - 0s - loss: 2.3535e-04 - val_loss: 2.2794e-06
Epoch 175/512
448/448 - 0s - loss: 2.2738e-04 - val_loss: 2.3279e-06
Epoch 176/512
448/448 - 0s - loss: 2.3251e-04 - val_loss: 2.3045e-06
Epoch 177/512
448/448 - 0s - loss: 2.2726e-04 - val_loss: 2.2163e-06
Epoch 178/512
448/448 - 0s - loss: 2.2206e-04 - val_loss: 2.1630e-06
Epoch 179/512
448/448 - 0s - loss: 2.1976e-04 - val_loss: 2.1656e-06
Epoch 180/512
448/448 - 0s - loss: 2.1793e-04 - val_loss: 2.2167e-06
Epoch 181/512
448/448 - 0s - loss: 2.2117e-04 - val_loss: 2.1104e-06
Epoch 182/512
448/448 - 0s - loss: 2.0981e-04 - val_loss: 2.0937e-06
Epoch 183/512
448/448 - 0s - loss: 2.1029e-04 - val_loss: 2.1586e-06
Epoch 184/512
448/448 - 0s - loss: 2.1537e-04 - val_loss: 2.0186e-06
Epoch 185/512
448/448 - 0s - loss: 1.9980e-04 - val_loss: 2.0151e-06
Epoch 186/512
448/448 - 0s - loss: 2.0393e-04 - val_loss: 2.0804e-06
Epoch 187/512
448/448 - 0s - loss: 2.0594e-04 - val_loss: 2.0117e-06
Epoch 188/512
448/448 - 0s - loss: 1.9706e-04 - val_loss: 1.9801e-06
Epoch 189/512
448/448 - 0s - loss: 1.9822e-04 - val_loss: 1.9359e-06
Epoch 190/512
448/448 - 0s - loss: 1.9358e-04 - val_loss: 1.9026e-06
Epoch 191/512
448/448 - 0s - loss: 1.9187e-04 - val_loss: 1.9076e-06
Epoch 192/512
448/448 - 0s - loss: 1.9072e-04 - val_loss: 1.8799e-06
Epoch 193/512
448/448 - 0s - loss: 1.8797e-04 - val_loss: 1.8130e-06
Epoch 194/512
448/448 - 0s - loss: 1.8202e-04 - val_loss: 1.8416e-06
Epoch 195/512
448/448 - 0s - loss: 1.8516e-04 - val_loss: 1.8427e-06
Epoch 196/512
448/448 - 0s - loss: 1.8156e-04 - val_loss: 1.7811e-06
Epoch 197/512
448/448 - 0s - loss: 1.7696e-04 - val_loss: 1.7513e-06
Epoch 198/512
448/448 - 0s - loss: 1.7623e-04 - val_loss: 1.7186e-06
Epoch 199/512
448/448 - 0s - loss: 1.7265e-04 - val_loss: 1.7148e-06
Epoch 200/512
448/448 - 0s - loss: 1.7241e-04 - val_loss: 1.7084e-06
Epoch 201/512
448/448 - 0s - loss: 1.7022e-04 - val_loss: 1.6574e-06
Epoch 202/512
448/448 - 0s - loss: 1.6547e-04 - val_loss: 1.6347e-06
Epoch 203/512
448/448 - 0s - loss: 1.6501e-04 - val_loss: 1.6305e-06
Epoch 204/512
448/448 - 0s - loss: 1.6277e-04 - val_loss: 1.6156e-06
Epoch 205/512
448/448 - 0s - loss: 1.6144e-04 - val_loss: 1.5653e-06
Epoch 206/512
448/448 - 0s - loss: 1.5695e-04 - val_loss: 1.5377e-06
Epoch 207/512
448/448 - 0s - loss: 1.5612e-04 - val_loss: 1.5204e-06
Epoch 208/512
448/448 - 0s - loss: 1.5302e-04 - val_loss: 1.5166e-06
Epoch 209/512
448/448 - 0s - loss: 1.5137e-04 - val_loss: 1.5277e-06
Epoch 210/512
448/448 - 0s - loss: 1.5249e-04 - val_loss: 1.4522e-06
Epoch 211/512
448/448 - 0s - loss: 1.4569e-04 - val_loss: 1.3941e-06
Epoch 212/512
448/448 - 0s - loss: 1.4188e-04 - val_loss: 1.4511e-06
Epoch 213/512
448/448 - 0s - loss: 1.4619e-04 - val_loss: 1.4551e-06
Epoch 214/512
448/448 - 0s - loss: 1.4289e-04 - val_loss: 1.3667e-06
Epoch 215/512
448/448 - 0s - loss: 1.3600e-04 - val_loss: 1.3595e-06
Epoch 216/512
448/448 - 0s - loss: 1.3755e-04 - val_loss: 1.3574e-06
Epoch 217/512
448/448 - 0s - loss: 1.3555e-04 - val_loss: 1.3297e-06
Epoch 218/512
448/448 - 0s - loss: 1.3289e-04 - val_loss: 1.3018e-06
Epoch 219/512
448/448 - 0s - loss: 1.3054e-04 - val_loss: 1.2788e-06
Epoch 220/512
448/448 - 0s - loss: 1.2871e-04 - val_loss: 1.2580e-06
Epoch 221/512
448/448 - 0s - loss: 1.2668e-04 - val_loss: 1.2437e-06
Epoch 222/512
448/448 - 0s - loss: 1.2512e-04 - val_loss: 1.2080e-06
Epoch 223/512
448/448 - 0s - loss: 1.2147e-04 - val_loss: 1.2013e-06
Epoch 224/512
448/448 - 0s - loss: 1.2112e-04 - val_loss: 1.1915e-06
Epoch 225/512
448/448 - 0s - loss: 1.1978e-04 - val_loss: 1.1521e-06
Epoch 226/512
448/448 - 0s - loss: 1.1502e-04 - val_loss: 1.1657e-06
Epoch 227/512
448/448 - 0s - loss: 1.1759e-04 - val_loss: 1.1052e-06
Epoch 228/512
448/448 - 0s - loss: 1.1090e-04 - val_loss: 1.0692e-06
Epoch 229/512
448/448 - 0s - loss: 1.0918e-04 - val_loss: 1.1138e-06
Epoch 230/512
448/448 - 0s - loss: 1.1222e-04 - val_loss: 1.0765e-06
Epoch 231/512
448/448 - 0s - loss: 1.0658e-04 - val_loss: 1.0164e-06
Epoch 232/512
448/448 - 0s - loss: 1.0293e-04 - val_loss: 1.0387e-06
Epoch 233/512
448/448 - 0s - loss: 1.0479e-04 - val_loss: 1.0548e-06
Epoch 234/512
448/448 - 0s - loss: 1.0371e-04 - val_loss: 9.9952e-07
Epoch 235/512
448/448 - 0s - loss: 9.8973e-05 - val_loss: 9.4456e-07
Epoch 236/512
448/448 - 0s - loss: 9.5752e-05 - val_loss: 9.7375e-07
Epoch 237/512
448/448 - 0s - loss: 9.8689e-05 - val_loss: 9.5549e-07
Epoch 238/512
448/448 - 0s - loss: 9.4037e-05 - val_loss: 9.2448e-07
Epoch 239/512
448/448 - 0s - loss: 9.2290e-05 - val_loss: 9.0787e-07
Epoch 240/512
448/448 - 0s - loss: 9.0862e-05 - val_loss: 8.9541e-07
Epoch 241/512
448/448 - 0s - loss: 9.0235e-05 - val_loss: 8.5082e-07
Epoch 242/512
448/448 - 0s - loss: 8.5938e-05 - val_loss: 8.4814e-07
Epoch 243/512
448/448 - 0s - loss: 8.5980e-05 - val_loss: 8.5663e-07
Epoch 244/512
448/448 - 0s - loss: 8.5530e-05 - val_loss: 8.1645e-07
Epoch 245/512
448/448 - 0s - loss: 8.1247e-05 - val_loss: 7.9806e-07
Epoch 246/512
448/448 - 0s - loss: 8.1050e-05 - val_loss: 7.8924e-07
Epoch 247/512
448/448 - 0s - loss: 7.9082e-05 - val_loss: 7.7947e-07
Epoch 248/512
448/448 - 0s - loss: 7.7999e-05 - val_loss: 7.5682e-07
Epoch 249/512
448/448 - 0s - loss: 7.5917e-05 - val_loss: 7.2850e-07
Epoch 250/512
448/448 - 0s - loss: 7.3676e-05 - val_loss: 7.2682e-07
Epoch 251/512
448/448 - 0s - loss: 7.3175e-05 - val_loss: 7.2324e-07
Epoch 252/512
448/448 - 0s - loss: 7.1992e-05 - val_loss: 6.9222e-07
Epoch 253/512
448/448 - 0s - loss: 6.9478e-05 - val_loss: 6.5883e-07
Epoch 254/512
448/448 - 0s - loss: 6.6570e-05 - val_loss: 6.7467e-07
Epoch 255/512
448/448 - 0s - loss: 6.8255e-05 - val_loss: 6.6327e-07
Epoch 256/512
448/448 - 0s - loss: 6.6096e-05 - val_loss: 6.1160e-07
Epoch 257/512
448/448 - 0s - loss: 6.1427e-05 - val_loss: 6.1778e-07
Epoch 258/512
448/448 - 0s - loss: 6.3225e-05 - val_loss: 6.2721e-07
Epoch 259/512
448/448 - 0s - loss: 6.2532e-05 - val_loss: 5.8041e-07
Epoch 260/512
448/448 - 0s - loss: 5.7887e-05 - val_loss: 5.7202e-07
Epoch 261/512
448/448 - 0s - loss: 5.8398e-05 - val_loss: 5.7504e-07
Epoch 262/512
448/448 - 0s - loss: 5.7353e-05 - val_loss: 5.6144e-07
Epoch 263/512
448/448 - 0s - loss: 5.6385e-05 - val_loss: 5.1688e-07
Epoch 264/512
448/448 - 0s - loss: 5.2187e-05 - val_loss: 5.2116e-07
Epoch 265/512
448/448 - 0s - loss: 5.3663e-05 - val_loss: 5.2728e-07
Epoch 266/512
448/448 - 0s - loss: 5.3009e-05 - val_loss: 4.8536e-07
Epoch 267/512
448/448 - 0s - loss: 4.8857e-05 - val_loss: 4.7048e-07
Epoch 268/512
448/448 - 0s - loss: 4.8520e-05 - val_loss: 4.9318e-07
Epoch 269/512
448/448 - 0s - loss: 4.9855e-05 - val_loss: 4.6959e-07
Epoch 270/512
448/448 - 0s - loss: 4.6069e-05 - val_loss: 4.4652e-07
Epoch 271/512
448/448 - 0s - loss: 4.5277e-05 - val_loss: 4.4254e-07
Epoch 272/512
448/448 - 0s - loss: 4.4898e-05 - val_loss: 4.2866e-07
Epoch 273/512
448/448 - 0s - loss: 4.2974e-05 - val_loss: 4.2024e-07
Epoch 274/512
448/448 - 0s - loss: 4.2463e-05 - val_loss: 4.0499e-07
Epoch 275/512
448/448 - 0s - loss: 4.0941e-05 - val_loss: 3.8998e-07
Epoch 276/512
448/448 - 0s - loss: 3.9520e-05 - val_loss: 3.9200e-07
Epoch 277/512
448/448 - 0s - loss: 3.9783e-05 - val_loss: 3.7388e-07
Epoch 278/512
448/448 - 0s - loss: 3.7326e-05 - val_loss: 3.6330e-07
Epoch 279/512
448/448 - 0s - loss: 3.6925e-05 - val_loss: 3.5847e-07
Epoch 280/512
448/448 - 0s - loss: 3.6265e-05 - val_loss: 3.4050e-07
Epoch 281/512
448/448 - 0s - loss: 3.4479e-05 - val_loss: 3.2749e-07
Epoch 282/512
448/448 - 0s - loss: 3.3560e-05 - val_loss: 3.2820e-07
Epoch 283/512
448/448 - 0s - loss: 3.3600e-05 - val_loss: 3.1138e-07
Epoch 284/512
448/448 - 0s - loss: 3.1448e-05 - val_loss: 3.0165e-07
Epoch 285/512
448/448 - 0s - loss: 3.0775e-05 - val_loss: 3.0666e-07
Epoch 286/512
448/448 - 0s - loss: 3.1060e-05 - val_loss: 2.9423e-07
Epoch 287/512
448/448 - 0s - loss: 2.9340e-05 - val_loss: 2.7381e-07
Epoch 288/512
448/448 - 0s - loss: 2.7948e-05 - val_loss: 2.7001e-07
Epoch 289/512
448/448 - 0s - loss: 2.7986e-05 - val_loss: 2.6237e-07
Epoch 290/512
448/448 - 0s - loss: 2.6666e-05 - val_loss: 2.5914e-07
Epoch 291/512
448/448 - 0s - loss: 2.6356e-05 - val_loss: 2.4891e-07
Epoch 292/512
448/448 - 0s - loss: 2.5210e-05 - val_loss: 2.3918e-07
Epoch 293/512
448/448 - 0s - loss: 2.4516e-05 - val_loss: 2.3191e-07
Epoch 294/512
448/448 - 0s - loss: 2.3680e-05 - val_loss: 2.2918e-07
Epoch 295/512
448/448 - 0s - loss: 2.3225e-05 - val_loss: 2.2604e-07
Epoch 296/512
448/448 - 0s - loss: 2.2844e-05 - val_loss: 2.0708e-07
Epoch 297/512
448/448 - 0s - loss: 2.0965e-05 - val_loss: 2.0703e-07
Epoch 298/512
448/448 - 0s - loss: 2.1272e-05 - val_loss: 2.0868e-07
Epoch 299/512
448/448 - 0s - loss: 2.0892e-05 - val_loss: 1.9384e-07
Epoch 300/512
448/448 - 0s - loss: 1.9461e-05 - val_loss: 1.8204e-07
Epoch 301/512
448/448 - 0s - loss: 1.8698e-05 - val_loss: 1.8710e-07
Epoch 302/512
448/448 - 0s - loss: 1.9120e-05 - val_loss: 1.7827e-07
Epoch 303/512
448/448 - 0s - loss: 1.7841e-05 - val_loss: 1.6536e-07
Epoch 304/512
448/448 - 0s - loss: 1.6972e-05 - val_loss: 1.6433e-07
Epoch 305/512
448/448 - 0s - loss: 1.6999e-05 - val_loss: 1.6278e-07
Epoch 306/512
448/448 - 0s - loss: 1.6382e-05 - val_loss: 1.5312e-07
Epoch 307/512
448/448 - 0s - loss: 1.5544e-05 - val_loss: 1.4752e-07
Epoch 308/512
448/448 - 0s - loss: 1.5087e-05 - val_loss: 1.4697e-07
Epoch 309/512
448/448 - 0s - loss: 1.4882e-05 - val_loss: 1.4171e-07
Epoch 310/512
448/448 - 0s - loss: 1.4172e-05 - val_loss: 1.3448e-07
Epoch 311/512
448/448 - 0s - loss: 1.3604e-05 - val_loss: 1.3073e-07
Epoch 312/512
448/448 - 0s - loss: 1.3235e-05 - val_loss: 1.2952e-07
Epoch 313/512
448/448 - 0s - loss: 1.3031e-05 - val_loss: 1.2200e-07
Epoch 314/512
448/448 - 0s - loss: 1.2313e-05 - val_loss: 1.1303e-07
Epoch 315/512
448/448 - 0s - loss: 1.1623e-05 - val_loss: 1.1338e-07
Epoch 316/512
448/448 - 0s - loss: 1.1770e-05 - val_loss: 1.1016e-07
Epoch 317/512
448/448 - 0s - loss: 1.1127e-05 - val_loss: 1.0511e-07
Epoch 318/512
448/448 - 0s - loss: 1.0657e-05 - val_loss: 1.0285e-07
Epoch 319/512
448/448 - 0s - loss: 1.0457e-05 - val_loss: 9.8420e-08
Epoch 320/512
448/448 - 0s - loss: 9.9622e-06 - val_loss: 9.4898e-08
Epoch 321/512
448/448 - 0s - loss: 9.6690e-06 - val_loss: 9.0608e-08
Epoch 322/512
448/448 - 0s - loss: 9.2292e-06 - val_loss: 8.8869e-08
Epoch 323/512
448/448 - 0s - loss: 9.0691e-06 - val_loss: 8.4609e-08
Epoch 324/512
448/448 - 0s - loss: 8.5598e-06 - val_loss: 8.2219e-08
Epoch 325/512
448/448 - 0s - loss: 8.4134e-06 - val_loss: 7.8247e-08
Epoch 326/512
448/448 - 0s - loss: 7.9624e-06 - val_loss: 7.5887e-08
Epoch 327/512
448/448 - 0s - loss: 7.7751e-06 - val_loss: 7.2800e-08
Epoch 328/512
448/448 - 0s - loss: 7.3993e-06 - val_loss: 7.0449e-08
Epoch 329/512
448/448 - 0s - loss: 7.2024e-06 - val_loss: 6.8092e-08
Epoch 330/512
448/448 - 0s - loss: 6.9222e-06 - val_loss: 6.4753e-08
Epoch 331/512
448/448 - 0s - loss: 6.6282e-06 - val_loss: 6.1676e-08
Epoch 332/512
448/448 - 0s - loss: 6.3817e-06 - val_loss: 5.9250e-08
Epoch 333/512
448/448 - 0s - loss: 6.1060e-06 - val_loss: 5.9029e-08
Epoch 334/512
448/448 - 0s - loss: 6.0336e-06 - val_loss: 5.6106e-08
Epoch 335/512
448/448 - 0s - loss: 5.6812e-06 - val_loss: 5.2860e-08
Epoch 336/512
448/448 - 0s - loss: 5.4276e-06 - val_loss: 5.1444e-08
Epoch 337/512
448/448 - 0s - loss: 5.2978e-06 - val_loss: 5.0335e-08
Epoch 338/512
448/448 - 0s - loss: 5.1221e-06 - val_loss: 4.7395e-08
Epoch 339/512
448/448 - 0s - loss: 4.8353e-06 - val_loss: 4.4565e-08
Epoch 340/512
448/448 - 0s - loss: 4.6364e-06 - val_loss: 4.3722e-08
Epoch 341/512
448/448 - 0s - loss: 4.5207e-06 - val_loss: 4.3017e-08
Epoch 342/512
448/448 - 0s - loss: 4.3880e-06 - val_loss: 4.0829e-08
Epoch 343/512
448/448 - 0s - loss: 4.1597e-06 - val_loss: 3.8524e-08
Epoch 344/512
448/448 - 0s - loss: 3.9332e-06 - val_loss: 3.8048e-08
Epoch 345/512
448/448 - 0s - loss: 3.8948e-06 - val_loss: 3.6572e-08
Epoch 346/512
448/448 - 0s - loss: 3.7103e-06 - val_loss: 3.4227e-08
Epoch 347/512
448/448 - 0s - loss: 3.5158e-06 - val_loss: 3.1830e-08
Epoch 348/512
448/448 - 0s - loss: 3.2871e-06 - val_loss: 3.3089e-08
Epoch 349/512
448/448 - 0s - loss: 3.4013e-06 - val_loss: 3.1606e-08
Epoch 350/512
448/448 - 0s - loss: 3.1303e-06 - val_loss: 2.8716e-08
Epoch 351/512
448/448 - 0s - loss: 2.9387e-06 - val_loss: 2.7779e-08
Epoch 352/512
448/448 - 0s - loss: 2.8716e-06 - val_loss: 2.7691e-08
Epoch 353/512
448/448 - 0s - loss: 2.8359e-06 - val_loss: 2.5637e-08
Epoch 354/512
448/448 - 0s - loss: 2.5984e-06 - val_loss: 2.4463e-08
Epoch 355/512
448/448 - 0s - loss: 2.5321e-06 - val_loss: 2.3897e-08
Epoch 356/512
448/448 - 0s - loss: 2.4622e-06 - val_loss: 2.2899e-08
Epoch 357/512
448/448 - 0s - loss: 2.3430e-06 - val_loss: 2.1496e-08
Epoch 358/512
448/448 - 0s - loss: 2.2145e-06 - val_loss: 2.0679e-08
Epoch 359/512
448/448 - 0s - loss: 2.1332e-06 - val_loss: 2.0571e-08
Epoch 360/512
448/448 - 0s - loss: 2.1080e-06 - val_loss: 1.9069e-08
Epoch 361/512
448/448 - 0s - loss: 1.9437e-06 - val_loss: 1.7752e-08
Epoch 362/512
448/448 - 0s - loss: 1.8452e-06 - val_loss: 1.7776e-08
Epoch 363/512
448/448 - 0s - loss: 1.8463e-06 - val_loss: 1.7282e-08
Epoch 364/512
448/448 - 0s - loss: 1.7534e-06 - val_loss: 1.5845e-08
Epoch 365/512
448/448 - 0s - loss: 1.6147e-06 - val_loss: 1.5343e-08
Epoch 366/512
448/448 - 0s - loss: 1.6012e-06 - val_loss: 1.5081e-08
Epoch 367/512
448/448 - 0s - loss: 1.5419e-06 - val_loss: 1.4103e-08
Epoch 368/512
448/448 - 0s - loss: 1.4422e-06 - val_loss: 1.3312e-08
Epoch 369/512
448/448 - 0s - loss: 1.3757e-06 - val_loss: 1.3088e-08
Epoch 370/512
448/448 - 0s - loss: 1.3596e-06 - val_loss: 1.2323e-08
Epoch 371/512
448/448 - 0s - loss: 1.2655e-06 - val_loss: 1.1519e-08
Epoch 372/512
448/448 - 0s - loss: 1.1924e-06 - val_loss: 1.1511e-08
Epoch 373/512
448/448 - 0s - loss: 1.1933e-06 - val_loss: 1.0986e-08
Epoch 374/512
448/448 - 0s - loss: 1.1199e-06 - val_loss: 1.0150e-08
Epoch 375/512
448/448 - 0s - loss: 1.0466e-06 - val_loss: 9.8891e-09
Epoch 376/512
448/448 - 0s - loss: 1.0291e-06 - val_loss: 9.5533e-09
Epoch 377/512
448/448 - 0s - loss: 9.7957e-07 - val_loss: 9.0377e-09
Epoch 378/512
448/448 - 0s - loss: 9.3610e-07 - val_loss: 8.2816e-09
Epoch 379/512
448/448 - 0s - loss: 8.6257e-07 - val_loss: 8.4396e-09
Epoch 380/512
448/448 - 0s - loss: 8.7911e-07 - val_loss: 8.1950e-09
Epoch 381/512
448/448 - 0s - loss: 8.2754e-07 - val_loss: 7.2422e-09
Epoch 382/512
448/448 - 0s - loss: 7.4811e-07 - val_loss: 7.0459e-09
Epoch 383/512
448/448 - 0s - loss: 7.4557e-07 - val_loss: 7.0663e-09
Epoch 384/512
448/448 - 0s - loss: 7.2444e-07 - val_loss: 6.6466e-09
Epoch 385/512
448/448 - 0s - loss: 6.7795e-07 - val_loss: 6.1216e-09
Epoch 386/512
448/448 - 0s - loss: 6.3302e-07 - val_loss: 6.0431e-09
Epoch 387/512
448/448 - 0s - loss: 6.2854e-07 - val_loss: 5.8920e-09
Epoch 388/512
448/448 - 0s - loss: 6.0053e-07 - val_loss: 5.3533e-09
Epoch 389/512
448/448 - 0s - loss: 5.5191e-07 - val_loss: 5.0819e-09
Epoch 390/512
448/448 - 0s - loss: 5.3411e-07 - val_loss: 5.0723e-09
Epoch 391/512
448/448 - 0s - loss: 5.2329e-07 - val_loss: 4.8756e-09
Epoch 392/512
448/448 - 0s - loss: 4.9494e-07 - val_loss: 4.4641e-09
Epoch 393/512
448/448 - 0s - loss: 4.5854e-07 - val_loss: 4.3116e-09
Epoch 394/512
448/448 - 0s - loss: 4.5170e-07 - val_loss: 4.1362e-09
Epoch 395/512
448/448 - 0s - loss: 4.2776e-07 - val_loss: 3.8792e-09
Epoch 396/512
448/448 - 0s - loss: 4.0347e-07 - val_loss: 3.7314e-09
Epoch 397/512
448/448 - 0s - loss: 3.8925e-07 - val_loss: 3.5753e-09
Epoch 398/512
448/448 - 0s - loss: 3.6867e-07 - val_loss: 3.4581e-09
Epoch 399/512
448/448 - 0s - loss: 3.5780e-07 - val_loss: 3.2468e-09
Epoch 400/512
448/448 - 0s - loss: 3.3492e-07 - val_loss: 3.0414e-09
Epoch 401/512
448/448 - 0s - loss: 3.1717e-07 - val_loss: 3.0046e-09
Epoch 402/512
448/448 - 0s - loss: 3.1158e-07 - val_loss: 2.8908e-09
Epoch 403/512
448/448 - 0s - loss: 2.9520e-07 - val_loss: 2.7073e-09
Epoch 404/512
448/448 - 0s - loss: 2.7774e-07 - val_loss: 2.5101e-09
Epoch 405/512
448/448 - 0s - loss: 2.6177e-07 - val_loss: 2.4421e-09
Epoch 406/512
448/448 - 0s - loss: 2.5686e-07 - val_loss: 2.3466e-09
Epoch 407/512
448/448 - 0s - loss: 2.4199e-07 - val_loss: 2.1894e-09
Epoch 408/512
448/448 - 0s - loss: 2.2806e-07 - val_loss: 2.0816e-09
Epoch 409/512
448/448 - 0s - loss: 2.1986e-07 - val_loss: 1.9992e-09
Epoch 410/512
448/448 - 0s - loss: 2.0823e-07 - val_loss: 1.9270e-09
Epoch 411/512
448/448 - 0s - loss: 2.0074e-07 - val_loss: 1.8278e-09
Epoch 412/512
448/448 - 0s - loss: 1.8909e-07 - val_loss: 1.7458e-09
Epoch 413/512
448/448 - 0s - loss: 1.8225e-07 - val_loss: 1.6453e-09
Epoch 414/512
448/448 - 0s - loss: 1.7054e-07 - val_loss: 1.5760e-09
Epoch 415/512
448/448 - 0s - loss: 1.6439e-07 - val_loss: 1.5147e-09
Epoch 416/512
448/448 - 0s - loss: 1.5850e-07 - val_loss: 1.4033e-09
Epoch 417/512
448/448 - 0s - loss: 1.4537e-07 - val_loss: 1.3795e-09
Epoch 418/512
448/448 - 0s - loss: 1.4426e-07 - val_loss: 1.3529e-09
Epoch 419/512
448/448 - 0s - loss: 1.3886e-07 - val_loss: 1.2004e-09
Epoch 420/512
448/448 - 0s - loss: 1.2418e-07 - val_loss: 1.1362e-09
Epoch 421/512
448/448 - 0s - loss: 1.2108e-07 - val_loss: 1.1601e-09
Epoch 422/512
448/448 - 0s - loss: 1.2101e-07 - val_loss: 1.0883e-09
Epoch 423/512
448/448 - 0s - loss: 1.1124e-07 - val_loss: 9.6700e-10
Epoch 424/512
448/448 - 0s - loss: 1.0144e-07 - val_loss: 9.7501e-10
Epoch 425/512
448/448 - 0s - loss: 1.0393e-07 - val_loss: 9.4553e-10
Epoch 426/512
448/448 - 0s - loss: 9.6738e-08 - val_loss: 8.8060e-10
Epoch 427/512
448/448 - 0s - loss: 9.0853e-08 - val_loss: 8.2893e-10
Epoch 428/512
448/448 - 0s - loss: 8.6560e-08 - val_loss: 7.9840e-10
Epoch 429/512
448/448 - 0s - loss: 8.3033e-08 - val_loss: 7.7880e-10
Epoch 430/512
448/448 - 0s - loss: 8.1016e-08 - val_loss: 7.0535e-10
Epoch 431/512
448/448 - 0s - loss: 7.2589e-08 - val_loss: 6.9419e-10
Epoch 432/512
448/448 - 0s - loss: 7.3224e-08 - val_loss: 6.5131e-10
Epoch 433/512
448/448 - 0s - loss: 6.7282e-08 - val_loss: 6.2042e-10
Epoch 434/512
448/448 - 0s - loss: 6.5059e-08 - val_loss: 5.9455e-10
Epoch 435/512
448/448 - 0s - loss: 6.2311e-08 - val_loss: 5.5464e-10
Epoch 436/512
448/448 - 0s - loss: 5.7654e-08 - val_loss: 5.3839e-10
Epoch 437/512
448/448 - 0s - loss: 5.6516e-08 - val_loss: 5.1498e-10
Epoch 438/512
448/448 - 0s - loss: 5.3300e-08 - val_loss: 4.8886e-10
Epoch 439/512
448/448 - 0s - loss: 5.0844e-08 - val_loss: 4.6276e-10
Epoch 440/512
448/448 - 0s - loss: 4.8193e-08 - val_loss: 4.3585e-10
Epoch 441/512
448/448 - 0s - loss: 4.5644e-08 - val_loss: 4.1765e-10
Epoch 442/512
448/448 - 0s - loss: 4.3704e-08 - val_loss: 4.0703e-10
Epoch 443/512
448/448 - 0s - loss: 4.2502e-08 - val_loss: 3.7176e-10
Epoch 444/512
448/448 - 0s - loss: 3.8314e-08 - val_loss: 3.6802e-10
Epoch 445/512
448/448 - 0s - loss: 3.8722e-08 - val_loss: 3.5094e-10
Epoch 446/512
448/448 - 0s - loss: 3.6317e-08 - val_loss: 3.1830e-10
Epoch 447/512
448/448 - 0s - loss: 3.2911e-08 - val_loss: 3.1502e-10
Epoch 448/512
448/448 - 0s - loss: 3.3169e-08 - val_loss: 3.0751e-10
Epoch 449/512
448/448 - 0s - loss: 3.1663e-08 - val_loss: 2.8310e-10
Epoch 450/512
448/448 - 0s - loss: 2.9122e-08 - val_loss: 2.6629e-10
Epoch 451/512
448/448 - 0s - loss: 2.8031e-08 - val_loss: 2.5489e-10
Epoch 452/512
448/448 - 0s - loss: 2.6722e-08 - val_loss: 2.4459e-10
Epoch 453/512
448/448 - 0s - loss: 2.5475e-08 - val_loss: 2.3689e-10
Epoch 454/512
448/448 - 0s - loss: 2.4618e-08 - val_loss: 2.2608e-10
Epoch 455/512
448/448 - 0s - loss: 2.3411e-08 - val_loss: 2.0546e-10
Epoch 456/512
448/448 - 0s - loss: 2.1346e-08 - val_loss: 1.9851e-10
Epoch 457/512
448/448 - 0s - loss: 2.0930e-08 - val_loss: 1.9949e-10
Epoch 458/512
448/448 - 0s - loss: 2.0726e-08 - val_loss: 1.8539e-10
Epoch 459/512
448/448 - 0s - loss: 1.9013e-08 - val_loss: 1.6830e-10
Epoch 460/512
448/448 - 0s - loss: 1.7529e-08 - val_loss: 1.6716e-10
Epoch 461/512
448/448 - 0s - loss: 1.7628e-08 - val_loss: 1.6450e-10
Epoch 462/512
448/448 - 0s - loss: 1.6951e-08 - val_loss: 1.4837e-10
Epoch 463/512
448/448 - 0s - loss: 1.5341e-08 - val_loss: 1.3955e-10
Epoch 464/512
448/448 - 0s - loss: 1.4777e-08 - val_loss: 1.4067e-10
Epoch 465/512
448/448 - 0s - loss: 1.4782e-08 - val_loss: 1.3179e-10
Epoch 466/512
448/448 - 0s - loss: 1.3508e-08 - val_loss: 1.2389e-10
Epoch 467/512
448/448 - 0s - loss: 1.2799e-08 - val_loss: 1.2230e-10
Epoch 468/512
448/448 - 0s - loss: 1.2618e-08 - val_loss: 1.1647e-10
Epoch 469/512
448/448 - 0s - loss: 1.1984e-08 - val_loss: 1.0839e-10
Epoch 470/512
448/448 - 0s - loss: 1.1226e-08 - val_loss: 1.0106e-10
Epoch 471/512
448/448 - 0s - loss: 1.0541e-08 - val_loss: 9.9268e-11
Epoch 472/512
448/448 - 0s - loss: 1.0319e-08 - val_loss: 9.8585e-11
Epoch 473/512
448/448 - 0s - loss: 1.0207e-08 - val_loss: 9.0710e-11
Epoch 474/512
448/448 - 0s - loss: 9.2658e-09 - val_loss: 8.3990e-11
Epoch 475/512
448/448 - 0s - loss: 8.7691e-09 - val_loss: 8.1785e-11
Epoch 476/512
448/448 - 0s - loss: 8.5074e-09 - val_loss: 8.2871e-11
Epoch 477/512
448/448 - 0s - loss: 8.5797e-09 - val_loss: 7.6744e-11
Epoch 478/512
448/448 - 0s - loss: 7.8230e-09 - val_loss: 7.0447e-11
Epoch 479/512
448/448 - 0s - loss: 7.2863e-09 - val_loss: 6.7609e-11
Epoch 480/512
448/448 - 0s - loss: 7.1151e-09 - val_loss: 6.6932e-11
Epoch 481/512
448/448 - 0s - loss: 6.9562e-09 - val_loss: 6.4399e-11
Epoch 482/512
448/448 - 0s - loss: 6.6146e-09 - val_loss: 6.1208e-11
Epoch 483/512
448/448 - 0s - loss: 6.3036e-09 - val_loss: 5.7883e-11
Epoch 484/512
448/448 - 0s - loss: 5.9744e-09 - val_loss: 5.5401e-11
Epoch 485/512
448/448 - 0s - loss: 5.7248e-09 - val_loss: 5.3207e-11
Epoch 486/512
448/448 - 0s - loss: 5.5279e-09 - val_loss: 5.1238e-11
Epoch 487/512
448/448 - 0s - loss: 5.2854e-09 - val_loss: 4.9386e-11
Epoch 488/512
448/448 - 0s - loss: 5.0910e-09 - val_loss: 4.7640e-11
Epoch 489/512
448/448 - 0s - loss: 4.9101e-09 - val_loss: 4.5223e-11
Epoch 490/512
448/448 - 0s - loss: 4.6167e-09 - val_loss: 4.3735e-11
Epoch 491/512
448/448 - 0s - loss: 4.4784e-09 - val_loss: 4.2863e-11
Epoch 492/512
448/448 - 0s - loss: 4.3868e-09 - val_loss: 4.0730e-11
Epoch 493/512
448/448 - 0s - loss: 4.1482e-09 - val_loss: 3.7752e-11
Epoch 494/512
448/448 - 0s - loss: 3.8291e-09 - val_loss: 3.6941e-11
Epoch 495/512
448/448 - 0s - loss: 3.8183e-09 - val_loss: 3.6439e-11
Epoch 496/512
448/448 - 0s - loss: 3.7364e-09 - val_loss: 3.4164e-11
Epoch 497/512
448/448 - 0s - loss: 3.4907e-09 - val_loss: 3.2038e-11
Epoch 498/512
448/448 - 0s - loss: 3.2995e-09 - val_loss: 3.0823e-11
Epoch 499/512
448/448 - 0s - loss: 3.2226e-09 - val_loss: 3.0497e-11
Epoch 500/512
448/448 - 0s - loss: 3.1300e-09 - val_loss: 3.0018e-11
Epoch 501/512
448/448 - 0s - loss: 3.0525e-09 - val_loss: 2.8751e-11
Epoch 502/512
448/448 - 0s - loss: 2.9288e-09 - val_loss: 2.6715e-11
Epoch 503/512
448/448 - 0s - loss: 2.7117e-09 - val_loss: 2.5653e-11
Epoch 504/512
448/448 - 0s - loss: 2.6395e-09 - val_loss: 2.5609e-11
Epoch 505/512
448/448 - 0s - loss: 2.6444e-09 - val_loss: 2.4673e-11
Epoch 506/512
448/448 - 0s - loss: 2.5392e-09 - val_loss: 2.3538e-11
Epoch 507/512
448/448 - 0s - loss: 2.3892e-09 - val_loss: 2.2231e-11
Epoch 508/512
448/448 - 0s - loss: 2.2683e-09 - val_loss: 2.1876e-11
Epoch 509/512
448/448 - 0s - loss: 2.2499e-09 - val_loss: 2.1488e-11
Epoch 510/512
448/448 - 0s - loss: 2.1895e-09 - val_loss: 2.0678e-11
Epoch 511/512
448/448 - 0s - loss: 2.1111e-09 - val_loss: 1.9609e-11
Epoch 512/512
448/448 - 0s - loss: 2.0086e-09 - val_loss: 1.8438e-11
2024-04-18 19:34:21.237261: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6006e-09 - val_loss: 1.3663e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3937e-09 - val_loss: 1.4181e-09
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5421e-09 - val_loss: 1.6618e-09
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7727e-09 - val_loss: 1.7682e-09
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7868e-09 - val_loss: 1.5932e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5656e-09 - val_loss: 1.3628e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3658e-09 - val_loss: 1.2769e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3194e-09 - val_loss: 1.3403e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3993e-09 - val_loss: 1.3717e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4146e-09 - val_loss: 1.3405e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3468e-09 - val_loss: 1.2542e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2649e-09 - val_loss: 1.1670e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1838e-09 - val_loss: 1.1264e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1575e-09 - val_loss: 1.1199e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1524e-09 - val_loss: 1.1175e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1351e-09 - val_loss: 1.0810e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0997e-09 - val_loss: 1.0405e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0636e-09 - val_loss: 1.0025e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0139e-09 - val_loss: 9.7372e-10
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.9351e-10 - val_loss: 9.5744e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.7829e-10 - val_loss: 9.4675e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.5660e-10 - val_loss: 9.1522e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.1962e-10 - val_loss: 8.7790e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.9205e-10 - val_loss: 8.5729e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.6901e-10 - val_loss: 8.2533e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.3933e-10 - val_loss: 7.9284e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.0925e-10 - val_loss: 7.7809e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.8909e-10 - val_loss: 7.6951e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.8935e-10 - val_loss: 7.5959e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.7106e-10 - val_loss: 7.4102e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.5309e-10 - val_loss: 7.1494e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.2404e-10 - val_loss: 6.8699e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.9706e-10 - val_loss: 6.6127e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.7238e-10 - val_loss: 6.5910e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6884e-10 - val_loss: 6.4606e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6128e-10 - val_loss: 6.4555e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.5126e-10 - val_loss: 6.2760e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.3402e-10 - val_loss: 6.0164e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.0918e-10 - val_loss: 5.8522e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.9869e-10 - val_loss: 5.7157e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.7933e-10 - val_loss: 5.5601e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.6238e-10 - val_loss: 5.4190e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.4929e-10 - val_loss: 5.3144e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3279e-10 - val_loss: 5.1764e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3197e-10 - val_loss: 5.2083e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3157e-10 - val_loss: 5.1013e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.1247e-10 - val_loss: 4.9683e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.9550e-10 - val_loss: 4.7100e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.7254e-10 - val_loss: 4.5860e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.6828e-10 - val_loss: 4.5556e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.6269e-10 - val_loss: 4.5216e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.5488e-10 - val_loss: 4.3720e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.4620e-10 - val_loss: 4.2767e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.3314e-10 - val_loss: 4.2456e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.2627e-10 - val_loss: 4.1729e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.2322e-10 - val_loss: 4.0914e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.1274e-10 - val_loss: 3.9605e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.0145e-10 - val_loss: 3.9422e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.0233e-10 - val_loss: 3.9328e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.9854e-10 - val_loss: 3.8316e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.8374e-10 - val_loss: 3.6483e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.6862e-10 - val_loss: 3.4901e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5520e-10 - val_loss: 3.4407e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4862e-10 - val_loss: 3.4752e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5633e-10 - val_loss: 3.5004e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5076e-10 - val_loss: 3.4569e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5153e-10 - val_loss: 3.4052e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.4220e-10 - val_loss: 3.3334e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.3044e-10 - val_loss: 3.1296e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.1815e-10 - val_loss: 3.0744e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0822e-10 - val_loss: 2.9687e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0380e-10 - val_loss: 2.9978e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0223e-10 - val_loss: 2.9285e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9781e-10 - val_loss: 2.9197e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9570e-10 - val_loss: 2.9134e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9760e-10 - val_loss: 2.8732e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8455e-10 - val_loss: 2.7049e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6964e-10 - val_loss: 2.5936e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6212e-10 - val_loss: 2.6189e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6403e-10 - val_loss: 2.6016e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6513e-10 - val_loss: 2.6743e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7499e-10 - val_loss: 2.6315e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6362e-10 - val_loss: 2.5574e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.5954e-10 - val_loss: 2.4714e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.5012e-10 - val_loss: 2.4245e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4664e-10 - val_loss: 2.4330e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.4429e-10 - val_loss: 2.3828e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.3964e-10 - val_loss: 2.2831e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2920e-10 - val_loss: 2.2424e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2897e-10 - val_loss: 2.2888e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2982e-10 - val_loss: 2.2505e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2641e-10 - val_loss: 2.1986e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1858e-10 - val_loss: 2.1155e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1473e-10 - val_loss: 2.0958e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1133e-10 - val_loss: 2.0618e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.0964e-10 - val_loss: 2.0591e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0811e-10 - val_loss: 2.0839e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.0636e-10 - val_loss: 1.9533e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9839e-10 - val_loss: 1.9575e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9687e-10 - val_loss: 1.8475e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.8517e-10 - val_loss: 1.8462e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8866e-10 - val_loss: 1.8811e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.8618e-10 - val_loss: 1.7781e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7951e-10 - val_loss: 1.7664e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8244e-10 - val_loss: 1.8452e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8398e-10 - val_loss: 1.7683e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7943e-10 - val_loss: 1.7948e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.8068e-10 - val_loss: 1.7444e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7142e-10 - val_loss: 1.6099e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6229e-10 - val_loss: 1.6431e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6751e-10 - val_loss: 1.5908e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6004e-10 - val_loss: 1.5662e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6014e-10 - val_loss: 1.6404e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6716e-10 - val_loss: 1.6261e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6241e-10 - val_loss: 1.5359e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5351e-10 - val_loss: 1.5225e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5527e-10 - val_loss: 1.5221e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5393e-10 - val_loss: 1.4955e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5062e-10 - val_loss: 1.4616e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4936e-10 - val_loss: 1.5341e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5031e-10 - val_loss: 1.4353e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4509e-10 - val_loss: 1.4217e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4330e-10 - val_loss: 1.4654e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4960e-10 - val_loss: 1.4376e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4316e-10 - val_loss: 1.3907e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3794e-10 - val_loss: 1.3501e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3859e-10 - val_loss: 1.3997e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3913e-10 - val_loss: 1.3244e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3131e-10 - val_loss: 1.2717e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2821e-10 - val_loss: 1.3070e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3329e-10 - val_loss: 1.3221e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3125e-10 - val_loss: 1.2721e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2740e-10 - val_loss: 1.2346e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2428e-10 - val_loss: 1.2157e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2237e-10 - val_loss: 1.2068e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2191e-10 - val_loss: 1.1847e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1925e-10 - val_loss: 1.2023e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2133e-10 - val_loss: 1.2156e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2394e-10 - val_loss: 1.2387e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2493e-10 - val_loss: 1.2140e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2126e-10 - val_loss: 1.1741e-10
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1699e-10 - val_loss: 1.1213e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1298e-10 - val_loss: 1.1099e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1130e-10 - val_loss: 1.0799e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0807e-10 - val_loss: 1.0553e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0752e-10 - val_loss: 1.0856e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1035e-10 - val_loss: 1.1072e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1123e-10 - val_loss: 1.0811e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0874e-10 - val_loss: 1.0582e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0596e-10 - val_loss: 1.0307e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0157e-10 - val_loss: 9.7681e-11
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.4967e-11 - val_loss: 8.6629e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7388e-11 - val_loss: 8.8938e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1323e-11 - val_loss: 9.4750e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6892e-11 - val_loss: 9.7950e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8802e-11 - val_loss: 9.7540e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8647e-11 - val_loss: 9.7527e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7690e-11 - val_loss: 9.6750e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7412e-11 - val_loss: 9.6101e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6548e-11 - val_loss: 9.3701e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3243e-11 - val_loss: 8.9292e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.8818e-11 - val_loss: 8.3305e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.0867e-11 - val_loss: 7.3420e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4297e-11 - val_loss: 7.7018e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8290e-11 - val_loss: 7.9726e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2449e-11 - val_loss: 8.4699e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7052e-11 - val_loss: 9.0736e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9748e-11 - val_loss: 8.5113e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4032e-11 - val_loss: 7.8334e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9899e-11 - val_loss: 8.0190e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0043e-11 - val_loss: 8.0435e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1653e-11 - val_loss: 8.0040e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0004e-11 - val_loss: 7.8107e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.5862e-11 - val_loss: 7.2403e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.3201e-11 - val_loss: 7.1325e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2129e-11 - val_loss: 7.4201e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6818e-11 - val_loss: 7.9866e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2348e-11 - val_loss: 8.4227e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4797e-11 - val_loss: 8.3313e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3064e-11 - val_loss: 7.8846e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.5413e-11 - val_loss: 6.7678e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.7489e-11 - val_loss: 6.4949e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5562e-11 - val_loss: 6.8027e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0393e-11 - val_loss: 7.2033e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4099e-11 - val_loss: 7.4872e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7906e-11 - val_loss: 7.9176e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8889e-11 - val_loss: 7.3183e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1573e-11 - val_loss: 6.8395e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8602e-11 - val_loss: 6.5808e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7312e-11 - val_loss: 6.9091e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1604e-11 - val_loss: 7.4234e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5384e-11 - val_loss: 7.7768e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9280e-11 - val_loss: 7.9666e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0493e-11 - val_loss: 7.5070e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.2649e-11 - val_loss: 6.4213e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.1325e-11 - val_loss: 5.5482e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.5270e-11 - val_loss: 5.2989e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3075e-11 - val_loss: 5.4810e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7368e-11 - val_loss: 6.4463e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7371e-11 - val_loss: 7.0605e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2693e-11 - val_loss: 7.3475e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3440e-11 - val_loss: 7.0392e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8898e-11 - val_loss: 6.2386e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9845e-11 - val_loss: 5.4318e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.2770e-11 - val_loss: 5.0758e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.0626e-11 - val_loss: 5.0183e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1468e-11 - val_loss: 5.5776e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8456e-11 - val_loss: 6.2100e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4703e-11 - val_loss: 6.7706e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8786e-11 - val_loss: 6.8234e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8254e-11 - val_loss: 6.2579e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9447e-11 - val_loss: 5.3643e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.1645e-11 - val_loss: 4.8732e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.8799e-11 - val_loss: 4.7178e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.7250e-11 - val_loss: 4.6137e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6321e-11 - val_loss: 4.8723e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0383e-11 - val_loss: 5.5428e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8605e-11 - val_loss: 6.2502e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4989e-11 - val_loss: 6.5672e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6097e-11 - val_loss: 6.4671e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1943e-11 - val_loss: 5.6807e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5280e-11 - val_loss: 5.0836e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0125e-11 - val_loss: 4.6744e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.5661e-11 - val_loss: 4.4290e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4361e-11 - val_loss: 4.4526e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4997e-11 - val_loss: 4.5431e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6916e-11 - val_loss: 5.1469e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3642e-11 - val_loss: 5.7679e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9800e-11 - val_loss: 6.2159e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2092e-11 - val_loss: 6.0479e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8442e-11 - val_loss: 5.4067e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2116e-11 - val_loss: 4.6819e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.5263e-11 - val_loss: 4.3553e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.3159e-11 - val_loss: 4.0871e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.0012e-11 - val_loss: 3.9010e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0063e-11 - val_loss: 4.1512e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1826e-11 - val_loss: 4.1280e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1898e-11 - val_loss: 4.2308e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3892e-11 - val_loss: 4.7089e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9930e-11 - val_loss: 5.2384e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3287e-11 - val_loss: 5.5890e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7785e-11 - val_loss: 5.8753e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6448e-11 - val_loss: 5.1687e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0163e-11 - val_loss: 4.4258e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3269e-11 - val_loss: 4.0398e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.9662e-11 - val_loss: 3.7680e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.7290e-11 - val_loss: 3.6371e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5994e-11 - val_loss: 3.5567e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6338e-11 - val_loss: 3.7144e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7551e-11 - val_loss: 3.7803e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8123e-11 - val_loss: 3.8583e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8619e-11 - val_loss: 3.9251e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2096e-11 - val_loss: 4.7543e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8715e-11 - val_loss: 5.0366e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1437e-11 - val_loss: 5.1528e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9667e-11 - val_loss: 4.4081e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2516e-11 - val_loss: 3.9682e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8312e-11 - val_loss: 3.6561e-11
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.6302e-11 - val_loss: 3.5254e-11
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5080e-11 - val_loss: 3.5009e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.4832e-11 - val_loss: 3.3813e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4159e-11 - val_loss: 3.4414e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5089e-11 - val_loss: 3.6249e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6737e-11 - val_loss: 3.5959e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6543e-11 - val_loss: 3.7528e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7116e-11 - val_loss: 3.5470e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5728e-11 - val_loss: 3.7958e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0502e-11 - val_loss: 4.3799e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4901e-11 - val_loss: 4.6914e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6545e-11 - val_loss: 4.3612e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3077e-11 - val_loss: 3.9968e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8838e-11 - val_loss: 3.5728e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5344e-11 - val_loss: 3.3623e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.3235e-11 - val_loss: 3.1358e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.1265e-11 - val_loss: 3.0542e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0903e-11 - val_loss: 3.0202e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0101e-11 - val_loss: 3.0332e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1016e-11 - val_loss: 3.0885e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1311e-11 - val_loss: 3.2377e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2565e-11 - val_loss: 3.2020e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2179e-11 - val_loss: 3.1415e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0927e-11 - val_loss: 3.0061e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9962e-11 - val_loss: 2.9046e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9222e-11 - val_loss: 2.9539e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0718e-11 - val_loss: 3.5078e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6425e-11 - val_loss: 3.7637e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7013e-11 - val_loss: 3.4693e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3691e-11 - val_loss: 3.1477e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0718e-11 - val_loss: 2.8874e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8657e-11 - val_loss: 2.8081e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8093e-11 - val_loss: 2.7365e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.7925e-11 - val_loss: 2.7108e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6685e-11 - val_loss: 2.6685e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6290e-11 - val_loss: 2.6781e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7484e-11 - val_loss: 2.7647e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8041e-11 - val_loss: 2.8892e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9340e-11 - val_loss: 2.9101e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9220e-11 - val_loss: 2.7847e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7862e-11 - val_loss: 2.8098e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8012e-11 - val_loss: 2.7991e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7552e-11 - val_loss: 2.6801e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.7064e-11 - val_loss: 2.6215e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6737e-11 - val_loss: 2.7515e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.7356e-11 - val_loss: 2.6104e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6449e-11 - val_loss: 2.6094e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.4689e-11 - val_loss: 2.1152e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9991e-11 - val_loss: 1.8783e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7836e-11 - val_loss: 1.5643e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5593e-11 - val_loss: 1.5815e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7013e-11 - val_loss: 1.9271e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9992e-11 - val_loss: 2.1101e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1957e-11 - val_loss: 2.4205e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5060e-11 - val_loss: 2.6085e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6724e-11 - val_loss: 2.6847e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6997e-11 - val_loss: 2.7131e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6408e-11 - val_loss: 2.4511e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4249e-11 - val_loss: 2.3266e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2972e-11 - val_loss: 2.2834e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3413e-11 - val_loss: 2.4204e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4629e-11 - val_loss: 2.4089e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4520e-11 - val_loss: 2.5522e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5872e-11 - val_loss: 2.5958e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6143e-11 - val_loss: 2.5114e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4487e-11 - val_loss: 2.3104e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3373e-11 - val_loss: 2.2908e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2790e-11 - val_loss: 2.2103e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1412e-11 - val_loss: 2.0397e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0736e-11 - val_loss: 2.1158e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1573e-11 - val_loss: 2.1644e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0523e-11 - val_loss: 1.8280e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7549e-11 - val_loss: 1.5702e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5519e-11 - val_loss: 1.4547e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4425e-11 - val_loss: 1.3404e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3553e-11 - val_loss: 1.3041e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3085e-11 - val_loss: 1.3409e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3489e-11 - val_loss: 1.3497e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3352e-11 - val_loss: 1.4380e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5747e-11 - val_loss: 1.8673e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0148e-11 - val_loss: 2.3231e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4222e-11 - val_loss: 2.5888e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6553e-11 - val_loss: 2.7409e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7644e-11 - val_loss: 2.7996e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7675e-11 - val_loss: 2.6332e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5705e-11 - val_loss: 2.4426e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4692e-11 - val_loss: 2.4119e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4656e-11 - val_loss: 2.5969e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6343e-11 - val_loss: 2.6356e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6702e-11 - val_loss: 2.6636e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6539e-11 - val_loss: 2.5030e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4437e-11 - val_loss: 2.3142e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3601e-11 - val_loss: 2.3031e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2584e-11 - val_loss: 2.1705e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1811e-11 - val_loss: 2.2477e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2655e-11 - val_loss: 2.2204e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2599e-11 - val_loss: 2.2990e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2665e-11 - val_loss: 2.2728e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2576e-11 - val_loss: 2.3514e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3963e-11 - val_loss: 2.4517e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4844e-11 - val_loss: 2.4013e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3794e-11 - val_loss: 2.3461e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3455e-11 - val_loss: 2.0998e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9305e-11 - val_loss: 1.5994e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5021e-11 - val_loss: 1.3834e-11
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3266e-11 - val_loss: 1.2319e-11
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2430e-11 - val_loss: 1.1657e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1504e-11 - val_loss: 1.1769e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2115e-11 - val_loss: 1.1770e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1865e-11 - val_loss: 1.2045e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1601e-11 - val_loss: 1.1196e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1161e-11 - val_loss: 1.0406e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0320e-11 - val_loss: 1.0248e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0366e-11 - val_loss: 1.1720e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2869e-11 - val_loss: 1.4375e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5015e-11 - val_loss: 1.6709e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7486e-11 - val_loss: 1.8597e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9611e-11 - val_loss: 2.1695e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3092e-11 - val_loss: 2.3413e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3707e-11 - val_loss: 2.3304e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2827e-11 - val_loss: 2.1188e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1259e-11 - val_loss: 2.1706e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1796e-11 - val_loss: 2.1308e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0290e-11 - val_loss: 1.8044e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7760e-11 - val_loss: 1.8759e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8437e-11 - val_loss: 1.7023e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6662e-11 - val_loss: 1.7764e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7914e-11 - val_loss: 1.8517e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8987e-11 - val_loss: 1.9384e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9741e-11 - val_loss: 1.9605e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9824e-11 - val_loss: 1.9591e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9118e-11 - val_loss: 1.7295e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7323e-11 - val_loss: 1.6950e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7666e-11 - val_loss: 1.8157e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9198e-11 - val_loss: 2.0116e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0230e-11 - val_loss: 1.9597e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9159e-11 - val_loss: 1.8381e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8221e-11 - val_loss: 1.7740e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8207e-11 - val_loss: 1.7754e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7217e-11 - val_loss: 1.5607e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4436e-11 - val_loss: 1.2133e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1307e-11 - val_loss: 9.8877e-12
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.7792e-12 - val_loss: 8.5036e-12
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4460e-12 - val_loss: 8.5773e-12
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4239e-12 - val_loss: 8.9891e-12
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5623e-12 - val_loss: 9.5130e-12
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4635e-12 - val_loss: 9.0091e-12
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2350e-12 - val_loss: 9.0101e-12
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1421e-12 - val_loss: 9.4033e-12
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4810e-12 - val_loss: 9.7629e-12
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5320e-12 - val_loss: 9.7739e-12
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0167e-11 - val_loss: 1.0711e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0259e-11 - val_loss: 9.5025e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3948e-12 - val_loss: 9.9860e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8094e-12 - val_loss: 1.0328e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0591e-11 - val_loss: 1.1134e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1492e-11 - val_loss: 1.2902e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3762e-11 - val_loss: 1.5870e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6568e-11 - val_loss: 1.7642e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8063e-11 - val_loss: 1.9058e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9716e-11 - val_loss: 2.0776e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1299e-11 - val_loss: 2.0607e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9915e-11 - val_loss: 1.9071e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9933e-11 - val_loss: 2.1190e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1508e-11 - val_loss: 2.1450e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1205e-11 - val_loss: 2.0601e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1027e-11 - val_loss: 2.0885e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0357e-11 - val_loss: 1.9507e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9287e-11 - val_loss: 1.8933e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8754e-11 - val_loss: 1.8204e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8034e-11 - val_loss: 1.7348e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6575e-11 - val_loss: 1.6220e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5838e-11 - val_loss: 1.5079e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5596e-11 - val_loss: 1.6519e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6888e-11 - val_loss: 1.7558e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7552e-11 - val_loss: 1.7306e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7054e-11 - val_loss: 1.6078e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5909e-11 - val_loss: 1.6907e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6821e-11 - val_loss: 1.5466e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5479e-11 - val_loss: 1.5119e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5745e-11 - val_loss: 1.5551e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4914e-11 - val_loss: 1.3848e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4642e-11 - val_loss: 1.5363e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5157e-11 - val_loss: 1.5104e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5142e-11 - val_loss: 1.4549e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4760e-11 - val_loss: 1.5189e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4867e-11 - val_loss: 1.4853e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5651e-11 - val_loss: 1.5524e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4866e-11 - val_loss: 1.3139e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2620e-11 - val_loss: 1.1309e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0767e-11 - val_loss: 1.0236e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.7655e-12 - val_loss: 8.2989e-12
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2859e-12 - val_loss: 8.4099e-12
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.0055e-12 - val_loss: 6.7628e-12
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.4499e-12 - val_loss: 6.1326e-12
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2069e-12 - val_loss: 7.1761e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0074e-12 - val_loss: 7.2792e-12
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.9846e-12 - val_loss: 6.1276e-12
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4745e-12 - val_loss: 6.6624e-12
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3491e-12 - val_loss: 6.2620e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6545e-12 - val_loss: 6.6520e-12
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6382e-12 - val_loss: 6.5477e-12
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.1566e-12 - val_loss: 5.8051e-12
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.7739e-12 - val_loss: 5.4140e-12
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3638e-12 - val_loss: 5.3626e-12
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5058e-12 - val_loss: 6.2334e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0186e-12 - val_loss: 5.9137e-12
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.8398e-12 - val_loss: 5.2041e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2807e-12 - val_loss: 5.5191e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9435e-12 - val_loss: 6.5180e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0490e-12 - val_loss: 8.0537e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0580e-12 - val_loss: 8.1621e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2569e-12 - val_loss: 8.9770e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1716e-12 - val_loss: 8.5287e-12
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9974e-12 - val_loss: 6.8424e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3698e-12 - val_loss: 5.7187e-12
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3542e-12 - val_loss: 5.0333e-12
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.6838e-12 - val_loss: 4.2026e-12
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.8280e-12 - val_loss: 3.2906e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4433e-12 - val_loss: 3.3637e-12
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.3498e-12 - val_loss: 3.0842e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1633e-12 - val_loss: 3.3724e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5972e-12 - val_loss: 3.8089e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6458e-12 - val_loss: 3.9925e-12
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2642e-12 - val_loss: 5.1197e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6107e-12 - val_loss: 9.5278e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0681e-11 - val_loss: 1.2484e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2974e-11 - val_loss: 1.4458e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5664e-11 - val_loss: 1.8066e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8744e-11 - val_loss: 1.9808e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0006e-11 - val_loss: 1.9155e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8692e-11 - val_loss: 1.7972e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7639e-11 - val_loss: 1.7117e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6820e-11 - val_loss: 1.6750e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7069e-11 - val_loss: 1.7529e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7181e-11 - val_loss: 1.6868e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6563e-11 - val_loss: 1.5578e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5640e-11 - val_loss: 1.5991e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5796e-11 - val_loss: 1.4823e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5234e-11 - val_loss: 1.5396e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4875e-11 - val_loss: 1.3168e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2908e-11 - val_loss: 1.2062e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1783e-11 - val_loss: 1.2777e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3778e-11 - val_loss: 1.4192e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4901e-11 - val_loss: 1.6086e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6104e-11 - val_loss: 1.5443e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5197e-11 - val_loss: 1.4928e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5182e-11 - val_loss: 1.5451e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5787e-11 - val_loss: 1.5591e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5463e-11 - val_loss: 1.5241e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5706e-11 - val_loss: 1.6765e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6613e-11 - val_loss: 1.6328e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6628e-11 - val_loss: 1.6456e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6179e-11 - val_loss: 1.4985e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0458 - val_loss: 7.6421e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1295e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.2422e-04
Epoch 4/512
448/448 - 0s - loss: 9.2720e-04 - val_loss: 2.2639e-04
Epoch 5/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.3711e-04
Epoch 6/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.3535e-04
Epoch 7/512
448/448 - 0s - loss: 5.3861e-04 - val_loss: 1.1174e-04
Epoch 8/512
448/448 - 0s - loss: 7.5622e-04 - val_loss: 4.9059e-05
Epoch 9/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.1450e-05
Epoch 10/512
448/448 - 0s - loss: 6.2499e-04 - val_loss: 2.0814e-05
Epoch 11/512
448/448 - 0s - loss: 4.9326e-04 - val_loss: 1.1460e-05
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0234e-05
Epoch 13/512
448/448 - 0s - loss: 7.0687e-04 - val_loss: 1.1939e-05
Epoch 14/512
448/448 - 0s - loss: 3.8470e-04 - val_loss: 1.3879e-05
Epoch 15/512
448/448 - 0s - loss: 7.6663e-04 - val_loss: 1.8742e-05
Epoch 16/512
448/448 - 0s - loss: 6.6853e-04 - val_loss: 2.0261e-05
Epoch 17/512
448/448 - 0s - loss: 3.3066e-04 - val_loss: 2.0582e-05
Epoch 18/512
448/448 - 0s - loss: 5.0362e-04 - val_loss: 2.1949e-05
Epoch 19/512
448/448 - 0s - loss: 5.5318e-04 - val_loss: 2.2824e-05
Epoch 20/512
448/448 - 0s - loss: 2.8782e-04 - val_loss: 2.1361e-05
Epoch 21/512
448/448 - 0s - loss: 3.5145e-04 - val_loss: 2.0778e-05
Epoch 22/512
448/448 - 0s - loss: 4.1941e-04 - val_loss: 2.1005e-05
Epoch 23/512
448/448 - 0s - loss: 2.4712e-04 - val_loss: 1.8632e-05
Epoch 24/512
448/448 - 0s - loss: 2.6201e-04 - val_loss: 1.7594e-05
Epoch 25/512
448/448 - 0s - loss: 3.0576e-04 - val_loss: 1.6902e-05
Epoch 26/512
448/448 - 0s - loss: 2.1779e-04 - val_loss: 1.4478e-05
Epoch 27/512
448/448 - 0s - loss: 2.0100e-04 - val_loss: 1.2856e-05
Epoch 28/512
448/448 - 0s - loss: 2.2568e-04 - val_loss: 1.1852e-05
Epoch 29/512
448/448 - 0s - loss: 1.8547e-04 - val_loss: 9.9300e-06
Epoch 30/512
448/448 - 0s - loss: 1.6306e-04 - val_loss: 8.5173e-06
Epoch 31/512
448/448 - 0s - loss: 1.7496e-04 - val_loss: 7.7123e-06
Epoch 32/512
448/448 - 0s - loss: 1.5586e-04 - val_loss: 6.8888e-06
Epoch 33/512
448/448 - 0s - loss: 1.3892e-04 - val_loss: 6.3319e-06
Epoch 34/512
448/448 - 0s - loss: 1.3918e-04 - val_loss: 6.0566e-06
Epoch 35/512
448/448 - 0s - loss: 1.3295e-04 - val_loss: 5.9353e-06
Epoch 36/512
448/448 - 0s - loss: 1.2276e-04 - val_loss: 5.8788e-06
Epoch 37/512
448/448 - 0s - loss: 1.1751e-04 - val_loss: 5.8072e-06
Epoch 38/512
448/448 - 0s - loss: 1.1535e-04 - val_loss: 5.7095e-06
Epoch 39/512
448/448 - 0s - loss: 1.0678e-04 - val_loss: 5.5254e-06
Epoch 40/512
448/448 - 0s - loss: 1.0367e-04 - val_loss: 5.3184e-06
Epoch 41/512
448/448 - 0s - loss: 9.9281e-05 - val_loss: 5.0588e-06
Epoch 42/512
448/448 - 0s - loss: 9.8270e-05 - val_loss: 4.6263e-06
Epoch 43/512
448/448 - 0s - loss: 9.2470e-05 - val_loss: 4.0847e-06
Epoch 44/512
448/448 - 0s - loss: 8.9367e-05 - val_loss: 3.5536e-06
Epoch 45/512
448/448 - 0s - loss: 8.8119e-05 - val_loss: 3.3343e-06
Epoch 46/512
448/448 - 0s - loss: 8.4285e-05 - val_loss: 3.1674e-06
Epoch 47/512
448/448 - 0s - loss: 8.0982e-05 - val_loss: 3.0266e-06
Epoch 48/512
448/448 - 0s - loss: 8.0370e-05 - val_loss: 2.7320e-06
Epoch 49/512
448/448 - 0s - loss: 7.7993e-05 - val_loss: 2.4581e-06
Epoch 50/512
448/448 - 0s - loss: 7.4677e-05 - val_loss: 2.3405e-06
Epoch 51/512
448/448 - 0s - loss: 7.3304e-05 - val_loss: 2.1862e-06
Epoch 52/512
448/448 - 0s - loss: 7.1400e-05 - val_loss: 1.9946e-06
Epoch 53/512
448/448 - 0s - loss: 6.9551e-05 - val_loss: 1.8713e-06
Epoch 54/512
448/448 - 0s - loss: 6.7504e-05 - val_loss: 1.7798e-06
Epoch 55/512
448/448 - 0s - loss: 6.7460e-05 - val_loss: 1.6296e-06
Epoch 56/512
448/448 - 0s - loss: 6.4240e-05 - val_loss: 1.5193e-06
Epoch 57/512
448/448 - 0s - loss: 6.3565e-05 - val_loss: 1.4248e-06
Epoch 58/512
448/448 - 0s - loss: 6.1945e-05 - val_loss: 1.3166e-06
Epoch 59/512
448/448 - 0s - loss: 6.0088e-05 - val_loss: 1.2410e-06
Epoch 60/512
448/448 - 0s - loss: 5.9422e-05 - val_loss: 1.1698e-06
Epoch 61/512
448/448 - 0s - loss: 5.8131e-05 - val_loss: 1.0837e-06
Epoch 62/512
448/448 - 0s - loss: 5.6144e-05 - val_loss: 1.0128e-06
Epoch 63/512
448/448 - 0s - loss: 5.5578e-05 - val_loss: 9.5465e-07
Epoch 64/512
448/448 - 0s - loss: 5.5019e-05 - val_loss: 8.8532e-07
Epoch 65/512
448/448 - 0s - loss: 5.2890e-05 - val_loss: 8.4388e-07
Epoch 66/512
448/448 - 0s - loss: 5.2179e-05 - val_loss: 7.8789e-07
Epoch 67/512
448/448 - 0s - loss: 5.1370e-05 - val_loss: 7.4214e-07
Epoch 68/512
448/448 - 0s - loss: 5.0659e-05 - val_loss: 6.8730e-07
Epoch 69/512
448/448 - 0s - loss: 4.8978e-05 - val_loss: 6.5751e-07
Epoch 70/512
448/448 - 0s - loss: 4.8610e-05 - val_loss: 6.2555e-07
Epoch 71/512
448/448 - 0s - loss: 4.7570e-05 - val_loss: 6.0104e-07
Epoch 72/512
448/448 - 0s - loss: 4.6143e-05 - val_loss: 5.7554e-07
Epoch 73/512
448/448 - 0s - loss: 4.6451e-05 - val_loss: 5.3483e-07
Epoch 74/512
448/448 - 0s - loss: 4.5075e-05 - val_loss: 5.0959e-07
Epoch 75/512
448/448 - 0s - loss: 4.3867e-05 - val_loss: 4.8814e-07
Epoch 76/512
448/448 - 0s - loss: 4.3173e-05 - val_loss: 4.7082e-07
Epoch 77/512
448/448 - 0s - loss: 4.3792e-05 - val_loss: 4.2919e-07
Epoch 78/512
448/448 - 0s - loss: 4.1559e-05 - val_loss: 4.0479e-07
Epoch 79/512
448/448 - 0s - loss: 4.0967e-05 - val_loss: 3.9637e-07
Epoch 80/512
448/448 - 0s - loss: 4.1036e-05 - val_loss: 3.8350e-07
Epoch 81/512
448/448 - 0s - loss: 3.9843e-05 - val_loss: 3.6745e-07
Epoch 82/512
448/448 - 0s - loss: 3.8808e-05 - val_loss: 3.5068e-07
Epoch 83/512
448/448 - 0s - loss: 3.8730e-05 - val_loss: 3.4251e-07
Epoch 84/512
448/448 - 0s - loss: 3.8604e-05 - val_loss: 3.2012e-07
Epoch 85/512
448/448 - 0s - loss: 3.6640e-05 - val_loss: 3.1711e-07
Epoch 86/512
448/448 - 0s - loss: 3.6702e-05 - val_loss: 3.1020e-07
Epoch 87/512
448/448 - 0s - loss: 3.6184e-05 - val_loss: 2.9868e-07
Epoch 88/512
448/448 - 0s - loss: 3.5329e-05 - val_loss: 2.8585e-07
Epoch 89/512
448/448 - 0s - loss: 3.4930e-05 - val_loss: 2.7471e-07
Epoch 90/512
448/448 - 0s - loss: 3.4449e-05 - val_loss: 2.6535e-07
Epoch 91/512
448/448 - 0s - loss: 3.4010e-05 - val_loss: 2.5447e-07
Epoch 92/512
448/448 - 0s - loss: 3.3022e-05 - val_loss: 2.4772e-07
Epoch 93/512
448/448 - 0s - loss: 3.2861e-05 - val_loss: 2.3726e-07
Epoch 94/512
448/448 - 0s - loss: 3.2024e-05 - val_loss: 2.3027e-07
Epoch 95/512
448/448 - 0s - loss: 3.1689e-05 - val_loss: 2.2750e-07
Epoch 96/512
448/448 - 0s - loss: 3.0979e-05 - val_loss: 2.2192e-07
Epoch 97/512
448/448 - 0s - loss: 3.0553e-05 - val_loss: 2.1728e-07
Epoch 98/512
448/448 - 0s - loss: 3.0325e-05 - val_loss: 2.0788e-07
Epoch 99/512
448/448 - 0s - loss: 2.9305e-05 - val_loss: 2.0179e-07
Epoch 100/512
448/448 - 0s - loss: 2.9053e-05 - val_loss: 2.0007e-07
Epoch 101/512
448/448 - 0s - loss: 2.8604e-05 - val_loss: 1.9559e-07
Epoch 102/512
448/448 - 0s - loss: 2.7996e-05 - val_loss: 1.9048e-07
Epoch 103/512
448/448 - 0s - loss: 2.7626e-05 - val_loss: 1.8645e-07
Epoch 104/512
448/448 - 0s - loss: 2.7076e-05 - val_loss: 1.8318e-07
Epoch 105/512
448/448 - 0s - loss: 2.6673e-05 - val_loss: 1.7842e-07
Epoch 106/512
448/448 - 0s - loss: 2.6046e-05 - val_loss: 1.7633e-07
Epoch 107/512
448/448 - 0s - loss: 2.5866e-05 - val_loss: 1.7084e-07
Epoch 108/512
448/448 - 0s - loss: 2.5113e-05 - val_loss: 1.6748e-07
Epoch 109/512
448/448 - 0s - loss: 2.4890e-05 - val_loss: 1.6475e-07
Epoch 110/512
448/448 - 0s - loss: 2.4535e-05 - val_loss: 1.5960e-07
Epoch 111/512
448/448 - 0s - loss: 2.3733e-05 - val_loss: 1.5694e-07
Epoch 112/512
448/448 - 0s - loss: 2.3500e-05 - val_loss: 1.5485e-07
Epoch 113/512
448/448 - 0s - loss: 2.3137e-05 - val_loss: 1.5199e-07
Epoch 114/512
448/448 - 0s - loss: 2.2841e-05 - val_loss: 1.4551e-07
Epoch 115/512
448/448 - 0s - loss: 2.1802e-05 - val_loss: 1.4382e-07
Epoch 116/512
448/448 - 0s - loss: 2.1914e-05 - val_loss: 1.4352e-07
Epoch 117/512
448/448 - 0s - loss: 2.1665e-05 - val_loss: 1.3853e-07
Epoch 118/512
448/448 - 0s - loss: 2.0758e-05 - val_loss: 1.3510e-07
Epoch 119/512
448/448 - 0s - loss: 2.0551e-05 - val_loss: 1.3299e-07
Epoch 120/512
448/448 - 0s - loss: 2.0263e-05 - val_loss: 1.3039e-07
Epoch 121/512
448/448 - 0s - loss: 1.9786e-05 - val_loss: 1.2594e-07
Epoch 122/512
448/448 - 0s - loss: 1.9325e-05 - val_loss: 1.2336e-07
Epoch 123/512
448/448 - 0s - loss: 1.9000e-05 - val_loss: 1.2219e-07
Epoch 124/512
448/448 - 0s - loss: 1.8770e-05 - val_loss: 1.1878e-07
Epoch 125/512
448/448 - 0s - loss: 1.8227e-05 - val_loss: 1.1525e-07
Epoch 126/512
448/448 - 0s - loss: 1.7946e-05 - val_loss: 1.1183e-07
Epoch 127/512
448/448 - 0s - loss: 1.7485e-05 - val_loss: 1.0945e-07
Epoch 128/512
448/448 - 0s - loss: 1.7236e-05 - val_loss: 1.0660e-07
Epoch 129/512
448/448 - 0s - loss: 1.6687e-05 - val_loss: 1.0523e-07
Epoch 130/512
448/448 - 0s - loss: 1.6589e-05 - val_loss: 1.0292e-07
Epoch 131/512
448/448 - 0s - loss: 1.6199e-05 - val_loss: 9.8354e-08
Epoch 132/512
448/448 - 0s - loss: 1.5570e-05 - val_loss: 9.7378e-08
Epoch 133/512
448/448 - 0s - loss: 1.5565e-05 - val_loss: 9.5649e-08
Epoch 134/512
448/448 - 0s - loss: 1.5204e-05 - val_loss: 9.2411e-08
Epoch 135/512
448/448 - 0s - loss: 1.4739e-05 - val_loss: 8.8782e-08
Epoch 136/512
448/448 - 0s - loss: 1.4405e-05 - val_loss: 8.7107e-08
Epoch 137/512
448/448 - 0s - loss: 1.4112e-05 - val_loss: 8.5322e-08
Epoch 138/512
448/448 - 0s - loss: 1.3824e-05 - val_loss: 8.3928e-08
Epoch 139/512
448/448 - 0s - loss: 1.3692e-05 - val_loss: 7.9861e-08
Epoch 140/512
448/448 - 0s - loss: 1.3030e-05 - val_loss: 7.7733e-08
Epoch 141/512
448/448 - 0s - loss: 1.2837e-05 - val_loss: 7.7603e-08
Epoch 142/512
448/448 - 0s - loss: 1.2799e-05 - val_loss: 7.4911e-08
Epoch 143/512
448/448 - 0s - loss: 1.2310e-05 - val_loss: 7.0933e-08
Epoch 144/512
448/448 - 0s - loss: 1.1814e-05 - val_loss: 7.0471e-08
Epoch 145/512
448/448 - 0s - loss: 1.1799e-05 - val_loss: 6.9710e-08
Epoch 146/512
448/448 - 0s - loss: 1.1608e-05 - val_loss: 6.6203e-08
Epoch 147/512
448/448 - 0s - loss: 1.1021e-05 - val_loss: 6.4151e-08
Epoch 148/512
448/448 - 0s - loss: 1.0840e-05 - val_loss: 6.3528e-08
Epoch 149/512
448/448 - 0s - loss: 1.0737e-05 - val_loss: 6.2042e-08
Epoch 150/512
448/448 - 0s - loss: 1.0387e-05 - val_loss: 5.9115e-08
Epoch 151/512
448/448 - 0s - loss: 1.0008e-05 - val_loss: 5.7825e-08
Epoch 152/512
448/448 - 0s - loss: 9.8366e-06 - val_loss: 5.6909e-08
Epoch 153/512
448/448 - 0s - loss: 9.6569e-06 - val_loss: 5.5154e-08
Epoch 154/512
448/448 - 0s - loss: 9.3274e-06 - val_loss: 5.3192e-08
Epoch 155/512
448/448 - 0s - loss: 9.0034e-06 - val_loss: 5.2812e-08
Epoch 156/512
448/448 - 0s - loss: 8.9718e-06 - val_loss: 5.1566e-08
Epoch 157/512
448/448 - 0s - loss: 8.6669e-06 - val_loss: 4.9525e-08
Epoch 158/512
448/448 - 0s - loss: 8.3351e-06 - val_loss: 4.8482e-08
Epoch 159/512
448/448 - 0s - loss: 8.2415e-06 - val_loss: 4.6953e-08
Epoch 160/512
448/448 - 0s - loss: 7.9137e-06 - val_loss: 4.5800e-08
Epoch 161/512
448/448 - 0s - loss: 7.7512e-06 - val_loss: 4.4908e-08
Epoch 162/512
448/448 - 0s - loss: 7.5512e-06 - val_loss: 4.3440e-08
Epoch 163/512
448/448 - 0s - loss: 7.3171e-06 - val_loss: 4.1877e-08
Epoch 164/512
448/448 - 0s - loss: 7.0577e-06 - val_loss: 4.1249e-08
Epoch 165/512
448/448 - 0s - loss: 6.9430e-06 - val_loss: 4.0504e-08
Epoch 166/512
448/448 - 0s - loss: 6.7719e-06 - val_loss: 3.8900e-08
Epoch 167/512
448/448 - 0s - loss: 6.4705e-06 - val_loss: 3.7821e-08
Epoch 168/512
448/448 - 0s - loss: 6.2693e-06 - val_loss: 3.7932e-08
Epoch 169/512
448/448 - 0s - loss: 6.2983e-06 - val_loss: 3.6309e-08
Epoch 170/512
448/448 - 0s - loss: 5.9066e-06 - val_loss: 3.4843e-08
Epoch 171/512
448/448 - 0s - loss: 5.7502e-06 - val_loss: 3.4563e-08
Epoch 172/512
448/448 - 0s - loss: 5.6803e-06 - val_loss: 3.3702e-08
Epoch 173/512
448/448 - 0s - loss: 5.4483e-06 - val_loss: 3.2644e-08
Epoch 174/512
448/448 - 0s - loss: 5.2704e-06 - val_loss: 3.1942e-08
Epoch 175/512
448/448 - 0s - loss: 5.1567e-06 - val_loss: 3.1058e-08
Epoch 176/512
448/448 - 0s - loss: 4.9699e-06 - val_loss: 3.0129e-08
Epoch 177/512
448/448 - 0s - loss: 4.7972e-06 - val_loss: 2.9725e-08
Epoch 178/512
448/448 - 0s - loss: 4.7130e-06 - val_loss: 2.8763e-08
Epoch 179/512
448/448 - 0s - loss: 4.4922e-06 - val_loss: 2.7982e-08
Epoch 180/512
448/448 - 0s - loss: 4.3609e-06 - val_loss: 2.7770e-08
Epoch 181/512
448/448 - 0s - loss: 4.3070e-06 - val_loss: 2.6756e-08
Epoch 182/512
448/448 - 0s - loss: 4.0807e-06 - val_loss: 2.5717e-08
Epoch 183/512
448/448 - 0s - loss: 3.9463e-06 - val_loss: 2.5255e-08
Epoch 184/512
448/448 - 0s - loss: 3.8651e-06 - val_loss: 2.4637e-08
Epoch 185/512
448/448 - 0s - loss: 3.7265e-06 - val_loss: 2.3759e-08
Epoch 186/512
448/448 - 0s - loss: 3.5482e-06 - val_loss: 2.3514e-08
Epoch 187/512
448/448 - 0s - loss: 3.5037e-06 - val_loss: 2.3178e-08
Epoch 188/512
448/448 - 0s - loss: 3.4206e-06 - val_loss: 2.1885e-08
Epoch 189/512
448/448 - 0s - loss: 3.1554e-06 - val_loss: 2.1674e-08
Epoch 190/512
448/448 - 0s - loss: 3.1772e-06 - val_loss: 2.1522e-08
Epoch 191/512
448/448 - 0s - loss: 3.0585e-06 - val_loss: 2.0690e-08
Epoch 192/512
448/448 - 0s - loss: 2.9211e-06 - val_loss: 1.9909e-08
Epoch 193/512
448/448 - 0s - loss: 2.7873e-06 - val_loss: 1.9770e-08
Epoch 194/512
448/448 - 0s - loss: 2.7738e-06 - val_loss: 1.9188e-08
Epoch 195/512
448/448 - 0s - loss: 2.6326e-06 - val_loss: 1.8486e-08
Epoch 196/512
448/448 - 0s - loss: 2.5033e-06 - val_loss: 1.8431e-08
Epoch 197/512
448/448 - 0s - loss: 2.4969e-06 - val_loss: 1.7981e-08
Epoch 198/512
448/448 - 0s - loss: 2.3800e-06 - val_loss: 1.7135e-08
Epoch 199/512
448/448 - 0s - loss: 2.2465e-06 - val_loss: 1.6804e-08
Epoch 200/512
448/448 - 0s - loss: 2.1997e-06 - val_loss: 1.6788e-08
Epoch 201/512
448/448 - 0s - loss: 2.1677e-06 - val_loss: 1.6087e-08
Epoch 202/512
448/448 - 0s - loss: 2.0277e-06 - val_loss: 1.5471e-08
Epoch 203/512
448/448 - 0s - loss: 1.9377e-06 - val_loss: 1.5476e-08
Epoch 204/512
448/448 - 0s - loss: 1.9365e-06 - val_loss: 1.5089e-08
Epoch 205/512
448/448 - 0s - loss: 1.8340e-06 - val_loss: 1.4430e-08
Epoch 206/512
448/448 - 0s - loss: 1.7308e-06 - val_loss: 1.4313e-08
Epoch 207/512
448/448 - 0s - loss: 1.7060e-06 - val_loss: 1.4306e-08
Epoch 208/512
448/448 - 0s - loss: 1.6740e-06 - val_loss: 1.3641e-08
Epoch 209/512
448/448 - 0s - loss: 1.5615e-06 - val_loss: 1.3011e-08
Epoch 210/512
448/448 - 0s - loss: 1.4784e-06 - val_loss: 1.3058e-08
Epoch 211/512
448/448 - 0s - loss: 1.4867e-06 - val_loss: 1.2854e-08
Epoch 212/512
448/448 - 0s - loss: 1.4163e-06 - val_loss: 1.2242e-08
Epoch 213/512
448/448 - 0s - loss: 1.3191e-06 - val_loss: 1.2078e-08
Epoch 214/512
448/448 - 0s - loss: 1.3121e-06 - val_loss: 1.1836e-08
Epoch 215/512
448/448 - 0s - loss: 1.2479e-06 - val_loss: 1.1432e-08
Epoch 216/512
448/448 - 0s - loss: 1.1878e-06 - val_loss: 1.1218e-08
Epoch 217/512
448/448 - 0s - loss: 1.1557e-06 - val_loss: 1.0995e-08
Epoch 218/512
448/448 - 0s - loss: 1.1137e-06 - val_loss: 1.0695e-08
Epoch 219/512
448/448 - 0s - loss: 1.0570e-06 - val_loss: 1.0458e-08
Epoch 220/512
448/448 - 0s - loss: 1.0212e-06 - val_loss: 1.0331e-08
Epoch 221/512
448/448 - 0s - loss: 9.9706e-07 - val_loss: 9.9126e-09
Epoch 222/512
448/448 - 0s - loss: 9.2722e-07 - val_loss: 9.6529e-09
Epoch 223/512
448/448 - 0s - loss: 9.0061e-07 - val_loss: 9.5720e-09
Epoch 224/512
448/448 - 0s - loss: 8.8091e-07 - val_loss: 9.3015e-09
Epoch 225/512
448/448 - 0s - loss: 8.2713e-07 - val_loss: 9.0879e-09
Epoch 226/512
448/448 - 0s - loss: 7.9763e-07 - val_loss: 8.9868e-09
Epoch 227/512
448/448 - 0s - loss: 7.7853e-07 - val_loss: 8.7216e-09
Epoch 228/512
448/448 - 0s - loss: 7.3300e-07 - val_loss: 8.4420e-09
Epoch 229/512
448/448 - 0s - loss: 6.9864e-07 - val_loss: 8.3025e-09
Epoch 230/512
448/448 - 0s - loss: 6.8010e-07 - val_loss: 8.2053e-09
Epoch 231/512
448/448 - 0s - loss: 6.5838e-07 - val_loss: 7.9534e-09
Epoch 232/512
448/448 - 0s - loss: 6.1900e-07 - val_loss: 7.7341e-09
Epoch 233/512
448/448 - 0s - loss: 5.9538e-07 - val_loss: 7.5664e-09
Epoch 234/512
448/448 - 0s - loss: 5.7286e-07 - val_loss: 7.3950e-09
Epoch 235/512
448/448 - 0s - loss: 5.4685e-07 - val_loss: 7.2485e-09
Epoch 236/512
448/448 - 0s - loss: 5.2205e-07 - val_loss: 7.1613e-09
Epoch 237/512
448/448 - 0s - loss: 5.1135e-07 - val_loss: 6.9839e-09
Epoch 238/512
448/448 - 0s - loss: 4.8444e-07 - val_loss: 6.7163e-09
Epoch 239/512
448/448 - 0s - loss: 4.4968e-07 - val_loss: 6.6847e-09
Epoch 240/512
448/448 - 0s - loss: 4.5163e-07 - val_loss: 6.5856e-09
Epoch 241/512
448/448 - 0s - loss: 4.2928e-07 - val_loss: 6.3305e-09
Epoch 242/512
448/448 - 0s - loss: 3.9802e-07 - val_loss: 6.2199e-09
Epoch 243/512
448/448 - 0s - loss: 3.8861e-07 - val_loss: 6.1654e-09
Epoch 244/512
448/448 - 0s - loss: 3.7794e-07 - val_loss: 5.9923e-09
Epoch 245/512
448/448 - 0s - loss: 3.5584e-07 - val_loss: 5.7645e-09
Epoch 246/512
448/448 - 0s - loss: 3.3171e-07 - val_loss: 5.7474e-09
Epoch 247/512
448/448 - 0s - loss: 3.3216e-07 - val_loss: 5.6926e-09
Epoch 248/512
448/448 - 0s - loss: 3.1981e-07 - val_loss: 5.4349e-09
Epoch 249/512
448/448 - 0s - loss: 2.8962e-07 - val_loss: 5.2873e-09
Epoch 250/512
448/448 - 0s - loss: 2.7949e-07 - val_loss: 5.3398e-09
Epoch 251/512
448/448 - 0s - loss: 2.8462e-07 - val_loss: 5.2294e-09
Epoch 252/512
448/448 - 0s - loss: 2.6240e-07 - val_loss: 5.0050e-09
Epoch 253/512
448/448 - 0s - loss: 2.4287e-07 - val_loss: 4.9273e-09
Epoch 254/512
448/448 - 0s - loss: 2.3929e-07 - val_loss: 4.8981e-09
Epoch 255/512
448/448 - 0s - loss: 2.3321e-07 - val_loss: 4.7640e-09
Epoch 256/512
448/448 - 0s - loss: 2.1598e-07 - val_loss: 4.6522e-09
Epoch 257/512
448/448 - 0s - loss: 2.0721e-07 - val_loss: 4.6220e-09
Epoch 258/512
448/448 - 0s - loss: 2.0508e-07 - val_loss: 4.4684e-09
Epoch 259/512
448/448 - 0s - loss: 1.8738e-07 - val_loss: 4.3466e-09
Epoch 260/512
448/448 - 0s - loss: 1.7883e-07 - val_loss: 4.3520e-09
Epoch 261/512
448/448 - 0s - loss: 1.8048e-07 - val_loss: 4.2517e-09
Epoch 262/512
448/448 - 0s - loss: 1.6668e-07 - val_loss: 4.1001e-09
Epoch 263/512
448/448 - 0s - loss: 1.5434e-07 - val_loss: 4.0602e-09
Epoch 264/512
448/448 - 0s - loss: 1.5355e-07 - val_loss: 4.0441e-09
Epoch 265/512
448/448 - 0s - loss: 1.5000e-07 - val_loss: 3.8976e-09
Epoch 266/512
448/448 - 0s - loss: 1.3482e-07 - val_loss: 3.8220e-09
Epoch 267/512
448/448 - 0s - loss: 1.3214e-07 - val_loss: 3.8043e-09
Epoch 268/512
448/448 - 0s - loss: 1.3010e-07 - val_loss: 3.7124e-09
Epoch 269/512
448/448 - 0s - loss: 1.2041e-07 - val_loss: 3.6152e-09
Epoch 270/512
448/448 - 0s - loss: 1.1403e-07 - val_loss: 3.5699e-09
Epoch 271/512
448/448 - 0s - loss: 1.1232e-07 - val_loss: 3.5045e-09
Epoch 272/512
448/448 - 0s - loss: 1.0535e-07 - val_loss: 3.4293e-09
Epoch 273/512
448/448 - 0s - loss: 1.0017e-07 - val_loss: 3.3781e-09
Epoch 274/512
448/448 - 0s - loss: 9.7151e-08 - val_loss: 3.3201e-09
Epoch 275/512
448/448 - 0s - loss: 9.2030e-08 - val_loss: 3.2561e-09
Epoch 276/512
448/448 - 0s - loss: 8.7992e-08 - val_loss: 3.1954e-09
Epoch 277/512
448/448 - 0s - loss: 8.3293e-08 - val_loss: 3.1623e-09
Epoch 278/512
448/448 - 0s - loss: 8.2108e-08 - val_loss: 3.0891e-09
Epoch 279/512
448/448 - 0s - loss: 7.5710e-08 - val_loss: 3.0344e-09
Epoch 280/512
448/448 - 0s - loss: 7.3326e-08 - val_loss: 2.9853e-09
Epoch 281/512
448/448 - 0s - loss: 7.0178e-08 - val_loss: 2.9361e-09
Epoch 282/512
448/448 - 0s - loss: 6.6673e-08 - val_loss: 2.8878e-09
Epoch 283/512
448/448 - 0s - loss: 6.3930e-08 - val_loss: 2.8505e-09
Epoch 284/512
448/448 - 0s - loss: 6.1766e-08 - val_loss: 2.7999e-09
Epoch 285/512
448/448 - 0s - loss: 5.8420e-08 - val_loss: 2.7411e-09
Epoch 286/512
448/448 - 0s - loss: 5.4845e-08 - val_loss: 2.7097e-09
Epoch 287/512
448/448 - 0s - loss: 5.3657e-08 - val_loss: 2.6765e-09
Epoch 288/512
448/448 - 0s - loss: 5.1854e-08 - val_loss: 2.6145e-09
Epoch 289/512
448/448 - 0s - loss: 4.7772e-08 - val_loss: 2.5680e-09
Epoch 290/512
448/448 - 0s - loss: 4.5997e-08 - val_loss: 2.5490e-09
Epoch 291/512
448/448 - 0s - loss: 4.5432e-08 - val_loss: 2.5037e-09
Epoch 292/512
448/448 - 0s - loss: 4.2325e-08 - val_loss: 2.4564e-09
Epoch 293/512
448/448 - 0s - loss: 3.9905e-08 - val_loss: 2.4363e-09
Epoch 294/512
448/448 - 0s - loss: 3.9550e-08 - val_loss: 2.4080e-09
Epoch 295/512
448/448 - 0s - loss: 3.7939e-08 - val_loss: 2.3549e-09
Epoch 296/512
448/448 - 0s - loss: 3.4756e-08 - val_loss: 2.3249e-09
Epoch 297/512
448/448 - 0s - loss: 3.4172e-08 - val_loss: 2.2985e-09
Epoch 298/512
448/448 - 0s - loss: 3.2806e-08 - val_loss: 2.2693e-09
Epoch 299/512
448/448 - 0s - loss: 3.1410e-08 - val_loss: 2.2313e-09
Epoch 300/512
448/448 - 0s - loss: 2.9701e-08 - val_loss: 2.1997e-09
Epoch 301/512
448/448 - 0s - loss: 2.8642e-08 - val_loss: 2.1693e-09
Epoch 302/512
448/448 - 0s - loss: 2.7452e-08 - val_loss: 2.1382e-09
Epoch 303/512
448/448 - 0s - loss: 2.6015e-08 - val_loss: 2.1125e-09
Epoch 304/512
448/448 - 0s - loss: 2.5273e-08 - val_loss: 2.0834e-09
Epoch 305/512
448/448 - 0s - loss: 2.3947e-08 - val_loss: 2.0553e-09
Epoch 306/512
448/448 - 0s - loss: 2.3041e-08 - val_loss: 2.0256e-09
Epoch 307/512
448/448 - 0s - loss: 2.1808e-08 - val_loss: 2.0099e-09
Epoch 308/512
448/448 - 0s - loss: 2.1649e-08 - val_loss: 1.9789e-09
Epoch 309/512
448/448 - 0s - loss: 2.0221e-08 - val_loss: 1.9487e-09
Epoch 310/512
448/448 - 0s - loss: 1.9238e-08 - val_loss: 1.9250e-09
Epoch 311/512
448/448 - 0s - loss: 1.8541e-08 - val_loss: 1.9111e-09
Epoch 312/512
448/448 - 0s - loss: 1.8270e-08 - val_loss: 1.8856e-09
Epoch 313/512
448/448 - 0s - loss: 1.7241e-08 - val_loss: 1.8565e-09
Epoch 314/512
448/448 - 0s - loss: 1.6361e-08 - val_loss: 1.8349e-09
Epoch 315/512
448/448 - 0s - loss: 1.5789e-08 - val_loss: 1.8161e-09
Epoch 316/512
448/448 - 0s - loss: 1.5315e-08 - val_loss: 1.7931e-09
Epoch 317/512
448/448 - 0s - loss: 1.4575e-08 - val_loss: 1.7717e-09
Epoch 318/512
448/448 - 0s - loss: 1.3979e-08 - val_loss: 1.7573e-09
Epoch 319/512
448/448 - 0s - loss: 1.3842e-08 - val_loss: 1.7328e-09
Epoch 320/512
448/448 - 0s - loss: 1.2873e-08 - val_loss: 1.7114e-09
Epoch 321/512
448/448 - 0s - loss: 1.2378e-08 - val_loss: 1.6935e-09
Epoch 322/512
448/448 - 0s - loss: 1.2109e-08 - val_loss: 1.6746e-09
Epoch 323/512
448/448 - 0s - loss: 1.1498e-08 - val_loss: 1.6603e-09
Epoch 324/512
448/448 - 0s - loss: 1.1281e-08 - val_loss: 1.6460e-09
Epoch 325/512
448/448 - 0s - loss: 1.1007e-08 - val_loss: 1.6198e-09
Epoch 326/512
448/448 - 0s - loss: 1.0204e-08 - val_loss: 1.5997e-09
Epoch 327/512
448/448 - 0s - loss: 9.8146e-09 - val_loss: 1.5855e-09
Epoch 328/512
448/448 - 0s - loss: 9.6526e-09 - val_loss: 1.5733e-09
Epoch 329/512
448/448 - 0s - loss: 9.4681e-09 - val_loss: 1.5536e-09
Epoch 330/512
448/448 - 0s - loss: 8.9282e-09 - val_loss: 1.5326e-09
Epoch 331/512
448/448 - 0s - loss: 8.4616e-09 - val_loss: 1.5200e-09
Epoch 332/512
448/448 - 0s - loss: 8.3288e-09 - val_loss: 1.5059e-09
Epoch 333/512
448/448 - 0s - loss: 8.0708e-09 - val_loss: 1.4914e-09
Epoch 334/512
448/448 - 0s - loss: 7.8648e-09 - val_loss: 1.4789e-09
Epoch 335/512
448/448 - 0s - loss: 7.6607e-09 - val_loss: 1.4549e-09
Epoch 336/512
448/448 - 0s - loss: 6.9932e-09 - val_loss: 1.4435e-09
Epoch 337/512
448/448 - 0s - loss: 6.9688e-09 - val_loss: 1.4343e-09
Epoch 338/512
448/448 - 0s - loss: 6.9889e-09 - val_loss: 1.4220e-09
Epoch 339/512
448/448 - 0s - loss: 6.7250e-09 - val_loss: 1.4035e-09
Epoch 340/512
448/448 - 0s - loss: 6.2602e-09 - val_loss: 1.3867e-09
Epoch 341/512
448/448 - 0s - loss: 6.0572e-09 - val_loss: 1.3759e-09
Epoch 342/512
448/448 - 0s - loss: 5.9624e-09 - val_loss: 1.3648e-09
Epoch 343/512
448/448 - 0s - loss: 5.8414e-09 - val_loss: 1.3525e-09
Epoch 344/512
448/448 - 0s - loss: 5.6023e-09 - val_loss: 1.3402e-09
Epoch 345/512
448/448 - 0s - loss: 5.5090e-09 - val_loss: 1.3248e-09
Epoch 346/512
448/448 - 0s - loss: 5.1907e-09 - val_loss: 1.3133e-09
Epoch 347/512
448/448 - 0s - loss: 5.1062e-09 - val_loss: 1.3027e-09
Epoch 348/512
448/448 - 0s - loss: 4.9831e-09 - val_loss: 1.2911e-09
Epoch 349/512
448/448 - 0s - loss: 4.8656e-09 - val_loss: 1.2789e-09
Epoch 350/512
448/448 - 0s - loss: 4.6653e-09 - val_loss: 1.2666e-09
Epoch 351/512
448/448 - 0s - loss: 4.5509e-09 - val_loss: 1.2552e-09
Epoch 352/512
448/448 - 0s - loss: 4.4237e-09 - val_loss: 1.2432e-09
Epoch 353/512
448/448 - 0s - loss: 4.2395e-09 - val_loss: 1.2334e-09
Epoch 354/512
448/448 - 0s - loss: 4.1916e-09 - val_loss: 1.2244e-09
Epoch 355/512
448/448 - 0s - loss: 4.1191e-09 - val_loss: 1.2137e-09
Epoch 356/512
448/448 - 0s - loss: 3.9479e-09 - val_loss: 1.2009e-09
Epoch 357/512
448/448 - 0s - loss: 3.8175e-09 - val_loss: 1.1924e-09
Epoch 358/512
448/448 - 0s - loss: 3.7572e-09 - val_loss: 1.1826e-09
Epoch 359/512
448/448 - 0s - loss: 3.6663e-09 - val_loss: 1.1729e-09
Epoch 360/512
448/448 - 0s - loss: 3.5955e-09 - val_loss: 1.1622e-09
Epoch 361/512
448/448 - 0s - loss: 3.4469e-09 - val_loss: 1.1543e-09
Epoch 362/512
448/448 - 0s - loss: 3.4148e-09 - val_loss: 1.1448e-09
Epoch 363/512
448/448 - 0s - loss: 3.3371e-09 - val_loss: 1.1343e-09
Epoch 364/512
448/448 - 0s - loss: 3.2299e-09 - val_loss: 1.1234e-09
Epoch 365/512
448/448 - 0s - loss: 3.1120e-09 - val_loss: 1.1132e-09
Epoch 366/512
448/448 - 0s - loss: 2.9873e-09 - val_loss: 1.1065e-09
Epoch 367/512
448/448 - 0s - loss: 2.9963e-09 - val_loss: 1.0985e-09
Epoch 368/512
448/448 - 0s - loss: 2.9307e-09 - val_loss: 1.0914e-09
Epoch 369/512
448/448 - 0s - loss: 2.8843e-09 - val_loss: 1.0818e-09
Epoch 370/512
448/448 - 0s - loss: 2.8174e-09 - val_loss: 1.0719e-09
Epoch 371/512
448/448 - 0s - loss: 2.7095e-09 - val_loss: 1.0630e-09
Epoch 372/512
448/448 - 0s - loss: 2.6183e-09 - val_loss: 1.0562e-09
Epoch 373/512
448/448 - 0s - loss: 2.6075e-09 - val_loss: 1.0482e-09
Epoch 374/512
448/448 - 0s - loss: 2.5453e-09 - val_loss: 1.0399e-09
Epoch 375/512
448/448 - 0s - loss: 2.4743e-09 - val_loss: 1.0313e-09
Epoch 376/512
448/448 - 0s - loss: 2.4437e-09 - val_loss: 1.0248e-09
Epoch 377/512
448/448 - 0s - loss: 2.4028e-09 - val_loss: 1.0169e-09
Epoch 378/512
448/448 - 0s - loss: 2.3345e-09 - val_loss: 1.0092e-09
Epoch 379/512
448/448 - 0s - loss: 2.2795e-09 - val_loss: 1.0028e-09
Epoch 380/512
448/448 - 0s - loss: 2.2665e-09 - val_loss: 9.9447e-10
Epoch 381/512
448/448 - 0s - loss: 2.1910e-09 - val_loss: 9.8829e-10
Epoch 382/512
448/448 - 0s - loss: 2.1702e-09 - val_loss: 9.8154e-10
Epoch 383/512
448/448 - 0s - loss: 2.1382e-09 - val_loss: 9.7229e-10
Epoch 384/512
448/448 - 0s - loss: 2.0232e-09 - val_loss: 9.6519e-10
Epoch 385/512
448/448 - 0s - loss: 1.9841e-09 - val_loss: 9.5834e-10
Epoch 386/512
448/448 - 0s - loss: 1.9613e-09 - val_loss: 9.5400e-10
Epoch 387/512
448/448 - 0s - loss: 1.9829e-09 - val_loss: 9.4935e-10
Epoch 388/512
448/448 - 0s - loss: 1.9961e-09 - val_loss: 9.4102e-10
Epoch 389/512
448/448 - 0s - loss: 1.9065e-09 - val_loss: 9.3244e-10
Epoch 390/512
448/448 - 0s - loss: 1.8093e-09 - val_loss: 9.2329e-10
Epoch 391/512
448/448 - 0s - loss: 1.7361e-09 - val_loss: 9.1947e-10
Epoch 392/512
448/448 - 0s - loss: 1.7663e-09 - val_loss: 9.1484e-10
Epoch 393/512
448/448 - 0s - loss: 1.7732e-09 - val_loss: 9.1025e-10
Epoch 394/512
448/448 - 0s - loss: 1.7771e-09 - val_loss: 9.0367e-10
Epoch 395/512
448/448 - 0s - loss: 1.7099e-09 - val_loss: 8.9658e-10
Epoch 396/512
448/448 - 0s - loss: 1.6406e-09 - val_loss: 8.8995e-10
Epoch 397/512
448/448 - 0s - loss: 1.6060e-09 - val_loss: 8.8356e-10
Epoch 398/512
448/448 - 0s - loss: 1.5842e-09 - val_loss: 8.8009e-10
Epoch 399/512
448/448 - 0s - loss: 1.6063e-09 - val_loss: 8.7383e-10
Epoch 400/512
448/448 - 0s - loss: 1.5633e-09 - val_loss: 8.6834e-10
Epoch 401/512
448/448 - 0s - loss: 1.5294e-09 - val_loss: 8.6168e-10
Epoch 402/512
448/448 - 0s - loss: 1.4878e-09 - val_loss: 8.5668e-10
Epoch 403/512
448/448 - 0s - loss: 1.4662e-09 - val_loss: 8.5195e-10
Epoch 404/512
448/448 - 0s - loss: 1.4545e-09 - val_loss: 8.4585e-10
Epoch 405/512
448/448 - 0s - loss: 1.4207e-09 - val_loss: 8.4123e-10
Epoch 406/512
448/448 - 0s - loss: 1.4307e-09 - val_loss: 8.3601e-10
Epoch 407/512
448/448 - 0s - loss: 1.3822e-09 - val_loss: 8.2982e-10
Epoch 408/512
448/448 - 0s - loss: 1.3717e-09 - val_loss: 8.2615e-10
Epoch 409/512
448/448 - 0s - loss: 1.3577e-09 - val_loss: 8.1983e-10
Epoch 410/512
448/448 - 0s - loss: 1.3145e-09 - val_loss: 8.1565e-10
Epoch 411/512
448/448 - 0s - loss: 1.3076e-09 - val_loss: 8.0986e-10
Epoch 412/512
448/448 - 0s - loss: 1.2769e-09 - val_loss: 8.0531e-10
Epoch 413/512
448/448 - 0s - loss: 1.2657e-09 - val_loss: 8.0179e-10
Epoch 414/512
448/448 - 0s - loss: 1.2870e-09 - val_loss: 7.9595e-10
Epoch 415/512
448/448 - 0s - loss: 1.2224e-09 - val_loss: 7.9169e-10
Epoch 416/512
448/448 - 0s - loss: 1.2119e-09 - val_loss: 7.8649e-10
Epoch 417/512
448/448 - 0s - loss: 1.1922e-09 - val_loss: 7.8169e-10
Epoch 418/512
448/448 - 0s - loss: 1.1608e-09 - val_loss: 7.7708e-10
Epoch 419/512
448/448 - 0s - loss: 1.1625e-09 - val_loss: 7.7434e-10
Epoch 420/512
448/448 - 0s - loss: 1.1715e-09 - val_loss: 7.7038e-10
Epoch 421/512
448/448 - 0s - loss: 1.1542e-09 - val_loss: 7.6535e-10
Epoch 422/512
448/448 - 0s - loss: 1.1395e-09 - val_loss: 7.5984e-10
Epoch 423/512
448/448 - 0s - loss: 1.0941e-09 - val_loss: 7.5497e-10
Epoch 424/512
448/448 - 0s - loss: 1.0658e-09 - val_loss: 7.5107e-10
Epoch 425/512
448/448 - 0s - loss: 1.0769e-09 - val_loss: 7.4791e-10
Epoch 426/512
448/448 - 0s - loss: 1.0765e-09 - val_loss: 7.4431e-10
Epoch 427/512
448/448 - 0s - loss: 1.0663e-09 - val_loss: 7.3988e-10
Epoch 428/512
448/448 - 0s - loss: 1.0408e-09 - val_loss: 7.3490e-10
Epoch 429/512
448/448 - 0s - loss: 1.0193e-09 - val_loss: 7.3037e-10
Epoch 430/512
448/448 - 0s - loss: 9.8954e-10 - val_loss: 7.2561e-10
Epoch 431/512
448/448 - 0s - loss: 9.6877e-10 - val_loss: 7.2223e-10
Epoch 432/512
448/448 - 0s - loss: 9.7055e-10 - val_loss: 7.1948e-10
Epoch 433/512
448/448 - 0s - loss: 9.7789e-10 - val_loss: 7.1575e-10
Epoch 434/512
448/448 - 0s - loss: 9.6363e-10 - val_loss: 7.1178e-10
Epoch 435/512
448/448 - 0s - loss: 9.6381e-10 - val_loss: 7.0824e-10
Epoch 436/512
448/448 - 0s - loss: 9.2895e-10 - val_loss: 7.0386e-10
Epoch 437/512
448/448 - 0s - loss: 9.1615e-10 - val_loss: 7.0047e-10
Epoch 438/512
448/448 - 0s - loss: 9.1132e-10 - val_loss: 6.9768e-10
Epoch 439/512
448/448 - 0s - loss: 9.1418e-10 - val_loss: 6.9367e-10
Epoch 440/512
448/448 - 0s - loss: 9.0096e-10 - val_loss: 6.9015e-10
Epoch 441/512
448/448 - 0s - loss: 8.9811e-10 - val_loss: 6.8657e-10
Epoch 442/512
448/448 - 0s - loss: 8.8275e-10 - val_loss: 6.8282e-10
Epoch 443/512
448/448 - 0s - loss: 8.6551e-10 - val_loss: 6.7926e-10
Epoch 444/512
448/448 - 0s - loss: 8.5947e-10 - val_loss: 6.7632e-10
Epoch 445/512
448/448 - 0s - loss: 8.5734e-10 - val_loss: 6.7331e-10
Epoch 446/512
448/448 - 0s - loss: 8.4693e-10 - val_loss: 6.6950e-10
Epoch 447/512
448/448 - 0s - loss: 8.3460e-10 - val_loss: 6.6605e-10
Epoch 448/512
448/448 - 0s - loss: 8.2575e-10 - val_loss: 6.6187e-10
Epoch 449/512
448/448 - 0s - loss: 8.0402e-10 - val_loss: 6.5856e-10
Epoch 450/512
448/448 - 0s - loss: 7.8934e-10 - val_loss: 6.5523e-10
Epoch 451/512
448/448 - 0s - loss: 7.9450e-10 - val_loss: 6.5367e-10
Epoch 452/512
448/448 - 0s - loss: 8.0043e-10 - val_loss: 6.5010e-10
Epoch 453/512
448/448 - 0s - loss: 7.9304e-10 - val_loss: 6.4790e-10
Epoch 454/512
448/448 - 0s - loss: 7.8478e-10 - val_loss: 6.4413e-10
Epoch 455/512
448/448 - 0s - loss: 7.7619e-10 - val_loss: 6.4092e-10
Epoch 456/512
448/448 - 0s - loss: 7.6173e-10 - val_loss: 6.3717e-10
Epoch 457/512
448/448 - 0s - loss: 7.3918e-10 - val_loss: 6.3386e-10
Epoch 458/512
448/448 - 0s - loss: 7.2022e-10 - val_loss: 6.3111e-10
Epoch 459/512
448/448 - 0s - loss: 7.2275e-10 - val_loss: 6.2801e-10
Epoch 460/512
448/448 - 0s - loss: 7.2272e-10 - val_loss: 6.2605e-10
Epoch 461/512
448/448 - 0s - loss: 7.3007e-10 - val_loss: 6.2388e-10
Epoch 462/512
448/448 - 0s - loss: 7.3466e-10 - val_loss: 6.2030e-10
Epoch 463/512
448/448 - 0s - loss: 7.0950e-10 - val_loss: 6.1634e-10
Epoch 464/512
448/448 - 0s - loss: 6.9378e-10 - val_loss: 6.1399e-10
Epoch 465/512
448/448 - 0s - loss: 6.7804e-10 - val_loss: 6.1102e-10
Epoch 466/512
448/448 - 0s - loss: 6.8060e-10 - val_loss: 6.0830e-10
Epoch 467/512
448/448 - 0s - loss: 6.7602e-10 - val_loss: 6.0622e-10
Epoch 468/512
448/448 - 0s - loss: 6.8022e-10 - val_loss: 6.0392e-10
Epoch 469/512
448/448 - 0s - loss: 6.7088e-10 - val_loss: 6.0034e-10
Epoch 470/512
448/448 - 0s - loss: 6.5594e-10 - val_loss: 5.9765e-10
Epoch 471/512
448/448 - 0s - loss: 6.5281e-10 - val_loss: 5.9517e-10
Epoch 472/512
448/448 - 0s - loss: 6.4959e-10 - val_loss: 5.9306e-10
Epoch 473/512
448/448 - 0s - loss: 6.4698e-10 - val_loss: 5.8976e-10
Epoch 474/512
448/448 - 0s - loss: 6.4089e-10 - val_loss: 5.8748e-10
Epoch 475/512
448/448 - 0s - loss: 6.3924e-10 - val_loss: 5.8574e-10
Epoch 476/512
448/448 - 0s - loss: 6.3263e-10 - val_loss: 5.8306e-10
Epoch 477/512
448/448 - 0s - loss: 6.2735e-10 - val_loss: 5.8045e-10
Epoch 478/512
448/448 - 0s - loss: 6.1443e-10 - val_loss: 5.7797e-10
Epoch 479/512
448/448 - 0s - loss: 6.2137e-10 - val_loss: 5.7552e-10
Epoch 480/512
448/448 - 0s - loss: 6.2625e-10 - val_loss: 5.7321e-10
Epoch 481/512
448/448 - 0s - loss: 6.1140e-10 - val_loss: 5.7023e-10
Epoch 482/512
448/448 - 0s - loss: 6.0218e-10 - val_loss: 5.6722e-10
Epoch 483/512
448/448 - 0s - loss: 5.8928e-10 - val_loss: 5.6463e-10
Epoch 484/512
448/448 - 0s - loss: 5.7882e-10 - val_loss: 5.6228e-10
Epoch 485/512
448/448 - 0s - loss: 5.7288e-10 - val_loss: 5.6047e-10
Epoch 486/512
448/448 - 0s - loss: 5.7867e-10 - val_loss: 5.5874e-10
Epoch 487/512
448/448 - 0s - loss: 5.8419e-10 - val_loss: 5.5657e-10
Epoch 488/512
448/448 - 0s - loss: 5.7855e-10 - val_loss: 5.5413e-10
Epoch 489/512
448/448 - 0s - loss: 5.6883e-10 - val_loss: 5.5159e-10
Epoch 490/512
448/448 - 0s - loss: 5.6052e-10 - val_loss: 5.4944e-10
Epoch 491/512
448/448 - 0s - loss: 5.6175e-10 - val_loss: 5.4729e-10
Epoch 492/512
448/448 - 0s - loss: 5.5279e-10 - val_loss: 5.4490e-10
Epoch 493/512
448/448 - 0s - loss: 5.4682e-10 - val_loss: 5.4252e-10
Epoch 494/512
448/448 - 0s - loss: 5.4501e-10 - val_loss: 5.4036e-10
Epoch 495/512
448/448 - 0s - loss: 5.3924e-10 - val_loss: 5.3835e-10
Epoch 496/512
448/448 - 0s - loss: 5.3747e-10 - val_loss: 5.3636e-10
Epoch 497/512
448/448 - 0s - loss: 5.3001e-10 - val_loss: 5.3426e-10
Epoch 498/512
448/448 - 0s - loss: 5.3134e-10 - val_loss: 5.3196e-10
Epoch 499/512
448/448 - 0s - loss: 5.3399e-10 - val_loss: 5.3065e-10
Epoch 500/512
448/448 - 0s - loss: 5.3252e-10 - val_loss: 5.2836e-10
Epoch 501/512
448/448 - 0s - loss: 5.2581e-10 - val_loss: 5.2557e-10
Epoch 502/512
448/448 - 0s - loss: 5.1137e-10 - val_loss: 5.2336e-10
Epoch 503/512
448/448 - 0s - loss: 5.0289e-10 - val_loss: 5.2184e-10
Epoch 504/512
448/448 - 0s - loss: 5.0310e-10 - val_loss: 5.1976e-10
Epoch 505/512
448/448 - 0s - loss: 5.0383e-10 - val_loss: 5.1792e-10
Epoch 506/512
448/448 - 0s - loss: 5.0293e-10 - val_loss: 5.1617e-10
Epoch 507/512
448/448 - 0s - loss: 4.9802e-10 - val_loss: 5.1334e-10
Epoch 508/512
448/448 - 0s - loss: 4.9068e-10 - val_loss: 5.1177e-10
Epoch 509/512
448/448 - 0s - loss: 4.9012e-10 - val_loss: 5.0962e-10
Epoch 510/512
448/448 - 0s - loss: 4.8554e-10 - val_loss: 5.0809e-10
Epoch 511/512
448/448 - 0s - loss: 4.8457e-10 - val_loss: 5.0667e-10
Epoch 512/512
448/448 - 0s - loss: 4.8893e-10 - val_loss: 5.0417e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.0091e-09 - val_loss: 2.2657e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4590e-08 - val_loss: 1.9604e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2014e-09 - val_loss: 4.1752e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.7087e-10 - val_loss: 3.6346e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6967e-10 - val_loss: 9.2432e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7243e-09 - val_loss: 4.5242e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5419e-09 - val_loss: 6.8681e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7089e-09 - val_loss: 2.7300e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2342e-09 - val_loss: 1.4716e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4835e-09 - val_loss: 1.6284e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9972e-09 - val_loss: 2.8065e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4186e-09 - val_loss: 3.9265e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9570e-09 - val_loss: 3.1129e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8612e-09 - val_loss: 2.1864e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1415e-09 - val_loss: 1.9465e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0796e-09 - val_loss: 2.2353e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4765e-09 - val_loss: 2.6351e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7744e-09 - val_loss: 2.6211e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6142e-09 - val_loss: 2.2644e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2466e-09 - val_loss: 2.0166e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0709e-09 - val_loss: 1.9985e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1017e-09 - val_loss: 2.1080e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2182e-09 - val_loss: 2.1457e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1951e-09 - val_loss: 2.0412e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0608e-09 - val_loss: 1.8953e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9291e-09 - val_loss: 1.8327e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8931e-09 - val_loss: 1.8366e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8994e-09 - val_loss: 1.8241e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8845e-09 - val_loss: 1.7933e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8385e-09 - val_loss: 1.7288e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7658e-09 - val_loss: 1.6615e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7035e-09 - val_loss: 1.6425e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6770e-09 - val_loss: 1.6339e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6731e-09 - val_loss: 1.5934e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6289e-09 - val_loss: 1.5584e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5879e-09 - val_loss: 1.5114e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5421e-09 - val_loss: 1.4595e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4924e-09 - val_loss: 1.4408e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4831e-09 - val_loss: 1.4334e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4591e-09 - val_loss: 1.4073e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4361e-09 - val_loss: 1.3697e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3991e-09 - val_loss: 1.3220e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3464e-09 - val_loss: 1.2832e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3207e-09 - val_loss: 1.2645e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2993e-09 - val_loss: 1.2553e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2851e-09 - val_loss: 1.2415e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2709e-09 - val_loss: 1.2201e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2396e-09 - val_loss: 1.1834e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2069e-09 - val_loss: 1.1485e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1796e-09 - val_loss: 1.1196e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1437e-09 - val_loss: 1.0982e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1260e-09 - val_loss: 1.1039e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1327e-09 - val_loss: 1.0895e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1109e-09 - val_loss: 1.0649e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0842e-09 - val_loss: 1.0246e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0458e-09 - val_loss: 1.0088e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0340e-09 - val_loss: 9.9234e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0122e-09 - val_loss: 9.8113e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0006e-09 - val_loss: 9.6040e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7859e-10 - val_loss: 9.3959e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5891e-10 - val_loss: 9.2666e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5023e-10 - val_loss: 8.9756e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2179e-10 - val_loss: 9.0549e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2680e-10 - val_loss: 9.1049e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2355e-10 - val_loss: 8.8681e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0008e-10 - val_loss: 8.5214e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6811e-10 - val_loss: 8.3513e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5094e-10 - val_loss: 8.1965e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3987e-10 - val_loss: 8.0930e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2574e-10 - val_loss: 7.9989e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1402e-10 - val_loss: 7.8955e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0578e-10 - val_loss: 7.8674e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0567e-10 - val_loss: 7.8608e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9876e-10 - val_loss: 7.5981e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6861e-10 - val_loss: 7.4344e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5448e-10 - val_loss: 7.2869e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4179e-10 - val_loss: 7.1848e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3079e-10 - val_loss: 7.0293e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1344e-10 - val_loss: 6.9257e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0801e-10 - val_loss: 7.0183e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1747e-10 - val_loss: 6.9547e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0479e-10 - val_loss: 6.7987e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8722e-10 - val_loss: 6.5787e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6877e-10 - val_loss: 6.3803e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5115e-10 - val_loss: 6.3039e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4383e-10 - val_loss: 6.2380e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3709e-10 - val_loss: 6.2906e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4072e-10 - val_loss: 6.3106e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4123e-10 - val_loss: 6.1462e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2477e-10 - val_loss: 5.9829e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0596e-10 - val_loss: 5.7947e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8972e-10 - val_loss: 5.8002e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9349e-10 - val_loss: 5.7749e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8731e-10 - val_loss: 5.7753e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8708e-10 - val_loss: 5.7308e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7983e-10 - val_loss: 5.6427e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6927e-10 - val_loss: 5.4882e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5521e-10 - val_loss: 5.3876e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4988e-10 - val_loss: 5.3258e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3704e-10 - val_loss: 5.2256e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2957e-10 - val_loss: 5.1847e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3021e-10 - val_loss: 5.2349e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3399e-10 - val_loss: 5.2126e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3109e-10 - val_loss: 5.1392e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1706e-10 - val_loss: 4.9701e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0431e-10 - val_loss: 4.8461e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8932e-10 - val_loss: 4.7678e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8781e-10 - val_loss: 4.8304e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9237e-10 - val_loss: 4.8296e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9187e-10 - val_loss: 4.8106e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8655e-10 - val_loss: 4.7380e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7577e-10 - val_loss: 4.5914e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6356e-10 - val_loss: 4.4748e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5381e-10 - val_loss: 4.4451e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5203e-10 - val_loss: 4.4396e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4824e-10 - val_loss: 4.3796e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4410e-10 - val_loss: 4.3168e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3720e-10 - val_loss: 4.2395e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2966e-10 - val_loss: 4.2621e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3451e-10 - val_loss: 4.3005e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3699e-10 - val_loss: 4.2446e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3089e-10 - val_loss: 4.1735e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1864e-10 - val_loss: 4.0500e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1024e-10 - val_loss: 3.9855e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0526e-10 - val_loss: 3.9803e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0555e-10 - val_loss: 3.9777e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0512e-10 - val_loss: 3.9634e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0141e-10 - val_loss: 3.9428e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9949e-10 - val_loss: 3.8418e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8651e-10 - val_loss: 3.7559e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8173e-10 - val_loss: 3.7619e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7945e-10 - val_loss: 3.6885e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7376e-10 - val_loss: 3.7291e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7802e-10 - val_loss: 3.7225e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7663e-10 - val_loss: 3.7084e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.7333e-10 - val_loss: 3.6239e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.6497e-10 - val_loss: 3.5419e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.5761e-10 - val_loss: 3.5209e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.5693e-10 - val_loss: 3.4477e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4929e-10 - val_loss: 3.4676e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5327e-10 - val_loss: 3.5329e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5767e-10 - val_loss: 3.4596e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.4846e-10 - val_loss: 3.4289e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.4625e-10 - val_loss: 3.4077e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.4510e-10 - val_loss: 3.3477e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.3612e-10 - val_loss: 3.2533e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2560e-10 - val_loss: 3.1976e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2517e-10 - val_loss: 3.1836e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2313e-10 - val_loss: 3.1954e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2365e-10 - val_loss: 3.1634e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2060e-10 - val_loss: 3.1347e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.1661e-10 - val_loss: 3.1159e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.1354e-10 - val_loss: 3.0688e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0968e-10 - val_loss: 3.0299e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0673e-10 - val_loss: 3.0326e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0819e-10 - val_loss: 3.0262e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0716e-10 - val_loss: 3.0199e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0581e-10 - val_loss: 3.0057e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0246e-10 - val_loss: 2.9324e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9420e-10 - val_loss: 2.9036e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9250e-10 - val_loss: 2.8421e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8823e-10 - val_loss: 2.8350e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8640e-10 - val_loss: 2.8294e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8508e-10 - val_loss: 2.8043e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8416e-10 - val_loss: 2.8325e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8555e-10 - val_loss: 2.8068e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8248e-10 - val_loss: 2.7687e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7985e-10 - val_loss: 2.7783e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8092e-10 - val_loss: 2.7542e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.7695e-10 - val_loss: 2.7102e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.7274e-10 - val_loss: 2.6670e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6986e-10 - val_loss: 2.6406e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6665e-10 - val_loss: 2.5920e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6147e-10 - val_loss: 2.6007e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6455e-10 - val_loss: 2.6302e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6491e-10 - val_loss: 2.5852e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6345e-10 - val_loss: 2.6263e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6489e-10 - val_loss: 2.5659e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5877e-10 - val_loss: 2.5709e-10
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6159e-10 - val_loss: 2.5950e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6074e-10 - val_loss: 2.5424e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.5570e-10 - val_loss: 2.4586e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4752e-10 - val_loss: 2.4493e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4576e-10 - val_loss: 2.4326e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4628e-10 - val_loss: 2.3995e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4293e-10 - val_loss: 2.3935e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4125e-10 - val_loss: 2.4016e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4257e-10 - val_loss: 2.3793e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4014e-10 - val_loss: 2.3713e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3961e-10 - val_loss: 2.4045e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4127e-10 - val_loss: 2.3644e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3909e-10 - val_loss: 2.3175e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3134e-10 - val_loss: 2.2947e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3206e-10 - val_loss: 2.2703e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3002e-10 - val_loss: 2.2732e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2888e-10 - val_loss: 2.2246e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2506e-10 - val_loss: 2.1929e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2213e-10 - val_loss: 2.1908e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2162e-10 - val_loss: 2.2347e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2646e-10 - val_loss: 2.2396e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2469e-10 - val_loss: 2.2088e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2152e-10 - val_loss: 2.1674e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1785e-10 - val_loss: 2.1712e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.1706e-10 - val_loss: 2.1256e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1402e-10 - val_loss: 2.1361e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1621e-10 - val_loss: 2.1484e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1663e-10 - val_loss: 2.1699e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1758e-10 - val_loss: 2.1355e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1445e-10 - val_loss: 2.1436e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1562e-10 - val_loss: 2.1297e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.1368e-10 - val_loss: 2.0754e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0935e-10 - val_loss: 2.0759e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0871e-10 - val_loss: 2.0522e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0677e-10 - val_loss: 2.0391e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0461e-10 - val_loss: 2.0061e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0088e-10 - val_loss: 1.9790e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9875e-10 - val_loss: 1.9675e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9823e-10 - val_loss: 1.9339e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9568e-10 - val_loss: 1.9257e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9383e-10 - val_loss: 1.9293e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9342e-10 - val_loss: 1.9111e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9266e-10 - val_loss: 1.9027e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9213e-10 - val_loss: 1.9399e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9576e-10 - val_loss: 1.9259e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9436e-10 - val_loss: 1.9149e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9352e-10 - val_loss: 1.9186e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9301e-10 - val_loss: 1.9065e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9174e-10 - val_loss: 1.8946e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9019e-10 - val_loss: 1.8653e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8858e-10 - val_loss: 1.8550e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8717e-10 - val_loss: 1.8497e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8478e-10 - val_loss: 1.8022e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8195e-10 - val_loss: 1.7893e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8029e-10 - val_loss: 1.7809e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7855e-10 - val_loss: 1.7669e-10
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8006e-10 - val_loss: 1.8158e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8145e-10 - val_loss: 1.7776e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7922e-10 - val_loss: 1.7612e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7610e-10 - val_loss: 1.7156e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7260e-10 - val_loss: 1.7111e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7324e-10 - val_loss: 1.7212e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7266e-10 - val_loss: 1.7046e-10
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7155e-10 - val_loss: 1.7327e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7429e-10 - val_loss: 1.7448e-10
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7544e-10 - val_loss: 1.7446e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7587e-10 - val_loss: 1.7287e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7196e-10 - val_loss: 1.6677e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6743e-10 - val_loss: 1.6558e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6737e-10 - val_loss: 1.6646e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6794e-10 - val_loss: 1.6675e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6739e-10 - val_loss: 1.6671e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6714e-10 - val_loss: 1.6237e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6343e-10 - val_loss: 1.6144e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6211e-10 - val_loss: 1.6128e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6201e-10 - val_loss: 1.6162e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6310e-10 - val_loss: 1.6110e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6256e-10 - val_loss: 1.6188e-10
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6346e-10 - val_loss: 1.6190e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6301e-10 - val_loss: 1.6112e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6012e-10 - val_loss: 1.5754e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5931e-10 - val_loss: 1.5644e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5846e-10 - val_loss: 1.5869e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6023e-10 - val_loss: 1.5729e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5678e-10 - val_loss: 1.5480e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5614e-10 - val_loss: 1.5622e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5633e-10 - val_loss: 1.5244e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5374e-10 - val_loss: 1.5278e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5213e-10 - val_loss: 1.4778e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4798e-10 - val_loss: 1.4622e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4843e-10 - val_loss: 1.4949e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5174e-10 - val_loss: 1.5316e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5578e-10 - val_loss: 1.5665e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5791e-10 - val_loss: 1.5716e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5705e-10 - val_loss: 1.5191e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5186e-10 - val_loss: 1.4837e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4994e-10 - val_loss: 1.4889e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4887e-10 - val_loss: 1.4647e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4681e-10 - val_loss: 1.4410e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4516e-10 - val_loss: 1.4421e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4545e-10 - val_loss: 1.4289e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4260e-10 - val_loss: 1.4029e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4226e-10 - val_loss: 1.4383e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4470e-10 - val_loss: 1.4381e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4573e-10 - val_loss: 1.4488e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4551e-10 - val_loss: 1.4266e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4251e-10 - val_loss: 1.4153e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4205e-10 - val_loss: 1.4023e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4067e-10 - val_loss: 1.3971e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4044e-10 - val_loss: 1.3912e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3962e-10 - val_loss: 1.3692e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3754e-10 - val_loss: 1.3824e-10
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3891e-10 - val_loss: 1.3776e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3905e-10 - val_loss: 1.3834e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3969e-10 - val_loss: 1.4115e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4130e-10 - val_loss: 1.3820e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3792e-10 - val_loss: 1.3660e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3772e-10 - val_loss: 1.3760e-10
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3867e-10 - val_loss: 1.3625e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3713e-10 - val_loss: 1.3622e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3727e-10 - val_loss: 1.3428e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3515e-10 - val_loss: 1.3329e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3325e-10 - val_loss: 1.3188e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3363e-10 - val_loss: 1.3302e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3367e-10 - val_loss: 1.3262e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3189e-10 - val_loss: 1.2950e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3037e-10 - val_loss: 1.2871e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2960e-10 - val_loss: 1.2848e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2936e-10 - val_loss: 1.2819e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2987e-10 - val_loss: 1.3068e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3118e-10 - val_loss: 1.2895e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2859e-10 - val_loss: 1.2770e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2877e-10 - val_loss: 1.2957e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2952e-10 - val_loss: 1.2831e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2796e-10 - val_loss: 1.2679e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2792e-10 - val_loss: 1.2862e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2818e-10 - val_loss: 1.2490e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2572e-10 - val_loss: 1.2251e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2221e-10 - val_loss: 1.2055e-10
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2149e-10 - val_loss: 1.2093e-10
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2157e-10 - val_loss: 1.2108e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2118e-10 - val_loss: 1.1920e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2071e-10 - val_loss: 1.2185e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2300e-10 - val_loss: 1.2356e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2501e-10 - val_loss: 1.2527e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2565e-10 - val_loss: 1.2484e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2507e-10 - val_loss: 1.2240e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2383e-10 - val_loss: 1.2484e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2475e-10 - val_loss: 1.2246e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2270e-10 - val_loss: 1.2263e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2402e-10 - val_loss: 1.2217e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2229e-10 - val_loss: 1.2140e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2217e-10 - val_loss: 1.2168e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2204e-10 - val_loss: 1.1859e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1904e-10 - val_loss: 1.1813e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1918e-10 - val_loss: 1.1802e-10
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1836e-10 - val_loss: 1.1558e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1577e-10 - val_loss: 1.1324e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1469e-10 - val_loss: 1.1518e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1628e-10 - val_loss: 1.1489e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1492e-10 - val_loss: 1.1471e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1512e-10 - val_loss: 1.1471e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1609e-10 - val_loss: 1.1568e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1577e-10 - val_loss: 1.1470e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1483e-10 - val_loss: 1.1390e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1502e-10 - val_loss: 1.1633e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1822e-10 - val_loss: 1.1923e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1836e-10 - val_loss: 1.1448e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1527e-10 - val_loss: 1.1473e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1543e-10 - val_loss: 1.1423e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1451e-10 - val_loss: 1.1425e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1485e-10 - val_loss: 1.1312e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1461e-10 - val_loss: 1.1536e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1576e-10 - val_loss: 1.1345e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1309e-10 - val_loss: 1.1152e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1141e-10 - val_loss: 1.0939e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0960e-10 - val_loss: 1.0888e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1001e-10 - val_loss: 1.0959e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1096e-10 - val_loss: 1.1091e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1218e-10 - val_loss: 1.1259e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1399e-10 - val_loss: 1.1292e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1290e-10 - val_loss: 1.1103e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1047e-10 - val_loss: 1.0931e-10
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0815e-10 - val_loss: 1.0483e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0464e-10 - val_loss: 1.0445e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0588e-10 - val_loss: 1.0634e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0742e-10 - val_loss: 1.0796e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0883e-10 - val_loss: 1.0938e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1111e-10 - val_loss: 1.1013e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1093e-10 - val_loss: 1.1013e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1030e-10 - val_loss: 1.0875e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0961e-10 - val_loss: 1.1014e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1061e-10 - val_loss: 1.1017e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1046e-10 - val_loss: 1.0925e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0862e-10 - val_loss: 1.0571e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0570e-10 - val_loss: 1.0464e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0559e-10 - val_loss: 1.0652e-10
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0714e-10 - val_loss: 1.0832e-10
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0870e-10 - val_loss: 1.0751e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0764e-10 - val_loss: 1.0717e-10
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0819e-10 - val_loss: 1.0682e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0722e-10 - val_loss: 1.0702e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0729e-10 - val_loss: 1.0477e-10
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0393e-10 - val_loss: 1.0199e-10
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0221e-10 - val_loss: 1.0171e-10
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0237e-10 - val_loss: 1.0180e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0230e-10 - val_loss: 1.0328e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0363e-10 - val_loss: 1.0267e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0310e-10 - val_loss: 1.0355e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0418e-10 - val_loss: 1.0263e-10
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0199e-10 - val_loss: 9.8596e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.8369e-11 - val_loss: 9.8314e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.8289e-11 - val_loss: 9.8021e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.8195e-11 - val_loss: 9.5840e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.5092e-11 - val_loss: 9.4574e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6178e-11 - val_loss: 9.5950e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6236e-11 - val_loss: 9.5966e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7217e-11 - val_loss: 9.7415e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6967e-11 - val_loss: 9.6647e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7715e-11 - val_loss: 9.7400e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8157e-11 - val_loss: 9.6801e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7798e-11 - val_loss: 9.8368e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8367e-11 - val_loss: 9.6984e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7891e-11 - val_loss: 9.8908e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9897e-11 - val_loss: 1.0006e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9188e-11 - val_loss: 9.5557e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.4924e-11 - val_loss: 9.2163e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2405e-11 - val_loss: 9.2677e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3156e-11 - val_loss: 9.2600e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3704e-11 - val_loss: 9.5145e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6508e-11 - val_loss: 9.7859e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8398e-11 - val_loss: 9.7452e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7302e-11 - val_loss: 9.4535e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4585e-11 - val_loss: 9.3756e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4519e-11 - val_loss: 9.5772e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6616e-11 - val_loss: 9.5728e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6518e-11 - val_loss: 9.6099e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5062e-11 - val_loss: 9.3348e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2810e-11 - val_loss: 9.2697e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3364e-11 - val_loss: 9.4333e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4707e-11 - val_loss: 9.4131e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4838e-11 - val_loss: 9.3617e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.3055e-11 - val_loss: 9.1231e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.1357e-11 - val_loss: 8.9219e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9224e-11 - val_loss: 8.9758e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0676e-11 - val_loss: 9.0974e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1355e-11 - val_loss: 9.0872e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1132e-11 - val_loss: 9.0126e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0707e-11 - val_loss: 9.0848e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1156e-11 - val_loss: 9.0563e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1362e-11 - val_loss: 9.1763e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2470e-11 - val_loss: 9.1226e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1198e-11 - val_loss: 8.9922e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0252e-11 - val_loss: 9.0381e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.0092e-11 - val_loss: 8.8245e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.8303e-11 - val_loss: 8.7651e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8680e-11 - val_loss: 8.9045e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9688e-11 - val_loss: 8.9836e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9999e-11 - val_loss: 9.0313e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0787e-11 - val_loss: 8.8349e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.8396e-11 - val_loss: 8.6968e-11
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.7347e-11 - val_loss: 8.6863e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7608e-11 - val_loss: 8.7054e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7373e-11 - val_loss: 8.7405e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7705e-11 - val_loss: 8.7359e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7568e-11 - val_loss: 8.7489e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8158e-11 - val_loss: 8.7512e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8202e-11 - val_loss: 8.7060e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.7362e-11 - val_loss: 8.6818e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.7482e-11 - val_loss: 8.6470e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6452e-11 - val_loss: 8.6560e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6979e-11 - val_loss: 8.6671e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7042e-11 - val_loss: 8.7283e-11
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.6727e-11 - val_loss: 8.6361e-11
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.5944e-11 - val_loss: 8.3988e-11
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.3901e-11 - val_loss: 8.3836e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4681e-11 - val_loss: 8.5724e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5947e-11 - val_loss: 8.5410e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5411e-11 - val_loss: 8.3861e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.4081e-11 - val_loss: 8.3426e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3979e-11 - val_loss: 8.4599e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5126e-11 - val_loss: 8.4480e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4673e-11 - val_loss: 8.4121e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4768e-11 - val_loss: 8.4825e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5106e-11 - val_loss: 8.3922e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.3508e-11 - val_loss: 8.1228e-11
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.1112e-11 - val_loss: 8.0755e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.0256e-11 - val_loss: 8.0125e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1217e-11 - val_loss: 8.1567e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2797e-11 - val_loss: 8.4190e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4602e-11 - val_loss: 8.4661e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4455e-11 - val_loss: 8.3105e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3074e-11 - val_loss: 8.2914e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3394e-11 - val_loss: 8.3478e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3346e-11 - val_loss: 8.2981e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3105e-11 - val_loss: 8.3308e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4238e-11 - val_loss: 8.3924e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3892e-11 - val_loss: 8.1870e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1352e-11 - val_loss: 8.0461e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.0485e-11 - val_loss: 7.8835e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8757e-11 - val_loss: 7.9014e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9810e-11 - val_loss: 8.1232e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1552e-11 - val_loss: 8.1723e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1904e-11 - val_loss: 8.1667e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1561e-11 - val_loss: 7.9031e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8974e-11 - val_loss: 7.8870e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.9039e-11 - val_loss: 7.7604e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.7644e-11 - val_loss: 7.7482e-11
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.7547e-11 - val_loss: 7.6954e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6887e-11 - val_loss: 7.7850e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8492e-11 - val_loss: 7.9194e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9502e-11 - val_loss: 8.0213e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9776e-11 - val_loss: 7.8698e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8439e-11 - val_loss: 7.7633e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6990e-11 - val_loss: 7.7124e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7470e-11 - val_loss: 7.7465e-11
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.7743e-11 - val_loss: 7.6387e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6827e-11 - val_loss: 7.6577e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7459e-11 - val_loss: 7.8236e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8366e-11 - val_loss: 7.8171e-11
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.7710e-11 - val_loss: 7.6164e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.6574e-11 - val_loss: 7.6153e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.6482e-11 - val_loss: 7.5890e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.5780e-11 - val_loss: 7.5295e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5531e-11 - val_loss: 7.5324e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.5314e-11 - val_loss: 7.5001e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5528e-11 - val_loss: 7.5309e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5800e-11 - val_loss: 7.5137e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5522e-11 - val_loss: 7.5266e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.5417e-11 - val_loss: 7.4872e-11
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.4633e-11 - val_loss: 7.3521e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3631e-11 - val_loss: 7.3580e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-curve-secp521r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.3444e-11 - val_loss: 7.3331e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.363 | eve: 9.673 | bob: 9.227Epoch   0:   0% | abe: 9.314 | eve: 9.669 | bob: 9.188Epoch   0:   1% | abe: 9.296 | eve: 9.669 | bob: 9.180Epoch   0:   2% | abe: 9.247 | eve: 9.670 | bob: 9.140Epoch   0:   2% | abe: 9.234 | eve: 9.665 | bob: 9.135Epoch   0:   3% | abe: 9.221 | eve: 9.665 | bob: 9.129Epoch   0:   4% | abe: 9.210 | eve: 9.662 | bob: 9.123Epoch   0:   4% | abe: 9.200 | eve: 9.669 | bob: 9.117Epoch   0:   5% | abe: 9.184 | eve: 9.677 | bob: 9.106Epoch   0:   6% | abe: 9.164 | eve: 9.676 | bob: 9.089Epoch   0:   6% | abe: 9.155 | eve: 9.677 | bob: 9.082Epoch   0:   7% | abe: 9.149 | eve: 9.683 | bob: 9.078Epoch   0:   8% | abe: 9.145 | eve: 9.685 | bob: 9.076Epoch   0:   8% | abe: 9.139 | eve: 9.689 | bob: 9.072Epoch   0:   9% | abe: 9.137 | eve: 9.695 | bob: 9.070Epoch   0:  10% | abe: 9.129 | eve: 9.693 | bob: 9.063Epoch   0:  10% | abe: 9.127 | eve: 9.694 | bob: 9.062Epoch   0:  11% | abe: 9.124 | eve: 9.696 | bob: 9.059Epoch   0:  12% | abe: 9.117 | eve: 9.698 | bob: 9.052Epoch   0:  13% | abe: 9.114 | eve: 9.703 | bob: 9.050Epoch   0:  13% | abe: 9.113 | eve: 9.705 | bob: 9.049Epoch   0:  14% | abe: 9.112 | eve: 9.711 | bob: 9.047Epoch   0:  15% | abe: 9.108 | eve: 9.711 | bob: 9.044Epoch   0:  15% | abe: 9.105 | eve: 9.718 | bob: 9.040Epoch   0:  16% | abe: 9.102 | eve: 9.720 | bob: 9.038Epoch   0:  17% | abe: 9.098 | eve: 9.720 | bob: 9.033Epoch   0:  17% | abe: 9.097 | eve: 9.723 | bob: 9.031Epoch   0:  18% | abe: 9.097 | eve: 9.721 | bob: 9.032Epoch   0:  19% | abe: 9.098 | eve: 9.723 | bob: 9.032Epoch   0:  19% | abe: 9.099 | eve: 9.725 | bob: 9.033Epoch   0:  20% | abe: 9.096 | eve: 9.728 | bob: 9.030Epoch   0:  21% | abe: 9.095 | eve: 9.730 | bob: 9.028Epoch   0:  21% | abe: 9.095 | eve: 9.733 | bob: 9.028Epoch   0:  22% | abe: 9.092 | eve: 9.740 | bob: 9.025Epoch   0:  23% | abe: 9.090 | eve: 9.742 | bob: 9.022Epoch   0:  23% | abe: 9.089 | eve: 9.745 | bob: 9.021Epoch   0:  24% | abe: 9.088 | eve: 9.746 | bob: 9.021Epoch   0:  25% | abe: 9.088 | eve: 9.750 | bob: 9.020Epoch   0:  26% | abe: 9.085 | eve: 9.754 | bob: 9.017Epoch   0:  26% | abe: 9.087 | eve: 9.755 | bob: 9.019Epoch   0:  27% | abe: 9.086 | eve: 9.754 | bob: 9.017Epoch   0:  28% | abe: 9.085 | eve: 9.757 | bob: 9.016Epoch   0:  28% | abe: 9.084 | eve: 9.760 | bob: 9.015Epoch   0:  29% | abe: 9.084 | eve: 9.761 | bob: 9.014Epoch   0:  30% | abe: 9.082 | eve: 9.764 | bob: 9.012Epoch   0:  30% | abe: 9.080 | eve: 9.764 | bob: 9.010Epoch   0:  31% | abe: 9.080 | eve: 9.767 | bob: 9.010Epoch   0:  32% | abe: 9.080 | eve: 9.769 | bob: 9.010Epoch   0:  32% | abe: 9.078 | eve: 9.770 | bob: 9.008Epoch   0:  33% | abe: 9.077 | eve: 9.773 | bob: 9.006Epoch   0:  34% | abe: 9.075 | eve: 9.773 | bob: 9.004Epoch   0:  34% | abe: 9.075 | eve: 9.775 | bob: 9.004Epoch   0:  35% | abe: 9.074 | eve: 9.776 | bob: 9.003Epoch   0:  36% | abe: 9.072 | eve: 9.777 | bob: 9.001Epoch   0:  36% | abe: 9.071 | eve: 9.778 | bob: 9.000Epoch   0:  37% | abe: 9.070 | eve: 9.780 | bob: 8.998Epoch   0:  38% | abe: 9.070 | eve: 9.780 | bob: 8.998Epoch   0:  39% | abe: 9.069 | eve: 9.780 | bob: 8.997Epoch   0:  39% | abe: 9.069 | eve: 9.782 | bob: 8.997Epoch   0:  40% | abe: 9.068 | eve: 9.783 | bob: 8.996Epoch   0:  41% | abe: 9.068 | eve: 9.784 | bob: 8.995Epoch   0:  41% | abe: 9.067 | eve: 9.785 | bob: 8.994Epoch   0:  42% | abe: 9.066 | eve: 9.785 | bob: 8.993Epoch   0:  43% | abe: 9.065 | eve: 9.786 | bob: 8.992Epoch   0:  43% | abe: 9.065 | eve: 9.788 | bob: 8.992Epoch   0:  44% | abe: 9.064 | eve: 9.789 | bob: 8.991Epoch   0:  45% | abe: 9.064 | eve: 9.790 | bob: 8.990Epoch   0:  45% | abe: 9.062 | eve: 9.790 | bob: 8.989Epoch   0:  46% | abe: 9.062 | eve: 9.791 | bob: 8.987Epoch   0:  47% | abe: 9.060 | eve: 9.792 | bob: 8.986Epoch   0:  47% | abe: 9.059 | eve: 9.793 | bob: 8.985Epoch   0:  48% | abe: 9.059 | eve: 9.795 | bob: 8.984Epoch   0:  49% | abe: 9.058 | eve: 9.796 | bob: 8.983Epoch   0:  50% | abe: 9.057 | eve: 9.796 | bob: 8.982Epoch   0:  50% | abe: 9.057 | eve: 9.798 | bob: 8.981Epoch   0:  51% | abe: 9.056 | eve: 9.798 | bob: 8.981Epoch   0:  52% | abe: 9.055 | eve: 9.799 | bob: 8.980Epoch   0:  52% | abe: 9.054 | eve: 9.799 | bob: 8.979Epoch   0:  53% | abe: 9.054 | eve: 9.800 | bob: 8.978Epoch   0:  54% | abe: 9.053 | eve: 9.800 | bob: 8.977Epoch   0:  54% | abe: 9.053 | eve: 9.801 | bob: 8.977Epoch   0:  55% | abe: 9.052 | eve: 9.802 | bob: 8.976Epoch   0:  56% | abe: 9.053 | eve: 9.803 | bob: 8.976Epoch   0:  56% | abe: 9.052 | eve: 9.804 | bob: 8.976Epoch   0:  57% | abe: 9.051 | eve: 9.804 | bob: 8.974Epoch   0:  58% | abe: 9.051 | eve: 9.804 | bob: 8.974Epoch   0:  58% | abe: 9.052 | eve: 9.805 | bob: 8.975Epoch   0:  59% | abe: 9.050 | eve: 9.805 | bob: 8.973Epoch   0:  60% | abe: 9.050 | eve: 9.806 | bob: 8.972Epoch   0:  60% | abe: 9.050 | eve: 9.806 | bob: 8.973Epoch   0:  61% | abe: 9.048 | eve: 9.806 | bob: 8.971Epoch   0:  62% | abe: 9.047 | eve: 9.807 | bob: 8.970Epoch   0:  63% | abe: 9.047 | eve: 9.807 | bob: 8.969Epoch   0:  63% | abe: 9.047 | eve: 9.807 | bob: 8.969Epoch   0:  64% | abe: 9.045 | eve: 9.809 | bob: 8.967Epoch   0:  65% | abe: 9.045 | eve: 9.809 | bob: 8.966Epoch   0:  65% | abe: 9.044 | eve: 9.809 | bob: 8.966Epoch   0:  66% | abe: 9.044 | eve: 9.809 | bob: 8.965Epoch   0:  67% | abe: 9.044 | eve: 9.810 | bob: 8.965Epoch   0:  67% | abe: 9.044 | eve: 9.810 | bob: 8.965Epoch   0:  68% | abe: 9.043 | eve: 9.810 | bob: 8.964Epoch   0:  69% | abe: 9.042 | eve: 9.810 | bob: 8.963Epoch   0:  69% | abe: 9.041 | eve: 9.811 | bob: 8.962Epoch   0:  70% | abe: 9.040 | eve: 9.811 | bob: 8.961Epoch   0:  71% | abe: 9.040 | eve: 9.812 | bob: 8.961Epoch   0:  71% | abe: 9.039 | eve: 9.812 | bob: 8.960Epoch   0:  72% | abe: 9.039 | eve: 9.812 | bob: 8.959Epoch   0:  73% | abe: 9.038 | eve: 9.813 | bob: 8.958Epoch   0:  73% | abe: 9.037 | eve: 9.814 | bob: 8.957Epoch   0:  74% | abe: 9.037 | eve: 9.814 | bob: 8.957Epoch   0:  75% | abe: 9.037 | eve: 9.814 | bob: 8.957Epoch   0:  76% | abe: 9.036 | eve: 9.814 | bob: 8.956Epoch   0:  76% | abe: 9.036 | eve: 9.814 | bob: 8.956Epoch   0:  77% | abe: 9.035 | eve: 9.814 | bob: 8.954Epoch   0:  78% | abe: 9.034 | eve: 9.815 | bob: 8.954Epoch   0:  78% | abe: 9.034 | eve: 9.815 | bob: 8.953Epoch   0:  79% | abe: 9.033 | eve: 9.816 | bob: 8.953Epoch   0:  80% | abe: 9.033 | eve: 9.816 | bob: 8.952Epoch   0:  80% | abe: 9.032 | eve: 9.817 | bob: 8.952Epoch   0:  81% | abe: 9.032 | eve: 9.817 | bob: 8.951Epoch   0:  82% | abe: 9.032 | eve: 9.817 | bob: 8.951Epoch   0:  82% | abe: 9.031 | eve: 9.817 | bob: 8.950Epoch   0:  83% | abe: 9.030 | eve: 9.817 | bob: 8.949Epoch   0:  84% | abe: 9.029 | eve: 9.818 | bob: 8.948Epoch   0:  84% | abe: 9.029 | eve: 9.818 | bob: 8.948Epoch   0:  85% | abe: 9.029 | eve: 9.819 | bob: 8.948Epoch   0:  86% | abe: 9.028 | eve: 9.819 | bob: 8.947Epoch   0:  86% | abe: 9.028 | eve: 9.819 | bob: 8.947Epoch   0:  87% | abe: 9.028 | eve: 9.819 | bob: 8.947Epoch   0:  88% | abe: 9.028 | eve: 9.819 | bob: 8.946Epoch   0:  89% | abe: 9.028 | eve: 9.820 | bob: 8.946Epoch   0:  89% | abe: 9.027 | eve: 9.820 | bob: 8.945Epoch   0:  90% | abe: 9.027 | eve: 9.820 | bob: 8.945Epoch   0:  91% | abe: 9.026 | eve: 9.821 | bob: 8.944Epoch   0:  91% | abe: 9.025 | eve: 9.821 | bob: 8.943Epoch   0:  92% | abe: 9.025 | eve: 9.822 | bob: 8.943Epoch   0:  93% | abe: 9.024 | eve: 9.822 | bob: 8.942Epoch   0:  93% | abe: 9.024 | eve: 9.822 | bob: 8.942Epoch   0:  94% | abe: 9.024 | eve: 9.823 | bob: 8.941Epoch   0:  95% | abe: 9.023 | eve: 9.824 | bob: 8.941Epoch   0:  95% | abe: 9.023 | eve: 9.824 | bob: 8.941Epoch   0:  96% | abe: 9.022 | eve: 9.824 | bob: 8.940Epoch   0:  97% | abe: 9.022 | eve: 9.825 | bob: 8.939Epoch   0:  97% | abe: 9.022 | eve: 9.824 | bob: 8.939Epoch   0:  98% | abe: 9.022 | eve: 9.824 | bob: 8.939Epoch   0:  99% | abe: 9.021 | eve: 9.825 | bob: 8.939
New best Bob loss 8.938514652042313 at epoch 0
Epoch   1:   0% | abe: 8.970 | eve: 9.799 | bob: 8.881Epoch   1:   0% | abe: 8.951 | eve: 9.830 | bob: 8.861Epoch   1:   1% | abe: 8.978 | eve: 9.825 | bob: 8.889Epoch   1:   2% | abe: 8.973 | eve: 9.815 | bob: 8.885Epoch   1:   2% | abe: 8.966 | eve: 9.817 | bob: 8.878Epoch   1:   3% | abe: 8.961 | eve: 9.817 | bob: 8.873Epoch   1:   4% | abe: 8.956 | eve: 9.813 | bob: 8.868Epoch   1:   4% | abe: 8.944 | eve: 9.826 | bob: 8.855Epoch   1:   5% | abe: 8.943 | eve: 9.835 | bob: 8.854Epoch   1:   6% | abe: 8.946 | eve: 9.830 | bob: 8.857Epoch   1:   6% | abe: 8.946 | eve: 9.827 | bob: 8.856Epoch   1:   7% | abe: 8.942 | eve: 9.830 | bob: 8.853Epoch   1:   8% | abe: 8.940 | eve: 9.830 | bob: 8.850Epoch   1:   8% | abe: 8.946 | eve: 9.830 | bob: 8.856Epoch   1:   9% | abe: 8.948 | eve: 9.832 | bob: 8.859Epoch   1:  10% | abe: 8.950 | eve: 9.831 | bob: 8.861Epoch   1:  10% | abe: 8.948 | eve: 9.834 | bob: 8.858Epoch   1:  11% | abe: 8.948 | eve: 9.835 | bob: 8.859Epoch   1:  12% | abe: 8.949 | eve: 9.837 | bob: 8.860Epoch   1:  13% | abe: 8.947 | eve: 9.836 | bob: 8.857Epoch   1:  13% | abe: 8.945 | eve: 9.835 | bob: 8.855Epoch   1:  14% | abe: 8.942 | eve: 9.833 | bob: 8.852Epoch   1:  15% | abe: 8.940 | eve: 9.834 | bob: 8.851Epoch   1:  15% | abe: 8.939 | eve: 9.838 | bob: 8.849Epoch   1:  16% | abe: 8.939 | eve: 9.839 | bob: 8.850Epoch   1:  17% | abe: 8.937 | eve: 9.837 | bob: 8.848Epoch   1:  17% | abe: 8.937 | eve: 9.836 | bob: 8.847Epoch   1:  18% | abe: 8.937 | eve: 9.837 | bob: 8.847Epoch   1:  19% | abe: 8.936 | eve: 9.841 | bob: 8.847Epoch   1:  19% | abe: 8.936 | eve: 9.839 | bob: 8.846Epoch   1:  20% | abe: 8.935 | eve: 9.836 | bob: 8.846Epoch   1:  21% | abe: 8.936 | eve: 9.836 | bob: 8.847Epoch   1:  21% | abe: 8.937 | eve: 9.836 | bob: 8.848Epoch   1:  22% | abe: 8.936 | eve: 9.837 | bob: 8.847Epoch   1:  23% | abe: 8.935 | eve: 9.837 | bob: 8.846Epoch   1:  23% | abe: 8.935 | eve: 9.838 | bob: 8.846Epoch   1:  24% | abe: 8.936 | eve: 9.838 | bob: 8.846Epoch   1:  25% | abe: 8.933 | eve: 9.838 | bob: 8.844Epoch   1:  26% | abe: 8.933 | eve: 9.839 | bob: 8.844Epoch   1:  26% | abe: 8.932 | eve: 9.840 | bob: 8.843Epoch   1:  27% | abe: 8.932 | eve: 9.841 | bob: 8.843Epoch   1:  28% | abe: 8.931 | eve: 9.842 | bob: 8.842Epoch   1:  28% | abe: 8.931 | eve: 9.844 | bob: 8.843Epoch   1:  29% | abe: 8.932 | eve: 9.843 | bob: 8.843Epoch   1:  30% | abe: 8.933 | eve: 9.845 | bob: 8.844Epoch   1:  30% | abe: 8.932 | eve: 9.846 | bob: 8.843Epoch   1:  31% | abe: 8.932 | eve: 9.845 | bob: 8.843Epoch   1:  32% | abe: 8.933 | eve: 9.846 | bob: 8.845Epoch   1:  32% | abe: 8.934 | eve: 9.847 | bob: 8.846Epoch   1:  33% | abe: 8.935 | eve: 9.847 | bob: 8.846Epoch   1:  34% | abe: 8.935 | eve: 9.847 | bob: 8.847Epoch   1:  34% | abe: 8.935 | eve: 9.847 | bob: 8.847Epoch   1:  35% | abe: 8.935 | eve: 9.847 | bob: 8.847Epoch   1:  36% | abe: 8.935 | eve: 9.847 | bob: 8.847Epoch   1:  36% | abe: 8.935 | eve: 9.847 | bob: 8.847Epoch   1:  37% | abe: 8.935 | eve: 9.849 | bob: 8.847Epoch   1:  38% | abe: 8.934 | eve: 9.850 | bob: 8.846Epoch   1:  39% | abe: 8.934 | eve: 9.851 | bob: 8.846Epoch   1:  39% | abe: 8.934 | eve: 9.853 | bob: 8.846Epoch   1:  40% | abe: 8.934 | eve: 9.851 | bob: 8.847Epoch   1:  41% | abe: 8.933 | eve: 9.852 | bob: 8.846Epoch   1:  41% | abe: 8.934 | eve: 9.853 | bob: 8.846Epoch   1:  42% | abe: 8.932 | eve: 9.853 | bob: 8.845Epoch   1:  43% | abe: 8.932 | eve: 9.853 | bob: 8.844Epoch   1:  43% | abe: 8.931 | eve: 9.853 | bob: 8.844Epoch   1:  44% | abe: 8.931 | eve: 9.853 | bob: 8.843Epoch   1:  45% | abe: 8.930 | eve: 9.855 | bob: 8.843Epoch   1:  45% | abe: 8.930 | eve: 9.854 | bob: 8.842Epoch   1:  46% | abe: 8.928 | eve: 9.855 | bob: 8.841Epoch   1:  47% | abe: 8.928 | eve: 9.854 | bob: 8.840Epoch   1:  47% | abe: 8.927 | eve: 9.854 | bob: 8.840Epoch   1:  48% | abe: 8.926 | eve: 9.854 | bob: 8.839Epoch   1:  49% | abe: 8.926 | eve: 9.853 | bob: 8.839Epoch   1:  50% | abe: 8.925 | eve: 9.852 | bob: 8.838Epoch   1:  50% | abe: 8.925 | eve: 9.854 | bob: 8.838Epoch   1:  51% | abe: 8.925 | eve: 9.855 | bob: 8.838Epoch   1:  52% | abe: 8.924 | eve: 9.855 | bob: 8.837Epoch   1:  52% | abe: 8.925 | eve: 9.855 | bob: 8.838Epoch   1:  53% | abe: 8.924 | eve: 9.855 | bob: 8.837Epoch   1:  54% | abe: 8.923 | eve: 9.855 | bob: 8.836Epoch   1:  54% | abe: 8.923 | eve: 9.856 | bob: 8.837Epoch   1:  55% | abe: 8.923 | eve: 9.857 | bob: 8.836Epoch   1:  56% | abe: 8.922 | eve: 9.858 | bob: 8.835Epoch   1:  56% | abe: 8.922 | eve: 9.859 | bob: 8.835Epoch   1:  57% | abe: 8.922 | eve: 9.860 | bob: 8.835Epoch   1:  58% | abe: 8.922 | eve: 9.860 | bob: 8.836Epoch   1:  58% | abe: 8.922 | eve: 9.860 | bob: 8.835Epoch   1:  59% | abe: 8.921 | eve: 9.860 | bob: 8.835Epoch   1:  60% | abe: 8.920 | eve: 9.860 | bob: 8.834Epoch   1:  60% | abe: 8.920 | eve: 9.860 | bob: 8.833Epoch   1:  61% | abe: 8.919 | eve: 9.859 | bob: 8.833Epoch   1:  62% | abe: 8.920 | eve: 9.860 | bob: 8.833Epoch   1:  63% | abe: 8.919 | eve: 9.860 | bob: 8.832Epoch   1:  63% | abe: 8.918 | eve: 9.859 | bob: 8.832Epoch   1:  64% | abe: 8.919 | eve: 9.859 | bob: 8.832Epoch   1:  65% | abe: 8.918 | eve: 9.860 | bob: 8.832Epoch   1:  65% | abe: 8.917 | eve: 9.860 | bob: 8.831Epoch   1:  66% | abe: 8.917 | eve: 9.861 | bob: 8.831Epoch   1:  67% | abe: 8.917 | eve: 9.861 | bob: 8.831Epoch   1:  67% | abe: 8.916 | eve: 9.861 | bob: 8.830Epoch   1:  68% | abe: 8.916 | eve: 9.862 | bob: 8.830Epoch   1:  69% | abe: 8.916 | eve: 9.863 | bob: 8.830Epoch   1:  69% | abe: 8.916 | eve: 9.862 | bob: 8.830Epoch   1:  70% | abe: 8.916 | eve: 9.863 | bob: 8.830Epoch   1:  71% | abe: 8.916 | eve: 9.863 | bob: 8.830Epoch   1:  71% | abe: 8.915 | eve: 9.863 | bob: 8.830Epoch   1:  72% | abe: 8.916 | eve: 9.864 | bob: 8.830Epoch   1:  73% | abe: 8.915 | eve: 9.864 | bob: 8.830Epoch   1:  73% | abe: 8.915 | eve: 9.864 | bob: 8.829Epoch   1:  74% | abe: 8.915 | eve: 9.865 | bob: 8.829Epoch   1:  75% | abe: 8.915 | eve: 9.866 | bob: 8.830Epoch   1:  76% | abe: 8.915 | eve: 9.866 | bob: 8.830Epoch   1:  76% | abe: 8.914 | eve: 9.866 | bob: 8.829Epoch   1:  77% | abe: 8.914 | eve: 9.866 | bob: 8.829Epoch   1:  78% | abe: 8.914 | eve: 9.867 | bob: 8.829Epoch   1:  78% | abe: 8.914 | eve: 9.866 | bob: 8.829Epoch   1:  79% | abe: 8.913 | eve: 9.867 | bob: 8.828Epoch   1:  80% | abe: 8.913 | eve: 9.867 | bob: 8.828Epoch   1:  80% | abe: 8.913 | eve: 9.868 | bob: 8.828Epoch   1:  81% | abe: 8.913 | eve: 9.868 | bob: 8.828Epoch   1:  82% | abe: 8.913 | eve: 9.868 | bob: 8.828Epoch   1:  82% | abe: 8.913 | eve: 9.869 | bob: 8.828Epoch   1:  83% | abe: 8.912 | eve: 9.869 | bob: 8.827Epoch   1:  84% | abe: 8.912 | eve: 9.869 | bob: 8.827Epoch   1:  84% | abe: 8.911 | eve: 9.869 | bob: 8.827Epoch   1:  85% | abe: 8.911 | eve: 9.869 | bob: 8.827Epoch   1:  86% | abe: 8.911 | eve: 9.869 | bob: 8.827Epoch   1:  86% | abe: 8.910 | eve: 9.869 | bob: 8.826Epoch   1:  87% | abe: 8.910 | eve: 9.869 | bob: 8.825Epoch   1:  88% | abe: 8.910 | eve: 9.869 | bob: 8.825Epoch   1:  89% | abe: 8.909 | eve: 9.869 | bob: 8.825Epoch   1:  89% | abe: 8.909 | eve: 9.869 | bob: 8.825Epoch   1:  90% | abe: 8.909 | eve: 9.869 | bob: 8.825Epoch   1:  91% | abe: 8.909 | eve: 9.869 | bob: 8.824Epoch   1:  91% | abe: 8.908 | eve: 9.869 | bob: 8.824Epoch   1:  92% | abe: 8.908 | eve: 9.869 | bob: 8.823Epoch   1:  93% | abe: 8.907 | eve: 9.869 | bob: 8.823Epoch   1:  93% | abe: 8.907 | eve: 9.869 | bob: 8.823Epoch   1:  94% | abe: 8.907 | eve: 9.869 | bob: 8.823Epoch   1:  95% | abe: 8.907 | eve: 9.870 | bob: 8.823Epoch   1:  95% | abe: 8.907 | eve: 9.870 | bob: 8.823Epoch   1:  96% | abe: 8.906 | eve: 9.870 | bob: 8.822Epoch   1:  97% | abe: 8.906 | eve: 9.870 | bob: 8.822Epoch   1:  97% | abe: 8.906 | eve: 9.870 | bob: 8.823Epoch   1:  98% | abe: 8.906 | eve: 9.871 | bob: 8.823Epoch   1:  99% | abe: 8.906 | eve: 9.871 | bob: 8.822
New best Bob loss 8.822414731638279 at epoch 1
Epoch   2:   0% | abe: 8.856 | eve: 9.842 | bob: 8.780Epoch   2:   0% | abe: 8.850 | eve: 9.860 | bob: 8.772Epoch   2:   1% | abe: 8.847 | eve: 9.877 | bob: 8.769Epoch   2:   2% | abe: 8.863 | eve: 9.877 | bob: 8.786Epoch   2:   2% | abe: 8.850 | eve: 9.877 | bob: 8.772Epoch   2:   3% | abe: 8.847 | eve: 9.870 | bob: 8.770Epoch   2:   4% | abe: 8.854 | eve: 9.884 | bob: 8.777Epoch   2:   4% | abe: 8.857 | eve: 9.884 | bob: 8.780Epoch   2:   5% | abe: 8.855 | eve: 9.879 | bob: 8.778Epoch   2:   6% | abe: 8.857 | eve: 9.877 | bob: 8.780Epoch   2:   6% | abe: 8.856 | eve: 9.871 | bob: 8.779Epoch   2:   7% | abe: 8.853 | eve: 9.873 | bob: 8.776Epoch   2:   8% | abe: 8.852 | eve: 9.874 | bob: 8.775Epoch   2:   8% | abe: 8.855 | eve: 9.870 | bob: 8.778Epoch   2:   9% | abe: 8.856 | eve: 9.867 | bob: 8.779Epoch   2:  10% | abe: 8.856 | eve: 9.860 | bob: 8.779Epoch   2:  10% | abe: 8.854 | eve: 9.856 | bob: 8.777Epoch   2:  11% | abe: 8.857 | eve: 9.852 | bob: 8.780Epoch   2:  12% | abe: 8.857 | eve: 9.852 | bob: 8.781Epoch   2:  13% | abe: 8.861 | eve: 9.853 | bob: 8.785Epoch   2:  13% | abe: 8.862 | eve: 9.851 | bob: 8.786Epoch   2:  14% | abe: 8.862 | eve: 9.854 | bob: 8.785Epoch   2:  15% | abe: 8.862 | eve: 9.857 | bob: 8.785Epoch   2:  15% | abe: 8.862 | eve: 9.856 | bob: 8.785Epoch   2:  16% | abe: 8.860 | eve: 9.853 | bob: 8.784Epoch   2:  17% | abe: 8.859 | eve: 9.852 | bob: 8.783Epoch   2:  17% | abe: 8.858 | eve: 9.850 | bob: 8.782Epoch   2:  18% | abe: 8.857 | eve: 9.853 | bob: 8.781Epoch   2:  19% | abe: 8.855 | eve: 9.855 | bob: 8.779Epoch   2:  19% | abe: 8.857 | eve: 9.858 | bob: 8.780Epoch   2:  20% | abe: 8.857 | eve: 9.856 | bob: 8.781Epoch   2:  21% | abe: 8.858 | eve: 9.859 | bob: 8.781Epoch   2:  21% | abe: 8.857 | eve: 9.860 | bob: 8.781Epoch   2:  22% | abe: 8.856 | eve: 9.861 | bob: 8.779Epoch   2:  23% | abe: 8.854 | eve: 9.862 | bob: 8.778Epoch   2:  23% | abe: 8.852 | eve: 9.863 | bob: 8.775Epoch   2:  24% | abe: 8.851 | eve: 9.863 | bob: 8.774Epoch   2:  25% | abe: 8.851 | eve: 9.867 | bob: 8.774Epoch   2:  26% | abe: 8.850 | eve: 9.865 | bob: 8.773Epoch   2:  26% | abe: 8.850 | eve: 9.865 | bob: 8.773Epoch   2:  27% | abe: 8.849 | eve: 9.863 | bob: 8.772Epoch   2:  28% | abe: 8.848 | eve: 9.864 | bob: 8.770Epoch   2:  28% | abe: 8.849 | eve: 9.863 | bob: 8.772Epoch   2:  29% | abe: 8.848 | eve: 9.866 | bob: 8.771Epoch   2:  30% | abe: 8.850 | eve: 9.865 | bob: 8.772Epoch   2:  30% | abe: 8.849 | eve: 9.867 | bob: 8.771Epoch   2:  31% | abe: 8.849 | eve: 9.869 | bob: 8.771Epoch   2:  32% | abe: 8.848 | eve: 9.869 | bob: 8.770Epoch   2:  32% | abe: 8.847 | eve: 9.869 | bob: 8.769Epoch   2:  33% | abe: 8.847 | eve: 9.870 | bob: 8.769Epoch   2:  34% | abe: 8.847 | eve: 9.870 | bob: 8.769Epoch   2:  34% | abe: 8.848 | eve: 9.869 | bob: 8.770Epoch   2:  35% | abe: 8.847 | eve: 9.869 | bob: 8.769Epoch   2:  36% | abe: 8.846 | eve: 9.869 | bob: 8.768Epoch   2:  36% | abe: 8.847 | eve: 9.868 | bob: 8.768