WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-08 06:51:46.718321: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-08 06:51:46.812833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-08 06:51:46.814195: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 06:51:46.818021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-08 06:51:46.821267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-08 06:51:46.822722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-08 06:51:46.826430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-08 06:51:46.828870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-08 06:51:46.834302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-08 06:51:46.879670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-08 06:51:46.880581: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-08 06:51:46.901127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-08 06:51:46.904317: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5003fe0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-08 06:51:46.904403: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-08 06:51:47.369884: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2a22d90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-08 06:51:47.370023: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-08 06:51:47.401702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-08 06:51:47.401918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 06:51:47.401951: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-08 06:51:47.401977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-08 06:51:47.402002: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-08 06:51:47.402026: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-08 06:51:47.402057: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-08 06:51:47.402084: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-08 06:51:47.410985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-08 06:51:47.411218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 06:51:47.419776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-08 06:51:47.419886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-08 06:51:47.419901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-08 06:51:47.435492: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-08 06:51:54.356638: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6995 - val_loss: 0.0039
Epoch 2/512
512/512 - 0s - loss: 0.2209 - val_loss: 4.1622e-04
Epoch 3/512
512/512 - 0s - loss: 0.0195 - val_loss: 3.2125e-05
Epoch 4/512
512/512 - 0s - loss: 0.0022 - val_loss: 1.3293e-05
Epoch 5/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.1813e-06
Epoch 6/512
512/512 - 0s - loss: 7.8910e-04 - val_loss: 5.8866e-06
Epoch 7/512
512/512 - 0s - loss: 4.8755e-04 - val_loss: 3.3621e-06
Epoch 8/512
512/512 - 0s - loss: 2.6610e-04 - val_loss: 1.6592e-06
Epoch 9/512
512/512 - 0s - loss: 1.2427e-04 - val_loss: 6.8098e-07
Epoch 10/512
512/512 - 0s - loss: 4.7731e-05 - val_loss: 2.2127e-07
Epoch 11/512
512/512 - 0s - loss: 1.4334e-05 - val_loss: 5.3446e-08
Epoch 12/512
512/512 - 0s - loss: 3.1575e-06 - val_loss: 8.9147e-09
Epoch 13/512
512/512 - 0s - loss: 5.2196e-07 - val_loss: 6.2714e-09
Epoch 14/512
512/512 - 0s - loss: 2.1184e-05 - val_loss: 4.6535e-06
Epoch 15/512
512/512 - 0s - loss: 0.0048 - val_loss: 2.1565e-05
Epoch 16/512
512/512 - 0s - loss: 9.0219e-04 - val_loss: 1.2631e-06
Epoch 17/512
512/512 - 0s - loss: 1.1341e-04 - val_loss: 1.7340e-06
Epoch 18/512
512/512 - 0s - loss: 5.4858e-04 - val_loss: 2.1763e-05
Epoch 19/512
512/512 - 0s - loss: 0.0030 - val_loss: 1.6081e-05
Epoch 20/512
512/512 - 0s - loss: 9.8570e-04 - val_loss: 4.6406e-06
Epoch 21/512
512/512 - 0s - loss: 5.4663e-04 - val_loss: 9.2482e-06
Epoch 22/512
512/512 - 0s - loss: 0.0016 - val_loss: 2.1091e-05
Epoch 23/512
512/512 - 0s - loss: 0.0017 - val_loss: 9.3089e-06
Epoch 24/512
512/512 - 0s - loss: 8.2885e-04 - val_loss: 8.0255e-06
Epoch 25/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.4916e-05
Epoch 26/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.2768e-05
Epoch 27/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.0231e-06
Epoch 28/512
512/512 - 0s - loss: 9.8718e-04 - val_loss: 1.1535e-05
Epoch 29/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2830e-05
Epoch 30/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0187e-05
Epoch 31/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0399e-05
Epoch 32/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1780e-05
Epoch 33/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0653e-05
Epoch 34/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.9527e-06
Epoch 35/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0708e-05
Epoch 36/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0545e-05
Epoch 37/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.8631e-06
Epoch 38/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0265e-05
Epoch 39/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0036e-05
Epoch 40/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.6602e-06
Epoch 41/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.7795e-06
Epoch 42/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8563e-06
Epoch 43/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.5157e-06
Epoch 44/512
512/512 - 0s - loss: 9.6646e-04 - val_loss: 9.3522e-06
Epoch 45/512
512/512 - 0s - loss: 9.6239e-04 - val_loss: 9.3912e-06
Epoch 46/512
512/512 - 0s - loss: 9.6965e-04 - val_loss: 9.2244e-06
Epoch 47/512
512/512 - 0s - loss: 9.3714e-04 - val_loss: 9.0719e-06
Epoch 48/512
512/512 - 0s - loss: 9.3277e-04 - val_loss: 9.1329e-06
Epoch 49/512
512/512 - 0s - loss: 9.3610e-04 - val_loss: 8.9189e-06
Epoch 50/512
512/512 - 0s - loss: 9.0211e-04 - val_loss: 8.7982e-06
Epoch 51/512
512/512 - 0s - loss: 9.0104e-04 - val_loss: 8.9931e-06
Epoch 52/512
512/512 - 0s - loss: 9.1198e-04 - val_loss: 8.7271e-06
Epoch 53/512
512/512 - 0s - loss: 8.7940e-04 - val_loss: 8.4890e-06
Epoch 54/512
512/512 - 0s - loss: 8.6818e-04 - val_loss: 8.5296e-06
Epoch 55/512
512/512 - 0s - loss: 8.6471e-04 - val_loss: 8.6645e-06
Epoch 56/512
512/512 - 0s - loss: 8.7151e-04 - val_loss: 8.4645e-06
Epoch 57/512
512/512 - 0s - loss: 8.4934e-04 - val_loss: 8.1166e-06
Epoch 58/512
512/512 - 0s - loss: 8.2595e-04 - val_loss: 8.2211e-06
Epoch 59/512
512/512 - 0s - loss: 8.4269e-04 - val_loss: 8.1854e-06
Epoch 60/512
512/512 - 0s - loss: 8.2194e-04 - val_loss: 8.0319e-06
Epoch 61/512
512/512 - 0s - loss: 8.1327e-04 - val_loss: 7.9936e-06
Epoch 62/512
512/512 - 0s - loss: 8.0918e-04 - val_loss: 7.8551e-06
Epoch 63/512
512/512 - 0s - loss: 7.9858e-04 - val_loss: 7.7896e-06
Epoch 64/512
512/512 - 0s - loss: 7.9185e-04 - val_loss: 7.6975e-06
Epoch 65/512
512/512 - 0s - loss: 7.8101e-04 - val_loss: 7.6713e-06
Epoch 66/512
512/512 - 0s - loss: 7.7478e-04 - val_loss: 7.6840e-06
Epoch 67/512
512/512 - 0s - loss: 7.7548e-04 - val_loss: 7.5307e-06
Epoch 68/512
512/512 - 0s - loss: 7.5989e-04 - val_loss: 7.3792e-06
Epoch 69/512
512/512 - 0s - loss: 7.5029e-04 - val_loss: 7.3873e-06
Epoch 70/512
512/512 - 0s - loss: 7.4879e-04 - val_loss: 7.3970e-06
Epoch 71/512
512/512 - 0s - loss: 7.4768e-04 - val_loss: 7.1636e-06
Epoch 72/512
512/512 - 0s - loss: 7.2369e-04 - val_loss: 7.1453e-06
Epoch 73/512
512/512 - 0s - loss: 7.2354e-04 - val_loss: 7.2785e-06
Epoch 74/512
512/512 - 0s - loss: 7.4001e-04 - val_loss: 6.9312e-06
Epoch 75/512
512/512 - 0s - loss: 6.9762e-04 - val_loss: 6.8757e-06
Epoch 76/512
512/512 - 0s - loss: 7.0286e-04 - val_loss: 7.0941e-06
Epoch 77/512
512/512 - 0s - loss: 7.1450e-04 - val_loss: 6.9464e-06
Epoch 78/512
512/512 - 0s - loss: 6.9754e-04 - val_loss: 6.6135e-06
Epoch 79/512
512/512 - 0s - loss: 6.7378e-04 - val_loss: 6.6885e-06
Epoch 80/512
512/512 - 0s - loss: 6.8286e-04 - val_loss: 6.9577e-06
Epoch 81/512
512/512 - 0s - loss: 6.9384e-04 - val_loss: 6.6880e-06
Epoch 82/512
512/512 - 0s - loss: 6.6350e-04 - val_loss: 6.4627e-06
Epoch 83/512
512/512 - 0s - loss: 6.5880e-04 - val_loss: 6.5635e-06
Epoch 84/512
512/512 - 0s - loss: 6.6286e-04 - val_loss: 6.6270e-06
Epoch 85/512
512/512 - 0s - loss: 6.6475e-04 - val_loss: 6.3719e-06
Epoch 86/512
512/512 - 0s - loss: 6.3808e-04 - val_loss: 6.3077e-06
Epoch 87/512
512/512 - 0s - loss: 6.4243e-04 - val_loss: 6.4415e-06
Epoch 88/512
512/512 - 0s - loss: 6.4675e-04 - val_loss: 6.3023e-06
Epoch 89/512
512/512 - 0s - loss: 6.2587e-04 - val_loss: 6.2511e-06
Epoch 90/512
512/512 - 0s - loss: 6.2766e-04 - val_loss: 6.2801e-06
Epoch 91/512
512/512 - 0s - loss: 6.2799e-04 - val_loss: 6.1775e-06
Epoch 92/512
512/512 - 0s - loss: 6.1673e-04 - val_loss: 6.0242e-06
Epoch 93/512
512/512 - 0s - loss: 6.1159e-04 - val_loss: 5.9891e-06
Epoch 94/512
512/512 - 0s - loss: 6.0279e-04 - val_loss: 6.0254e-06
Epoch 95/512
512/512 - 0s - loss: 6.0981e-04 - val_loss: 5.9496e-06
Epoch 96/512
512/512 - 0s - loss: 5.9881e-04 - val_loss: 5.7790e-06
Epoch 97/512
512/512 - 0s - loss: 5.7952e-04 - val_loss: 5.9538e-06
Epoch 98/512
512/512 - 0s - loss: 6.0208e-04 - val_loss: 5.9202e-06
Epoch 99/512
512/512 - 0s - loss: 5.8600e-04 - val_loss: 5.6286e-06
Epoch 100/512
512/512 - 0s - loss: 5.6834e-04 - val_loss: 5.6529e-06
Epoch 101/512
512/512 - 0s - loss: 5.7605e-04 - val_loss: 5.7071e-06
Epoch 102/512
512/512 - 0s - loss: 5.7184e-04 - val_loss: 5.6622e-06
Epoch 103/512
512/512 - 0s - loss: 5.6047e-04 - val_loss: 5.7028e-06
Epoch 104/512
512/512 - 0s - loss: 5.6862e-04 - val_loss: 5.5905e-06
Epoch 105/512
512/512 - 0s - loss: 5.5762e-04 - val_loss: 5.4283e-06
Epoch 106/512
512/512 - 0s - loss: 5.4199e-04 - val_loss: 5.5188e-06
Epoch 107/512
512/512 - 0s - loss: 5.5445e-04 - val_loss: 5.5320e-06
Epoch 108/512
512/512 - 0s - loss: 5.5310e-04 - val_loss: 5.2115e-06
Epoch 109/512
512/512 - 0s - loss: 5.1882e-04 - val_loss: 5.3529e-06
Epoch 110/512
512/512 - 0s - loss: 5.4470e-04 - val_loss: 5.4798e-06
Epoch 111/512
512/512 - 0s - loss: 5.4268e-04 - val_loss: 5.1400e-06
Epoch 112/512
512/512 - 0s - loss: 5.1688e-04 - val_loss: 5.0016e-06
Epoch 113/512
512/512 - 0s - loss: 5.1510e-04 - val_loss: 5.1914e-06
Epoch 114/512
512/512 - 0s - loss: 5.2968e-04 - val_loss: 5.1558e-06
Epoch 115/512
512/512 - 0s - loss: 5.1169e-04 - val_loss: 5.0091e-06
Epoch 116/512
512/512 - 0s - loss: 5.0469e-04 - val_loss: 5.0745e-06
Epoch 117/512
512/512 - 0s - loss: 5.1021e-04 - val_loss: 5.0694e-06
Epoch 118/512
512/512 - 0s - loss: 5.0439e-04 - val_loss: 4.9955e-06
Epoch 119/512
512/512 - 0s - loss: 4.9679e-04 - val_loss: 4.9374e-06
Epoch 120/512
512/512 - 0s - loss: 4.9567e-04 - val_loss: 4.8939e-06
Epoch 121/512
512/512 - 0s - loss: 4.9011e-04 - val_loss: 4.8552e-06
Epoch 122/512
512/512 - 0s - loss: 4.9074e-04 - val_loss: 4.7750e-06
Epoch 123/512
512/512 - 0s - loss: 4.7736e-04 - val_loss: 4.7744e-06
Epoch 124/512
512/512 - 0s - loss: 4.8082e-04 - val_loss: 4.7864e-06
Epoch 125/512
512/512 - 0s - loss: 4.7908e-04 - val_loss: 4.6733e-06
Epoch 126/512
512/512 - 0s - loss: 4.6881e-04 - val_loss: 4.6250e-06
Epoch 127/512
512/512 - 0s - loss: 4.6322e-04 - val_loss: 4.7338e-06
Epoch 128/512
512/512 - 0s - loss: 4.7461e-04 - val_loss: 4.6275e-06
Epoch 129/512
512/512 - 0s - loss: 4.5662e-04 - val_loss: 4.5009e-06
Epoch 130/512
512/512 - 0s - loss: 4.5384e-04 - val_loss: 4.5496e-06
Epoch 131/512
512/512 - 0s - loss: 4.5639e-04 - val_loss: 4.5171e-06
Epoch 132/512
512/512 - 0s - loss: 4.4896e-04 - val_loss: 4.4570e-06
Epoch 133/512
512/512 - 0s - loss: 4.4667e-04 - val_loss: 4.4262e-06
Epoch 134/512
512/512 - 0s - loss: 4.4110e-04 - val_loss: 4.3997e-06
Epoch 135/512
512/512 - 0s - loss: 4.3970e-04 - val_loss: 4.3785e-06
Epoch 136/512
512/512 - 0s - loss: 4.3692e-04 - val_loss: 4.3333e-06
Epoch 137/512
512/512 - 0s - loss: 4.3267e-04 - val_loss: 4.2291e-06
Epoch 138/512
512/512 - 0s - loss: 4.2407e-04 - val_loss: 4.2561e-06
Epoch 139/512
512/512 - 0s - loss: 4.2729e-04 - val_loss: 4.2714e-06
Epoch 140/512
512/512 - 0s - loss: 4.2235e-04 - val_loss: 4.2066e-06
Epoch 141/512
512/512 - 0s - loss: 4.1967e-04 - val_loss: 4.1028e-06
Epoch 142/512
512/512 - 0s - loss: 4.1312e-04 - val_loss: 4.0252e-06
Epoch 143/512
512/512 - 0s - loss: 4.0705e-04 - val_loss: 4.1009e-06
Epoch 144/512
512/512 - 0s - loss: 4.1183e-04 - val_loss: 4.1057e-06
Epoch 145/512
512/512 - 0s - loss: 4.0674e-04 - val_loss: 3.9840e-06
Epoch 146/512
512/512 - 0s - loss: 3.9776e-04 - val_loss: 3.9284e-06
Epoch 147/512
512/512 - 0s - loss: 3.9610e-04 - val_loss: 3.9740e-06
Epoch 148/512
512/512 - 0s - loss: 3.9675e-04 - val_loss: 3.9739e-06
Epoch 149/512
512/512 - 0s - loss: 3.9437e-04 - val_loss: 3.8689e-06
Epoch 150/512
512/512 - 0s - loss: 3.8486e-04 - val_loss: 3.8273e-06
Epoch 151/512
512/512 - 0s - loss: 3.8615e-04 - val_loss: 3.7940e-06
Epoch 152/512
512/512 - 0s - loss: 3.8028e-04 - val_loss: 3.7645e-06
Epoch 153/512
512/512 - 0s - loss: 3.7727e-04 - val_loss: 3.7540e-06
Epoch 154/512
512/512 - 0s - loss: 3.7607e-04 - val_loss: 3.7344e-06
Epoch 155/512
512/512 - 0s - loss: 3.7217e-04 - val_loss: 3.7191e-06
Epoch 156/512
512/512 - 0s - loss: 3.7184e-04 - val_loss: 3.6170e-06
Epoch 157/512
512/512 - 0s - loss: 3.6175e-04 - val_loss: 3.5717e-06
Epoch 158/512
512/512 - 0s - loss: 3.6059e-04 - val_loss: 3.6246e-06
Epoch 159/512
512/512 - 0s - loss: 3.6347e-04 - val_loss: 3.6011e-06
Epoch 160/512
512/512 - 0s - loss: 3.5885e-04 - val_loss: 3.4509e-06
Epoch 161/512
512/512 - 0s - loss: 3.4589e-04 - val_loss: 3.4830e-06
Epoch 162/512
512/512 - 0s - loss: 3.5381e-04 - val_loss: 3.5050e-06
Epoch 163/512
512/512 - 0s - loss: 3.4940e-04 - val_loss: 3.4242e-06
Epoch 164/512
512/512 - 0s - loss: 3.3893e-04 - val_loss: 3.4236e-06
Epoch 165/512
512/512 - 0s - loss: 3.4544e-04 - val_loss: 3.3635e-06
Epoch 166/512
512/512 - 0s - loss: 3.3783e-04 - val_loss: 3.3069e-06
Epoch 167/512
512/512 - 0s - loss: 3.3211e-04 - val_loss: 3.3214e-06
Epoch 168/512
512/512 - 0s - loss: 3.3218e-04 - val_loss: 3.3696e-06
Epoch 169/512
512/512 - 0s - loss: 3.3086e-04 - val_loss: 3.3364e-06
Epoch 170/512
512/512 - 0s - loss: 3.3074e-04 - val_loss: 3.1857e-06
Epoch 171/512
512/512 - 0s - loss: 3.1668e-04 - val_loss: 3.1933e-06
Epoch 172/512
512/512 - 0s - loss: 3.2278e-04 - val_loss: 3.2258e-06
Epoch 173/512
512/512 - 0s - loss: 3.2172e-04 - val_loss: 3.1237e-06
Epoch 174/512
512/512 - 0s - loss: 3.1215e-04 - val_loss: 3.0696e-06
Epoch 175/512
512/512 - 0s - loss: 3.1031e-04 - val_loss: 3.1002e-06
Epoch 176/512
512/512 - 0s - loss: 3.1320e-04 - val_loss: 3.0396e-06
Epoch 177/512
512/512 - 0s - loss: 3.0216e-04 - val_loss: 3.0363e-06
Epoch 178/512
512/512 - 0s - loss: 3.0746e-04 - val_loss: 2.9971e-06
Epoch 179/512
512/512 - 0s - loss: 2.9721e-04 - val_loss: 2.9801e-06
Epoch 180/512
512/512 - 0s - loss: 2.9988e-04 - val_loss: 2.9995e-06
Epoch 181/512
512/512 - 0s - loss: 2.9626e-04 - val_loss: 2.9581e-06
Epoch 182/512
512/512 - 0s - loss: 2.9234e-04 - val_loss: 2.9308e-06
Epoch 183/512
512/512 - 0s - loss: 2.9207e-04 - val_loss: 2.8536e-06
Epoch 184/512
512/512 - 0s - loss: 2.8434e-04 - val_loss: 2.8456e-06
Epoch 185/512
512/512 - 0s - loss: 2.8631e-04 - val_loss: 2.8452e-06
Epoch 186/512
512/512 - 0s - loss: 2.8221e-04 - val_loss: 2.8055e-06
Epoch 187/512
512/512 - 0s - loss: 2.8028e-04 - val_loss: 2.7683e-06
Epoch 188/512
512/512 - 0s - loss: 2.7607e-04 - val_loss: 2.7408e-06
Epoch 189/512
512/512 - 0s - loss: 2.7386e-04 - val_loss: 2.7305e-06
Epoch 190/512
512/512 - 0s - loss: 2.7313e-04 - val_loss: 2.6935e-06
Epoch 191/512
512/512 - 0s - loss: 2.6875e-04 - val_loss: 2.6705e-06
Epoch 192/512
512/512 - 0s - loss: 2.6619e-04 - val_loss: 2.6516e-06
Epoch 193/512
512/512 - 0s - loss: 2.6364e-04 - val_loss: 2.6480e-06
Epoch 194/512
512/512 - 0s - loss: 2.6339e-04 - val_loss: 2.6055e-06
Epoch 195/512
512/512 - 0s - loss: 2.5975e-04 - val_loss: 2.5385e-06
Epoch 196/512
512/512 - 0s - loss: 2.5443e-04 - val_loss: 2.5408e-06
Epoch 197/512
512/512 - 0s - loss: 2.5372e-04 - val_loss: 2.5615e-06
Epoch 198/512
512/512 - 0s - loss: 2.5672e-04 - val_loss: 2.4583e-06
Epoch 199/512
512/512 - 0s - loss: 2.4517e-04 - val_loss: 2.4189e-06
Epoch 200/512
512/512 - 0s - loss: 2.4658e-04 - val_loss: 2.4578e-06
Epoch 201/512
512/512 - 0s - loss: 2.4546e-04 - val_loss: 2.4434e-06
Epoch 202/512
512/512 - 0s - loss: 2.4337e-04 - val_loss: 2.3848e-06
Epoch 203/512
512/512 - 0s - loss: 2.3830e-04 - val_loss: 2.3514e-06
Epoch 204/512
512/512 - 0s - loss: 2.3620e-04 - val_loss: 2.3893e-06
Epoch 205/512
512/512 - 0s - loss: 2.3927e-04 - val_loss: 2.3123e-06
Epoch 206/512
512/512 - 0s - loss: 2.3010e-04 - val_loss: 2.2507e-06
Epoch 207/512
512/512 - 0s - loss: 2.2715e-04 - val_loss: 2.3163e-06
Epoch 208/512
512/512 - 0s - loss: 2.3302e-04 - val_loss: 2.2871e-06
Epoch 209/512
512/512 - 0s - loss: 2.2392e-04 - val_loss: 2.2319e-06
Epoch 210/512
512/512 - 0s - loss: 2.2362e-04 - val_loss: 2.2037e-06
Epoch 211/512
512/512 - 0s - loss: 2.2114e-04 - val_loss: 2.1848e-06
Epoch 212/512
512/512 - 0s - loss: 2.1764e-04 - val_loss: 2.1945e-06
Epoch 213/512
512/512 - 0s - loss: 2.1934e-04 - val_loss: 2.1631e-06
Epoch 214/512
512/512 - 0s - loss: 2.1522e-04 - val_loss: 2.1010e-06
Epoch 215/512
512/512 - 0s - loss: 2.1081e-04 - val_loss: 2.0714e-06
Epoch 216/512
512/512 - 0s - loss: 2.0732e-04 - val_loss: 2.1274e-06
Epoch 217/512
512/512 - 0s - loss: 2.1373e-04 - val_loss: 2.0655e-06
Epoch 218/512
512/512 - 0s - loss: 2.0190e-04 - val_loss: 2.0148e-06
Epoch 219/512
512/512 - 0s - loss: 2.0370e-04 - val_loss: 2.0331e-06
Epoch 220/512
512/512 - 0s - loss: 2.0175e-04 - val_loss: 2.0297e-06
Epoch 221/512
512/512 - 0s - loss: 2.0217e-04 - val_loss: 1.9550e-06
Epoch 222/512
512/512 - 0s - loss: 1.9357e-04 - val_loss: 1.9399e-06
Epoch 223/512
512/512 - 0s - loss: 1.9552e-04 - val_loss: 1.9679e-06
Epoch 224/512
512/512 - 0s - loss: 1.9633e-04 - val_loss: 1.8907e-06
Epoch 225/512
512/512 - 0s - loss: 1.8711e-04 - val_loss: 1.8720e-06
Epoch 226/512
512/512 - 0s - loss: 1.8940e-04 - val_loss: 1.9107e-06
Epoch 227/512
512/512 - 0s - loss: 1.9006e-04 - val_loss: 1.8208e-06
Epoch 228/512
512/512 - 0s - loss: 1.8217e-04 - val_loss: 1.7771e-06
Epoch 229/512
512/512 - 0s - loss: 1.8011e-04 - val_loss: 1.8523e-06
Epoch 230/512
512/512 - 0s - loss: 1.8398e-04 - val_loss: 1.8396e-06
Epoch 231/512
512/512 - 0s - loss: 1.8087e-04 - val_loss: 1.7339e-06
Epoch 232/512
512/512 - 0s - loss: 1.7301e-04 - val_loss: 1.7103e-06
Epoch 233/512
512/512 - 0s - loss: 1.7348e-04 - val_loss: 1.7688e-06
Epoch 234/512
512/512 - 0s - loss: 1.7615e-04 - val_loss: 1.7374e-06
Epoch 235/512
512/512 - 0s - loss: 1.7098e-04 - val_loss: 1.6467e-06
Epoch 236/512
512/512 - 0s - loss: 1.6530e-04 - val_loss: 1.6619e-06
Epoch 237/512
512/512 - 0s - loss: 1.6848e-04 - val_loss: 1.6782e-06
Epoch 238/512
512/512 - 0s - loss: 1.6666e-04 - val_loss: 1.6156e-06
Epoch 239/512
512/512 - 0s - loss: 1.6069e-04 - val_loss: 1.5958e-06
Epoch 240/512
512/512 - 0s - loss: 1.6065e-04 - val_loss: 1.6199e-06
Epoch 241/512
512/512 - 0s - loss: 1.6050e-04 - val_loss: 1.6001e-06
Epoch 242/512
512/512 - 0s - loss: 1.5784e-04 - val_loss: 1.5511e-06
Epoch 243/512
512/512 - 0s - loss: 1.5513e-04 - val_loss: 1.5066e-06
Epoch 244/512
512/512 - 0s - loss: 1.5167e-04 - val_loss: 1.5187e-06
Epoch 245/512
512/512 - 0s - loss: 1.5244e-04 - val_loss: 1.5255e-06
Epoch 246/512
512/512 - 0s - loss: 1.5186e-04 - val_loss: 1.4663e-06
Epoch 247/512
512/512 - 0s - loss: 1.4479e-04 - val_loss: 1.4641e-06
Epoch 248/512
512/512 - 0s - loss: 1.4645e-04 - val_loss: 1.4891e-06
Epoch 249/512
512/512 - 0s - loss: 1.4818e-04 - val_loss: 1.4105e-06
Epoch 250/512
512/512 - 0s - loss: 1.3822e-04 - val_loss: 1.3934e-06
Epoch 251/512
512/512 - 0s - loss: 1.4240e-04 - val_loss: 1.3948e-06
Epoch 252/512
512/512 - 0s - loss: 1.3848e-04 - val_loss: 1.3721e-06
Epoch 253/512
512/512 - 0s - loss: 1.3774e-04 - val_loss: 1.3509e-06
Epoch 254/512
512/512 - 0s - loss: 1.3408e-04 - val_loss: 1.3509e-06
Epoch 255/512
512/512 - 0s - loss: 1.3474e-04 - val_loss: 1.3314e-06
Epoch 256/512
512/512 - 0s - loss: 1.3212e-04 - val_loss: 1.2884e-06
Epoch 257/512
512/512 - 0s - loss: 1.2903e-04 - val_loss: 1.2847e-06
Epoch 258/512
512/512 - 0s - loss: 1.2864e-04 - val_loss: 1.2817e-06
Epoch 259/512
512/512 - 0s - loss: 1.2710e-04 - val_loss: 1.2702e-06
Epoch 260/512
512/512 - 0s - loss: 1.2472e-04 - val_loss: 1.2521e-06
Epoch 261/512
512/512 - 0s - loss: 1.2422e-04 - val_loss: 1.2185e-06
Epoch 262/512
512/512 - 0s - loss: 1.2153e-04 - val_loss: 1.1906e-06
Epoch 263/512
512/512 - 0s - loss: 1.1922e-04 - val_loss: 1.1874e-06
Epoch 264/512
512/512 - 0s - loss: 1.2017e-04 - val_loss: 1.1463e-06
Epoch 265/512
512/512 - 0s - loss: 1.1427e-04 - val_loss: 1.1452e-06
Epoch 266/512
512/512 - 0s - loss: 1.1495e-04 - val_loss: 1.1782e-06
Epoch 267/512
512/512 - 0s - loss: 1.1670e-04 - val_loss: 1.1190e-06
Epoch 268/512
512/512 - 0s - loss: 1.1017e-04 - val_loss: 1.0713e-06
Epoch 269/512
512/512 - 0s - loss: 1.0915e-04 - val_loss: 1.0864e-06
Epoch 270/512
512/512 - 0s - loss: 1.0844e-04 - val_loss: 1.1044e-06
Epoch 271/512
512/512 - 0s - loss: 1.0916e-04 - val_loss: 1.0640e-06
Epoch 272/512
512/512 - 0s - loss: 1.0495e-04 - val_loss: 1.0255e-06
Epoch 273/512
512/512 - 0s - loss: 1.0279e-04 - val_loss: 1.0267e-06
Epoch 274/512
512/512 - 0s - loss: 1.0292e-04 - val_loss: 1.0352e-06
Epoch 275/512
512/512 - 0s - loss: 1.0271e-04 - val_loss: 9.8292e-07
Epoch 276/512
512/512 - 0s - loss: 9.7325e-05 - val_loss: 9.6868e-07
Epoch 277/512
512/512 - 0s - loss: 9.7468e-05 - val_loss: 9.9328e-07
Epoch 278/512
512/512 - 0s - loss: 9.8433e-05 - val_loss: 9.7146e-07
Epoch 279/512
512/512 - 0s - loss: 9.5031e-05 - val_loss: 9.2053e-07
Epoch 280/512
512/512 - 0s - loss: 9.1865e-05 - val_loss: 9.1910e-07
Epoch 281/512
512/512 - 0s - loss: 9.2403e-05 - val_loss: 9.2787e-07
Epoch 282/512
512/512 - 0s - loss: 9.2640e-05 - val_loss: 8.7189e-07
Epoch 283/512
512/512 - 0s - loss: 8.6317e-05 - val_loss: 8.7298e-07
Epoch 284/512
512/512 - 0s - loss: 8.8780e-05 - val_loss: 8.8793e-07
Epoch 285/512
512/512 - 0s - loss: 8.6519e-05 - val_loss: 8.6804e-07
Epoch 286/512
512/512 - 0s - loss: 8.5567e-05 - val_loss: 8.4161e-07
Epoch 287/512
512/512 - 0s - loss: 8.3136e-05 - val_loss: 8.1913e-07
Epoch 288/512
512/512 - 0s - loss: 8.2374e-05 - val_loss: 8.0268e-07
Epoch 289/512
512/512 - 0s - loss: 8.0483e-05 - val_loss: 7.9586e-07
Epoch 290/512
512/512 - 0s - loss: 8.0457e-05 - val_loss: 7.7304e-07
Epoch 291/512
512/512 - 0s - loss: 7.6771e-05 - val_loss: 7.7319e-07
Epoch 292/512
512/512 - 0s - loss: 7.8053e-05 - val_loss: 7.6839e-07
Epoch 293/512
512/512 - 0s - loss: 7.6187e-05 - val_loss: 7.2115e-07
Epoch 294/512
512/512 - 0s - loss: 7.2809e-05 - val_loss: 7.2274e-07
Epoch 295/512
512/512 - 0s - loss: 7.3389e-05 - val_loss: 7.2803e-07
Epoch 296/512
512/512 - 0s - loss: 7.2440e-05 - val_loss: 7.0792e-07
Epoch 297/512
512/512 - 0s - loss: 7.0537e-05 - val_loss: 6.8427e-07
Epoch 298/512
512/512 - 0s - loss: 6.8543e-05 - val_loss: 6.8449e-07
Epoch 299/512
512/512 - 0s - loss: 6.8199e-05 - val_loss: 6.8604e-07
Epoch 300/512
512/512 - 0s - loss: 6.7532e-05 - val_loss: 6.6738e-07
Epoch 301/512
512/512 - 0s - loss: 6.5676e-05 - val_loss: 6.4576e-07
Epoch 302/512
512/512 - 0s - loss: 6.4185e-05 - val_loss: 6.3545e-07
Epoch 303/512
512/512 - 0s - loss: 6.3182e-05 - val_loss: 6.3113e-07
Epoch 304/512
512/512 - 0s - loss: 6.2972e-05 - val_loss: 6.0815e-07
Epoch 305/512
512/512 - 0s - loss: 5.9686e-05 - val_loss: 6.1403e-07
Epoch 306/512
512/512 - 0s - loss: 6.1202e-05 - val_loss: 6.0543e-07
Epoch 307/512
512/512 - 0s - loss: 5.9257e-05 - val_loss: 5.6786e-07
Epoch 308/512
512/512 - 0s - loss: 5.6740e-05 - val_loss: 5.5698e-07
Epoch 309/512
512/512 - 0s - loss: 5.6086e-05 - val_loss: 5.7530e-07
Epoch 310/512
512/512 - 0s - loss: 5.7248e-05 - val_loss: 5.5211e-07
Epoch 311/512
512/512 - 0s - loss: 5.4226e-05 - val_loss: 5.1594e-07
Epoch 312/512
512/512 - 0s - loss: 5.2309e-05 - val_loss: 5.2251e-07
Epoch 313/512
512/512 - 0s - loss: 5.2981e-05 - val_loss: 5.2563e-07
Epoch 314/512
512/512 - 0s - loss: 5.2070e-05 - val_loss: 5.0243e-07
Epoch 315/512
512/512 - 0s - loss: 5.0110e-05 - val_loss: 4.8534e-07
Epoch 316/512
512/512 - 0s - loss: 4.8568e-05 - val_loss: 4.9454e-07
Epoch 317/512
512/512 - 0s - loss: 4.9498e-05 - val_loss: 4.8629e-07
Epoch 318/512
512/512 - 0s - loss: 4.8357e-05 - val_loss: 4.4784e-07
Epoch 319/512
512/512 - 0s - loss: 4.4401e-05 - val_loss: 4.5220e-07
Epoch 320/512
512/512 - 0s - loss: 4.6069e-05 - val_loss: 4.7414e-07
Epoch 321/512
512/512 - 0s - loss: 4.6267e-05 - val_loss: 4.4296e-07
Epoch 322/512
512/512 - 0s - loss: 4.2620e-05 - val_loss: 4.2273e-07
Epoch 323/512
512/512 - 0s - loss: 4.2759e-05 - val_loss: 4.2861e-07
Epoch 324/512
512/512 - 0s - loss: 4.2559e-05 - val_loss: 4.2054e-07
Epoch 325/512
512/512 - 0s - loss: 4.1368e-05 - val_loss: 3.9883e-07
Epoch 326/512
512/512 - 0s - loss: 3.9590e-05 - val_loss: 3.9772e-07
Epoch 327/512
512/512 - 0s - loss: 3.9702e-05 - val_loss: 3.9604e-07
Epoch 328/512
512/512 - 0s - loss: 3.9054e-05 - val_loss: 3.8135e-07
Epoch 329/512
512/512 - 0s - loss: 3.7470e-05 - val_loss: 3.6742e-07
Epoch 330/512
512/512 - 0s - loss: 3.7032e-05 - val_loss: 3.6564e-07
Epoch 331/512
512/512 - 0s - loss: 3.6224e-05 - val_loss: 3.5973e-07
Epoch 332/512
512/512 - 0s - loss: 3.5821e-05 - val_loss: 3.4510e-07
Epoch 333/512
512/512 - 0s - loss: 3.4196e-05 - val_loss: 3.3949e-07
Epoch 334/512
512/512 - 0s - loss: 3.3938e-05 - val_loss: 3.3837e-07
Epoch 335/512
512/512 - 0s - loss: 3.3779e-05 - val_loss: 3.2173e-07
Epoch 336/512
512/512 - 0s - loss: 3.1705e-05 - val_loss: 3.1815e-07
Epoch 337/512
512/512 - 0s - loss: 3.2080e-05 - val_loss: 3.1693e-07
Epoch 338/512
512/512 - 0s - loss: 3.1081e-05 - val_loss: 3.1108e-07
Epoch 339/512
512/512 - 0s - loss: 3.0622e-05 - val_loss: 2.9823e-07
Epoch 340/512
512/512 - 0s - loss: 2.9644e-05 - val_loss: 2.8434e-07
Epoch 341/512
512/512 - 0s - loss: 2.8449e-05 - val_loss: 2.8666e-07
Epoch 342/512
512/512 - 0s - loss: 2.8848e-05 - val_loss: 2.8350e-07
Epoch 343/512
512/512 - 0s - loss: 2.7934e-05 - val_loss: 2.6690e-07
Epoch 344/512
512/512 - 0s - loss: 2.6255e-05 - val_loss: 2.6989e-07
Epoch 345/512
512/512 - 0s - loss: 2.7207e-05 - val_loss: 2.6718e-07
Epoch 346/512
512/512 - 0s - loss: 2.6141e-05 - val_loss: 2.4558e-07
Epoch 347/512
512/512 - 0s - loss: 2.4156e-05 - val_loss: 2.5006e-07
Epoch 348/512
512/512 - 0s - loss: 2.5492e-05 - val_loss: 2.4943e-07
Epoch 349/512
512/512 - 0s - loss: 2.4376e-05 - val_loss: 2.2978e-07
Epoch 350/512
512/512 - 0s - loss: 2.2645e-05 - val_loss: 2.3079e-07
Epoch 351/512
512/512 - 0s - loss: 2.3305e-05 - val_loss: 2.3545e-07
Epoch 352/512
512/512 - 0s - loss: 2.3174e-05 - val_loss: 2.1620e-07
Epoch 353/512
512/512 - 0s - loss: 2.1143e-05 - val_loss: 2.1087e-07
Epoch 354/512
512/512 - 0s - loss: 2.1191e-05 - val_loss: 2.2027e-07
Epoch 355/512
512/512 - 0s - loss: 2.1642e-05 - val_loss: 2.0990e-07
Epoch 356/512
512/512 - 0s - loss: 2.0387e-05 - val_loss: 1.9170e-07
Epoch 357/512
512/512 - 0s - loss: 1.9138e-05 - val_loss: 1.9570e-07
Epoch 358/512
512/512 - 0s - loss: 1.9767e-05 - val_loss: 1.9917e-07
Epoch 359/512
512/512 - 0s - loss: 1.9416e-05 - val_loss: 1.8377e-07
Epoch 360/512
512/512 - 0s - loss: 1.8022e-05 - val_loss: 1.7591e-07
Epoch 361/512
512/512 - 0s - loss: 1.7753e-05 - val_loss: 1.8323e-07
Epoch 362/512
512/512 - 0s - loss: 1.8121e-05 - val_loss: 1.7756e-07
Epoch 363/512
512/512 - 0s - loss: 1.7140e-05 - val_loss: 1.6477e-07
Epoch 364/512
512/512 - 0s - loss: 1.6376e-05 - val_loss: 1.6421e-07
Epoch 365/512
512/512 - 0s - loss: 1.6486e-05 - val_loss: 1.6365e-07
Epoch 366/512
512/512 - 0s - loss: 1.6081e-05 - val_loss: 1.5543e-07
Epoch 367/512
512/512 - 0s - loss: 1.5189e-05 - val_loss: 1.5332e-07
Epoch 368/512
512/512 - 0s - loss: 1.5428e-05 - val_loss: 1.4953e-07
Epoch 369/512
512/512 - 0s - loss: 1.4636e-05 - val_loss: 1.4336e-07
Epoch 370/512
512/512 - 0s - loss: 1.4238e-05 - val_loss: 1.4231e-07
Epoch 371/512
512/512 - 0s - loss: 1.4231e-05 - val_loss: 1.3733e-07
Epoch 372/512
512/512 - 0s - loss: 1.3440e-05 - val_loss: 1.3404e-07
Epoch 373/512
512/512 - 0s - loss: 1.3267e-05 - val_loss: 1.3453e-07
Epoch 374/512
512/512 - 0s - loss: 1.3261e-05 - val_loss: 1.2742e-07
Epoch 375/512
512/512 - 0s - loss: 1.2490e-05 - val_loss: 1.2123e-07
Epoch 376/512
512/512 - 0s - loss: 1.2178e-05 - val_loss: 1.2137e-07
Epoch 377/512
512/512 - 0s - loss: 1.2136e-05 - val_loss: 1.1845e-07
Epoch 378/512
512/512 - 0s - loss: 1.1593e-05 - val_loss: 1.1458e-07
Epoch 379/512
512/512 - 0s - loss: 1.1433e-05 - val_loss: 1.1082e-07
Epoch 380/512
512/512 - 0s - loss: 1.0959e-05 - val_loss: 1.0962e-07
Epoch 381/512
512/512 - 0s - loss: 1.0871e-05 - val_loss: 1.0685e-07
Epoch 382/512
512/512 - 0s - loss: 1.0529e-05 - val_loss: 1.0273e-07
Epoch 383/512
512/512 - 0s - loss: 1.0151e-05 - val_loss: 1.0061e-07
Epoch 384/512
512/512 - 0s - loss: 1.0171e-05 - val_loss: 9.4956e-08
Epoch 385/512
512/512 - 0s - loss: 9.4102e-06 - val_loss: 9.3203e-08
Epoch 386/512
512/512 - 0s - loss: 9.3964e-06 - val_loss: 9.5597e-08
Epoch 387/512
512/512 - 0s - loss: 9.4723e-06 - val_loss: 9.0551e-08
Epoch 388/512
512/512 - 0s - loss: 8.7854e-06 - val_loss: 8.6066e-08
Epoch 389/512
512/512 - 0s - loss: 8.6268e-06 - val_loss: 8.6647e-08
Epoch 390/512
512/512 - 0s - loss: 8.5534e-06 - val_loss: 8.5223e-08
Epoch 391/512
512/512 - 0s - loss: 8.2942e-06 - val_loss: 8.0456e-08
Epoch 392/512
512/512 - 0s - loss: 7.9502e-06 - val_loss: 7.7702e-08
Epoch 393/512
512/512 - 0s - loss: 7.7518e-06 - val_loss: 7.7329e-08
Epoch 394/512
512/512 - 0s - loss: 7.6784e-06 - val_loss: 7.4333e-08
Epoch 395/512
512/512 - 0s - loss: 7.3089e-06 - val_loss: 7.2123e-08
Epoch 396/512
512/512 - 0s - loss: 7.2493e-06 - val_loss: 6.9933e-08
Epoch 397/512
512/512 - 0s - loss: 6.9432e-06 - val_loss: 6.7962e-08
Epoch 398/512
512/512 - 0s - loss: 6.8125e-06 - val_loss: 6.6279e-08
Epoch 399/512
512/512 - 0s - loss: 6.6256e-06 - val_loss: 6.3481e-08
Epoch 400/512
512/512 - 0s - loss: 6.3360e-06 - val_loss: 6.3269e-08
Epoch 401/512
512/512 - 0s - loss: 6.2860e-06 - val_loss: 6.2626e-08
Epoch 402/512
512/512 - 0s - loss: 6.1365e-06 - val_loss: 5.9959e-08
Epoch 403/512
512/512 - 0s - loss: 5.8710e-06 - val_loss: 5.7272e-08
Epoch 404/512
512/512 - 0s - loss: 5.7123e-06 - val_loss: 5.6883e-08
Epoch 405/512
512/512 - 0s - loss: 5.6727e-06 - val_loss: 5.4502e-08
Epoch 406/512
512/512 - 0s - loss: 5.3894e-06 - val_loss: 5.1955e-08
Epoch 407/512
512/512 - 0s - loss: 5.1574e-06 - val_loss: 5.2977e-08
Epoch 408/512
512/512 - 0s - loss: 5.3263e-06 - val_loss: 5.0625e-08
Epoch 409/512
512/512 - 0s - loss: 4.9435e-06 - val_loss: 4.6855e-08
Epoch 410/512
512/512 - 0s - loss: 4.7142e-06 - val_loss: 4.7477e-08
Epoch 411/512
512/512 - 0s - loss: 4.7867e-06 - val_loss: 4.7559e-08
Epoch 412/512
512/512 - 0s - loss: 4.6170e-06 - val_loss: 4.5084e-08
Epoch 413/512
512/512 - 0s - loss: 4.4257e-06 - val_loss: 4.2985e-08
Epoch 414/512
512/512 - 0s - loss: 4.3066e-06 - val_loss: 4.1676e-08
Epoch 415/512
512/512 - 0s - loss: 4.1414e-06 - val_loss: 4.2061e-08
Epoch 416/512
512/512 - 0s - loss: 4.1761e-06 - val_loss: 4.0585e-08
Epoch 417/512
512/512 - 0s - loss: 3.9279e-06 - val_loss: 3.8463e-08
Epoch 418/512
512/512 - 0s - loss: 3.8073e-06 - val_loss: 3.8343e-08
Epoch 419/512
512/512 - 0s - loss: 3.8222e-06 - val_loss: 3.6610e-08
Epoch 420/512
512/512 - 0s - loss: 3.5801e-06 - val_loss: 3.5156e-08
Epoch 421/512
512/512 - 0s - loss: 3.5294e-06 - val_loss: 3.4751e-08
Epoch 422/512
512/512 - 0s - loss: 3.4174e-06 - val_loss: 3.4416e-08
Epoch 423/512
512/512 - 0s - loss: 3.3651e-06 - val_loss: 3.3112e-08
Epoch 424/512
512/512 - 0s - loss: 3.2819e-06 - val_loss: 3.0708e-08
Epoch 425/512
512/512 - 0s - loss: 3.0253e-06 - val_loss: 3.0581e-08
Epoch 426/512
512/512 - 0s - loss: 3.0840e-06 - val_loss: 3.1210e-08
Epoch 427/512
512/512 - 0s - loss: 3.0541e-06 - val_loss: 2.8668e-08
Epoch 428/512
512/512 - 0s - loss: 2.7904e-06 - val_loss: 2.7278e-08
Epoch 429/512
512/512 - 0s - loss: 2.7929e-06 - val_loss: 2.7528e-08
Epoch 430/512
512/512 - 0s - loss: 2.7287e-06 - val_loss: 2.6849e-08
Epoch 431/512
512/512 - 0s - loss: 2.6543e-06 - val_loss: 2.5577e-08
Epoch 432/512
512/512 - 0s - loss: 2.5255e-06 - val_loss: 2.4836e-08
Epoch 433/512
512/512 - 0s - loss: 2.5101e-06 - val_loss: 2.3904e-08
Epoch 434/512
512/512 - 0s - loss: 2.3756e-06 - val_loss: 2.3310e-08
Epoch 435/512
512/512 - 0s - loss: 2.3081e-06 - val_loss: 2.3867e-08
Epoch 436/512
512/512 - 0s - loss: 2.3410e-06 - val_loss: 2.2543e-08
Epoch 437/512
512/512 - 0s - loss: 2.1832e-06 - val_loss: 2.1004e-08
Epoch 438/512
512/512 - 0s - loss: 2.0949e-06 - val_loss: 2.1141e-08
Epoch 439/512
512/512 - 0s - loss: 2.0987e-06 - val_loss: 2.0951e-08
Epoch 440/512
512/512 - 0s - loss: 2.0490e-06 - val_loss: 1.9604e-08
Epoch 441/512
512/512 - 0s - loss: 1.9225e-06 - val_loss: 1.8820e-08
Epoch 442/512
512/512 - 0s - loss: 1.8963e-06 - val_loss: 1.8583e-08
Epoch 443/512
512/512 - 0s - loss: 1.8372e-06 - val_loss: 1.8276e-08
Epoch 444/512
512/512 - 0s - loss: 1.8012e-06 - val_loss: 1.7658e-08
Epoch 445/512
512/512 - 0s - loss: 1.7426e-06 - val_loss: 1.6679e-08
Epoch 446/512
512/512 - 0s - loss: 1.6430e-06 - val_loss: 1.6597e-08
Epoch 447/512
512/512 - 0s - loss: 1.6537e-06 - val_loss: 1.6398e-08
Epoch 448/512
512/512 - 0s - loss: 1.5996e-06 - val_loss: 1.5508e-08
Epoch 449/512
512/512 - 0s - loss: 1.5161e-06 - val_loss: 1.4981e-08
Epoch 450/512
512/512 - 0s - loss: 1.4955e-06 - val_loss: 1.4710e-08
Epoch 451/512
512/512 - 0s - loss: 1.4544e-06 - val_loss: 1.4097e-08
Epoch 452/512
512/512 - 0s - loss: 1.3800e-06 - val_loss: 1.3948e-08
Epoch 453/512
512/512 - 0s - loss: 1.3756e-06 - val_loss: 1.3609e-08
Epoch 454/512
512/512 - 0s - loss: 1.3259e-06 - val_loss: 1.3065e-08
Epoch 455/512
512/512 - 0s - loss: 1.2748e-06 - val_loss: 1.2570e-08
Epoch 456/512
512/512 - 0s - loss: 1.2467e-06 - val_loss: 1.2032e-08
Epoch 457/512
512/512 - 0s - loss: 1.1957e-06 - val_loss: 1.1707e-08
Epoch 458/512
512/512 - 0s - loss: 1.1676e-06 - val_loss: 1.1629e-08
Epoch 459/512
512/512 - 0s - loss: 1.1476e-06 - val_loss: 1.1114e-08
Epoch 460/512
512/512 - 0s - loss: 1.0850e-06 - val_loss: 1.0752e-08
Epoch 461/512
512/512 - 0s - loss: 1.0709e-06 - val_loss: 1.0543e-08
Epoch 462/512
512/512 - 0s - loss: 1.0416e-06 - val_loss: 1.0034e-08
Epoch 463/512
512/512 - 0s - loss: 9.8667e-07 - val_loss: 9.8953e-09
Epoch 464/512
512/512 - 0s - loss: 9.8565e-07 - val_loss: 9.6309e-09
Epoch 465/512
512/512 - 0s - loss: 9.4283e-07 - val_loss: 9.1622e-09
Epoch 466/512
512/512 - 0s - loss: 9.0500e-07 - val_loss: 8.9674e-09
Epoch 467/512
512/512 - 0s - loss: 8.8999e-07 - val_loss: 8.8195e-09
Epoch 468/512
512/512 - 0s - loss: 8.6616e-07 - val_loss: 8.4669e-09
Epoch 469/512
512/512 - 0s - loss: 8.3263e-07 - val_loss: 8.0708e-09
Epoch 470/512
512/512 - 0s - loss: 7.9012e-07 - val_loss: 8.1473e-09
Epoch 471/512
512/512 - 0s - loss: 8.1815e-07 - val_loss: 7.6343e-09
Epoch 472/512
512/512 - 0s - loss: 7.4648e-07 - val_loss: 7.1035e-09
Epoch 473/512
512/512 - 0s - loss: 7.1198e-07 - val_loss: 7.3669e-09
Epoch 474/512
512/512 - 0s - loss: 7.4073e-07 - val_loss: 7.2628e-09
Epoch 475/512
512/512 - 0s - loss: 7.0076e-07 - val_loss: 6.5423e-09
Epoch 476/512
512/512 - 0s - loss: 6.4503e-07 - val_loss: 6.4998e-09
Epoch 477/512
512/512 - 0s - loss: 6.6246e-07 - val_loss: 6.5210e-09
Epoch 478/512
512/512 - 0s - loss: 6.3723e-07 - val_loss: 6.1524e-09
Epoch 479/512
512/512 - 0s - loss: 6.0473e-07 - val_loss: 5.9720e-09
Epoch 480/512
512/512 - 0s - loss: 5.9605e-07 - val_loss: 5.8051e-09
Epoch 481/512
512/512 - 0s - loss: 5.7060e-07 - val_loss: 5.6961e-09
Epoch 482/512
512/512 - 0s - loss: 5.6382e-07 - val_loss: 5.5008e-09
Epoch 483/512
512/512 - 0s - loss: 5.4265e-07 - val_loss: 5.1848e-09
Epoch 484/512
512/512 - 0s - loss: 5.1457e-07 - val_loss: 5.0654e-09
Epoch 485/512
512/512 - 0s - loss: 5.1043e-07 - val_loss: 5.0152e-09
Epoch 486/512
512/512 - 0s - loss: 4.9351e-07 - val_loss: 4.8592e-09
Epoch 487/512
512/512 - 0s - loss: 4.7551e-07 - val_loss: 4.7143e-09
Epoch 488/512
512/512 - 0s - loss: 4.6299e-07 - val_loss: 4.5849e-09
Epoch 489/512
512/512 - 0s - loss: 4.4948e-07 - val_loss: 4.4095e-09
Epoch 490/512
512/512 - 0s - loss: 4.3075e-07 - val_loss: 4.3231e-09
Epoch 491/512
512/512 - 0s - loss: 4.2841e-07 - val_loss: 4.1101e-09
Epoch 492/512
512/512 - 0s - loss: 4.0206e-07 - val_loss: 3.9632e-09
Epoch 493/512
512/512 - 0s - loss: 3.9299e-07 - val_loss: 3.9454e-09
Epoch 494/512
512/512 - 0s - loss: 3.9165e-07 - val_loss: 3.7385e-09
Epoch 495/512
512/512 - 0s - loss: 3.6736e-07 - val_loss: 3.5646e-09
Epoch 496/512
512/512 - 0s - loss: 3.5523e-07 - val_loss: 3.5442e-09
Epoch 497/512
512/512 - 0s - loss: 3.5363e-07 - val_loss: 3.4676e-09
Epoch 498/512
512/512 - 0s - loss: 3.4060e-07 - val_loss: 3.2599e-09
Epoch 499/512
512/512 - 0s - loss: 3.2241e-07 - val_loss: 3.1777e-09
Epoch 500/512
512/512 - 0s - loss: 3.1624e-07 - val_loss: 3.1853e-09
Epoch 501/512
512/512 - 0s - loss: 3.1611e-07 - val_loss: 3.0088e-09
Epoch 502/512
512/512 - 0s - loss: 2.9398e-07 - val_loss: 2.8227e-09
Epoch 503/512
512/512 - 0s - loss: 2.8292e-07 - val_loss: 2.8635e-09
Epoch 504/512
512/512 - 0s - loss: 2.8609e-07 - val_loss: 2.7814e-09
Epoch 505/512
512/512 - 0s - loss: 2.7210e-07 - val_loss: 2.6155e-09
Epoch 506/512
512/512 - 0s - loss: 2.5850e-07 - val_loss: 2.5715e-09
Epoch 507/512
512/512 - 0s - loss: 2.5653e-07 - val_loss: 2.5245e-09
Epoch 508/512
512/512 - 0s - loss: 2.4922e-07 - val_loss: 2.4019e-09
Epoch 509/512
512/512 - 0s - loss: 2.3562e-07 - val_loss: 2.3349e-09
Epoch 510/512
512/512 - 0s - loss: 2.3350e-07 - val_loss: 2.2881e-09
Epoch 511/512
512/512 - 0s - loss: 2.2629e-07 - val_loss: 2.1632e-09
Epoch 512/512
512/512 - 0s - loss: 2.1300e-07 - val_loss: 2.1246e-09
2024-04-08 06:52:17.931694: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3510e-07 - val_loss: 2.4472e-07
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2506e-07 - val_loss: 1.9086e-07
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8091e-07 - val_loss: 1.7961e-07
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9031e-07 - val_loss: 2.0863e-07
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0731e-07 - val_loss: 1.9432e-07
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8107e-07 - val_loss: 1.6604e-07
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6634e-07 - val_loss: 1.7355e-07
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7820e-07 - val_loss: 1.8063e-07
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7372e-07 - val_loss: 1.6073e-07
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5525e-07 - val_loss: 1.5219e-07
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5441e-07 - val_loss: 1.5903e-07
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5766e-07 - val_loss: 1.5291e-07
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4726e-07 - val_loss: 1.3990e-07
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3836e-07 - val_loss: 1.3903e-07
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3957e-07 - val_loss: 1.3976e-07
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3639e-07 - val_loss: 1.3151e-07
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2812e-07 - val_loss: 1.2488e-07
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2437e-07 - val_loss: 1.2433e-07
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2319e-07 - val_loss: 1.2110e-07
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1840e-07 - val_loss: 1.1492e-07
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1307e-07 - val_loss: 1.1186e-07
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1090e-07 - val_loss: 1.1008e-07
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0811e-07 - val_loss: 1.0533e-07
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0330e-07 - val_loss: 1.0149e-07
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0037e-07 - val_loss: 9.9401e-08
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8043e-08 - val_loss: 9.6424e-08
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4577e-08 - val_loss: 9.2649e-08
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1020e-08 - val_loss: 8.9890e-08
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8796e-08 - val_loss: 8.7783e-08
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6219e-08 - val_loss: 8.4602e-08
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2992e-08 - val_loss: 8.1837e-08
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0561e-08 - val_loss: 7.9588e-08
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8358e-08 - val_loss: 7.7007e-08
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5634e-08 - val_loss: 7.4485e-08
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3280e-08 - val_loss: 7.2176e-08
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1118e-08 - val_loss: 6.9880e-08
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8765e-08 - val_loss: 6.7807e-08
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6680e-08 - val_loss: 6.5756e-08
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4711e-08 - val_loss: 6.3589e-08
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2526e-08 - val_loss: 6.1626e-08
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0734e-08 - val_loss: 5.9604e-08
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8576e-08 - val_loss: 5.7880e-08
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7006e-08 - val_loss: 5.6138e-08
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5177e-08 - val_loss: 5.4530e-08
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3581e-08 - val_loss: 5.2582e-08
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1610e-08 - val_loss: 5.0956e-08
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0155e-08 - val_loss: 4.9486e-08
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8636e-08 - val_loss: 4.8122e-08
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7202e-08 - val_loss: 4.6549e-08
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5658e-08 - val_loss: 4.4937e-08
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4204e-08 - val_loss: 4.3628e-08
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2804e-08 - val_loss: 4.2294e-08
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1610e-08 - val_loss: 4.1097e-08
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0329e-08 - val_loss: 3.9639e-08
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8872e-08 - val_loss: 3.8350e-08
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7767e-08 - val_loss: 3.7353e-08
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6707e-08 - val_loss: 3.6201e-08
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5556e-08 - val_loss: 3.5015e-08
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4332e-08 - val_loss: 3.3894e-08
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3292e-08 - val_loss: 3.2996e-08
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2370e-08 - val_loss: 3.1870e-08
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1277e-08 - val_loss: 3.0945e-08
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0405e-08 - val_loss: 2.9983e-08
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9405e-08 - val_loss: 2.8998e-08
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8486e-08 - val_loss: 2.8117e-08
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7615e-08 - val_loss: 2.7317e-08
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6800e-08 - val_loss: 2.6407e-08
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5895e-08 - val_loss: 2.5635e-08
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5203e-08 - val_loss: 2.4942e-08
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4456e-08 - val_loss: 2.4107e-08
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3614e-08 - val_loss: 2.3300e-08
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2886e-08 - val_loss: 2.2740e-08
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2301e-08 - val_loss: 2.2055e-08
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1626e-08 - val_loss: 2.1377e-08
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0900e-08 - val_loss: 2.0644e-08
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0228e-08 - val_loss: 2.0024e-08
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9663e-08 - val_loss: 1.9454e-08
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9049e-08 - val_loss: 1.8855e-08
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8496e-08 - val_loss: 1.8294e-08
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7923e-08 - val_loss: 1.7766e-08
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7422e-08 - val_loss: 1.7267e-08
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6891e-08 - val_loss: 1.6679e-08
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6320e-08 - val_loss: 1.6157e-08
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5856e-08 - val_loss: 1.5718e-08
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5414e-08 - val_loss: 1.5315e-08
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4993e-08 - val_loss: 1.4814e-08
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4462e-08 - val_loss: 1.4282e-08
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3997e-08 - val_loss: 1.3909e-08
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3649e-08 - val_loss: 1.3595e-08
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3305e-08 - val_loss: 1.3140e-08
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2848e-08 - val_loss: 1.2714e-08
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2420e-08 - val_loss: 1.2319e-08
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2073e-08 - val_loss: 1.2020e-08
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1774e-08 - val_loss: 1.1725e-08
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1478e-08 - val_loss: 1.1325e-08
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1038e-08 - val_loss: 1.0892e-08
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0676e-08 - val_loss: 1.0671e-08
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0476e-08 - val_loss: 1.0462e-08
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0208e-08 - val_loss: 1.0072e-08
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8137e-09 - val_loss: 9.7056e-09
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4977e-09 - val_loss: 9.4286e-09
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2715e-09 - val_loss: 9.2464e-09
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0493e-09 - val_loss: 8.9411e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7339e-09 - val_loss: 8.6415e-09
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4748e-09 - val_loss: 8.4727e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2903e-09 - val_loss: 8.2387e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0431e-09 - val_loss: 7.9777e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8045e-09 - val_loss: 7.7485e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5722e-09 - val_loss: 7.5381e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3673e-09 - val_loss: 7.3382e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1739e-09 - val_loss: 7.0747e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9450e-09 - val_loss: 6.9268e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8052e-09 - val_loss: 6.7534e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5936e-09 - val_loss: 6.5249e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3737e-09 - val_loss: 6.3316e-09
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2181e-09 - val_loss: 6.2113e-09
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0883e-09 - val_loss: 6.0705e-09
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8982e-09 - val_loss: 5.8560e-09
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7341e-09 - val_loss: 5.6851e-09
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5443e-09 - val_loss: 5.5119e-09
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4143e-09 - val_loss: 5.4196e-09
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3172e-09 - val_loss: 5.2948e-09
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1759e-09 - val_loss: 5.1189e-09
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9908e-09 - val_loss: 4.9417e-09
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8542e-09 - val_loss: 4.8625e-09
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7718e-09 - val_loss: 4.7463e-09
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6181e-09 - val_loss: 4.5635e-09
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4709e-09 - val_loss: 4.4872e-09
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3938e-09 - val_loss: 4.3723e-09
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2720e-09 - val_loss: 4.2539e-09
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1568e-09 - val_loss: 4.1218e-09
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0493e-09 - val_loss: 4.0612e-09
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9704e-09 - val_loss: 3.9473e-09
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8528e-09 - val_loss: 3.8050e-09
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7282e-09 - val_loss: 3.7304e-09
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6589e-09 - val_loss: 3.6670e-09
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5940e-09 - val_loss: 3.5681e-09
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4750e-09 - val_loss: 3.4456e-09
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3637e-09 - val_loss: 3.3459e-09
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2871e-09 - val_loss: 3.2916e-09
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2435e-09 - val_loss: 3.2480e-09
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1758e-09 - val_loss: 3.1616e-09
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0830e-09 - val_loss: 3.0594e-09
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9849e-09 - val_loss: 2.9526e-09
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8926e-09 - val_loss: 2.8998e-09
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8536e-09 - val_loss: 2.8635e-09
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8136e-09 - val_loss: 2.8034e-09
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7367e-09 - val_loss: 2.7169e-09
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6493e-09 - val_loss: 2.6209e-09
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5659e-09 - val_loss: 2.5682e-09
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5354e-09 - val_loss: 2.5663e-09
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5201e-09 - val_loss: 2.5111e-09
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4485e-09 - val_loss: 2.4233e-09
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3552e-09 - val_loss: 2.3179e-09
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2680e-09 - val_loss: 2.2713e-09
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2395e-09 - val_loss: 2.2609e-09
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2271e-09 - val_loss: 2.2247e-09
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1706e-09 - val_loss: 2.1646e-09
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1119e-09 - val_loss: 2.1011e-09
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0672e-09 - val_loss: 2.0581e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0181e-09 - val_loss: 2.0085e-09
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9755e-09 - val_loss: 1.9690e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9207e-09 - val_loss: 1.9271e-09
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9016e-09 - val_loss: 1.8978e-09
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8634e-09 - val_loss: 1.8384e-09
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7971e-09 - val_loss: 1.7969e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7627e-09 - val_loss: 1.7634e-09
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7395e-09 - val_loss: 1.7504e-09
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7187e-09 - val_loss: 1.7035e-09
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6678e-09 - val_loss: 1.6612e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6155e-09 - val_loss: 1.6053e-09
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5821e-09 - val_loss: 1.5956e-09
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5715e-09 - val_loss: 1.5857e-09
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5508e-09 - val_loss: 1.5469e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5155e-09 - val_loss: 1.5042e-09
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4689e-09 - val_loss: 1.4537e-09
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4258e-09 - val_loss: 1.4362e-09
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4081e-09 - val_loss: 1.4098e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3896e-09 - val_loss: 1.3959e-09
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3704e-09 - val_loss: 1.3675e-09
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3324e-09 - val_loss: 1.3128e-09
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2863e-09 - val_loss: 1.2873e-09
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2734e-09 - val_loss: 1.2831e-09
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2656e-09 - val_loss: 1.2590e-09
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2332e-09 - val_loss: 1.2287e-09
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2071e-09 - val_loss: 1.2023e-09
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1757e-09 - val_loss: 1.1696e-09
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1453e-09 - val_loss: 1.1538e-09
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1429e-09 - val_loss: 1.1570e-09
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1381e-09 - val_loss: 1.1291e-09
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0991e-09 - val_loss: 1.0905e-09
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0611e-09 - val_loss: 1.0584e-09
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0445e-09 - val_loss: 1.0514e-09
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0361e-09 - val_loss: 1.0414e-09
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0275e-09 - val_loss: 1.0276e-09
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0107e-09 - val_loss: 1.0121e-09
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.9419e-10 - val_loss: 9.9072e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6280e-10 - val_loss: 9.5180e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3475e-10 - val_loss: 9.3579e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2179e-10 - val_loss: 9.2367e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1097e-10 - val_loss: 9.1621e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0862e-10 - val_loss: 9.1787e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0130e-10 - val_loss: 9.0034e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8352e-10 - val_loss: 8.7945e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5327e-10 - val_loss: 8.4883e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3792e-10 - val_loss: 8.3908e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2064e-10 - val_loss: 8.2509e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0743e-10 - val_loss: 8.0236e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0040e-10 - val_loss: 8.0753e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8214e-10 - val_loss: 7.6728e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6231e-10 - val_loss: 7.7069e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5385e-10 - val_loss: 7.5689e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5179e-10 - val_loss: 7.5522e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4268e-10 - val_loss: 7.3943e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2697e-10 - val_loss: 7.3276e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1698e-10 - val_loss: 7.0350e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9051e-10 - val_loss: 6.9446e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8688e-10 - val_loss: 6.9994e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9462e-10 - val_loss: 6.9837e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8156e-10 - val_loss: 6.7215e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5246e-10 - val_loss: 6.4384e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2991e-10 - val_loss: 6.2302e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1447e-10 - val_loss: 6.2606e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2582e-10 - val_loss: 6.3894e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4230e-10 - val_loss: 6.5370e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3242e-10 - val_loss: 6.1985e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0636e-10 - val_loss: 5.9790e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8423e-10 - val_loss: 5.7796e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7320e-10 - val_loss: 5.8085e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7653e-10 - val_loss: 5.7929e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7287e-10 - val_loss: 5.7713e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6775e-10 - val_loss: 5.7331e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6447e-10 - val_loss: 5.6174e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5189e-10 - val_loss: 5.4451e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3101e-10 - val_loss: 5.1644e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0639e-10 - val_loss: 5.1091e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1050e-10 - val_loss: 5.1364e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0505e-10 - val_loss: 5.0602e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0739e-10 - val_loss: 5.2043e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0972e-10 - val_loss: 4.9705e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9238e-10 - val_loss: 4.8969e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8286e-10 - val_loss: 4.8617e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7982e-10 - val_loss: 4.7749e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7301e-10 - val_loss: 4.7211e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6593e-10 - val_loss: 4.6421e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5467e-10 - val_loss: 4.4790e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4060e-10 - val_loss: 4.4782e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4316e-10 - val_loss: 4.4778e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4925e-10 - val_loss: 4.5429e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4607e-10 - val_loss: 4.3934e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2514e-10 - val_loss: 4.1917e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1415e-10 - val_loss: 4.1784e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1151e-10 - val_loss: 4.1786e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1388e-10 - val_loss: 4.0794e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0352e-10 - val_loss: 4.1226e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0797e-10 - val_loss: 4.1104e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0312e-10 - val_loss: 3.9822e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8888e-10 - val_loss: 3.8669e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8390e-10 - val_loss: 3.8660e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7680e-10 - val_loss: 3.7525e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7188e-10 - val_loss: 3.7281e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6994e-10 - val_loss: 3.7638e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7162e-10 - val_loss: 3.6931e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6298e-10 - val_loss: 3.5716e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4819e-10 - val_loss: 3.5074e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5129e-10 - val_loss: 3.5932e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5546e-10 - val_loss: 3.5135e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4369e-10 - val_loss: 3.3715e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3870e-10 - val_loss: 3.4446e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3805e-10 - val_loss: 3.3822e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3413e-10 - val_loss: 3.3817e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3660e-10 - val_loss: 3.3541e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2953e-10 - val_loss: 3.2331e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1617e-10 - val_loss: 3.1291e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0795e-10 - val_loss: 3.1177e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0477e-10 - val_loss: 3.0377e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0314e-10 - val_loss: 3.0699e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0530e-10 - val_loss: 3.0482e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0157e-10 - val_loss: 3.0265e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0195e-10 - val_loss: 3.0956e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0926e-10 - val_loss: 3.0801e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9951e-10 - val_loss: 2.9470e-10
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8618e-10 - val_loss: 2.7978e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7321e-10 - val_loss: 2.7463e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7429e-10 - val_loss: 2.8471e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8663e-10 - val_loss: 2.9305e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8734e-10 - val_loss: 2.8124e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7171e-10 - val_loss: 2.6756e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6233e-10 - val_loss: 2.6054e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5968e-10 - val_loss: 2.6360e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5878e-10 - val_loss: 2.5811e-10
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5690e-10 - val_loss: 2.6499e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6550e-10 - val_loss: 2.7087e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6539e-10 - val_loss: 2.6254e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5669e-10 - val_loss: 2.5271e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4964e-10 - val_loss: 2.4173e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3209e-10 - val_loss: 2.2998e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2924e-10 - val_loss: 2.3551e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3681e-10 - val_loss: 2.4590e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4284e-10 - val_loss: 2.4210e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4305e-10 - val_loss: 2.4402e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3570e-10 - val_loss: 2.3082e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2664e-10 - val_loss: 2.2801e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2526e-10 - val_loss: 2.2622e-10
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2624e-10 - val_loss: 2.2895e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2502e-10 - val_loss: 2.2205e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1958e-10 - val_loss: 2.1932e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2067e-10 - val_loss: 2.2935e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2702e-10 - val_loss: 2.2849e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2683e-10 - val_loss: 2.2752e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2544e-10 - val_loss: 2.2155e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1908e-10 - val_loss: 2.1751e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1066e-10 - val_loss: 2.0123e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9865e-10 - val_loss: 2.0069e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0270e-10 - val_loss: 2.0850e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0783e-10 - val_loss: 2.1126e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1008e-10 - val_loss: 2.0908e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0354e-10 - val_loss: 2.0202e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9759e-10 - val_loss: 1.9724e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9340e-10 - val_loss: 1.9472e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9195e-10 - val_loss: 1.9466e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9394e-10 - val_loss: 1.9301e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9130e-10 - val_loss: 1.9085e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8931e-10 - val_loss: 1.8912e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9054e-10 - val_loss: 1.9131e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8808e-10 - val_loss: 1.8664e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8339e-10 - val_loss: 1.8006e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7906e-10 - val_loss: 1.8411e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8501e-10 - val_loss: 1.8496e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8001e-10 - val_loss: 1.7595e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7192e-10 - val_loss: 1.6872e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6882e-10 - val_loss: 1.7029e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6617e-10 - val_loss: 1.6639e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6405e-10 - val_loss: 1.6399e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6480e-10 - val_loss: 1.6916e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6961e-10 - val_loss: 1.7795e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7603e-10 - val_loss: 1.7472e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7142e-10 - val_loss: 1.6790e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6519e-10 - val_loss: 1.6512e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6208e-10 - val_loss: 1.5857e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5593e-10 - val_loss: 1.5324e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5094e-10 - val_loss: 1.5244e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5224e-10 - val_loss: 1.5129e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5222e-10 - val_loss: 1.5774e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5763e-10 - val_loss: 1.5929e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5727e-10 - val_loss: 1.5473e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5077e-10 - val_loss: 1.4830e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4521e-10 - val_loss: 1.4283e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4236e-10 - val_loss: 1.4570e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4656e-10 - val_loss: 1.5088e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5073e-10 - val_loss: 1.4980e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4927e-10 - val_loss: 1.4903e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4666e-10 - val_loss: 1.4532e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4582e-10 - val_loss: 1.4717e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4630e-10 - val_loss: 1.4602e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4499e-10 - val_loss: 1.4600e-10
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4175e-10 - val_loss: 1.3593e-10
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3317e-10 - val_loss: 1.3102e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3273e-10 - val_loss: 1.3589e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3509e-10 - val_loss: 1.3948e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3927e-10 - val_loss: 1.4160e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4071e-10 - val_loss: 1.4157e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3989e-10 - val_loss: 1.4343e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4102e-10 - val_loss: 1.3697e-10
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3343e-10 - val_loss: 1.2749e-10
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2664e-10 - val_loss: 1.2546e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2484e-10 - val_loss: 1.2747e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2724e-10 - val_loss: 1.3018e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3234e-10 - val_loss: 1.3655e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3402e-10 - val_loss: 1.3024e-10
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2582e-10 - val_loss: 1.2435e-10
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2306e-10 - val_loss: 1.2168e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2045e-10 - val_loss: 1.2367e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2340e-10 - val_loss: 1.2676e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2596e-10 - val_loss: 1.2710e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2689e-10 - val_loss: 1.3090e-10
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2922e-10 - val_loss: 1.2677e-10
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2317e-10 - val_loss: 1.1929e-10
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1845e-10 - val_loss: 1.1899e-10
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1688e-10 - val_loss: 1.1997e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2145e-10 - val_loss: 1.2535e-10
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2291e-10 - val_loss: 1.1766e-10
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1318e-10 - val_loss: 1.0826e-10
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0747e-10 - val_loss: 1.0759e-10
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0706e-10 - val_loss: 1.0952e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1025e-10 - val_loss: 1.0985e-10
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0810e-10 - val_loss: 1.0664e-10
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0714e-10 - val_loss: 1.0640e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0731e-10 - val_loss: 1.0828e-10
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0747e-10 - val_loss: 1.0730e-10
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0770e-10 - val_loss: 1.0890e-10
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0842e-10 - val_loss: 1.0698e-10
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0606e-10 - val_loss: 1.0447e-10
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0449e-10 - val_loss: 1.0597e-10
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0580e-10 - val_loss: 1.0725e-10
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0587e-10 - val_loss: 1.0318e-10
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0125e-10 - val_loss: 9.9542e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6807e-11 - val_loss: 9.3557e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3363e-11 - val_loss: 9.4432e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3593e-11 - val_loss: 9.2638e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3582e-11 - val_loss: 9.6208e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5730e-11 - val_loss: 9.8563e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0238e-10 - val_loss: 1.0692e-10
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0661e-10 - val_loss: 1.0289e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0107e-10 - val_loss: 1.0254e-10
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0281e-10 - val_loss: 1.0565e-10
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0111e-10 - val_loss: 9.5586e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2419e-11 - val_loss: 8.7729e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3230e-11 - val_loss: 8.0202e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1190e-11 - val_loss: 8.5699e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6489e-11 - val_loss: 8.9129e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9587e-11 - val_loss: 9.0881e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0799e-11 - val_loss: 9.2581e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3766e-11 - val_loss: 9.4075e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2926e-11 - val_loss: 9.4655e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4890e-11 - val_loss: 9.7059e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5712e-11 - val_loss: 9.2591e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0029e-11 - val_loss: 8.4302e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3529e-11 - val_loss: 8.4286e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2365e-11 - val_loss: 8.1239e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0170e-11 - val_loss: 8.2306e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3330e-11 - val_loss: 8.5954e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7143e-11 - val_loss: 8.5936e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4393e-11 - val_loss: 8.1800e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0730e-11 - val_loss: 8.1800e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9524e-11 - val_loss: 7.9311e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9417e-11 - val_loss: 8.1232e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1049e-11 - val_loss: 8.3143e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2231e-11 - val_loss: 8.2602e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2375e-11 - val_loss: 8.4246e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3200e-11 - val_loss: 8.3114e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3327e-11 - val_loss: 8.3392e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4510e-11 - val_loss: 8.5631e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1434e-11 - val_loss: 7.6881e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5675e-11 - val_loss: 7.4741e-11
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3187e-11 - val_loss: 7.1375e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2195e-11 - val_loss: 7.4724e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4693e-11 - val_loss: 7.4485e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5017e-11 - val_loss: 7.5823e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6195e-11 - val_loss: 7.9274e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1743e-11 - val_loss: 8.5093e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3402e-11 - val_loss: 8.2008e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2224e-11 - val_loss: 8.1715e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0430e-11 - val_loss: 7.8713e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0226e-11 - val_loss: 8.2014e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8901e-11 - val_loss: 7.7128e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7067e-11 - val_loss: 7.7421e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6741e-11 - val_loss: 7.5520e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4089e-11 - val_loss: 7.1869e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9831e-11 - val_loss: 7.0799e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0638e-11 - val_loss: 7.0073e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9185e-11 - val_loss: 7.1042e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0971e-11 - val_loss: 7.2089e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0841e-11 - val_loss: 7.1817e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2295e-11 - val_loss: 7.5757e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6573e-11 - val_loss: 7.6023e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4279e-11 - val_loss: 7.4213e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3556e-11 - val_loss: 7.3399e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2581e-11 - val_loss: 7.1083e-11
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0081e-11 - val_loss: 7.0056e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8312e-11 - val_loss: 6.7494e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7657e-11 - val_loss: 6.9252e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7058e-11 - val_loss: 6.4766e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4509e-11 - val_loss: 6.4979e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5993e-11 - val_loss: 6.5756e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6480e-11 - val_loss: 7.0247e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0714e-11 - val_loss: 6.9491e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8051e-11 - val_loss: 6.6284e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3692e-11 - val_loss: 6.2333e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1563e-11 - val_loss: 6.0461e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8798e-11 - val_loss: 5.7144e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8251e-11 - val_loss: 5.9489e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0201e-11 - val_loss: 6.1620e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0608e-11 - val_loss: 6.1129e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3294e-11 - val_loss: 6.6006e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5702e-11 - val_loss: 6.5735e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4211e-11 - val_loss: 6.2582e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2203e-11 - val_loss: 6.2799e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2684e-11 - val_loss: 6.3798e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3675e-11 - val_loss: 6.0929e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0013e-11 - val_loss: 6.1350e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0592e-11 - val_loss: 5.9031e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7288e-11 - val_loss: 5.5352e-11
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4216e-11 - val_loss: 5.4619e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4765e-11 - val_loss: 5.5102e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4151e-11 - val_loss: 5.5339e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5785e-11 - val_loss: 5.6692e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6594e-11 - val_loss: 5.6472e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5681e-11 - val_loss: 5.5772e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6483e-11 - val_loss: 5.7606e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6809e-11 - val_loss: 5.5316e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3337e-11 - val_loss: 5.1118e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0464e-11 - val_loss: 5.1734e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0109e-11 - val_loss: 4.8109e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7593e-11 - val_loss: 4.7032e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6595e-11 - val_loss: 4.7846e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8426e-11 - val_loss: 5.0450e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1712e-11 - val_loss: 5.3068e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4099e-11 - val_loss: 5.5280e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5835e-11 - val_loss: 5.6433e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5961e-11 - val_loss: 5.4178e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2652e-11 - val_loss: 5.1311e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0463e-11 - val_loss: 4.9993e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1127e-11 - val_loss: 5.4166e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5113e-11 - val_loss: 5.4608e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4529e-11 - val_loss: 5.5071e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6055e-11 - val_loss: 5.5970e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5331e-11 - val_loss: 5.3047e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1933e-11 - val_loss: 5.0879e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9987e-11 - val_loss: 4.9372e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9403e-11 - val_loss: 4.9782e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9280e-11 - val_loss: 4.9793e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.0489 - val_loss: 0.0129
Epoch 2/512
512/512 - 0s - loss: 0.0226 - val_loss: 0.0096
Epoch 3/512
512/512 - 0s - loss: 0.0250 - val_loss: 0.0022
Epoch 4/512
512/512 - 0s - loss: 0.0186 - val_loss: 0.0042
Epoch 5/512
512/512 - 0s - loss: 0.0163 - val_loss: 0.0055
Epoch 6/512
512/512 - 0s - loss: 0.0145 - val_loss: 0.0051
Epoch 7/512
512/512 - 0s - loss: 0.0127 - val_loss: 0.0046
Epoch 8/512
512/512 - 0s - loss: 0.0109 - val_loss: 0.0033
Epoch 9/512
512/512 - 0s - loss: 0.0094 - val_loss: 0.0019
Epoch 10/512
512/512 - 0s - loss: 0.0083 - val_loss: 0.0016
Epoch 11/512
512/512 - 0s - loss: 0.0070 - val_loss: 0.0013
Epoch 12/512
512/512 - 0s - loss: 0.0056 - val_loss: 0.0011
Epoch 13/512
512/512 - 0s - loss: 0.0042 - val_loss: 8.4874e-04
Epoch 14/512
512/512 - 0s - loss: 0.0031 - val_loss: 6.6938e-04
Epoch 15/512
512/512 - 0s - loss: 0.0021 - val_loss: 5.5022e-04
Epoch 16/512
512/512 - 0s - loss: 0.0014 - val_loss: 4.9941e-04
Epoch 17/512
512/512 - 0s - loss: 0.0010 - val_loss: 4.9220e-04
Epoch 18/512
512/512 - 0s - loss: 7.1887e-04 - val_loss: 5.2962e-04
Epoch 19/512
512/512 - 0s - loss: 5.7319e-04 - val_loss: 9.7115e-04
Epoch 20/512
512/512 - 0s - loss: 5.5604e-04 - val_loss: 7.8082e-04
Epoch 21/512
512/512 - 0s - loss: 3.3367e-04 - val_loss: 6.2110e-04
Epoch 22/512
512/512 - 0s - loss: 2.5878e-04 - val_loss: 6.3972e-04
Epoch 23/512
512/512 - 0s - loss: 2.4559e-04 - val_loss: 9.1628e-04
Epoch 24/512
512/512 - 0s - loss: 2.5699e-04 - val_loss: 7.8087e-04
Epoch 25/512
512/512 - 0s - loss: 1.6639e-04 - val_loss: 6.6566e-04
Epoch 26/512
512/512 - 0s - loss: 1.4622e-04 - val_loss: 7.6675e-04
Epoch 27/512
512/512 - 0s - loss: 1.6476e-04 - val_loss: 8.2214e-04
Epoch 28/512
512/512 - 0s - loss: 1.2776e-04 - val_loss: 6.6077e-04
Epoch 29/512
512/512 - 0s - loss: 1.0413e-04 - val_loss: 7.0568e-04
Epoch 30/512
512/512 - 0s - loss: 1.1345e-04 - val_loss: 7.4185e-04
Epoch 31/512
512/512 - 0s - loss: 9.3981e-05 - val_loss: 6.2592e-04
Epoch 32/512
512/512 - 0s - loss: 8.1414e-05 - val_loss: 6.8861e-04
Epoch 33/512
512/512 - 0s - loss: 8.3350e-05 - val_loss: 6.2026e-04
Epoch 34/512
512/512 - 0s - loss: 6.8528e-05 - val_loss: 6.0498e-04
Epoch 35/512
512/512 - 0s - loss: 6.8286e-05 - val_loss: 5.9964e-04
Epoch 36/512
512/512 - 0s - loss: 5.8932e-05 - val_loss: 5.3343e-04
Epoch 37/512
512/512 - 0s - loss: 5.6163e-05 - val_loss: 5.8760e-04
Epoch 38/512
512/512 - 0s - loss: 5.1117e-05 - val_loss: 4.9944e-04
Epoch 39/512
512/512 - 0s - loss: 4.7229e-05 - val_loss: 5.2064e-04
Epoch 40/512
512/512 - 0s - loss: 4.3006e-05 - val_loss: 4.8459e-04
Epoch 41/512
512/512 - 0s - loss: 4.0881e-05 - val_loss: 4.6310e-04
Epoch 42/512
512/512 - 0s - loss: 3.6777e-05 - val_loss: 4.7030e-04
Epoch 43/512
512/512 - 0s - loss: 3.4524e-05 - val_loss: 4.1994e-04
Epoch 44/512
512/512 - 0s - loss: 3.2103e-05 - val_loss: 4.3294e-04
Epoch 45/512
512/512 - 0s - loss: 2.9593e-05 - val_loss: 4.2280e-04
Epoch 46/512
512/512 - 0s - loss: 2.7841e-05 - val_loss: 3.5775e-04
Epoch 47/512
512/512 - 0s - loss: 2.6002e-05 - val_loss: 3.2793e-04
Epoch 48/512
512/512 - 0s - loss: 2.3477e-05 - val_loss: 3.9162e-04
Epoch 49/512
512/512 - 0s - loss: 2.1641e-05 - val_loss: 4.3075e-04
Epoch 50/512
512/512 - 0s - loss: 2.0794e-05 - val_loss: 3.8104e-04
Epoch 51/512
512/512 - 0s - loss: 2.0089e-05 - val_loss: 3.0152e-04
Epoch 52/512
512/512 - 0s - loss: 1.8484e-05 - val_loss: 2.1443e-04
Epoch 53/512
512/512 - 0s - loss: 1.5932e-05 - val_loss: 2.8406e-04
Epoch 54/512
512/512 - 0s - loss: 1.7353e-05 - val_loss: 1.4551e-04
Epoch 55/512
512/512 - 0s - loss: 1.0850e-05 - val_loss: 3.8233e-04
Epoch 56/512
512/512 - 0s - loss: 1.5463e-05 - val_loss: 2.3337e-04
Epoch 57/512
512/512 - 0s - loss: 1.4912e-05 - val_loss: 1.5958e-04
Epoch 58/512
512/512 - 0s - loss: 9.0260e-06 - val_loss: 7.6895e-05
Epoch 59/512
512/512 - 0s - loss: 1.5365e-05 - val_loss: 3.9235e-04
Epoch 60/512
512/512 - 0s - loss: 1.1933e-05 - val_loss: 2.0963e-04
Epoch 61/512
512/512 - 0s - loss: 8.0836e-06 - val_loss: 1.0356e-04
Epoch 62/512
512/512 - 0s - loss: 9.3377e-06 - val_loss: 4.8566e-04
Epoch 63/512
512/512 - 0s - loss: 1.5626e-05 - val_loss: 2.8639e-04
Epoch 64/512
512/512 - 0s - loss: 8.5376e-06 - val_loss: 2.1394e-04
Epoch 65/512
512/512 - 0s - loss: 6.7868e-06 - val_loss: 1.2639e-04
Epoch 66/512
512/512 - 0s - loss: 5.4136e-06 - val_loss: 7.7097e-05
Epoch 67/512
512/512 - 0s - loss: 9.0175e-06 - val_loss: 3.0161e-04
Epoch 68/512
512/512 - 0s - loss: 7.1118e-06 - val_loss: 1.7972e-04
Epoch 69/512
512/512 - 0s - loss: 5.1307e-06 - val_loss: 9.2177e-05
Epoch 70/512
512/512 - 0s - loss: 4.8039e-06 - val_loss: 3.1482e-05
Epoch 71/512
512/512 - 0s - loss: 6.5441e-06 - val_loss: 1.4555e-04
Epoch 72/512
512/512 - 0s - loss: 4.0401e-06 - val_loss: 1.1302e-04
Epoch 73/512
512/512 - 0s - loss: 6.5637e-06 - val_loss: 2.4104e-04
Epoch 74/512
512/512 - 0s - loss: 5.0424e-06 - val_loss: 1.8310e-04
Epoch 75/512
512/512 - 0s - loss: 4.0719e-06 - val_loss: 1.2629e-04
Epoch 76/512
512/512 - 0s - loss: 3.1662e-06 - val_loss: 9.3134e-05
Epoch 77/512
512/512 - 0s - loss: 3.5382e-06 - val_loss: 1.2364e-04
Epoch 78/512
512/512 - 0s - loss: 3.7326e-06 - val_loss: 1.2362e-04
Epoch 79/512
512/512 - 0s - loss: 2.7124e-06 - val_loss: 1.0378e-04
Epoch 80/512
512/512 - 0s - loss: 4.3394e-06 - val_loss: 1.9784e-04
Epoch 81/512
512/512 - 0s - loss: 3.5874e-06 - val_loss: 1.6415e-04
Epoch 82/512
512/512 - 0s - loss: 3.0448e-06 - val_loss: 1.4016e-04
Epoch 83/512
512/512 - 0s - loss: 2.6111e-06 - val_loss: 1.0458e-04
Epoch 84/512
512/512 - 0s - loss: 2.0889e-06 - val_loss: 8.6566e-05
Epoch 85/512
512/512 - 0s - loss: 2.9776e-06 - val_loss: 1.5069e-04
Epoch 86/512
512/512 - 0s - loss: 2.4466e-06 - val_loss: 1.1385e-04
Epoch 87/512
512/512 - 0s - loss: 1.9794e-06 - val_loss: 6.7580e-05
Epoch 88/512
512/512 - 0s - loss: 1.6256e-06 - val_loss: 1.3206e-04
Epoch 89/512
512/512 - 0s - loss: 1.8125e-06 - val_loss: 1.3880e-04
Epoch 90/512
512/512 - 0s - loss: 1.8480e-06 - val_loss: 6.4047e-05
Epoch 91/512
512/512 - 0s - loss: 4.3610e-06 - val_loss: 1.5574e-04
Epoch 92/512
512/512 - 0s - loss: 2.9139e-06 - val_loss: 1.4109e-04
Epoch 93/512
512/512 - 0s - loss: 2.3990e-06 - val_loss: 1.2984e-04
Epoch 94/512
512/512 - 0s - loss: 2.0669e-06 - val_loss: 1.2026e-04
Epoch 95/512
512/512 - 0s - loss: 1.8247e-06 - val_loss: 1.1182e-04
Epoch 96/512
512/512 - 0s - loss: 1.6447e-06 - val_loss: 1.0427e-04
Epoch 97/512
512/512 - 0s - loss: 1.5112e-06 - val_loss: 9.7525e-05
Epoch 98/512
512/512 - 0s - loss: 1.4096e-06 - val_loss: 9.1478e-05
Epoch 99/512
512/512 - 0s - loss: 1.3252e-06 - val_loss: 8.5880e-05
Epoch 100/512
512/512 - 0s - loss: 1.2494e-06 - val_loss: 8.0697e-05
Epoch 101/512
512/512 - 0s - loss: 1.1749e-06 - val_loss: 7.5408e-05
Epoch 102/512
512/512 - 0s - loss: 1.1030e-06 - val_loss: 7.0464e-05
Epoch 103/512
512/512 - 0s - loss: 1.0298e-06 - val_loss: 6.5306e-05
Epoch 104/512
512/512 - 0s - loss: 9.6073e-07 - val_loss: 6.0861e-05
Epoch 105/512
512/512 - 0s - loss: 9.0858e-07 - val_loss: 5.6889e-05
Epoch 106/512
512/512 - 0s - loss: 8.7001e-07 - val_loss: 5.4726e-05
Epoch 107/512
512/512 - 0s - loss: 8.3682e-07 - val_loss: 5.1281e-05
Epoch 108/512
512/512 - 0s - loss: 8.0463e-07 - val_loss: 4.9916e-05
Epoch 109/512
512/512 - 0s - loss: 7.7496e-07 - val_loss: 4.6918e-05
Epoch 110/512
512/512 - 0s - loss: 7.5435e-07 - val_loss: 5.1226e-05
Epoch 111/512
512/512 - 0s - loss: 7.8809e-07 - val_loss: 4.6439e-05
Epoch 112/512
512/512 - 0s - loss: 7.1616e-07 - val_loss: 4.3772e-05
Epoch 113/512
512/512 - 0s - loss: 6.8479e-07 - val_loss: 4.1827e-05
Epoch 114/512
512/512 - 0s - loss: 6.7653e-07 - val_loss: 3.8912e-05
Epoch 115/512
512/512 - 0s - loss: 6.4402e-07 - val_loss: 3.8267e-05
Epoch 116/512
512/512 - 0s - loss: 6.3079e-07 - val_loss: 4.6864e-05
Epoch 117/512
512/512 - 0s - loss: 6.1513e-07 - val_loss: 3.8458e-05
Epoch 118/512
512/512 - 0s - loss: 5.5603e-07 - val_loss: 3.0177e-05
Epoch 119/512
512/512 - 0s - loss: 5.1896e-07 - val_loss: 3.3518e-05
Epoch 120/512
512/512 - 0s - loss: 5.6310e-07 - val_loss: 2.7673e-05
Epoch 121/512
512/512 - 0s - loss: 4.7195e-07 - val_loss: 3.4090e-05
Epoch 122/512
512/512 - 0s - loss: 6.0897e-07 - val_loss: 3.2627e-05
Epoch 123/512
512/512 - 0s - loss: 4.8506e-07 - val_loss: 1.8830e-05
Epoch 124/512
512/512 - 0s - loss: 5.1127e-07 - val_loss: 2.2962e-05
Epoch 125/512
512/512 - 0s - loss: 4.5187e-07 - val_loss: 1.6991e-05
Epoch 126/512
512/512 - 0s - loss: 4.9391e-07 - val_loss: 2.1657e-05
Epoch 127/512
512/512 - 0s - loss: 4.1348e-07 - val_loss: 2.4571e-05
Epoch 128/512
512/512 - 0s - loss: 6.4944e-07 - val_loss: 4.8040e-05
Epoch 129/512
512/512 - 0s - loss: 5.9680e-07 - val_loss: 4.0349e-05
Epoch 130/512
512/512 - 0s - loss: 5.4933e-07 - val_loss: 3.6755e-05
Epoch 131/512
512/512 - 0s - loss: 5.2758e-07 - val_loss: 3.3768e-05
Epoch 132/512
512/512 - 0s - loss: 4.9065e-07 - val_loss: 3.0570e-05
Epoch 133/512
512/512 - 0s - loss: 4.5806e-07 - val_loss: 2.6287e-05
Epoch 134/512
512/512 - 0s - loss: 4.1174e-07 - val_loss: 2.3126e-05
Epoch 135/512
512/512 - 0s - loss: 3.9466e-07 - val_loss: 3.1140e-05
Epoch 136/512
512/512 - 0s - loss: 3.8455e-07 - val_loss: 2.4071e-05
Epoch 137/512
512/512 - 0s - loss: 3.7839e-07 - val_loss: 1.7831e-05
Epoch 138/512
512/512 - 0s - loss: 3.4829e-07 - val_loss: 3.3126e-05
Epoch 139/512
512/512 - 0s - loss: 3.6878e-07 - val_loss: 2.3241e-05
Epoch 140/512
512/512 - 0s - loss: 4.6627e-07 - val_loss: 3.5897e-05
Epoch 141/512
512/512 - 0s - loss: 4.5123e-07 - val_loss: 2.6882e-05
Epoch 142/512
512/512 - 0s - loss: 3.8431e-07 - val_loss: 1.8401e-05
Epoch 143/512
512/512 - 0s - loss: 3.3899e-07 - val_loss: 2.8701e-05
Epoch 144/512
512/512 - 0s - loss: 3.4207e-07 - val_loss: 1.6221e-05
Epoch 145/512
512/512 - 0s - loss: 3.3880e-07 - val_loss: 1.2869e-05
Epoch 146/512
512/512 - 0s - loss: 3.4305e-07 - val_loss: 1.4714e-05
Epoch 147/512
512/512 - 0s - loss: 3.0184e-07 - val_loss: 2121.9174
Epoch 148/512
512/512 - 0s - loss: 530.4792 - val_loss: 4.4942e-05
Epoch 149/512
512/512 - 0s - loss: 5.7161e-07 - val_loss: 4.4076e-05
Epoch 150/512
512/512 - 0s - loss: 5.4887e-07 - val_loss: 4.3290e-05
Epoch 151/512
512/512 - 0s - loss: 5.3327e-07 - val_loss: 4.2555e-05
Epoch 152/512
512/512 - 0s - loss: 5.2036e-07 - val_loss: 4.1854e-05
Epoch 153/512
512/512 - 0s - loss: 5.0885e-07 - val_loss: 4.1182e-05
Epoch 154/512
512/512 - 0s - loss: 4.9830e-07 - val_loss: 4.0532e-05
Epoch 155/512
512/512 - 0s - loss: 4.8830e-07 - val_loss: 3.9901e-05
Epoch 156/512
512/512 - 0s - loss: 4.7879e-07 - val_loss: 3.9286e-05
Epoch 157/512
512/512 - 0s - loss: 4.6968e-07 - val_loss: 3.8683e-05
Epoch 158/512
512/512 - 0s - loss: 4.6094e-07 - val_loss: 3.8091e-05
Epoch 159/512
512/512 - 0s - loss: 4.5243e-07 - val_loss: 3.7506e-05
Epoch 160/512
512/512 - 0s - loss: 4.4407e-07 - val_loss: 3.6922e-05
Epoch 161/512
512/512 - 0s - loss: 4.3584e-07 - val_loss: 3.6338e-05
Epoch 162/512
512/512 - 0s - loss: 4.2773e-07 - val_loss: 3.5749e-05
Epoch 163/512
512/512 - 0s - loss: 4.1965e-07 - val_loss: 3.5151e-05
Epoch 164/512
512/512 - 0s - loss: 4.1151e-07 - val_loss: 3.4538e-05
Epoch 165/512
512/512 - 0s - loss: 4.0325e-07 - val_loss: 3.3909e-05
Epoch 166/512
512/512 - 0s - loss: 4.2587e-07 - val_loss: 3.4617e-05
Epoch 167/512
512/512 - 0s - loss: 4.0363e-07 - val_loss: 3.4194e-05
Epoch 168/512
512/512 - 0s - loss: 3.9790e-07 - val_loss: 3.3782e-05
Epoch 169/512
512/512 - 0s - loss: 3.9239e-07 - val_loss: 3.3382e-05
Epoch 170/512
512/512 - 0s - loss: 3.8701e-07 - val_loss: 3.2991e-05
Epoch 171/512
512/512 - 0s - loss: 3.8179e-07 - val_loss: 3.2609e-05
Epoch 172/512
512/512 - 0s - loss: 3.7672e-07 - val_loss: 3.2237e-05
Epoch 173/512
512/512 - 0s - loss: 3.7182e-07 - val_loss: 3.1872e-05
Epoch 174/512
512/512 - 0s - loss: 3.6702e-07 - val_loss: 3.1516e-05
Epoch 175/512
512/512 - 0s - loss: 3.6232e-07 - val_loss: 3.1166e-05
Epoch 176/512
512/512 - 0s - loss: 3.5779e-07 - val_loss: 3.0824e-05
Epoch 177/512
512/512 - 0s - loss: 3.5332e-07 - val_loss: 3.0486e-05
Epoch 178/512
512/512 - 0s - loss: 3.4893e-07 - val_loss: 3.0155e-05
Epoch 179/512
512/512 - 0s - loss: 3.4464e-07 - val_loss: 2.9827e-05
Epoch 180/512
512/512 - 0s - loss: 3.4043e-07 - val_loss: 2.9502e-05
Epoch 181/512
512/512 - 0s - loss: 3.3624e-07 - val_loss: 2.9180e-05
Epoch 182/512
512/512 - 0s - loss: 3.3211e-07 - val_loss: 2.8859e-05
Epoch 183/512
512/512 - 0s - loss: 3.2799e-07 - val_loss: 2.8536e-05
Epoch 184/512
512/512 - 0s - loss: 3.2390e-07 - val_loss: 2.8210e-05
Epoch 185/512
512/512 - 0s - loss: 3.1977e-07 - val_loss: 2.7880e-05
Epoch 186/512
512/512 - 0s - loss: 3.1559e-07 - val_loss: 2.7541e-05
Epoch 187/512
512/512 - 0s - loss: 3.1130e-07 - val_loss: 2.7190e-05
Epoch 188/512
512/512 - 0s - loss: 3.0690e-07 - val_loss: 2.6823e-05
Epoch 189/512
512/512 - 0s - loss: 3.0232e-07 - val_loss: 2.6437e-05
Epoch 190/512
512/512 - 0s - loss: 2.9753e-07 - val_loss: 2.6028e-05
Epoch 191/512
512/512 - 0s - loss: 2.9251e-07 - val_loss: 2.5717e-05
Epoch 192/512
512/512 - 0s - loss: 3.2675e-07 - val_loss: 2.6469e-05
Epoch 193/512
512/512 - 0s - loss: 2.9786e-07 - val_loss: 2.6238e-05
Epoch 194/512
512/512 - 0s - loss: 2.9500e-07 - val_loss: 2.6010e-05
Epoch 195/512
512/512 - 0s - loss: 2.9217e-07 - val_loss: 2.5787e-05
Epoch 196/512
512/512 - 0s - loss: 2.8941e-07 - val_loss: 2.5568e-05
Epoch 197/512
512/512 - 0s - loss: 2.8670e-07 - val_loss: 2.5352e-05
Epoch 198/512
512/512 - 0s - loss: 2.8405e-07 - val_loss: 2.5141e-05
Epoch 199/512
512/512 - 0s - loss: 2.8144e-07 - val_loss: 2.4931e-05
Epoch 200/512
512/512 - 0s - loss: 2.7886e-07 - val_loss: 2.4726e-05
Epoch 201/512
512/512 - 0s - loss: 2.7637e-07 - val_loss: 2.4523e-05
Epoch 202/512
512/512 - 0s - loss: 2.7388e-07 - val_loss: 2.4321e-05
Epoch 203/512
512/512 - 0s - loss: 2.7140e-07 - val_loss: 2.4122e-05
Epoch 204/512
512/512 - 0s - loss: 2.6899e-07 - val_loss: 2.3924e-05
Epoch 205/512
512/512 - 0s - loss: 2.6659e-07 - val_loss: 2.3727e-05
Epoch 206/512
512/512 - 0s - loss: 2.6419e-07 - val_loss: 2.3530e-05
Epoch 207/512
512/512 - 0s - loss: 2.6180e-07 - val_loss: 2.3332e-05
Epoch 208/512
512/512 - 0s - loss: 2.5939e-07 - val_loss: 2.3132e-05
Epoch 209/512
512/512 - 0s - loss: 2.5696e-07 - val_loss: 2.2929e-05
Epoch 210/512
512/512 - 0s - loss: 2.5450e-07 - val_loss: 2.2722e-05
Epoch 211/512
512/512 - 0s - loss: 2.5199e-07 - val_loss: 2.2508e-05
Epoch 212/512
512/512 - 0s - loss: 2.4942e-07 - val_loss: 2.2286e-05
Epoch 213/512
512/512 - 0s - loss: 2.4674e-07 - val_loss: 2.2052e-05
Epoch 214/512
512/512 - 0s - loss: 2.4393e-07 - val_loss: 2.1803e-05
Epoch 215/512
512/512 - 0s - loss: 2.4095e-07 - val_loss: 2.1537e-05
Epoch 216/512
512/512 - 0s - loss: 2.3777e-07 - val_loss: 2.1249e-05
Epoch 217/512
512/512 - 0s - loss: 2.3437e-07 - val_loss: 2.0938e-05
Epoch 218/512
512/512 - 0s - loss: 4.3093e-07 - val_loss: 2.1705e-05
Epoch 219/512
512/512 - 0s - loss: 2.3984e-07 - val_loss: 2.1557e-05
Epoch 220/512
512/512 - 0s - loss: 2.3809e-07 - val_loss: 2.1412e-05
Epoch 221/512
512/512 - 0s - loss: 2.3635e-07 - val_loss: 2.1268e-05
Epoch 222/512
512/512 - 0s - loss: 2.3464e-07 - val_loss: 2.1127e-05
Epoch 223/512
512/512 - 0s - loss: 2.3295e-07 - val_loss: 2.0986e-05
Epoch 224/512
512/512 - 0s - loss: 2.3128e-07 - val_loss: 2.0848e-05
Epoch 225/512
512/512 - 0s - loss: 2.2963e-07 - val_loss: 2.0710e-05
Epoch 226/512
512/512 - 0s - loss: 2.2800e-07 - val_loss: 2.0574e-05
Epoch 227/512
512/512 - 0s - loss: 2.2637e-07 - val_loss: 2.0438e-05
Epoch 228/512
512/512 - 0s - loss: 2.2475e-07 - val_loss: 2.0302e-05
Epoch 229/512
512/512 - 0s - loss: 2.2315e-07 - val_loss: 2.0167e-05
Epoch 230/512
512/512 - 0s - loss: 2.2155e-07 - val_loss: 2.0031e-05
Epoch 231/512
512/512 - 0s - loss: 2.1995e-07 - val_loss: 1.9895e-05
Epoch 232/512
512/512 - 0s - loss: 2.1833e-07 - val_loss: 1.9756e-05
Epoch 233/512
512/512 - 0s - loss: 2.1670e-07 - val_loss: 1.9616e-05
Epoch 234/512
512/512 - 0s - loss: 2.1505e-07 - val_loss: 1.9472e-05
Epoch 235/512
512/512 - 0s - loss: 2.1336e-07 - val_loss: 1.9324e-05
Epoch 236/512
512/512 - 0s - loss: 2.1160e-07 - val_loss: 1.9169e-05
Epoch 237/512
512/512 - 0s - loss: 2.0979e-07 - val_loss: 1.9007e-05
Epoch 238/512
512/512 - 0s - loss: 2.0791e-07 - val_loss: 1.8836e-05
Epoch 239/512
512/512 - 0s - loss: 2.0594e-07 - val_loss: 1.8652e-05
Epoch 240/512
512/512 - 0s - loss: 2.0384e-07 - val_loss: 1.8453e-05
Epoch 241/512
512/512 - 0s - loss: 2.0160e-07 - val_loss: 1.8235e-05
Epoch 242/512
512/512 - 0s - loss: 1.9918e-07 - val_loss: 1.7991e-05
Epoch 243/512
512/512 - 0s - loss: 1.9656e-07 - val_loss: 1.7718e-05
Epoch 244/512
512/512 - 0s - loss: 1.9372e-07 - val_loss: 1.7406e-05
Epoch 245/512
512/512 - 0s - loss: 1.9052e-07 - val_loss: 1.7045e-05
Epoch 246/512
512/512 - 0s - loss: 1.8699e-07 - val_loss: 1.6615e-05
Epoch 247/512
512/512 - 0s - loss: 1.8297e-07 - val_loss: 1.6096e-05
Epoch 248/512
512/512 - 0s - loss: 1.7836e-07 - val_loss: 1.5462e-05
Epoch 249/512
512/512 - 0s - loss: 1.7306e-07 - val_loss: 1.4675e-05
Epoch 250/512
512/512 - 0s - loss: 1.6692e-07 - val_loss: 1.3696e-05
Epoch 251/512
512/512 - 0s - loss: 1.5970e-07 - val_loss: 1.7279e-05
Epoch 252/512
512/512 - 0s - loss: 1.3903e-06 - val_loss: 1.9366e-05
Epoch 253/512
512/512 - 0s - loss: 2.1555e-07 - val_loss: 1.9160e-05
Epoch 254/512
512/512 - 0s - loss: 2.1196e-07 - val_loss: 1.8946e-05
Epoch 255/512
512/512 - 0s - loss: 2.0840e-07 - val_loss: 1.8721e-05
Epoch 256/512
512/512 - 0s - loss: 2.0489e-07 - val_loss: 1.8488e-05
Epoch 257/512
512/512 - 0s - loss: 2.0150e-07 - val_loss: 1.8247e-05
Epoch 258/512
512/512 - 0s - loss: 1.9832e-07 - val_loss: 1.8002e-05
Epoch 259/512
512/512 - 0s - loss: 1.9543e-07 - val_loss: 1.7759e-05
Epoch 260/512
512/512 - 0s - loss: 1.9289e-07 - val_loss: 1.7522e-05
Epoch 261/512
512/512 - 0s - loss: 1.9072e-07 - val_loss: 1.7296e-05
Epoch 262/512
512/512 - 0s - loss: 1.8893e-07 - val_loss: 1.7091e-05
Epoch 263/512
512/512 - 0s - loss: 1.8743e-07 - val_loss: 1.6908e-05
Epoch 264/512
512/512 - 0s - loss: 1.8614e-07 - val_loss: 1.6747e-05
Epoch 265/512
512/512 - 0s - loss: 1.8495e-07 - val_loss: 1.6608e-05
Epoch 266/512
512/512 - 0s - loss: 1.8381e-07 - val_loss: 1.6481e-05
Epoch 267/512
512/512 - 0s - loss: 1.8265e-07 - val_loss: 1.6364e-05
Epoch 268/512
512/512 - 0s - loss: 1.8152e-07 - val_loss: 1.6247e-05
Epoch 269/512
512/512 - 0s - loss: 1.8031e-07 - val_loss: 1.6139e-05
Epoch 270/512
512/512 - 0s - loss: 1.7902e-07 - val_loss: 1.5992e-05
Epoch 271/512
512/512 - 0s - loss: 1.7766e-07 - val_loss: 1.5844e-05
Epoch 272/512
512/512 - 0s - loss: 1.7621e-07 - val_loss: 1.5691e-05
Epoch 273/512
512/512 - 0s - loss: 1.7466e-07 - val_loss: 1.5512e-05
Epoch 274/512
512/512 - 0s - loss: 1.7298e-07 - val_loss: 1.5307e-05
Epoch 275/512
512/512 - 0s - loss: 1.7105e-07 - val_loss: 1.5082e-05
Epoch 276/512
512/512 - 0s - loss: 1.6899e-07 - val_loss: 1.4856e-05
Epoch 277/512
512/512 - 0s - loss: 1.6665e-07 - val_loss: 1.4547e-05
Epoch 278/512
512/512 - 0s - loss: 1.6396e-07 - val_loss: 1.4197e-05
Epoch 279/512
512/512 - 0s - loss: 1.6102e-07 - val_loss: 1.3857e-05
Epoch 280/512
512/512 - 0s - loss: 1.5805e-07 - val_loss: 1.3443e-05
Epoch 281/512
512/512 - 0s - loss: 1.5517e-07 - val_loss: 1.3124e-05
Epoch 282/512
512/512 - 0s - loss: 1.5247e-07 - val_loss: 1.2780e-05
Epoch 283/512
512/512 - 0s - loss: 1.5028e-07 - val_loss: 1.2587e-05
Epoch 284/512
512/512 - 0s - loss: 1.4824e-07 - val_loss: 1.2263e-05
Epoch 285/512
512/512 - 0s - loss: 1.4623e-07 - val_loss: 1.1959e-05
Epoch 286/512
512/512 - 0s - loss: 1.4390e-07 - val_loss: 1.1737e-05
Epoch 287/512
512/512 - 0s - loss: 1.4172e-07 - val_loss: 1.1150e-05
Epoch 288/512
512/512 - 0s - loss: 1.3887e-07 - val_loss: 1.0879e-05
Epoch 289/512
512/512 - 0s - loss: 1.3636e-07 - val_loss: 1.0773e-05
Epoch 290/512
512/512 - 0s - loss: 1.3217e-07 - val_loss: 9.5581e-06
Epoch 291/512
512/512 - 0s - loss: 1.2381e-07 - val_loss: 8.2504e-06
Epoch 292/512
512/512 - 0s - loss: 1.1517e-07 - val_loss: 7.1952e-06
Epoch 293/512
512/512 - 0s - loss: 1.1084e-07 - val_loss: 6.5679e-06
Epoch 294/512
512/512 - 0s - loss: 1.1178e-07 - val_loss: 7.0911e-06
Epoch 295/512
512/512 - 0s - loss: 1.1567e-07 - val_loss: 85.0759
Epoch 296/512
512/512 - 0s - loss: 21.2690 - val_loss: 1.6132e-05
Epoch 297/512
512/512 - 0s - loss: 1.8633e-07 - val_loss: 1.6059e-05
Epoch 298/512
512/512 - 0s - loss: 1.8324e-07 - val_loss: 1.5989e-05
Epoch 299/512
512/512 - 0s - loss: 1.8090e-07 - val_loss: 1.5920e-05
Epoch 300/512
512/512 - 0s - loss: 1.7901e-07 - val_loss: 1.5853e-05
Epoch 301/512
512/512 - 0s - loss: 1.7740e-07 - val_loss: 1.5787e-05
Epoch 302/512
512/512 - 0s - loss: 1.7598e-07 - val_loss: 1.5722e-05
Epoch 303/512
512/512 - 0s - loss: 1.7472e-07 - val_loss: 1.5657e-05
Epoch 304/512
512/512 - 0s - loss: 1.7356e-07 - val_loss: 1.5594e-05
Epoch 305/512
512/512 - 0s - loss: 1.7248e-07 - val_loss: 1.5531e-05
Epoch 306/512
512/512 - 0s - loss: 1.7147e-07 - val_loss: 1.5469e-05
Epoch 307/512
512/512 - 0s - loss: 1.7052e-07 - val_loss: 1.5407e-05
Epoch 308/512
512/512 - 0s - loss: 1.6961e-07 - val_loss: 1.5346e-05
Epoch 309/512
512/512 - 0s - loss: 1.6873e-07 - val_loss: 1.5286e-05
Epoch 310/512
512/512 - 0s - loss: 1.6789e-07 - val_loss: 1.5226e-05
Epoch 311/512
512/512 - 0s - loss: 1.6707e-07 - val_loss: 1.5167e-05
Epoch 312/512
512/512 - 0s - loss: 1.6628e-07 - val_loss: 1.5109e-05
Epoch 313/512
512/512 - 0s - loss: 1.6551e-07 - val_loss: 1.5051e-05
Epoch 314/512
512/512 - 0s - loss: 1.6476e-07 - val_loss: 1.4994e-05
Epoch 315/512
512/512 - 0s - loss: 1.6402e-07 - val_loss: 1.4937e-05
Epoch 316/512
512/512 - 0s - loss: 1.6330e-07 - val_loss: 1.4881e-05
Epoch 317/512
512/512 - 0s - loss: 1.6260e-07 - val_loss: 1.4825e-05
Epoch 318/512
512/512 - 0s - loss: 1.6190e-07 - val_loss: 1.4770e-05
Epoch 319/512
512/512 - 0s - loss: 1.6122e-07 - val_loss: 1.4715e-05
Epoch 320/512
512/512 - 0s - loss: 1.6055e-07 - val_loss: 1.4661e-05
Epoch 321/512
512/512 - 0s - loss: 1.5988e-07 - val_loss: 1.4608e-05
Epoch 322/512
512/512 - 0s - loss: 1.5923e-07 - val_loss: 1.4554e-05
Epoch 323/512
512/512 - 0s - loss: 1.5859e-07 - val_loss: 1.4502e-05
Epoch 324/512
512/512 - 0s - loss: 1.5795e-07 - val_loss: 1.4449e-05
Epoch 325/512
512/512 - 0s - loss: 1.5733e-07 - val_loss: 1.4398e-05
Epoch 326/512
512/512 - 0s - loss: 1.5671e-07 - val_loss: 1.4346e-05
Epoch 327/512
512/512 - 0s - loss: 1.5609e-07 - val_loss: 1.4295e-05
Epoch 328/512
512/512 - 0s - loss: 1.5549e-07 - val_loss: 1.4245e-05
Epoch 329/512
512/512 - 0s - loss: 1.5489e-07 - val_loss: 1.4195e-05
Epoch 330/512
512/512 - 0s - loss: 1.5430e-07 - val_loss: 1.4145e-05
Epoch 331/512
512/512 - 0s - loss: 1.5371e-07 - val_loss: 1.4096e-05
Epoch 332/512
512/512 - 0s - loss: 1.5313e-07 - val_loss: 1.4047e-05
Epoch 333/512
512/512 - 0s - loss: 1.5256e-07 - val_loss: 1.3999e-05
Epoch 334/512
512/512 - 0s - loss: 1.5199e-07 - val_loss: 1.3951e-05
Epoch 335/512
512/512 - 0s - loss: 1.5143e-07 - val_loss: 1.3904e-05
Epoch 336/512
512/512 - 0s - loss: 1.5088e-07 - val_loss: 1.3857e-05
Epoch 337/512
512/512 - 0s - loss: 1.5033e-07 - val_loss: 1.3810e-05
Epoch 338/512
512/512 - 0s - loss: 1.4978e-07 - val_loss: 1.3763e-05
Epoch 339/512
512/512 - 0s - loss: 1.4924e-07 - val_loss: 1.3717e-05
Epoch 340/512
512/512 - 0s - loss: 1.4871e-07 - val_loss: 1.3672e-05
Epoch 341/512
512/512 - 0s - loss: 1.4818e-07 - val_loss: 1.3626e-05
Epoch 342/512
512/512 - 0s - loss: 1.4765e-07 - val_loss: 1.3581e-05
Epoch 343/512
512/512 - 0s - loss: 1.4713e-07 - val_loss: 1.3537e-05
Epoch 344/512
512/512 - 0s - loss: 1.4661e-07 - val_loss: 1.3493e-05
Epoch 345/512
512/512 - 0s - loss: 1.4610e-07 - val_loss: 1.3449e-05
Epoch 346/512
512/512 - 0s - loss: 1.4559e-07 - val_loss: 1.3405e-05
Epoch 347/512
512/512 - 0s - loss: 1.4509e-07 - val_loss: 1.3362e-05
Epoch 348/512
512/512 - 0s - loss: 1.4459e-07 - val_loss: 1.3319e-05
Epoch 349/512
512/512 - 0s - loss: 1.4409e-07 - val_loss: 1.3276e-05
Epoch 350/512
512/512 - 0s - loss: 1.4360e-07 - val_loss: 1.3234e-05
Epoch 351/512
512/512 - 0s - loss: 1.4312e-07 - val_loss: 1.3191e-05
Epoch 352/512
512/512 - 0s - loss: 1.4263e-07 - val_loss: 1.3150e-05
Epoch 353/512
512/512 - 0s - loss: 1.4215e-07 - val_loss: 1.3108e-05
Epoch 354/512
512/512 - 0s - loss: 1.4167e-07 - val_loss: 1.3067e-05
Epoch 355/512
512/512 - 0s - loss: 1.4119e-07 - val_loss: 1.3025e-05
Epoch 356/512
512/512 - 0s - loss: 1.4072e-07 - val_loss: 1.2984e-05
Epoch 357/512
512/512 - 0s - loss: 1.4025e-07 - val_loss: 1.2943e-05
Epoch 358/512
512/512 - 0s - loss: 1.3977e-07 - val_loss: 1.2902e-05
Epoch 359/512
512/512 - 0s - loss: 1.3930e-07 - val_loss: 1.2861e-05
Epoch 360/512
512/512 - 0s - loss: 1.3883e-07 - val_loss: 1.2820e-05
Epoch 361/512
512/512 - 0s - loss: 1.3835e-07 - val_loss: 1.2779e-05
Epoch 362/512
512/512 - 0s - loss: 1.3788e-07 - val_loss: 1.2737e-05
Epoch 363/512
512/512 - 0s - loss: 1.3740e-07 - val_loss: 1.2695e-05
Epoch 364/512
512/512 - 0s - loss: 1.3691e-07 - val_loss: 1.2652e-05
Epoch 365/512
512/512 - 0s - loss: 1.3641e-07 - val_loss: 1.2609e-05
Epoch 366/512
512/512 - 0s - loss: 1.3590e-07 - val_loss: 1.2565e-05
Epoch 367/512
512/512 - 0s - loss: 1.3538e-07 - val_loss: 1.2519e-05
Epoch 368/512
512/512 - 0s - loss: 1.3484e-07 - val_loss: 1.2471e-05
Epoch 369/512
512/512 - 0s - loss: 1.3428e-07 - val_loss: 1.2422e-05
Epoch 370/512
512/512 - 0s - loss: 1.3369e-07 - val_loss: 1.2370e-05
Epoch 371/512
512/512 - 0s - loss: 1.3306e-07 - val_loss: 1.2314e-05
Epoch 372/512
512/512 - 0s - loss: 1.3239e-07 - val_loss: 1.2255e-05
Epoch 373/512
512/512 - 0s - loss: 1.3167e-07 - val_loss: 1.2190e-05
Epoch 374/512
512/512 - 0s - loss: 1.3089e-07 - val_loss: 1.2120e-05
Epoch 375/512
512/512 - 0s - loss: 1.3003e-07 - val_loss: 1.2042e-05
Epoch 376/512
512/512 - 0s - loss: 1.2908e-07 - val_loss: 1.1956e-05
Epoch 377/512
512/512 - 0s - loss: 1.2802e-07 - val_loss: 1.1859e-05
Epoch 378/512
512/512 - 0s - loss: 1.2685e-07 - val_loss: 1.1749e-05
Epoch 379/512
512/512 - 0s - loss: 1.2554e-07 - val_loss: 1.1624e-05
Epoch 380/512
512/512 - 0s - loss: 1.2413e-07 - val_loss: 1.1504e-05
Epoch 381/512
512/512 - 0s - loss: 1.2254e-07 - val_loss: 1.1321e-05
Epoch 382/512
512/512 - 0s - loss: 1.2086e-07 - val_loss: 1.1155e-05
Epoch 383/512
512/512 - 0s - loss: 1.1924e-07 - val_loss: 1.1126e-05
Epoch 384/512
512/512 - 0s - loss: 1.1846e-07 - val_loss: 1.0902e-05
Epoch 385/512
512/512 - 0s - loss: 1.1609e-07 - val_loss: 1.0617e-05
Epoch 386/512
512/512 - 0s - loss: 1.1320e-07 - val_loss: 1.0247e-05
Epoch 387/512
512/512 - 0s - loss: 1.1239e-07 - val_loss: 1.0534e-05
Epoch 388/512
512/512 - 0s - loss: 1.1435e-07 - val_loss: 1.0361e-05
Epoch 389/512
512/512 - 0s - loss: 1.1318e-07 - val_loss: 1.0181e-05
Epoch 390/512
512/512 - 0s - loss: 1.1206e-07 - val_loss: 9.9935e-06
Epoch 391/512
512/512 - 0s - loss: 1.1097e-07 - val_loss: 9.8052e-06
Epoch 392/512
512/512 - 0s - loss: 1.0985e-07 - val_loss: 9.5981e-06
Epoch 393/512
512/512 - 0s - loss: 1.0872e-07 - val_loss: 9.3987e-06
Epoch 394/512
512/512 - 0s - loss: 1.0740e-07 - val_loss: 9.1715e-06
Epoch 395/512
512/512 - 0s - loss: 1.0584e-07 - val_loss: 8.7927e-06
Epoch 396/512
512/512 - 0s - loss: 1.0377e-07 - val_loss: 8.4559e-06
Epoch 397/512
512/512 - 0s - loss: 1.0051e-07 - val_loss: 7.9603e-06
Epoch 398/512
512/512 - 0s - loss: 9.6129e-08 - val_loss: 7.1541e-06
Epoch 399/512
512/512 - 0s - loss: 0.1646 - val_loss: 1.1540e-05
Epoch 400/512
512/512 - 0s - loss: 1.2989e-07 - val_loss: 1.1498e-05
Epoch 401/512
512/512 - 0s - loss: 1.2386e-07 - val_loss: 1.1467e-05
Epoch 402/512
512/512 - 0s - loss: 1.2349e-07 - val_loss: 1.1437e-05
Epoch 403/512
512/512 - 0s - loss: 1.2313e-07 - val_loss: 1.1408e-05
Epoch 404/512
512/512 - 0s - loss: 1.2277e-07 - val_loss: 1.1378e-05
Epoch 405/512
512/512 - 0s - loss: 1.2241e-07 - val_loss: 1.1349e-05
Epoch 406/512
512/512 - 0s - loss: 1.2206e-07 - val_loss: 1.1319e-05
Epoch 407/512
512/512 - 0s - loss: 1.2171e-07 - val_loss: 1.1291e-05
Epoch 408/512
512/512 - 0s - loss: 1.2136e-07 - val_loss: 1.1262e-05
Epoch 409/512
512/512 - 0s - loss: 1.2102e-07 - val_loss: 1.1233e-05
Epoch 410/512
512/512 - 0s - loss: 1.2069e-07 - val_loss: 1.1205e-05
Epoch 411/512
512/512 - 0s - loss: 1.2035e-07 - val_loss: 1.1176e-05
Epoch 412/512
512/512 - 0s - loss: 1.2002e-07 - val_loss: 1.1148e-05
Epoch 413/512
512/512 - 0s - loss: 1.1969e-07 - val_loss: 1.1120e-05
Epoch 414/512
512/512 - 0s - loss: 1.1937e-07 - val_loss: 1.1093e-05
Epoch 415/512
512/512 - 0s - loss: 1.1905e-07 - val_loss: 1.1065e-05
Epoch 416/512
512/512 - 0s - loss: 1.1873e-07 - val_loss: 1.1038e-05
Epoch 417/512
512/512 - 0s - loss: 1.1841e-07 - val_loss: 1.1010e-05
Epoch 418/512
512/512 - 0s - loss: 1.1810e-07 - val_loss: 1.0983e-05
Epoch 419/512
512/512 - 0s - loss: 1.1779e-07 - val_loss: 1.0956e-05
Epoch 420/512
512/512 - 0s - loss: 1.1748e-07 - val_loss: 1.0930e-05
Epoch 421/512
512/512 - 0s - loss: 1.1717e-07 - val_loss: 1.0903e-05
Epoch 422/512
512/512 - 0s - loss: 1.1687e-07 - val_loss: 1.0877e-05
Epoch 423/512
512/512 - 0s - loss: 1.1657e-07 - val_loss: 1.0850e-05
Epoch 424/512
512/512 - 0s - loss: 1.1627e-07 - val_loss: 1.0824e-05
Epoch 425/512
512/512 - 0s - loss: 1.1597e-07 - val_loss: 1.0798e-05
Epoch 426/512
512/512 - 0s - loss: 1.1567e-07 - val_loss: 1.0773e-05
Epoch 427/512
512/512 - 0s - loss: 1.1538e-07 - val_loss: 1.0747e-05
Epoch 428/512
512/512 - 0s - loss: 1.1509e-07 - val_loss: 1.0721e-05
Epoch 429/512
512/512 - 0s - loss: 1.1480e-07 - val_loss: 1.0696e-05
Epoch 430/512
512/512 - 0s - loss: 1.1452e-07 - val_loss: 1.0671e-05
Epoch 431/512
512/512 - 0s - loss: 1.1423e-07 - val_loss: 1.0646e-05
Epoch 432/512
512/512 - 0s - loss: 1.1395e-07 - val_loss: 1.0621e-05
Epoch 433/512
512/512 - 0s - loss: 1.1367e-07 - val_loss: 1.0596e-05
Epoch 434/512
512/512 - 0s - loss: 1.1339e-07 - val_loss: 1.0571e-05
Epoch 435/512
512/512 - 0s - loss: 1.1311e-07 - val_loss: 1.0547e-05
Epoch 436/512
512/512 - 0s - loss: 1.1283e-07 - val_loss: 1.0523e-05
Epoch 437/512
512/512 - 0s - loss: 1.1256e-07 - val_loss: 1.0498e-05
Epoch 438/512
512/512 - 0s - loss: 1.1228e-07 - val_loss: 1.0474e-05
Epoch 439/512
512/512 - 0s - loss: 1.1201e-07 - val_loss: 1.0450e-05
Epoch 440/512
512/512 - 0s - loss: 1.1174e-07 - val_loss: 1.0426e-05
Epoch 441/512
512/512 - 0s - loss: 1.1147e-07 - val_loss: 1.0402e-05
Epoch 442/512
512/512 - 0s - loss: 1.1120e-07 - val_loss: 1.0378e-05
Epoch 443/512
512/512 - 0s - loss: 1.1093e-07 - val_loss: 1.0354e-05
Epoch 444/512
512/512 - 0s - loss: 1.1066e-07 - val_loss: 1.0330e-05
Epoch 445/512
512/512 - 0s - loss: 1.1039e-07 - val_loss: 1.0306e-05
Epoch 446/512
512/512 - 0s - loss: 1.1012e-07 - val_loss: 1.0282e-05
Epoch 447/512
512/512 - 0s - loss: 1.0985e-07 - val_loss: 1.0258e-05
Epoch 448/512
512/512 - 0s - loss: 1.0957e-07 - val_loss: 1.0233e-05
Epoch 449/512
512/512 - 0s - loss: 1.0930e-07 - val_loss: 1.0209e-05
Epoch 450/512
512/512 - 0s - loss: 1.0902e-07 - val_loss: 1.0184e-05
Epoch 451/512
512/512 - 0s - loss: 1.0873e-07 - val_loss: 1.0159e-05
Epoch 452/512
512/512 - 0s - loss: 1.0844e-07 - val_loss: 1.0133e-05
Epoch 453/512
512/512 - 0s - loss: 1.0815e-07 - val_loss: 1.0106e-05
Epoch 454/512
512/512 - 0s - loss: 1.0784e-07 - val_loss: 1.0079e-05
Epoch 455/512
512/512 - 0s - loss: 1.0752e-07 - val_loss: 1.0050e-05
Epoch 456/512
512/512 - 0s - loss: 1.0719e-07 - val_loss: 1.0020e-05
Epoch 457/512
512/512 - 0s - loss: 1.0683e-07 - val_loss: 9.9886e-06
Epoch 458/512
512/512 - 0s - loss: 1.0646e-07 - val_loss: 9.9550e-06
Epoch 459/512
512/512 - 0s - loss: 1.0606e-07 - val_loss: 9.9189e-06
Epoch 460/512
512/512 - 0s - loss: 1.0563e-07 - val_loss: 9.8798e-06
Epoch 461/512
512/512 - 0s - loss: 1.0516e-07 - val_loss: 9.8369e-06
Epoch 462/512
512/512 - 0s - loss: 1.0464e-07 - val_loss: 9.7896e-06
Epoch 463/512
512/512 - 0s - loss: 1.0407e-07 - val_loss: 9.7372e-06
Epoch 464/512
512/512 - 0s - loss: 1.0344e-07 - val_loss: 9.6783e-06
Epoch 465/512
512/512 - 0s - loss: 1.0273e-07 - val_loss: 9.6122e-06
Epoch 466/512
512/512 - 0s - loss: 1.0195e-07 - val_loss: 9.5369e-06
Epoch 467/512
512/512 - 0s - loss: 1.0108e-07 - val_loss: 9.4512e-06
Epoch 468/512
512/512 - 0s - loss: 1.0010e-07 - val_loss: 9.3528e-06
Epoch 469/512
512/512 - 0s - loss: 9.9023e-08 - val_loss: 9.2398e-06
Epoch 470/512
512/512 - 0s - loss: 9.7833e-08 - val_loss: 9.1091e-06
Epoch 471/512
512/512 - 0s - loss: 9.6503e-08 - val_loss: 8.9540e-06
Epoch 472/512
512/512 - 0s - loss: 9.5072e-08 - val_loss: 8.7769e-06
Epoch 473/512
512/512 - 0s - loss: 9.3528e-08 - val_loss: 8.5640e-06
Epoch 474/512
512/512 - 0s - loss: 9.1873e-08 - val_loss: 8.3189e-06
Epoch 475/512
512/512 - 0s - loss: 9.0142e-08 - val_loss: 8.0589e-06
Epoch 476/512
512/512 - 0s - loss: 8.8523e-08 - val_loss: 7.7511e-06
Epoch 477/512
512/512 - 0s - loss: 8.6907e-08 - val_loss: 7.4401e-06
Epoch 478/512
512/512 - 0s - loss: 8.5692e-08 - val_loss: 7.2192e-06
Epoch 479/512
512/512 - 0s - loss: 8.4302e-08 - val_loss: 6.8552e-06
Epoch 480/512
512/512 - 0s - loss: 1.1792e-07 - val_loss: 8.6516e-06
Epoch 481/512
512/512 - 0s - loss: 9.6166e-08 - val_loss: 8.6848e-06
Epoch 482/512
512/512 - 0s - loss: 9.5688e-08 - val_loss: 8.6739e-06
Epoch 483/512
512/512 - 0s - loss: 9.5225e-08 - val_loss: 8.6485e-06
Epoch 484/512
512/512 - 0s - loss: 9.4836e-08 - val_loss: 8.6090e-06
Epoch 485/512
512/512 - 0s - loss: 9.4389e-08 - val_loss: 8.5580e-06
Epoch 486/512
512/512 - 0s - loss: 9.3908e-08 - val_loss: 8.5017e-06
Epoch 487/512
512/512 - 0s - loss: 9.3386e-08 - val_loss: 8.4383e-06
Epoch 488/512
512/512 - 0s - loss: 9.2813e-08 - val_loss: 8.3659e-06
Epoch 489/512
512/512 - 0s - loss: 9.2171e-08 - val_loss: 8.2838e-06
Epoch 490/512
512/512 - 0s - loss: 9.1440e-08 - val_loss: 8.1839e-06
Epoch 491/512
512/512 - 0s - loss: 9.0610e-08 - val_loss: 8.0797e-06
Epoch 492/512
512/512 - 0s - loss: 8.9685e-08 - val_loss: 7.9513e-06
Epoch 493/512
512/512 - 0s - loss: 8.8689e-08 - val_loss: 7.8118e-06
Epoch 494/512
512/512 - 0s - loss: 8.7583e-08 - val_loss: 7.6465e-06
Epoch 495/512
512/512 - 0s - loss: 8.6349e-08 - val_loss: 7.4706e-06
Epoch 496/512
512/512 - 0s - loss: 8.4948e-08 - val_loss: 7.2713e-06
Epoch 497/512
512/512 - 0s - loss: 8.3414e-08 - val_loss: 7.0465e-06
Epoch 498/512
512/512 - 0s - loss: 8.1589e-08 - val_loss: 6.7118e-06
Epoch 499/512
512/512 - 0s - loss: 7.9267e-08 - val_loss: 6.2622e-06
Epoch 500/512
512/512 - 0s - loss: 7.6002e-08 - val_loss: 5.6742e-06
Epoch 501/512
512/512 - 0s - loss: 7.2854e-08 - val_loss: 5.7205e-06
Epoch 502/512
512/512 - 0s - loss: 0.2976 - val_loss: 9.2228e-06
Epoch 503/512
512/512 - 0s - loss: 9.8960e-08 - val_loss: 9.2048e-06
Epoch 504/512
512/512 - 0s - loss: 9.8713e-08 - val_loss: 9.1869e-06
Epoch 505/512
512/512 - 0s - loss: 9.8471e-08 - val_loss: 9.1691e-06
Epoch 506/512
512/512 - 0s - loss: 9.8234e-08 - val_loss: 9.1514e-06
Epoch 507/512
512/512 - 0s - loss: 9.8000e-08 - val_loss: 9.1338e-06
Epoch 508/512
512/512 - 0s - loss: 9.7771e-08 - val_loss: 9.1163e-06
Epoch 509/512
512/512 - 0s - loss: 9.7544e-08 - val_loss: 9.0988e-06
Epoch 510/512
512/512 - 0s - loss: 9.7321e-08 - val_loss: 9.0814e-06
Epoch 511/512
512/512 - 0s - loss: 9.7100e-08 - val_loss: 9.0640e-06
Epoch 512/512
512/512 - 0s - loss: 9.6884e-08 - val_loss: 9.0468e-06
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5471e-10 - val_loss: 3.5224e-10
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5165e-10 - val_loss: 3.4922e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4859e-10 - val_loss: 3.4614e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4535e-10 - val_loss: 3.4369e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4285e-10 - val_loss: 3.4080e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4000e-10 - val_loss: 3.3797e-10
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3710e-10 - val_loss: 3.3510e-10
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3464e-10 - val_loss: 3.3282e-10
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3232e-10 - val_loss: 3.3056e-10
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2992e-10 - val_loss: 3.2792e-10
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2775e-10 - val_loss: 3.2563e-10
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2529e-10 - val_loss: 3.2390e-10
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2315e-10 - val_loss: 3.2098e-10
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2095e-10 - val_loss: 3.1909e-10
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1898e-10 - val_loss: 3.1745e-10
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1704e-10 - val_loss: 3.1554e-10
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1505e-10 - val_loss: 3.1413e-10
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1332e-10 - val_loss: 3.1216e-10
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1170e-10 - val_loss: 3.1059e-10
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1002e-10 - val_loss: 3.0873e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0831e-10 - val_loss: 3.0684e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0681e-10 - val_loss: 3.0526e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0506e-10 - val_loss: 3.0395e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0360e-10 - val_loss: 3.0249e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0194e-10 - val_loss: 3.0087e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0063e-10 - val_loss: 2.9980e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9935e-10 - val_loss: 2.9823e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9800e-10 - val_loss: 2.9701e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9672e-10 - val_loss: 2.9547e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9545e-10 - val_loss: 2.9426e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9414e-10 - val_loss: 2.9290e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9299e-10 - val_loss: 2.9158e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9167e-10 - val_loss: 2.9052e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9042e-10 - val_loss: 2.8939e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8942e-10 - val_loss: 2.8844e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8829e-10 - val_loss: 2.8724e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8721e-10 - val_loss: 2.8630e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8617e-10 - val_loss: 2.8523e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8512e-10 - val_loss: 2.8407e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8413e-10 - val_loss: 2.8329e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8319e-10 - val_loss: 2.8244e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8228e-10 - val_loss: 2.8121e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8119e-10 - val_loss: 2.8036e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8020e-10 - val_loss: 2.7946e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7930e-10 - val_loss: 2.7808e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7817e-10 - val_loss: 2.7730e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7739e-10 - val_loss: 2.7635e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7631e-10 - val_loss: 2.7522e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7532e-10 - val_loss: 2.7422e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7434e-10 - val_loss: 2.7330e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7316e-10 - val_loss: 2.7205e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7207e-10 - val_loss: 2.7096e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7088e-10 - val_loss: 2.6969e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6945e-10 - val_loss: 2.6825e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6808e-10 - val_loss: 2.6681e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6652e-10 - val_loss: 2.6482e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6451e-10 - val_loss: 2.6301e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6266e-10 - val_loss: 2.6081e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6032e-10 - val_loss: 2.5870e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5781e-10 - val_loss: 2.5558e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5496e-10 - val_loss: 2.5233e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5142e-10 - val_loss: 2.4876e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4765e-10 - val_loss: 2.4467e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4335e-10 - val_loss: 2.4002e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3833e-10 - val_loss: 2.3459e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3282e-10 - val_loss: 2.2887e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2691e-10 - val_loss: 2.2246e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2014e-10 - val_loss: 2.1526e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1283e-10 - val_loss: 2.0797e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0526e-10 - val_loss: 1.9990e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9710e-10 - val_loss: 1.9174e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8862e-10 - val_loss: 1.8324e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8025e-10 - val_loss: 1.7474e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7165e-10 - val_loss: 1.6631e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6312e-10 - val_loss: 1.5798e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5490e-10 - val_loss: 1.4972e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4676e-10 - val_loss: 1.4183e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3897e-10 - val_loss: 1.3445e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3157e-10 - val_loss: 1.2730e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2459e-10 - val_loss: 1.2061e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1802e-10 - val_loss: 1.1448e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1181e-10 - val_loss: 1.0852e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0617e-10 - val_loss: 1.0328e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0093e-10 - val_loss: 9.8356e-11
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.6277e-11 - val_loss: 9.3361e-11
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.1362e-11 - val_loss: 8.9416e-11
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.7473e-11 - val_loss: 8.5418e-11
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.3581e-11 - val_loss: 8.2028e-11
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.0208e-11 - val_loss: 7.8982e-11
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.7256e-11 - val_loss: 7.6258e-11
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.4563e-11 - val_loss: 7.3859e-11
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.2141e-11 - val_loss: 7.1440e-11
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.9841e-11 - val_loss: 6.9141e-11
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.7300e-11 - val_loss: 6.6786e-11
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.5541e-11 - val_loss: 6.5349e-11
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3755e-11 - val_loss: 6.3540e-11
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2055e-11 - val_loss: 6.2304e-11
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0828e-11 - val_loss: 6.1099e-11
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.9577e-11 - val_loss: 5.9840e-11
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8391e-11 - val_loss: 5.8891e-11
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7546e-11 - val_loss: 5.8182e-11
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6850e-11 - val_loss: 5.7627e-11
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6222e-11 - val_loss: 5.6733e-11
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5422e-11 - val_loss: 5.6439e-11
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.4971e-11 - val_loss: 5.6142e-11
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.4744e-11 - val_loss: 5.5576e-11
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.4023e-11 - val_loss: 5.4517e-11
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.2986e-11 - val_loss: 5.4038e-11
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.2360e-11 - val_loss: 5.3301e-11
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1821e-11 - val_loss: 5.3078e-11
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1594e-11 - val_loss: 5.2763e-11
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1228e-11 - val_loss: 5.2485e-11
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.0819e-11 - val_loss: 5.2101e-11
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.0556e-11 - val_loss: 5.1837e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0275e-11 - val_loss: 5.2276e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0720e-11 - val_loss: 5.2131e-11
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.0492e-11 - val_loss: 5.1438e-11
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.9842e-11 - val_loss: 5.1291e-11
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.9730e-11 - val_loss: 5.1111e-11
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.9332e-11 - val_loss: 5.0459e-11
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8799e-11 - val_loss: 5.0197e-11
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8591e-11 - val_loss: 5.0045e-11
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8464e-11 - val_loss: 4.9922e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8489e-11 - val_loss: 5.0195e-11
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8409e-11 - val_loss: 4.9797e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8163e-11 - val_loss: 5.0582e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8924e-11 - val_loss: 5.0423e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8761e-11 - val_loss: 5.0373e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8646e-11 - val_loss: 5.0236e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8448e-11 - val_loss: 5.0008e-11
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8197e-11 - val_loss: 4.9662e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7944e-11 - val_loss: 5.0379e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8640e-11 - val_loss: 5.0241e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8554e-11 - val_loss: 5.0077e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8301e-11 - val_loss: 4.9732e-11
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7947e-11 - val_loss: 4.9405e-11
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7639e-11 - val_loss: 4.8918e-11
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7226e-11 - val_loss: 4.8829e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7118e-11 - val_loss: 4.8791e-11
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6691e-11 - val_loss: 4.8087e-11
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6407e-11 - val_loss: 4.7929e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6308e-11 - val_loss: 4.7958e-11
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6389e-11 - val_loss: 4.7855e-11
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6053e-11 - val_loss: 4.7524e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5998e-11 - val_loss: 4.7643e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6013e-11 - val_loss: 4.7620e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5954e-11 - val_loss: 4.7571e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6031e-11 - val_loss: 4.7741e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6319e-11 - val_loss: 4.8288e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6657e-11 - val_loss: 4.8435e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6608e-11 - val_loss: 4.8270e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6660e-11 - val_loss: 4.8273e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6446e-11 - val_loss: 4.8012e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6395e-11 - val_loss: 4.8255e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6490e-11 - val_loss: 4.8163e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6332e-11 - val_loss: 4.7980e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6263e-11 - val_loss: 4.7998e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6148e-11 - val_loss: 4.7714e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5964e-11 - val_loss: 4.7672e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5956e-11 - val_loss: 4.7346e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5593e-11 - val_loss: 4.7171e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5399e-11 - val_loss: 4.7017e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5322e-11 - val_loss: 4.7021e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5312e-11 - val_loss: 4.6935e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5217e-11 - val_loss: 4.6858e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5564e-11 - val_loss: 4.7688e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5871e-11 - val_loss: 4.7476e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5717e-11 - val_loss: 4.7491e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5475e-11 - val_loss: 4.6914e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5260e-11 - val_loss: 4.7442e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5537e-11 - val_loss: 4.7307e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5515e-11 - val_loss: 4.6970e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5254e-11 - val_loss: 4.6907e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5175e-11 - val_loss: 4.6792e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4955e-11 - val_loss: 4.6565e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4814e-11 - val_loss: 4.6461e-11
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4710e-11 - val_loss: 4.6407e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4765e-11 - val_loss: 4.6476e-11
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4499e-11 - val_loss: 4.6087e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4360e-11 - val_loss: 4.6005e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4356e-11 - val_loss: 4.6168e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4368e-11 - val_loss: 4.5822e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4130e-11 - val_loss: 4.5809e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4103e-11 - val_loss: 4.5754e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3986e-11 - val_loss: 4.5577e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3917e-11 - val_loss: 4.5541e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3811e-11 - val_loss: 4.5414e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3718e-11 - val_loss: 4.5376e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3745e-11 - val_loss: 4.5390e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3713e-11 - val_loss: 4.5375e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3707e-11 - val_loss: 4.5449e-11
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3725e-11 - val_loss: 4.5299e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3541e-11 - val_loss: 4.5212e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3677e-11 - val_loss: 4.5422e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3628e-11 - val_loss: 4.5048e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3315e-11 - val_loss: 4.4943e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3299e-11 - val_loss: 4.5068e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3541e-11 - val_loss: 4.5246e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3555e-11 - val_loss: 4.5219e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3487e-11 - val_loss: 4.5150e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3384e-11 - val_loss: 4.5014e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3311e-11 - val_loss: 4.4888e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3157e-11 - val_loss: 4.4823e-11
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3059e-11 - val_loss: 4.4758e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2986e-11 - val_loss: 4.4575e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2845e-11 - val_loss: 4.4490e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2811e-11 - val_loss: 4.4466e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2633e-11 - val_loss: 4.4239e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2547e-11 - val_loss: 4.4231e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2522e-11 - val_loss: 4.4019e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2344e-11 - val_loss: 4.3955e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2288e-11 - val_loss: 4.3853e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2208e-11 - val_loss: 4.3813e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2132e-11 - val_loss: 4.3751e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2135e-11 - val_loss: 4.3727e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2048e-11 - val_loss: 4.3600e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1984e-11 - val_loss: 4.3486e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1878e-11 - val_loss: 4.3473e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1837e-11 - val_loss: 4.3354e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1728e-11 - val_loss: 4.3304e-11
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1647e-11 - val_loss: 4.3189e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1556e-11 - val_loss: 4.3174e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1490e-11 - val_loss: 4.3058e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1402e-11 - val_loss: 4.2981e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1320e-11 - val_loss: 4.2939e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1339e-11 - val_loss: 4.2929e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1347e-11 - val_loss: 4.2898e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1229e-11 - val_loss: 4.2797e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1144e-11 - val_loss: 4.2645e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1019e-11 - val_loss: 4.2608e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0934e-11 - val_loss: 4.2524e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0893e-11 - val_loss: 4.2554e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0899e-11 - val_loss: 4.2455e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0744e-11 - val_loss: 4.2344e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0694e-11 - val_loss: 4.2246e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0598e-11 - val_loss: 4.2279e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0720e-11 - val_loss: 4.2337e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0602e-11 - val_loss: 4.2062e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0387e-11 - val_loss: 4.2053e-11
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0387e-11 - val_loss: 4.2012e-11
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0369e-11 - val_loss: 4.2005e-11
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0308e-11 - val_loss: 4.1906e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0196e-11 - val_loss: 4.1866e-11
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0185e-11 - val_loss: 4.1854e-11
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0204e-11 - val_loss: 4.1806e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0132e-11 - val_loss: 4.1739e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0054e-11 - val_loss: 4.1690e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9969e-11 - val_loss: 4.1628e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9926e-11 - val_loss: 4.1521e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9837e-11 - val_loss: 4.1417e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9785e-11 - val_loss: 4.1477e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9800e-11 - val_loss: 4.1383e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9711e-11 - val_loss: 4.1296e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9610e-11 - val_loss: 4.1203e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9514e-11 - val_loss: 4.1098e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9492e-11 - val_loss: 4.1052e-11
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9393e-11 - val_loss: 4.0908e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9303e-11 - val_loss: 4.0881e-11
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9231e-11 - val_loss: 4.0812e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9214e-11 - val_loss: 4.0849e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9198e-11 - val_loss: 4.0834e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9321e-11 - val_loss: 4.1079e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9391e-11 - val_loss: 4.1178e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9388e-11 - val_loss: 4.0822e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9203e-11 - val_loss: 4.0835e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9158e-11 - val_loss: 4.0808e-11
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9054e-11 - val_loss: 4.0663e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8952e-11 - val_loss: 4.0562e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8900e-11 - val_loss: 4.0605e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8957e-11 - val_loss: 4.0665e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8979e-11 - val_loss: 4.0522e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8811e-11 - val_loss: 4.0379e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8725e-11 - val_loss: 4.0325e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8683e-11 - val_loss: 4.0328e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8670e-11 - val_loss: 4.0297e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8644e-11 - val_loss: 4.0137e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8540e-11 - val_loss: 4.0213e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8551e-11 - val_loss: 4.0162e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8593e-11 - val_loss: 4.0319e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8510e-11 - val_loss: 4.0063e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8254e-11 - val_loss: 3.9792e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8201e-11 - val_loss: 3.9915e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8259e-11 - val_loss: 3.9884e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8237e-11 - val_loss: 3.9843e-11
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8119e-11 - val_loss: 3.9645e-11
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8034e-11 - val_loss: 3.9614e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8009e-11 - val_loss: 3.9616e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7990e-11 - val_loss: 3.9505e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7906e-11 - val_loss: 3.9523e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7868e-11 - val_loss: 3.9461e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7852e-11 - val_loss: 3.9401e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7718e-11 - val_loss: 3.9312e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7650e-11 - val_loss: 3.9201e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7558e-11 - val_loss: 3.9209e-11
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7553e-11 - val_loss: 3.9162e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7487e-11 - val_loss: 3.9072e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7428e-11 - val_loss: 3.9040e-11
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7407e-11 - val_loss: 3.8975e-11
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7314e-11 - val_loss: 3.8860e-11
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7287e-11 - val_loss: 3.8834e-11
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7211e-11 - val_loss: 3.8745e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7176e-11 - val_loss: 3.8738e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7133e-11 - val_loss: 3.8671e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7100e-11 - val_loss: 3.8720e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7075e-11 - val_loss: 3.8620e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7002e-11 - val_loss: 3.8586e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7010e-11 - val_loss: 3.8594e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6953e-11 - val_loss: 3.8467e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6906e-11 - val_loss: 3.8466e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6872e-11 - val_loss: 3.8392e-11
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6825e-11 - val_loss: 3.8373e-11
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6795e-11 - val_loss: 3.8300e-11
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6723e-11 - val_loss: 3.8261e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6675e-11 - val_loss: 3.8159e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6603e-11 - val_loss: 3.8162e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6590e-11 - val_loss: 3.8167e-11
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6586e-11 - val_loss: 3.8080e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6523e-11 - val_loss: 3.8032e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6432e-11 - val_loss: 3.7932e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6387e-11 - val_loss: 3.7912e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6356e-11 - val_loss: 3.7864e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6315e-11 - val_loss: 3.7821e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6218e-11 - val_loss: 3.7727e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6184e-11 - val_loss: 3.7684e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6129e-11 - val_loss: 3.7620e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6119e-11 - val_loss: 3.7651e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6102e-11 - val_loss: 3.7637e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6119e-11 - val_loss: 3.7635e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5998e-11 - val_loss: 3.7458e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5928e-11 - val_loss: 3.7347e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5797e-11 - val_loss: 3.7297e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5754e-11 - val_loss: 3.7226e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5659e-11 - val_loss: 3.7177e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5619e-11 - val_loss: 3.7151e-11
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5561e-11 - val_loss: 3.7083e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5560e-11 - val_loss: 3.7061e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5515e-11 - val_loss: 3.7007e-11
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5456e-11 - val_loss: 3.6979e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5401e-11 - val_loss: 3.6870e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5336e-11 - val_loss: 3.6844e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5271e-11 - val_loss: 3.6766e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5213e-11 - val_loss: 3.6751e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5324e-11 - val_loss: 3.7021e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5402e-11 - val_loss: 3.6903e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5281e-11 - val_loss: 3.6799e-11
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5186e-11 - val_loss: 3.6711e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5198e-11 - val_loss: 3.6736e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5131e-11 - val_loss: 3.6709e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5142e-11 - val_loss: 3.6692e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5128e-11 - val_loss: 3.6714e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5121e-11 - val_loss: 3.6689e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5089e-11 - val_loss: 3.6694e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5083e-11 - val_loss: 3.6639e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4969e-11 - val_loss: 3.6454e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4872e-11 - val_loss: 3.6427e-11
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4888e-11 - val_loss: 3.6388e-11
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4797e-11 - val_loss: 3.6277e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4742e-11 - val_loss: 3.6321e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4733e-11 - val_loss: 3.6308e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4722e-11 - val_loss: 3.6317e-11
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4672e-11 - val_loss: 3.6226e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4664e-11 - val_loss: 3.6323e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4920e-11 - val_loss: 3.6618e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5096e-11 - val_loss: 3.6674e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5232e-11 - val_loss: 3.7192e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5573e-11 - val_loss: 3.7149e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5385e-11 - val_loss: 3.7020e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5194e-11 - val_loss: 3.6676e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4926e-11 - val_loss: 3.6459e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4968e-11 - val_loss: 3.6658e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5056e-11 - val_loss: 3.6757e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5035e-11 - val_loss: 3.6604e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5006e-11 - val_loss: 3.6677e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5178e-11 - val_loss: 3.6804e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5132e-11 - val_loss: 3.6537e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4862e-11 - val_loss: 3.6426e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4872e-11 - val_loss: 3.6698e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4932e-11 - val_loss: 3.6429e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4902e-11 - val_loss: 3.6507e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4709e-11 - val_loss: 3.6164e-11
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4482e-11 - val_loss: 3.5982e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4371e-11 - val_loss: 3.6001e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4362e-11 - val_loss: 3.5888e-11
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4205e-11 - val_loss: 3.5754e-11
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4090e-11 - val_loss: 3.5679e-11
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4064e-11 - val_loss: 3.5647e-11
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4010e-11 - val_loss: 3.5586e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4036e-11 - val_loss: 3.5753e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4178e-11 - val_loss: 3.5799e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4186e-11 - val_loss: 3.5758e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4128e-11 - val_loss: 3.5676e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4054e-11 - val_loss: 3.5622e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4136e-11 - val_loss: 3.5829e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4097e-11 - val_loss: 3.5509e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3916e-11 - val_loss: 3.5704e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4106e-11 - val_loss: 3.5637e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4035e-11 - val_loss: 3.5617e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3967e-11 - val_loss: 3.5452e-11
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3858e-11 - val_loss: 3.5434e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3846e-11 - val_loss: 3.5413e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3875e-11 - val_loss: 3.5440e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3867e-11 - val_loss: 3.5416e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3823e-11 - val_loss: 3.5420e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3792e-11 - val_loss: 3.5299e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3692e-11 - val_loss: 3.5208e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3626e-11 - val_loss: 3.5234e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3637e-11 - val_loss: 3.5231e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3505e-11 - val_loss: 3.5031e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3435e-11 - val_loss: 3.4931e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3383e-11 - val_loss: 3.4952e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3318e-11 - val_loss: 3.4862e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3276e-11 - val_loss: 3.4846e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3265e-11 - val_loss: 3.4819e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3246e-11 - val_loss: 3.4835e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3297e-11 - val_loss: 3.4862e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3264e-11 - val_loss: 3.4846e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3270e-11 - val_loss: 3.4870e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3508e-11 - val_loss: 3.5231e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3617e-11 - val_loss: 3.5257e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3627e-11 - val_loss: 3.5187e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3544e-11 - val_loss: 3.5103e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3425e-11 - val_loss: 3.4935e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3380e-11 - val_loss: 3.4913e-11
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3288e-11 - val_loss: 3.4802e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3146e-11 - val_loss: 3.4701e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3080e-11 - val_loss: 3.4604e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3014e-11 - val_loss: 3.4563e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2980e-11 - val_loss: 3.4568e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2970e-11 - val_loss: 3.4541e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2800e-11 - val_loss: 3.4323e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2744e-11 - val_loss: 3.4196e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2666e-11 - val_loss: 3.4247e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2649e-11 - val_loss: 3.4069e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2451e-11 - val_loss: 3.3962e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2368e-11 - val_loss: 3.3928e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2352e-11 - val_loss: 3.3930e-11
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2337e-11 - val_loss: 3.3879e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2239e-11 - val_loss: 3.3792e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2214e-11 - val_loss: 3.3766e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2181e-11 - val_loss: 3.3717e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2177e-11 - val_loss: 3.3778e-11
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2164e-11 - val_loss: 3.3673e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2107e-11 - val_loss: 3.3615e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2071e-11 - val_loss: 3.3585e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2027e-11 - val_loss: 3.3531e-11
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1993e-11 - val_loss: 3.3518e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1964e-11 - val_loss: 3.3492e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1923e-11 - val_loss: 3.3431e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1886e-11 - val_loss: 3.3457e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1887e-11 - val_loss: 3.3414e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1828e-11 - val_loss: 3.3332e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1785e-11 - val_loss: 3.3323e-11
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1777e-11 - val_loss: 3.3316e-11
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1740e-11 - val_loss: 3.3246e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1702e-11 - val_loss: 3.3256e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1729e-11 - val_loss: 3.3307e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1721e-11 - val_loss: 3.3275e-11
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1657e-11 - val_loss: 3.3137e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1584e-11 - val_loss: 3.3086e-11
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1523e-11 - val_loss: 3.3041e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1491e-11 - val_loss: 3.2991e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1420e-11 - val_loss: 3.2940e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1394e-11 - val_loss: 3.2915e-11
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1349e-11 - val_loss: 3.2824e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1292e-11 - val_loss: 3.2813e-11
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1282e-11 - val_loss: 3.2804e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1238e-11 - val_loss: 3.2782e-11
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1220e-11 - val_loss: 3.2727e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1174e-11 - val_loss: 3.2725e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1167e-11 - val_loss: 3.2677e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1121e-11 - val_loss: 3.2647e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1083e-11 - val_loss: 3.2592e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1045e-11 - val_loss: 3.2571e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0994e-11 - val_loss: 3.2519e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0989e-11 - val_loss: 3.2548e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0991e-11 - val_loss: 3.2440e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0917e-11 - val_loss: 3.2436e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0880e-11 - val_loss: 3.2406e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0852e-11 - val_loss: 3.2356e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0835e-11 - val_loss: 3.2319e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0768e-11 - val_loss: 3.2269e-11
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0735e-11 - val_loss: 3.2237e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0715e-11 - val_loss: 3.2159e-11
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0668e-11 - val_loss: 3.2154e-11
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0665e-11 - val_loss: 3.2130e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0593e-11 - val_loss: 3.2054e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0536e-11 - val_loss: 3.2033e-11
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0517e-11 - val_loss: 3.2014e-11
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0490e-11 - val_loss: 3.1930e-11
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0433e-11 - val_loss: 3.1902e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0406e-11 - val_loss: 3.1894e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0376e-11 - val_loss: 3.1859e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0349e-11 - val_loss: 3.1859e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0337e-11 - val_loss: 3.1821e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0311e-11 - val_loss: 3.1745e-11
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0249e-11 - val_loss: 3.1696e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0198e-11 - val_loss: 3.1696e-11
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0189e-11 - val_loss: 3.1619e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0138e-11 - val_loss: 3.1605e-11
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0121e-11 - val_loss: 3.1557e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0069e-11 - val_loss: 3.1550e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0033e-11 - val_loss: 3.1478e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9995e-11 - val_loss: 3.1456e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9981e-11 - val_loss: 3.1403e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9915e-11 - val_loss: 3.1375e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9905e-11 - val_loss: 3.1358e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9877e-11 - val_loss: 3.1301e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9823e-11 - val_loss: 3.1277e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9805e-11 - val_loss: 3.1264e-11
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9772e-11 - val_loss: 3.1195e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-4/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9687e-11 - val_loss: 3.1122e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9663e-11 - val_loss: 3.1130e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.498 | eve: 9.413 | bob: 9.334Epoch   0:   0% | abe: 9.423 | eve: 9.419 | bob: 9.277Epoch   0:   1% | abe: 9.371 | eve: 9.404 | bob: 9.238Epoch   0:   2% | abe: 9.343 | eve: 9.412 | bob: 9.221Epoch   0:   3% | abe: 9.295 | eve: 9.403 | bob: 9.183Epoch   0:   3% | abe: 9.272 | eve: 9.403 | bob: 9.170Epoch   0:   4% | abe: 9.240 | eve: 9.406 | bob: 9.145Epoch   0:   5% | abe: 9.228 | eve: 9.403 | bob: 9.137Epoch   0:   6% | abe: 9.220 | eve: 9.411 | bob: 9.133Epoch   0:   7% | abe: 9.210 | eve: 9.422 | bob: 9.126Epoch   0:   7% | abe: 9.205 | eve: 9.427 | bob: 9.124Epoch   0:   8% | abe: 9.195 | eve: 9.430 | bob: 9.116Epoch   0:   9% | abe: 9.182 | eve: 9.440 | bob: 9.105Epoch   0:  10% | abe: 9.175 | eve: 9.442 | bob: 9.099Epoch   0:  10% | abe: 9.168 | eve: 9.453 | bob: 9.094Epoch   0:  11% | abe: 9.161 | eve: 9.451 | bob: 9.087Epoch   0:  12% | abe: 9.155 | eve: 9.452 | bob: 9.082Epoch   0:  13% | abe: 9.153 | eve: 9.455 | bob: 9.081Epoch   0:  14% | abe: 9.152 | eve: 9.456 | bob: 9.081Epoch   0:  14% | abe: 9.147 | eve: 9.457 | bob: 9.076Epoch   0:  15% | abe: 9.146 | eve: 9.460 | bob: 9.076Epoch   0:  16% | abe: 9.140 | eve: 9.463 | bob: 9.071Epoch   0:  17% | abe: 9.136 | eve: 9.462 | bob: 9.067Epoch   0:  17% | abe: 9.132 | eve: 9.464 | bob: 9.063Epoch   0:  18% | abe: 9.132 | eve: 9.464 | bob: 9.064Epoch   0:  19% | abe: 9.130 | eve: 9.468 | bob: 9.062Epoch   0:  20% | abe: 9.130 | eve: 9.471 | bob: 9.062Epoch   0:  21% | abe: 9.127 | eve: 9.474 | bob: 9.060Epoch   0:  21% | abe: 9.127 | eve: 9.474 | bob: 9.059Epoch   0:  22% | abe: 9.126 | eve: 9.475 | bob: 9.058Epoch   0:  23% | abe: 9.124 | eve: 9.481 | bob: 9.057Epoch   0:  24% | abe: 9.122 | eve: 9.482 | bob: 9.056Epoch   0:  25% | abe: 9.123 | eve: 9.484 | bob: 9.057Epoch   0:  25% | abe: 9.123 | eve: 9.488 | bob: 9.056Epoch   0:  26% | abe: 9.119 | eve: 9.489 | bob: 9.053Epoch   0:  27% | abe: 9.118 | eve: 9.490 | bob: 9.052Epoch   0:  28% | abe: 9.116 | eve: 9.491 | bob: 9.050Epoch   0:  28% | abe: 9.116 | eve: 9.494 | bob: 9.050Epoch   0:  29% | abe: 9.114 | eve: 9.497 | bob: 9.049Epoch   0:  30% | abe: 9.112 | eve: 9.498 | bob: 9.047Epoch   0:  31% | abe: 9.112 | eve: 9.501 | bob: 9.046Epoch   0:  32% | abe: 9.110 | eve: 9.501 | bob: 9.045Epoch   0:  32% | abe: 9.111 | eve: 9.504 | bob: 9.045Epoch   0:  33% | abe: 9.110 | eve: 9.506 | bob: 9.045Epoch   0:  34% | abe: 9.108 | eve: 9.508 | bob: 9.043Epoch   0:  35% | abe: 9.106 | eve: 9.510 | bob: 9.041Epoch   0:  35% | abe: 9.106 | eve: 9.512 | bob: 9.041Epoch   0:  36% | abe: 9.105 | eve: 9.513 | bob: 9.039Epoch   0:  37% | abe: 9.104 | eve: 9.516 | bob: 9.039Epoch   0:  38% | abe: 9.104 | eve: 9.516 | bob: 9.039Epoch   0:  39% | abe: 9.103 | eve: 9.517 | bob: 9.038Epoch   0:  39% | abe: 9.103 | eve: 9.519 | bob: 9.038Epoch   0:  40% | abe: 9.103 | eve: 9.521 | bob: 9.038Epoch   0:  41% | abe: 9.103 | eve: 9.521 | bob: 9.039Epoch   0:  42% | abe: 9.103 | eve: 9.523 | bob: 9.038Epoch   0:  42% | abe: 9.103 | eve: 9.525 | bob: 9.039Epoch   0:  43% | abe: 9.102 | eve: 9.525 | bob: 9.038Epoch   0:  44% | abe: 9.102 | eve: 9.526 | bob: 9.038Epoch   0:  45% | abe: 9.101 | eve: 9.527 | bob: 9.037Epoch   0:  46% | abe: 9.101 | eve: 9.529 | bob: 9.036Epoch   0:  46% | abe: 9.100 | eve: 9.530 | bob: 9.036Epoch   0:  47% | abe: 9.100 | eve: 9.531 | bob: 9.036Epoch   0:  48% | abe: 9.100 | eve: 9.532 | bob: 9.036Epoch   0:  49% | abe: 9.100 | eve: 9.533 | bob: 9.036Epoch   0:  50% | abe: 9.099 | eve: 9.534 | bob: 9.035Epoch   0:  50% | abe: 9.098 | eve: 9.535 | bob: 9.035Epoch   0:  51% | abe: 9.098 | eve: 9.536 | bob: 9.034Epoch   0:  52% | abe: 9.097 | eve: 9.536 | bob: 9.034Epoch   0:  53% | abe: 9.097 | eve: 9.535 | bob: 9.033Epoch   0:  53% | abe: 9.097 | eve: 9.537 | bob: 9.033Epoch   0:  54% | abe: 9.098 | eve: 9.538 | bob: 9.034Epoch   0:  55% | abe: 9.097 | eve: 9.540 | bob: 9.034Epoch   0:  56% | abe: 9.096 | eve: 9.541 | bob: 9.033Epoch   0:  57% | abe: 9.095 | eve: 9.543 | bob: 9.032Epoch   0:  57% | abe: 9.095 | eve: 9.544 | bob: 9.032Epoch   0:  58% | abe: 9.094 | eve: 9.545 | bob: 9.031Epoch   0:  59% | abe: 9.094 | eve: 9.546 | bob: 9.031Epoch   0:  60% | abe: 9.093 | eve: 9.547 | bob: 9.030Epoch   0:  60% | abe: 9.094 | eve: 9.548 | bob: 9.031Epoch   0:  61% | abe: 9.093 | eve: 9.549 | bob: 9.031Epoch   0:  62% | abe: 9.093 | eve: 9.549 | bob: 9.031Epoch   0:  63% | abe: 9.093 | eve: 9.550 | bob: 9.031Epoch   0:  64% | abe: 9.093 | eve: 9.550 | bob: 9.030Epoch   0:  64% | abe: 9.092 | eve: 9.551 | bob: 9.030Epoch   0:  65% | abe: 9.092 | eve: 9.552 | bob: 9.030Epoch   0:  66% | abe: 9.092 | eve: 9.552 | bob: 9.030Epoch   0:  67% | abe: 9.092 | eve: 9.553 | bob: 9.030Epoch   0:  67% | abe: 9.091 | eve: 9.555 | bob: 9.029Epoch   0:  68% | abe: 9.091 | eve: 9.556 | bob: 9.029Epoch   0:  69% | abe: 9.091 | eve: 9.556 | bob: 9.029Epoch   0:  70% | abe: 9.091 | eve: 9.558 | bob: 9.029Epoch   0:  71% | abe: 9.091 | eve: 9.558 | bob: 9.029Epoch   0:  71% | abe: 9.091 | eve: 9.560 | bob: 9.029Epoch   0:  72% | abe: 9.090 | eve: 9.561 | bob: 9.028Epoch   0:  73% | abe: 9.089 | eve: 9.562 | bob: 9.027Epoch   0:  74% | abe: 9.089 | eve: 9.563 | bob: 9.027Epoch   0:  75% | abe: 9.089 | eve: 9.564 | bob: 9.027Epoch   0:  75% | abe: 9.088 | eve: 9.565 | bob: 9.027Epoch   0:  76% | abe: 9.088 | eve: 9.566 | bob: 9.027Epoch   0:  77% | abe: 9.088 | eve: 9.567 | bob: 9.026Epoch   0:  78% | abe: 9.088 | eve: 9.568 | bob: 9.026Epoch   0:  78% | abe: 9.088 | eve: 9.568 | bob: 9.026Epoch   0:  79% | abe: 9.087 | eve: 9.569 | bob: 9.026Epoch   0:  80% | abe: 9.087 | eve: 9.570 | bob: 9.025Epoch   0:  81% | abe: 9.086 | eve: 9.570 | bob: 9.025Epoch   0:  82% | abe: 9.086 | eve: 9.571 | bob: 9.025Epoch   0:  82% | abe: 9.086 | eve: 9.572 | bob: 9.025Epoch   0:  83% | abe: 9.086 | eve: 9.572 | bob: 9.025Epoch   0:  84% | abe: 9.086 | eve: 9.573 | bob: 9.025Epoch   0:  85% | abe: 9.086 | eve: 9.574 | bob: 9.026Epoch   0:  85% | abe: 9.086 | eve: 9.574 | bob: 9.025Epoch   0:  86% | abe: 9.086 | eve: 9.576 | bob: 9.025Epoch   0:  87% | abe: 9.085 | eve: 9.577 | bob: 9.025Epoch   0:  88% | abe: 9.086 | eve: 9.578 | bob: 9.025Epoch   0:  89% | abe: 9.086 | eve: 9.579 | bob: 9.025Epoch   0:  89% | abe: 9.086 | eve: 9.579 | bob: 9.026Epoch   0:  90% | abe: 9.086 | eve: 9.580 | bob: 9.026Epoch   0:  91% | abe: 9.086 | eve: 9.581 | bob: 9.025Epoch   0:  92% | abe: 9.086 | eve: 9.581 | bob: 9.025Epoch   0:  92% | abe: 9.085 | eve: 9.582 | bob: 9.025Epoch   0:  93% | abe: 9.085 | eve: 9.582 | bob: 9.025Epoch   0:  94% | abe: 9.085 | eve: 9.584 | bob: 9.025Epoch   0:  95% | abe: 9.086 | eve: 9.584 | bob: 9.025Epoch   0:  96% | abe: 9.086 | eve: 9.585 | bob: 9.026Epoch   0:  96% | abe: 9.085 | eve: 9.585 | bob: 9.025Epoch   0:  97% | abe: 9.085 | eve: 9.586 | bob: 9.025Epoch   0:  98% | abe: 9.085 | eve: 9.587 | bob: 9.025Epoch   0:  99% | abe: 9.085 | eve: 9.587 | bob: 9.025
New best Bob loss 9.024546604000534 at epoch 0
Epoch   1:   0% | abe: 9.031 | eve: 9.736 | bob: 8.974Epoch   1:   0% | abe: 9.064 | eve: 9.687 | bob: 9.009Epoch   1:   1% | abe: 9.069 | eve: 9.702 | bob: 9.014Epoch   1:   2% | abe: 9.083 | eve: 9.699 | bob: 9.029Epoch   1:   3% | abe: 9.083 | eve: 9.690 | bob: 9.029Epoch   1:   3% | abe: 9.075 | eve: 9.692 | bob: 9.021Epoch   1:   4% | abe: 9.076 | eve: 9.683 | bob: 9.022Epoch   1:   5% | abe: 9.067 | eve: 9.679 | bob: 9.012Epoch   1:   6% | abe: 9.060 | eve: 9.673 | bob: 9.005Epoch   1:   7% | abe: 9.060 | eve: 9.661 | bob: 9.005Epoch   1:   7% | abe: 9.061 | eve: 9.660 | bob: 9.006Epoch   1:   8% | abe: 9.060 | eve: 9.665 | bob: 9.005Epoch   1:   9% | abe: 9.059 | eve: 9.663 | bob: 9.004Epoch   1:  10% | abe: 9.057 | eve: 9.662 | bob: 9.002Epoch   1:  10% | abe: 9.062 | eve: 9.662 | bob: 9.006Epoch   1:  11% | abe: 9.062 | eve: 9.664 | bob: 9.007Epoch   1:  12% | abe: 9.065 | eve: 9.665 | bob: 9.010Epoch   1:  13% | abe: 9.061 | eve: 9.665 | bob: 9.005Epoch   1:  14% | abe: 9.058 | eve: 9.662 | bob: 9.002Epoch   1:  14% | abe: 9.057 | eve: 9.664 | bob: 9.001Epoch   1:  15% | abe: 9.058 | eve: 9.664 | bob: 9.002Epoch   1:  16% | abe: 9.057 | eve: 9.664 | bob: 9.001Epoch   1:  17% | abe: 9.059 | eve: 9.664 | bob: 9.002Epoch   1:  17% | abe: 9.061 | eve: 9.665 | bob: 9.005Epoch   1:  18% | abe: 9.060 | eve: 9.668 | bob: 9.004Epoch   1:  19% | abe: 9.058 | eve: 9.668 | bob: 9.002Epoch   1:  20% | abe: 9.057 | eve: 9.666 | bob: 9.001Epoch   1:  21% | abe: 9.056 | eve: 9.666 | bob: 8.999Epoch   1:  21% | abe: 9.059 | eve: 9.667 | bob: 9.002Epoch   1:  22% | abe: 9.058 | eve: 9.669 | bob: 9.000Epoch   1:  23% | abe: 9.057 | eve: 9.670 | bob: 9.000Epoch   1:  24% | abe: 9.058 | eve: 9.668 | bob: 9.000Epoch   1:  25% | abe: 9.055 | eve: 9.669 | bob: 8.998Epoch   1:  25% | abe: 9.053 | eve: 9.673 | bob: 8.996Epoch   1:  26% | abe: 9.053 | eve: 9.674 | bob: 8.996Epoch   1:  27% | abe: 9.053 | eve: 9.675 | bob: 8.996Epoch   1:  28% | abe: 9.054 | eve: 9.677 | bob: 8.996Epoch   1:  28% | abe: 9.055 | eve: 9.677 | bob: 8.997Epoch   1:  29% | abe: 9.055 | eve: 9.678 | bob: 8.997Epoch   1:  30% | abe: 9.054 | eve: 9.679 | bob: 8.996Epoch   1:  31% | abe: 9.053 | eve: 9.682 | bob: 8.995Epoch   1:  32% | abe: 9.052 | eve: 9.681 | bob: 8.994Epoch   1:  32% | abe: 9.053 | eve: 9.684 | bob: 8.994Epoch   1:  33% | abe: 9.054 | eve: 9.682 | bob: 8.996Epoch   1:  34% | abe: 9.053 | eve: 9.684 | bob: 8.995Epoch   1:  35% | abe: 9.053 | eve: 9.684 | bob: 8.995Epoch   1:  35% | abe: 9.053 | eve: 9.685 | bob: 8.995Epoch   1:  36% | abe: 9.054 | eve: 9.685 | bob: 8.996Epoch   1:  37% | abe: 9.054 | eve: 9.684 | bob: 8.996Epoch   1:  38% | abe: 9.055 | eve: 9.685 | bob: 8.997Epoch   1:  39% | abe: 9.056 | eve: 9.686 | bob: 8.998Epoch   1:  39% | abe: 9.056 | eve: 9.686 | bob: 8.998Epoch   1:  40% | abe: 9.056 | eve: 9.687 | bob: 8.998Epoch   1:  41% | abe: 9.054 | eve: 9.687 | bob: 8.996Epoch   1:  42% | abe: 9.054 | eve: 9.688 | bob: 8.996Epoch   1:  42% | abe: 9.054 | eve: 9.688 | bob: 8.996Epoch   1:  43% | abe: 9.054 | eve: 9.688 | bob: 8.996Epoch   1:  44% | abe: 9.054 | eve: 9.688 | bob: 8.995Epoch   1:  45% | abe: 9.054 | eve: 9.688 | bob: 8.996Epoch   1:  46% | abe: 9.055 | eve: 9.689 | bob: 8.997Epoch   1:  46% | abe: 9.055 | eve: 9.689 | bob: 8.996Epoch   1:  47% | abe: 9.054 | eve: 9.691 | bob: 8.996Epoch   1:  48% | abe: 9.055 | eve: 9.691 | bob: 8.997Epoch   1:  49% | abe: 9.054 | eve: 9.692 | bob: 8.996Epoch   1:  50% | abe: 9.054 | eve: 9.692 | bob: 8.996Epoch   1:  50% | abe: 9.053 | eve: 9.694 | bob: 8.995Epoch   1:  51% | abe: 9.053 | eve: 9.693 | bob: 8.994Epoch   1:  52% | abe: 9.054 | eve: 9.693 | bob: 8.995Epoch   1:  53% | abe: 9.053 | eve: 9.693 | bob: 8.995Epoch   1:  53% | abe: 9.052 | eve: 9.693 | bob: 8.994Epoch   1:  54% | abe: 9.052 | eve: 9.693 | bob: 8.993Epoch   1:  55% | abe: 9.051 | eve: 9.694 | bob: 8.993Epoch   1:  56% | abe: 9.051 | eve: 9.693 | bob: 8.992Epoch   1:  57% | abe: 9.051 | eve: 9.694 | bob: 8.993Epoch   1:  57% | abe: 9.052 | eve: 9.693 | bob: 8.993Epoch   1:  58% | abe: 9.051 | eve: 9.693 | bob: 8.992Epoch   1:  59% | abe: 9.052 | eve: 9.693 | bob: 8.993Epoch   1:  60% | abe: 9.051 | eve: 9.693 | bob: 8.993Epoch   1:  60% | abe: 9.051 | eve: 9.691 | bob: 8.992Epoch   1:  61% | abe: 9.051 | eve: 9.691 | bob: 8.992Epoch   1:  62% | abe: 9.050 | eve: 9.691 | bob: 8.991Epoch   1:  63% | abe: 9.049 | eve: 9.691 | bob: 8.991Epoch   1:  64% | abe: 9.049 | eve: 9.692 | bob: 8.991Epoch   1:  64% | abe: 9.049 | eve: 9.692 | bob: 8.991Epoch   1:  65% | abe: 9.049 | eve: 9.691 | bob: 8.990Epoch   1:  66% | abe: 9.049 | eve: 9.690 | bob: 8.990Epoch   1:  67% | abe: 9.049 | eve: 9.690 | bob: 8.990Epoch   1:  67% | abe: 9.048 | eve: 9.692 | bob: 8.990Epoch   1:  68% | abe: 9.048 | eve: 9.691 | bob: 8.989Epoch   1:  69% | abe: 9.047 | eve: 9.692 | bob: 8.989Epoch   1:  70% | abe: 9.047 | eve: 9.691 | bob: 8.988Epoch   1:  71% | abe: 9.046 | eve: 9.691 | bob: 8.988Epoch   1:  71% | abe: 9.047 | eve: 9.691 | bob: 8.988Epoch   1:  72% | abe: 9.046 | eve: 9.691 | bob: 8.987Epoch   1:  73% | abe: 9.046 | eve: 9.691 | bob: 8.987Epoch   1:  74% | abe: 9.046 | eve: 9.692 | bob: 8.987Epoch   1:  75% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  75% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  76% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  77% | abe: 9.045 | eve: 9.692 | bob: 8.986Epoch   1:  78% | abe: 9.045 | eve: 9.692 | bob: 8.986Epoch   1:  78% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  79% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  80% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  81% | abe: 9.045 | eve: 9.691 | bob: 8.986Epoch   1:  82% | abe: 9.044 | eve: 9.692 | bob: 8.985Epoch   1:  82% | abe: 9.043 | eve: 9.692 | bob: 8.984Epoch   1:  83% | abe: 9.043 | eve: 9.691 | bob: 8.984Epoch   1:  84% | abe: 9.043 | eve: 9.691 | bob: 8.984Epoch   1:  85% | abe: 9.042 | eve: 9.691 | bob: 8.983Epoch   1:  85% | abe: 9.041 | eve: 9.691 | bob: 8.982Epoch   1:  86% | abe: 9.041 | eve: 9.691 | bob: 8.982Epoch   1:  87% | abe: 9.040 | eve: 9.691 | bob: 8.981Epoch   1:  88% | abe: 9.040 | eve: 9.690 | bob: 8.981Epoch   1:  89% | abe: 9.039 | eve: 9.691 | bob: 8.980Epoch   1:  89% | abe: 9.039 | eve: 9.690 | bob: 8.980Epoch   1:  90% | abe: 9.039 | eve: 9.690 | bob: 8.980Epoch   1:  91% | abe: 9.039 | eve: 9.690 | bob: 8.980Epoch   1:  92% | abe: 9.038 | eve: 9.689 | bob: 8.980Epoch   1:  92% | abe: 9.038 | eve: 9.689 | bob: 8.979Epoch   1:  93% | abe: 9.038 | eve: 9.690 | bob: 8.979Epoch   1:  94% | abe: 9.037 | eve: 9.689 | bob: 8.978Epoch   1:  95% | abe: 9.037 | eve: 9.690 | bob: 8.978Epoch   1:  96% | abe: 9.036 | eve: 9.690 | bob: 8.978Epoch   1:  96% | abe: 9.036 | eve: 9.690 | bob: 8.977Epoch   1:  97% | abe: 9.035 | eve: 9.690 | bob: 8.977Epoch   1:  98% | abe: 9.035 | eve: 9.690 | bob: 8.977Epoch   1:  99% | abe: 9.035 | eve: 9.690 | bob: 8.977
New best Bob loss 8.976505015288467 at epoch 1
Epoch   2:   0% | abe: 8.927 | eve: 9.672 | bob: 8.873Epoch   2:   0% | abe: 8.962 | eve: 9.703 | bob: 8.910Epoch   2:   1% | abe: 8.962 | eve: 9.684 | bob: 8.910Epoch   2:   2% | abe: 8.967 | eve: 9.684 | bob: 8.914Epoch   2:   3% | abe: 8.966 | eve: 9.689 | bob: 8.914Epoch   2:   3% | abe: 8.967 | eve: 9.693 | bob: 8.914Epoch   2:   4% | abe: 8.968 | eve: 9.694 | bob: 8.917Epoch   2:   5% | abe: 8.968 | eve: 9.689 | bob: 8.917Epoch   2:   6% | abe: 8.971 | eve: 9.695 | bob: 8.920Epoch   2:   7% | abe: 8.973 | eve: 9.693 | bob: 8.923Epoch   2:   7% | abe: 8.977 | eve: 9.697 | bob: 8.926Epoch   2:   8% | abe: 8.978 | eve: 9.703 | bob: 8.928Epoch   2:   9% | abe: 8.975 | eve: 9.700 | bob: 8.925Epoch   2:  10% | abe: 8.973 | eve: 9.697 | bob: 8.923Epoch   2:  10% | abe: 8.975 | eve: 9.699 | bob: 8.925Epoch   2:  11% | abe: 8.978 | eve: 9.702 | bob: 8.929Epoch   2:  12% | abe: 8.974 | eve: 9.705 | bob: 8.925Epoch   2:  13% | abe: 8.971 | eve: 9.706 | bob: 8.921Epoch   2:  14% | abe: 8.973 | eve: 9.708 | bob: 8.924Epoch   2:  14% | abe: 8.973 | eve: 9.710 | bob: 8.924Epoch   2:  15% | abe: 8.969 | eve: 9.710 | bob: 8.920Epoch   2:  16% | abe: 8.966 | eve: 9.708 | bob: 8.917Epoch   2:  17% | abe: 8.965 | eve: 9.709 | bob: 8.916Epoch   2:  17% | abe: 8.965 | eve: 9.708 | bob: 8.916Epoch   2:  18% | abe: 8.965 | eve: 9.707 | bob: 8.917Epoch   2:  19% | abe: 8.965 | eve: 9.705 | bob: 8.916Epoch   2:  20% | abe: 8.963 | eve: 9.706 | bob: 8.914Epoch   2:  21% | abe: 8.962 | eve: 9.707 | bob: 8.913Epoch   2:  21% | abe: 8.961 | eve: 9.706 | bob: 8.912Epoch   2:  22% | abe: 8.961 | eve: 9.702 | bob: 8.912Epoch   2:  23% | abe: 8.960 | eve: 9.702 | bob: 8.911Epoch   2:  24% | abe: 8.959 | eve: 9.702 | bob: 8.911Epoch   2:  25% | abe: 8.958 | eve: 9.704 | bob: 8.910Epoch   2:  25% | abe: 8.958 | eve: 9.703 | bob: 8.910Epoch   2:  26% | abe: 8.957 | eve: 9.703 | bob: 8.909Epoch   2:  27% | abe: 8.956 | eve: 9.702 | bob: 8.908Epoch   2:  28% | abe: 8.956 | eve: 9.702 | bob: 8.909Epoch   2:  28% | abe: 8.955 | eve: 9.702 | bob: 8.908Epoch   2:  29% | abe: 8.954 | eve: 9.703 | bob: 8.907Epoch   2:  30% | abe: 8.953 | eve: 9.702 | bob: 8.906Epoch   2:  31% | abe: 8.952 | eve: 9.701 | bob: 8.905Epoch   2:  32% | abe: 8.951 | eve: 9.703 | bob: 8.904Epoch   2:  32% | abe: 8.950 | eve: 9.702 | bob: 8.903Epoch   2:  33% | abe: 8.948 | eve: 9.702 | bob: 8.902Epoch   2:  34% | abe: 8.947 | eve: 9.701 | bob: 8.901Epoch   2:  35% | abe: 8.946 | eve: 9.701 | bob: 8.900Epoch   2:  35% | abe: 8.946 | eve: 9.701 | bob: 8.900Epoch   2:  36% | abe: 8.946 | eve: 9.701 | bob: 8.900Epoch   2:  37% | abe: 8.946 | eve: 9.702 | bob: 8.900Epoch   2:  38% | abe: 8.945 | eve: 9.703 | bob: 8.899Epoch   2:  39% | abe: 8.944 | eve: 9.704 | bob: 8.898Epoch   2:  39% | abe: 8.942 | eve: 9.704 | bob: 8.897Epoch   2:  40% | abe: 8.941 | eve: 9.705 | bob: 8.896Epoch   2:  41% | abe: 8.941 | eve: 9.706 | bob: 8.896Epoch   2:  42% | abe: 8.940 | eve: 9.705 | bob: 8.895Epoch   2:  42% | abe: 8.940 | eve: 9.705 | bob: 8.895Epoch   2:  43% | abe: 8.939 | eve: 9.706 | bob: 8.895Epoch   2:  44% | abe: 8.937 | eve: 9.705 | bob: 8.893Epoch   2:  45% | abe: 8.937 | eve: 9.705 | bob: 8.892Epoch   2:  46% | abe: 8.935 | eve: 9.704 | bob: 8.891Epoch   2:  46% | abe: 8.934 | eve: 9.703 | bob: 8.890Epoch   2:  47% | abe: 8.933 | eve: 9.704 | bob: 8.889Epoch   2:  48% | abe: 8.932 | eve: 9.703 | bob: 8.888Epoch   2:  49% | abe: 8.931 | eve: 9.704 | bob: 8.887Epoch   2:  50% | abe: 8.930 | eve: 9.704 | bob: 8.887Epoch   2:  50% | abe: 8.929 | eve: 9.703 | bob: 8.886Epoch   2:  51% | abe: 8.928 | eve: 9.703 | bob: 8.885Epoch   2:  52% | abe: 8.927 | eve: 9.702 | bob: 8.884Epoch   2:  53% | abe: 8.926 | eve: 9.703 | bob: 8.883Epoch   2:  53% | abe: 8.926 | eve: 9.703 | bob: 8.883Epoch   2:  54% | abe: 8.925 | eve: 9.704 | bob: 8.882Epoch   2:  55% | abe: 8.924 | eve: 9.703 | bob: 8.882Epoch   2:  56% | abe: 8.922 | eve: 9.704 | bob: 8.880Epoch   2:  57% | abe: 8.922 | eve: 9.705 | bob: 8.880Epoch   2:  57% | abe: 8.920 | eve: 9.705 | bob: 8.878Epoch   2:  58% | abe: 8.919 | eve: 9.705 | bob: 8.877Epoch   2:  59% | abe: 8.918 | eve: 9.705 | bob: 8.877Epoch   2:  60% | abe: 8.917 | eve: 9.706 | bob: 8.876Epoch   2:  60% | abe: 8.917 | eve: 9.706 | bob: 8.876Epoch   2:  61% | abe: 8.916 | eve: 9.706 | bob: 8.874Epoch   2:  62% | abe: 8.914 | eve: 9.706 | bob: 8.873Epoch   2:  63% | abe: 8.913 | eve: 9.706 | bob: 8.872Epoch   2:  64% | abe: 8.912 | eve: 9.706 | bob: 8.871Epoch   2:  64% | abe: 8.910 | eve: 9.705 | bob: 8.870Epoch   2:  65% | abe: 8.910 | eve: 9.704 | bob: 8.869Epoch   2:  66% | abe: 8.909 | eve: 9.704 | bob: 8.869Epoch   2:  67% | abe: 8.907 | eve: 9.705 | bob: 8.867Epoch   2:  67% | abe: 8.906 | eve: 9.705 | bob: 8.866Epoch   2:  68% | abe: 8.905 | eve: 9.706 | bob: 8.865Epoch   2:  69% | abe: 8.904 | eve: 9.707 | bob: 8.864Epoch   2:  70% | abe: 8.904 | eve: 9.706 | bob: 8.864Epoch   2:  71% | abe: 8.903 | eve: 9.706 | bob: 8.864Epoch   2:  71% | abe: 8.903 | eve: 9.707 | bob: 8.863Epoch   2:  72% | abe: 8.901 | eve: 9.707 | bob: 8.862Epoch   2:  73% | abe: 8.900 | eve: 9.706 | bob: 8.861Epoch   2:  74% | abe: 8.899 | eve: 9.706 | bob: 8.860Epoch   2:  75% | abe: 8.898 | eve: 9.707 | bob: 8.859Epoch   2:  75% | abe: 8.897 | eve: 9.708 | bob: 8.858Epoch   2:  76% | abe: 8.896 | eve: 9.708 | bob: 8.857Epoch   2:  77% | abe: 8.895 | eve: 9.708 | bob: 8.856Epoch   2:  78% | abe: 8.894 | eve: 9.707 | bob: 8.855Epoch   2:  78% | abe: 8.893 | eve: 9.707 | bob: 8.854Epoch   2:  79% | abe: 8.892 | eve: 9.707 | bob: 8.854Epoch   2:  80% | abe: 8.890 | eve: 9.706 | bob: 8.852Epoch   2:  81% | abe: 8.889 | eve: 9.705 | bob: 8.851Epoch   2:  82% | abe: 8.888 | eve: 9.706 | bob: 8.850Epoch   2:  82% | abe: 8.887 | eve: 9.706 | bob: 8.849Epoch   2:  83% | abe: 8.886 | eve: 9.705 | bob: 8.848Epoch   2:  84% | abe: 8.884 | eve: 9.706 | bob: 8.847Epoch   2:  85% | abe: 8.883 | eve: 9.706 | bob: 8.845Epoch   2:  85% | abe: 8.882 | eve: 9.706 | bob: 8.844Epoch   2:  86% | abe: 8.881 | eve: 9.705 | bob: 8.844Epoch   2:  87% | abe: 8.879 | eve: 9.706 | bob: 8.842Epoch   2:  88% | abe: 8.878 | eve: 9.705 | bob: 8.841Epoch   2:  89% | abe: 8.877 | eve: 9.705 | bob: 8.840Epoch   2:  89% | abe: 8.876 | eve: 9.705 | bob: 8.839Epoch   2:  90% | abe: 8.875 | eve: 9.705 | bob: 8.838Epoch   2:  91% | abe: 8.873 | eve: 9.706 | bob: 8.837Epoch   2:  92% | abe: 8.872 | eve: 9.706 | bob: 8.836Epoch   2:  92% | abe: 8.871 | eve: 9.706 | bob: 8.835Epoch   2:  93% | abe: 8.869 | eve: 9.706 | bob: 8.833Epoch   2:  94% | abe: 8.868 | eve: 9.706 | bob: 8.832Epoch   2:  95% | abe: 8.867 | eve: 9.705 | bob: 8.831Epoch   2:  96% | abe: 8.866 | eve: 9.705 | bob: 8.831Epoch   2:  96% | abe: 8.865 | eve: 9.705 | bob: 8.829Epoch   2:  97% | abe: 8.863 | eve: 9.706 | bob: 8.828Epoch   2:  98% | abe: 8.862 | eve: 9.706 | bob: 8.826Epoch   2:  99% | abe: 8.861 | eve: 9.706 | bob: 8.825
New best Bob loss 8.825401630369925 at epoch 2
Epoch   3:   0% | abe: 8.678 | eve: 9.696 | bob: 8.654Epoch   3:   0% | abe: 8.695 | eve: 9.711 | bob: 8.668Epoch   3:   1% | abe: 8.668 | eve: 9.723 | bob: 8.645Epoch   3:   2% | abe: 8.675 | eve: 9.728 | bob: 8.652Epoch   3:   3% | abe: 8.687 | eve: 9.718 | bob: 8.664Epoch   3:   3% | abe: 8.686 | eve: 9.719 | bob: 8.665Epoch   3:   4% | abe: 8.692 | eve: 9.713 | bob: 8.670Epoch   3:   5% | abe: 8.686 | eve: 9.717 | bob: 8.663Epoch   3:   6% | abe: 8.685 | eve: 9.716 | bob: 8.663Epoch   3:   7% | abe: 8.680 | eve: 9.715 | bob: 8.659Epoch   3:   7% | abe: 8.678 | eve: 9.718 | bob: 8.657Epoch   3:   8% | abe: 8.669 | eve: 9.729 | bob: 8.647Epoch   3:   9% | abe: 8.670 | eve: 9.726 | bob: 8.648Epoch   3:  10% | abe: 8.669 | eve: 9.722 | bob: 8.647Epoch   3:  10% | abe: 8.668 | eve: 9.719 | bob: 8.646Epoch   3:  11% | abe: 8.664 | eve: 9.714 | bob: 8.642Epoch   3:  12% | abe: 8.662 | eve: 9.714 | bob: 8.640Epoch   3:  13% | abe: 8.663 | eve: 9.713 | bob: 8.642Epoch   3:  14% | abe: 8.660 | eve: 9.710 | bob: 8.638Epoch   3:  14% | abe: 8.659 | eve: 9.710 | bob: 8.637Epoch   3:  15% | abe: 8.657 | eve: 9.709 | bob: 8.636Epoch   3:  16% | abe: 8.656 | eve: 9.707 | bob: 8.634Epoch   3:  17% | abe: 8.657 | eve: 9.710 | bob: 8.636Epoch   3:  17% | abe: 8.656 | eve: 9.710 | bob: 8.634Epoch   3:  18% | abe: 8.653 | eve: 9.709 | bob: 8.631Epoch   3:  19% | abe: 8.648 | eve: 9.709 | bob: 8.626Epoch   3:  20% | abe: 8.646 | eve: 9.709 | bob: 8.624Epoch   3:  21% | abe: 8.644 | eve: 9.710 | bob: 8.622Epoch   3:  21% | abe: 8.642 | eve: 9.712 | bob: 8.620Epoch   3:  22% | abe: 8.640 | eve: 9.712 | bob: 8.619Epoch   3:  23% | abe: 8.637 | eve: 9.715 | bob: 8.615Epoch   3:  24% | abe: 8.636 | eve: 9.713 | bob: 8.614Epoch   3:  25% | abe: 8.633 | eve: 9.714 | bob: 8.611Epoch   3:  25% | abe: 8.630 | eve: 9.715 | bob: 8.608Epoch   3:  26% | abe: 8.627 | eve: 9.712 | bob: 8.605Epoch   3:  27% | abe: 8.624 | eve: 9.712 | bob: 8.602Epoch   3:  28% | abe: 8.624 | eve: 9.712 | bob: 8.602Epoch   3:  28% | abe: 8.622 | eve: 9.713 | bob: 8.600Epoch   3:  29% | abe: 8.620 | eve: 9.715 | bob: 8.598Epoch   3:  30% | abe: 8.617 | eve: 9.714 | bob: 8.595Epoch   3:  31% | abe: 8.615 | eve: 9.715 | bob: 8.593Epoch   3:  32% | abe: 8.613 | eve: 9.716 | bob: 8.590Epoch   3:  32% | abe: 8.609 | eve: 9.716 | bob: 8.587Epoch   3:  33% | abe: 8.607 | eve: 9.715 | bob: 8.585Epoch   3:  34% | abe: 8.606 | eve: 9.716 | bob: 8.583Epoch   3:  35% | abe: 8.603 | eve: 9.716 | bob: 8.581Epoch   3:  35% | abe: 8.601 | eve: 9.716 | bob: 8.579Epoch   3:  36% | abe: 8.599 | eve: 9.716 | bob: 8.577Epoch   3:  37% | abe: 8.597 | eve: 9.717 | bob: 8.575Epoch   3:  38% | abe: 8.596 | eve: 9.717 | bob: 8.574Epoch   3:  39% | abe: 8.594 | eve: 9.717 | bob: 8.572Epoch   3:  39% | abe: 8.593 | eve: 9.716 | bob: 8.570Epoch   3:  40% | abe: 8.591 | eve: 9.717 | bob: 8.568Epoch   3:  41% | abe: 8.588 | eve: 9.716 | bob: 8.565Epoch   3:  42% | abe: 8.587 | eve: 9.716 | bob: 8.564Epoch   3:  42% | abe: 8.585 | eve: 9.714 | bob: 8.562Epoch   3:  43% | abe: 8.583 | eve: 9.715 | bob: 8.560Epoch   3:  44% | abe: 8.581 | eve: 9.715 | bob: 8.558Epoch   3:  45% | abe: 8.580 | eve: 9.715 | bob: 8.557Epoch   3:  46% | abe: 8.578 | eve: 9.716 | bob: 8.555Epoch   3:  46% | abe: 8.576 | eve: 9.717 | bob: 8.553Epoch   3:  47% | abe: 8.575 | eve: 9.717 | bob: 8.552Epoch   3:  48% | abe: 8.574 | eve: 9.717 | bob: 8.551Epoch   3:  49% | abe: 8.572 | eve: 9.719 | bob: 8.548Epoch   3:  50% | abe: 8.570 | eve: 9.720 | bob: 8.547Epoch   3:  50% | abe: 8.568 | eve: 9.721 | bob: 8.544Epoch   3:  51% | abe: 8.567 | eve: 9.720 | bob: 8.543Epoch   3:  52% | abe: 8.564 | eve: 9.719 | bob: 8.540Epoch   3:  53% | abe: 8.563 | eve: 9.719 | bob: 8.539Epoch   3:  53% | abe: 8.561 | eve: 9.720 | bob: 8.537Epoch   3:  54% | abe: 8.560 | eve: 9.719 | bob: 8.536Epoch   3:  55% | abe: 8.558 | eve: 9.719 | bob: 8.534Epoch   3:  56% | abe: 8.556 | eve: 9.719 | bob: 8.532Epoch   3:  57% | abe: 8.554 | eve: 9.719 | bob: 8.530Epoch   3:  57% | abe: 8.552 | eve: 9.719 | bob: 8.528Epoch   3:  58% | abe: 8.550 | eve: 9.719 | bob: 8.526Epoch   3:  59% | abe: 8.549 | eve: 9.720 | bob: 8.524Epoch   3:  60% | abe: 8.547 | eve: 9.720 | bob: 8.522Epoch   3:  60% | abe: 8.545 | eve: 9.720 | bob: 8.520Epoch   3:  61% | abe: 8.543 | eve: 9.721 | bob: 8.519Epoch   3:  62% | abe: 8.541 | eve: 9.721 | bob: 8.517Epoch   3:  63% | abe: 8.539 | eve: 9.722 | bob: 8.515Epoch   3:  64% | abe: 8.538 | eve: 9.723 | bob: 8.513Epoch   3:  64% | abe: 8.536 | eve: 9.723 | bob: 8.512Epoch   3:  65% | abe: 8.535 | eve: 9.723 | bob: 8.510Epoch   3:  66% | abe: 8.534 | eve: 9.724 | bob: 8.509Epoch   3:  67% | abe: 8.532 | eve: 9.724 | bob: 8.507Epoch   3:  67% | abe: 8.530 | eve: 9.724 | bob: 8.505Epoch   3:  68% | abe: 8.528 | eve: 9.724 | bob: 8.503Epoch   3:  69% | abe: 8.526 | eve: 9.724 | bob: 8.501Epoch   3:  70% | abe: 8.524 | eve: 9.724 | bob: 8.499Epoch   3:  71% | abe: 8.522 | eve: 9.725 | bob: 8.497Epoch   3:  71% | abe: 8.520 | eve: 9.726 | bob: 8.495Epoch   3:  72% | abe: 8.518 | eve: 9.727 | bob: 8.493Epoch   3:  73% | abe: 8.517 | eve: 9.727 | bob: 8.491Epoch   3:  74% | abe: 8.515 | eve: 9.728 | bob: 8.489Epoch   3:  75% | abe: 8.514 | eve: 9.728 | bob: 8.488Epoch   3:  75% | abe: 8.512 | eve: 9.729 | bob: 8.486Epoch   3:  76% | abe: 8.510 | eve: 9.729 | bob: 8.484Epoch   3:  77% | abe: 8.508 | eve: 9.729 | bob: 8.482Epoch   3:  78% | abe: 8.506 | eve: 9.730 | bob: 8.480Epoch   3:  78% | abe: 8.505 | eve: 9.729 | bob: 8.479Epoch   3:  79% | abe: 8.503 | eve: 9.730 | bob: 8.477Epoch   3:  80% | abe: 8.501 | eve: 9.731 | bob: 8.475Epoch   3:  81% | abe: 8.499 | eve: 9.731 | bob: 8.472Epoch   3:  82% | abe: 8.497 | eve: 9.732 | bob: 8.471Epoch   3:  82% | abe: 8.496 | eve: 9.733 | bob: 8.469Epoch   3:  83% | abe: 8.494 | eve: 9.734 | bob: 8.467Epoch   3:  84% | abe: 8.493 | eve: 9.735 | bob: 8.466Epoch   3:  85% | abe: 8.491 | eve: 9.735 | bob: 8.464Epoch   3:  85% | abe: 8.489 | eve: 9.736 | bob: 8.462Epoch   3:  86% | abe: 8.487 | eve: 9.737 | bob: 8.460Epoch   3:  87% | abe: 8.485 | eve: 9.738 | bob: 8.458Epoch   3:  88% | abe: 8.483 | eve: 9.738 | bob: 8.456Epoch   3:  89% | abe: 8.481 | eve: 9.738 | bob: 8.454Epoch   3:  89% | abe: 8.479 | eve: 9.738 | bob: 8.452Epoch   3:  90% | abe: 8.477 | eve: 9.739 | bob: 8.450Epoch   3:  91% | abe: 8.476 | eve: 9.739 | bob: 8.448Epoch   3:  92% | abe: 8.474 | eve: 9.739 | bob: 8.446Epoch   3:  92% | abe: 8.472 | eve: 9.740 | bob: 8.444Epoch   3:  93% | abe: 8.470 | eve: 9.741 | bob: 8.442Epoch   3:  94% | abe: 8.468 | eve: 9.741 | bob: 8.440Epoch   3:  95% | abe: 8.466 | eve: 9.742 | bob: 8.438Epoch   3:  96% | abe: 8.465 | eve: 9.742 | bob: 8.437Epoch   3:  96% | abe: 8.463 | eve: 9.743 | bob: 8.435Epoch   3:  97% | abe: 8.461 | eve: 9.743 | bob: 8.433Epoch   3:  98% | abe: 8.459 | eve: 9.744 | bob: 8.431Epoch   3:  99% | abe: 8.457 | eve: 9.745 | bob: 8.429
New best Bob loss 8.429001264063572 at epoch 3
Epoch   4:   0% | abe: 8.210 | eve: 9.863 | bob: 8.169Epoch   4:   0% | abe: 8.188 | eve: 9.873 | bob: 8.148Epoch   4:   1% | abe: 8.181 | eve: 9.853 | bob: 8.142Epoch   4:   2% | abe: 8.195 | eve: 9.830 | bob: 8.156Epoch   4:   3% | abe: 8.193 | eve: 9.823 | bob: 8.154Epoch   4:   3% | abe: 8.190 | eve: 9.827 | bob: 8.150Epoch   4:   4% | abe: 8.187 | eve: 9.826 | bob: 8.146Epoch   4:   5% | abe: 8.192 | eve: 9.816 | bob: 8.151Epoch   4:   6% | abe: 8.192 | eve: 9.819 | bob: 8.151Epoch   4:   7% | abe: 8.195 | eve: 9.823 | bob: 8.153Epoch   4:   7% | abe: 8.194 | eve: 9.813 | bob: 8.152Epoch   4:   8% | abe: 8.192 | eve: 9.817 | bob: 8.151Epoch   4:   9% | abe: 8.184 | eve: 9.810 | bob: 8.143Epoch   4:  10% | abe: 8.182 | eve: 9.809 | bob: 8.142Epoch   4:  10% | abe: 8.183 | eve: 9.809 | bob: 8.142Epoch   4:  11% | abe: 8.181 | eve: 9.808 | bob: 8.140Epoch   4:  12% | abe: 8.180 | eve: 9.805 | bob: 8.138Epoch   4:  13% | abe: 8.177 | eve: 9.809 | bob: 8.135Epoch   4:  14% | abe: 8.178 | eve: 9.813 | bob: 8.135Epoch   4:  14% | abe: 8.176 | eve: 9.812 | bob: 8.134Epoch   4:  15% | abe: 8.174 | eve: 9.813 | bob: 8.132Epoch   4:  16% | abe: 8.173 | eve: 9.817 | bob: 8.131Epoch   4:  17% | abe: 8.171 | eve: 9.816 | bob: 8.130Epoch   4:  17% | abe: 8.169 | eve: 9.818 | bob: 8.128Epoch   4:  18% | abe: 8.166 | eve: 9.820 | bob: 8.125Epoch   4:  19% | abe: 8.163 | eve: 9.819 | bob: 8.122Epoch   4:  20% | abe: 8.161 | eve: 9.819 | bob: 8.120Epoch   4:  21% | abe: 8.160 | eve: 9.819 | bob: 8.118Epoch   4:  21% | abe: 8.158 | eve: 9.819 | bob: 8.116Epoch   4:  22% | abe: 8.156 | eve: 9.821 | bob: 8.114Epoch   4:  23% | abe: 8.155 | eve: 9.820 | bob: 8.113Epoch   4:  24% | abe: 8.152 | eve: 9.824 | bob: 8.110Epoch   4:  25% | abe: 8.150 | eve: 9.825 | bob: 8.107Epoch   4:  25% | abe: 8.149 | eve: 9.824 | bob: 8.106Epoch   4:  26% | abe: 8.148 | eve: 9.823 | bob: 8.105Epoch   4:  27% | abe: 8.146 | eve: 9.823 | bob: 8.103Epoch   4:  28% | abe: 8.144 | eve: 9.825 | bob: 8.100Epoch   4:  28% | abe: 8.141 | eve: 9.825 | bob: 8.097Epoch   4:  29% | abe: 8.139 | eve: 9.823 | bob: 8.096Epoch   4:  30% | abe: 8.137 | eve: 9.823 | bob: 8.093Epoch   4:  31% | abe: 8.135 | eve: 9.823 | bob: 8.091Epoch   4:  32% | abe: 8.132 | eve: 9.823 | bob: 8.089Epoch   4:  32% | abe: 8.130 | eve: 9.823 | bob: 8.087Epoch   4:  33% | abe: 8.128 | eve: 9.823 | bob: 8.085Epoch   4:  34% | abe: 8.126 | eve: 9.824 | bob: 8.082Epoch   4:  35% | abe: 8.124 | eve: 9.825 | bob: 8.080Epoch   4:  35% | abe: 8.123 | eve: 9.825 | bob: 8.078Epoch   4:  36% | abe: 8.121 | eve: 9.824 | bob: 8.077Epoch   4:  37% | abe: 8.119 | eve: 9.825 | bob: 8.075Epoch   4:  38% | abe: 8.117 | eve: 9.825 | bob: 8.072Epoch   4:  39% | abe: 8.114 | eve: 9.825 | bob: 8.070Epoch   4:  39% | abe: 8.111 | eve: 9.826 | bob: 8.066Epoch   4:  40% | abe: 8.109 | eve: 9.825 | bob: 8.064Epoch   4:  41% | abe: 8.108 | eve: 9.823 | bob: 8.063Epoch   4:  42% | abe: 8.107 | eve: 9.824 | bob: 8.061Epoch   4:  42% | abe: 8.106 | eve: 9.823 | bob: 8.060Epoch   4:  43% | abe: 8.104 | eve: 9.822 | bob: 8.059Epoch   4:  44% | abe: 8.102 | eve: 9.822 | bob: 8.057Epoch   4:  45% | abe: 8.100 | eve: 9.822 | bob: 8.055Epoch   4:  46% | abe: 8.098 | eve: 9.823 | bob: 8.053Epoch   4:  46% | abe: 8.096 | eve: 9.823 | bob: 8.051Epoch   4:  47% | abe: 8.094 | eve: 9.823 | bob: 8.049Epoch   4:  48% | abe: 8.092 | eve: 9.823 | bob: 8.047Epoch   4:  49% | abe: 8.090 | eve: 9.822 | bob: 8.044Epoch   4:  50% | abe: 8.088 | eve: 9.822 | bob: 8.043Epoch   4:  50% | abe: 8.087 | eve: 9.821 | bob: 8.041Epoch   4:  51% | abe: 8.085 | eve: 9.822 | bob: 8.039Epoch   4:  52% | abe: 8.083 | eve: 9.822 | bob: 8.037Epoch   4:  53% | abe: 8.081 | eve: 9.821 | bob: 8.035Epoch   4:  53% | abe: 8.080 | eve: 9.821 | bob: 8.034Epoch   4:  54% | abe: 8.078 | eve: 9.820 | bob: 8.032Epoch   4:  55% | abe: 8.076 | eve: 9.821 | bob: 8.030Epoch   4:  56% | abe: 8.074 | eve: 9.821 | bob: 8.028Epoch   4:  57% | abe: 8.072 | eve: 9.820 | bob: 8.026Epoch   4:  57% | abe: 8.070 | eve: 9.820 | bob: 8.024Epoch   4:  58% | abe: 8.068 | eve: 9.820 | bob: 8.022Epoch   4:  59% | abe: 8.067 | eve: 9.819 | bob: 8.020Epoch   4:  60% | abe: 8.064 | eve: 9.818 | bob: 8.017Epoch   4:  60% | abe: 8.062 | eve: 9.818 | bob: 8.015Epoch   4:  61% | abe: 8.060 | eve: 9.817 | bob: 8.014Epoch   4:  62% | abe: 8.059 | eve: 9.818 | bob: 8.012Epoch   4:  63% | abe: 8.057 | eve: 9.819 | bob: 8.010Epoch   4:  64% | abe: 8.055 | eve: 9.819 | bob: 8.008Epoch   4:  64% | abe: 8.053 | eve: 9.818 | bob: 8.006Epoch   4:  65% | abe: 8.051 | eve: 9.818 | bob: 8.004Epoch   4:  66% | abe: 8.049 | eve: 9.818 | bob: 8.002Epoch   4:  67% | abe: 8.048 | eve: 9.818 | bob: 8.000Epoch   4:  67% | abe: 8.046 | eve: 9.818 | bob: 7.998Epoch   4:  68% | abe: 8.044 | eve: 9.817 | bob: 7.997Epoch   4:  69% | abe: 8.043 | eve: 9.817 | bob: 7.995Epoch   4:  70% | abe: 8.040 | eve: 9.818 | bob: 7.992Epoch   4:  71% | abe: 8.038 | eve: 9.818 | bob: 7.990Epoch   4:  71% | abe: 8.036 | eve: 9.818 | bob: 7.988Epoch   4:  72% | abe: 8.034 | eve: 9.818 | bob: 7.986Epoch   4:  73% | abe: 8.032 | eve: 9.819 | bob: 7.984Epoch   4:  74% | abe: 8.030 | eve: 9.819 | bob: 7.982Epoch   4:  75% | abe: 8.027 | eve: 9.819 | bob: 7.980Epoch   4:  75% | abe: 8.025 | eve: 9.820 | bob: 7.977Epoch   4:  76% | abe: 8.023 | eve: 9.820 | bob: 7.975Epoch   4:  77% | abe: 8.021 | eve: 9.820 | bob: 7.973Epoch   4:  78% | abe: 8.019 | eve: 9.820 | bob: 7.971Epoch   4:  78% | abe: 8.017 | eve: 9.820 | bob: 7.969Epoch   4:  79% | abe: 8.015 | eve: 9.820 | bob: 7.967Epoch   4:  80% | abe: 8.013 | eve: 9.821 | bob: 7.965Epoch   4:  81% | abe: 8.012 | eve: 9.821 | bob: 7.963Epoch   4:  82% | abe: 8.010 | eve: 9.821 | bob: 7.962Epoch   4:  82% | abe: 8.008 | eve: 9.821 | bob: 7.960Epoch   4:  83% | abe: 8.006 | eve: 9.821 | bob: 7.958Epoch   4:  84% | abe: 8.004 | eve: 9.820 | bob: 7.956Epoch   4:  85% | abe: 8.002 | eve: 9.821 | bob: 7.953Epoch   4:  85% | abe: 8.000 | eve: 9.821 | bob: 7.951Epoch   4:  86% | abe: 7.998 | eve: 9.820 | bob: 7.949Epoch   4:  87% | abe: 7.996 | eve: 9.820 | bob: 7.947Epoch   4:  88% | abe: 7.994 | eve: 9.819 | bob: 7.945Epoch   4:  89% | abe: 7.992 | eve: 9.819 | bob: 7.943Epoch   4:  89% | abe: 7.990 | eve: 9.819 | bob: 7.941Epoch   4:  90% | abe: 7.987 | eve: 9.819 | bob: 7.939Epoch   4:  91% | abe: 7.985 | eve: 9.819 | bob: 7.936Epoch   4:  92% | abe: 7.983 | eve: 9.820 | bob: 7.934Epoch   4:  92% | abe: 7.981 | eve: 9.820 | bob: 7.932Epoch   4:  93% | abe: 7.979 | eve: 9.821 | bob: 7.930Epoch   4:  94% | abe: 7.977 | eve: 9.821 | bob: 7.928Epoch   4:  95% | abe: 7.975 | eve: 9.822 | bob: 7.926Epoch   4:  96% | abe: 7.973 | eve: 9.822 | bob: 7.923Epoch   4:  96% | abe: 7.971 | eve: 9.822 | bob: 7.921Epoch   4:  97% | abe: 7.969 | eve: 9.822 | bob: 7.919Epoch   4:  98% | abe: 7.967 | eve: 9.823 | bob: 7.918Epoch   4:  99% | abe: 7.965 | eve: 9.823 | bob: 7.915
New best Bob loss 7.915472496030134 at epoch 4
Epoch   5:   0% | abe: 7.694 | eve: 9.910 | bob: 7.640Epoch   5:   0% | abe: 7.698 | eve: 9.884 | bob: 7.641Epoch   5:   1% | abe: 7.693 | eve: 9.876 | bob: 7.636Epoch   5:   2% | abe: 7.691 | eve: 9.863 | bob: 7.635Epoch   5:   3% | abe: 7.687 | eve: 9.868 | bob: 7.632Epoch   5:   3% | abe: 7.686 | eve: 9.874 | bob: 7.631Epoch   5:   4% | abe: 7.687 | eve: 9.863 | bob: 7.633Epoch   5:   5% | abe: 7.678 | eve: 9.865 | bob: 7.622Epoch   5:   6% | abe: 7.673 | eve: 9.865 | bob: 7.617Epoch   5:   7% | abe: 7.668 | eve: 9.856 | bob: 7.610Epoch   5:   7% | abe: 7.665 | eve: 9.860 | bob: 7.607Epoch   5:   8% | abe: 7.662 | eve: 9.860 | bob: 7.604Epoch   5:   9% | abe: 7.660 | eve: 9.862 | bob: 7.603Epoch   5:  10% | abe: 7.657 | eve: 9.862 | bob: 7.599Epoch   5:  10% | abe: 7.656 | eve: 9.859 | bob: 7.598Epoch   5:  11% | abe: 7.657 | eve: 9.856 | bob: 7.601Epoch   5:  12% | abe: 7.654 | eve: 9.862 | bob: 7.598Epoch   5:  13% | abe: 7.652 | eve: 9.860 | bob: 7.596Epoch   5:  14% | abe: 7.647 | eve: 9.859 | bob: 7.592Epoch   5:  14% | abe: 7.647 | eve: 9.859 | bob: 7.592Epoch   5:  15% | abe: 7.647 | eve: 9.859 | bob: 7.591Epoch   5:  16% | abe: 7.644 | eve: 9.860 | bob: 7.587Epoch   5:  17% | abe: 7.643 | eve: 9.859 | bob: 7.586Epoch   5:  17% | abe: 7.640 | eve: 9.860 | bob: 7.584Epoch   5:  18% | abe: 7.638 | eve: 9.861 | bob: 7.582Epoch   5:  19% | abe: 7.635 | eve: 9.859 | bob: 7.579Epoch   5:  20% | abe: 7.631 | eve: 9.859 | bob: 7.575Epoch   5:  21% | abe: 7.629 | eve: 9.859 | bob: 7.573Epoch   5:  21% | abe: 7.626 | eve: 9.858 | bob: 7.570Epoch   5:  22% | abe: 7.625 | eve: 9.858 | bob: 7.568Epoch   5:  23% | abe: 7.622 | eve: 9.857 | bob: 7.565Epoch   5:  24% | abe: 7.619 | eve: 9.856 | bob: 7.563Epoch   5:  25% | abe: 7.616 | eve: 9.856 | bob: 7.561Epoch   5:  25% | abe: 7.614 | eve: 9.855 | bob: 7.558Epoch   5:  26% | abe: 7.611 | eve: 9.854 | bob: 7.555Epoch   5:  27% | abe: 7.609 | eve: 9.855 | bob: 7.554Epoch   5:  28% | abe: 7.607 | eve: 9.855 | bob: 7.552Epoch   5:  28% | abe: 7.605 | eve: 9.854 | bob: 7.549Epoch   5:  29% | abe: 7.602 | eve: 9.855 | bob: 7.546Epoch   5:  30% | abe: 7.599 | eve: 9.855 | bob: 7.544Epoch   5:  31% | abe: 7.596 | eve: 9.853 | bob: 7.541Epoch   5:  32% | abe: 7.594 | eve: 9.854 | bob: 7.539Epoch   5:  32% | abe: 7.592 | eve: 9.855 | bob: 7.537Epoch   5:  33% | abe: 7.589 | eve: 9.855 | bob: 7.534Epoch   5:  34% | abe: 7.587 | eve: 9.854 | bob: 7.532Epoch   5:  35% | abe: 7.584 | eve: 9.855 | bob: 7.529Epoch   5:  35% | abe: 7.582 | eve: 9.854 | bob: 7.527Epoch   5:  36% | abe: 7.579 | eve: 9.854 | bob: 7.524Epoch   5:  37% | abe: 7.577 | eve: 9.854 | bob: 7.521Epoch   5:  38% | abe: 7.574 | eve: 9.852 | bob: 7.519Epoch   5:  39% | abe: 7.572 | eve: 9.854 | bob: 7.517Epoch   5:  39% | abe: 7.569 | eve: 9.854 | bob: 7.514Epoch   5:  40% | abe: 7.566 | eve: 9.854 | bob: 7.511Epoch   5:  41% | abe: 7.565 | eve: 9.854 | bob: 7.510Epoch   5:  42% | abe: 7.562 | eve: 9.854 | bob: 7.507Epoch   5:  42% | abe: 7.559 | eve: 9.853 | bob: 7.504Epoch   5:  43% | abe: 7.557 | eve: 9.852 | bob: 7.502Epoch   5:  44% | abe: 7.555 | eve: 9.853 | bob: 7.500Epoch   5:  45% | abe: 7.552 | eve: 9.853 | bob: 7.497Epoch   5:  46% | abe: 7.550 | eve: 9.854 | bob: 7.495Epoch   5:  46% | abe: 7.548 | eve: 9.854 | bob: 7.494Epoch   5:  47% | abe: 7.546 | eve: 9.853 | bob: 7.491Epoch   5:  48% | abe: 7.543 | eve: 9.854 | bob: 7.488Epoch   5:  49% | abe: 7.540 | eve: 9.855 | bob: 7.486Epoch   5:  50% | abe: 7.538 | eve: 9.854 | bob: 7.483Epoch   5:  50% | abe: 7.535 | eve: 9.853 | bob: 7.481Epoch   5:  51% | abe: 7.532 | eve: 9.853 | bob: 7.478Epoch   5:  52% | abe: 7.530 | eve: 9.852 | bob: 7.475Epoch   5:  53% | abe: 7.527 | eve: 9.852 | bob: 7.473Epoch   5:  53% | abe: 7.525 | eve: 9.853 | bob: 7.470Epoch   5:  54% | abe: 7.522 | eve: 9.853 | bob: 7.467Epoch   5:  55% | abe: 7.519 | eve: 9.855 | bob: 7.465Epoch   5:  56% | abe: 7.516 | eve: 9.854 | bob: 7.462Epoch   5:  57% | abe: 7.513 | eve: 9.855 | bob: 7.459Epoch   5:  57% | abe: 7.511 | eve: 9.855 | bob: 7.456Epoch   5:  58% | abe: 7.508 | eve: 9.855 | bob: 7.454Epoch   5:  59% | abe: 7.506 | eve: 9.854 | bob: 7.451Epoch   5:  60% | abe: 7.503 | eve: 9.854 | bob: 7.448Epoch   5:  60% | abe: 7.500 | eve: 9.854 | bob: 7.446Epoch   5:  61% | abe: 7.497 | eve: 9.854 | bob: 7.443Epoch   5:  62% | abe: 7.495 | eve: 9.855 | bob: 7.440Epoch   5:  63% | abe: 7.492 | eve: 9.854 | bob: 7.438Epoch   5:  64% | abe: 7.489 | eve: 9.854 | bob: 7.435Epoch   5:  64% | abe: 7.487 | eve: 9.854 | bob: 7.433Epoch   5:  65% | abe: 7.484 | eve: 9.854 | bob: 7.430Epoch   5:  66% | abe: 7.482 | eve: 9.855 | bob: 7.428Epoch   5:  67% | abe: 7.479 | eve: 9.854 | bob: 7.425Epoch   5:  67% | abe: 7.476 | eve: 9.854 | bob: 7.422Epoch   5:  68% | abe: 7.473 | eve: 9.854 | bob: 7.419Epoch   5:  69% | abe: 7.470 | eve: 9.854 | bob: 7.416Epoch   5:  70% | abe: 7.467 | eve: 9.853 | bob: 7.413Epoch   5:  71% | abe: 7.465 | eve: 9.853 | bob: 7.411Epoch   5:  71% | abe: 7.462 | eve: 9.854 | bob: 7.408Epoch   5:  72% | abe: 7.460 | eve: 9.853 | bob: 7.406Epoch   5:  73% | abe: 7.457 | eve: 9.854 | bob: 7.403Epoch   5:  74% | abe: 7.454 | eve: 9.854 | bob: 7.400Epoch   5:  75% | abe: 7.451 | eve: 9.854 | bob: 7.397Epoch   5:  75% | abe: 7.448 | eve: 9.854 | bob: 7.394Epoch   5:  76% | abe: 7.445 | eve: 9.854 | bob: 7.391Epoch   5:  77% | abe: 7.443 | eve: 9.854 | bob: 7.388Epoch   5:  78% | abe: 7.440 | eve: 9.854 | bob: 7.386Epoch   5:  78% | abe: 7.437 | eve: 9.855 | bob: 7.383Epoch   5:  79% | abe: 7.435 | eve: 9.855 | bob: 7.381Epoch   5:  80% | abe: 7.432 | eve: 9.854 | bob: 7.378Epoch   5:  81% | abe: 7.429 | eve: 9.855 | bob: 7.376Epoch   5:  82% | abe: 7.427 | eve: 9.855 | bob: 7.373Epoch   5:  82% | abe: 7.424 | eve: 9.855 | bob: 7.370Epoch   5:  83% | abe: 7.421 | eve: 9.855 | bob: 7.368Epoch   5:  84% | abe: 7.419 | eve: 9.855 | bob: 7.365Epoch   5:  85% | abe: 7.416 | eve: 9.855 | bob: 7.362Epoch   5:  85% | abe: 7.413 | eve: 9.856 | bob: 7.360Epoch   5:  86% | abe: 7.411 | eve: 9.855 | bob: 7.357Epoch   5:  87% | abe: 7.408 | eve: 9.855 | bob: 7.355Epoch   5:  88% | abe: 7.405 | eve: 9.855 | bob: 7.352Epoch   5:  89% | abe: 7.403 | eve: 9.856 | bob: 7.350Epoch   5:  89% | abe: 7.401 | eve: 9.856 | bob: 7.347Epoch   5:  90% | abe: 7.398 | eve: 9.856 | bob: 7.345Epoch   5:  91% | abe: 7.395 | eve: 9.856 | bob: 7.342Epoch   5:  92% | abe: 7.392 | eve: 9.856 | bob: 7.339Epoch   5:  92% | abe: 7.389 | eve: 9.856 | bob: 7.336Epoch   5:  93% | abe: 7.387 | eve: 9.856 | bob: 7.334Epoch   5:  94% | abe: 7.384 | eve: 9.856 | bob: 7.331Epoch   5:  95% | abe: 7.381 | eve: 9.856 | bob: 7.328Epoch   5:  96% | abe: 7.379 | eve: 9.856 | bob: 7.326Epoch   5:  96% | abe: 7.376 | eve: 9.857 | bob: 7.323Epoch   5:  97% | abe: 7.373 | eve: 9.857 | bob: 7.320Epoch   5:  98% | abe: 7.370 | eve: 9.857 | bob: 7.318Epoch   5:  99% | abe: 7.368 | eve: 9.857 | bob: 7.315
New best Bob loss 7.314936687421778 at epoch 5
Epoch   6:   0% | abe: 7.015 | eve: 9.885 | bob: 6.978Epoch   6:   0% | abe: 7.006 | eve: 9.861 | bob: 6.965Epoch   6:   1% | abe: 7.007 | eve: 9.884 | bob: 6.963Epoch   6:   2% | abe: 7.006 | eve: 9.864 | bob: 6.961Epoch   6:   3% | abe: 7.004 | eve: 9.859 | bob: 6.958Epoch   6:   3% | abe: 6.999 | eve: 9.856 | bob: 6.954Epoch   6:   4% | abe: 6.999 | eve: 9.870 | bob: 6.954Epoch   6:   5% | abe: 6.997 | eve: 9.865 | bob: 6.953Epoch   6:   6% | abe: 6.992 | eve: 9.867 | bob: 6.948Epoch   6:   7% | abe: 6.990 | eve: 9.868 | bob: 6.947Epoch   6:   7% | abe: 6.987 | eve: 9.874 | bob: 6.942Epoch   6:   8% | abe: 6.985 | eve: 9.871 | bob: 6.941Epoch   6:   9% | abe: 6.981 | eve: 9.873 | bob: 6.938Epoch   6:  10% | abe: 6.980 | eve: 9.870 | bob: 6.937Epoch   6:  10% | abe: 6.977 | eve: 9.870 | bob: 6.935Epoch   6:  11% | abe: 6.976 | eve: 9.868 | bob: 6.934Epoch   6:  12% | abe: 6.973 | eve: 9.867 | bob: 6.932Epoch   6:  13% | abe: 6.971 | eve: 9.868 | bob: 6.930Epoch   6:  14% | abe: 6.967 | eve: 9.861 | bob: 6.926Epoch   6:  14% | abe: 6.963 | eve: 9.863 | bob: 6.922Epoch   6:  15% | abe: 6.960 | eve: 9.865 | bob: 6.919Epoch   6:  16% | abe: 6.958 | eve: 9.868 | bob: 6.917Epoch   6:  17% | abe: 6.955 | eve: 9.868 | bob: 6.915Epoch   6:  17% | abe: 6.952 | eve: 9.864 | bob: 6.911Epoch   6:  18% | abe: 6.950 | eve: 9.863 | bob: 6.910Epoch   6:  19% | abe: 6.946 | eve: 9.868 | bob: 6.906Epoch   6:  20% | abe: 6.945 | eve: 9.873 | bob: 6.905Epoch   6:  21% | abe: 6.941 | eve: 9.873 | bob: 6.902Epoch   6:  21% | abe: 6.938 | eve: 9.871 | bob: 6.899Epoch   6:  22% | abe: 6.935 | eve: 9.871 | bob: 6.896Epoch   6:  23% | abe: 6.932 | eve: 9.872 | bob: 6.892Epoch   6:  24% | abe: 6.929 | eve: 9.871 | bob: 6.889Epoch   6:  25% | abe: 6.925 | eve: 9.873 | bob: 6.886Epoch   6:  25% | abe: 6.922 | eve: 9.873 | bob: 6.884Epoch   6:  26% | abe: 6.919 | eve: 9.873 | bob: 6.880Epoch   6:  27% | abe: 6.916 | eve: 9.872 | bob: 6.877Epoch   6:  28% | abe: 6.914 | eve: 9.872 | bob: 6.875Epoch   6:  28% | abe: 6.911 | eve: 9.872 | bob: 6.872Epoch   6:  29% | abe: 6.909 | eve: 9.871 | bob: 6.870Epoch   6:  30% | abe: 6.906 | eve: 9.871 | bob: 6.868Epoch   6:  31% | abe: 6.903 | eve: 9.872 | bob: 6.865Epoch   6:  32% | abe: 6.900 | eve: 9.872 | bob: 6.862Epoch   6:  32% | abe: 6.897 | eve: 9.874 | bob: 6.859Epoch   6:  33% | abe: 6.894 | eve: 9.873 | bob: 6.855Epoch   6:  34% | abe: 6.891 | eve: 9.873 | bob: 6.853Epoch   6:  35% | abe: 6.888 | eve: 9.872 | bob: 6.850Epoch   6:  35% | abe: 6.886 | eve: 9.873 | bob: 6.849Epoch   6:  36% | abe: 6.883 | eve: 9.872 | bob: 6.846Epoch   6:  37% | abe: 6.881 | eve: 9.872 | bob: 6.844Epoch   6:  38% | abe: 6.879 | eve: 9.873 | bob: 6.842Epoch   6:  39% | abe: 6.876 | eve: 9.873 | bob: 6.839Epoch   6:  39% | abe: 6.873 | eve: 9.873 | bob: 6.836Epoch   6:  40% | abe: 6.871 | eve: 9.872 | bob: 6.834Epoch   6:  41% | abe: 6.868 | eve: 9.872 | bob: 6.832Epoch   6:  42% | abe: 6.865 | eve: 9.872 | bob: 6.829Epoch   6:  42% | abe: 6.863 | eve: 9.872 | bob: 6.826Epoch   6:  43% | abe: 6.860 | eve: 9.873 | bob: 6.824Epoch   6:  44% | abe: 6.857 | eve: 9.873 | bob: 6.821Epoch   6:  45% | abe: 6.855 | eve: 9.873 | bob: 6.819Epoch   6:  46% | abe: 6.852 | eve: 9.871 | bob: 6.816Epoch   6:  46% | abe: 6.850 | eve: 9.872 | bob: 6.814Epoch   6:  47% | abe: 6.847 | eve: 9.872 | bob: 6.811Epoch   6:  48% | abe: 6.844 | eve: 9.872 | bob: 6.809Epoch   6:  49% | abe: 6.842 | eve: 9.872 | bob: 6.806Epoch   6:  50% | abe: 6.839 | eve: 9.872 | bob: 6.803Epoch   6:  50% | abe: 6.836 | eve: 9.872 | bob: 6.801Epoch   6:  51% | abe: 6.834 | eve: 9.872 | bob: 6.798Epoch   6:  52% | abe: 6.831 | eve: 9.872 | bob: 6.796Epoch   6:  53% | abe: 6.828 | eve: 9.873 | bob: 6.793Epoch   6:  53% | abe: 6.826 | eve: 9.872 | bob: 6.791Epoch   6:  54% | abe: 6.823 | eve: 9.871 | bob: 6.789Epoch   6:  55% | abe: 6.821 | eve: 9.870 | bob: 6.786Epoch   6:  56% | abe: 6.817 | eve: 9.870 | bob: 6.783Epoch   6:  57% | abe: 6.815 | eve: 9.869 | bob: 6.780Epoch   6:  57% | abe: 6.812 | eve: 9.868 | bob: 6.777Epoch   6:  58% | abe: 6.809 | eve: 9.867 | bob: 6.775Epoch   6:  59% | abe: 6.807 | eve: 9.867 | bob: 6.773Epoch   6:  60% | abe: 6.804 | eve: 9.866 | bob: 6.770Epoch   6:  60% | abe: 6.801 | eve: 9.866 | bob: 6.767Epoch   6:  61% | abe: 6.798 | eve: 9.865 | bob: 6.764Epoch   6:  62% | abe: 6.795 | eve: 9.864 | bob: 6.762Epoch   6:  63% | abe: 6.793 | eve: 9.865 | bob: 6.759Epoch   6:  64% | abe: 6.790 | eve: 9.865 | bob: 6.757Epoch   6:  64% | abe: 6.787 | eve: 9.864 | bob: 6.754Epoch   6:  65% | abe: 6.784 | eve: 9.865 | bob: 6.751Epoch   6:  66% | abe: 6.782 | eve: 9.865 | bob: 6.749Epoch   6:  67% | abe: 6.779 | eve: 9.865 | bob: 6.746Epoch   6:  67% | abe: 6.777 | eve: 9.864 | bob: 6.744Epoch   6:  68% | abe: 6.774 | eve: 9.864 | bob: 6.741Epoch   6:  69% | abe: 6.771 | eve: 9.864 | bob: 6.739Epoch   6:  70% | abe: 6.768 | eve: 9.863 | bob: 6.736Epoch   6:  71% | abe: 6.766 | eve: 9.863 | bob: 6.734Epoch   6:  71% | abe: 6.763 | eve: 9.862 | bob: 6.731Epoch   6:  72% | abe: 6.760 | eve: 9.863 | bob: 6.729Epoch   6:  73% | abe: 6.758 | eve: 9.862 | bob: 6.726Epoch   6:  74% | abe: 6.755 | eve: 9.863 | bob: 6.724Epoch   6:  75% | abe: 6.752 | eve: 9.863 | bob: 6.721Epoch   6:  75% | abe: 6.749 | eve: 9.863 | bob: 6.718Epoch   6:  76% | abe: 6.747 | eve: 9.863 | bob: 6.716Epoch   6:  77% | abe: 6.744 | eve: 9.863 | bob: 6.713Epoch   6:  78% | abe: 6.741 | eve: 9.863 | bob: 6.710Epoch   6:  78% | abe: 6.738 | eve: 9.863 | bob: 6.708Epoch   6:  79% | abe: 6.736 | eve: 9.864 | bob: 6.705Epoch   6:  80% | abe: 6.733 | eve: 9.863 | bob: 6.703Epoch   6:  81% | abe: 6.730 | eve: 9.863 | bob: 6.700Epoch   6:  82% | abe: 6.727 | eve: 9.863 | bob: 6.697Epoch   6:  82% | abe: 6.724 | eve: 9.863 | bob: 6.695Epoch   6:  83% | abe: 6.722 | eve: 9.863 | bob: 6.692Epoch   6:  84% | abe: 6.719 | eve: 9.862 | bob: 6.690Epoch   6:  85% | abe: 6.716 | eve: 9.863 | bob: 6.688Epoch   6:  85% | abe: 6.714 | eve: 9.863 | bob: 6.685Epoch   6:  86% | abe: 6.711 | eve: 9.862 | bob: 6.683Epoch   6:  87% | abe: 6.708 | eve: 9.862 | bob: 6.680Epoch   6:  88% | abe: 6.706 | eve: 9.861 | bob: 6.678Epoch   6:  89% | abe: 6.703 | eve: 9.861 | bob: 6.675Epoch   6:  89% | abe: 6.700 | eve: 9.860 | bob: 6.673Epoch   6:  90% | abe: 6.698 | eve: 9.861 | bob: 6.670Epoch   6:  91% | abe: 6.695 | eve: 9.860 | bob: 6.667Epoch   6:  92% | abe: 6.692 | eve: 9.860 | bob: 6.665Epoch   6:  92% | abe: 6.689 | eve: 9.860 | bob: 6.662Epoch   6:  93% | abe: 6.687 | eve: 9.859 | bob: 6.660Epoch   6:  94% | abe: 6.684 | eve: 9.859 | bob: 6.657Epoch   6:  95% | abe: 6.681 | eve: 9.860 | bob: 6.655Epoch   6:  96% | abe: 6.678 | eve: 9.860 | bob: 6.652Epoch   6:  96% | abe: 6.676 | eve: 9.860 | bob: 6.650Epoch   6:  97% | abe: 6.673 | eve: 9.859 | bob: 6.647Epoch   6:  98% | abe: 6.670 | eve: 9.860 | bob: 6.645Epoch   6:  99% | abe: 6.668 | eve: 9.859 | bob: 6.642
New best Bob loss 6.642291404909884 at epoch 6
Epoch   7:   0% | abe: 6.349 | eve: 9.868 | bob: 6.363Epoch   7:   0% | abe: 6.324 | eve: 9.867 | bob: 6.332Epoch   7:   1% | abe: 6.319 | eve: 9.864 | bob: 6.323Epoch   7:   2% | abe: 6.319 | eve: 9.840 | bob: 6.324Epoch   7:   3% | abe: 6.313 | eve: 9.818 | bob: 6.317Epoch   7:   3% | abe: 6.309 | eve: 9.823 | bob: 6.315Epoch   7:   4% | abe: 6.309 | eve: 9.824 | bob: 6.315Epoch   7:   5% | abe: 6.304 | eve: 9.821 | bob: 6.309Epoch   7:   6% | abe: 6.303 | eve: 9.831 | bob: 6.309Epoch   7:   7% | abe: 6.302 | eve: 9.837 | bob: 6.308Epoch   7:   7% | abe: 6.299 | eve: 9.840 | bob: 6.305Epoch   7:   8% | abe: 6.297 | eve: 9.837 | bob: 6.302Epoch   7:   9% | abe: 6.295 | eve: 9.835 | bob: 6.301Epoch   7:  10% | abe: 6.292 | eve: 9.837 | bob: 6.298Epoch   7:  10% | abe: 6.289 | eve: 9.841 | bob: 6.297Epoch   7:  11% | abe: 6.285 | eve: 9.843 | bob: 6.292Epoch   7:  12% | abe: 6.284 | eve: 9.845 | bob: 6.291Epoch   7:  13% | abe: 6.283 | eve: 9.844 | bob: 6.291Epoch   7:  14% | abe: 6.279 | eve: 9.846 | bob: 6.289Epoch   7:  14% | abe: 6.277 | eve: 9.849 | bob: 6.286Epoch   7:  15% | abe: 6.274 | eve: 9.848 | bob: 6.283Epoch   7:  16% | abe: 6.271 | eve: 9.846 | bob: 6.280Epoch   7:  17% | abe: 6.269 | eve: 9.848 | bob: 6.279Epoch   7:  17% | abe: 6.267 | eve: 9.852 | bob: 6.277Epoch   7:  18% | abe: 6.264 | eve: 9.855 | bob: 6.275Epoch   7:  19% | abe: 6.261 | eve: 9.859 | bob: 6.271Epoch   7:  20% | abe: 6.258 | eve: 9.859 | bob: 6.269Epoch   7:  21% | abe: 6.256 | eve: 9.859 | bob: 6.266Epoch   7:  21% | abe: 6.252 | eve: 9.861 | bob: 6.262Epoch   7:  22% | abe: 6.250 | eve: 9.860 | bob: 6.261Epoch   7:  23% | abe: 6.248 | eve: 9.860 | bob: 6.259Epoch   7:  24% | abe: 6.246 | eve: 9.858 | bob: 6.257Epoch   7:  25% | abe: 6.244 | eve: 9.858 | bob: 6.255Epoch   7:  25% | abe: 6.241 | eve: 9.860 | bob: 6.253Epoch   7:  26% | abe: 6.239 | eve: 9.861 | bob: 6.250Epoch   7:  27% | abe: 6.237 | eve: 9.860 | bob: 6.248Epoch   7:  28% | abe: 6.234 | eve: 9.858 | bob: 6.245Epoch   7:  28% | abe: 6.231 | eve: 9.859 | bob: 6.243Epoch   7:  29% | abe: 6.228 | eve: 9.860 | bob: 6.240Epoch   7:  30% | abe: 6.225 | eve: 9.859 | bob: 6.237Epoch   7:  31% | abe: 6.223 | eve: 9.860 | bob: 6.235Epoch   7:  32% | abe: 6.220 | eve: 9.859 | bob: 6.233Epoch   7:  32% | abe: 6.218 | eve: 9.859 | bob: 6.230Epoch   7:  33% | abe: 6.216 | eve: 9.859 | bob: 6.228Epoch   7:  34% | abe: 6.213 | eve: 9.858 | bob: 6.226Epoch   7:  35% | abe: 6.211 | eve: 9.856 | bob: 6.224Epoch   7:  35% | abe: 6.208 | eve: 9.857 | bob: 6.222Epoch   7:  36% | abe: 6.206 | eve: 9.857 | bob: 6.219Epoch   7:  37% | abe: 6.204 | eve: 9.859 | bob: 6.217Epoch   7:  38% | abe: 6.201 | eve: 9.858 | bob: 6.215Epoch   7:  39% | abe: 6.199 | eve: 9.856 | bob: 6.213Epoch   7:  39% | abe: 6.197 | eve: 9.855 | bob: 6.211Epoch   7:  40% | abe: 6.195 | eve: 9.853 | bob: 6.209Epoch   7:  41% | abe: 6.192 | eve: 9.854 | bob: 6.207Epoch   7:  42% | abe: 6.190 | eve: 9.854 | bob: 6.204Epoch   7:  42% | abe: 6.187 | eve: 9.853 | bob: 6.202Epoch   7:  43% | abe: 6.185 | eve: 9.854 | bob: 6.200Epoch   7:  44% | abe: 6.183 | eve: 9.854 | bob: 6.198Epoch   7:  45% | abe: 6.180 | eve: 9.856 | bob: 6.196Epoch   7:  46% | abe: 6.178 | eve: 9.857 | bob: 6.193Epoch   7:  46% | abe: 6.175 | eve: 9.857 | bob: 6.190Epoch   7:  47% | abe: 6.173 | eve: 9.856 | bob: 6.188Epoch   7:  48% | abe: 6.170 | eve: 9.856 | bob: 6.186Epoch   7:  49% | abe: 6.168 | eve: 9.855 | bob: 6.184Epoch   7:  50% | abe: 6.166 | eve: 9.857 | bob: 6.181Epoch   7:  50% | abe: 6.163 | eve: 9.858 | bob: 6.179Epoch   7:  51% | abe: 6.161 | eve: 9.857 | bob: 6.177Epoch   7:  52% | abe: 6.159 | eve: 9.857 | bob: 6.175Epoch   7:  53% | abe: 6.157 | eve: 9.857 | bob: 6.173Epoch   7:  53% | abe: 6.154 | eve: 9.858 | bob: 6.170Epoch   7:  54% | abe: 6.152 | eve: 9.858 | bob: 6.168Epoch   7:  55% | abe: 6.150 | eve: 9.859 | bob: 6.166Epoch   7:  56% | abe: 6.148 | eve: 9.859 | bob: 6.164Epoch   7:  57% | abe: 6.145 | eve: 9.859 | bob: 6.162Epoch   7:  57% | abe: 6.143 | eve: 9.859 | bob: 6.160Epoch   7:  58% | abe: 6.141 | eve: 9.859 | bob: 6.158Epoch   7:  59% | abe: 6.138 | eve: 9.860 | bob: 6.155Epoch   7:  60% | abe: 6.136 | eve: 9.861 | bob: 6.153Epoch   7:  60% | abe: 6.134 | eve: 9.861 | bob: 6.151Epoch   7:  61% | abe: 6.131 | eve: 9.861 | bob: 6.148Epoch   7:  62% | abe: 6.129 | eve: 9.862 | bob: 6.146Epoch   7:  63% | abe: 6.126 | eve: 9.862 | bob: 6.144Epoch   7:  64% | abe: 6.124 | eve: 9.862 | bob: 6.142Epoch   7:  64% | abe: 6.122 | eve: 9.861 | bob: 6.139Epoch   7:  65% | abe: 6.119 | eve: 9.862 | bob: 6.137Epoch   7:  66% | abe: 6.117 | eve: 9.861 | bob: 6.135Epoch   7:  67% | abe: 6.114 | eve: 9.862 | bob: 6.132Epoch   7:  67% | abe: 6.112 | eve: 9.862 | bob: 6.130Epoch   7:  68% | abe: 6.110 | eve: 9.862 | bob: 6.128Epoch   7:  69% | abe: 6.107 | eve: 9.862 | bob: 6.126Epoch   7:  70% | abe: 6.105 | eve: 9.862 | bob: 6.123Epoch   7:  71% | abe: 6.103 | eve: 9.863 | bob: 6.121Epoch   7:  71% | abe: 6.100 | eve: 9.862 | bob: 6.119Epoch   7:  72% | abe: 6.098 | eve: 9.862 | bob: 6.117Epoch   7:  73% | abe: 6.096 | eve: 9.862 | bob: 6.115Epoch   7:  74% | abe: 6.094 | eve: 9.862 | bob: 6.113Epoch   7:  75% | abe: 6.091 | eve: 9.862 | bob: 6.110Epoch   7:  75% | abe: 6.089 | eve: 9.862 | bob: 6.108Epoch   7:  76% | abe: 6.086 | eve: 9.862 | bob: 6.106Epoch   7:  77% | abe: 6.084 | eve: 9.863 | bob: 6.104Epoch   7:  78% | abe: 6.082 | eve: 9.863 | bob: 6.102Epoch   7:  78% | abe: 6.080 | eve: 9.863 | bob: 6.100Epoch   7:  79% | abe: 6.078 | eve: 9.864 | bob: 6.097Epoch   7:  80% | abe: 6.075 | eve: 9.864 | bob: 6.095Epoch   7:  81% | abe: 6.073 | eve: 9.865 | bob: 6.093Epoch   7:  82% | abe: 6.071 | eve: 9.865 | bob: 6.091Epoch   7:  82% | abe: 6.069 | eve: 9.865 | bob: 6.089Epoch   7:  83% | abe: 6.066 | eve: 9.864 | bob: 6.087Epoch   7:  84% | abe: 6.064 | eve: 9.864 | bob: 6.085Epoch   7:  85% | abe: 6.062 | eve: 9.864 | bob: 6.083Epoch   7:  85% | abe: 6.059 | eve: 9.864 | bob: 6.080Epoch   7:  86% | abe: 6.057 | eve: 9.864 | bob: 6.078Epoch   7:  87% | abe: 6.055 | eve: 9.865 | bob: 6.076Epoch   7:  88% | abe: 6.053 | eve: 9.865 | bob: 6.074Epoch   7:  89% | abe: 6.051 | eve: 9.865 | bob: 6.072Epoch   7:  89% | abe: 6.048 | eve: 9.865 | bob: 6.070Epoch   7:  90% | abe: 6.046 | eve: 9.866 | bob: 6.068Epoch   7:  91% | abe: 6.044 | eve: 9.865 | bob: 6.066Epoch   7:  92% | abe: 6.041 | eve: 9.866 | bob: 6.063Epoch   7:  92% | abe: 6.039 | eve: 9.865 | bob: 6.061Epoch   7:  93% | abe: 6.037 | eve: 9.865 | bob: 6.059Epoch   7:  94% | abe: 6.035 | eve: 9.865 | bob: 6.057Epoch   7:  95% | abe: 6.033 | eve: 9.865 | bob: 6.055Epoch   7:  96% | abe: 6.030 | eve: 9.865 | bob: 6.053Epoch   7:  96% | abe: 6.028 | eve: 9.865 | bob: 6.051Epoch   7:  97% | abe: 6.026 | eve: 9.865 | bob: 6.049Epoch   7:  98% | abe: 6.024 | eve: 9.865 | bob: 6.047Epoch   7:  99% | abe: 6.022 | eve: 9.866 | bob: 6.045
New best Bob loss 6.0449191734348915 at epoch 7
Epoch   8:   0% | abe: 5.734 | eve: 9.867 | bob: 5.774Epoch   8:   0% | abe: 5.729 | eve: 9.875 | bob: 5.764Epoch   8:   1% | abe: 5.728 | eve: 9.862 | bob: 5.764Epoch   8:   2% | abe: 5.727 | eve: 9.848 | bob: 5.763Epoch   8:   3% | abe: 5.726 | eve: 9.863 | bob: 5.763Epoch   8:   3% | abe: 5.723 | eve: 9.868 | bob: 5.760Epoch   8:   4% | abe: 5.720 | eve: 9.876 | bob: 5.757Epoch   8:   5% | abe: 5.720 | eve: 9.883 | bob: 5.757Epoch   8:   6% | abe: 5.718 | eve: 9.886 | bob: 5.755Epoch   8:   7% | abe: 5.716 | eve: 9.895 | bob: 5.752Epoch   8:   7% | abe: 5.714 | eve: 9.893 | bob: 5.751Epoch   8:   8% | abe: 5.712 | eve: 9.885 | bob: 5.749Epoch   8:   9% | abe: 5.709 | eve: 9.887 | bob: 5.746Epoch   8:  10% | abe: 5.707 | eve: 9.885 | bob: 5.744Epoch   8:  10% | abe: 5.706 | eve: 9.881 | bob: 5.743Epoch   8:  11% | abe: 5.704 | eve: 9.887 | bob: 5.741Epoch   8:  12% | abe: 5.703 | eve: 9.890 | bob: 5.740Epoch   8:  13% | abe: 5.701 | eve: 9.888 | bob: 5.739Epoch   8:  14% | abe: 5.699 | eve: 9.888 | bob: 5.737Epoch   8:  14% | abe: 5.697 | eve: 9.889 | bob: 5.735Epoch   8:  15% | abe: 5.695 | eve: 9.883 | bob: 5.733Epoch   8:  16% | abe: 5.693 | eve: 9.883 | bob: 5.731Epoch   8:  17% | abe: 5.691 | eve: 9.884 | bob: 5.729Epoch   8:  17% | abe: 5.689 | eve: 9.882 | bob: 5.726Epoch   8:  18% | abe: 5.687 | eve: 9.881 | bob: 5.723Epoch   8:  19% | abe: 5.685 | eve: 9.879 | bob: 5.722Epoch   8:  20% | abe: 5.683 | eve: 9.876 | bob: 5.721Epoch   8:  21% | abe: 5.681 | eve: 9.876 | bob: 5.719Epoch   8:  21% | abe: 5.679 | eve: 9.875 | bob: 5.717Epoch   8:  22% | abe: 5.677 | eve: 9.875 | bob: 5.715Epoch   8:  23% | abe: 5.675 | eve: 9.875 | bob: 5.713Epoch   8:  24% | abe: 5.673 | eve: 9.876 | bob: 5.712Epoch   8:  25% | abe: 5.671 | eve: 9.877 | bob: 5.709Epoch   8:  25% | abe: 5.669 | eve: 9.878 | bob: 5.707Epoch   8:  26% | abe: 5.668 | eve: 9.876 | bob: 5.706Epoch   8:  27% | abe: 5.666 | eve: 9.875 | bob: 5.704Epoch   8:  28% | abe: 5.664 | eve: 9.875 | bob: 5.702Epoch   8:  28% | abe: 5.662 | eve: 9.876 | bob: 5.700Epoch   8:  29% | abe: 5.660 | eve: 9.877 | bob: 5.698Epoch   8:  30% | abe: 5.657 | eve: 9.876 | bob: 5.696Epoch   8:  31% | abe: 5.655 | eve: 9.875 | bob: 5.694Epoch   8:  32% | abe: 5.653 | eve: 9.874 | bob: 5.692Epoch   8:  32% | abe: 5.652 | eve: 9.873 | bob: 5.690Epoch   8:  33% | abe: 5.650 | eve: 9.873 | bob: 5.689Epoch   8:  34% | abe: 5.648 | eve: 9.871 | bob: 5.687Epoch   8:  35% | abe: 5.645 | eve: 9.870 | bob: 5.685Epoch   8:  35% | abe: 5.643 | eve: 9.869 | bob: 5.683Epoch   8:  36% | abe: 5.641 | eve: 9.871 | bob: 5.681Epoch   8:  37% | abe: 5.640 | eve: 9.871 | bob: 5.679Epoch   8:  38% | abe: 5.638 | eve: 9.871 | bob: 5.677Epoch   8:  39% | abe: 5.636 | eve: 9.871 | bob: 5.675Epoch   8:  39% | abe: 5.634 | eve: 9.871 | bob: 5.674Epoch   8:  40% | abe: 5.632 | eve: 9.872 | bob: 5.672Epoch   8:  41% | abe: 5.630 | eve: 9.871 | bob: 5.670Epoch   8:  42% | abe: 5.628 | eve: 9.871 | bob: 5.668Epoch   8:  42% | abe: 5.626 | eve: 9.871 | bob: 5.667Epoch   8:  43% | abe: 5.624 | eve: 9.870 | bob: 5.665Epoch   8:  44% | abe: 5.622 | eve: 9.871 | bob: 5.663Epoch   8:  45% | abe: 5.620 | eve: 9.871 | bob: 5.661Epoch   8:  46% | abe: 5.618 | eve: 9.871 | bob: 5.659Epoch   8:  46% | abe: 5.616 | eve: 9.870 | bob: 5.657Epoch   8:  47% | abe: 5.615 | eve: 9.870 | bob: 5.656Epoch   8:  48% | abe: 5.613 | eve: 9.870 | bob: 5.654Epoch   8:  49% | abe: 5.611 | eve: 9.869 | bob: 5.652Epoch   8:  50% | abe: 5.609 | eve: 9.869 | bob: 5.650Epoch   8:  50% | abe: 5.607 | eve: 9.869 | bob: 5.648Epoch   8:  51% | abe: 5.605 | eve: 9.871 | bob: 5.646Epoch   8:  52% | abe: 5.603 | eve: 9.870 | bob: 5.644Epoch   8:  53% | abe: 5.601 | eve: 9.870 | bob: 5.642Epoch   8:  53% | abe: 5.599 | eve: 9.870 | bob: 5.640Epoch   8:  54% | abe: 5.597 | eve: 9.869 | bob: 5.639Epoch   8:  55% | abe: 5.595 | eve: 9.869 | bob: 5.637Epoch   8:  56% | abe: 5.593 | eve: 9.869 | bob: 5.635Epoch   8:  57% | abe: 5.591 | eve: 9.869 | bob: 5.633Epoch   8:  57% | abe: 5.589 | eve: 9.869 | bob: 5.631Epoch   8:  58% | abe: 5.587 | eve: 9.869 | bob: 5.630Epoch   8:  59% | abe: 5.585 | eve: 9.868 | bob: 5.628Epoch   8:  60% | abe: 5.583 | eve: 9.869 | bob: 5.626Epoch   8:  60% | abe: 5.581 | eve: 9.869 | bob: 5.624Epoch   8:  61% | abe: 5.579 | eve: 9.870 | bob: 5.622Epoch   8:  62% | abe: 5.577 | eve: 9.868 | bob: 5.621Epoch   8:  63% | abe: 5.576 | eve: 9.869 | bob: 5.619Epoch   8:  64% | abe: 5.574 | eve: 9.868 | bob: 5.617Epoch   8:  64% | abe: 5.572 | eve: 9.869 | bob: 5.615Epoch   8:  65% | abe: 5.570 | eve: 9.869 | bob: 5.613Epoch   8:  66% | abe: 5.568 | eve: 9.869 | bob: 5.612Epoch   8:  67% | abe: 5.566 | eve: 9.868 | bob: 5.610Epoch   8:  67% | abe: 5.564 | eve: 9.869 | bob: 5.608Epoch   8:  68% | abe: 5.562 | eve: 9.869 | bob: 5.606Epoch   8:  69% | abe: 5.560 | eve: 9.869 | bob: 5.604Epoch   8:  70% | abe: 5.558 | eve: 9.869 | bob: 5.602Epoch   8:  71% | abe: 5.556 | eve: 9.869 | bob: 5.601Epoch   8:  71% | abe: 5.554 | eve: 9.869 | bob: 5.599Epoch   8:  72% | abe: 5.552 | eve: 9.870 | bob: 5.597Epoch   8:  73% | abe: 5.550 | eve: 9.870 | bob: 5.595Epoch   8:  74% | abe: 5.548 | eve: 9.871 | bob: 5.593Epoch   8:  75% | abe: 5.546 | eve: 9.870 | bob: 5.591Epoch   8:  75% | abe: 5.545 | eve: 9.869 | bob: 5.590Epoch   8:  76% | abe: 5.543 | eve: 9.868 | bob: 5.588Epoch   8:  77% | abe: 5.541 | eve: 9.868 | bob: 5.586Epoch   8:  78% | abe: 5.539 | eve: 9.868 | bob: 5.584Epoch   8:  78% | abe: 5.537 | eve: 9.868 | bob: 5.583Epoch   8:  79% | abe: 5.535 | eve: 9.868 | bob: 5.581Epoch   8:  80% | abe: 5.533 | eve: 9.868 | bob: 5.579Epoch   8:  81% | abe: 5.531 | eve: 9.867 | bob: 5.577Epoch   8:  82% | abe: 5.529 | eve: 9.868 | bob: 5.575Epoch   8:  82% | abe: 5.527 | eve: 9.868 | bob: 5.574Epoch   8:  83% | abe: 5.525 | eve: 9.868 | bob: 5.572Epoch   8:  84% | abe: 5.523 | eve: 9.868 | bob: 5.570Epoch   8:  85% | abe: 5.521 | eve: 9.868 | bob: 5.569Epoch   8:  85% | abe: 5.519 | eve: 9.868 | bob: 5.567Epoch   8:  86% | abe: 5.517 | eve: 9.868 | bob: 5.565Epoch   8:  87% | abe: 5.516 | eve: 9.868 | bob: 5.563Epoch   8:  88% | abe: 5.514 | eve: 9.868 | bob: 5.562Epoch   8:  89% | abe: 5.512 | eve: 9.868 | bob: 5.560Epoch   8:  89% | abe: 5.510 | eve: 9.868 | bob: 5.558Epoch   8:  90% | abe: 5.508 | eve: 9.868 | bob: 5.556Epoch   8:  91% | abe: 5.506 | eve: 9.867 | bob: 5.554Epoch   8:  92% | abe: 5.504 | eve: 9.867 | bob: 5.553Epoch   8:  92% | abe: 5.502 | eve: 9.866 | bob: 5.551Epoch   8:  93% | abe: 5.500 | eve: 9.866 | bob: 5.549Epoch   8:  94% | abe: 5.499 | eve: 9.866 | bob: 5.548Epoch   8:  95% | abe: 5.497 | eve: 9.866 | bob: 5.546Epoch   8:  96% | abe: 5.495 | eve: 9.866 | bob: 5.544Epoch   8:  96% | abe: 5.493 | eve: 9.866 | bob: 5.542Epoch   8:  97% | abe: 5.491 | eve: 9.866 | bob: 5.541Epoch   8:  98% | abe: 5.489 | eve: 9.866 | bob: 5.539Epoch   8:  99% | abe: 5.487 | eve: 9.866 | bob: 5.537
New best Bob loss 5.536985651161785 at epoch 8
Epoch   9:   0% | abe: 5.237 | eve: 9.847 | bob: 5.297Epoch   9:   0% | abe: 5.241 | eve: 9.843 | bob: 5.304Epoch   9:   1% | abe: 5.240 | eve: 9.843 | bob: 5.303Epoch   9:   2% | abe: 5.237 | eve: 9.862 | bob: 5.300Epoch   9:   3% | abe: 5.236 | eve: 9.877 | bob: 5.300Epoch   9:   3% | abe: 5.236 | eve: 9.886 | bob: 5.304Epoch   9:   4% | abe: 5.236 | eve: 9.880 | bob: 5.304Epoch   9:   5% | abe: 5.234 | eve: 9.872 | bob: 5.303Epoch   9:   6% | abe: 5.231 | eve: 9.868 | bob: 5.299Epoch   9:   7% | abe: 5.228 | eve: 9.863 | bob: 5.297Epoch   9:   7% | abe: 5.226 | eve: 9.863 | bob: 5.294Epoch   9:   8% | abe: 5.225 | eve: 9.869 | bob: 5.294Epoch   9:   9% | abe: 5.223 | eve: 9.870 | bob: 5.292Epoch   9:  10% | abe: 5.221 | eve: 9.871 | bob: 5.290Epoch   9:  10% | abe: 5.218 | eve: 9.870 | bob: 5.288Epoch   9:  11% | abe: 5.217 | eve: 9.871 | bob: 5.287Epoch   9:  12% | abe: 5.215 | eve: 9.875 | bob: 5.286Epoch   9:  13% | abe: 5.212 | eve: 9.873 | bob: 5.282Epoch   9:  14% | abe: 5.210 | eve: 9.871 | bob: 5.280Epoch   9:  14% | abe: 5.209 | eve: 9.870 | bob: 5.279Epoch   9:  15% | abe: 5.206 | eve: 9.872 | bob: 5.277Epoch   9:  16% | abe: 5.205 | eve: 9.867 | bob: 5.276Epoch   9:  17% | abe: 5.203 | eve: 9.867 | bob: 5.274Epoch   9:  17% | abe: 5.201 | eve: 9.867 | bob: 5.272Epoch   9:  18% | abe: 5.200 | eve: 9.864 | bob: 5.271Epoch   9:  19% | abe: 5.198 | eve: 9.865 | bob: 5.269Epoch   9:  20% | abe: 5.196 | eve: 9.865 | bob: 5.268Epoch   9:  21% | abe: 5.195 | eve: 9.860 | bob: 5.266Epoch   9:  21% | abe: 5.193 | eve: 9.861 | bob: 5.265Epoch   9:  22% | abe: 5.191 | eve: 9.861 | bob: 5.263Epoch   9:  23% | abe: 5.189 | eve: 9.861 | bob: 5.261Epoch   9:  24% | abe: 5.187 | eve: 9.861 | bob: 5.259Epoch   9:  25% | abe: 5.186 | eve: 9.861 | bob: 5.257Epoch   9:  25% | abe: 5.184 | eve: 9.861 | bob: 5.256Epoch   9:  26% | abe: 5.182 | eve: 9.860 | bob: 5.254Epoch   9:  27% | abe: 5.180 | eve: 9.861 | bob: 5.252Epoch   9:  28% | abe: 5.178 | eve: 9.859 | bob: 5.250Epoch   9:  28% | abe: 5.176 | eve: 9.858 | bob: 5.248Epoch   9:  29% | abe: 5.174 | eve: 9.858 | bob: 5.246Epoch   9:  30% | abe: 5.172 | eve: 9.858 | bob: 5.245Epoch   9:  31% | abe: 5.170 | eve: 9.858 | bob: 5.243Epoch   9:  32% | abe: 5.169 | eve: 9.860 | bob: 5.242Epoch   9:  32% | abe: 5.167 | eve: 9.858 | bob: 5.241Epoch   9:  33% | abe: 5.166 | eve: 9.859 | bob: 5.239Epoch   9:  34% | abe: 5.164 | eve: 9.860 | bob: 5.238Epoch   9:  35% | abe: 5.162 | eve: 9.861 | bob: 5.236Epoch   9:  35% | abe: 5.160 | eve: 9.862 | bob: 5.235Epoch   9:  36% | abe: 5.159 | eve: 9.862 | bob: 5.233Epoch   9:  37% | abe: 5.157 | eve: 9.864 | bob: 5.232Epoch   9:  38% | abe: 5.155 | eve: 9.863 | bob: 5.230Epoch   9:  39% | abe: 5.153 | eve: 9.863 | bob: 5.228Epoch   9:  39% | abe: 5.152 | eve: 9.864 | bob: 5.227Epoch   9:  40% | abe: 5.150 | eve: 9.864 | bob: 5.225Epoch   9:  41% | abe: 5.148 | eve: 9.864 | bob: 5.224Epoch   9:  42% | abe: 5.146 | eve: 9.863 | bob: 5.222Epoch   9:  42% | abe: 5.145 | eve: 9.863 | bob: 5.220Epoch   9:  43% | abe: 5.143 | eve: 9.863 | bob: 5.219Epoch   9:  44% | abe: 5.142 | eve: 9.863 | bob: 5.218Epoch   9:  45% | abe: 5.140 | eve: 9.862 | bob: 5.216Epoch   9:  46% | abe: 5.138 | eve: 9.862 | bob: 5.214Epoch   9:  46% | abe: 5.136 | eve: 9.863 | bob: 5.213Epoch   9:  47% | abe: 5.135 | eve: 9.864 | bob: 5.212Epoch   9:  48% | abe: 5.133 | eve: 9.864 | bob: 5.210Epoch   9:  49% | abe: 5.131 | eve: 9.863 | bob: 5.208Epoch   9:  50% | abe: 5.129 | eve: 9.864 | bob: 5.207Epoch   9:  50% | abe: 5.128 | eve: 9.864 | bob: 5.205Epoch   9:  51% | abe: 5.126 | eve: 9.862 | bob: 5.203Epoch   9:  52% | abe: 5.124 | eve: 9.862 | bob: 5.202Epoch   9:  53% | abe: 5.122 | eve: 9.862 | bob: 5.200Epoch   9:  53% | abe: 5.121 | eve: 9.862 | bob: 5.199Epoch   9:  54% | abe: 5.119 | eve: 9.863 | bob: 5.197Epoch   9:  55% | abe: 5.117 | eve: 9.863 | bob: 5.196Epoch   9:  56% | abe: 5.116 | eve: 9.862 | bob: 5.195Epoch   9:  57% | abe: 5.114 | eve: 9.862 | bob: 5.193Epoch   9:  57% | abe: 5.112 | eve: 9.862 | bob: 5.191Epoch   9:  58% | abe: 5.111 | eve: 9.862 | bob: 5.190Epoch   9:  59% | abe: 5.109 | eve: 9.862 | bob: 5.188Epoch   9:  60% | abe: 5.107 | eve: 9.862 | bob: 5.187Epoch   9:  60% | abe: 5.105 | eve: 9.862 | bob: 5.185Epoch   9:  61% | abe: 5.104 | eve: 9.862 | bob: 5.183Epoch   9:  62% | abe: 5.102 | eve: 9.863 | bob: 5.182Epoch   9:  63% | abe: 5.100 | eve: 9.862 | bob: 5.180Epoch   9:  64% | abe: 5.098 | eve: 9.862 | bob: 5.179Epoch   9:  64% | abe: 5.097 | eve: 9.863 | bob: 5.177Epoch   9:  65% | abe: 5.095 | eve: 9.862 | bob: 5.176Epoch   9:  66% | abe: 5.093 | eve: 9.862 | bob: 5.174Epoch   9:  67% | abe: 5.092 | eve: 9.862 | bob: 5.173Epoch   9:  67% | abe: 5.090 | eve: 9.861 | bob: 5.171Epoch   9:  68% | abe: 5.088 | eve: 9.862 | bob: 5.170Epoch   9:  69% | abe: 5.087 | eve: 9.861 | bob: 5.168Epoch   9:  70% | abe: 5.085 | eve: 9.861 | bob: 5.167Epoch   9:  71% | abe: 5.083 | eve: 9.861 | bob: 5.165Epoch   9:  71% | abe: 5.081 | eve: 9.861 | bob: 5.163Epoch   9:  72% | abe: 5.080 | eve: 9.861 | bob: 5.162Epoch   9:  73% | abe: 5.078 | eve: 9.862 | bob: 5.161Epoch   9:  74% | abe: 5.076 | eve: 9.862 | bob: 5.159Epoch   9:  75% | abe: 5.075 | eve: 9.861 | bob: 5.158Epoch   9:  75% | abe: 5.073 | eve: 9.861 | bob: 5.156Epoch   9:  76% | abe: 5.071 | eve: 9.862 | bob: 5.155Epoch   9:  77% | abe: 5.069 | eve: 9.861 | bob: 5.153Epoch   9:  78% | abe: 5.068 | eve: 9.861 | bob: 5.152Epoch   9:  78% | abe: 5.066 | eve: 9.861 | bob: 5.150Epoch   9:  79% | abe: 5.064 | eve: 9.860 | bob: 5.149Epoch   9:  80% | abe: 5.063 | eve: 9.860 | bob: 5.147Epoch   9:  81% | abe: 5.061 | eve: 9.861 | bob: 5.146Epoch   9:  82% | abe: 5.059 | eve: 9.862 | bob: 5.144Epoch   9:  82% | abe: 5.057 | eve: 9.862 | bob: 5.143Epoch   9:  83% | abe: 5.056 | eve: 9.862 | bob: 5.141Epoch   9:  84% | abe: 5.054 | eve: 9.862 | bob: 5.140Epoch   9:  85% | abe: 5.053 | eve: 9.862 | bob: 5.138Epoch   9:  85% | abe: 5.051 | eve: 9.863 | bob: 5.137Epoch   9:  86% | abe: 5.049 | eve: 9.863 | bob: 5.135Epoch   9:  87% | abe: 5.047 | eve: 9.863 | bob: 5.134Epoch   9:  88% | abe: 5.046 | eve: 9.863 | bob: 5.132Epoch   9:  89% | abe: 5.044 | eve: 9.863 | bob: 5.130Epoch   9:  89% | abe: 5.042 | eve: 9.863 | bob: 5.129Epoch   9:  90% | abe: 5.041 | eve: 9.863 | bob: 5.127Epoch   9:  91% | abe: 5.039 | eve: 9.863 | bob: 5.126Epoch   9:  92% | abe: 5.037 | eve: 9.863 | bob: 5.124Epoch   9:  92% | abe: 5.035 | eve: 9.864 | bob: 5.123Epoch   9:  93% | abe: 5.033 | eve: 9.864 | bob: 5.121Epoch   9:  94% | abe: 5.032 | eve: 9.865 | bob: 5.120Epoch   9:  95% | abe: 5.030 | eve: 9.865 | bob: 5.118Epoch   9:  96% | abe: 5.028 | eve: 9.866 | bob: 5.117Epoch   9:  96% | abe: 5.027 | eve: 9.865 | bob: 5.115Epoch   9:  97% | abe: 5.025 | eve: 9.865 | bob: 5.114Epoch   9:  98% | abe: 5.024 | eve: 9.865 | bob: 5.112Epoch   9:  99% | abe: 5.022 | eve: 9.865 | bob: 5.111
New best Bob loss 5.110961700758935 at epoch 9
Epoch  10:   0% | abe: 4.803 | eve: 9.789 | bob: 4.924Epoch  10:   0% | abe: 4.804 | eve: 9.835 | bob: 4.918Epoch  10:   1% | abe: 4.802 | eve: 9.847 | bob: 4.915Epoch  10:   2% | abe: 4.799 | eve: 9.862 | bob: 4.912Epoch  10:   3% | abe: 4.799 | eve: 9.882 | bob: 4.913Epoch  10:   3% | abe: 4.799 | eve: 9.888 | bob: 4.913Epoch  10:   4% | abe: 4.797 | eve: 9.886 | bob: 4.913Epoch  10:   5% | abe: 4.796 | eve: 9.878 | bob: 4.912Epoch  10:   6% | abe: 4.793 | eve: 9.875 | bob: 4.909Epoch  10:   7% | abe: 4.791 | eve: 9.869 | bob: 4.907Epoch  10:   7% | abe: 4.789 | eve: 9.870 | bob: 4.905Epoch  10:   8% | abe: 4.787 | eve: 9.865 | bob: 4.903Epoch  10:   9% | abe: 4.785 | eve: 9.863 | bob: 4.901Epoch  10:  10% | abe: 4.784 | eve: 9.866 | bob: 4.899Epoch  10:  10% | abe: 4.781 | eve: 9.867 | bob: 4.896Epoch  10:  11% | abe: 4.779 | eve: 9.870 | bob: 4.894Epoch  10:  12% | abe: 4.778 | eve: 9.870 | bob: 4.893Epoch  10:  13% | abe: 4.776 | eve: 9.867 | bob: 4.892Epoch  10:  14% | abe: 4.775 | eve: 9.871 | bob: 4.891Epoch  10:  14% | abe: 4.774 | eve: 9.871 | bob: 4.890Epoch  10:  15% | abe: 4.772 | eve: 9.869 | bob: 4.888Epoch  10:  16% | abe: 4.770 | eve: 9.869 | bob: 4.886Epoch  10:  17% | abe: 4.768 | eve: 9.872 | bob: 4.885Epoch  10:  17% | abe: 4.766 | eve: 9.873 | bob: 4.884Epoch  10:  18% | abe: 4.765 | eve: 9.870 | bob: 4.882Epoch  10:  19% | abe: 4.763 | eve: 9.870 | bob: 4.880Epoch  10:  20% | abe: 4.762 | eve: 9.873 | bob: 4.879Epoch  10:  21% | abe: 4.760 | eve: 9.872 | bob: 4.878Epoch  10:  21% | abe: 4.759 | eve: 9.874 | bob: 4.877Epoch  10:  22% | abe: 4.757 | eve: 9.874 | bob: 4.875Epoch  10:  23% | abe: 4.755 | eve: 9.874 | bob: 4.873Epoch  10:  24% | abe: 4.753 | eve: 9.873 | bob: 4.872Epoch  10:  25% | abe: 4.752 | eve: 9.874 | bob: 4.870Epoch  10:  25% | abe: 4.750 | eve: 9.874 | bob: 4.868Epoch  10:  26% | abe: 4.748 | eve: 9.873 | bob: 4.867Epoch  10:  27% | abe: 4.747 | eve: 9.871 | bob: 4.866Epoch  10:  28% | abe: 4.745 | eve: 9.870 | bob: 4.864Epoch  10:  28% | abe: 4.744 | eve: 9.868 | bob: 4.863Epoch  10:  29% | abe: 4.742 | eve: 9.866 | bob: 4.862Epoch  10:  30% | abe: 4.741 | eve: 9.865 | bob: 4.860Epoch  10:  31% | abe: 4.740 | eve: 9.866 | bob: 4.859Epoch  10:  32% | abe: 4.738 | eve: 9.865 | bob: 4.858Epoch  10:  32% | abe: 4.737 | eve: 9.865 | bob: 4.857Epoch  10:  33% | abe: 4.735 | eve: 9.866 | bob: 4.855Epoch  10:  34% | abe: 4.733 | eve: 9.866 | bob: 4.854Epoch  10:  35% | abe: 4.732 | eve: 9.868 | bob: 4.853Epoch  10:  35% | abe: 4.730 | eve: 9.867 | bob: 4.851Epoch  10:  36% | abe: 4.729 | eve: 9.867 | bob: 4.849Epoch  10:  37% | abe: 4.727 | eve: 9.865 | bob: 4.848Epoch  10:  38% | abe: 4.725 | eve: 9.867 | bob: 4.846Epoch  10:  39% | abe: 4.724 | eve: 9.867 | bob: 4.845Epoch  10:  39% | abe: 4.722 | eve: 9.867 | bob: 4.843Epoch  10:  40% | abe: 4.720 | eve: 9.868 | bob: 4.842Epoch  10:  41% | abe: 4.719 | eve: 9.866 | bob: 4.840Epoch  10:  42% | abe: 4.717 | eve: 9.867 | bob: 4.839Epoch  10:  42% | abe: 4.716 | eve: 9.867 | bob: 4.838Epoch  10:  43% | abe: 4.714 | eve: 9.868 | bob: 4.836Epoch  10:  44% | abe: 4.712 | eve: 9.868 | bob: 4.834Epoch  10:  45% | abe: 4.711 | eve: 9.868 | bob: 4.833Epoch  10:  46% | abe: 4.709 | eve: 9.868 | bob: 4.832Epoch  10:  46% | abe: 4.708 | eve: 9.867 | bob: 4.831Epoch  10:  47% | abe: 4.706 | eve: 9.867 | bob: 4.829Epoch  10:  48% | abe: 4.704 | eve: 9.865 | bob: 4.828Epoch  10:  49% | abe: 4.703 | eve: 9.865 | bob: 4.827Epoch  10:  50% | abe: 4.702 | eve: 9.867 | bob: 4.825Epoch  10:  50% | abe: 4.700 | eve: 9.866 | bob: 4.824Epoch  10:  51% | abe: 4.699 | eve: 9.866 | bob: 4.822Epoch  10:  52% | abe: 4.697 | eve: 9.866 | bob: 4.821Epoch  10:  53% | abe: 4.695 | eve: 9.866 | bob: 4.820Epoch  10:  53% | abe: 4.694 | eve: 9.866 | bob: 4.818Epoch  10:  54% | abe: 4.692 | eve: 9.866 | bob: 4.817Epoch  10:  55% | abe: 4.691 | eve: 9.867 | bob: 4.816Epoch  10:  56% | abe: 4.689 | eve: 9.870 | bob: 4.815Epoch  10:  57% | abe: 4.688 | eve: 9.869 | bob: 4.813Epoch  10:  57% | abe: 4.686 | eve: 9.870 | bob: 4.812Epoch  10:  58% | abe: 4.684 | eve: 9.871 | bob: 4.810Epoch  10:  59% | abe: 4.683 | eve: 9.872 | bob: 4.809Epoch  10:  60% | abe: 4.681 | eve: 9.872 | bob: 4.808Epoch  10:  60% | abe: 4.680 | eve: 9.872 | bob: 4.806Epoch  10:  61% | abe: 4.678 | eve: 9.872 | bob: 4.805Epoch  10:  62% | abe: 4.677 | eve: 9.873 | bob: 4.804Epoch  10:  63% | abe: 4.675 | eve: 9.872 | bob: 4.802Epoch  10:  64% | abe: 4.673 | eve: 9.873 | bob: 4.801Epoch  10:  64% | abe: 4.672 | eve: 9.873 | bob: 4.799Epoch  10:  65% | abe: 4.671 | eve: 9.874 | bob: 4.798Epoch  10:  66% | abe: 4.669 | eve: 9.874 | bob: 4.797Epoch  10:  67% | abe: 4.668 | eve: 9.873 | bob: 4.795Epoch  10:  67% | abe: 4.666 | eve: 9.874 | bob: 4.794Epoch  10:  68% | abe: 4.664 | eve: 9.873 | bob: 4.793Epoch  10:  69% | abe: 4.663 | eve: 9.874 | bob: 4.791Epoch  10:  70% | abe: 4.661 | eve: 9.873 | bob: 4.790Epoch  10:  71% | abe: 4.660 | eve: 9.873 | bob: 4.788Epoch  10:  71% | abe: 4.658 | eve: 9.873 | bob: 4.787Epoch  10:  72% | abe: 4.657 | eve: 9.873 | bob: 4.786Epoch  10:  73% | abe: 4.655 | eve: 9.873 | bob: 4.784Epoch  10:  74% | abe: 4.654 | eve: 9.873 | bob: 4.783Epoch  10:  75% | abe: 4.652 | eve: 9.873 | bob: 4.782Epoch  10:  75% | abe: 4.651 | eve: 9.873 | bob: 4.781Epoch  10:  76% | abe: 4.649 | eve: 9.873 | bob: 4.779Epoch  10:  77% | abe: 4.648 | eve: 9.874 | bob: 4.778Epoch  10:  78% | abe: 4.646 | eve: 9.874 | bob: 4.776Epoch  10:  78% | abe: 4.645 | eve: 9.873 | bob: 4.775Epoch  10:  79% | abe: 4.643 | eve: 9.874 | bob: 4.774Epoch  10:  80% | abe: 4.642 | eve: 9.873 | bob: 4.772Epoch  10:  81% | abe: 4.640 | eve: 9.874 | bob: 4.771Epoch  10:  82% | abe: 4.639 | eve: 9.874 | bob: 4.770Epoch  10:  82% | abe: 4.637 | eve: 9.875 | bob: 4.768Epoch  10:  83% | abe: 4.635 | eve: 9.874 | bob: 4.767Epoch  10:  84% | abe: 4.634 | eve: 9.873 | bob: 4.766Epoch  10:  85% | abe: 4.632 | eve: 9.874 | bob: 4.764Epoch  10:  85% | abe: 4.631 | eve: 9.874 | bob: 4.763Epoch  10:  86% | abe: 4.629 | eve: 9.874 | bob: 4.761Epoch  10:  87% | abe: 4.628 | eve: 9.874 | bob: 4.760Epoch  10:  88% | abe: 4.626 | eve: 9.874 | bob: 4.759Epoch  10:  89% | abe: 4.625 | eve: 9.875 | bob: 4.757Epoch  10:  89% | abe: 4.623 | eve: 9.875 | bob: 4.756Epoch  10:  90% | abe: 4.622 | eve: 9.875 | bob: 4.755Epoch  10:  91% | abe: 4.620 | eve: 9.874 | bob: 4.753Epoch  10:  92% | abe: 4.618 | eve: 9.875 | bob: 4.752Epoch  10:  92% | abe: 4.617 | eve: 9.875 | bob: 4.751Epoch  10:  93% | abe: 4.616 | eve: 9.875 | bob: 4.749Epoch  10:  94% | abe: 4.614 | eve: 9.875 | bob: 4.748Epoch  10:  95% | abe: 4.613 | eve: 9.874 | bob: 4.747Epoch  10:  96% | abe: 4.611 | eve: 9.874 | bob: 4.746Epoch  10:  96% | abe: 4.610 | eve: 9.875 | bob: 4.744Epoch  10:  97% | abe: 4.608 | eve: 9.874 | bob: 4.743Epoch  10:  98% | abe: 4.607 | eve: 9.874 | bob: 4.742Epoch  10:  99% | abe: 4.605 | eve: 9.875 | bob: 4.740
New best Bob loss 4.740287536651749 at epoch 10
Epoch  11:   0% | abe: 4.415 | eve: 9.848 | bob: 4.578Epoch  11:   0% | abe: 4.416 | eve: 9.895 | bob: 4.571Epoch  11:   1% | abe: 4.412 | eve: 9.899 | bob: 4.569Epoch  11:   2% | abe: 4.409 | eve: 9.887 | bob: 4.567Epoch  11:   3% | abe: 4.408 | eve: 9.881 | bob: 4.566Epoch  11:   3% | abe: 4.407 | eve: 9.865 | bob: 4.566Epoch  11:   4% | abe: 4.405 | eve: 9.870 | bob: 4.565Epoch  11:   5% | abe: 4.403 | eve: 9.864 | bob: 4.563Epoch  11:   6% | abe: 4.401 | eve: 9.860 | bob: 4.562Epoch  11:   7% | abe: 4.400 | eve: 9.864 | bob: 4.561Epoch  11:   7% | abe: 4.398 | eve: 9.865 | bob: 4.560Epoch  11:   8% | abe: 4.396 | eve: 9.869 | bob: 4.558Epoch  11:   9% | abe: 4.393 | eve: 9.869 | bob: 4.554Epoch  11:  10% | abe: 4.392 | eve: 9.869 | bob: 4.553Epoch  11:  10% | abe: 4.390 | eve: 9.873 | bob: 4.552Epoch  11:  11% | abe: 4.389 | eve: 9.877 | bob: 4.551Epoch  11:  12% | abe: 4.388 | eve: 9.876 | bob: 4.550Epoch  11:  13% | abe: 4.387 | eve: 9.878 | bob: 4.549Epoch  11:  14% | abe: 4.386 | eve: 9.871 | bob: 4.549Epoch  11:  14% | abe: 4.385 | eve: 9.872 | bob: 4.549Epoch  11:  15% | abe: 4.382 | eve: 9.874 | bob: 4.547Epoch  11:  16% | abe: 4.381 | eve: 9.873 | bob: 4.546Epoch  11:  17% | abe: 4.380 | eve: 9.874 | bob: 4.545Epoch  11:  17% | abe: 4.378 | eve: 9.869 | bob: 4.544Epoch  11:  18% | abe: 4.377 | eve: 9.868 | bob: 4.543Epoch  11:  19% | abe: 4.376 | eve: 9.871 | bob: 4.542Epoch  11:  20% | abe: 4.375 | eve: 9.873 | bob: 4.542Epoch  11:  21% | abe: 4.374 | eve: 9.874 | bob: 4.541Epoch  11:  21% | abe: 4.373 | eve: 9.876 | bob: 4.539Epoch  11:  22% | abe: 4.371 | eve: 9.875 | bob: 4.537Epoch  11:  23% | abe: 4.369 | eve: 9.873 | bob: 4.536Epoch  11:  24% | abe: 4.368 | eve: 9.873 | bob: 4.535Epoch  11:  25% | abe: 4.366 | eve: 9.872 | bob: 4.533Epoch  11:  25% | abe: 4.365 | eve: 9.875 | bob: 4.532Epoch  11:  26% | abe: 4.364 | eve: 9.875 | bob: 4.531Epoch  11:  27% | abe: 4.363 | eve: 9.875 | bob: 4.530Epoch  11:  28% | abe: 4.361 | eve: 9.876 | bob: 4.528Epoch  11:  28% | abe: 4.359 | eve: 9.877 | bob: 4.527Epoch  11:  29% | abe: 4.357 | eve: 9.875 | bob: 4.525Epoch  11:  30% | abe: 4.356 | eve: 9.875 | bob: 4.524Epoch  11:  31% | abe: 4.355 | eve: 9.876 | bob: 4.523Epoch  11:  32% | abe: 4.354 | eve: 9.878 | bob: 4.522Epoch  11:  32% | abe: 4.353 | eve: 9.879 | bob: 4.521Epoch  11:  33% | abe: 4.351 | eve: 9.880 | bob: 4.520Epoch  11:  34% | abe: 4.349 | eve: 9.880 | bob: 4.518Epoch  11:  35% | abe: 4.348 | eve: 9.880 | bob: 4.517Epoch  11:  35% | abe: 4.346 | eve: 9.880 | bob: 4.515Epoch  11:  36% | abe: 4.345 | eve: 9.881 | bob: 4.514Epoch  11:  37% | abe: 4.344 | eve: 9.881 | bob: 4.513Epoch  11:  38% | abe: 4.342 | eve: 9.881 | bob: 4.512Epoch  11:  39% | abe: 4.341 | eve: 9.880 | bob: 4.511Epoch  11:  39% | abe: 4.339 | eve: 9.881 | bob: 4.509Epoch  11:  40% | abe: 4.337 | eve: 9.881 | bob: 4.508Epoch  11:  41% | abe: 4.336 | eve: 9.881 | bob: 4.506Epoch  11:  42% | abe: 4.334 | eve: 9.881 | bob: 4.505Epoch  11:  42% | abe: 4.333 | eve: 9.880 | bob: 4.504Epoch  11:  43% | abe: 4.332 | eve: 9.880 | bob: 4.503Epoch  11:  44% | abe: 4.331 | eve: 9.881 | bob: 4.502Epoch  11:  45% | abe: 4.330 | eve: 9.882 | bob: 4.500Epoch  11:  46% | abe: 4.328 | eve: 9.883 | bob: 4.499Epoch  11:  46% | abe: 4.327 | eve: 9.884 | bob: 4.497Epoch  11:  47% | abe: 4.325 | eve: 9.885 | bob: 4.496Epoch  11:  48% | abe: 4.324 | eve: 9.884 | bob: 4.495Epoch  11:  49% | abe: 4.322 | eve: 9.884 | bob: 4.493Epoch  11:  50% | abe: 4.321 | eve: 9.882 | bob: 4.492Epoch  11:  50% | abe: 4.320 | eve: 9.882 | bob: 4.491Epoch  11:  51% | abe: 4.318 | eve: 9.882 | bob: 4.489Epoch  11:  52% | abe: 4.316 | eve: 9.882 | bob: 4.488Epoch  11:  53% | abe: 4.315 | eve: 9.883 | bob: 4.487Epoch  11:  53% | abe: 4.314 | eve: 9.883 | bob: 4.486Epoch  11:  54% | abe: 4.313 | eve: 9.883 | bob: 4.484Epoch  11:  55% | abe: 4.311 | eve: 9.884 | bob: 4.483Epoch  11:  56% | abe: 4.310 | eve: 9.885 | bob: 4.482Epoch  11:  57% | abe: 4.309 | eve: 9.885 | bob: 4.481Epoch  11:  57% | abe: 4.308 | eve: 9.885 | bob: 4.480Epoch  11:  58% | abe: 4.306 | eve: 9.886 | bob: 4.479Epoch  11:  59% | abe: 4.305 | eve: 9.887 | bob: 4.478Epoch  11:  60% | abe: 4.304 | eve: 9.886 | bob: 4.477Epoch  11:  60% | abe: 4.303 | eve: 9.887 | bob: 4.476Epoch  11:  61% | abe: 4.301 | eve: 9.886 | bob: 4.474Epoch  11:  62% | abe: 4.300 | eve: 9.886 | bob: 4.473Epoch  11:  63% | abe: 4.298 | eve: 9.887 | bob: 4.472Epoch  11:  64% | abe: 4.297 | eve: 9.887 | bob: 4.470Epoch  11:  64% | abe: 4.295 | eve: 9.887 | bob: 4.469Epoch  11:  65% | abe: 4.294 | eve: 9.886 | bob: 4.468Epoch  11:  66% | abe: 4.293 | eve: 9.887 | bob: 4.467Epoch  11:  67% | abe: 4.291 | eve: 9.887 | bob: 4.465Epoch  11:  67% | abe: 4.290 | eve: 9.886 | bob: 4.464Epoch  11:  68% | abe: 4.289 | eve: 9.886 | bob: 4.463Epoch  11:  69% | abe: 4.287 | eve: 9.886 | bob: 4.462Epoch  11:  70% | abe: 4.286 | eve: 9.886 | bob: 4.460Epoch  11:  71% | abe: 4.285 | eve: 9.886 | bob: 4.459Epoch  11:  71% | abe: 4.283 | eve: 9.886 | bob: 4.458Epoch  11:  72% | abe: 4.282 | eve: 9.886 | bob: 4.457Epoch  11:  73% | abe: 4.280 | eve: 9.886 | bob: 4.456Epoch  11:  74% | abe: 4.279 | eve: 9.886 | bob: 4.454Epoch  11:  75% | abe: 4.278 | eve: 9.886 | bob: 4.453Epoch  11:  75% | abe: 4.276 | eve: 9.886 | bob: 4.452Epoch  11:  76% | abe: 4.275 | eve: 9.886 | bob: 4.451Epoch  11:  77% | abe: 4.273 | eve: 9.886 | bob: 4.450Epoch  11:  78% | abe: 4.272 | eve: 9.887 | bob: 4.449Epoch  11:  78% | abe: 4.271 | eve: 9.887 | bob: 4.447Epoch  11:  79% | abe: 4.269 | eve: 9.887 | bob: 4.446Epoch  11:  80% | abe: 4.268 | eve: 9.887 | bob: 4.445Epoch  11:  81% | abe: 4.266 | eve: 9.887 | bob: 4.443Epoch  11:  82% | abe: 4.265 | eve: 9.887 | bob: 4.442Epoch  11:  82% | abe: 4.264 | eve: 9.887 | bob: 4.441Epoch  11:  83% | abe: 4.262 | eve: 9.887 | bob: 4.440Epoch  11:  84% | abe: 4.261 | eve: 9.888 | bob: 4.439Epoch  11:  85% | abe: 4.260 | eve: 9.888 | bob: 4.438Epoch  11:  85% | abe: 4.258 | eve: 9.888 | bob: 4.436Epoch  11:  86% | abe: 4.257 | eve: 9.887 | bob: 4.435Epoch  11:  87% | abe: 4.255 | eve: 9.887 | bob: 4.434Epoch  11:  88% | abe: 4.254 | eve: 9.888 | bob: 4.432Epoch  11:  89% | abe: 4.253 | eve: 9.888 | bob: 4.431Epoch  11:  89% | abe: 4.251 | eve: 9.888 | bob: 4.430Epoch  11:  90% | abe: 4.250 | eve: 9.889 | bob: 4.428Epoch  11:  91% | abe: 4.248 | eve: 9.889 | bob: 4.427Epoch  11:  92% | abe: 4.247 | eve: 9.889 | bob: 4.426Epoch  11:  92% | abe: 4.245 | eve: 9.889 | bob: 4.425Epoch  11:  93% | abe: 4.244 | eve: 9.889 | bob: 4.424Epoch  11:  94% | abe: 4.243 | eve: 9.890 | bob: 4.422Epoch  11:  95% | abe: 4.242 | eve: 9.890 | bob: 4.421Epoch  11:  96% | abe: 4.240 | eve: 9.890 | bob: 4.420Epoch  11:  96% | abe: 4.239 | eve: 9.890 | bob: 4.419Epoch  11:  97% | abe: 4.238 | eve: 9.890 | bob: 4.418Epoch  11:  98% | abe: 4.236 | eve: 9.890 | bob: 4.416Epoch  11:  99% | abe: 4.235 | eve: 9.889 | bob: 4.415
New best Bob loss 4.4153137887306 at epoch 11
Epoch  12:   0% | abe: 4.070 | eve: 9.856 | bob: 4.267Epoch  12:   0% | abe: 4.060 | eve: 9.877 | bob: 4.258Epoch  12:   1% | abe: 4.062 | eve: 9.878 | bob: 4.258Epoch  12:   2% | abe: 4.064 | eve: 9.892 | bob: 4.262Epoch  12:   3% | abe: 4.059 | eve: 9.893 | bob: 4.254Epoch  12:   3% | abe: 4.057 | eve: 9.891 | bob: 4.258Epoch  12:   4% | abe: 4.057 | eve: 9.891 | bob: 4.256Epoch  12:   5% | abe: 4.057 | eve: 9.897 | bob: 4.260Epoch  12:   6% | abe: 4.056 | eve: 9.890 | bob: 4.259Epoch  12:   7% | abe: 4.055 | eve: 9.888 | bob: 4.257Epoch  12:   7% | abe: 4.054 | eve: 9.889 | bob: 4.255Epoch  12:   8% | abe: 4.051 | eve: 9.881 | bob: 4.253Epoch  12:   9% | abe: 4.050 | eve: 9.889 | bob: 4.251Epoch  12:  10% | abe: 4.046 | eve: 9.887 | bob: 4.248Epoch  12:  10% | abe: 4.047 | eve: 9.893 | bob: 4.249Epoch  12:  11% | abe: 4.045 | eve: 9.892 | bob: 4.248Epoch  12:  12% | abe: 4.045 | eve: 9.891 | bob: 4.248Epoch  12:  13% | abe: 4.043 | eve: 9.889 | bob: 4.246Epoch  12:  14% | abe: 4.041 | eve: 9.888 | bob: 4.245Epoch  12:  14% | abe: 4.039 | eve: 9.891 | bob: 4.244Epoch  12:  15% | abe: 4.038 | eve: 9.893 | bob: 4.242Epoch  12:  16% | abe: 4.037 | eve: 9.893 | bob: 4.241Epoch  12:  17% | abe: 4.037 | eve: 9.889 | bob: 4.241Epoch  12:  17% | abe: 4.035 | eve: 9.891 | bob: 4.239Epoch  12:  18% | abe: 4.033 | eve: 9.888 | bob: 4.237Epoch  12:  19% | abe: 4.031 | eve: 9.888 | bob: 4.235Epoch  12:  20% | abe: 4.030 | eve: 9.890 | bob: 4.235Epoch  12:  21% | abe: 4.029 | eve: 9.889 | bob: 4.234Epoch  12:  21% | abe: 4.028 | eve: 9.889 | bob: 4.233Epoch  12:  22% | abe: 4.025 | eve: 9.890 | bob: 4.231Epoch  12:  23% | abe: 4.025 | eve: 9.891 | bob: 4.229Epoch  12:  24% | abe: 4.023 | eve: 9.889 | bob: 4.228Epoch  12:  25% | abe: 4.022 | eve: 9.893 | bob: 4.226Epoch  12:  25% | abe: 4.021 | eve: 9.893 | bob: 4.226Epoch  12:  26% | abe: 4.019 | eve: 9.892 | bob: 4.224Epoch  12:  27% | abe: 4.017 | eve: 9.893 | bob: 4.222Epoch  12:  28% | abe: 4.016 | eve: 9.893 | bob: 4.221Epoch  12:  28% | abe: 4.015 | eve: 9.891 | bob: 4.220Epoch  12:  29% | abe: 4.014 | eve: 9.892 | bob: 4.219Epoch  12:  30% | abe: 4.012 | eve: 9.893 | bob: 4.218Epoch  12:  31% | abe: 4.011 | eve: 9.894 | bob: 4.217Epoch  12:  32% | abe: 4.009 | eve: 9.893 | bob: 4.215Epoch  12:  32% | abe: 4.008 | eve: 9.892 | bob: 4.214Epoch  12:  33% | abe: 4.007 | eve: 9.894 | bob: 4.213Epoch  12:  34% | abe: 4.006 | eve: 9.895 | bob: 4.212Epoch  12:  35% | abe: 4.004 | eve: 9.895 | bob: 4.211Epoch  12:  35% | abe: 4.003 | eve: 9.896 | bob: 4.209Epoch  12:  36% | abe: 4.002 | eve: 9.895 | bob: 4.209Epoch  12:  37% | abe: 4.001 | eve: 9.896 | bob: 4.207Epoch  12:  38% | abe: 3.999 | eve: 9.895 | bob: 4.206Epoch  12:  39% | abe: 3.998 | eve: 9.895 | bob: 4.205Epoch  12:  39% | abe: 3.996 | eve: 9.895 | bob: 4.204Epoch  12:  40% | abe: 3.995 | eve: 9.896 | bob: 4.202Epoch  12:  41% | abe: 3.993 | eve: 9.896 | bob: 4.200Epoch  12:  42% | abe: 3.991 | eve: 9.896 | bob: 4.199Epoch  12:  42% | abe: 3.990 | eve: 9.896 | bob: 4.198Epoch  12:  43% | abe: 3.989 | eve: 9.897 | bob: 4.197Epoch  12:  44% | abe: 3.987 | eve: 9.898 | bob: 4.195Epoch  12:  45% | abe: 3.987 | eve: 9.897 | bob: 4.195Epoch  12:  46% | abe: 3.985 | eve: 9.897 | bob: 4.193Epoch  12:  46% | abe: 3.983 | eve: 9.898 | bob: 4.192Epoch  12:  47% | abe: 3.982 | eve: 9.896 | bob: 4.190Epoch  12:  48% | abe: 3.980 | eve: 9.896 | bob: 4.189Epoch  12:  49% | abe: 3.979 | eve: 9.895 | bob: 4.188Epoch  12:  50% | abe: 3.977 | eve: 9.895 | bob: 4.186Epoch  12:  50% | abe: 3.976 | eve: 9.896 | bob: 4.185Epoch  12:  51% | abe: 3.975 | eve: 9.897 | bob: 4.184Epoch  12:  52% | abe: 3.974 | eve: 9.897 | bob: 4.183Epoch  12:  53% | abe: 3.973 | eve: 9.896 | bob: 4.182Epoch  12:  53% | abe: 3.971 | eve: 9.897 | bob: 4.180Epoch  12:  54% | abe: 3.970 | eve: 9.896 | bob: 4.179Epoch  12:  55% | abe: 3.969 | eve: 9.896 | bob: 4.178Epoch  12:  56% | abe: 3.967 | eve: 9.897 | bob: 4.177Epoch  12:  57% | abe: 3.966 | eve: 9.897 | bob: 4.175Epoch  12:  57% | abe: 3.964 | eve: 9.897 | bob: 4.174Epoch  12:  58% | abe: 3.964 | eve: 9.898 | bob: 4.173Epoch  12:  59% | abe: 3.962 | eve: 9.898 | bob: 4.172Epoch  12:  60% | abe: 3.961 | eve: 9.898 | bob: 4.171Epoch  12:  60% | abe: 3.960 | eve: 9.898 | bob: 4.169Epoch  12:  61% | abe: 3.958 | eve: 9.898 | bob: 4.168Epoch  12:  62% | abe: 3.957 | eve: 9.898 | bob: 4.167Epoch  12:  63% | abe: 3.956 | eve: 9.898 | bob: 4.166Epoch  12:  64% | abe: 3.954 | eve: 9.898 | bob: 4.165Epoch  12:  64% | abe: 3.953 | eve: 9.898 | bob: 4.164Epoch  12:  65% | abe: 3.952 | eve: 9.898 | bob: 4.162Epoch  12:  66% | abe: 3.950 | eve: 9.897 | bob: 4.161Epoch  12:  67% | abe: 3.949 | eve: 9.897 | bob: 4.160Epoch  12:  67% | abe: 3.948 | eve: 9.897 | bob: 4.159Epoch  12:  68% | abe: 3.946 | eve: 9.896 | bob: 4.157Epoch  12:  69% | abe: 3.945 | eve: 9.895 | bob: 4.156Epoch  12:  70% | abe: 3.944 | eve: 9.895 | bob: 4.155Epoch  12:  71% | abe: 3.942 | eve: 9.896 | bob: 4.154Epoch  12:  71% | abe: 3.941 | eve: 9.896 | bob: 4.153Epoch  12:  72% | abe: 3.940 | eve: 9.896 | bob: 4.152Epoch  12:  73% | abe: 3.938 | eve: 9.895 | bob: 4.151Epoch  12:  74% | abe: 3.937 | eve: 9.896 | bob: 4.150Epoch  12:  75% | abe: 3.936 | eve: 9.895 | bob: 4.148Epoch  12:  75% | abe: 3.935 | eve: 9.895 | bob: 4.147Epoch  12:  76% | abe: 3.934 | eve: 9.896 | bob: 4.147Epoch  12:  77% | abe: 3.932 | eve: 9.896 | bob: 4.145Epoch  12:  78% | abe: 3.931 | eve: 9.895 | bob: 4.144Epoch  12:  78% | abe: 3.930 | eve: 9.896 | bob: 4.143Epoch  12:  79% | abe: 3.929 | eve: 9.896 | bob: 4.142Epoch  12:  80% | abe: 3.927 | eve: 9.897 | bob: 4.141Epoch  12:  81% | abe: 3.926 | eve: 9.896 | bob: 4.140Epoch  12:  82% | abe: 3.925 | eve: 9.897 | bob: 4.139Epoch  12:  82% | abe: 3.924 | eve: 9.897 | bob: 4.138Epoch  12:  83% | abe: 3.922 | eve: 9.896 | bob: 4.137Epoch  12:  84% | abe: 3.921 | eve: 9.896 | bob: 4.135Epoch  12:  85% | abe: 3.919 | eve: 9.896 | bob: 4.134Epoch  12:  85% | abe: 3.918 | eve: 9.895 | bob: 4.133Epoch  12:  86% | abe: 3.917 | eve: 9.895 | bob: 4.131Epoch  12:  87% | abe: 3.915 | eve: 9.894 | bob: 4.130Epoch  12:  88% | abe: 3.914 | eve: 9.894 | bob: 4.129Epoch  12:  89% | abe: 3.913 | eve: 9.894 | bob: 4.128Epoch  12:  89% | abe: 3.912 | eve: 9.894 | bob: 4.127Epoch  12:  90% | abe: 3.910 | eve: 9.894 | bob: 4.126Epoch  12:  91% | abe: 3.909 | eve: 9.894 | bob: 4.124Epoch  12:  92% | abe: 3.908 | eve: 9.894 | bob: 4.123Epoch  12:  92% | abe: 3.906 | eve: 9.893 | bob: 4.122Epoch  12:  93% | abe: 3.905 | eve: 9.893 | bob: 4.121Epoch  12:  94% | abe: 3.904 | eve: 9.894 | bob: 4.119Epoch  12:  95% | abe: 3.902 | eve: 9.893 | bob: 4.119Epoch  12:  96% | abe: 3.901 | eve: 9.894 | bob: 4.118Epoch  12:  96% | abe: 3.900 | eve: 9.893 | bob: 4.116Epoch  12:  97% | abe: 3.899 | eve: 9.893 | bob: 4.116Epoch  12:  98% | abe: 3.898 | eve: 9.894 | bob: 4.115Epoch  12:  99% | abe: 3.897 | eve: 9.894 | bob: 4.113
New best Bob loss 4.113455900482904 at epoch 12
Epoch  13:   0% | abe: 3.740 | eve: 9.986 | bob: 3.981Epoch  13:   0% | abe: 3.740 | eve: 9.957 | bob: 3.968Epoch  13:   1% | abe: 3.744 | eve: 9.952 | bob: 3.978Epoch  13:   2% | abe: 3.739 | eve: 9.956 | bob: 3.975Epoch  13:   3% | abe: 3.737 | eve: 9.955 | bob: 3.977Epoch  13:   3% | abe: 3.734 | eve: 9.941 | bob: 3.972Epoch  13:   4% | abe: 3.736 | eve: 9.927 | bob: 3.974Epoch  13:   5% | abe: 3.732 | eve: 9.911 | bob: 3.970Epoch  13:   6% | abe: 3.732 | eve: 9.914 | bob: 3.970Epoch  13:   7% | abe: 3.728 | eve: 9.915 | bob: 3.967Epoch  13:   7% | abe: 3.728 | eve: 9.911 | bob: 3.966Epoch  13:   8% | abe: 3.728 | eve: 9.906 | bob: 3.966Epoch  13:   9% | abe: 3.726 | eve: 9.900 | bob: 3.963Epoch  13:  10% | abe: 3.723 | eve: 9.902 | bob: 3.960Epoch  13:  10% | abe: 3.721 | eve: 9.906 | bob: 3.958Epoch  13:  11% | abe: 3.720 | eve: 9.905 | bob: 3.958Epoch  13:  12% | abe: 3.718 | eve: 9.904 | bob: 3.955Epoch  13:  13% | abe: 3.716 | eve: 9.900 | bob: 3.954Epoch  13:  14% | abe: 3.714 | eve: 9.902 | bob: 3.952Epoch  13:  14% | abe: 3.712 | eve: 9.901 | bob: 3.951Epoch  13:  15% | abe: 3.711 | eve: 9.900 | bob: 3.951Epoch  13:  16% | abe: 3.709 | eve: 9.898 | bob: 3.949Epoch  13:  17% | abe: 3.707 | eve: 9.895 | bob: 3.946Epoch  13:  17% | abe: 3.706 | eve: 9.894 | bob: 3.945Epoch  13:  18% | abe: 3.705 | eve: 9.894 | bob: 3.944Epoch  13:  19% | abe: 3.705 | eve: 9.895 | bob: 3.944Epoch  13:  20% | abe: 3.704 | eve: 9.893 | bob: 3.944Epoch  13:  21% | abe: 3.703 | eve: 9.892 | bob: 3.943Epoch  13:  21% | abe: 3.702 | eve: 9.893 | bob: 3.942Epoch  13:  22% | abe: 3.700 | eve: 9.893 | bob: 3.940Epoch  13:  23% | abe: 3.698 | eve: 9.894 | bob: 3.939Epoch  13:  24% | abe: 3.697 | eve: 9.893 | bob: 3.938Epoch  13:  25% | abe: 3.696 | eve: 9.896 | bob: 3.936Epoch  13:  25% | abe: 3.695 | eve: 9.897 | bob: 3.936Epoch  13:  26% | abe: 3.695 | eve: 9.897 | bob: 3.936Epoch  13:  27% | abe: 3.693 | eve: 9.899 | bob: 3.935Epoch  13:  28% | abe: 3.692 | eve: 9.899 | bob: 3.934Epoch  13:  28% | abe: 3.691 | eve: 9.898 | bob: 3.933Epoch  13:  29% | abe: 3.690 | eve: 9.900 | bob: 3.932Epoch  13:  30% | abe: 3.689 | eve: 9.898 | bob: 3.932Epoch  13:  31% | abe: 3.688 | eve: 9.899 | bob: 3.932Epoch  13:  32% | abe: 3.688 | eve: 9.900 | bob: 3.932Epoch  13:  32% | abe: 3.686 | eve: 9.899 | bob: 3.930Epoch  13:  33% | abe: 3.685 | eve: 9.899 | bob: 3.929Epoch  13:  34% | abe: 3.683 | eve: 9.900 | bob: 3.927Epoch  13:  35% | abe: 3.682 | eve: 9.900 | bob: 3.926Epoch  13:  35% | abe: 3.681 | eve: 9.901 | bob: 3.925Epoch  13:  36% | abe: 3.680 | eve: 9.900 | bob: 3.924Epoch  13:  37% | abe: 3.678 | eve: 9.899 | bob: 3.922Epoch  13:  38% | abe: 3.678 | eve: 9.897 | bob: 3.922Epoch  13:  39% | abe: 3.676 | eve: 9.897 | bob: 3.920Epoch  13:  39% | abe: 3.674 | eve: 9.897 | bob: 3.918Epoch  13:  40% | abe: 3.673 | eve: 9.897 | bob: 3.917Epoch  13:  41% | abe: 3.672 | eve: 9.897 | bob: 3.916Epoch  13:  42% | abe: 3.670 | eve: 9.897 | bob: 3.914Epoch  13:  42% | abe: 3.669 | eve: 9.897 | bob: 3.913Epoch  13:  43% | abe: 3.668 | eve: 9.895 | bob: 3.912Epoch  13:  44% | abe: 3.667 | eve: 9.895 | bob: 3.911Epoch  13:  45% | abe: 3.667 | eve: 9.896 | bob: 3.910Epoch  13:  46% | abe: 3.666 | eve: 9.895 | bob: 3.910Epoch  13:  46% | abe: 3.665 | eve: 9.894 | bob: 3.909Epoch  13:  47% | abe: 3.664 | eve: 9.895 | bob: 3.908Epoch  13:  48% | abe: 3.663 | eve: 9.896 | bob: 3.908Epoch  13:  49% | abe: 3.662 | eve: 9.895 | bob: 3.907Epoch  13:  50% | abe: 3.661 | eve: 9.894 | bob: 3.906Epoch  13:  50% | abe: 3.660 | eve: 9.894 | bob: 3.906Epoch  13:  51% | abe: 3.659 | eve: 9.894 | bob: 3.904Epoch  13:  52% | abe: 3.658 | eve: 9.895 | bob: 3.904Epoch  13:  53% | abe: 3.657 | eve: 9.895 | bob: 3.903Epoch  13:  53% | abe: 3.656 | eve: 9.895 | bob: 3.902Epoch  13:  54% | abe: 3.654 | eve: 9.895 | bob: 3.901Epoch  13:  55% | abe: 3.654 | eve: 9.895 | bob: 3.900Epoch  13:  56% | abe: 3.652 | eve: 9.895 | bob: 3.899Epoch  13:  57% | abe: 3.651 | eve: 9.896 | bob: 3.897Epoch  13:  57% | abe: 3.650 | eve: 9.896 | bob: 3.897Epoch  13:  58% | abe: 3.649 | eve: 9.896 | bob: 3.896Epoch  13:  59% | abe: 3.648 | eve: 9.896 | bob: 3.894Epoch  13:  60% | abe: 3.646 | eve: 9.896 | bob: 3.893Epoch  13:  60% | abe: 3.645 | eve: 9.896 | bob: 3.892Epoch  13:  61% | abe: 3.644 | eve: 9.896 | bob: 3.891Epoch  13:  62% | abe: 3.643 | eve: 9.896 | bob: 3.890Epoch  13:  63% | abe: 3.642 | eve: 9.896 | bob: 3.889Epoch  13:  64% | abe: 3.640 | eve: 9.896 | bob: 3.887Epoch  13:  64% | abe: 3.639 | eve: 9.896 | bob: 3.887Epoch  13:  65% | abe: 3.638 | eve: 9.896 | bob: 3.885Epoch  13:  66% | abe: 3.637 | eve: 9.897 | bob: 3.884Epoch  13:  67% | abe: 3.635 | eve: 9.896 | bob: 3.883Epoch  13:  67% | abe: 3.634 | eve: 9.896 | bob: 3.882Epoch  13:  68% | abe: 3.633 | eve: 9.896 | bob: 3.881Epoch  13:  69% | abe: 3.632 | eve: 9.896 | bob: 3.880Epoch  13:  70% | abe: 3.631 | eve: 9.896 | bob: 3.879Epoch  13:  71% | abe: 3.630 | eve: 9.896 | bob: 3.878Epoch  13:  71% | abe: 3.629 | eve: 9.896 | bob: 3.877Epoch  13:  72% | abe: 3.628 | eve: 9.896 | bob: 3.876Epoch  13:  73% | abe: 3.626 | eve: 9.897 | bob: 3.875Epoch  13:  74% | abe: 3.625 | eve: 9.897 | bob: 3.874Epoch  13:  75% | abe: 3.624 | eve: 9.898 | bob: 3.873Epoch  13:  75% | abe: 3.622 | eve: 9.898 | bob: 3.871Epoch  13:  76% | abe: 3.621 | eve: 9.897 | bob: 3.871Epoch  13:  77% | abe: 3.620 | eve: 9.897 | bob: 3.869Epoch  13:  78% | abe: 3.619 | eve: 9.896 | bob: 3.868Epoch  13:  78% | abe: 3.617 | eve: 9.896 | bob: 3.867Epoch  13:  79% | abe: 3.616 | eve: 9.895 | bob: 3.866Epoch  13:  80% | abe: 3.615 | eve: 9.896 | bob: 3.865Epoch  13:  81% | abe: 3.614 | eve: 9.895 | bob: 3.864Epoch  13:  82% | abe: 3.614 | eve: 9.895 | bob: 3.863Epoch  13:  82% | abe: 3.612 | eve: 9.895 | bob: 3.862Epoch  13:  83% | abe: 3.611 | eve: 9.895 | bob: 3.861Epoch  13:  84% | abe: 3.610 | eve: 9.895 | bob: 3.860Epoch  13:  85% | abe: 3.609 | eve: 9.895 | bob: 3.859Epoch  13:  85% | abe: 3.608 | eve: 9.895 | bob: 3.859Epoch  13:  86% | abe: 3.607 | eve: 9.895 | bob: 3.858Epoch  13:  87% | abe: 3.606 | eve: 9.895 | bob: 3.857Epoch  13:  88% | abe: 3.605 | eve: 9.894 | bob: 3.856Epoch  13:  89% | abe: 3.604 | eve: 9.894 | bob: 3.855Epoch  13:  89% | abe: 3.602 | eve: 9.895 | bob: 3.854Epoch  13:  90% | abe: 3.601 | eve: 9.895 | bob: 3.852Epoch  13:  91% | abe: 3.600 | eve: 9.895 | bob: 3.851Epoch  13:  92% | abe: 3.598 | eve: 9.895 | bob: 3.850Epoch  13:  92% | abe: 3.597 | eve: 9.895 | bob: 3.849Epoch  13:  93% | abe: 3.596 | eve: 9.896 | bob: 3.848Epoch  13:  94% | abe: 3.595 | eve: 9.896 | bob: 3.847Epoch  13:  95% | abe: 3.594 | eve: 9.896 | bob: 3.845Epoch  13:  96% | abe: 3.592 | eve: 9.896 | bob: 3.844Epoch  13:  96% | abe: 3.591 | eve: 9.896 | bob: 3.843Epoch  13:  97% | abe: 3.590 | eve: 9.895 | bob: 3.842Epoch  13:  98% | abe: 3.589 | eve: 9.896 | bob: 3.841Epoch  13:  99% | abe: 3.588 | eve: 9.895 | bob: 3.840
New best Bob loss 3.840420969876277 at epoch 13
Epoch  14:   0% | abe: 3.419 | eve: 9.932 | bob: 3.688Epoch  14:   0% | abe: 3.431 | eve: 9.884 | bob: 3.701Epoch  14:   1% | abe: 3.435 | eve: 9.887 | bob: 3.706Epoch  14:   2% | abe: 3.430 | eve: 9.885 | bob: 3.702Epoch  14:   3% | abe: 3.436 | eve: 9.901 | bob: 3.709Epoch  14:   3% | abe: 3.439 | eve: 9.888 | bob: 3.711Epoch  14:   4% | abe: 3.438 | eve: 9.897 | bob: 3.710Epoch  14:   5% | abe: 3.438 | eve: 9.895 | bob: 3.711Epoch  14:   6% | abe: 3.436 | eve: 9.897 | bob: 3.708Epoch  14:   7% | abe: 3.433 | eve: 9.890 | bob: 3.706Epoch  14:   7% | abe: 3.433 | eve: 9.899 | bob: 3.707Epoch  14:   8% | abe: 3.431 | eve: 9.897 | bob: 3.705Epoch  14:   9% | abe: 3.427 | eve: 9.892 | bob: 3.700Epoch  14:  10% | abe: 3.426 | eve: 9.899 | bob: 3.699Epoch  14:  10% | abe: 3.424 | eve: 9.902 | bob: 3.697Epoch  14:  11% | abe: 3.425 | eve: 9.903 | bob: 3.699Epoch  14:  12% | abe: 3.424 | eve: 9.902 | bob: 3.697Epoch  14:  13% | abe: 3.425 | eve: 9.898 | bob: 3.699Epoch  14:  14% | abe: 3.423 | eve: 9.899 | bob: 3.697Epoch  14:  14% | abe: 3.421 | eve: 9.903 | bob: 3.694Epoch  14:  15% | abe: 3.420 | eve: 9.902 | bob: 3.693Epoch  14:  16% | abe: 3.420 | eve: 9.901 | bob: 3.693Epoch  14:  17% | abe: 3.419 | eve: 9.903 | bob: 3.692Epoch  14:  17% | abe: 3.417 | eve: 9.900 | bob: 3.690Epoch  14:  18% | abe: 3.416 | eve: 9.903 | bob: 3.690Epoch  14:  19% | abe: 3.415 | eve: 9.904 | bob: 3.689Epoch  14:  20% | abe: 3.415 | eve: 9.902 | bob: 3.689Epoch  14:  21% | abe: 3.411 | eve: 9.903 | bob: 3.686Epoch  14:  21% | abe: 3.410 | eve: 9.904 | bob: 3.684Epoch  14:  22% | abe: 3.409 | eve: 9.905 | bob: 3.684Epoch  14:  23% | abe: 3.408 | eve: 9.907 | bob: 3.683Epoch  14:  24% | abe: 3.407 | eve: 9.908 | bob: 3.682Epoch  14:  25% | abe: 3.406 | eve: 9.909 | bob: 3.681Epoch  14:  25% | abe: 3.405 | eve: 9.908 | bob: 3.680Epoch  14:  26% | abe: 3.404 | eve: 9.905 | bob: 3.679Epoch  14:  27% | abe: 3.403 | eve: 9.904 | bob: 3.678Epoch  14:  28% | abe: 3.403 | eve: 9.904 | bob: 3.679Epoch  14:  28% | abe: 3.403 | eve: 9.904 | bob: 3.679Epoch  14:  29% | abe: 3.401 | eve: 9.903 | bob: 3.677Epoch  14:  30% | abe: 3.400 | eve: 9.902 | bob: 3.676Epoch  14:  31% | abe: 3.399 | eve: 9.901 | bob: 3.675Epoch  14:  32% | abe: 3.399 | eve: 9.900 | bob: 3.675Epoch  14:  32% | abe: 3.398 | eve: 9.901 | bob: 3.675Epoch  14:  33% | abe: 3.398 | eve: 9.900 | bob: 3.673Epoch  14:  34% | abe: 3.396 | eve: 9.901 | bob: 3.672Epoch  14:  35% | abe: 3.394 | eve: 9.900 | bob: 3.670Epoch  14:  35% | abe: 3.393 | eve: 9.900 | bob: 3.669Epoch  14:  36% | abe: 3.392 | eve: 9.900 | bob: 3.669Epoch  14:  37% | abe: 3.391 | eve: 9.901 | bob: 3.668Epoch  14:  38% | abe: 3.389 | eve: 9.899 | bob: 3.666Epoch  14:  39% | abe: 3.388 | eve: 9.900 | bob: 3.664Epoch  14:  39% | abe: 3.386 | eve: 9.900 | bob: 3.663Epoch  14:  40% | abe: 3.384 | eve: 9.902 | bob: 3.662Epoch  14:  41% | abe: 3.383 | eve: 9.901 | bob: 3.660Epoch  14:  42% | abe: 3.382 | eve: 9.900 | bob: 3.660Epoch  14:  42% | abe: 3.381 | eve: 9.901 | bob: 3.659Epoch  14:  43% | abe: 3.381 | eve: 9.900 | bob: 3.658Epoch  14:  44% | abe: 3.380 | eve: 9.901 | bob: 3.657Epoch  14:  45% | abe: 3.379 | eve: 9.901 | bob: 3.656Epoch  14:  46% | abe: 3.378 | eve: 9.901 | bob: 3.656Epoch  14:  46% | abe: 3.377 | eve: 9.900 | bob: 3.655Epoch  14:  47% | abe: 3.376 | eve: 9.900 | bob: 3.654Epoch  14:  48% | abe: 3.374 | eve: 9.901 | bob: 3.653Epoch  14:  49% | abe: 3.374 | eve: 9.901 | bob: 3.652Epoch  14:  50% | abe: 3.372 | eve: 9.901 | bob: 3.651Epoch  14:  50% | abe: 3.371 | eve: 9.901 | bob: 3.650Epoch  14:  51% | abe: 3.370 | eve: 9.902 | bob: 3.649Epoch  14:  52% | abe: 3.369 | eve: 9.901 | bob: 3.647Epoch  14:  53% | abe: 3.368 | eve: 9.900 | bob: 3.647Epoch  14:  53% | abe: 3.367 | eve: 9.900 | bob: 3.646Epoch  14:  54% | abe: 3.366 | eve: 9.900 | bob: 3.646Epoch  14:  55% | abe: 3.365 | eve: 9.900 | bob: 3.645Epoch  14:  56% | abe: 3.364 | eve: 9.899 | bob: 3.644Epoch  14:  57% | abe: 3.363 | eve: 9.899 | bob: 3.642Epoch  14:  57% | abe: 3.362 | eve: 9.899 | bob: 3.641Epoch  14:  58% | abe: 3.360 | eve: 9.899 | bob: 3.640Epoch  14:  59% | abe: 3.359 | eve: 9.899 | bob: 3.639Epoch  14:  60% | abe: 3.358 | eve: 9.900 | bob: 3.639Epoch  14:  60% | abe: 3.358 | eve: 9.899 | bob: 3.638Epoch  14:  61% | abe: 3.356 | eve: 9.898 | bob: 3.637Epoch  14:  62% | abe: 3.355 | eve: 9.898 | bob: 3.636Epoch  14:  63% | abe: 3.355 | eve: 9.898 | bob: 3.635Epoch  14:  64% | abe: 3.353 | eve: 9.898 | bob: 3.634Epoch  14:  64% | abe: 3.353 | eve: 9.897 | bob: 3.634Epoch  14:  65% | abe: 3.352 | eve: 9.898 | bob: 3.633Epoch  14:  66% | abe: 3.351 | eve: 9.897 | bob: 3.632Epoch  14:  67% | abe: 3.350 | eve: 9.898 | bob: 3.631Epoch  14:  67% | abe: 3.349 | eve: 9.898 | bob: 3.631Epoch  14:  68% | abe: 3.348 | eve: 9.898 | bob: 3.630Epoch  14:  69% | abe: 3.347 | eve: 9.897 | bob: 3.629Epoch  14:  70% | abe: 3.346 | eve: 9.897 | bob: 3.628Epoch  14:  71% | abe: 3.346 | eve: 9.897 | bob: 3.627Epoch  14:  71% | abe: 3.344 | eve: 9.897 | bob: 3.626Epoch  14:  72% | abe: 3.343 | eve: 9.897 | bob: 3.625Epoch  14:  73% | abe: 3.342 | eve: 9.897 | bob: 3.624Epoch  14:  74% | abe: 3.342 | eve: 9.898 | bob: 3.624Epoch  14:  75% | abe: 3.341 | eve: 9.898 | bob: 3.623Epoch  14:  75% | abe: 3.340 | eve: 9.898 | bob: 3.622Epoch  14:  76% | abe: 3.340 | eve: 9.898 | bob: 3.622Epoch  14:  77% | abe: 3.339 | eve: 9.898 | bob: 3.621Epoch  14:  78% | abe: 3.338 | eve: 9.898 | bob: 3.621Epoch  14:  78% | abe: 3.338 | eve: 9.898 | bob: 3.620Epoch  14:  79% | abe: 3.337 | eve: 9.897 | bob: 3.620Epoch  14:  80% | abe: 3.336 | eve: 9.897 | bob: 3.619Epoch  14:  81% | abe: 3.335 | eve: 9.897 | bob: 3.619Epoch  14:  82% | abe: 3.335 | eve: 9.898 | bob: 3.618Epoch  14:  82% | abe: 3.334 | eve: 9.898 | bob: 3.618Epoch  14:  83% | abe: 3.333 | eve: 9.897 | bob: 3.617Epoch  14:  84% | abe: 3.333 | eve: 9.897 | bob: 3.617Epoch  14:  85% | abe: 3.332 | eve: 9.897 | bob: 3.616Epoch  14:  85% | abe: 3.331 | eve: 9.896 | bob: 3.616Epoch  14:  86% | abe: 3.331 | eve: 9.896 | bob: 3.615Epoch  14:  87% | abe: 3.330 | eve: 9.896 | bob: 3.615Epoch  14:  88% | abe: 3.330 | eve: 9.896 | bob: 3.615Epoch  14:  89% | abe: 3.329 | eve: 9.896 | bob: 3.615Epoch  14:  89% | abe: 3.328 | eve: 9.896 | bob: 3.614Epoch  14:  90% | abe: 3.328 | eve: 9.896 | bob: 3.614Epoch  14:  91% | abe: 3.327 | eve: 9.897 | bob: 3.613Epoch  14:  92% | abe: 3.327 | eve: 9.896 | bob: 3.613Epoch  14:  92% | abe: 3.326 | eve: 9.896 | bob: 3.612Epoch  14:  93% | abe: 3.325 | eve: 9.895 | bob: 3.612Epoch  14:  94% | abe: 3.325 | eve: 9.895 | bob: 3.611Epoch  14:  95% | abe: 3.324 | eve: 9.895 | bob: 3.611Epoch  14:  96% | abe: 3.323 | eve: 9.895 | bob: 3.611Epoch  14:  96% | abe: 3.323 | eve: 9.895 | bob: 3.610Epoch  14:  97% | abe: 3.322 | eve: 9.895 | bob: 3.610Epoch  14:  98% | abe: 3.322 | eve: 9.895 | bob: 3.609Epoch  14:  99% | abe: 3.321 | eve: 9.894 | bob: 3.609
New best Bob loss 3.6089050150185358 at epoch 14
Epoch  15:   0% | abe: 3.229 | eve: 9.861 | bob: 3.527Epoch  15:   0% | abe: 3.240 | eve: 9.871 | bob: 3.546Epoch  15:   1% | abe: 3.255 | eve: 9.876 | bob: 3.567Epoch  15:   2% | abe: 3.241 | eve: 9.873 | bob: 3.547Epoch  15:   3% | abe: 3.244 | eve: 9.882 | bob: 3.550Epoch  15:   3% | abe: 3.240 | eve: 9.885 | bob: 3.548Epoch  15:   4% | abe: 3.240 | eve: 9.894 | bob: 3.548Epoch  15:   5% | abe: 3.243 | eve: 9.891 | bob: 3.555Epoch  15:   6% | abe: 3.241 | eve: 9.901 | bob: 3.551Epoch  15:   7% | abe: 3.240 | eve: 9.903 | bob: 3.550Epoch  15:   7% | abe: 3.238 | eve: 9.894 | bob: 3.549Epoch  15:   8% | abe: 3.240 | eve: 9.895 | bob: 3.550Epoch  15:   9% | abe: 3.242 | eve: 9.895 | bob: 3.552Epoch  15:  10% | abe: 3.243 | eve: 9.894 | bob: 3.554Epoch  15:  10% | abe: 3.241 | eve: 9.892 | bob: 3.552Epoch  15:  11% | abe: 3.242 | eve: 9.894 | bob: 3.553Epoch  15:  12% | abe: 3.242 | eve: 9.894 | bob: 3.553Epoch  15:  13% | abe: 3.240 | eve: 9.895 | bob: 3.551Epoch  15:  14% | abe: 3.240 | eve: 9.895 | bob: 3.552Epoch  15:  14% | abe: 3.241 | eve: 9.896 | bob: 3.553Epoch  15:  15% | abe: 3.242 | eve: 9.898 | bob: 3.555Epoch  15:  16% | abe: 3.242 | eve: 9.899 | bob: 3.555Epoch  15:  17% | abe: 3.242 | eve: 9.893 | bob: 3.555Epoch  15:  17% | abe: 3.241 | eve: 9.893 | bob: 3.556Epoch  15:  18% | abe: 3.242 | eve: 9.890 | bob: 3.557Epoch  15:  19% | abe: 3.244 | eve: 9.888 | bob: 3.559Epoch  15:  20% | abe: 3.243 | eve: 9.890 | bob: 3.560Epoch  15:  21% | abe: 3.243 | eve: 9.891 | bob: 3.559Epoch  15:  21% | abe: 3.242 | eve: 9.893 | bob: 3.558Epoch  15:  22% | abe: 3.242 | eve: 9.891 | bob: 3.558Epoch  15:  23% | abe: 3.242 | eve: 9.891 | bob: 3.559Epoch  15:  24% | abe: 3.242 | eve: 9.890 | bob: 3.558Epoch  15:  25% | abe: 3.243 | eve: 9.891 | bob: 3.558Epoch  15:  25% | abe: 3.243 | eve: 9.894 | bob: 3.559Epoch  15:  26% | abe: 3.243 | eve: 9.894 | bob: 3.559Epoch  15:  27% | abe: 3.242 | eve: 9.893 | bob: 3.559Epoch  15:  28% | abe: 3.242 | eve: 9.892 | bob: 3.559Epoch  15:  28% | abe: 3.242 | eve: 9.891 | bob: 3.559Epoch  15:  29% | abe: 3.242 | eve: 9.891 | bob: 3.560Epoch  15:  30% | abe: 3.243 | eve: 9.891 | bob: 3.560Epoch  15:  31% | abe: 3.242 | eve: 9.890 | bob: 3.559Epoch  15:  32% | abe: 3.241 | eve: 9.890 | bob: 3.559Epoch  15:  32% | abe: 3.241 | eve: 9.891 | bob: 3.559Epoch  15:  33% | abe: 3.241 | eve: 9.892 | bob: 3.560Epoch  15:  34% | abe: 3.242 | eve: 9.891 | bob: 3.560Epoch  15:  35% | abe: 3.241 | eve: 9.892 | bob: 3.560Epoch  15:  35% | abe: 3.241 | eve: 9.891 | bob: 3.560Epoch  15:  36% | abe: 3.241 | eve: 9.891 | bob: 3.560Epoch  15:  37% | abe: 3.242 | eve: 9.891 | bob: 3.561Epoch  15:  38% | abe: 3.242 | eve: 9.892 | bob: 3.561Epoch  15:  39% | abe: 3.242 | eve: 9.892 | bob: 3.561Epoch  15:  39% | abe: 3.242 | eve: 9.892 | bob: 3.562Epoch  15:  40% | abe: 3.242 | eve: 9.891 | bob: 3.562Epoch  15:  41% | abe: 3.242 | eve: 9.892 | bob: 3.562Epoch  15:  42% | abe: 3.242 | eve: 9.893 | bob: 3.562Epoch  15:  42% | abe: 3.242 | eve: 9.892 | bob: 3.561Epoch  15:  43% | abe: 3.242 | eve: 9.891 | bob: 3.562Epoch  15:  44% | abe: 3.243 | eve: 9.892 | bob: 3.563Epoch  15:  45% | abe: 3.243 | eve: 9.892 | bob: 3.563Epoch  15:  46% | abe: 3.242 | eve: 9.892 | bob: 3.563Epoch  15:  46% | abe: 3.242 | eve: 9.892 | bob: 3.562Epoch  15:  47% | abe: 3.242 | eve: 9.894 | bob: 3.563Epoch  15:  48% | abe: 3.241 | eve: 9.893 | bob: 3.562Epoch  15:  49% | abe: 3.241 | eve: 9.894 | bob: 3.562Epoch  15:  50% | abe: 3.241 | eve: 9.895 | bob: 3.562Epoch  15:  50% | abe: 3.241 | eve: 9.895 | bob: 3.562Epoch  15:  51% | abe: 3.241 | eve: 9.895 | bob: 3.562Epoch  15:  52% | abe: 3.240 | eve: 9.895 | bob: 3.561Epoch  15:  53% | abe: 3.241 | eve: 9.895 | bob: 3.562Epoch  15:  53% | abe: 3.241 | eve: 9.895 | bob: 3.562Epoch  15:  54% | abe: 3.241 | eve: 9.894 | bob: 3.562Epoch  15:  55% | abe: 3.241 | eve: 9.892 | bob: 3.562Epoch  15:  56% | abe: 3.241 | eve: 9.893 | bob: 3.562Epoch  15:  57% | abe: 3.242 | eve: 9.893 | bob: 3.563Epoch  15:  57% | abe: 3.242 | eve: 9.893 | bob: 3.564Epoch  15:  58% | abe: 3.242 | eve: 9.893 | bob: 3.564Epoch  15:  59% | abe: 3.242 | eve: 9.893 | bob: 3.564Epoch  15:  60% | abe: 3.242 | eve: 9.893 | bob: 3.564Epoch  15:  60% | abe: 3.242 | eve: 9.892 | bob: 3.564Epoch  15:  61% | abe: 3.242 | eve: 9.891 | bob: 3.564Epoch  15:  62% | abe: 3.242 | eve: 9.891 | bob: 3.564Epoch  15:  63% | abe: 3.242 | eve: 9.892 | bob: 3.564Epoch  15:  64% | abe: 3.242 | eve: 9.891 | bob: 3.565Epoch  15:  64% | abe: 3.242 | eve: 9.891 | bob: 3.565Epoch  15:  65% | abe: 3.242 | eve: 9.891 | bob: 3.565Epoch  15:  66% | abe: 3.242 | eve: 9.891 | bob: 3.565Epoch  15:  67% | abe: 3.242 | eve: 9.891 | bob: 3.565Epoch  15:  67% | abe: 3.242 | eve: 9.890 | bob: 3.565Epoch  15:  68% | abe: 3.242 | eve: 9.890 | bob: 3.566Epoch  15:  69% | abe: 3.241 | eve: 9.890 | bob: 3.565Epoch  15:  70% | abe: 3.241 | eve: 9.890 | bob: 3.565Epoch  15:  71% | abe: 3.241 | eve: 9.890 | bob: 3.566Epoch  15:  71% | abe: 3.241 | eve: 9.890 | bob: 3.565Epoch  15:  72% | abe: 3.241 | eve: 9.890 | bob: 3.565Epoch  15:  73% | abe: 3.241 | eve: 9.890 | bob: 3.565Epoch  15:  74% | abe: 3.241 | eve: 9.891 | bob: 3.565Epoch  15:  75% | abe: 3.240 | eve: 9.891 | bob: 3.566Epoch  15:  75% | abe: 3.241 | eve: 9.891 | bob: 3.566Epoch  15:  76% | abe: 3.240 | eve: 9.892 | bob: 3.566Epoch  15:  77% | abe: 3.240 | eve: 9.892 | bob: 3.566Epoch  15:  78% | abe: 3.240 | eve: 9.892 | bob: 3.566Epoch  15:  78% | abe: 3.240 | eve: 9.893 | bob: 3.566Epoch  15:  79% | abe: 3.240 | eve: 9.893 | bob: 3.566Epoch  15:  80% | abe: 3.240 | eve: 9.894 | bob: 3.566Epoch  15:  81% | abe: 3.240 | eve: 9.894 | bob: 3.566Epoch  15:  82% | abe: 3.240 | eve: 9.895 | bob: 3.566Epoch  15:  82% | abe: 3.240 | eve: 9.895 | bob: 3.566Epoch  15:  83% | abe: 3.240 | eve: 9.895 | bob: 3.566Epoch  15:  84% | abe: 3.240 | eve: 9.895 | bob: 3.567Epoch  15:  85% | abe: 3.240 | eve: 9.894 | bob: 3.567Epoch  15:  85% | abe: 3.240 | eve: 9.894 | bob: 3.567Epoch  15:  86% | abe: 3.240 | eve: 9.894 | bob: 3.567Epoch  15:  87% | abe: 3.240 | eve: 9.894 | bob: 3.567Epoch  15:  88% | abe: 3.240 | eve: 9.893 | bob: 3.567Epoch  15:  89% | abe: 3.240 | eve: 9.893 | bob: 3.568Epoch  15:  89% | abe: 3.240 | eve: 9.893 | bob: 3.568Epoch  15:  90% | abe: 3.240 | eve: 9.892 | bob: 3.568Epoch  15:  91% | abe: 3.240 | eve: 9.891 | bob: 3.568Epoch  15:  92% | abe: 3.240 | eve: 9.891 | bob: 3.568Epoch  15:  92% | abe: 3.240 | eve: 9.892 | bob: 3.568Epoch  15:  93% | abe: 3.240 | eve: 9.892 | bob: 3.568Epoch  15:  94% | abe: 3.240 | eve: 9.893 | bob: 3.568Epoch  15:  95% | abe: 3.240 | eve: 9.892 | bob: 3.568Epoch  15:  96% | abe: 3.239 | eve: 9.893 | bob: 3.568Epoch  15:  96% | abe: 3.239 | eve: 9.893 | bob: 3.568Epoch  15:  97% | abe: 3.239 | eve: 9.892 | bob: 3.568Epoch  15:  98% | abe: 3.239 | eve: 9.892 | bob: 3.568Epoch  15:  99% | abe: 3.239 | eve: 9.892 | bob: 3.568
New best Bob loss 3.567648114336862 at epoch 15
Epoch  16:   0% | abe: 3.216 | eve: 9.881 | bob: 3.559Epoch  16:   0% | abe: 3.221 | eve: 9.890 | bob: 3.572Epoch  16:   1% | abe: 3.227 | eve: 9.900 | bob: 3.578Epoch  16:   2% | abe: 3.227 | eve: 9.902 | bob: 3.579Epoch  16:   3% | abe: 3.230 | eve: 9.907 | bob: 3.582Epoch  16:   3% | abe: 3.229 | eve: 9.900 | bob: 3.582Epoch  16:   4% | abe: 3.230 | eve: 9.912 | bob: 3.584Epoch  16:   5% | abe: 3.232 | eve: 9.907 | bob: 3.586Epoch  16:   6% | abe: 3.228 | eve: 9.910 | bob: 3.583Epoch  16:   7% | abe: 3.226 | eve: 9.910 | bob: 3.582Epoch  16:   7% | abe: 3.227 | eve: 9.906 | bob: 3.583Epoch  16:   8% | abe: 3.227 | eve: 9.905 | bob: 3.582Epoch  16:   9% | abe: 3.227 | eve: 9.903 | bob: 3.583Epoch  16:  10% | abe: 3.228 | eve: 9.903 | bob: 3.584Epoch  16:  10% | abe: 3.227 | eve: 9.906 | bob: 3.583Epoch  16:  11% | abe: 3.225 | eve: 9.911 | bob: 3.580Epoch  16:  12% | abe: 3.226 | eve: 9.910 | bob: 3.581Epoch  16:  13% | abe: 3.227 | eve: 9.910 | bob: 3.582Epoch  16:  14% | abe: 3.228 | eve: 9.911 | bob: 3.584Epoch  16:  14% | abe: 3.229 | eve: 9.909 | bob: 3.584Epoch  16:  15% | abe: 3.228 | eve: 9.906 | bob: 3.582Epoch  16:  16% | abe: 3.227 | eve: 9.906 | bob: 3.582Epoch  16:  17% | abe: 3.228 | eve: 9.903 | bob: 3.583Epoch  16:  17% | abe: 3.228 | eve: 9.905 | bob: 3.582Epoch  16:  18% | abe: 3.228 | eve: 9.905 | bob: 3.583Epoch  16:  19% | abe: 3.228 | eve: 9.907 | bob: 3.582Epoch  16:  20% | abe: 3.229 | eve: 9.905 | bob: 3.584Epoch  16:  21% | abe: 3.229 | eve: 9.902 | bob: 3.584Epoch  16:  21% | abe: 3.229 | eve: 9.903 | bob: 3.585Epoch  16:  22% | abe: 3.229 | eve: 9.903 | bob: 3.585Epoch  16:  23% | abe: 3.230 | eve: 9.904 | bob: 3.585Epoch  16:  24% | abe: 3.230 | eve: 9.905 | bob: 3.586Epoch  16:  25% | abe: 3.230 | eve: 9.906 | bob: 3.585Epoch  16:  25% | abe: 3.231 | eve: 9.906 | bob: 3.587Epoch  16:  26% | abe: 3.231 | eve: 9.905 | bob: 3.587Epoch  16:  27% | abe: 3.232 | eve: 9.905 | bob: 3.588Epoch  16:  28% | abe: 3.231 | eve: 9.904 | bob: 3.587Epoch  16:  28% | abe: 3.231 | eve: 9.904 | bob: 3.588Epoch  16:  29% | abe: 3.232 | eve: 9.902 | bob: 3.588Epoch  16:  30% | abe: 3.233 | eve: 9.902 | bob: 3.589Epoch  16:  31% | abe: 3.233 | eve: 9.901 | bob: 3.590Epoch  16:  32% | abe: 3.232 | eve: 9.901 | bob: 3.589Epoch  16:  32% | abe: 3.232 | eve: 9.900 | bob: 3.589Epoch  16:  33% | abe: 3.232 | eve: 9.899 | bob: 3.589Epoch  16:  34% | abe: 3.232 | eve: 9.900 | bob: 3.589Epoch  16:  35% | abe: 3.232 | eve: 9.898 | bob: 3.589Epoch  16:  35% | abe: 3.233 | eve: 9.897 | bob: 3.590Epoch  16:  36% | abe: 3.232 | eve: 9.897 | bob: 3.589Epoch  16:  37% | abe: 3.232 | eve: 9.897 | bob: 3.590Epoch  16:  38% | abe: 3.231 | eve: 9.895 | bob: 3.589Epoch  16:  39% | abe: 3.231 | eve: 9.896 | bob: 3.589Epoch  16:  39% | abe: 3.231 | eve: 9.896 | bob: 3.589Epoch  16:  40% | abe: 3.230 | eve: 9.897 | bob: 3.589Epoch  16:  41% | abe: 3.231 | eve: 9.897 | bob: 3.589Epoch  16:  42% | abe: 3.230 | eve: 9.897 | bob: 3.589Epoch  16:  42% | abe: 3.230 | eve: 9.897 | bob: 3.589Epoch  16:  43% | abe: 3.230 | eve: 9.896 | bob: 3.589Epoch  16:  44% | abe: 3.230 | eve: 9.896 | bob: 3.589Epoch  16:  45% | abe: 3.230 | eve: 9.897 | bob: 3.589Epoch  16:  46% | abe: 3.230 | eve: 9.896 | bob: 3.589Epoch  16:  46% | abe: 3.230 | eve: 9.896 | bob: 3.590Epoch  16:  47% | abe: 3.230 | eve: 9.896 | bob: 3.590Epoch  16:  48% | abe: 3.231 | eve: 9.895 | bob: 3.591Epoch  16:  49% | abe: 3.231 | eve: 9.895 | bob: 3.591Epoch  16:  50% | abe: 3.230 | eve: 9.894 | bob: 3.591Epoch  16:  50% | abe: 3.230 | eve: 9.893 | bob: 3.590Epoch  16:  51% | abe: 3.229 | eve: 9.892 | bob: 3.590Epoch  16:  52% | abe: 3.229 | eve: 9.893 | bob: 3.590Epoch  16:  53% | abe: 3.229 | eve: 9.892 | bob: 3.590Epoch  16:  53% | abe: 3.229 | eve: 9.892 | bob: 3.589Epoch  16:  54% | abe: 3.228 | eve: 9.892 | bob: 3.589Epoch  16:  55% | abe: 3.228 | eve: 9.892 | bob: 3.589Epoch  16:  56% | abe: 3.228 | eve: 9.893 | bob: 3.589Epoch  16:  57% | abe: 3.228 | eve: 9.893 | bob: 3.589Epoch  16:  57% | abe: 3.228 | eve: 9.893 | bob: 3.589Epoch  16:  58% | abe: 3.228 | eve: 9.893 | bob: 3.589Epoch  16:  59% | abe: 3.227 | eve: 9.892 | bob: 3.589Epoch  16:  60% | abe: 3.227 | eve: 9.893 | bob: 3.589Epoch  16:  60% | abe: 3.227 | eve: 9.893 | bob: 3.589Epoch  16:  61% | abe: 3.227 | eve: 9.894 | bob: 3.589Epoch  16:  62% | abe: 3.227 | eve: 9.894 | bob: 3.589Epoch  16:  63% | abe: 3.226 | eve: 9.894 | bob: 3.588Epoch  16:  64% | abe: 3.226 | eve: 9.895 | bob: 3.589Epoch  16:  64% | abe: 3.226 | eve: 9.894 | bob: 3.589Epoch  16:  65% | abe: 3.226 | eve: 9.894 | bob: 3.589Epoch  16:  66% | abe: 3.225 | eve: 9.893 | bob: 3.588Epoch  16:  67% | abe: 3.225 | eve: 9.893 | bob: 3.588Epoch  16:  67% | abe: 3.225 | eve: 9.893 | bob: 3.588Epoch  16:  68% | abe: 3.225 | eve: 9.893 | bob: 3.588Epoch  16:  69% | abe: 3.225 | eve: 9.893 | bob: 3.588Epoch  16:  70% | abe: 3.225 | eve: 9.894 | bob: 3.588Epoch  16:  71% | abe: 3.225 | eve: 9.894 | bob: 3.588Epoch  16:  71% | abe: 3.224 | eve: 9.894 | bob: 3.588Epoch  16:  72% | abe: 3.224 | eve: 9.894 | bob: 3.588Epoch  16:  73% | abe: 3.225 | eve: 9.895 | bob: 3.588Epoch  16:  74% | abe: 3.225 | eve: 9.895 | bob: 3.589Epoch  16:  75% | abe: 3.225 | eve: 9.896 | bob: 3.589Epoch  16:  75% | abe: 3.225 | eve: 9.896 | bob: 3.589Epoch  16:  76% | abe: 3.225 | eve: 9.896 | bob: 3.589Epoch  16:  77% | abe: 3.225 | eve: 9.897 | bob: 3.589Epoch  16:  78% | abe: 3.225 | eve: 9.897 | bob: 3.589Epoch  16:  78% | abe: 3.225 | eve: 9.897 | bob: 3.590Epoch  16:  79% | abe: 3.225 | eve: 9.897 | bob: 3.590Epoch  16:  80% | abe: 3.225 | eve: 9.896 | bob: 3.590Epoch  16:  81% | abe: 3.225 | eve: 9.897 | bob: 3.590Epoch  16:  82% | abe: 3.225 | eve: 9.897 | bob: 3.590Epoch  16:  82% | abe: 3.224 | eve: 9.897 | bob: 3.589Epoch  16:  83% | abe: 3.225 | eve: 9.896 | bob: 3.590Epoch  16:  84% | abe: 3.224 | eve: 9.895 | bob: 3.590Epoch  16:  85% | abe: 3.225 | eve: 9.895 | bob: 3.590Epoch  16:  85% | abe: 3.224 | eve: 9.895 | bob: 3.590Epoch  16:  86% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  87% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  88% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  89% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  89% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  90% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  91% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  92% | abe: 3.224 | eve: 9.896 | bob: 3.590Epoch  16:  92% | abe: 3.223 | eve: 9.896 | bob: 3.589Epoch  16:  93% | abe: 3.223 | eve: 9.897 | bob: 3.589Epoch  16:  94% | abe: 3.223 | eve: 9.896 | bob: 3.589Epoch  16:  95% | abe: 3.223 | eve: 9.896 | bob: 3.589Epoch  16:  96% | abe: 3.223 | eve: 9.895 | bob: 3.589Epoch  16:  96% | abe: 3.223 | eve: 9.895 | bob: 3.589Epoch  16:  97% | abe: 3.222 | eve: 9.896 | bob: 3.589Epoch  16:  98% | abe: 3.223 | eve: 9.896 | bob: 3.589Epoch  16:  99% | abe: 3.223 | eve: 9.897 | bob: 3.589Epoch  17:   0% | abe: 3.189 | eve: 9.849 | bob: 3.567Epoch  17:   0% | abe: 3.217 | eve: 9.871 | bob: 3.586Epoch  17:   1% | abe: 3.228 | eve: 9.863 | bob: 3.597Epoch  17:   2% | abe: 3.228 | eve: 9.880 | bob: 3.606Epoch  17:   3% | abe: 3.224 | eve: 9.891 | bob: 3.604Epoch  17:   3% | abe: 3.226 | eve: 9.886 | bob: 3.606Epoch  17:   4% | abe: 3.221 | eve: 9.883 | bob: 3.602Epoch  17:   5% | abe: 3.220 | eve: 9.886 | bob: 3.602Epoch  17:   6% | abe: 3.217 | eve: 9.890 | bob: 3.600Epoch  17:   7% | abe: 3.216 | eve: 9.892 | bob: 3.599Epoch  17:   7% | abe: 3.215 | eve: 9.891 | bob: 3.599Epoch  17:   8% | abe: 3.215 | eve: 9.887 | bob: 3.598Epoch  17:   9% | abe: 3.217 | eve: 9.894 | bob: 3.600Epoch  17:  10% | abe: 3.216 | eve: 9.889 | bob: 3.598Epoch  17:  10% | abe: 3.212 | eve: 9.893 | bob: 3.594Epoch  17:  11% | abe: 3.212 | eve: 9.891 | bob: 3.595Epoch  17:  12% | abe: 3.214 | eve: 9.892 | bob: 3.595Epoch  17:  13% | abe: 3.215 | eve: 9.890 | bob: 3.596Epoch  17:  14% | abe: 3.215 | eve: 9.893 | bob: 3.596Epoch  17:  14% | abe: 3.214 | eve: 9.891 | bob: 3.595Epoch  17:  15% | abe: 3.213 | eve: 9.889 | bob: 3.594Epoch  17:  16% | abe: 3.213 | eve: 9.889 | bob: 3.595Epoch  17:  17% | abe: 3.212 | eve: 9.890 | bob: 3.594Epoch  17:  17% | abe: 3.211 | eve: 9.889 | bob: 3.593Epoch  17:  18% | abe: 3.210 | eve: 9.887 | bob: 3.591Epoch  17:  19% | abe: 3.209 | eve: 9.886 | bob: 3.590Epoch  17:  20% | abe: 3.209 | eve: 9.887 | bob: 3.591Epoch  17:  21% | abe: 3.209 | eve: 9.887 | bob: 3.591Epoch  17:  21% | abe: 3.209 | eve: 9.887 | bob: 3.591Epoch  17:  22% | abe: 3.208 | eve: 9.886 | bob: 3.590Epoch  17:  23% | abe: 3.208 | eve: 9.888 | bob: 3.590Epoch  17:  24% | abe: 3.209 | eve: 9.889 | bob: 3.590Epoch  17:  25% | abe: 3.209 | eve: 9.890 | bob: 3.590Epoch  17:  25% | abe: 3.209 | eve: 9.892 | bob: 3.590Epoch  17:  26% | abe: 3.210 | eve: 9.891 | bob: 3.592Epoch  17:  27% | abe: 3.210 | eve: 9.891 | bob: 3.592Epoch  17:  28% | abe: 3.210 | eve: 9.890 | bob: 3.592Epoch  17:  28% | abe: 3.210 | eve: 9.890 | bob: 3.592Epoch  17:  29% | abe: 3.211 | eve: 9.888 | bob: 3.593Epoch  17:  30% | abe: 3.211 | eve: 9.887 | bob: 3.593Epoch  17:  31% | abe: 3.211 | eve: 9.887 | bob: 3.593Epoch  17:  32% | abe: 3.210 | eve: 9.888 | bob: 3.593Epoch  17:  32% | abe: 3.210 | eve: 9.887 | bob: 3.592Epoch  17:  33% | abe: 3.209 | eve: 9.888 | bob: 3.592Epoch  17:  34% | abe: 3.209 | eve: 9.888 | bob: 3.592Epoch  17:  35% | abe: 3.209 | eve: 9.889 | bob: 3.591Epoch  17:  35% | abe: 3.208 | eve: 9.889 | bob: 3.591Epoch  17:  36% | abe: 3.207 | eve: 9.888 | bob: 3.590Epoch  17:  37% | abe: 3.207 | eve: 9.888 | bob: 3.590Epoch  17:  38% | abe: 3.207 | eve: 9.888 | bob: 3.591Epoch  17:  39% | abe: 3.207 | eve: 9.888 | bob: 3.590Epoch  17:  39% | abe: 3.207 | eve: 9.887 | bob: 3.590Epoch  17:  40% | abe: 3.208 | eve: 9.888 | bob: 3.591Epoch  17:  41% | abe: 3.208 | eve: 9.888 | bob: 3.592Epoch  17:  42% | abe: 3.208 | eve: 9.888 | bob: 3.593Epoch  17:  42% | abe: 3.209 | eve: 9.889 | bob: 3.593Epoch  17:  43% | abe: 3.209 | eve: 9.889 | bob: 3.594Epoch  17:  44% | abe: 3.208 | eve: 9.888 | bob: 3.593Epoch  17:  45% | abe: 3.208 | eve: 9.888 | bob: 3.593Epoch  17:  46% | abe: 3.208 | eve: 9.889 | bob: 3.593Epoch  17:  46% | abe: 3.208 | eve: 9.888 | bob: 3.593Epoch  17:  47% | abe: 3.208 | eve: 9.888 | bob: 3.593Epoch  17:  48% | abe: 3.208 | eve: 9.888 | bob: 3.594Epoch  17:  49% | abe: 3.208 | eve: 9.889 | bob: 3.594Epoch  17:  50% | abe: 3.208 | eve: 9.888 | bob: 3.594Epoch  17:  50% | abe: 3.209 | eve: 9.887 | bob: 3.594Epoch  17:  51% | abe: 3.209 | eve: 9.887 | bob: 3.594Epoch  17:  52% | abe: 3.209 | eve: 9.886 | bob: 3.595Epoch  17:  53% | abe: 3.208 | eve: 9.886 | bob: 3.594Epoch  17:  53% | abe: 3.208 | eve: 9.886 | bob: 3.594Epoch  17:  54% | abe: 3.208 | eve: 9.885 | bob: 3.594Epoch  17:  55% | abe: 3.208 | eve: 9.886 | bob: 3.594Epoch  17:  56% | abe: 3.208 | eve: 9.885 | bob: 3.594Epoch  17:  57% | abe: 3.207 | eve: 9.885 | bob: 3.594Epoch  17:  57% | abe: 3.207 | eve: 9.885 | bob: 3.593Epoch  17:  58% | abe: 3.207 | eve: 9.885 | bob: 3.593Epoch  17:  59% | abe: 3.206 | eve: 9.885 | bob: 3.593Epoch  17:  60% | abe: 3.206 | eve: 9.885 | bob: 3.593Epoch  17:  60% | abe: 3.206 | eve: 9.885 | bob: 3.593Epoch  17:  61% | abe: 3.206 | eve: 9.885 | bob: 3.593Epoch  17:  62% | abe: 3.206 | eve: 9.885 | bob: 3.593Epoch  17:  63% | abe: 3.206 | eve: 9.886 | bob: 3.593Epoch  17:  64% | abe: 3.206 | eve: 9.886 | bob: 3.593Epoch  17:  64% | abe: 3.206 | eve: 9.887 | bob: 3.593Epoch  17:  65% | abe: 3.206 | eve: 9.886 | bob: 3.594Epoch  17:  66% | abe: 3.206 | eve: 9.885 | bob: 3.594Epoch  17:  67% | abe: 3.206 | eve: 9.884 | bob: 3.594Epoch  17:  67% | abe: 3.206 | eve: 9.884 | bob: 3.594Epoch  17:  68% | abe: 3.206 | eve: 9.884 | bob: 3.594Epoch  17:  69% | abe: 3.206 | eve: 9.883 | bob: 3.594Epoch  17:  70% | abe: 3.206 | eve: 9.884 | bob: 3.594Epoch  17:  71% | abe: 3.206 | eve: 9.884 | bob: 3.595Epoch  17:  71% | abe: 3.206 | eve: 9.883 | bob: 3.595Epoch  17:  72% | abe: 3.206 | eve: 9.883 | bob: 3.595Epoch  17:  73% | abe: 3.205 | eve: 9.883 | bob: 3.595Epoch  17:  74% | abe: 3.205 | eve: 9.882 | bob: 3.594Epoch  17:  75% | abe: 3.205 | eve: 9.883 | bob: 3.594Epoch  17:  75% | abe: 3.205 | eve: 9.883 | bob: 3.595Epoch  17:  76% | abe: 3.205 | eve: 9.883 | bob: 3.595Epoch  17:  77% | abe: 3.205 | eve: 9.883 | bob: 3.595Epoch  17:  78% | abe: 3.205 | eve: 9.882 | bob: 3.595Epoch  17:  78% | abe: 3.205 | eve: 9.882 | bob: 3.595Epoch  17:  79% | abe: 3.205 | eve: 9.881 | bob: 3.595Epoch  17:  80% | abe: 3.205 | eve: 9.881 | bob: 3.595Epoch  17:  81% | abe: 3.205 | eve: 9.881 | bob: 3.595Epoch  17:  82% | abe: 3.205 | eve: 9.882 | bob: 3.596Epoch  17:  82% | abe: 3.205 | eve: 9.882 | bob: 3.596Epoch  17:  83% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  84% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  85% | abe: 3.205 | eve: 9.880 | bob: 3.596Epoch  17:  85% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  86% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  87% | abe: 3.205 | eve: 9.880 | bob: 3.596Epoch  17:  88% | abe: 3.205 | eve: 9.880 | bob: 3.596Epoch  17:  89% | abe: 3.205 | eve: 9.880 | bob: 3.596Epoch  17:  89% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  90% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  91% | abe: 3.205 | eve: 9.881 | bob: 3.596Epoch  17:  92% | abe: 3.205 | eve: 9.882 | bob: 3.596Epoch  17:  92% | abe: 3.205 | eve: 9.882 | bob: 3.596Epoch  17:  93% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  94% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  95% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  96% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  96% | abe: 3.205 | eve: 9.882 | bob: 3.597Epoch  17:  97% | abe: 3.205 | eve: 9.881 | bob: 3.597Epoch  17:  98% | abe: 3.205 | eve: 9.881 | bob: 3.597Epoch  17:  99% | abe: 3.205 | eve: 9.880 | bob: 3.597Epoch  18:   0% | abe: 3.195 | eve: 9.899 | bob: 3.603Epoch  18:   0% | abe: 3.177 | eve: 9.841 | bob: 3.593Epoch  18:   1% | abe: 3.178 | eve: 9.858 | bob: 3.593Epoch  18:   2% | abe: 3.184 | eve: 9.873 | bob: 3.593Epoch  18:   3% | abe: 3.184 | eve: 9.865 | bob: 3.592Epoch  18:   3% | abe: 3.185 | eve: 9.869 | bob: 3.596Epoch  18:   4% | abe: 3.186 | eve: 9.865 | bob: 3.597Epoch  18:   5% | abe: 3.191 | eve: 9.874 | bob: 3.603Epoch  18:   6% | abe: 3.187 | eve: 9.872 | bob: 3.599Epoch  18:   7% | abe: 3.190 | eve: 9.864 | bob: 3.600Epoch  18:   7% | abe: 3.192 | eve: 9.861 | bob: 3.603Epoch  18:   8% | abe: 3.193 | eve: 9.866 | bob: 3.605Epoch  18:   9% | abe: 3.195 | eve: 9.868 | bob: 3.606Epoch  18:  10% | abe: 3.196 | eve: 9.870 | bob: 3.607Epoch  18:  10% | abe: 3.195 | eve: 9.870 | bob: 3.606Epoch  18:  11% | abe: 3.194 | eve: 9.871 | bob: 3.605Epoch  18:  12% | abe: 3.196 | eve: 9.869 | bob: 3.606Epoch  18:  13% | abe: 3.194 | eve: 9.869 | bob: 3.604Epoch  18:  14% | abe: 3.195 | eve: 9.867 | bob: 3.604Epoch  18:  14% | abe: 3.196 | eve: 9.866 | bob: 3.605Epoch  18:  15% | abe: 3.197 | eve: 9.865 | bob: 3.606Epoch  18:  16% | abe: 3.198 | eve: 9.868 | bob: 3.607Epoch  18:  17% | abe: 3.199 | eve: 9.868 | bob: 3.609Epoch  18:  17% | abe: 3.199 | eve: 9.864 | bob: 3.609Epoch  18:  18% | abe: 3.200 | eve: 9.864 | bob: 3.609Epoch  18:  19% | abe: 3.199 | eve: 9.864 | bob: 3.608Epoch  18:  20% | abe: 3.200 | eve: 9.862 | bob: 3.610Epoch  18:  21% | abe: 3.201 | eve: 9.863 | bob: 3.611Epoch  18:  21% | abe: 3.201 | eve: 9.862 | bob: 3.611Epoch  18:  22% | abe: 3.201 | eve: 9.862 | bob: 3.611Epoch  18:  23% | abe: 3.201 | eve: 9.860 | bob: 3.610Epoch  18:  24% | abe: 3.201 | eve: 9.861 | bob: 3.610Epoch  18:  25% | abe: 3.201 | eve: 9.860 | bob: 3.612Epoch  18:  25% | abe: 3.201 | eve: 9.858 | bob: 3.612Epoch  18:  26% | abe: 3.201 | eve: 9.858 | bob: 3.611Epoch  18:  27% | abe: 3.201 | eve: 9.859 | bob: 3.612Epoch  18:  28% | abe: 3.202 | eve: 9.859 | bob: 3.613Epoch  18:  28% | abe: 3.202 | eve: 9.858 | bob: 3.614Epoch  18:  29% | abe: 3.202 | eve: 9.858 | bob: 3.614Epoch  18:  30% | abe: 3.202 | eve: 9.857 | bob: 3.613Epoch  18:  31% | abe: 3.202 | eve: 9.855 | bob: 3.613Epoch  18:  32% | abe: 3.202 | eve: 9.855 | bob: 3.612Epoch  18:  32% | abe: 3.202 | eve: 9.856 | bob: 3.612Epoch  18:  33% | abe: 3.201 | eve: 9.855 | bob: 3.612Epoch  18:  34% | abe: 3.201 | eve: 9.855 | bob: 3.612Epoch  18:  35% | abe: 3.201 | eve: 9.855 | bob: 3.613Epoch  18:  35% | abe: 3.201 | eve: 9.856 | bob: 3.613Epoch  18:  36% | abe: 3.202 | eve: 9.855 | bob: 3.613Epoch  18:  37% | abe: 3.202 | eve: 9.856 | bob: 3.613Epoch  18:  38% | abe: 3.203 | eve: 9.856 | bob: 3.614Epoch  18:  39% | abe: 3.204 | eve: 9.856 | bob: 3.615Epoch  18:  39% | abe: 3.203 | eve: 9.855 | bob: 3.615Epoch  18:  40% | abe: 3.203 | eve: 9.855 | bob: 3.614Epoch  18:  41% | abe: 3.203 | eve: 9.854 | bob: 3.614Epoch  18:  42% | abe: 3.203 | eve: 9.855 | bob: 3.615Epoch  18:  42% | abe: 3.202 | eve: 9.855 | bob: 3.614Epoch  18:  43% | abe: 3.202 | eve: 9.856 | bob: 3.614Epoch  18:  44% | abe: 3.201 | eve: 9.857 | bob: 3.613Epoch  18:  45% | abe: 3.201 | eve: 9.858 | bob: 3.613Epoch  18:  46% | abe: 3.201 | eve: 9.859 | bob: 3.613Epoch  18:  46% | abe: 3.201 | eve: 9.859 | bob: 3.613Epoch  18:  47% | abe: 3.202 | eve: 9.859 | bob: 3.614Epoch  18:  48% | abe: 3.202 | eve: 9.859 | bob: 3.613Epoch  18:  49% | abe: 3.201 | eve: 9.859 | bob: 3.613Epoch  18:  50% | abe: 3.201 | eve: 9.858 | bob: 3.613Epoch  18:  50% | abe: 3.201 | eve: 9.859 | bob: 3.613Epoch  18:  51% | abe: 3.201 | eve: 9.860 | bob: 3.613Epoch  18:  52% | abe: 3.201 | eve: 9.861 | bob: 3.613Epoch  18:  53% | abe: 3.201 | eve: 9.861 | bob: 3.613Epoch  18:  53% | abe: 3.201 | eve: 9.861 | bob: 3.613Epoch  18:  54% | abe: 3.200 | eve: 9.862 | bob: 3.613Epoch  18:  55% | abe: 3.200 | eve: 9.862 | bob: 3.612Epoch  18:  56% | abe: 3.199 | eve: 9.863 | bob: 3.612Epoch  18:  57% | abe: 3.199 | eve: 9.862 | bob: 3.611Epoch  18:  57% | abe: 3.199 | eve: 9.861 | bob: 3.611Epoch  18:  58% | abe: 3.198 | eve: 9.861 | bob: 3.610Epoch  18:  59% | abe: 3.198 | eve: 9.860 | bob: 3.610Epoch  18:  60% | abe: 3.198 | eve: 9.860 | bob: 3.610Epoch  18:  60% | abe: 3.198 | eve: 9.860 | bob: 3.610Epoch  18:  61% | abe: 3.198 | eve: 9.861 | bob: 3.610Epoch  18:  62% | abe: 3.198 | eve: 9.862 | bob: 3.610Epoch  18:  63% | abe: 3.198 | eve: 9.862 | bob: 3.610Epoch  18:  64% | abe: 3.198 | eve: 9.861 | bob: 3.611Epoch  18:  64% | abe: 3.198 | eve: 9.862 | bob: 3.611Epoch  18:  65% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  66% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  67% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  67% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  68% | abe: 3.198 | eve: 9.863 | bob: 3.610Epoch  18:  69% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  70% | abe: 3.198 | eve: 9.863 | bob: 3.611Epoch  18:  71% | abe: 3.197 | eve: 9.864 | bob: 3.610Epoch  18:  71% | abe: 3.197 | eve: 9.863 | bob: 3.610Epoch  18:  72% | abe: 3.197 | eve: 9.864 | bob: 3.610Epoch  18:  73% | abe: 3.197 | eve: 9.864 | bob: 3.610Epoch  18:  74% | abe: 3.197 | eve: 9.864 | bob: 3.609Epoch  18:  75% | abe: 3.197 | eve: 9.864 | bob: 3.609Epoch  18:  75% | abe: 3.196 | eve: 9.863 | bob: 3.609Epoch  18:  76% | abe: 3.196 | eve: 9.863 | bob: 3.609Epoch  18:  77% | abe: 3.196 | eve: 9.864 | bob: 3.609Epoch  18:  78% | abe: 3.196 | eve: 9.864 | bob: 3.609Epoch  18:  78% | abe: 3.196 | eve: 9.864 | bob: 3.609Epoch  18:  79% | abe: 3.196 | eve: 9.864 | bob: 3.610Epoch  18:  80% | abe: 3.196 | eve: 9.863 | bob: 3.610Epoch  18:  81% | abe: 3.196 | eve: 9.864 | bob: 3.610Epoch  18:  82% | abe: 3.196 | eve: 9.864 | bob: 3.610Epoch  18:  82% | abe: 3.196 | eve: 9.865 | bob: 3.610Epoch  18:  83% | abe: 3.196 | eve: 9.864 | bob: 3.610Epoch  18:  84% | abe: 3.196 | eve: 9.864 | bob: 3.610Epoch  18:  85% | abe: 3.196 | eve: 9.865 | bob: 3.610Epoch  18:  85% | abe: 3.196 | eve: 9.866 | bob: 3.610Epoch  18:  86% | abe: 3.196 | eve: 9.866 | bob: 3.610Epoch  18:  87% | abe: 3.196 | eve: 9.866 | bob: 3.610Epoch  18:  88% | abe: 3.196 | eve: 9.866 | bob: 3.610Epoch  18:  89% | abe: 3.196 | eve: 9.865 | bob: 3.611Epoch  18:  89% | abe: 3.196 | eve: 9.866 | bob: 3.611Epoch  18:  90% | abe: 3.196 | eve: 9.865 | bob: 3.610Epoch  18:  91% | abe: 3.195 | eve: 9.865 | bob: 3.610Epoch  18:  92% | abe: 3.195 | eve: 9.865 | bob: 3.610Epoch  18:  92% | abe: 3.195 | eve: 9.866 | bob: 3.610Epoch  18:  93% | abe: 3.195 | eve: 9.866 | bob: 3.610Epoch  18:  94% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  95% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  96% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  96% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  97% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  98% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  18:  99% | abe: 3.195 | eve: 9.867 | bob: 3.610Epoch  19:   0% | abe: 3.210 | eve: 9.832 | bob: 3.658Epoch  19:   0% | abe: 3.196 | eve: 9.866 | bob: 3.626Epoch  19:   1% | abe: 3.187 | eve: 9.870 | bob: 3.617Epoch  19:   2% | abe: 3.176 | eve: 9.881 | bob: 3.603Epoch  19:   3% | abe: 3.179 | eve: 9.874 | bob: 3.606Epoch  19:   3% | abe: 3.179 | eve: 9.888 | bob: 3.603Epoch  19:   4% | abe: 3.183 | eve: 9.884 | bob: 3.605Epoch  19:   5% | abe: 3.188 | eve: 9.890 | bob: 3.608Epoch  19:   6% | abe: 3.187 | eve: 9.887 | bob: 3.608Epoch  19:   7% | abe: 3.187 | eve: 9.887 | bob: 3.610Epoch  19:   7% | abe: 3.191 | eve: 9.883 | bob: 3.613Epoch  19:   8% | abe: 3.191 | eve: 9.885 | bob: 3.614Epoch  19:   9% | abe: 3.188 | eve: 9.885 | bob: 3.613Epoch  19:  10% | abe: 3.190 | eve: 9.883 | bob: 3.615Epoch  19:  10% | abe: 3.189 | eve: 9.880 | bob: 3.613Epoch  19:  11% | abe: 3.187 | eve: 9.881 | bob: 3.612Epoch  19:  12% | abe: 3.187 | eve: 9.880 | bob: 3.612Epoch  19:  13% | abe: 3.184 | eve: 9.882 | bob: 3.609Epoch  19:  14% | abe: 3.185 | eve: 9.882 | bob: 3.611Epoch  19:  14% | abe: 3.186 | eve: 9.878 | bob: 3.612Epoch  19:  15% | abe: 3.186 | eve: 9.876 | bob: 3.612Epoch  19:  16% | abe: 3.186 | eve: 9.880 | bob: 3.613Epoch  19:  17% | abe: 3.186 | eve: 9.879 | bob: 3.613Epoch  19:  17% | abe: 3.188 | eve: 9.880 | bob: 3.615Epoch  19:  18% | abe: 3.188 | eve: 9.883 | bob: 3.614Epoch  19:  19% | abe: 3.187 | eve: 9.881 | bob: 3.613Epoch  19:  20% | abe: 3.186 | eve: 9.880 | bob: 3.612Epoch  19:  21% | abe: 3.188 | eve: 9.879 | bob: 3.613Epoch  19:  21% | abe: 3.188 | eve: 9.880 | bob: 3.613Epoch  19:  22% | abe: 3.188 | eve: 9.880 | bob: 3.613Epoch  19:  23% | abe: 3.188 | eve: 9.883 | bob: 3.612Epoch  19:  24% | abe: 3.187 | eve: 9.881 | bob: 3.611Epoch  19:  25% | abe: 3.188 | eve: 9.881 | bob: 3.611Epoch  19:  25% | abe: 3.188 | eve: 9.882 | bob: 3.612Epoch  19:  26% | abe: 3.189 | eve: 9.882 | bob: 3.613Epoch  19:  27% | abe: 3.188 | eve: 9.882 | bob: 3.611Epoch  19:  28% | abe: 3.187 | eve: 9.881 | bob: 3.611Epoch  19:  28% | abe: 3.187 | eve: 9.882 | bob: 3.611Epoch  19:  29% | abe: 3.187 | eve: 9.883 | bob: 3.612Epoch  19:  30% | abe: 3.187 | eve: 9.881 | bob: 3.612Epoch  19:  31% | abe: 3.186 | eve: 9.879 | bob: 3.611Epoch  19:  32% | abe: 3.186 | eve: 9.879 | bob: 3.611Epoch  19:  32% | abe: 3.187 | eve: 9.878 | bob: 3.612Epoch  19:  33% | abe: 3.186 | eve: 9.877 | bob: 3.612Epoch  19:  34% | abe: 3.186 | eve: 9.877 | bob: 3.612Epoch  19:  35% | abe: 3.187 | eve: 9.876 | bob: 3.613Epoch  19:  35% | abe: 3.187 | eve: 9.878 | bob: 3.614Epoch  19:  36% | abe: 3.187 | eve: 9.875 | bob: 3.614Epoch  19:  37% | abe: 3.188 | eve: 9.874 | bob: 3.614Epoch  19:  38% | abe: 3.188 | eve: 9.876 | bob: 3.614Epoch  19:  39% | abe: 3.188 | eve: 9.875 | bob: 3.614Epoch  19:  39% | abe: 3.188 | eve: 9.876 | bob: 3.614Epoch  19:  40% | abe: 3.188 | eve: 9.877 | bob: 3.614Epoch  19:  41% | abe: 3.189 | eve: 9.878 | bob: 3.615Epoch  19:  42% | abe: 3.189 | eve: 9.877 | bob: 3.616Epoch  19:  42% | abe: 3.189 | eve: 9.877 | bob: 3.616Epoch  19:  43% | abe: 3.189 | eve: 9.878 | bob: 3.616Epoch  19:  44% | abe: 3.189 | eve: 9.877 | bob: 3.616Epoch  19:  45% | abe: 3.189 | eve: 9.876 | bob: 3.616Epoch  19:  46% | abe: 3.189 | eve: 9.876 | bob: 3.616Epoch  19:  46% | abe: 3.188 | eve: 9.875 | bob: 3.615Epoch  19:  47% | abe: 3.188 | eve: 9.874 | bob: 3.615Epoch  19:  48% | abe: 3.188 | eve: 9.873 | bob: 3.615Epoch  19:  49% | abe: 3.188 | eve: 9.873 | bob: 3.615Epoch  19:  50% | abe: 3.188 | eve: 9.873 | bob: 3.615Epoch  19:  50% | abe: 3.189 | eve: 9.872 | bob: 3.616Epoch  19:  51% | abe: 3.188 | eve: 9.872 | bob: 3.616Epoch  19:  52% | abe: 3.188 | eve: 9.873 | bob: 3.615Epoch  19:  53% | abe: 3.188 | eve: 9.873 | bob: 3.615Epoch  19:  53% | abe: 3.188 | eve: 9.874 | bob: 3.616Epoch  19:  54% | abe: 3.188 | eve: 9.874 | bob: 3.616Epoch  19:  55% | abe: 3.189 | eve: 9.874 | bob: 3.616Epoch  19:  56% | abe: 3.189 | eve: 9.873 | bob: 3.617Epoch  19:  57% | abe: 3.189 | eve: 9.875 | bob: 3.617Epoch  19:  57% | abe: 3.189 | eve: 9.875 | bob: 3.617Epoch  19:  58% | abe: 3.189 | eve: 9.876 | bob: 3.617Epoch  19:  59% | abe: 3.189 | eve: 9.876 | bob: 3.617Epoch  19:  60% | abe: 3.188 | eve: 9.875 | bob: 3.616Epoch  19:  60% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  61% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  62% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  63% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  64% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  64% | abe: 3.188 | eve: 9.876 | bob: 3.616Epoch  19:  65% | abe: 3.188 | eve: 9.877 | bob: 3.617Epoch  19:  66% | abe: 3.188 | eve: 9.878 | bob: 3.617Epoch  19:  67% | abe: 3.188 | eve: 9.877 | bob: 3.617Epoch  19:  67% | abe: 3.188 | eve: 9.878 | bob: 3.617Epoch  19:  68% | abe: 3.187 | eve: 9.878 | bob: 3.616Epoch  19:  69% | abe: 3.188 | eve: 9.879 | bob: 3.617Epoch  19:  70% | abe: 3.188 | eve: 9.879 | bob: 3.617Epoch  19:  71% | abe: 3.187 | eve: 9.879 | bob: 3.617Epoch  19:  71% | abe: 3.188 | eve: 9.878 | bob: 3.617Epoch  19:  72% | abe: 3.188 | eve: 9.877 | bob: 3.617Epoch  19:  73% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  74% | abe: 3.188 | eve: 9.879 | bob: 3.618Epoch  19:  75% | abe: 3.187 | eve: 9.879 | bob: 3.617Epoch  19:  75% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  76% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  77% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  78% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  78% | abe: 3.187 | eve: 9.878 | bob: 3.617Epoch  19:  79% | abe: 3.187 | eve: 9.877 | bob: 3.617Epoch  19:  80% | abe: 3.187 | eve: 9.877 | bob: 3.617Epoch  19:  81% | abe: 3.186 | eve: 9.878 | bob: 3.616Epoch  19:  82% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  82% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  83% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  84% | abe: 3.186 | eve: 9.879 | bob: 3.616Epoch  19:  85% | abe: 3.186 | eve: 9.879 | bob: 3.616Epoch  19:  85% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  86% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  87% | abe: 3.186 | eve: 9.879 | bob: 3.617Epoch  19:  88% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  89% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  89% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  90% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  91% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  92% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  92% | abe: 3.186 | eve: 9.877 | bob: 3.618Epoch  19:  93% | abe: 3.186 | eve: 9.878 | bob: 3.617Epoch  19:  94% | abe: 3.186 | eve: 9.878 | bob: 3.618Epoch  19:  95% | abe: 3.186 | eve: 9.878 | bob: 3.618Epoch  19:  96% | abe: 3.185 | eve: 9.878 | bob: 3.617Epoch  19:  96% | abe: 3.185 | eve: 9.878 | bob: 3.617Epoch  19:  97% | abe: 3.185 | eve: 9.877 | bob: 3.617Epoch  19:  98% | abe: 3.185 | eve: 9.877 | bob: 3.617Epoch  19:  99% | abe: 3.185 | eve: 9.878 | bob: 3.617Epoch  20:   0% | abe: 3.179 | eve: 9.868 | bob: 3.610Epoch  20:   0% | abe: 3.186 | eve: 9.867 | bob: 3.629Epoch  20:   1% | abe: 3.183 | eve: 9.894 | bob: 3.632Epoch  20:   2% | abe: 3.172 | eve: 9.898 | bob: 3.617Epoch  20:   3% | abe: 3.176 | eve: 9.896 | bob: 3.624Epoch  20:   3% | abe: 3.176 | eve: 9.890 | bob: 3.626Epoch  20:   4% | abe: 3.173 | eve: 9.890 | bob: 3.623Epoch  20:   5% | abe: 3.174 | eve: 9.881 | bob: 3.625Epoch  20:   6% | abe: 3.177 | eve: 9.881 | bob: 3.627Epoch  20:   7% | abe: 3.173 | eve: 9.878 | bob: 3.623Epoch  20:   7% | abe: 3.175 | eve: 9.877 | bob: 3.627Epoch  20:   8% | abe: 3.173 | eve: 9.872 | bob: 3.622Epoch  20:   9% | abe: 3.172 | eve: 9.874 | bob: 3.621Epoch  20:  10% | abe: 3.173 | eve: 9.872 | bob: 3.623Epoch  20:  10% | abe: 3.173 | eve: 9.867 | bob: 3.624Epoch  20:  11% | abe: 3.173 | eve: 9.870 | bob: 3.622Epoch  20:  12% | abe: 3.175 | eve: 9.873 | bob: 3.623Epoch  20:  13% | abe: 3.175 | eve: 9.875 | bob: 3.623Epoch  20:  14% | abe: 3.175 | eve: 9.876 | bob: 3.623Epoch  20:  14% | abe: 3.174 | eve: 9.879 | bob: 3.623Epoch  20:  15% | abe: 3.175 | eve: 9.880 | bob: 3.622Epoch  20:  16% | abe: 3.176 | eve: 9.882 | bob: 3.624Epoch  20:  17% | abe: 3.175 | eve: 9.879 | bob: 3.624Epoch  20:  17% | abe: 3.175 | eve: 9.880 | bob: 3.623Epoch  20:  18% | abe: 3.176 | eve: 9.881 | bob: 3.625Epoch  20:  19% | abe: 3.176 | eve: 9.882 | bob: 3.625Epoch  20:  20% | abe: 3.176 | eve: 9.880 | bob: 3.625Epoch  20:  21% | abe: 3.176 | eve: 9.881 | bob: 3.625Epoch  20:  21% | abe: 3.177 | eve: 9.881 | bob: 3.626Epoch  20:  22% | abe: 3.178 | eve: 9.882 | bob: 3.626Epoch  20:  23% | abe: 3.177 | eve: 9.883 | bob: 3.626Epoch  20:  24% | abe: 3.177 | eve: 9.883 | bob: 3.625Epoch  20:  25% | abe: 3.178 | eve: 9.881 | bob: 3.625Epoch  20:  25% | abe: 3.177 | eve: 9.883 | bob: 3.625Epoch  20:  26% | abe: 3.179 | eve: 9.882 | bob: 3.626Epoch  20:  27% | abe: 3.178 | eve: 9.881 | bob: 3.626Epoch  20:  28% | abe: 3.178 | eve: 9.883 | bob: 3.625Epoch  20:  28% | abe: 3.178 | eve: 9.882 | bob: 3.625Epoch  20:  29% | abe: 3.179 | eve: 9.882 | bob: 3.626Epoch  20:  30% | abe: 3.179 | eve: 9.882 | bob: 3.626Epoch  20:  31% | abe: 3.179 | eve: 9.883 | bob: 3.626Epoch  20:  32% | abe: 3.178 | eve: 9.883 | bob: 3.626Epoch  20:  32% | abe: 3.178 | eve: 9.883 | bob: 3.626Epoch  20:  33% | abe: 3.178 | eve: 9.884 | bob: 3.626Epoch  20:  34% | abe: 3.177 | eve: 9.885 | bob: 3.625Epoch  20:  35% | abe: 3.177 | eve: 9.887 | bob: 3.624Epoch  20:  35% | abe: 3.176 | eve: 9.885 | bob: 3.623Epoch  20:  36% | abe: 3.176 | eve: 9.884 | bob: 3.623Epoch  20:  37% | abe: 3.175 | eve: 9.885 | bob: 3.622Epoch  20:  38% | abe: 3.175 | eve: 9.883 | bob: 3.622Epoch  20:  39% | abe: 3.175 | eve: 9.883 | bob: 3.622Epoch  20:  39% | abe: 3.175 | eve: 9.883 | bob: 3.623Epoch  20:  40% | abe: 3.174 | eve: 9.883 | bob: 3.622Epoch  20:  41% | abe: 3.175 | eve: 9.883 | bob: 3.622Epoch  20:  42% | abe: 3.175 | eve: 9.884 | bob: 3.622Epoch  20:  42% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  43% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  44% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  45% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  46% | abe: 3.175 | eve: 9.886 | bob: 3.624Epoch  20:  46% | abe: 3.176 | eve: 9.885 | bob: 3.624Epoch  20:  47% | abe: 3.176 | eve: 9.885 | bob: 3.624Epoch  20:  48% | abe: 3.176 | eve: 9.885 | bob: 3.624Epoch  20:  49% | abe: 3.176 | eve: 9.886 | bob: 3.625Epoch  20:  50% | abe: 3.176 | eve: 9.886 | bob: 3.625Epoch  20:  50% | abe: 3.176 | eve: 9.887 | bob: 3.625Epoch  20:  51% | abe: 3.176 | eve: 9.886 | bob: 3.625Epoch  20:  52% | abe: 3.177 | eve: 9.887 | bob: 3.625Epoch  20:  53% | abe: 3.176 | eve: 9.886 | bob: 3.625Epoch  20:  53% | abe: 3.176 | eve: 9.887 | bob: 3.625Epoch  20:  54% | abe: 3.177 | eve: 9.887 | bob: 3.625Epoch  20:  55% | abe: 3.176 | eve: 9.886 | bob: 3.625Epoch  20:  56% | abe: 3.176 | eve: 9.886 | bob: 3.624Epoch  20:  57% | abe: 3.176 | eve: 9.885 | bob: 3.624Epoch  20:  57% | abe: 3.176 | eve: 9.886 | bob: 3.624Epoch  20:  58% | abe: 3.176 | eve: 9.885 | bob: 3.624Epoch  20:  59% | abe: 3.176 | eve: 9.886 | bob: 3.623Epoch  20:  60% | abe: 3.175 | eve: 9.885 | bob: 3.622Epoch  20:  60% | abe: 3.175 | eve: 9.886 | bob: 3.622Epoch  20:  61% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  62% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  63% | abe: 3.175 | eve: 9.884 | bob: 3.622Epoch  20:  64% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  64% | abe: 3.175 | eve: 9.886 | bob: 3.623Epoch  20:  65% | abe: 3.175 | eve: 9.885 | bob: 3.623Epoch  20:  66% | abe: 3.175 | eve: 9.885 | bob: 3.622Epoch  20:  67% | abe: 3.175 | eve: 9.885 | bob: 3.622Epoch  20:  67% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  68% | abe: 3.174 | eve: 9.886 | bob: 3.622Epoch  20:  69% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  70% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  71% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  71% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  72% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  73% | abe: 3.174 | eve: 9.885 | bob: 3.621Epoch  20:  74% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  75% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  75% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  76% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  77% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  78% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  78% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  79% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  80% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  81% | abe: 3.174 | eve: 9.884 | bob: 3.622Epoch  20:  82% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  82% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  83% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  84% | abe: 3.174 | eve: 9.885 | bob: 3.622Epoch  20:  85% | abe: 3.173 | eve: 9.885 | bob: 3.621Epoch  20:  85% | abe: 3.173 | eve: 9.884 | bob: 3.621Epoch  20:  86% | abe: 3.173 | eve: 9.884 | bob: 3.621Epoch  20:  87% | abe: 3.173 | eve: 9.884 | bob: 3.621Epoch  20:  88% | abe: 3.173 | eve: 9.884 | bob: 3.622Epoch  20:  89% | abe: 3.173 | eve: 9.886 | bob: 3.622Epoch  20:  89% | abe: 3.173 | eve: 9.886 | bob: 3.622Epoch  20:  90% | abe: 3.173 | eve: 9.886 | bob: 3.622Epoch  20:  91% | abe: 3.173 | eve: 9.886 | bob: 3.621Epoch  20:  92% | abe: 3.173 | eve: 9.887 | bob: 3.621Epoch  20:  92% | abe: 3.173 | eve: 9.887 | bob: 3.621Epoch  20:  93% | abe: 3.173 | eve: 9.888 | bob: 3.621Epoch  20:  94% | abe: 3.173 | eve: 9.888 | bob: 3.621Epoch  20:  95% | abe: 3.173 | eve: 9.887 | bob: 3.621Epoch  20:  96% | abe: 3.173 | eve: 9.887 | bob: 3.622Epoch  20:  96% | abe: 3.173 | eve: 9.887 | bob: 3.622Epoch  20:  97% | abe: 3.173 | eve: 9.887 | bob: 3.622Epoch  20:  98% | abe: 3.173 | eve: 9.887 | bob: 3.622Epoch  20:  99% | abe: 3.172 | eve: 9.887 | bob: 3.622Epoch  21:   0% | abe: 3.203 | eve: 9.929 | bob: 3.675Epoch  21:   0% | abe: 3.191 | eve: 9.881 | bob: 3.656Epoch  21:   1% | abe: 3.185 | eve: 9.891 | bob: 3.655Epoch  21:   2% | abe: 3.178 | eve: 9.898 | bob: 3.646Epoch  21:   3% | abe: 3.177 | eve: 9.892 | bob: 3.643Epoch  21:   3% | abe: 3.175 | eve: 9.899 | bob: 3.641Epoch  21:   4% | abe: 3.174 | eve: 9.890 | bob: 3.640Epoch  21:   5% | abe: 3.169 | eve: 9.885 | bob: 3.636Epoch  21:   6% | abe: 3.170 | eve: 9.874 | bob: 3.635Epoch  21:   7% | abe: 3.170 | eve: 9.873 | bob: 3.635Epoch  21:   7% | abe: 3.173 | eve: 9.877 | bob: 3.637Epoch  21:   8% | abe: 3.172 | eve: 9.884 | bob: 3.636Epoch  21:   9% | abe: 3.173 | eve: 9.886 | bob: 3.637Epoch  21:  10% | abe: 3.175 | eve: 9.892 | bob: 3.637Epoch  21:  10% | abe: 3.174 | eve: 9.895 | bob: 3.636Epoch  21:  11% | abe: 3.176 | eve: 9.893 | bob: 3.637Epoch  21:  12% | abe: 3.175 | eve: 9.892 | bob: 3.637Epoch  21:  13% | abe: 3.176 | eve: 9.887 | bob: 3.637Epoch  21:  14% | abe: 3.178 | eve: 9.884 | bob: 3.638Epoch  21:  14% | abe: 3.177 | eve: 9.882 | bob: 3.638Epoch  21:  15% | abe: 3.178 | eve: 9.882 | bob: 3.639Epoch  21:  16% | abe: 3.178 | eve: 9.879 | bob: 3.638Epoch  21:  17% | abe: 3.179 | eve: 9.878 | bob: 3.639Epoch  21:  17% | abe: 3.178 | eve: 9.875 | bob: 3.638Epoch  21:  18% | abe: 3.177 | eve: 9.877 | bob: 3.637Epoch  21:  19% | abe: 3.178 | eve: 9.877 | bob: 3.638Epoch  21:  20% | abe: 3.178 | eve: 9.881 | bob: 3.638Epoch  21:  21% | abe: 3.178 | eve: 9.880 | bob: 3.639Epoch  21:  21% | abe: 3.178 | eve: 9.882 | bob: 3.637Epoch  21:  22% | abe: 3.177 | eve: 9.882 | bob: 3.637Epoch  21:  23% | abe: 3.176 | eve: 9.881 | bob: 3.636Epoch  21:  24% | abe: 3.177 | eve: 9.880 | bob: 3.636Epoch  21:  25% | abe: 3.177 | eve: 9.882 | bob: 3.636Epoch  21:  25% | abe: 3.175 | eve: 9.883 | bob: 3.635Epoch  21:  26% | abe: 3.175 | eve: 9.883 | bob: 3.635Epoch  21:  27% | abe: 3.175 | eve: 9.882 | bob: 3.636Epoch  21:  28% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  28% | abe: 3.174 | eve: 9.881 | bob: 3.634Epoch  21:  29% | abe: 3.174 | eve: 9.881 | bob: 3.634Epoch  21:  30% | abe: 3.175 | eve: 9.882 | bob: 3.635Epoch  21:  31% | abe: 3.174 | eve: 9.882 | bob: 3.634Epoch  21:  32% | abe: 3.174 | eve: 9.881 | bob: 3.634Epoch  21:  32% | abe: 3.174 | eve: 9.882 | bob: 3.634Epoch  21:  33% | abe: 3.174 | eve: 9.882 | bob: 3.634Epoch  21:  34% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  35% | abe: 3.173 | eve: 9.882 | bob: 3.634Epoch  21:  35% | abe: 3.173 | eve: 9.880 | bob: 3.634Epoch  21:  36% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  37% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  38% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  39% | abe: 3.174 | eve: 9.878 | bob: 3.634Epoch  21:  39% | abe: 3.174 | eve: 9.877 | bob: 3.635Epoch  21:  40% | abe: 3.174 | eve: 9.878 | bob: 3.634Epoch  21:  41% | abe: 3.174 | eve: 9.879 | bob: 3.634Epoch  21:  42% | abe: 3.173 | eve: 9.880 | bob: 3.634Epoch  21:  42% | abe: 3.173 | eve: 9.879 | bob: 3.634Epoch  21:  43% | abe: 3.174 | eve: 9.881 | bob: 3.634Epoch  21:  44% | abe: 3.173 | eve: 9.881 | bob: 3.634Epoch  21:  45% | abe: 3.174 | eve: 9.880 | bob: 3.634Epoch  21:  46% | abe: 3.173 | eve: 9.880 | bob: 3.634Epoch  21:  46% | abe: 3.173 | eve: 9.880 | bob: 3.635Epoch  21:  47% | abe: 3.174 | eve: 9.880 | bob: 3.636Epoch  21:  48% | abe: 3.174 | eve: 9.880 | bob: 3.636Epoch  21:  49% | abe: 3.174 | eve: 9.881 | bob: 3.636Epoch  21:  50% | abe: 3.174 | eve: 9.880 | bob: 3.636Epoch  21:  50% | abe: 3.174 | eve: 9.880 | bob: 3.636Epoch  21:  51% | abe: 3.174 | eve: 9.881 | bob: 3.636Epoch  21:  52% | abe: 3.174 | eve: 9.882 | bob: 3.636Epoch  21:  53% | abe: 3.174 | eve: 9.881 | bob: 3.636Epoch  21:  53% | abe: 3.174 | eve: 9.882 | bob: 3.636Epoch  21:  54% | abe: 3.174 | eve: 9.882 | bob: 3.636Epoch  21:  55% | abe: 3.174 | eve: 9.880 | bob: 3.636Epoch  21:  56% | abe: 3.173 | eve: 9.881 | bob: 3.635Epoch  21:  57% | abe: 3.173 | eve: 9.881 | bob: 3.636Epoch  21:  57% | abe: 3.173 | eve: 9.881 | bob: 3.636Epoch  21:  58% | abe: 3.173 | eve: 9.881 | bob: 3.635Epoch  21:  59% | abe: 3.173 | eve: 9.882 | bob: 3.636Epoch  21:  60% | abe: 3.173 | eve: 9.882 | bob: 3.636Epoch  21:  60% | abe: 3.173 | eve: 9.882 | bob: 3.636Epoch  21:  61% | abe: 3.173 | eve: 9.881 | bob: 3.635Epoch  21:  62% | abe: 3.173 | eve: 9.881 | bob: 3.636Epoch  21:  63% | abe: 3.173 | eve: 9.880 | bob: 3.636Epoch  21:  64% | abe: 3.174 | eve: 9.880 | bob: 3.637Epoch  21:  64% | abe: 3.174 | eve: 9.881 | bob: 3.637Epoch  21:  65% | abe: 3.174 | eve: 9.882 | bob: 3.637Epoch  21:  66% | abe: 3.174 | eve: 9.882 | bob: 3.637Epoch  21:  67% | abe: 3.174 | eve: 9.882 | bob: 3.638Epoch  21:  67% | abe: 3.174 | eve: 9.883 | bob: 3.637Epoch  21:  68% | abe: 3.174 | eve: 9.884 | bob: 3.638Epoch  21:  69% | abe: 3.174 | eve: 9.884 | bob: 3.638Epoch  21:  70% | abe: 3.174 | eve: 9.883 | bob: 3.638Epoch  21:  71% | abe: 3.174 | eve: 9.883 | bob: 3.637Epoch  21:  71% | abe: 3.174 | eve: 9.883 | bob: 3.638Epoch  21:  72% | abe: 3.173 | eve: 9.883 | bob: 3.638Epoch  21:  73% | abe: 3.173 | eve: 9.883 | bob: 3.638Epoch  21:  74% | abe: 3.173 | eve: 9.883 | bob: 3.638Epoch  21:  75% | abe: 3.173 | eve: 9.882 | bob: 3.637Epoch  21:  75% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  76% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  77% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  78% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  78% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  79% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  80% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  81% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  82% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  82% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  83% | abe: 3.172 | eve: 9.882 | bob: 3.637Epoch  21:  84% | abe: 3.172 | eve: 9.882 | bob: 3.638Epoch  21:  85% | abe: 3.172 | eve: 9.883 | bob: 3.637Epoch  21:  85% | abe: 3.172 | eve: 9.884 | bob: 3.637Epoch  21:  86% | abe: 3.172 | eve: 9.884 | bob: 3.637Epoch  21:  87% | abe: 3.171 | eve: 9.884 | bob: 3.637Epoch  21:  88% | abe: 3.171 | eve: 9.884 | bob: 3.637Epoch  21:  89% | abe: 3.171 | eve: 9.885 | bob: 3.637Epoch  21:  89% | abe: 3.171 | eve: 9.885 | bob: 3.637Epoch  21:  90% | abe: 3.171 | eve: 9.885 | bob: 3.637Epoch  21:  91% | abe: 3.170 | eve: 9.885 | bob: 3.636Epoch  21:  92% | abe: 3.170 | eve: 9.885 | bob: 3.636Epoch  21:  92% | abe: 3.170 | eve: 9.885 | bob: 3.637Epoch  21:  93% | abe: 3.170 | eve: 9.886 | bob: 3.637Epoch  21:  94% | abe: 3.170 | eve: 9.886 | bob: 3.636Epoch  21:  95% | abe: 3.170 | eve: 9.886 | bob: 3.636Epoch  21:  96% | abe: 3.170 | eve: 9.886 | bob: 3.636Epoch  21:  96% | abe: 3.170 | eve: 9.886 | bob: 3.636Epoch  21:  97% | abe: 3.170 | eve: 9.886 | bob: 3.637Epoch  21:  98% | abe: 3.170 | eve: 9.886 | bob: 3.637Epoch  21:  99% | abe: 3.170 | eve: 9.885 | bob: 3.637
Early stopping: No improvement after 5 epochs since epoch 15. Best Bob loss: 3.567648114336862
Training complete.
cipher1 + cipher2
[[1.3913517  1.7070624  1.61985    ... 0.77069366 0.9694812  1.4322666 ]
 [1.2810521  1.4553554  1.1946447  ... 1.5582237  1.6958915  1.048007  ]
 [0.7812513  0.98130786 1.7553048  ... 0.47788805 1.1220028  1.1908537 ]
 ...
 [1.0849625  1.0707016  1.4912302  ... 1.3112051  0.5099699  1.4460136 ]
 [0.78627086 1.7554475  0.39217046 ... 1.25716    0.25355455 1.6484759 ]
 [0.7062545  0.8417715  1.3100687  ... 1.9279269  0.4176831  1.0227473 ]]
HO addition:
[[1.3913882  1.7071145  1.619898   ... 0.7707195  0.96950406 1.4323115 ]
 [1.2810818  1.4553944  1.1946695  ... 1.5582721  1.6959387  1.0480335 ]
 [0.7812777  0.9813422  1.7553518  ... 0.47789884 1.1220324  1.1908813 ]
 ...
 [1.0849974  1.0707197  1.4912785  ... 1.3112482  0.50998336 1.4460584 ]
 [0.78629386 1.7554955  0.39218438 ... 1.2571898  0.2535652  1.6485238 ]
 [0.7062652  0.8418023  1.3101115  ... 1.927983   0.41769367 1.0227839 ]]
cipher1 * cipher2
[[0.46835384 0.7134104  0.65233105 ... 0.12743175 0.20221518 0.4912477 ]
 [0.3436123  0.52095217 0.2352082  ... 0.5846483  0.7166696  0.2580708 ]
 [0.12974684 0.18524499 0.7553048  ... 0.04534238 0.3054653  0.2972724 ]
 ...
 [0.27238303 0.0707016  0.51251036 ... 0.38612345 0.0634196  0.50553036]
 [0.15417245 0.7640236  0.02823962 ... 0.34253833 0.         0.678121  ]
 [0.00151531 0.11340594 0.38972855 ... 0.9279269  0.04113288 0.1857237 ]]
HO multiplication
[[ 4.6833280e-01  7.1334219e-01  6.5227604e-01 ...  1.2743758e-01
   2.0221663e-01  4.9122262e-01]
 [ 3.4361243e-01  5.2092230e-01  2.3521930e-01 ...  5.8460635e-01
   7.1660334e-01  2.5807077e-01]
 [ 1.2975368e-01  1.8526578e-01  7.5523239e-01 ...  4.5308117e-02
   3.0546311e-01  2.9727411e-01]
 ...
 [ 2.7239260e-01  7.0707291e-02  5.1248240e-01 ...  3.8611820e-01
   6.3393936e-02  5.0550258e-01]
 [ 1.5417205e-01  7.6394790e-01  2.8197490e-02 ...  3.4253708e-01
  -3.5252064e-04  6.7806107e-01]
 [ 1.4518076e-03  1.1342239e-01  3.8972235e-01 ...  9.2780888e-01
   4.1094273e-02  1.8574971e-01]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 99.00%
Bob decrypted addition: [[0.15195233 0.00575638 0.03189164 ... 0.06295639 0.         0.        ]
 [0.         0.0503419  0.0313037  ... 0.         0.35399866 1.0401304 ]
 [1.2689575  1.0883615  0.09420496 ... 1.099288   0.         0.        ]
 ...
 [0.         0.16320264 0.         ... 0.39173895 0.         0.        ]
 [0.         1.0651382  0.5883703  ... 0.01900244 1.0302858  1.0196291 ]
 [0.40276748 0.         1.1197387  ... 1.0642781  0.         0.        ]]
Bob decrypted bits addition: [[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 1]
 [1 1 0 ... 1 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 1 1 ... 0 1 1]
 [0 0 1 ... 1 0 0]]
Number of correctly decrypted bits addition: 2306
Total number of bits addition: 8192
Decryption accuracy addition: 28.1494140625%
Bob decrypted multiplication: [[2.3964375e-01 3.8657290e-01 5.9193099e-01 ... 6.3019824e-01
  7.1820915e-02 0.0000000e+00]
 [0.0000000e+00 1.8822277e-01 6.7791128e-01 ... 0.0000000e+00
  5.1219630e-01 1.0377902e+00]
 [1.0168743e+00 9.8470151e-01 5.8965838e-01 ... 9.8408151e-01
  0.0000000e+00 0.0000000e+00]
 ...
 [0.0000000e+00 3.8919050e-01 2.2810012e-01 ... 8.1853193e-01
  0.0000000e+00 4.0557343e-01]
 [0.0000000e+00 9.5033777e-01 8.0443799e-01 ... 3.3324718e-02
  1.0166188e+00 1.0054084e+00]
 [5.9433645e-01 0.0000000e+00 1.0711542e+00 ... 1.0769358e+00
  8.5135281e-02 8.1586838e-04]]
Bob decrypted bits multiplication: [[0 0 1 ... 1 0 0]
 [0 0 1 ... 0 1 1]
 [1 1 1 ... 1 0 0]
 ...
 [0 0 0 ... 1 0 0]
 [0 1 1 ... 0 1 1]
 [1 0 1 ... 1 0 0]]
Number of correctly decrypted bits multiplication: 6132
Total number of bits multiplication: 8192
Decryption accuracy multiplication: 74.853515625%
Eve decrypted addition: [[1.1720773  0.8332579  0.93869483 ... 0.97881466 1.2092904  0.8842655 ]
 [1.1619215  0.8289094  0.932499   ... 0.9809579  1.2065569  0.88410884]
 [1.1738119  0.83418924 0.93223774 ... 0.97226495 1.2089641  0.88402224]
 ...
 [1.1625713  0.8270265  0.92944974 ... 0.9747756  1.2015092  0.8829157 ]
 [1.151887   0.826133   0.9357894  ... 0.98254085 1.1997533  0.8832055 ]
 [1.1628557  0.8329388  0.93758804 ... 0.9804842  1.2011608  0.8828184 ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 4102
Total number of bits addition: 8192
Decryption accuracy by Eve addition: 50.0732421875%
Eve decrypted mulitplication: [[1.1828196  0.84745586 0.94529724 ... 0.9692242  1.1926497  0.8802947 ]
 [1.1634493  0.8289868  0.9236351  ... 0.9695665  1.1907082  0.88136744]
 [1.1778483  0.8413772  0.92399234 ... 0.94938016 1.1832236  0.87842363]
 ...
 [1.1686454  0.83275443 0.9083486  ... 0.9464375  1.1617568  0.8747661 ]
 [1.1492343  0.8255305  0.92537224 ... 0.9741149  1.1783521  0.87960523]
 [1.1767722  0.8460951  0.93259364 ... 0.9705629  1.1791521  0.8778855 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 2045
Total number of bits mulitplication: 8192
Decryption accuracy by Eve mulitplication: 24.96337890625%
Bob decrypted P1: [[0.         1.0057491  0.95913106 ... 1.0311518  0.9620714  0.        ]
 [0.         1.0269731  0.         ... 0.01538903 0.99930406 1.0077688 ]
 [1.0370529  0.97422963 0.         ... 0.99459314 0.         0.        ]
 ...
 [0.         0.         0.99921656 ... 0.0024004  0.         0.        ]
 [0.         0.98945373 1.0355892  ... 0.00973082 1.0110393  0.98824257]
 [0.9751743  0.         0.9885565  ... 0.98767954 0.         0.        ]]
Bob decrypted bits P1: [[0 1 1 ... 1 1 0]
 [0 1 0 ... 0 1 1]
 [1 1 0 ... 1 0 0]
 ...
 [0 0 1 ... 0 0 0]
 [0 1 1 ... 0 1 1]
 [1 0 1 ... 1 0 0]]
Number of correctly decrypted bits P1: 8192
Total number of bits P1: 8192
Decryption accuracy P1: 100.0%
Bob decrypted P2: [[0.9911725  0.         0.         ... 0.         0.         0.        ]
 [0.         0.         0.9651033  ... 0.         0.         1.0135555 ]
 [1.0093594  0.989565   0.9548244  ... 0.9843377  0.         0.        ]
 ...
 [0.         1.0075219  0.         ... 1.000945   0.         0.96182114]
 [0.         1.0036849  0.         ... 0.01028138 1.0070192  0.99970925]
 [0.         0.         1.0169439  ... 1.0211338  1.0164773  0.        ]]
Bob decrypted bits P2: [[1 0 0 ... 0 0 0]
 [0 0 1 ... 0 0 1]
 [1 1 1 ... 1 0 0]
 ...
 [0 1 0 ... 1 0 1]
 [0 1 0 ... 0 1 1]
 [0 0 1 ... 1 1 0]]
Number of correctly decrypted bits P2: 8192
Total number of bits P2: 8192
Decryption accuracy P2: 100.0%
