WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-12 10:50:36.599412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-12 10:50:36.706837: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-12 10:50:36.707712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:50:36.712282: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:50:36.714968: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:50:36.715813: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:50:36.719567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:50:36.722299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:50:36.730996: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:50:36.739079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:50:36.739561: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-12 10:50:36.757764: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-12 10:50:36.760092: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x51105f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:50:36.760122: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-12 10:50:37.093897: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x24dfa80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:50:37.093973: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-12 10:50:37.098135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-12 10:50:37.098312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:50:37.098346: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:50:37.098373: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:50:37.098398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:50:37.098423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:50:37.098448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:50:37.098474: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:50:37.107141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:50:37.107325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:50:37.114540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-12 10:50:37.114636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-12 10:50:37.114664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-12 10:50:37.126266: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-12 10:50:42.905294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6682 - val_loss: 0.0023
Epoch 2/512
512/512 - 0s - loss: 0.1176 - val_loss: 2.0652e-04
Epoch 3/512
512/512 - 0s - loss: 0.0100 - val_loss: 1.5960e-05
Epoch 4/512
512/512 - 0s - loss: 9.2512e-04 - val_loss: 4.2115e-06
Epoch 5/512
512/512 - 0s - loss: 3.6037e-04 - val_loss: 2.8125e-06
Epoch 6/512
512/512 - 0s - loss: 2.4374e-04 - val_loss: 1.8534e-06
Epoch 7/512
512/512 - 0s - loss: 1.5551e-04 - val_loss: 1.1042e-06
Epoch 8/512
512/512 - 0s - loss: 8.9001e-05 - val_loss: 5.7909e-07
Epoch 9/512
512/512 - 0s - loss: 4.4456e-05 - val_loss: 2.5879e-07
Epoch 10/512
512/512 - 0s - loss: 1.8731e-05 - val_loss: 9.4458e-08
Epoch 11/512
512/512 - 0s - loss: 6.3705e-06 - val_loss: 2.6667e-08
Epoch 12/512
512/512 - 0s - loss: 1.6538e-06 - val_loss: 5.4262e-09
Epoch 13/512
512/512 - 0s - loss: 3.0586e-07 - val_loss: 7.9112e-10
Epoch 14/512
512/512 - 0s - loss: 1.3552e-07 - val_loss: 1.4655e-08
Epoch 15/512
512/512 - 0s - loss: 1.1478e-04 - val_loss: 2.8375e-05
Epoch 16/512
512/512 - 0s - loss: 0.0056 - val_loss: 9.8500e-06
Epoch 17/512
512/512 - 0s - loss: 4.4189e-04 - val_loss: 1.0137e-06
Epoch 18/512
512/512 - 0s - loss: 1.1654e-04 - val_loss: 2.4476e-06
Epoch 19/512
512/512 - 0s - loss: 9.0590e-04 - val_loss: 3.1395e-05
Epoch 20/512
512/512 - 0s - loss: 0.0030 - val_loss: 1.1406e-05
Epoch 21/512
512/512 - 0s - loss: 7.5377e-04 - val_loss: 4.5866e-06
Epoch 22/512
512/512 - 0s - loss: 6.2704e-04 - val_loss: 1.1997e-05
Epoch 23/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.9116e-05
Epoch 24/512
512/512 - 0s - loss: 0.0015 - val_loss: 8.5014e-06
Epoch 25/512
512/512 - 0s - loss: 8.3259e-04 - val_loss: 9.3905e-06
Epoch 26/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.5715e-05
Epoch 27/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.1740e-05
Epoch 28/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.6561e-06
Epoch 29/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.2367e-05
Epoch 30/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2573e-05
Epoch 31/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0517e-05
Epoch 32/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1069e-05
Epoch 33/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1867e-05
Epoch 34/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0860e-05
Epoch 35/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0582e-05
Epoch 36/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1063e-05
Epoch 37/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0822e-05
Epoch 38/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0674e-05
Epoch 39/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0417e-05
Epoch 40/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0447e-05
Epoch 41/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0444e-05
Epoch 42/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0355e-05
Epoch 43/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0224e-05
Epoch 44/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0113e-05
Epoch 45/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0019e-05
Epoch 46/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.9112e-06
Epoch 47/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8581e-06
Epoch 48/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8920e-06
Epoch 49/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.6567e-06
Epoch 50/512
512/512 - 0s - loss: 9.8523e-04 - val_loss: 9.6104e-06
Epoch 51/512
512/512 - 0s - loss: 9.9123e-04 - val_loss: 9.8390e-06
Epoch 52/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.5444e-06
Epoch 53/512
512/512 - 0s - loss: 9.6859e-04 - val_loss: 9.3324e-06
Epoch 54/512
512/512 - 0s - loss: 9.6156e-04 - val_loss: 9.4240e-06
Epoch 55/512
512/512 - 0s - loss: 9.6093e-04 - val_loss: 9.5796e-06
Epoch 56/512
512/512 - 0s - loss: 9.6957e-04 - val_loss: 9.3814e-06
Epoch 57/512
512/512 - 0s - loss: 9.4800e-04 - val_loss: 9.0365e-06
Epoch 58/512
512/512 - 0s - loss: 9.2576e-04 - val_loss: 9.1800e-06
Epoch 59/512
512/512 - 0s - loss: 9.4664e-04 - val_loss: 9.1606e-06
Epoch 60/512
512/512 - 0s - loss: 9.2625e-04 - val_loss: 9.0304e-06
Epoch 61/512
512/512 - 0s - loss: 9.2051e-04 - val_loss: 9.0116e-06
Epoch 62/512
512/512 - 0s - loss: 9.1776e-04 - val_loss: 8.8796e-06
Epoch 63/512
512/512 - 0s - loss: 9.0896e-04 - val_loss: 8.8411e-06
Epoch 64/512
512/512 - 0s - loss: 9.0459e-04 - val_loss: 8.7649e-06
Epoch 65/512
512/512 - 0s - loss: 8.9500e-04 - val_loss: 8.7621e-06
Epoch 66/512
512/512 - 0s - loss: 8.9094e-04 - val_loss: 8.8102e-06
Epoch 67/512
512/512 - 0s - loss: 8.9494e-04 - val_loss: 8.6626e-06
Epoch 68/512
512/512 - 0s - loss: 8.7992e-04 - val_loss: 8.5212e-06
Epoch 69/512
512/512 - 0s - loss: 8.7215e-04 - val_loss: 8.5619e-06
Epoch 70/512
512/512 - 0s - loss: 8.7348e-04 - val_loss: 8.5957e-06
Epoch 71/512
512/512 - 0s - loss: 8.7479e-04 - val_loss: 8.3593e-06
Epoch 72/512
512/512 - 0s - loss: 8.5050e-04 - val_loss: 8.3782e-06
Epoch 73/512
512/512 - 0s - loss: 8.5358e-04 - val_loss: 8.5632e-06
Epoch 74/512
512/512 - 0s - loss: 8.7590e-04 - val_loss: 8.1768e-06
Epoch 75/512
512/512 - 0s - loss: 8.2912e-04 - val_loss: 8.1513e-06
Epoch 76/512
512/512 - 0s - loss: 8.3885e-04 - val_loss: 8.4361e-06
Epoch 77/512
512/512 - 0s - loss: 8.5513e-04 - val_loss: 8.2876e-06
Epoch 78/512
512/512 - 0s - loss: 8.3839e-04 - val_loss: 7.9364e-06
Epoch 79/512
512/512 - 0s - loss: 8.1428e-04 - val_loss: 8.0588e-06
Epoch 80/512
512/512 - 0s - loss: 8.2741e-04 - val_loss: 8.4052e-06
Epoch 81/512
512/512 - 0s - loss: 8.4372e-04 - val_loss: 8.1206e-06
Epoch 82/512
512/512 - 0s - loss: 8.1139e-04 - val_loss: 7.8875e-06
Epoch 83/512
512/512 - 0s - loss: 8.0910e-04 - val_loss: 8.0287e-06
Epoch 84/512
512/512 - 0s - loss: 8.1589e-04 - val_loss: 8.1419e-06
Epoch 85/512
512/512 - 0s - loss: 8.2211e-04 - val_loss: 7.8871e-06
Epoch 86/512
512/512 - 0s - loss: 7.9528e-04 - val_loss: 7.8230e-06
Epoch 87/512
512/512 - 0s - loss: 8.0140e-04 - val_loss: 7.9947e-06
Epoch 88/512
512/512 - 0s - loss: 8.0942e-04 - val_loss: 7.8822e-06
Epoch 89/512
512/512 - 0s - loss: 7.8893e-04 - val_loss: 7.8727e-06
Epoch 90/512
512/512 - 0s - loss: 7.9488e-04 - val_loss: 7.9313e-06
Epoch 91/512
512/512 - 0s - loss: 7.9774e-04 - val_loss: 7.8267e-06
Epoch 92/512
512/512 - 0s - loss: 7.8738e-04 - val_loss: 7.6720e-06
Epoch 93/512
512/512 - 0s - loss: 7.8447e-04 - val_loss: 7.6599e-06
Epoch 94/512
512/512 - 0s - loss: 7.7639e-04 - val_loss: 7.7510e-06
Epoch 95/512
512/512 - 0s - loss: 7.8979e-04 - val_loss: 7.6840e-06
Epoch 96/512
512/512 - 0s - loss: 7.7862e-04 - val_loss: 7.4986e-06
Epoch 97/512
512/512 - 0s - loss: 7.5760e-04 - val_loss: 7.7691e-06
Epoch 98/512
512/512 - 0s - loss: 7.9037e-04 - val_loss: 7.7529e-06
Epoch 99/512
512/512 - 0s - loss: 7.7318e-04 - val_loss: 7.4186e-06
Epoch 100/512
512/512 - 0s - loss: 7.5426e-04 - val_loss: 7.4923e-06
Epoch 101/512
512/512 - 0s - loss: 7.6858e-04 - val_loss: 7.5816e-06
Epoch 102/512
512/512 - 0s - loss: 7.6400e-04 - val_loss: 7.5707e-06
Epoch 103/512
512/512 - 0s - loss: 7.5611e-04 - val_loss: 7.6610e-06
Epoch 104/512
512/512 - 0s - loss: 7.6776e-04 - val_loss: 7.5538e-06
Epoch 105/512
512/512 - 0s - loss: 7.5946e-04 - val_loss: 7.3738e-06
Epoch 106/512
512/512 - 0s - loss: 7.4116e-04 - val_loss: 7.5219e-06
Epoch 107/512
512/512 - 0s - loss: 7.6085e-04 - val_loss: 7.5703e-06
Epoch 108/512
512/512 - 0s - loss: 7.6344e-04 - val_loss: 7.1896e-06
Epoch 109/512
512/512 - 0s - loss: 7.2197e-04 - val_loss: 7.4280e-06
Epoch 110/512
512/512 - 0s - loss: 7.6013e-04 - val_loss: 7.6114e-06
Epoch 111/512
512/512 - 0s - loss: 7.5964e-04 - val_loss: 7.2001e-06
Epoch 112/512
512/512 - 0s - loss: 7.3022e-04 - val_loss: 7.0542e-06
Epoch 113/512
512/512 - 0s - loss: 7.3129e-04 - val_loss: 7.3436e-06
Epoch 114/512
512/512 - 0s - loss: 7.5467e-04 - val_loss: 7.3190e-06
Epoch 115/512
512/512 - 0s - loss: 7.3324e-04 - val_loss: 7.1676e-06
Epoch 116/512
512/512 - 0s - loss: 7.2815e-04 - val_loss: 7.3056e-06
Epoch 117/512
512/512 - 0s - loss: 7.3919e-04 - val_loss: 7.3403e-06
Epoch 118/512
512/512 - 0s - loss: 7.3711e-04 - val_loss: 7.2487e-06
Epoch 119/512
512/512 - 0s - loss: 7.2598e-04 - val_loss: 7.2045e-06
Epoch 120/512
512/512 - 0s - loss: 7.3020e-04 - val_loss: 7.2227e-06
Epoch 121/512
512/512 - 0s - loss: 7.2888e-04 - val_loss: 7.1797e-06
Epoch 122/512
512/512 - 0s - loss: 7.2989e-04 - val_loss: 7.0900e-06
Epoch 123/512
512/512 - 0s - loss: 7.1544e-04 - val_loss: 7.1568e-06
Epoch 124/512
512/512 - 0s - loss: 7.2596e-04 - val_loss: 7.2199e-06
Epoch 125/512
512/512 - 0s - loss: 7.2849e-04 - val_loss: 7.0295e-06
Epoch 126/512
512/512 - 0s - loss: 7.1066e-04 - val_loss: 7.0421e-06
Epoch 127/512
512/512 - 0s - loss: 7.1319e-04 - val_loss: 7.2759e-06
Epoch 128/512
512/512 - 0s - loss: 7.3382e-04 - val_loss: 7.1087e-06
Epoch 129/512
512/512 - 0s - loss: 7.0749e-04 - val_loss: 6.9422e-06
Epoch 130/512
512/512 - 0s - loss: 7.0733e-04 - val_loss: 7.0976e-06
Epoch 131/512
512/512 - 0s - loss: 7.1924e-04 - val_loss: 7.0634e-06
Epoch 132/512
512/512 - 0s - loss: 7.0743e-04 - val_loss: 7.0157e-06
Epoch 133/512
512/512 - 0s - loss: 7.1060e-04 - val_loss: 7.0366e-06
Epoch 134/512
512/512 - 0s - loss: 7.0689e-04 - val_loss: 7.0176e-06
Epoch 135/512
512/512 - 0s - loss: 7.0661e-04 - val_loss: 7.0075e-06
Epoch 136/512
512/512 - 0s - loss: 7.0610e-04 - val_loss: 6.9759e-06
Epoch 137/512
512/512 - 0s - loss: 7.0442e-04 - val_loss: 6.8740e-06
Epoch 138/512
512/512 - 0s - loss: 6.9637e-04 - val_loss: 6.9480e-06
Epoch 139/512
512/512 - 0s - loss: 7.0348e-04 - val_loss: 7.0042e-06
Epoch 140/512
512/512 - 0s - loss: 6.9919e-04 - val_loss: 6.9695e-06
Epoch 141/512
512/512 - 0s - loss: 6.9982e-04 - val_loss: 6.8666e-06
Epoch 142/512
512/512 - 0s - loss: 6.9554e-04 - val_loss: 6.7676e-06
Epoch 143/512
512/512 - 0s - loss: 6.8762e-04 - val_loss: 6.8943e-06
Epoch 144/512
512/512 - 0s - loss: 6.9914e-04 - val_loss: 6.9311e-06
Epoch 145/512
512/512 - 0s - loss: 6.9321e-04 - val_loss: 6.8157e-06
Epoch 146/512
512/512 - 0s - loss: 6.8743e-04 - val_loss: 6.7563e-06
Epoch 147/512
512/512 - 0s - loss: 6.8442e-04 - val_loss: 6.8687e-06
Epoch 148/512
512/512 - 0s - loss: 6.9113e-04 - val_loss: 6.9349e-06
Epoch 149/512
512/512 - 0s - loss: 6.9453e-04 - val_loss: 6.7300e-06
Epoch 150/512
512/512 - 0s - loss: 6.7367e-04 - val_loss: 6.7316e-06
Epoch 151/512
512/512 - 0s - loss: 6.8702e-04 - val_loss: 6.7651e-06
Epoch 152/512
512/512 - 0s - loss: 6.8241e-04 - val_loss: 6.7097e-06
Epoch 153/512
512/512 - 0s - loss: 6.7665e-04 - val_loss: 6.7054e-06
Epoch 154/512
512/512 - 0s - loss: 6.7911e-04 - val_loss: 6.7276e-06
Epoch 155/512
512/512 - 0s - loss: 6.7688e-04 - val_loss: 6.7728e-06
Epoch 156/512
512/512 - 0s - loss: 6.8233e-04 - val_loss: 6.6199e-06
Epoch 157/512
512/512 - 0s - loss: 6.6655e-04 - val_loss: 6.5601e-06
Epoch 158/512
512/512 - 0s - loss: 6.6817e-04 - val_loss: 6.7198e-06
Epoch 159/512
512/512 - 0s - loss: 6.7884e-04 - val_loss: 6.7241e-06
Epoch 160/512
512/512 - 0s - loss: 6.7478e-04 - val_loss: 6.4636e-06
Epoch 161/512
512/512 - 0s - loss: 6.5374e-04 - val_loss: 6.5530e-06
Epoch 162/512
512/512 - 0s - loss: 6.7256e-04 - val_loss: 6.6282e-06
Epoch 163/512
512/512 - 0s - loss: 6.6695e-04 - val_loss: 6.5580e-06
Epoch 164/512
512/512 - 0s - loss: 6.5566e-04 - val_loss: 6.6188e-06
Epoch 165/512
512/512 - 0s - loss: 6.6929e-04 - val_loss: 6.5172e-06
Epoch 166/512
512/512 - 0s - loss: 6.5877e-04 - val_loss: 6.4318e-06
Epoch 167/512
512/512 - 0s - loss: 6.5139e-04 - val_loss: 6.5162e-06
Epoch 168/512
512/512 - 0s - loss: 6.5765e-04 - val_loss: 6.6420e-06
Epoch 169/512
512/512 - 0s - loss: 6.5662e-04 - val_loss: 6.6131e-06
Epoch 170/512
512/512 - 0s - loss: 6.6091e-04 - val_loss: 6.4031e-06
Epoch 171/512
512/512 - 0s - loss: 6.4202e-04 - val_loss: 6.3941e-06
Epoch 172/512
512/512 - 0s - loss: 6.5061e-04 - val_loss: 6.4630e-06
Epoch 173/512
512/512 - 0s - loss: 6.5233e-04 - val_loss: 6.4001e-06
Epoch 174/512
512/512 - 0s - loss: 6.4519e-04 - val_loss: 6.3419e-06
Epoch 175/512
512/512 - 0s - loss: 6.4323e-04 - val_loss: 6.3358e-06
Epoch 176/512
512/512 - 0s - loss: 6.4410e-04 - val_loss: 6.3059e-06
Epoch 177/512
512/512 - 0s - loss: 6.3360e-04 - val_loss: 6.4034e-06
Epoch 178/512
512/512 - 0s - loss: 6.5096e-04 - val_loss: 6.2703e-06
Epoch 179/512
512/512 - 0s - loss: 6.2748e-04 - val_loss: 6.2229e-06
Epoch 180/512
512/512 - 0s - loss: 6.3403e-04 - val_loss: 6.3796e-06
Epoch 181/512
512/512 - 0s - loss: 6.3856e-04 - val_loss: 6.3625e-06
Epoch 182/512
512/512 - 0s - loss: 6.3076e-04 - val_loss: 6.3032e-06
Epoch 183/512
512/512 - 0s - loss: 6.3125e-04 - val_loss: 6.2242e-06
Epoch 184/512
512/512 - 0s - loss: 6.2632e-04 - val_loss: 6.1919e-06
Epoch 185/512
512/512 - 0s - loss: 6.2540e-04 - val_loss: 6.2071e-06
Epoch 186/512
512/512 - 0s - loss: 6.2263e-04 - val_loss: 6.2181e-06
Epoch 187/512
512/512 - 0s - loss: 6.2710e-04 - val_loss: 6.1421e-06
Epoch 188/512
512/512 - 0s - loss: 6.1551e-04 - val_loss: 6.0931e-06
Epoch 189/512
512/512 - 0s - loss: 6.1588e-04 - val_loss: 6.1562e-06
Epoch 190/512
512/512 - 0s - loss: 6.2105e-04 - val_loss: 6.0857e-06
Epoch 191/512
512/512 - 0s - loss: 6.1205e-04 - val_loss: 6.0400e-06
Epoch 192/512
512/512 - 0s - loss: 6.0581e-04 - val_loss: 6.1244e-06
Epoch 193/512
512/512 - 0s - loss: 6.1466e-04 - val_loss: 6.1208e-06
Epoch 194/512
512/512 - 0s - loss: 6.1145e-04 - val_loss: 5.9509e-06
Epoch 195/512
512/512 - 0s - loss: 6.0017e-04 - val_loss: 5.9043e-06
Epoch 196/512
512/512 - 0s - loss: 6.0083e-04 - val_loss: 6.0164e-06
Epoch 197/512
512/512 - 0s - loss: 6.0168e-04 - val_loss: 6.0383e-06
Epoch 198/512
512/512 - 0s - loss: 6.0652e-04 - val_loss: 5.8636e-06
Epoch 199/512
512/512 - 0s - loss: 5.9206e-04 - val_loss: 5.7496e-06
Epoch 200/512
512/512 - 0s - loss: 5.8962e-04 - val_loss: 5.8171e-06
Epoch 201/512
512/512 - 0s - loss: 5.8909e-04 - val_loss: 5.9574e-06
Epoch 202/512
512/512 - 0s - loss: 5.9842e-04 - val_loss: 5.8579e-06
Epoch 203/512
512/512 - 0s - loss: 5.8575e-04 - val_loss: 5.7008e-06
Epoch 204/512
512/512 - 0s - loss: 5.7677e-04 - val_loss: 5.8303e-06
Epoch 205/512
512/512 - 0s - loss: 5.9187e-04 - val_loss: 5.7649e-06
Epoch 206/512
512/512 - 0s - loss: 5.7594e-04 - val_loss: 5.6520e-06
Epoch 207/512
512/512 - 0s - loss: 5.7157e-04 - val_loss: 5.7722e-06
Epoch 208/512
512/512 - 0s - loss: 5.8328e-04 - val_loss: 5.7240e-06
Epoch 209/512
512/512 - 0s - loss: 5.6710e-04 - val_loss: 5.7014e-06
Epoch 210/512
512/512 - 0s - loss: 5.7428e-04 - val_loss: 5.6199e-06
Epoch 211/512
512/512 - 0s - loss: 5.6527e-04 - val_loss: 5.5627e-06
Epoch 212/512
512/512 - 0s - loss: 5.5998e-04 - val_loss: 5.6521e-06
Epoch 213/512
512/512 - 0s - loss: 5.7011e-04 - val_loss: 5.6027e-06
Epoch 214/512
512/512 - 0s - loss: 5.6132e-04 - val_loss: 5.4418e-06
Epoch 215/512
512/512 - 0s - loss: 5.5047e-04 - val_loss: 5.4474e-06
Epoch 216/512
512/512 - 0s - loss: 5.5150e-04 - val_loss: 5.5983e-06
Epoch 217/512
512/512 - 0s - loss: 5.6152e-04 - val_loss: 5.4814e-06
Epoch 218/512
512/512 - 0s - loss: 5.4278e-04 - val_loss: 5.3862e-06
Epoch 219/512
512/512 - 0s - loss: 5.4639e-04 - val_loss: 5.3786e-06
Epoch 220/512
512/512 - 0s - loss: 5.3859e-04 - val_loss: 5.4849e-06
Epoch 221/512
512/512 - 0s - loss: 5.4824e-04 - val_loss: 5.4182e-06
Epoch 222/512
512/512 - 0s - loss: 5.3591e-04 - val_loss: 5.2918e-06
Epoch 223/512
512/512 - 0s - loss: 5.3094e-04 - val_loss: 5.3362e-06
Epoch 224/512
512/512 - 0s - loss: 5.3774e-04 - val_loss: 5.2695e-06
Epoch 225/512
512/512 - 0s - loss: 5.2843e-04 - val_loss: 5.1424e-06
Epoch 226/512
512/512 - 0s - loss: 5.2006e-04 - val_loss: 5.2365e-06
Epoch 227/512
512/512 - 0s - loss: 5.2894e-04 - val_loss: 5.1695e-06
Epoch 228/512
512/512 - 0s - loss: 5.1938e-04 - val_loss: 5.0664e-06
Epoch 229/512
512/512 - 0s - loss: 5.1175e-04 - val_loss: 5.1750e-06
Epoch 230/512
512/512 - 0s - loss: 5.1739e-04 - val_loss: 5.1972e-06
Epoch 231/512
512/512 - 0s - loss: 5.1851e-04 - val_loss: 4.9924e-06
Epoch 232/512
512/512 - 0s - loss: 4.9933e-04 - val_loss: 4.9469e-06
Epoch 233/512
512/512 - 0s - loss: 5.0546e-04 - val_loss: 5.0254e-06
Epoch 234/512
512/512 - 0s - loss: 5.0372e-04 - val_loss: 5.0091e-06
Epoch 235/512
512/512 - 0s - loss: 4.9945e-04 - val_loss: 4.9137e-06
Epoch 236/512
512/512 - 0s - loss: 4.9437e-04 - val_loss: 4.8632e-06
Epoch 237/512
512/512 - 0s - loss: 4.9112e-04 - val_loss: 4.8900e-06
Epoch 238/512
512/512 - 0s - loss: 4.9124e-04 - val_loss: 4.8495e-06
Epoch 239/512
512/512 - 0s - loss: 4.8613e-04 - val_loss: 4.7819e-06
Epoch 240/512
512/512 - 0s - loss: 4.8027e-04 - val_loss: 4.7736e-06
Epoch 241/512
512/512 - 0s - loss: 4.7897e-04 - val_loss: 4.7796e-06
Epoch 242/512
512/512 - 0s - loss: 4.7796e-04 - val_loss: 4.7314e-06
Epoch 243/512
512/512 - 0s - loss: 4.7391e-04 - val_loss: 4.6269e-06
Epoch 244/512
512/512 - 0s - loss: 4.6520e-04 - val_loss: 4.6279e-06
Epoch 245/512
512/512 - 0s - loss: 4.6622e-04 - val_loss: 4.6605e-06
Epoch 246/512
512/512 - 0s - loss: 4.6647e-04 - val_loss: 4.5638e-06
Epoch 247/512
512/512 - 0s - loss: 4.5454e-04 - val_loss: 4.5496e-06
Epoch 248/512
512/512 - 0s - loss: 4.5358e-04 - val_loss: 4.6064e-06
Epoch 249/512
512/512 - 0s - loss: 4.6124e-04 - val_loss: 4.4619e-06
Epoch 250/512
512/512 - 0s - loss: 4.4157e-04 - val_loss: 4.4092e-06
Epoch 251/512
512/512 - 0s - loss: 4.4830e-04 - val_loss: 4.3732e-06
Epoch 252/512
512/512 - 0s - loss: 4.3752e-04 - val_loss: 4.3750e-06
Epoch 253/512
512/512 - 0s - loss: 4.4329e-04 - val_loss: 4.3194e-06
Epoch 254/512
512/512 - 0s - loss: 4.2800e-04 - val_loss: 4.3334e-06
Epoch 255/512
512/512 - 0s - loss: 4.3557e-04 - val_loss: 4.3150e-06
Epoch 256/512
512/512 - 0s - loss: 4.2935e-04 - val_loss: 4.1653e-06
Epoch 257/512
512/512 - 0s - loss: 4.1962e-04 - val_loss: 4.1385e-06
Epoch 258/512
512/512 - 0s - loss: 4.1561e-04 - val_loss: 4.2553e-06
Epoch 259/512
512/512 - 0s - loss: 4.2583e-04 - val_loss: 4.1862e-06
Epoch 260/512
512/512 - 0s - loss: 4.0925e-04 - val_loss: 4.0725e-06
Epoch 261/512
512/512 - 0s - loss: 4.0791e-04 - val_loss: 4.0550e-06
Epoch 262/512
512/512 - 0s - loss: 4.0908e-04 - val_loss: 3.9988e-06
Epoch 263/512
512/512 - 0s - loss: 3.9916e-04 - val_loss: 3.9561e-06
Epoch 264/512
512/512 - 0s - loss: 4.0200e-04 - val_loss: 3.8388e-06
Epoch 265/512
512/512 - 0s - loss: 3.8591e-04 - val_loss: 3.9101e-06
Epoch 266/512
512/512 - 0s - loss: 3.9297e-04 - val_loss: 4.0096e-06
Epoch 267/512
512/512 - 0s - loss: 3.9739e-04 - val_loss: 3.7651e-06
Epoch 268/512
512/512 - 0s - loss: 3.7300e-04 - val_loss: 3.6902e-06
Epoch 269/512
512/512 - 0s - loss: 3.7757e-04 - val_loss: 3.8259e-06
Epoch 270/512
512/512 - 0s - loss: 3.8071e-04 - val_loss: 3.7806e-06
Epoch 271/512
512/512 - 0s - loss: 3.7232e-04 - val_loss: 3.6605e-06
Epoch 272/512
512/512 - 0s - loss: 3.6650e-04 - val_loss: 3.6215e-06
Epoch 273/512
512/512 - 0s - loss: 3.6306e-04 - val_loss: 3.6180e-06
Epoch 274/512
512/512 - 0s - loss: 3.6257e-04 - val_loss: 3.5870e-06
Epoch 275/512
512/512 - 0s - loss: 3.5803e-04 - val_loss: 3.4713e-06
Epoch 276/512
512/512 - 0s - loss: 3.4631e-04 - val_loss: 3.5252e-06
Epoch 277/512
512/512 - 0s - loss: 3.5376e-04 - val_loss: 3.5276e-06
Epoch 278/512
512/512 - 0s - loss: 3.4700e-04 - val_loss: 3.4302e-06
Epoch 279/512
512/512 - 0s - loss: 3.4178e-04 - val_loss: 3.3312e-06
Epoch 280/512
512/512 - 0s - loss: 3.3357e-04 - val_loss: 3.3395e-06
Epoch 281/512
512/512 - 0s - loss: 3.3447e-04 - val_loss: 3.3696e-06
Epoch 282/512
512/512 - 0s - loss: 3.3588e-04 - val_loss: 3.2097e-06
Epoch 283/512
512/512 - 0s - loss: 3.1965e-04 - val_loss: 3.1923e-06
Epoch 284/512
512/512 - 0s - loss: 3.2261e-04 - val_loss: 3.2474e-06
Epoch 285/512
512/512 - 0s - loss: 3.2033e-04 - val_loss: 3.2069e-06
Epoch 286/512
512/512 - 0s - loss: 3.1695e-04 - val_loss: 3.1254e-06
Epoch 287/512
512/512 - 0s - loss: 3.0826e-04 - val_loss: 3.0793e-06
Epoch 288/512
512/512 - 0s - loss: 3.0876e-04 - val_loss: 3.0195e-06
Epoch 289/512
512/512 - 0s - loss: 3.0308e-04 - val_loss: 2.9693e-06
Epoch 290/512
512/512 - 0s - loss: 2.9989e-04 - val_loss: 2.9293e-06
Epoch 291/512
512/512 - 0s - loss: 2.9387e-04 - val_loss: 2.9280e-06
Epoch 292/512
512/512 - 0s - loss: 2.9405e-04 - val_loss: 2.9088e-06
Epoch 293/512
512/512 - 0s - loss: 2.8963e-04 - val_loss: 2.7945e-06
Epoch 294/512
512/512 - 0s - loss: 2.8199e-04 - val_loss: 2.7824e-06
Epoch 295/512
512/512 - 0s - loss: 2.8117e-04 - val_loss: 2.7776e-06
Epoch 296/512
512/512 - 0s - loss: 2.7755e-04 - val_loss: 2.7400e-06
Epoch 297/512
512/512 - 0s - loss: 2.7395e-04 - val_loss: 2.6883e-06
Epoch 298/512
512/512 - 0s - loss: 2.6800e-04 - val_loss: 2.6764e-06
Epoch 299/512
512/512 - 0s - loss: 2.6554e-04 - val_loss: 2.6801e-06
Epoch 300/512
512/512 - 0s - loss: 2.6462e-04 - val_loss: 2.6293e-06
Epoch 301/512
512/512 - 0s - loss: 2.5877e-04 - val_loss: 2.5545e-06
Epoch 302/512
512/512 - 0s - loss: 2.5405e-04 - val_loss: 2.5224e-06
Epoch 303/512
512/512 - 0s - loss: 2.5041e-04 - val_loss: 2.5088e-06
Epoch 304/512
512/512 - 0s - loss: 2.4986e-04 - val_loss: 2.4575e-06
Epoch 305/512
512/512 - 0s - loss: 2.4203e-04 - val_loss: 2.4318e-06
Epoch 306/512
512/512 - 0s - loss: 2.4156e-04 - val_loss: 2.4246e-06
Epoch 307/512
512/512 - 0s - loss: 2.4001e-04 - val_loss: 2.3222e-06
Epoch 308/512
512/512 - 0s - loss: 2.3121e-04 - val_loss: 2.2532e-06
Epoch 309/512
512/512 - 0s - loss: 2.2621e-04 - val_loss: 2.3244e-06
Epoch 310/512
512/512 - 0s - loss: 2.3242e-04 - val_loss: 2.2665e-06
Epoch 311/512
512/512 - 0s - loss: 2.2388e-04 - val_loss: 2.1075e-06
Epoch 312/512
512/512 - 0s - loss: 2.1308e-04 - val_loss: 2.1469e-06
Epoch 313/512
512/512 - 0s - loss: 2.1817e-04 - val_loss: 2.1954e-06
Epoch 314/512
512/512 - 0s - loss: 2.1855e-04 - val_loss: 2.0487e-06
Epoch 315/512
512/512 - 0s - loss: 2.0320e-04 - val_loss: 2.0164e-06
Epoch 316/512
512/512 - 0s - loss: 2.0434e-04 - val_loss: 2.0970e-06
Epoch 317/512
512/512 - 0s - loss: 2.0918e-04 - val_loss: 2.0184e-06
Epoch 318/512
512/512 - 0s - loss: 1.9896e-04 - val_loss: 1.8804e-06
Epoch 319/512
512/512 - 0s - loss: 1.8865e-04 - val_loss: 1.9474e-06
Epoch 320/512
512/512 - 0s - loss: 1.9730e-04 - val_loss: 1.9793e-06
Epoch 321/512
512/512 - 0s - loss: 1.9262e-04 - val_loss: 1.8572e-06
Epoch 322/512
512/512 - 0s - loss: 1.8146e-04 - val_loss: 1.8379e-06
Epoch 323/512
512/512 - 0s - loss: 1.8468e-04 - val_loss: 1.8456e-06
Epoch 324/512
512/512 - 0s - loss: 1.8244e-04 - val_loss: 1.7719e-06
Epoch 325/512
512/512 - 0s - loss: 1.7538e-04 - val_loss: 1.7165e-06
Epoch 326/512
512/512 - 0s - loss: 1.7344e-04 - val_loss: 1.7158e-06
Epoch 327/512
512/512 - 0s - loss: 1.7030e-04 - val_loss: 1.7106e-06
Epoch 328/512
512/512 - 0s - loss: 1.6939e-04 - val_loss: 1.6711e-06
Epoch 329/512
512/512 - 0s - loss: 1.6524e-04 - val_loss: 1.6030e-06
Epoch 330/512
512/512 - 0s - loss: 1.6048e-04 - val_loss: 1.5825e-06
Epoch 331/512
512/512 - 0s - loss: 1.5843e-04 - val_loss: 1.5877e-06
Epoch 332/512
512/512 - 0s - loss: 1.5794e-04 - val_loss: 1.5475e-06
Epoch 333/512
512/512 - 0s - loss: 1.5274e-04 - val_loss: 1.4922e-06
Epoch 334/512
512/512 - 0s - loss: 1.4901e-04 - val_loss: 1.4815e-06
Epoch 335/512
512/512 - 0s - loss: 1.4930e-04 - val_loss: 1.4460e-06
Epoch 336/512
512/512 - 0s - loss: 1.4285e-04 - val_loss: 1.4296e-06
Epoch 337/512
512/512 - 0s - loss: 1.4358e-04 - val_loss: 1.4057e-06
Epoch 338/512
512/512 - 0s - loss: 1.3825e-04 - val_loss: 1.3886e-06
Epoch 339/512
512/512 - 0s - loss: 1.3787e-04 - val_loss: 1.3554e-06
Epoch 340/512
512/512 - 0s - loss: 1.3467e-04 - val_loss: 1.2963e-06
Epoch 341/512
512/512 - 0s - loss: 1.2920e-04 - val_loss: 1.2897e-06
Epoch 342/512
512/512 - 0s - loss: 1.2946e-04 - val_loss: 1.2938e-06
Epoch 343/512
512/512 - 0s - loss: 1.2772e-04 - val_loss: 1.2436e-06
Epoch 344/512
512/512 - 0s - loss: 1.2190e-04 - val_loss: 1.2280e-06
Epoch 345/512
512/512 - 0s - loss: 1.2282e-04 - val_loss: 1.2098e-06
Epoch 346/512
512/512 - 0s - loss: 1.1943e-04 - val_loss: 1.1540e-06
Epoch 347/512
512/512 - 0s - loss: 1.1413e-04 - val_loss: 1.1505e-06
Epoch 348/512
512/512 - 0s - loss: 1.1591e-04 - val_loss: 1.1383e-06
Epoch 349/512
512/512 - 0s - loss: 1.1244e-04 - val_loss: 1.0827e-06
Epoch 350/512
512/512 - 0s - loss: 1.0708e-04 - val_loss: 1.0828e-06
Epoch 351/512
512/512 - 0s - loss: 1.0799e-04 - val_loss: 1.0898e-06
Epoch 352/512
512/512 - 0s - loss: 1.0734e-04 - val_loss: 1.0254e-06
Epoch 353/512
512/512 - 0s - loss: 1.0054e-04 - val_loss: 1.0010e-06
Epoch 354/512
512/512 - 0s - loss: 1.0020e-04 - val_loss: 1.0181e-06
Epoch 355/512
512/512 - 0s - loss: 1.0036e-04 - val_loss: 9.7934e-07
Epoch 356/512
512/512 - 0s - loss: 9.6311e-05 - val_loss: 9.2907e-07
Epoch 357/512
512/512 - 0s - loss: 9.2520e-05 - val_loss: 9.3309e-07
Epoch 358/512
512/512 - 0s - loss: 9.3181e-05 - val_loss: 9.3115e-07
Epoch 359/512
512/512 - 0s - loss: 9.1277e-05 - val_loss: 8.8693e-07
Epoch 360/512
512/512 - 0s - loss: 8.7300e-05 - val_loss: 8.5953e-07
Epoch 361/512
512/512 - 0s - loss: 8.6155e-05 - val_loss: 8.5945e-07
Epoch 362/512
512/512 - 0s - loss: 8.5102e-05 - val_loss: 8.3903e-07
Epoch 363/512
512/512 - 0s - loss: 8.2046e-05 - val_loss: 8.2150e-07
Epoch 364/512
512/512 - 0s - loss: 8.1377e-05 - val_loss: 8.0071e-07
Epoch 365/512
512/512 - 0s - loss: 7.8922e-05 - val_loss: 7.7302e-07
Epoch 366/512
512/512 - 0s - loss: 7.7259e-05 - val_loss: 7.5098e-07
Epoch 367/512
512/512 - 0s - loss: 7.4374e-05 - val_loss: 7.4968e-07
Epoch 368/512
512/512 - 0s - loss: 7.4819e-05 - val_loss: 7.3836e-07
Epoch 369/512
512/512 - 0s - loss: 7.2039e-05 - val_loss: 7.0714e-07
Epoch 370/512
512/512 - 0s - loss: 6.9867e-05 - val_loss: 6.9324e-07
Epoch 371/512
512/512 - 0s - loss: 6.9542e-05 - val_loss: 6.6870e-07
Epoch 372/512
512/512 - 0s - loss: 6.5792e-05 - val_loss: 6.6507e-07
Epoch 373/512
512/512 - 0s - loss: 6.5912e-05 - val_loss: 6.7019e-07
Epoch 374/512
512/512 - 0s - loss: 6.5644e-05 - val_loss: 6.3050e-07
Epoch 375/512
512/512 - 0s - loss: 6.1719e-05 - val_loss: 5.9985e-07
Epoch 376/512
512/512 - 0s - loss: 6.0313e-05 - val_loss: 6.0444e-07
Epoch 377/512
512/512 - 0s - loss: 6.0505e-05 - val_loss: 5.9505e-07
Epoch 378/512
512/512 - 0s - loss: 5.8018e-05 - val_loss: 5.7403e-07
Epoch 379/512
512/512 - 0s - loss: 5.7015e-05 - val_loss: 5.5561e-07
Epoch 380/512
512/512 - 0s - loss: 5.5127e-05 - val_loss: 5.4653e-07
Epoch 381/512
512/512 - 0s - loss: 5.4306e-05 - val_loss: 5.3330e-07
Epoch 382/512
512/512 - 0s - loss: 5.2861e-05 - val_loss: 5.1795e-07
Epoch 383/512
512/512 - 0s - loss: 5.1421e-05 - val_loss: 5.0788e-07
Epoch 384/512
512/512 - 0s - loss: 5.0956e-05 - val_loss: 4.8478e-07
Epoch 385/512
512/512 - 0s - loss: 4.8047e-05 - val_loss: 4.7686e-07
Epoch 386/512
512/512 - 0s - loss: 4.7901e-05 - val_loss: 4.7997e-07
Epoch 387/512
512/512 - 0s - loss: 4.7387e-05 - val_loss: 4.6093e-07
Epoch 388/512
512/512 - 0s - loss: 4.5147e-05 - val_loss: 4.4387e-07
Epoch 389/512
512/512 - 0s - loss: 4.4331e-05 - val_loss: 4.3703e-07
Epoch 390/512
512/512 - 0s - loss: 4.3294e-05 - val_loss: 4.2881e-07
Epoch 391/512
512/512 - 0s - loss: 4.2456e-05 - val_loss: 4.1328e-07
Epoch 392/512
512/512 - 0s - loss: 4.0833e-05 - val_loss: 4.0364e-07
Epoch 393/512
512/512 - 0s - loss: 4.0084e-05 - val_loss: 3.9714e-07
Epoch 394/512
512/512 - 0s - loss: 3.9227e-05 - val_loss: 3.8267e-07
Epoch 395/512
512/512 - 0s - loss: 3.7844e-05 - val_loss: 3.7390e-07
Epoch 396/512
512/512 - 0s - loss: 3.7422e-05 - val_loss: 3.6287e-07
Epoch 397/512
512/512 - 0s - loss: 3.5820e-05 - val_loss: 3.5579e-07
Epoch 398/512
512/512 - 0s - loss: 3.5474e-05 - val_loss: 3.4708e-07
Epoch 399/512
512/512 - 0s - loss: 3.4356e-05 - val_loss: 3.3275e-07
Epoch 400/512
512/512 - 0s - loss: 3.3119e-05 - val_loss: 3.3068e-07
Epoch 401/512
512/512 - 0s - loss: 3.2772e-05 - val_loss: 3.2403e-07
Epoch 402/512
512/512 - 0s - loss: 3.1920e-05 - val_loss: 3.1099e-07
Epoch 403/512
512/512 - 0s - loss: 3.0587e-05 - val_loss: 3.0197e-07
Epoch 404/512
512/512 - 0s - loss: 3.0067e-05 - val_loss: 3.0036e-07
Epoch 405/512
512/512 - 0s - loss: 2.9812e-05 - val_loss: 2.8448e-07
Epoch 406/512
512/512 - 0s - loss: 2.8089e-05 - val_loss: 2.7423e-07
Epoch 407/512
512/512 - 0s - loss: 2.7343e-05 - val_loss: 2.8044e-07
Epoch 408/512
512/512 - 0s - loss: 2.7894e-05 - val_loss: 2.7057e-07
Epoch 409/512
512/512 - 0s - loss: 2.6290e-05 - val_loss: 2.4969e-07
Epoch 410/512
512/512 - 0s - loss: 2.4827e-05 - val_loss: 2.5122e-07
Epoch 411/512
512/512 - 0s - loss: 2.5233e-05 - val_loss: 2.5476e-07
Epoch 412/512
512/512 - 0s - loss: 2.4746e-05 - val_loss: 2.3983e-07
Epoch 413/512
512/512 - 0s - loss: 2.3366e-05 - val_loss: 2.2674e-07
Epoch 414/512
512/512 - 0s - loss: 2.2643e-05 - val_loss: 2.2601e-07
Epoch 415/512
512/512 - 0s - loss: 2.2684e-05 - val_loss: 2.2277e-07
Epoch 416/512
512/512 - 0s - loss: 2.1868e-05 - val_loss: 2.1068e-07
Epoch 417/512
512/512 - 0s - loss: 2.0637e-05 - val_loss: 2.1063e-07
Epoch 418/512
512/512 - 0s - loss: 2.0926e-05 - val_loss: 2.0788e-07
Epoch 419/512
512/512 - 0s - loss: 2.0361e-05 - val_loss: 1.9230e-07
Epoch 420/512
512/512 - 0s - loss: 1.8941e-05 - val_loss: 1.8840e-07
Epoch 421/512
512/512 - 0s - loss: 1.8949e-05 - val_loss: 1.9193e-07
Epoch 422/512
512/512 - 0s - loss: 1.8749e-05 - val_loss: 1.8448e-07
Epoch 423/512
512/512 - 0s - loss: 1.7812e-05 - val_loss: 1.7516e-07
Epoch 424/512
512/512 - 0s - loss: 1.7405e-05 - val_loss: 1.7127e-07
Epoch 425/512
512/512 - 0s - loss: 1.6903e-05 - val_loss: 1.6787e-07
Epoch 426/512
512/512 - 0s - loss: 1.6607e-05 - val_loss: 1.6238e-07
Epoch 427/512
512/512 - 0s - loss: 1.6047e-05 - val_loss: 1.5460e-07
Epoch 428/512
512/512 - 0s - loss: 1.5217e-05 - val_loss: 1.5448e-07
Epoch 429/512
512/512 - 0s - loss: 1.5609e-05 - val_loss: 1.4890e-07
Epoch 430/512
512/512 - 0s - loss: 1.4524e-05 - val_loss: 1.4034e-07
Epoch 431/512
512/512 - 0s - loss: 1.4100e-05 - val_loss: 1.4140e-07
Epoch 432/512
512/512 - 0s - loss: 1.4103e-05 - val_loss: 1.3939e-07
Epoch 433/512
512/512 - 0s - loss: 1.3720e-05 - val_loss: 1.2884e-07
Epoch 434/512
512/512 - 0s - loss: 1.2765e-05 - val_loss: 1.2568e-07
Epoch 435/512
512/512 - 0s - loss: 1.2599e-05 - val_loss: 1.3084e-07
Epoch 436/512
512/512 - 0s - loss: 1.2884e-05 - val_loss: 1.2377e-07
Epoch 437/512
512/512 - 0s - loss: 1.1879e-05 - val_loss: 1.1507e-07
Epoch 438/512
512/512 - 0s - loss: 1.1414e-05 - val_loss: 1.1787e-07
Epoch 439/512
512/512 - 0s - loss: 1.1663e-05 - val_loss: 1.1534e-07
Epoch 440/512
512/512 - 0s - loss: 1.1180e-05 - val_loss: 1.0560e-07
Epoch 441/512
512/512 - 0s - loss: 1.0454e-05 - val_loss: 1.0325e-07
Epoch 442/512
512/512 - 0s - loss: 1.0409e-05 - val_loss: 1.0517e-07
Epoch 443/512
512/512 - 0s - loss: 1.0361e-05 - val_loss: 9.9857e-08
Epoch 444/512
512/512 - 0s - loss: 9.7419e-06 - val_loss: 9.4482e-08
Epoch 445/512
512/512 - 0s - loss: 9.4874e-06 - val_loss: 9.2993e-08
Epoch 446/512
512/512 - 0s - loss: 9.3033e-06 - val_loss: 9.1463e-08
Epoch 447/512
512/512 - 0s - loss: 9.0010e-06 - val_loss: 8.8945e-08
Epoch 448/512
512/512 - 0s - loss: 8.7802e-06 - val_loss: 8.6541e-08
Epoch 449/512
512/512 - 0s - loss: 8.4925e-06 - val_loss: 8.3479e-08
Epoch 450/512
512/512 - 0s - loss: 8.2748e-06 - val_loss: 8.0293e-08
Epoch 451/512
512/512 - 0s - loss: 7.9935e-06 - val_loss: 7.7583e-08
Epoch 452/512
512/512 - 0s - loss: 7.6767e-06 - val_loss: 7.7521e-08
Epoch 453/512
512/512 - 0s - loss: 7.6726e-06 - val_loss: 7.5207e-08
Epoch 454/512
512/512 - 0s - loss: 7.3199e-06 - val_loss: 7.1996e-08
Epoch 455/512
512/512 - 0s - loss: 7.0924e-06 - val_loss: 6.9975e-08
Epoch 456/512
512/512 - 0s - loss: 6.9384e-06 - val_loss: 6.7317e-08
Epoch 457/512
512/512 - 0s - loss: 6.6994e-06 - val_loss: 6.4881e-08
Epoch 458/512
512/512 - 0s - loss: 6.4916e-06 - val_loss: 6.3538e-08
Epoch 459/512
512/512 - 0s - loss: 6.3136e-06 - val_loss: 6.2116e-08
Epoch 460/512
512/512 - 0s - loss: 6.1320e-06 - val_loss: 6.0420e-08
Epoch 461/512
512/512 - 0s - loss: 5.9484e-06 - val_loss: 5.9017e-08
Epoch 462/512
512/512 - 0s - loss: 5.8348e-06 - val_loss: 5.6578e-08
Epoch 463/512
512/512 - 0s - loss: 5.5229e-06 - val_loss: 5.5625e-08
Epoch 464/512
512/512 - 0s - loss: 5.5368e-06 - val_loss: 5.3487e-08
Epoch 465/512
512/512 - 0s - loss: 5.2343e-06 - val_loss: 5.1248e-08
Epoch 466/512
512/512 - 0s - loss: 5.1289e-06 - val_loss: 4.9709e-08
Epoch 467/512
512/512 - 0s - loss: 4.9437e-06 - val_loss: 4.8903e-08
Epoch 468/512
512/512 - 0s - loss: 4.8483e-06 - val_loss: 4.7731e-08
Epoch 469/512
512/512 - 0s - loss: 4.6880e-06 - val_loss: 4.5997e-08
Epoch 470/512
512/512 - 0s - loss: 4.5096e-06 - val_loss: 4.4919e-08
Epoch 471/512
512/512 - 0s - loss: 4.4819e-06 - val_loss: 4.2731e-08
Epoch 472/512
512/512 - 0s - loss: 4.2034e-06 - val_loss: 4.1693e-08
Epoch 473/512
512/512 - 0s - loss: 4.1442e-06 - val_loss: 4.1459e-08
Epoch 474/512
512/512 - 0s - loss: 4.0736e-06 - val_loss: 3.9889e-08
Epoch 475/512
512/512 - 0s - loss: 3.9136e-06 - val_loss: 3.7510e-08
Epoch 476/512
512/512 - 0s - loss: 3.7038e-06 - val_loss: 3.7283e-08
Epoch 477/512
512/512 - 0s - loss: 3.7358e-06 - val_loss: 3.6478e-08
Epoch 478/512
512/512 - 0s - loss: 3.5745e-06 - val_loss: 3.4496e-08
Epoch 479/512
512/512 - 0s - loss: 3.4040e-06 - val_loss: 3.3938e-08
Epoch 480/512
512/512 - 0s - loss: 3.3894e-06 - val_loss: 3.3067e-08
Epoch 481/512
512/512 - 0s - loss: 3.2395e-06 - val_loss: 3.1905e-08
Epoch 482/512
512/512 - 0s - loss: 3.1508e-06 - val_loss: 3.1008e-08
Epoch 483/512
512/512 - 0s - loss: 3.0460e-06 - val_loss: 3.0458e-08
Epoch 484/512
512/512 - 0s - loss: 2.9980e-06 - val_loss: 2.9147e-08
Epoch 485/512
512/512 - 0s - loss: 2.8642e-06 - val_loss: 2.7884e-08
Epoch 486/512
512/512 - 0s - loss: 2.7510e-06 - val_loss: 2.7819e-08
Epoch 487/512
512/512 - 0s - loss: 2.7520e-06 - val_loss: 2.6915e-08
Epoch 488/512
512/512 - 0s - loss: 2.6413e-06 - val_loss: 2.5124e-08
Epoch 489/512
512/512 - 0s - loss: 2.4760e-06 - val_loss: 2.4869e-08
Epoch 490/512
512/512 - 0s - loss: 2.4921e-06 - val_loss: 2.4838e-08
Epoch 491/512
512/512 - 0s - loss: 2.4424e-06 - val_loss: 2.2997e-08
Epoch 492/512
512/512 - 0s - loss: 2.2697e-06 - val_loss: 2.2018e-08
Epoch 493/512
512/512 - 0s - loss: 2.2118e-06 - val_loss: 2.2584e-08
Epoch 494/512
512/512 - 0s - loss: 2.2553e-06 - val_loss: 2.1560e-08
Epoch 495/512
512/512 - 0s - loss: 2.0863e-06 - val_loss: 2.0247e-08
Epoch 496/512
512/512 - 0s - loss: 2.0147e-06 - val_loss: 2.0264e-08
Epoch 497/512
512/512 - 0s - loss: 2.0277e-06 - val_loss: 1.9691e-08
Epoch 498/512
512/512 - 0s - loss: 1.9185e-06 - val_loss: 1.8628e-08
Epoch 499/512
512/512 - 0s - loss: 1.8543e-06 - val_loss: 1.8271e-08
Epoch 500/512
512/512 - 0s - loss: 1.8138e-06 - val_loss: 1.7864e-08
Epoch 501/512
512/512 - 0s - loss: 1.7706e-06 - val_loss: 1.7007e-08
Epoch 502/512
512/512 - 0s - loss: 1.6878e-06 - val_loss: 1.6302e-08
Epoch 503/512
512/512 - 0s - loss: 1.6319e-06 - val_loss: 1.6319e-08
Epoch 504/512
512/512 - 0s - loss: 1.6251e-06 - val_loss: 1.5673e-08
Epoch 505/512
512/512 - 0s - loss: 1.5367e-06 - val_loss: 1.4932e-08
Epoch 506/512
512/512 - 0s - loss: 1.4820e-06 - val_loss: 1.5028e-08
Epoch 507/512
512/512 - 0s - loss: 1.4915e-06 - val_loss: 1.4367e-08
Epoch 508/512
512/512 - 0s - loss: 1.4033e-06 - val_loss: 1.3522e-08
Epoch 509/512
512/512 - 0s - loss: 1.3439e-06 - val_loss: 1.3461e-08
Epoch 510/512
512/512 - 0s - loss: 1.3384e-06 - val_loss: 1.3237e-08
Epoch 511/512
512/512 - 0s - loss: 1.3004e-06 - val_loss: 1.2419e-08
Epoch 512/512
512/512 - 0s - loss: 1.2272e-06 - val_loss: 1.1931e-08
2024-04-12 10:51:05.130724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7687e-06 - val_loss: 1.9811e-06
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4980e-06 - val_loss: 8.6520e-07
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.8423e-07 - val_loss: 8.5579e-07
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1127e-06 - val_loss: 1.5631e-06
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5155e-06 - val_loss: 1.1693e-06
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.6767e-07 - val_loss: 7.9329e-07
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6361e-07 - val_loss: 1.0852e-06
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2082e-06 - val_loss: 1.2344e-06
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0828e-06 - val_loss: 8.5599e-07
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1843e-07 - val_loss: 8.5053e-07
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3933e-07 - val_loss: 1.0627e-06
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0321e-06 - val_loss: 9.0968e-07
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.3372e-07 - val_loss: 7.6737e-07
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9319e-07 - val_loss: 8.6758e-07
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9305e-07 - val_loss: 8.8103e-07
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.2584e-07 - val_loss: 7.4181e-07
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.2606e-07 - val_loss: 7.3472e-07
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5993e-07 - val_loss: 7.8885e-07
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.7024e-07 - val_loss: 7.2061e-07
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.9024e-07 - val_loss: 6.6078e-07
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6631e-07 - val_loss: 6.8570e-07
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8801e-07 - val_loss: 6.7432e-07
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.5057e-07 - val_loss: 6.1569e-07
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.0652e-07 - val_loss: 6.0461e-07
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0832e-07 - val_loss: 6.1003e-07
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.9812e-07 - val_loss: 5.7624e-07
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.6154e-07 - val_loss: 5.4761e-07
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.4465e-07 - val_loss: 5.4454e-07
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.3988e-07 - val_loss: 5.3010e-07
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.1858e-07 - val_loss: 5.0126e-07
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.9432e-07 - val_loss: 4.8846e-07
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8537e-07 - val_loss: 4.8188e-07
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.7415e-07 - val_loss: 4.6179e-07
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5267e-07 - val_loss: 4.4220e-07
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3774e-07 - val_loss: 4.3435e-07
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.2986e-07 - val_loss: 4.2216e-07
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.1463e-07 - val_loss: 4.0335e-07
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9759e-07 - val_loss: 3.9191e-07
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8862e-07 - val_loss: 3.8258e-07
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.7705e-07 - val_loss: 3.6871e-07
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6308e-07 - val_loss: 3.5627e-07
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5210e-07 - val_loss: 3.4719e-07
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4228e-07 - val_loss: 3.3653e-07
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.3144e-07 - val_loss: 3.2382e-07
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1933e-07 - val_loss: 3.1416e-07
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1043e-07 - val_loss: 3.0604e-07
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0152e-07 - val_loss: 2.9550e-07
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9092e-07 - val_loss: 2.8500e-07
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.8152e-07 - val_loss: 2.7736e-07
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7360e-07 - val_loss: 2.6884e-07
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6481e-07 - val_loss: 2.5938e-07
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5571e-07 - val_loss: 2.5147e-07
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4823e-07 - val_loss: 2.4462e-07
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4107e-07 - val_loss: 2.3630e-07
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3258e-07 - val_loss: 2.2812e-07
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2514e-07 - val_loss: 2.2178e-07
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1868e-07 - val_loss: 2.1516e-07
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1177e-07 - val_loss: 2.0791e-07
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0463e-07 - val_loss: 2.0171e-07
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9856e-07 - val_loss: 1.9535e-07
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9219e-07 - val_loss: 1.8914e-07
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8644e-07 - val_loss: 1.8306e-07
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8003e-07 - val_loss: 1.7737e-07
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7480e-07 - val_loss: 1.7187e-07
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6922e-07 - val_loss: 1.6616e-07
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6370e-07 - val_loss: 1.6097e-07
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5866e-07 - val_loss: 1.5645e-07
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5392e-07 - val_loss: 1.5105e-07
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4849e-07 - val_loss: 1.4643e-07
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4417e-07 - val_loss: 1.4213e-07
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3988e-07 - val_loss: 1.3712e-07
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3494e-07 - val_loss: 1.3297e-07
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3107e-07 - val_loss: 1.2892e-07
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2680e-07 - val_loss: 1.2492e-07
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2291e-07 - val_loss: 1.2097e-07
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1896e-07 - val_loss: 1.1698e-07
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1517e-07 - val_loss: 1.1342e-07
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1161e-07 - val_loss: 1.1015e-07
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0828e-07 - val_loss: 1.0647e-07
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0466e-07 - val_loss: 1.0288e-07
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0126e-07 - val_loss: 9.9737e-08
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.8186e-08 - val_loss: 9.7037e-08
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.5360e-08 - val_loss: 9.3792e-08
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.2087e-08 - val_loss: 9.0611e-08
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.9046e-08 - val_loss: 8.7943e-08
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.6539e-08 - val_loss: 8.5473e-08
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.3914e-08 - val_loss: 8.2561e-08
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.1122e-08 - val_loss: 7.9882e-08
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.8514e-08 - val_loss: 7.7301e-08
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.5981e-08 - val_loss: 7.5133e-08
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.3876e-08 - val_loss: 7.2940e-08
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.1525e-08 - val_loss: 7.0379e-08
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.9086e-08 - val_loss: 6.8162e-08
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.6989e-08 - val_loss: 6.6080e-08
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.4876e-08 - val_loss: 6.4047e-08
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.2967e-08 - val_loss: 6.2169e-08
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.0953e-08 - val_loss: 6.0072e-08
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.8967e-08 - val_loss: 5.8180e-08
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.7125e-08 - val_loss: 5.6560e-08
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.5580e-08 - val_loss: 5.4731e-08
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.3602e-08 - val_loss: 5.2790e-08
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.1941e-08 - val_loss: 5.1551e-08
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0537e-08 - val_loss: 4.9898e-08
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8858e-08 - val_loss: 4.8200e-08
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.7246e-08 - val_loss: 4.6712e-08
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5808e-08 - val_loss: 4.5320e-08
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4506e-08 - val_loss: 4.3997e-08
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3142e-08 - val_loss: 4.2548e-08
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.1744e-08 - val_loss: 4.1208e-08
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.0390e-08 - val_loss: 3.9932e-08
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9193e-08 - val_loss: 3.8857e-08
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8070e-08 - val_loss: 3.7636e-08
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6831e-08 - val_loss: 3.6376e-08
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5634e-08 - val_loss: 3.5256e-08
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4603e-08 - val_loss: 3.4299e-08
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.3577e-08 - val_loss: 3.3251e-08
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.2509e-08 - val_loss: 3.2185e-08
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1519e-08 - val_loss: 3.1218e-08
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0572e-08 - val_loss: 3.0231e-08
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9608e-08 - val_loss: 2.9345e-08
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.8746e-08 - val_loss: 2.8470e-08
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7895e-08 - val_loss: 2.7593e-08
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7029e-08 - val_loss: 2.6715e-08
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6173e-08 - val_loss: 2.6020e-08
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5485e-08 - val_loss: 2.5228e-08
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4683e-08 - val_loss: 2.4414e-08
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3872e-08 - val_loss: 2.3659e-08
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3132e-08 - val_loss: 2.2979e-08
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2491e-08 - val_loss: 2.2421e-08
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1957e-08 - val_loss: 2.1749e-08
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1247e-08 - val_loss: 2.0984e-08
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0491e-08 - val_loss: 2.0335e-08
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9921e-08 - val_loss: 1.9853e-08
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9437e-08 - val_loss: 1.9280e-08
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8829e-08 - val_loss: 1.8629e-08
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8207e-08 - val_loss: 1.8092e-08
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7708e-08 - val_loss: 1.7666e-08
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7301e-08 - val_loss: 1.7127e-08
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6691e-08 - val_loss: 1.6528e-08
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6176e-08 - val_loss: 1.6092e-08
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5748e-08 - val_loss: 1.5644e-08
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5287e-08 - val_loss: 1.5183e-08
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4864e-08 - val_loss: 1.4817e-08
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4483e-08 - val_loss: 1.4353e-08
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4024e-08 - val_loss: 1.3949e-08
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3638e-08 - val_loss: 1.3560e-08
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3240e-08 - val_loss: 1.3107e-08
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2809e-08 - val_loss: 1.2799e-08
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2549e-08 - val_loss: 1.2483e-08
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2174e-08 - val_loss: 1.2078e-08
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1788e-08 - val_loss: 1.1740e-08
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1488e-08 - val_loss: 1.1462e-08
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1176e-08 - val_loss: 1.1105e-08
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0851e-08 - val_loss: 1.0797e-08
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0592e-08 - val_loss: 1.0557e-08
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0307e-08 - val_loss: 1.0195e-08
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.9963e-09 - val_loss: 9.9806e-09
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.7226e-09 - val_loss: 9.6346e-09
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.4078e-09 - val_loss: 9.4018e-09
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.1929e-09 - val_loss: 9.1936e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.9837e-09 - val_loss: 8.9612e-09
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.7715e-09 - val_loss: 8.7411e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.5136e-09 - val_loss: 8.4068e-09
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.1872e-09 - val_loss: 8.1889e-09
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.0375e-09 - val_loss: 8.0266e-09
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.8565e-09 - val_loss: 7.8452e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.6531e-09 - val_loss: 7.6396e-09
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.4503e-09 - val_loss: 7.3958e-09
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.2290e-09 - val_loss: 7.2182e-09
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.0591e-09 - val_loss: 7.0432e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.8800e-09 - val_loss: 6.8856e-09
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.7278e-09 - val_loss: 6.6888e-09
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.5034e-09 - val_loss: 6.4817e-09
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.3623e-09 - val_loss: 6.3766e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.2472e-09 - val_loss: 6.2501e-09
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.1021e-09 - val_loss: 6.0699e-09
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.9104e-09 - val_loss: 5.8360e-09
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.7002e-09 - val_loss: 5.7019e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.6054e-09 - val_loss: 5.6494e-09
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.5379e-09 - val_loss: 5.5415e-09
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.4087e-09 - val_loss: 5.3458e-09
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.2037e-09 - val_loss: 5.1457e-09
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0375e-09 - val_loss: 5.0485e-09
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.9556e-09 - val_loss: 4.9866e-09
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8823e-09 - val_loss: 4.8891e-09
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.7510e-09 - val_loss: 4.6876e-09
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5827e-09 - val_loss: 4.5898e-09
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5021e-09 - val_loss: 4.5392e-09
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4521e-09 - val_loss: 4.4489e-09
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3585e-09 - val_loss: 4.3327e-09
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.2141e-09 - val_loss: 4.1777e-09
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.0734e-09 - val_loss: 4.0669e-09
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9987e-09 - val_loss: 4.0445e-09
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9756e-09 - val_loss: 3.9998e-09
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8856e-09 - val_loss: 3.8537e-09
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.7635e-09 - val_loss: 3.7259e-09
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6436e-09 - val_loss: 3.6470e-09
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5918e-09 - val_loss: 3.6086e-09
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5342e-09 - val_loss: 3.5373e-09
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4609e-09 - val_loss: 3.4485e-09
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.3599e-09 - val_loss: 3.3428e-09
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.2746e-09 - val_loss: 3.2928e-09
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.2298e-09 - val_loss: 3.2404e-09
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1663e-09 - val_loss: 3.1538e-09
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0935e-09 - val_loss: 3.1071e-09
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0370e-09 - val_loss: 3.0266e-09
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9725e-09 - val_loss: 2.9711e-09
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9041e-09 - val_loss: 2.9022e-09
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.8387e-09 - val_loss: 2.8266e-09
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7658e-09 - val_loss: 2.7581e-09
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7114e-09 - val_loss: 2.7430e-09
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6953e-09 - val_loss: 2.6927e-09
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6205e-09 - val_loss: 2.6100e-09
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5637e-09 - val_loss: 2.5798e-09
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5208e-09 - val_loss: 2.5194e-09
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4655e-09 - val_loss: 2.4564e-09
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4158e-09 - val_loss: 2.4313e-09
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3736e-09 - val_loss: 2.3566e-09
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2922e-09 - val_loss: 2.2841e-09
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2494e-09 - val_loss: 2.2822e-09
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2555e-09 - val_loss: 2.2715e-09
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2173e-09 - val_loss: 2.2083e-09
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1609e-09 - val_loss: 2.1564e-09
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1015e-09 - val_loss: 2.0811e-09
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0297e-09 - val_loss: 2.0159e-09
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9854e-09 - val_loss: 2.0144e-09
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9933e-09 - val_loss: 2.0153e-09
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9786e-09 - val_loss: 1.9704e-09
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9239e-09 - val_loss: 1.9157e-09
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8714e-09 - val_loss: 1.8647e-09
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8274e-09 - val_loss: 1.8365e-09
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8129e-09 - val_loss: 1.8344e-09
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7938e-09 - val_loss: 1.7925e-09
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7485e-09 - val_loss: 1.7390e-09
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7103e-09 - val_loss: 1.7195e-09
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6925e-09 - val_loss: 1.6948e-09
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6667e-09 - val_loss: 1.6718e-09
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6492e-09 - val_loss: 1.6465e-09
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6106e-09 - val_loss: 1.6021e-09
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5762e-09 - val_loss: 1.5719e-09
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5449e-09 - val_loss: 1.5405e-09
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5245e-09 - val_loss: 1.5426e-09
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5079e-09 - val_loss: 1.4964e-09
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4767e-09 - val_loss: 1.4819e-09
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4586e-09 - val_loss: 1.4613e-09
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4269e-09 - val_loss: 1.4199e-09
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3954e-09 - val_loss: 1.3903e-09
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3678e-09 - val_loss: 1.3822e-09
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3513e-09 - val_loss: 1.3500e-09
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3270e-09 - val_loss: 1.3236e-09
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3098e-09 - val_loss: 1.3189e-09
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3028e-09 - val_loss: 1.2997e-09
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2731e-09 - val_loss: 1.2665e-09
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2453e-09 - val_loss: 1.2476e-09
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2285e-09 - val_loss: 1.2401e-09
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2245e-09 - val_loss: 1.2245e-09
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2004e-09 - val_loss: 1.2037e-09
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1870e-09 - val_loss: 1.1809e-09
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1580e-09 - val_loss: 1.1467e-09
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1205e-09 - val_loss: 1.1180e-09
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1069e-09 - val_loss: 1.1252e-09
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1095e-09 - val_loss: 1.1242e-09
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1050e-09 - val_loss: 1.1017e-09
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0771e-09 - val_loss: 1.0646e-09
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0458e-09 - val_loss: 1.0490e-09
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0326e-09 - val_loss: 1.0395e-09
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0250e-09 - val_loss: 1.0293e-09
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0139e-09 - val_loss: 1.0147e-09
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0050e-09 - val_loss: 1.0101e-09
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.9708e-10 - val_loss: 9.8397e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.6794e-10 - val_loss: 9.6231e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.5006e-10 - val_loss: 9.5066e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.4069e-10 - val_loss: 9.4473e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.3198e-10 - val_loss: 9.3035e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.0748e-10 - val_loss: 8.9325e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.7875e-10 - val_loss: 8.8416e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.7495e-10 - val_loss: 8.8186e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7584e-10 - val_loss: 8.8200e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.6928e-10 - val_loss: 8.6557e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.5382e-10 - val_loss: 8.5128e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.3719e-10 - val_loss: 8.3898e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3898e-10 - val_loss: 8.4228e-10
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.2936e-10 - val_loss: 8.1323e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.9259e-10 - val_loss: 7.8810e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8424e-10 - val_loss: 7.9388e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.8123e-10 - val_loss: 7.8530e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.7219e-10 - val_loss: 7.7339e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.6278e-10 - val_loss: 7.5838e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.5227e-10 - val_loss: 7.5487e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4821e-10 - val_loss: 7.5865e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5099e-10 - val_loss: 7.5703e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.3990e-10 - val_loss: 7.1998e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.9533e-10 - val_loss: 6.8210e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.7691e-10 - val_loss: 6.7871e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7354e-10 - val_loss: 6.8661e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8672e-10 - val_loss: 6.9971e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9188e-10 - val_loss: 6.9353e-10
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.8053e-10 - val_loss: 6.7198e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.5573e-10 - val_loss: 6.4617e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4472e-10 - val_loss: 6.5115e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4269e-10 - val_loss: 6.5336e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4643e-10 - val_loss: 6.5506e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.4044e-10 - val_loss: 6.3116e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.1891e-10 - val_loss: 6.1158e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.9709e-10 - val_loss: 5.9201e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.8516e-10 - val_loss: 5.8852e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8948e-10 - val_loss: 5.9591e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8867e-10 - val_loss: 5.9213e-10
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.8459e-10 - val_loss: 5.7905e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.7280e-10 - val_loss: 5.7884e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7737e-10 - val_loss: 5.9053e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.8076e-10 - val_loss: 5.7580e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.6655e-10 - val_loss: 5.5572e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.3910e-10 - val_loss: 5.3359e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3126e-10 - val_loss: 5.3702e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3268e-10 - val_loss: 5.3510e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3363e-10 - val_loss: 5.3996e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.3118e-10 - val_loss: 5.3032e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.2530e-10 - val_loss: 5.2999e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.1856e-10 - val_loss: 5.1166e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0844e-10 - val_loss: 5.0936e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0123e-10 - val_loss: 5.0726e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0461e-10 - val_loss: 5.0426e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.9631e-10 - val_loss: 4.9499e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8976e-10 - val_loss: 4.9382e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8905e-10 - val_loss: 4.8662e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.7632e-10 - val_loss: 4.6968e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6823e-10 - val_loss: 4.7365e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.6412e-10 - val_loss: 4.6285e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5810e-10 - val_loss: 4.5740e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5484e-10 - val_loss: 4.5908e-10
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4908e-10 - val_loss: 4.4557e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4047e-10 - val_loss: 4.3950e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3566e-10 - val_loss: 4.3605e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3197e-10 - val_loss: 4.3809e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3388e-10 - val_loss: 4.3975e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3389e-10 - val_loss: 4.3018e-10
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.2060e-10 - val_loss: 4.1376e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1467e-10 - val_loss: 4.1997e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1458e-10 - val_loss: 4.2026e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1714e-10 - val_loss: 4.1758e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.0638e-10 - val_loss: 3.9471e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8727e-10 - val_loss: 3.9056e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9029e-10 - val_loss: 3.9761e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9584e-10 - val_loss: 3.9846e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9654e-10 - val_loss: 4.0084e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9475e-10 - val_loss: 3.8901e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8436e-10 - val_loss: 3.8557e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.8094e-10 - val_loss: 3.7860e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.7246e-10 - val_loss: 3.6819e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6549e-10 - val_loss: 3.7469e-10
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.7089e-10 - val_loss: 3.6722e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6413e-10 - val_loss: 3.6755e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6061e-10 - val_loss: 3.5983e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5729e-10 - val_loss: 3.5782e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.5329e-10 - val_loss: 3.4973e-10
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4368e-10 - val_loss: 3.4098e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3766e-10 - val_loss: 3.4184e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4388e-10 - val_loss: 3.4737e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4216e-10 - val_loss: 3.4265e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.3775e-10 - val_loss: 3.3878e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.3047e-10 - val_loss: 3.2939e-10
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.2402e-10 - val_loss: 3.2125e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2254e-10 - val_loss: 3.2555e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2120e-10 - val_loss: 3.2430e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2364e-10 - val_loss: 3.2672e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2266e-10 - val_loss: 3.2462e-10
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1612e-10 - val_loss: 3.1182e-10
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0836e-10 - val_loss: 3.0815e-10
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0539e-10 - val_loss: 3.0730e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0536e-10 - val_loss: 3.0757e-10
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0337e-10 - val_loss: 2.9995e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0322e-10 - val_loss: 3.1116e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0413e-10 - val_loss: 3.0279e-10
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9668e-10 - val_loss: 2.9522e-10
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9251e-10 - val_loss: 2.9242e-10
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.9016e-10 - val_loss: 2.9121e-10
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.8230e-10 - val_loss: 2.7937e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7848e-10 - val_loss: 2.8056e-10
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7830e-10 - val_loss: 2.7945e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7671e-10 - val_loss: 2.8019e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7807e-10 - val_loss: 2.8142e-10
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8215e-10 - val_loss: 2.8483e-10
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8386e-10 - val_loss: 2.8315e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7649e-10 - val_loss: 2.7130e-10
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6811e-10 - val_loss: 2.6095e-10
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5411e-10 - val_loss: 2.5230e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5233e-10 - val_loss: 2.5978e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5745e-10 - val_loss: 2.6137e-10
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6040e-10 - val_loss: 2.6261e-10
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5803e-10 - val_loss: 2.5293e-10
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5013e-10 - val_loss: 2.5518e-10
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5262e-10 - val_loss: 2.5444e-10
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5121e-10 - val_loss: 2.4792e-10
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4484e-10 - val_loss: 2.4297e-10
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3820e-10 - val_loss: 2.3634e-10
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3609e-10 - val_loss: 2.3990e-10
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3901e-10 - val_loss: 2.4165e-10
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3857e-10 - val_loss: 2.3767e-10
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3588e-10 - val_loss: 2.3628e-10
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3311e-10 - val_loss: 2.3122e-10
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3027e-10 - val_loss: 2.3283e-10
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3034e-10 - val_loss: 2.3319e-10
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3042e-10 - val_loss: 2.2931e-10
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2615e-10 - val_loss: 2.2473e-10
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2479e-10 - val_loss: 2.2836e-10
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2914e-10 - val_loss: 2.3004e-10
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.2553e-10 - val_loss: 2.2100e-10
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1889e-10 - val_loss: 2.1997e-10
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.1539e-10 - val_loss: 2.0942e-10
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0937e-10 - val_loss: 2.0948e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0671e-10 - val_loss: 2.0665e-10
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0371e-10 - val_loss: 2.0983e-10
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0918e-10 - val_loss: 2.1150e-10
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1119e-10 - val_loss: 2.1302e-10
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1113e-10 - val_loss: 2.0900e-10
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0529e-10 - val_loss: 1.9987e-10
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.9565e-10 - val_loss: 1.8899e-10
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8776e-10 - val_loss: 1.8721e-10
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8741e-10 - val_loss: 1.9313e-10
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9387e-10 - val_loss: 2.0083e-10
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0061e-10 - val_loss: 2.0608e-10
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0352e-10 - val_loss: 2.0350e-10
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0165e-10 - val_loss: 1.9935e-10
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9578e-10 - val_loss: 1.9597e-10
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9242e-10 - val_loss: 1.8943e-10
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8673e-10 - val_loss: 1.8433e-10
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8292e-10 - val_loss: 1.8649e-10
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8511e-10 - val_loss: 1.8662e-10
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8529e-10 - val_loss: 1.8295e-10
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8334e-10 - val_loss: 1.8507e-10
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8351e-10 - val_loss: 1.8363e-10
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8094e-10 - val_loss: 1.7924e-10
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7592e-10 - val_loss: 1.7401e-10
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7664e-10 - val_loss: 1.8140e-10
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8214e-10 - val_loss: 1.8755e-10
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8451e-10 - val_loss: 1.7948e-10
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7596e-10 - val_loss: 1.7274e-10
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7311e-10 - val_loss: 1.7889e-10
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7873e-10 - val_loss: 1.7658e-10
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7320e-10 - val_loss: 1.7152e-10
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7036e-10 - val_loss: 1.6894e-10
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6837e-10 - val_loss: 1.7093e-10
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6914e-10 - val_loss: 1.7073e-10
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6842e-10 - val_loss: 1.6706e-10
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6637e-10 - val_loss: 1.6805e-10
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6601e-10 - val_loss: 1.6956e-10
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6933e-10 - val_loss: 1.6785e-10
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6489e-10 - val_loss: 1.6143e-10
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5934e-10 - val_loss: 1.5696e-10
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5710e-10 - val_loss: 1.5552e-10
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5349e-10 - val_loss: 1.5248e-10
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5174e-10 - val_loss: 1.5404e-10
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5575e-10 - val_loss: 1.6366e-10
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6596e-10 - val_loss: 1.6766e-10
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6143e-10 - val_loss: 1.5462e-10
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5288e-10 - val_loss: 1.4919e-10
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4677e-10 - val_loss: 1.5013e-10
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5066e-10 - val_loss: 1.5172e-10
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4891e-10 - val_loss: 1.4824e-10
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4397e-10 - val_loss: 1.4472e-10
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4427e-10 - val_loss: 1.4230e-10
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3763e-10 - val_loss: 1.3308e-10
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3539e-10 - val_loss: 1.4078e-10
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4449e-10 - val_loss: 1.5206e-10
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5151e-10 - val_loss: 1.5069e-10
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5065e-10 - val_loss: 1.5368e-10
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5272e-10 - val_loss: 1.5629e-10
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5667e-10 - val_loss: 1.5741e-10
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5104e-10 - val_loss: 1.4322e-10
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3837e-10 - val_loss: 1.3467e-10
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3273e-10 - val_loss: 1.3389e-10
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3357e-10 - val_loss: 1.3297e-10
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3271e-10 - val_loss: 1.3942e-10
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3942e-10 - val_loss: 1.4202e-10
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4032e-10 - val_loss: 1.4202e-10
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4283e-10 - val_loss: 1.4322e-10
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4196e-10 - val_loss: 1.3915e-10
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3905e-10 - val_loss: 1.3891e-10
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3869e-10 - val_loss: 1.4031e-10
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3881e-10 - val_loss: 1.4010e-10
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3723e-10 - val_loss: 1.3303e-10
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3209e-10 - val_loss: 1.3257e-10
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3174e-10 - val_loss: 1.3269e-10
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3140e-10 - val_loss: 1.3073e-10
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3036e-10 - val_loss: 1.2943e-10
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2894e-10 - val_loss: 1.2944e-10
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2898e-10 - val_loss: 1.2761e-10
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2542e-10 - val_loss: 1.2734e-10
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2707e-10 - val_loss: 1.2913e-10
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2942e-10 - val_loss: 1.3083e-10
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3126e-10 - val_loss: 1.3355e-10
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3029e-10 - val_loss: 1.2468e-10
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2234e-10 - val_loss: 1.1888e-10
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1845e-10 - val_loss: 1.2368e-10
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2271e-10 - val_loss: 1.2475e-10
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2435e-10 - val_loss: 1.2481e-10
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2246e-10 - val_loss: 1.2099e-10
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2090e-10 - val_loss: 1.2308e-10
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2178e-10 - val_loss: 1.1965e-10
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1637e-10 - val_loss: 1.1404e-10
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1330e-10 - val_loss: 1.1026e-10
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0979e-10 - val_loss: 1.1018e-10
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1098e-10 - val_loss: 1.1391e-10
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1679e-10 - val_loss: 1.1855e-10
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1749e-10 - val_loss: 1.1848e-10
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1658e-10 - val_loss: 1.1522e-10
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1187e-10 - val_loss: 1.0749e-10
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0473e-10 - val_loss: 1.0415e-10
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0537e-10 - val_loss: 1.1099e-10
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1352e-10 - val_loss: 1.1554e-10
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1445e-10 - val_loss: 1.1434e-10
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 43.8307 - val_loss: 0.1983
Epoch 2/512
512/512 - 0s - loss: 0.0936 - val_loss: 0.1686
Epoch 3/512
512/512 - 0s - loss: 0.0849 - val_loss: 0.1592
Epoch 4/512
512/512 - 0s - loss: 0.0780 - val_loss: 0.1578
Epoch 5/512
512/512 - 0s - loss: 0.0739 - val_loss: 0.1303
Epoch 6/512
512/512 - 0s - loss: 0.0698 - val_loss: 0.1227
Epoch 7/512
512/512 - 0s - loss: 0.0633 - val_loss: 0.1484
Epoch 8/512
512/512 - 0s - loss: 0.0624 - val_loss: 0.0963
Epoch 9/512
512/512 - 0s - loss: 0.0580 - val_loss: 0.0889
Epoch 10/512
512/512 - 0s - loss: 0.0547 - val_loss: 0.0813
Epoch 11/512
512/512 - 0s - loss: 0.0514 - val_loss: 0.0728
Epoch 12/512
512/512 - 0s - loss: 0.0480 - val_loss: 0.0636
Epoch 13/512
512/512 - 0s - loss: 0.0442 - val_loss: 0.0543
Epoch 14/512
512/512 - 0s - loss: 0.0401 - val_loss: 0.0461
Epoch 15/512
512/512 - 0s - loss: 0.0359 - val_loss: 0.0393
Epoch 16/512
512/512 - 0s - loss: 0.0320 - val_loss: 0.0341
Epoch 17/512
512/512 - 0s - loss: 0.0279 - val_loss: 0.0299
Epoch 18/512
512/512 - 0s - loss: 0.0238 - val_loss: 0.0275
Epoch 19/512
512/512 - 0s - loss: 0.0196 - val_loss: 0.0268
Epoch 20/512
512/512 - 0s - loss: 0.0157 - val_loss: 0.0280
Epoch 21/512
512/512 - 0s - loss: 0.0123 - val_loss: 0.0288
Epoch 22/512
512/512 - 0s - loss: 0.0100 - val_loss: 0.0290
Epoch 23/512
512/512 - 0s - loss: 0.0095 - val_loss: 0.0291
Epoch 24/512
512/512 - 0s - loss: 0.0090 - val_loss: 0.0287
Epoch 25/512
512/512 - 0s - loss: 0.0088 - val_loss: 0.0237
Epoch 26/512
512/512 - 0s - loss: 0.0079 - val_loss: 0.0224
Epoch 27/512
512/512 - 0s - loss: 0.0079 - val_loss: 0.0207
Epoch 28/512
512/512 - 0s - loss: 0.0074 - val_loss: 0.0191
Epoch 29/512
512/512 - 0s - loss: 0.0071 - val_loss: 0.0177
Epoch 30/512
512/512 - 0s - loss: 0.0071 - val_loss: 0.0163
Epoch 31/512
512/512 - 0s - loss: 0.0066 - val_loss: 0.0151
Epoch 32/512
512/512 - 0s - loss: 0.0064 - val_loss: 0.0138
Epoch 33/512
512/512 - 0s - loss: 0.0064 - val_loss: 0.0124
Epoch 34/512
512/512 - 0s - loss: 0.0058 - val_loss: 0.0113
Epoch 35/512
512/512 - 0s - loss: 0.0054 - val_loss: 0.0101
Epoch 36/512
512/512 - 0s - loss: 0.0051 - val_loss: 0.0088
Epoch 37/512
512/512 - 0s - loss: 0.0048 - val_loss: 0.0077
Epoch 38/512
512/512 - 0s - loss: 0.0044 - val_loss: 0.0066
Epoch 39/512
512/512 - 0s - loss: 0.0040 - val_loss: 0.0058
Epoch 40/512
512/512 - 0s - loss: 0.0037 - val_loss: 0.0052
Epoch 41/512
512/512 - 0s - loss: 0.0034 - val_loss: 0.0048
Epoch 42/512
512/512 - 0s - loss: 0.0032 - val_loss: 0.0044
Epoch 43/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0041
Epoch 44/512
512/512 - 0s - loss: 0.0028 - val_loss: 0.0038
Epoch 45/512
512/512 - 0s - loss: 0.0027 - val_loss: 0.0035
Epoch 46/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0032
Epoch 47/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0029
Epoch 48/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0025
Epoch 49/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0032
Epoch 50/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0038
Epoch 51/512
512/512 - 0s - loss: 0.0034 - val_loss: 6.9894e-04
Epoch 52/512
512/512 - 0s - loss: 0.0014 - val_loss: 6.2115e-04
Epoch 53/512
512/512 - 0s - loss: 0.0033 - val_loss: 6.3315e-04
Epoch 54/512
512/512 - 0s - loss: 0.0020 - val_loss: 5.1281e-04
Epoch 55/512
512/512 - 0s - loss: 0.0015 - val_loss: 4.2726e-04
Epoch 56/512
512/512 - 0s - loss: 0.0023 - val_loss: 4.1270e-04
Epoch 57/512
512/512 - 0s - loss: 0.0021 - val_loss: 3.3063e-04
Epoch 58/512
512/512 - 0s - loss: 0.0016 - val_loss: 2.7953e-04
Epoch 59/512
512/512 - 0s - loss: 0.0018 - val_loss: 2.7356e-04
Epoch 60/512
512/512 - 0s - loss: 0.0020 - val_loss: 2.2047e-04
Epoch 61/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.9237e-04
Epoch 62/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.7984e-04
Epoch 63/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.5108e-04
Epoch 64/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.3510e-04
Epoch 65/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.0093e-04
Epoch 66/512
512/512 - 0s - loss: 0.0015 - val_loss: 8.8964e-05
Epoch 67/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.7126e-04
Epoch 68/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.3414e-04
Epoch 69/512
512/512 - 0s - loss: 0.0015 - val_loss: 6.9538e-05
Epoch 70/512
512/512 - 0s - loss: 0.0012 - val_loss: 7.4988e-05
Epoch 71/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1720e-04
Epoch 72/512
512/512 - 0s - loss: 0.0014 - val_loss: 4.7926e-05
Epoch 73/512
512/512 - 0s - loss: 0.0011 - val_loss: 4.5155e-05
Epoch 74/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.6552e-04
Epoch 75/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.6644e-04
Epoch 76/512
512/512 - 0s - loss: 0.0011 - val_loss: 3.1349e-05
Epoch 77/512
512/512 - 0s - loss: 0.0011 - val_loss: 3.3563e-05
Epoch 78/512
512/512 - 0s - loss: 9.5847e-04 - val_loss: 1.6019e-04
Epoch 79/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.2628e-05
Epoch 80/512
512/512 - 0s - loss: 0.0011 - val_loss: 4.1291e-05
Epoch 81/512
512/512 - 0s - loss: 8.4857e-04 - val_loss: 2.0181e-05
Epoch 82/512
512/512 - 0s - loss: 9.9211e-04 - val_loss: 3.8897e-04
Epoch 83/512
512/512 - 0s - loss: 9.4418e-04 - val_loss: 7.3073e-05
Epoch 84/512
512/512 - 0s - loss: 9.1717e-04 - val_loss: 1.9289e-05
Epoch 85/512
512/512 - 0s - loss: 9.3013e-04 - val_loss: 9.6940e-05
Epoch 86/512
512/512 - 0s - loss: 7.7577e-04 - val_loss: 1.5098e-04
Epoch 87/512
512/512 - 0s - loss: 8.5803e-04 - val_loss: 4.3041e-04
Epoch 88/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.7540e-05
Epoch 89/512
512/512 - 0s - loss: 6.0277e-04 - val_loss: 1.2615e-05
Epoch 90/512
512/512 - 0s - loss: 7.8305e-04 - val_loss: 2.0512e-04
Epoch 91/512
512/512 - 0s - loss: 0.0010 - val_loss: 2.3881e-04
Epoch 92/512
512/512 - 0s - loss: 6.8958e-04 - val_loss: 4.1146e-05
Epoch 93/512
512/512 - 0s - loss: 7.6819e-04 - val_loss: 2.0313e-04
Epoch 94/512
512/512 - 0s - loss: 9.9145e-04 - val_loss: 2.4593e-05
Epoch 95/512
512/512 - 0s - loss: 5.7711e-04 - val_loss: 3.3162e-05
Epoch 96/512
512/512 - 0s - loss: 7.3557e-04 - val_loss: 1.8081e-04
Epoch 97/512
512/512 - 0s - loss: 9.2614e-04 - val_loss: 2.8915e-05
Epoch 98/512
512/512 - 0s - loss: 5.8597e-04 - val_loss: 6.3040e-05
Epoch 99/512
512/512 - 0s - loss: 7.9096e-04 - val_loss: 1.7040e-04
Epoch 100/512
512/512 - 0s - loss: 8.2055e-04 - val_loss: 2.1591e-05
Epoch 101/512
512/512 - 0s - loss: 5.6236e-04 - val_loss: 2.3530e-05
Epoch 102/512
512/512 - 0s - loss: 7.8999e-04 - val_loss: 2.1961e-04
Epoch 103/512
512/512 - 0s - loss: 7.2802e-04 - val_loss: 4.8480e-05
Epoch 104/512
512/512 - 0s - loss: 5.7672e-04 - val_loss: 1.1814e-05
Epoch 105/512
512/512 - 0s - loss: 8.1686e-04 - val_loss: 8.5634e-05
Epoch 106/512
512/512 - 0s - loss: 6.1805e-04 - val_loss: 3.7672e-05
Epoch 107/512
512/512 - 0s - loss: 5.5733e-04 - val_loss: 1.9097e-05
Epoch 108/512
512/512 - 0s - loss: 8.4122e-04 - val_loss: 3.1421e-05
Epoch 109/512
512/512 - 0s - loss: 5.7190e-04 - val_loss: 2.8083e-05
Epoch 110/512
512/512 - 0s - loss: 5.7437e-04 - val_loss: 7.3717e-05
Epoch 111/512
512/512 - 0s - loss: 7.5177e-04 - val_loss: 1.0171e-05
Epoch 112/512
512/512 - 0s - loss: 5.5364e-04 - val_loss: 1.4178e-05
Epoch 113/512
512/512 - 0s - loss: 6.1114e-04 - val_loss: 1.2538e-04
Epoch 114/512
512/512 - 0s - loss: 6.5709e-04 - val_loss: 1.9770e-05
Epoch 115/512
512/512 - 0s - loss: 5.6273e-04 - val_loss: 2.1679e-05
Epoch 116/512
512/512 - 0s - loss: 6.4767e-04 - val_loss: 6.1783e-05
Epoch 117/512
512/512 - 0s - loss: 5.5274e-04 - val_loss: 1.9253e-05
Epoch 118/512
512/512 - 0s - loss: 5.7045e-04 - val_loss: 8.1865e-05
Epoch 119/512
512/512 - 0s - loss: 6.1795e-04 - val_loss: 1.2986e-05
Epoch 120/512
512/512 - 0s - loss: 5.1510e-04 - val_loss: 1.4124e-05
Epoch 121/512
512/512 - 0s - loss: 6.2179e-04 - val_loss: 1.1265e-04
Epoch 122/512
512/512 - 0s - loss: 5.5101e-04 - val_loss: 1.6138e-05
Epoch 123/512
512/512 - 0s - loss: 4.9933e-04 - val_loss: 5.9190e-05
Epoch 124/512
512/512 - 0s - loss: 6.1177e-04 - val_loss: 1.7715e-05
Epoch 125/512
512/512 - 0s - loss: 4.7584e-04 - val_loss: 1.4513e-05
Epoch 126/512
512/512 - 0s - loss: 5.6171e-04 - val_loss: 1.2465e-04
Epoch 127/512
512/512 - 0s - loss: 5.2771e-04 - val_loss: 1.2932e-05
Epoch 128/512
512/512 - 0s - loss: 4.8564e-04 - val_loss: 5.4220e-05
Epoch 129/512
512/512 - 0s - loss: 5.5258e-04 - val_loss: 2.5878e-05
Epoch 130/512
512/512 - 0s - loss: 4.7903e-04 - val_loss: 1.5443e-05
Epoch 131/512
512/512 - 0s - loss: 5.1730e-04 - val_loss: 1.1156e-04
Epoch 132/512
512/512 - 0s - loss: 4.9884e-04 - val_loss: 1.3799e-05
Epoch 133/512
512/512 - 0s - loss: 4.7037e-04 - val_loss: 4.7918e-05
Epoch 134/512
512/512 - 0s - loss: 5.1134e-04 - val_loss: 4.0126e-05
Epoch 135/512
512/512 - 0s - loss: 4.4915e-04 - val_loss: 1.5008e-05
Epoch 136/512
512/512 - 0s - loss: 4.6485e-04 - val_loss: 1.0989e-04
Epoch 137/512
512/512 - 0s - loss: 4.8481e-04 - val_loss: 1.7280e-05
Epoch 138/512
512/512 - 0s - loss: 4.3097e-04 - val_loss: 2.9330e-05
Epoch 139/512
512/512 - 0s - loss: 4.9476e-04 - val_loss: 7.3707e-05
Epoch 140/512
512/512 - 0s - loss: 4.2852e-04 - val_loss: 1.5817e-05
Epoch 141/512
512/512 - 0s - loss: 4.2510e-04 - val_loss: 7.5833e-05
Epoch 142/512
512/512 - 0s - loss: 4.6400e-04 - val_loss: 2.8953e-05
Epoch 143/512
512/512 - 0s - loss: 4.1336e-04 - val_loss: 2.4658e-05
Epoch 144/512
512/512 - 0s - loss: 4.3575e-04 - val_loss: 1.0340e-04
Epoch 145/512
512/512 - 0s - loss: 4.0451e-04 - val_loss: 2.0003e-05
Epoch 146/512
512/512 - 0s - loss: 3.8847e-04 - val_loss: 5.1332e-05
Epoch 147/512
512/512 - 0s - loss: 4.3613e-04 - val_loss: 6.7555e-05
Epoch 148/512
512/512 - 0s - loss: 3.7293e-04 - val_loss: 1.8570e-05
Epoch 149/512
512/512 - 0s - loss: 3.8380e-04 - val_loss: 9.5327e-05
Epoch 150/512
512/512 - 0s - loss: 4.1090e-04 - val_loss: 2.9298e-05
Epoch 151/512
512/512 - 0s - loss: 3.4424e-04 - val_loss: 3.7747e-05
Epoch 152/512
512/512 - 0s - loss: 3.7270e-04 - val_loss: 9.9087e-05
Epoch 153/512
512/512 - 0s - loss: 3.6917e-04 - val_loss: 2.4320e-05
Epoch 154/512
512/512 - 0s - loss: 3.3559e-04 - val_loss: 5.7845e-05
Epoch 155/512
512/512 - 0s - loss: 3.5912e-04 - val_loss: 7.3442e-05
Epoch 156/512
512/512 - 0s - loss: 3.2032e-04 - val_loss: 3.1908e-05
Epoch 157/512
512/512 - 0s - loss: 3.1489e-04 - val_loss: 7.9517e-05
Epoch 158/512
512/512 - 0s - loss: 3.3377e-04 - val_loss: 5.4558e-05
Epoch 159/512
512/512 - 0s - loss: 2.9036e-04 - val_loss: 3.7235e-05
Epoch 160/512
512/512 - 0s - loss: 2.8797e-04 - val_loss: 9.4711e-05
Epoch 161/512
512/512 - 0s - loss: 2.9926e-04 - val_loss: 5.5203e-05
Epoch 162/512
512/512 - 0s - loss: 2.5245e-04 - val_loss: 4.7414e-05
Epoch 163/512
512/512 - 0s - loss: 2.5671e-04 - val_loss: 8.6801e-05
Epoch 164/512
512/512 - 0s - loss: 2.6022e-04 - val_loss: 6.3150e-05
Epoch 165/512
512/512 - 0s - loss: 2.1690e-04 - val_loss: 4.9504e-05
Epoch 166/512
512/512 - 0s - loss: 2.1906e-04 - val_loss: 7.7317e-05
Epoch 167/512
512/512 - 0s - loss: 2.1750e-04 - val_loss: 6.9415e-05
Epoch 168/512
512/512 - 0s - loss: 1.7339e-04 - val_loss: 4.9438e-05
Epoch 169/512
512/512 - 0s - loss: 1.6916e-04 - val_loss: 5.9499e-05
Epoch 170/512
512/512 - 0s - loss: 1.7226e-04 - val_loss: 6.2110e-05
Epoch 171/512
512/512 - 0s - loss: 1.3802e-04 - val_loss: 4.8536e-05
Epoch 172/512
512/512 - 0s - loss: 1.1984e-04 - val_loss: 4.8987e-05
Epoch 173/512
512/512 - 0s - loss: 1.1310e-04 - val_loss: 4.3968e-05
Epoch 174/512
512/512 - 0s - loss: 1.0320e-04 - val_loss: 3.7522e-05
Epoch 175/512
512/512 - 0s - loss: 8.3586e-05 - val_loss: 3.6116e-05
Epoch 176/512
512/512 - 0s - loss: 6.8663e-05 - val_loss: 3.2568e-05
Epoch 177/512
512/512 - 0s - loss: 5.8867e-05 - val_loss: 2.8308e-05
Epoch 178/512
512/512 - 0s - loss: 5.2655e-05 - val_loss: 2.2586e-05
Epoch 179/512
512/512 - 0s - loss: 4.4915e-05 - val_loss: 2.0793e-05
Epoch 180/512
512/512 - 0s - loss: 3.5275e-05 - val_loss: 1.9052e-05
Epoch 181/512
512/512 - 0s - loss: 2.9526e-05 - val_loss: 1.6215e-05
Epoch 182/512
512/512 - 0s - loss: 2.5304e-05 - val_loss: 1.4283e-05
Epoch 183/512
512/512 - 0s - loss: 2.0434e-05 - val_loss: 1.3087e-05
Epoch 184/512
512/512 - 0s - loss: 1.6685e-05 - val_loss: 1.1549e-05
Epoch 185/512
512/512 - 0s - loss: 1.4577e-05 - val_loss: 1.0349e-05
Epoch 186/512
512/512 - 0s - loss: 1.2111e-05 - val_loss: 9.4778e-06
Epoch 187/512
512/512 - 0s - loss: 1.0222e-05 - val_loss: 8.5423e-06
Epoch 188/512
512/512 - 0s - loss: 8.9407e-06 - val_loss: 7.7477e-06
Epoch 189/512
512/512 - 0s - loss: 7.6772e-06 - val_loss: 7.1121e-06
Epoch 190/512
512/512 - 0s - loss: 6.5580e-06 - val_loss: 6.4373e-06
Epoch 191/512
512/512 - 0s - loss: 5.8687e-06 - val_loss: 5.8232e-06
Epoch 192/512
512/512 - 0s - loss: 5.1819e-06 - val_loss: 5.3364e-06
Epoch 193/512
512/512 - 0s - loss: 4.4089e-06 - val_loss: 4.8722e-06
Epoch 194/512
512/512 - 0s - loss: 3.9057e-06 - val_loss: 4.4113e-06
Epoch 195/512
512/512 - 0s - loss: 3.6245e-06 - val_loss: 4.0117e-06
Epoch 196/512
512/512 - 0s - loss: 3.1870e-06 - val_loss: 3.6858e-06
Epoch 197/512
512/512 - 0s - loss: 2.7403e-06 - val_loss: 3.3677e-06
Epoch 198/512
512/512 - 0s - loss: 2.5282e-06 - val_loss: 3.0656e-06
Epoch 199/512
512/512 - 0s - loss: 2.3041e-06 - val_loss: 2.8114e-06
Epoch 200/512
512/512 - 0s - loss: 2.0558e-06 - val_loss: 2.5873e-06
Epoch 201/512
512/512 - 0s - loss: 1.8200e-06 - val_loss: 2.3737e-06
Epoch 202/512
512/512 - 0s - loss: 1.7048e-06 - val_loss: 2.1803e-06
Epoch 203/512
512/512 - 0s - loss: 1.5438e-06 - val_loss: 2.0195e-06
Epoch 204/512
512/512 - 0s - loss: 1.3662e-06 - val_loss: 1.8689e-06
Epoch 205/512
512/512 - 0s - loss: 1.2701e-06 - val_loss: 1.7194e-06
Epoch 206/512
512/512 - 0s - loss: 1.1757e-06 - val_loss: 1.5930e-06
Epoch 207/512
512/512 - 0s - loss: 1.0464e-06 - val_loss: 1.4821e-06
Epoch 208/512
512/512 - 0s - loss: 9.6582e-07 - val_loss: 1.3749e-06
Epoch 209/512
512/512 - 0s - loss: 8.9941e-07 - val_loss: 1.2822e-06
Epoch 210/512
512/512 - 0s - loss: 8.1251e-07 - val_loss: 1.1947e-06
Epoch 211/512
512/512 - 0s - loss: 7.4402e-07 - val_loss: 1.1129e-06
Epoch 212/512
512/512 - 0s - loss: 6.9110e-07 - val_loss: 1.0393e-06
Epoch 213/512
512/512 - 0s - loss: 6.3350e-07 - val_loss: 9.7418e-07
Epoch 214/512
512/512 - 0s - loss: 5.7733e-07 - val_loss: 9.1145e-07
Epoch 215/512
512/512 - 0s - loss: 5.4116e-07 - val_loss: 8.5459e-07
Epoch 216/512
512/512 - 0s - loss: 4.9234e-07 - val_loss: 8.0196e-07
Epoch 217/512
512/512 - 0s - loss: 4.5095e-07 - val_loss: 7.5144e-07
Epoch 218/512
512/512 - 0s - loss: 4.2302e-07 - val_loss: 7.0461e-07
Epoch 219/512
512/512 - 0s - loss: 3.8853e-07 - val_loss: 6.6327e-07
Epoch 220/512
512/512 - 0s - loss: 3.5781e-07 - val_loss: 6.2388e-07
Epoch 221/512
512/512 - 0s - loss: 3.3079e-07 - val_loss: 5.8649e-07
Epoch 222/512
512/512 - 0s - loss: 3.0458e-07 - val_loss: 5.5279e-07
Epoch 223/512
512/512 - 0s - loss: 2.8345e-07 - val_loss: 5.2130e-07
Epoch 224/512
512/512 - 0s - loss: 2.6144e-07 - val_loss: 4.9226e-07
Epoch 225/512
512/512 - 0s - loss: 2.3969e-07 - val_loss: 4.6522e-07
Epoch 226/512
512/512 - 0s - loss: 2.2412e-07 - val_loss: 4.3900e-07
Epoch 227/512
512/512 - 0s - loss: 2.0847e-07 - val_loss: 4.1551e-07
Epoch 228/512
512/512 - 0s - loss: 1.8864e-07 - val_loss: 3.9232e-07
Epoch 229/512
512/512 - 0s - loss: 1.7990e-07 - val_loss: 3.7133e-07
Epoch 230/512
512/512 - 0s - loss: 1.6502e-07 - val_loss: 3.5215e-07
Epoch 231/512
512/512 - 0s - loss: 1.5065e-07 - val_loss: 3.3447e-07
Epoch 232/512
512/512 - 0s - loss: 1.4127e-07 - val_loss: 3.1688e-07
Epoch 233/512
512/512 - 0s - loss: 1.3278e-07 - val_loss: 3.0061e-07
Epoch 234/512
512/512 - 0s - loss: 1.2047e-07 - val_loss: 2.8597e-07
Epoch 235/512
512/512 - 0s - loss: 1.1371e-07 - val_loss: 2.7166e-07
Epoch 236/512
512/512 - 0s - loss: 1.0509e-07 - val_loss: 2.5860e-07
Epoch 237/512
512/512 - 0s - loss: 9.7427e-08 - val_loss: 2.4642e-07
Epoch 238/512
512/512 - 0s - loss: 9.1150e-08 - val_loss: 2.3427e-07
Epoch 239/512
512/512 - 0s - loss: 8.4099e-08 - val_loss: 2.2381e-07
Epoch 240/512
512/512 - 0s - loss: 7.7527e-08 - val_loss: 2.1318e-07
Epoch 241/512
512/512 - 0s - loss: 7.4090e-08 - val_loss: 2.0357e-07
Epoch 242/512
512/512 - 0s - loss: 6.8477e-08 - val_loss: 1.9483e-07
Epoch 243/512
512/512 - 0s - loss: 6.2918e-08 - val_loss: 1.8660e-07
Epoch 244/512
512/512 - 0s - loss: 5.8753e-08 - val_loss: 1.7842e-07
Epoch 245/512
512/512 - 0s - loss: 5.5343e-08 - val_loss: 1.7095e-07
Epoch 246/512
512/512 - 0s - loss: 5.1547e-08 - val_loss: 1.6414e-07
Epoch 247/512
512/512 - 0s - loss: 4.7923e-08 - val_loss: 1.5738e-07
Epoch 248/512
512/512 - 0s - loss: 4.5310e-08 - val_loss: 1.5104e-07
Epoch 249/512
512/512 - 0s - loss: 4.2646e-08 - val_loss: 1.4509e-07
Epoch 250/512
512/512 - 0s - loss: 3.9043e-08 - val_loss: 1.3966e-07
Epoch 251/512
512/512 - 0s - loss: 3.6490e-08 - val_loss: 1.3448e-07
Epoch 252/512
512/512 - 0s - loss: 3.4486e-08 - val_loss: 1.2946e-07
Epoch 253/512
512/512 - 0s - loss: 3.2569e-08 - val_loss: 1.2486e-07
Epoch 254/512
512/512 - 0s - loss: 3.0606e-08 - val_loss: 1.2056e-07
Epoch 255/512
512/512 - 0s - loss: 2.8396e-08 - val_loss: 1.1646e-07
Epoch 256/512
512/512 - 0s - loss: 2.7130e-08 - val_loss: 1.1254e-07
Epoch 257/512
512/512 - 0s - loss: 2.5207e-08 - val_loss: 1.0894e-07
Epoch 258/512
512/512 - 0s - loss: 2.3861e-08 - val_loss: 1.0541e-07
Epoch 259/512
512/512 - 0s - loss: 2.2519e-08 - val_loss: 1.0201e-07
Epoch 260/512
512/512 - 0s - loss: 2.1143e-08 - val_loss: 9.9010e-08
Epoch 261/512
512/512 - 0s - loss: 2.0245e-08 - val_loss: 9.6083e-08
Epoch 262/512
512/512 - 0s - loss: 1.9003e-08 - val_loss: 9.3224e-08
Epoch 263/512
512/512 - 0s - loss: 1.7823e-08 - val_loss: 9.0539e-08
Epoch 264/512
512/512 - 0s - loss: 1.6979e-08 - val_loss: 8.7926e-08
Epoch 265/512
512/512 - 0s - loss: 1.6067e-08 - val_loss: 8.5499e-08
Epoch 266/512
512/512 - 0s - loss: 1.5429e-08 - val_loss: 8.3176e-08
Epoch 267/512
512/512 - 0s - loss: 1.4542e-08 - val_loss: 8.1072e-08
Epoch 268/512
512/512 - 0s - loss: 1.3828e-08 - val_loss: 7.8982e-08
Epoch 269/512
512/512 - 0s - loss: 1.3205e-08 - val_loss: 7.6964e-08
Epoch 270/512
512/512 - 0s - loss: 1.2687e-08 - val_loss: 7.5050e-08
Epoch 271/512
512/512 - 0s - loss: 1.2002e-08 - val_loss: 7.3256e-08
Epoch 272/512
512/512 - 0s - loss: 1.1487e-08 - val_loss: 7.1546e-08
Epoch 273/512
512/512 - 0s - loss: 1.0920e-08 - val_loss: 6.9847e-08
Epoch 274/512
512/512 - 0s - loss: 1.0522e-08 - val_loss: 6.8235e-08
Epoch 275/512
512/512 - 0s - loss: 9.9791e-09 - val_loss: 6.6758e-08
Epoch 276/512
512/512 - 0s - loss: 9.6466e-09 - val_loss: 6.5291e-08
Epoch 277/512
512/512 - 0s - loss: 9.2560e-09 - val_loss: 6.3906e-08
Epoch 278/512
512/512 - 0s - loss: 8.9521e-09 - val_loss: 6.2535e-08
Epoch 279/512
512/512 - 0s - loss: 8.4896e-09 - val_loss: 6.1230e-08
Epoch 280/512
512/512 - 0s - loss: 8.1646e-09 - val_loss: 5.9984e-08
Epoch 281/512
512/512 - 0s - loss: 7.8719e-09 - val_loss: 5.8846e-08
Epoch 282/512
512/512 - 0s - loss: 7.7687e-09 - val_loss: 5.7681e-08
Epoch 283/512
512/512 - 0s - loss: 7.4842e-09 - val_loss: 5.6548e-08
Epoch 284/512
512/512 - 0s - loss: 7.1362e-09 - val_loss: 5.5458e-08
Epoch 285/512
512/512 - 0s - loss: 6.7760e-09 - val_loss: 5.4429e-08
Epoch 286/512
512/512 - 0s - loss: 6.5234e-09 - val_loss: 5.3432e-08
Epoch 287/512
512/512 - 0s - loss: 6.4232e-09 - val_loss: 5.2512e-08
Epoch 288/512
512/512 - 0s - loss: 6.3250e-09 - val_loss: 5.1575e-08
Epoch 289/512
512/512 - 0s - loss: 6.1663e-09 - val_loss: 5.0677e-08
Epoch 290/512
512/512 - 0s - loss: 5.9599e-09 - val_loss: 4.9791e-08
Epoch 291/512
512/512 - 0s - loss: 5.7135e-09 - val_loss: 4.8929e-08
Epoch 292/512
512/512 - 0s - loss: 5.4861e-09 - val_loss: 4.8133e-08
Epoch 293/512
512/512 - 0s - loss: 5.3608e-09 - val_loss: 4.7337e-08
Epoch 294/512
512/512 - 0s - loss: 5.2568e-09 - val_loss: 4.6592e-08
Epoch 295/512
512/512 - 0s - loss: 5.1787e-09 - val_loss: 4.5875e-08
Epoch 296/512
512/512 - 0s - loss: 5.0805e-09 - val_loss: 4.5155e-08
Epoch 297/512
512/512 - 0s - loss: 4.9619e-09 - val_loss: 4.4457e-08
Epoch 298/512
512/512 - 0s - loss: 4.8274e-09 - val_loss: 4.3781e-08
Epoch 299/512
512/512 - 0s - loss: 4.6722e-09 - val_loss: 4.3093e-08
Epoch 300/512
512/512 - 0s - loss: 4.5271e-09 - val_loss: 4.2454e-08
Epoch 301/512
512/512 - 0s - loss: 4.4482e-09 - val_loss: 4.1849e-08
Epoch 302/512
512/512 - 0s - loss: 4.3640e-09 - val_loss: 4.1246e-08
Epoch 303/512
512/512 - 0s - loss: 4.2890e-09 - val_loss: 4.0665e-08
Epoch 304/512
512/512 - 0s - loss: 4.2338e-09 - val_loss: 4.0112e-08
Epoch 305/512
512/512 - 0s - loss: 4.1439e-09 - val_loss: 3.9541e-08
Epoch 306/512
512/512 - 0s - loss: 4.0326e-09 - val_loss: 3.8990e-08
Epoch 307/512
512/512 - 0s - loss: 3.9352e-09 - val_loss: 3.8456e-08
Epoch 308/512
512/512 - 0s - loss: 3.8596e-09 - val_loss: 3.7950e-08
Epoch 309/512
512/512 - 0s - loss: 3.8095e-09 - val_loss: 3.7475e-08
Epoch 310/512
512/512 - 0s - loss: 3.7592e-09 - val_loss: 3.6984e-08
Epoch 311/512
512/512 - 0s - loss: 3.6899e-09 - val_loss: 3.6493e-08
Epoch 312/512
512/512 - 0s - loss: 3.6245e-09 - val_loss: 3.6029e-08
Epoch 313/512
512/512 - 0s - loss: 3.5578e-09 - val_loss: 3.5569e-08
Epoch 314/512
512/512 - 0s - loss: 3.5008e-09 - val_loss: 3.5131e-08
Epoch 315/512
512/512 - 0s - loss: 3.4515e-09 - val_loss: 3.4699e-08
Epoch 316/512
512/512 - 0s - loss: 3.3958e-09 - val_loss: 3.4278e-08
Epoch 317/512
512/512 - 0s - loss: 3.3433e-09 - val_loss: 3.3862e-08
Epoch 318/512
512/512 - 0s - loss: 3.2912e-09 - val_loss: 3.3450e-08
Epoch 319/512
512/512 - 0s - loss: 3.2295e-09 - val_loss: 3.3040e-08
Epoch 320/512
512/512 - 0s - loss: 3.1689e-09 - val_loss: 3.2670e-08
Epoch 321/512
512/512 - 0s - loss: 3.1320e-09 - val_loss: 3.2292e-08
Epoch 322/512
512/512 - 0s - loss: 3.0992e-09 - val_loss: 3.1944e-08
Epoch 323/512
512/512 - 0s - loss: 3.0662e-09 - val_loss: 3.1586e-08
Epoch 324/512
512/512 - 0s - loss: 3.0199e-09 - val_loss: 3.1209e-08
Epoch 325/512
512/512 - 0s - loss: 2.9768e-09 - val_loss: 3.0889e-08
Epoch 326/512
512/512 - 0s - loss: 2.9441e-09 - val_loss: 3.0547e-08
Epoch 327/512
512/512 - 0s - loss: 2.9041e-09 - val_loss: 3.0212e-08
Epoch 328/512
512/512 - 0s - loss: 2.8632e-09 - val_loss: 2.9886e-08
Epoch 329/512
512/512 - 0s - loss: 2.8234e-09 - val_loss: 2.9550e-08
Epoch 330/512
512/512 - 0s - loss: 2.7867e-09 - val_loss: 2.9254e-08
Epoch 331/512
512/512 - 0s - loss: 2.7603e-09 - val_loss: 2.8964e-08
Epoch 332/512
512/512 - 0s - loss: 2.7366e-09 - val_loss: 2.8671e-08
Epoch 333/512
512/512 - 0s - loss: 2.7073e-09 - val_loss: 2.8382e-08
Epoch 334/512
512/512 - 0s - loss: 2.6780e-09 - val_loss: 2.8094e-08
Epoch 335/512
512/512 - 0s - loss: 2.6464e-09 - val_loss: 2.7818e-08
Epoch 336/512
512/512 - 0s - loss: 2.6169e-09 - val_loss: 2.7542e-08
Epoch 337/512
512/512 - 0s - loss: 2.5812e-09 - val_loss: 2.7265e-08
Epoch 338/512
512/512 - 0s - loss: 2.5560e-09 - val_loss: 2.7005e-08
Epoch 339/512
512/512 - 0s - loss: 2.5324e-09 - val_loss: 2.6755e-08
Epoch 340/512
512/512 - 0s - loss: 2.5024e-09 - val_loss: 2.6484e-08
Epoch 341/512
512/512 - 0s - loss: 2.4696e-09 - val_loss: 2.6219e-08
Epoch 342/512
512/512 - 0s - loss: 2.4426e-09 - val_loss: 2.5977e-08
Epoch 343/512
512/512 - 0s - loss: 2.4159e-09 - val_loss: 2.5724e-08
Epoch 344/512
512/512 - 0s - loss: 2.3917e-09 - val_loss: 2.5495e-08
Epoch 345/512
512/512 - 0s - loss: 2.3702e-09 - val_loss: 2.5266e-08
Epoch 346/512
512/512 - 0s - loss: 2.3457e-09 - val_loss: 2.5040e-08
Epoch 347/512
512/512 - 0s - loss: 2.3234e-09 - val_loss: 2.4817e-08
Epoch 348/512
512/512 - 0s - loss: 2.3005e-09 - val_loss: 2.4583e-08
Epoch 349/512
512/512 - 0s - loss: 2.2769e-09 - val_loss: 2.4356e-08
Epoch 350/512
512/512 - 0s - loss: 2.2551e-09 - val_loss: 2.4151e-08
Epoch 351/512
512/512 - 0s - loss: 2.2351e-09 - val_loss: 2.3946e-08
Epoch 352/512
512/512 - 0s - loss: 2.2152e-09 - val_loss: 2.3734e-08
Epoch 353/512
512/512 - 0s - loss: 2.1957e-09 - val_loss: 2.3535e-08
Epoch 354/512
512/512 - 0s - loss: 2.1765e-09 - val_loss: 2.3340e-08
Epoch 355/512
512/512 - 0s - loss: 2.1580e-09 - val_loss: 2.3152e-08
Epoch 356/512
512/512 - 0s - loss: 2.1394e-09 - val_loss: 2.2952e-08
Epoch 357/512
512/512 - 0s - loss: 2.1213e-09 - val_loss: 2.2776e-08
Epoch 358/512
512/512 - 0s - loss: 2.1020e-09 - val_loss: 2.2573e-08
Epoch 359/512
512/512 - 0s - loss: 2.0828e-09 - val_loss: 2.2393e-08
Epoch 360/512
512/512 - 0s - loss: 2.0656e-09 - val_loss: 2.2216e-08
Epoch 361/512
512/512 - 0s - loss: 2.0484e-09 - val_loss: 2.2038e-08
Epoch 362/512
512/512 - 0s - loss: 2.0308e-09 - val_loss: 2.1867e-08
Epoch 363/512
512/512 - 0s - loss: 2.0147e-09 - val_loss: 2.1686e-08
Epoch 364/512
512/512 - 0s - loss: 1.9973e-09 - val_loss: 2.1518e-08
Epoch 365/512
512/512 - 0s - loss: 1.9811e-09 - val_loss: 2.1355e-08
Epoch 366/512
512/512 - 0s - loss: 1.9662e-09 - val_loss: 2.1192e-08
Epoch 367/512
512/512 - 0s - loss: 1.9508e-09 - val_loss: 2.1032e-08
Epoch 368/512
512/512 - 0s - loss: 1.9350e-09 - val_loss: 2.0870e-08
Epoch 369/512
512/512 - 0s - loss: 1.9193e-09 - val_loss: 2.0695e-08
Epoch 370/512
512/512 - 0s - loss: 1.9041e-09 - val_loss: 2.0551e-08
Epoch 371/512
512/512 - 0s - loss: 1.8895e-09 - val_loss: 2.0391e-08
Epoch 372/512
512/512 - 0s - loss: 1.8750e-09 - val_loss: 2.0245e-08
Epoch 373/512
512/512 - 0s - loss: 1.8610e-09 - val_loss: 2.0106e-08
Epoch 374/512
512/512 - 0s - loss: 1.8470e-09 - val_loss: 1.9956e-08
Epoch 375/512
512/512 - 0s - loss: 1.8331e-09 - val_loss: 1.9802e-08
Epoch 376/512
512/512 - 0s - loss: 1.8195e-09 - val_loss: 1.9664e-08
Epoch 377/512
512/512 - 0s - loss: 1.8062e-09 - val_loss: 1.9528e-08
Epoch 378/512
512/512 - 0s - loss: 1.7932e-09 - val_loss: 1.9378e-08
Epoch 379/512
512/512 - 0s - loss: 1.7803e-09 - val_loss: 1.9243e-08
Epoch 380/512
512/512 - 0s - loss: 1.7681e-09 - val_loss: 1.9096e-08
Epoch 381/512
512/512 - 0s - loss: 1.7562e-09 - val_loss: 1.8970e-08
Epoch 382/512
512/512 - 0s - loss: 1.7435e-09 - val_loss: 1.8848e-08
Epoch 383/512
512/512 - 0s - loss: 1.7310e-09 - val_loss: 1.8717e-08
Epoch 384/512
512/512 - 0s - loss: 1.7191e-09 - val_loss: 1.8589e-08
Epoch 385/512
512/512 - 0s - loss: 1.7071e-09 - val_loss: 1.8472e-08
Epoch 386/512
512/512 - 0s - loss: 1.6948e-09 - val_loss: 1.8368e-08
Epoch 387/512
512/512 - 0s - loss: 1.6834e-09 - val_loss: 1.8263e-08
Epoch 388/512
512/512 - 0s - loss: 1.6727e-09 - val_loss: 1.8151e-08
Epoch 389/512
512/512 - 0s - loss: 1.6620e-09 - val_loss: 1.8033e-08
Epoch 390/512
512/512 - 0s - loss: 1.6502e-09 - val_loss: 1.7911e-08
Epoch 391/512
512/512 - 0s - loss: 1.6387e-09 - val_loss: 1.7789e-08
Epoch 392/512
512/512 - 0s - loss: 1.6279e-09 - val_loss: 1.7674e-08
Epoch 393/512
512/512 - 0s - loss: 1.6171e-09 - val_loss: 1.7555e-08
Epoch 394/512
512/512 - 0s - loss: 1.6065e-09 - val_loss: 1.7436e-08
Epoch 395/512
512/512 - 0s - loss: 1.5963e-09 - val_loss: 1.7332e-08
Epoch 396/512
512/512 - 0s - loss: 1.5859e-09 - val_loss: 1.7226e-08
Epoch 397/512
512/512 - 0s - loss: 1.5757e-09 - val_loss: 1.7126e-08
Epoch 398/512
512/512 - 0s - loss: 1.5657e-09 - val_loss: 1.7019e-08
Epoch 399/512
512/512 - 0s - loss: 1.5559e-09 - val_loss: 1.6919e-08
Epoch 400/512
512/512 - 0s - loss: 1.5461e-09 - val_loss: 1.6812e-08
Epoch 401/512
512/512 - 0s - loss: 1.5365e-09 - val_loss: 1.6708e-08
Epoch 402/512
512/512 - 0s - loss: 1.5270e-09 - val_loss: 1.6621e-08
Epoch 403/512
512/512 - 0s - loss: 1.5178e-09 - val_loss: 1.6524e-08
Epoch 404/512
512/512 - 0s - loss: 1.5084e-09 - val_loss: 1.6426e-08
Epoch 405/512
512/512 - 0s - loss: 1.4992e-09 - val_loss: 1.6323e-08
Epoch 406/512
512/512 - 0s - loss: 1.4901e-09 - val_loss: 1.6223e-08
Epoch 407/512
512/512 - 0s - loss: 1.4813e-09 - val_loss: 1.6126e-08
Epoch 408/512
512/512 - 0s - loss: 1.4724e-09 - val_loss: 1.6035e-08
Epoch 409/512
512/512 - 0s - loss: 1.4636e-09 - val_loss: 1.5958e-08
Epoch 410/512
512/512 - 0s - loss: 1.4550e-09 - val_loss: 1.5863e-08
Epoch 411/512
512/512 - 0s - loss: 1.4465e-09 - val_loss: 1.5769e-08
Epoch 412/512
512/512 - 0s - loss: 1.4380e-09 - val_loss: 1.5684e-08
Epoch 413/512
512/512 - 0s - loss: 1.4298e-09 - val_loss: 1.5603e-08
Epoch 414/512
512/512 - 0s - loss: 1.4215e-09 - val_loss: 1.5513e-08
Epoch 415/512
512/512 - 0s - loss: 1.4134e-09 - val_loss: 1.5422e-08
Epoch 416/512
512/512 - 0s - loss: 1.4054e-09 - val_loss: 1.5330e-08
Epoch 417/512
512/512 - 0s - loss: 1.3976e-09 - val_loss: 1.5244e-08
Epoch 418/512
512/512 - 0s - loss: 1.3898e-09 - val_loss: 1.5159e-08
Epoch 419/512
512/512 - 0s - loss: 1.3819e-09 - val_loss: 1.5084e-08
Epoch 420/512
512/512 - 0s - loss: 1.3742e-09 - val_loss: 1.5003e-08
Epoch 421/512
512/512 - 0s - loss: 1.3666e-09 - val_loss: 1.4922e-08
Epoch 422/512
512/512 - 0s - loss: 1.3591e-09 - val_loss: 1.4844e-08
Epoch 423/512
512/512 - 0s - loss: 1.3517e-09 - val_loss: 1.4772e-08
Epoch 424/512
512/512 - 0s - loss: 1.3442e-09 - val_loss: 1.4690e-08
Epoch 425/512
512/512 - 0s - loss: 1.3370e-09 - val_loss: 1.4623e-08
Epoch 426/512
512/512 - 0s - loss: 1.3298e-09 - val_loss: 1.4540e-08
Epoch 427/512
512/512 - 0s - loss: 1.3227e-09 - val_loss: 1.4469e-08
Epoch 428/512
512/512 - 0s - loss: 1.3156e-09 - val_loss: 1.4399e-08
Epoch 429/512
512/512 - 0s - loss: 1.3088e-09 - val_loss: 1.4324e-08
Epoch 430/512
512/512 - 0s - loss: 1.3020e-09 - val_loss: 1.4252e-08
Epoch 431/512
512/512 - 0s - loss: 1.2950e-09 - val_loss: 1.4180e-08
Epoch 432/512
512/512 - 0s - loss: 1.2884e-09 - val_loss: 1.4111e-08
Epoch 433/512
512/512 - 0s - loss: 1.2817e-09 - val_loss: 1.4040e-08
Epoch 434/512
512/512 - 0s - loss: 1.2750e-09 - val_loss: 1.3968e-08
Epoch 435/512
512/512 - 0s - loss: 1.2685e-09 - val_loss: 1.3904e-08
Epoch 436/512
512/512 - 0s - loss: 1.2622e-09 - val_loss: 1.3839e-08
Epoch 437/512
512/512 - 0s - loss: 1.2563e-09 - val_loss: 1.3774e-08
Epoch 438/512
512/512 - 0s - loss: 1.2496e-09 - val_loss: 1.3702e-08
Epoch 439/512
512/512 - 0s - loss: 1.2433e-09 - val_loss: 1.3635e-08
Epoch 440/512
512/512 - 0s - loss: 1.2373e-09 - val_loss: 1.3571e-08
Epoch 441/512
512/512 - 0s - loss: 1.2309e-09 - val_loss: 1.3503e-08
Epoch 442/512
512/512 - 0s - loss: 1.2247e-09 - val_loss: 1.3427e-08
Epoch 443/512
512/512 - 0s - loss: 1.2186e-09 - val_loss: 1.3363e-08
Epoch 444/512
512/512 - 0s - loss: 1.2126e-09 - val_loss: 1.3305e-08
Epoch 445/512
512/512 - 0s - loss: 1.2067e-09 - val_loss: 1.3237e-08
Epoch 446/512
512/512 - 0s - loss: 1.2008e-09 - val_loss: 1.3180e-08
Epoch 447/512
512/512 - 0s - loss: 1.1952e-09 - val_loss: 1.3126e-08
Epoch 448/512
512/512 - 0s - loss: 1.1895e-09 - val_loss: 1.3063e-08
Epoch 449/512
512/512 - 0s - loss: 1.1836e-09 - val_loss: 1.2994e-08
Epoch 450/512
512/512 - 0s - loss: 1.1780e-09 - val_loss: 1.2946e-08
Epoch 451/512
512/512 - 0s - loss: 1.1725e-09 - val_loss: 1.2883e-08
Epoch 452/512
512/512 - 0s - loss: 1.1669e-09 - val_loss: 1.2813e-08
Epoch 453/512
512/512 - 0s - loss: 1.1616e-09 - val_loss: 1.2747e-08
Epoch 454/512
512/512 - 0s - loss: 1.1563e-09 - val_loss: 1.2694e-08
Epoch 455/512
512/512 - 0s - loss: 1.1507e-09 - val_loss: 1.2643e-08
Epoch 456/512
512/512 - 0s - loss: 1.1454e-09 - val_loss: 1.2578e-08
Epoch 457/512
512/512 - 0s - loss: 1.1402e-09 - val_loss: 1.2528e-08
Epoch 458/512
512/512 - 0s - loss: 1.1349e-09 - val_loss: 1.2480e-08
Epoch 459/512
512/512 - 0s - loss: 1.1297e-09 - val_loss: 1.2421e-08
Epoch 460/512
512/512 - 0s - loss: 1.1246e-09 - val_loss: 1.2360e-08
Epoch 461/512
512/512 - 0s - loss: 1.1195e-09 - val_loss: 1.2318e-08
Epoch 462/512
512/512 - 0s - loss: 1.1147e-09 - val_loss: 1.2271e-08
Epoch 463/512
512/512 - 0s - loss: 1.1099e-09 - val_loss: 1.2218e-08
Epoch 464/512
512/512 - 0s - loss: 1.1047e-09 - val_loss: 1.2147e-08
Epoch 465/512
512/512 - 0s - loss: 1.0998e-09 - val_loss: 1.2094e-08
Epoch 466/512
512/512 - 0s - loss: 1.0949e-09 - val_loss: 1.2039e-08
Epoch 467/512
512/512 - 0s - loss: 1.0901e-09 - val_loss: 1.1988e-08
Epoch 468/512
512/512 - 0s - loss: 1.0854e-09 - val_loss: 1.1944e-08
Epoch 469/512
512/512 - 0s - loss: 1.0806e-09 - val_loss: 1.1898e-08
Epoch 470/512
512/512 - 0s - loss: 1.0758e-09 - val_loss: 1.1846e-08
Epoch 471/512
512/512 - 0s - loss: 1.0712e-09 - val_loss: 1.1797e-08
Epoch 472/512
512/512 - 0s - loss: 1.0666e-09 - val_loss: 1.1745e-08
Epoch 473/512
512/512 - 0s - loss: 1.0621e-09 - val_loss: 1.1698e-08
Epoch 474/512
512/512 - 0s - loss: 1.0575e-09 - val_loss: 1.1645e-08
Epoch 475/512
512/512 - 0s - loss: 1.0531e-09 - val_loss: 1.1599e-08
Epoch 476/512
512/512 - 0s - loss: 1.0486e-09 - val_loss: 1.1555e-08
Epoch 477/512
512/512 - 0s - loss: 1.0442e-09 - val_loss: 1.1512e-08
Epoch 478/512
512/512 - 0s - loss: 1.0399e-09 - val_loss: 1.1461e-08
Epoch 479/512
512/512 - 0s - loss: 1.0355e-09 - val_loss: 1.1414e-08
Epoch 480/512
512/512 - 0s - loss: 1.0312e-09 - val_loss: 1.1369e-08
Epoch 481/512
512/512 - 0s - loss: 1.0270e-09 - val_loss: 1.1320e-08
Epoch 482/512
512/512 - 0s - loss: 1.0228e-09 - val_loss: 1.1275e-08
Epoch 483/512
512/512 - 0s - loss: 1.0186e-09 - val_loss: 1.1240e-08
Epoch 484/512
512/512 - 0s - loss: 1.0144e-09 - val_loss: 1.1187e-08
Epoch 485/512
512/512 - 0s - loss: 1.0103e-09 - val_loss: 1.1143e-08
Epoch 486/512
512/512 - 0s - loss: 1.0062e-09 - val_loss: 1.1103e-08
Epoch 487/512
512/512 - 0s - loss: 1.0021e-09 - val_loss: 1.1060e-08
Epoch 488/512
512/512 - 0s - loss: 9.9828e-10 - val_loss: 1.1021e-08
Epoch 489/512
512/512 - 0s - loss: 9.9411e-10 - val_loss: 1.0971e-08
Epoch 490/512
512/512 - 0s - loss: 9.9013e-10 - val_loss: 1.0929e-08
Epoch 491/512
512/512 - 0s - loss: 9.8618e-10 - val_loss: 1.0880e-08
Epoch 492/512
512/512 - 0s - loss: 9.8228e-10 - val_loss: 1.0838e-08
Epoch 493/512
512/512 - 0s - loss: 9.7840e-10 - val_loss: 1.0801e-08
Epoch 494/512
512/512 - 0s - loss: 9.7469e-10 - val_loss: 1.0763e-08
Epoch 495/512
512/512 - 0s - loss: 9.7082e-10 - val_loss: 1.0721e-08
Epoch 496/512
512/512 - 0s - loss: 9.6715e-10 - val_loss: 1.0684e-08
Epoch 497/512
512/512 - 0s - loss: 9.6336e-10 - val_loss: 1.0637e-08
Epoch 498/512
512/512 - 0s - loss: 9.5960e-10 - val_loss: 1.0591e-08
Epoch 499/512
512/512 - 0s - loss: 9.5589e-10 - val_loss: 1.0560e-08
Epoch 500/512
512/512 - 0s - loss: 9.5228e-10 - val_loss: 1.0525e-08
Epoch 501/512
512/512 - 0s - loss: 9.4859e-10 - val_loss: 1.0482e-08
Epoch 502/512
512/512 - 0s - loss: 9.4501e-10 - val_loss: 1.0431e-08
Epoch 503/512
512/512 - 0s - loss: 9.4159e-10 - val_loss: 1.0393e-08
Epoch 504/512
512/512 - 0s - loss: 9.3788e-10 - val_loss: 1.0366e-08
Epoch 505/512
512/512 - 0s - loss: 9.3436e-10 - val_loss: 1.0331e-08
Epoch 506/512
512/512 - 0s - loss: 9.3088e-10 - val_loss: 1.0290e-08
Epoch 507/512
512/512 - 0s - loss: 9.2746e-10 - val_loss: 1.0259e-08
Epoch 508/512
512/512 - 0s - loss: 9.2404e-10 - val_loss: 1.0219e-08
Epoch 509/512
512/512 - 0s - loss: 9.2066e-10 - val_loss: 1.0185e-08
Epoch 510/512
512/512 - 0s - loss: 9.1726e-10 - val_loss: 1.0143e-08
Epoch 511/512
512/512 - 0s - loss: 9.1379e-10 - val_loss: 1.0108e-08
Epoch 512/512
512/512 - 0s - loss: 9.1048e-10 - val_loss: 1.0079e-08
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.2784e-10 - val_loss: 1.9845e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7327e-09 - val_loss: 2.1831e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2918e-09 - val_loss: 4.5106e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.0786e-10 - val_loss: 1.7728e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5668e-10 - val_loss: 1.4199e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4964e-10 - val_loss: 1.8191e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2924e-10 - val_loss: 3.5263e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8169e-10 - val_loss: 7.3435e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2195e-10 - val_loss: 8.7656e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7475e-10 - val_loss: 6.0920e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1004e-10 - val_loss: 3.9026e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4929e-10 - val_loss: 3.0949e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0309e-10 - val_loss: 3.1420e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3068e-10 - val_loss: 3.7372e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9910e-10 - val_loss: 4.4583e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5689e-10 - val_loss: 4.6995e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5256e-10 - val_loss: 4.2790e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0076e-10 - val_loss: 3.6971e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4926e-10 - val_loss: 3.2830e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1819e-10 - val_loss: 3.1393e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1155e-10 - val_loss: 3.1461e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1534e-10 - val_loss: 3.2207e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2201e-10 - val_loss: 3.2395e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1986e-10 - val_loss: 3.1704e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0982e-10 - val_loss: 3.0099e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9127e-10 - val_loss: 2.8021e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7457e-10 - val_loss: 2.6860e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6360e-10 - val_loss: 2.6087e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5694e-10 - val_loss: 2.5513e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5330e-10 - val_loss: 2.5631e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5431e-10 - val_loss: 2.5250e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4872e-10 - val_loss: 2.4687e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4237e-10 - val_loss: 2.3820e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3308e-10 - val_loss: 2.2936e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2503e-10 - val_loss: 2.2106e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1669e-10 - val_loss: 2.1305e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0919e-10 - val_loss: 2.0747e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0560e-10 - val_loss: 2.0534e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0269e-10 - val_loss: 2.0099e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9862e-10 - val_loss: 1.9552e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9211e-10 - val_loss: 1.9096e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8819e-10 - val_loss: 1.8583e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8312e-10 - val_loss: 1.8196e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7907e-10 - val_loss: 1.7621e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7248e-10 - val_loss: 1.6966e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6727e-10 - val_loss: 1.6599e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6447e-10 - val_loss: 1.6483e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6188e-10 - val_loss: 1.5967e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5797e-10 - val_loss: 1.5599e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5350e-10 - val_loss: 1.5068e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4886e-10 - val_loss: 1.4852e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4655e-10 - val_loss: 1.4531e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4340e-10 - val_loss: 1.4336e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4076e-10 - val_loss: 1.3838e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3726e-10 - val_loss: 1.3610e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3467e-10 - val_loss: 1.3469e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3358e-10 - val_loss: 1.3369e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3289e-10 - val_loss: 1.3243e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3072e-10 - val_loss: 1.2995e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2788e-10 - val_loss: 1.2522e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2283e-10 - val_loss: 1.2105e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1908e-10 - val_loss: 1.1715e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1522e-10 - val_loss: 1.1338e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1237e-10 - val_loss: 1.1267e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1199e-10 - val_loss: 1.1180e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1115e-10 - val_loss: 1.1224e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1174e-10 - val_loss: 1.1066e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0931e-10 - val_loss: 1.0783e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0578e-10 - val_loss: 1.0499e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0324e-10 - val_loss: 1.0145e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0076e-10 - val_loss: 9.9138e-11
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.8006e-11 - val_loss: 9.6121e-11
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.4728e-11 - val_loss: 9.3998e-11
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3443e-11 - val_loss: 9.4222e-11
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3859e-11 - val_loss: 9.4483e-11
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.3614e-11 - val_loss: 9.3897e-11
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.3085e-11 - val_loss: 9.1536e-11
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.0478e-11 - val_loss: 8.9356e-11
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.8259e-11 - val_loss: 8.7842e-11
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.6028e-11 - val_loss: 8.4479e-11
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.3671e-11 - val_loss: 8.3701e-11
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3696e-11 - val_loss: 8.4625e-11
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.3512e-11 - val_loss: 8.1795e-11
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.0766e-11 - val_loss: 7.9650e-11
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.8651e-11 - val_loss: 7.7176e-11
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.6340e-11 - val_loss: 7.5634e-11
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.4353e-11 - val_loss: 7.3602e-11
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3396e-11 - val_loss: 7.3645e-11
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.2635e-11 - val_loss: 7.1000e-11
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.0315e-11 - val_loss: 7.0750e-11
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0878e-11 - val_loss: 7.1896e-11
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.0766e-11 - val_loss: 6.9388e-11
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.8582e-11 - val_loss: 6.8495e-11
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.7874e-11 - val_loss: 6.7205e-11
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.6877e-11 - val_loss: 6.6665e-11
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.6058e-11 - val_loss: 6.5146e-11
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4756e-11 - val_loss: 6.5159e-11
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.4596e-11 - val_loss: 6.4407e-11
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.4401e-11 - val_loss: 6.4356e-11
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.3492e-11 - val_loss: 6.2261e-11
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.1436e-11 - val_loss: 6.0355e-11
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.9914e-11 - val_loss: 5.9678e-11
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.9236e-11 - val_loss: 5.8910e-11
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.8588e-11 - val_loss: 5.8722e-11
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8899e-11 - val_loss: 5.9580e-11
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9374e-11 - val_loss: 5.8730e-11
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.7659e-11 - val_loss: 5.6630e-11
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.6105e-11 - val_loss: 5.6030e-11
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.5340e-11 - val_loss: 5.5288e-11
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.4875e-11 - val_loss: 5.4370e-11
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.3613e-11 - val_loss: 5.3793e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3818e-11 - val_loss: 5.4195e-11
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.3559e-11 - val_loss: 5.2795e-11
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.2313e-11 - val_loss: 5.1722e-11
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.1342e-11 - val_loss: 5.0917e-11
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.0386e-11 - val_loss: 4.9781e-11
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.9264e-11 - val_loss: 4.9000e-11
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.8543e-11 - val_loss: 4.8571e-11
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.8316e-11 - val_loss: 4.7920e-11
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8204e-11 - val_loss: 4.8453e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8139e-11 - val_loss: 4.8187e-11
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.7962e-11 - val_loss: 4.7622e-11
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.7091e-11 - val_loss: 4.6740e-11
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.6354e-11 - val_loss: 4.5460e-11
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.5213e-11 - val_loss: 4.5181e-11
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.4634e-11 - val_loss: 4.4534e-11
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.3964e-11 - val_loss: 4.3342e-11
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.3073e-11 - val_loss: 4.2835e-11
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.2194e-11 - val_loss: 4.0845e-11
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 4.0361e-11 - val_loss: 4.0003e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9827e-11 - val_loss: 4.0100e-11
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.9825e-11 - val_loss: 3.9791e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0078e-11 - val_loss: 4.0602e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0509e-11 - val_loss: 4.0624e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0210e-11 - val_loss: 3.9932e-11
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.9251e-11 - val_loss: 3.8575e-11
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.8034e-11 - val_loss: 3.7557e-11
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.7403e-11 - val_loss: 3.7391e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.7109e-11 - val_loss: 3.6583e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6550e-11 - val_loss: 3.7024e-11
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.6732e-11 - val_loss: 3.6396e-11
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.6215e-11 - val_loss: 3.5872e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6111e-11 - val_loss: 3.6389e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6369e-11 - val_loss: 3.6334e-11
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.5855e-11 - val_loss: 3.5382e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5407e-11 - val_loss: 3.5513e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.5190e-11 - val_loss: 3.4789e-11
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.4014e-11 - val_loss: 3.3637e-11
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.3326e-11 - val_loss: 3.3063e-11
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.2333e-11 - val_loss: 3.2005e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2088e-11 - val_loss: 3.2157e-11
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.1705e-11 - val_loss: 3.1388e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1791e-11 - val_loss: 3.1754e-11
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.1546e-11 - val_loss: 3.1219e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1010e-11 - val_loss: 3.1370e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1239e-11 - val_loss: 3.1345e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1461e-11 - val_loss: 3.1937e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1875e-11 - val_loss: 3.1482e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.0988e-11 - val_loss: 3.0551e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.0144e-11 - val_loss: 3.0113e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0363e-11 - val_loss: 3.0849e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.0240e-11 - val_loss: 2.9343e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.8894e-11 - val_loss: 2.9014e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.8995e-11 - val_loss: 2.8481e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8461e-11 - val_loss: 2.8787e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9027e-11 - val_loss: 2.9528e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9467e-11 - val_loss: 2.9096e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8852e-11 - val_loss: 2.9114e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9037e-11 - val_loss: 2.9098e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.8803e-11 - val_loss: 2.8307e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.7859e-11 - val_loss: 2.6885e-11
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.6330e-11 - val_loss: 2.5929e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6219e-11 - val_loss: 2.7036e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7088e-11 - val_loss: 2.6978e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6749e-11 - val_loss: 2.6942e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6924e-11 - val_loss: 2.7316e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6895e-11 - val_loss: 2.6265e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6185e-11 - val_loss: 2.5998e-11
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.5794e-11 - val_loss: 2.5749e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.5702e-11 - val_loss: 2.5371e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.4938e-11 - val_loss: 2.4928e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5155e-11 - val_loss: 2.5117e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.4685e-11 - val_loss: 2.4486e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4691e-11 - val_loss: 2.5044e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4951e-11 - val_loss: 2.5134e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.4895e-11 - val_loss: 2.4291e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.4088e-11 - val_loss: 2.3768e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4156e-11 - val_loss: 2.4718e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4920e-11 - val_loss: 2.5081e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4763e-11 - val_loss: 2.4224e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4075e-11 - val_loss: 2.4073e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4223e-11 - val_loss: 2.4467e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.4212e-11 - val_loss: 2.3560e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.2979e-11 - val_loss: 2.2333e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.2024e-11 - val_loss: 2.1727e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.1408e-11 - val_loss: 2.1481e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.1267e-11 - val_loss: 2.0966e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.0768e-11 - val_loss: 2.0566e-11
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 2.0286e-11 - val_loss: 1.9914e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9685e-11 - val_loss: 1.9917e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9990e-11 - val_loss: 2.0543e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0544e-11 - val_loss: 2.0843e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1070e-11 - val_loss: 2.1752e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1621e-11 - val_loss: 2.1565e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1351e-11 - val_loss: 2.1295e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1193e-11 - val_loss: 2.1352e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1135e-11 - val_loss: 2.1055e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0582e-11 - val_loss: 2.0181e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.9730e-11 - val_loss: 1.9353e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.9323e-11 - val_loss: 1.9215e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9656e-11 - val_loss: 2.0256e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0264e-11 - val_loss: 2.0655e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0733e-11 - val_loss: 2.0253e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0059e-11 - val_loss: 1.9961e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9748e-11 - val_loss: 1.9688e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9662e-11 - val_loss: 1.9974e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9846e-11 - val_loss: 1.9997e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9822e-11 - val_loss: 1.9628e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9481e-11 - val_loss: 1.9263e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.9102e-11 - val_loss: 1.9035e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9153e-11 - val_loss: 1.9153e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.9093e-11 - val_loss: 1.8923e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9080e-11 - val_loss: 1.9582e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9806e-11 - val_loss: 1.9956e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9653e-11 - val_loss: 1.9140e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.8896e-11 - val_loss: 1.8574e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.8585e-11 - val_loss: 1.8549e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.8343e-11 - val_loss: 1.7967e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.7859e-11 - val_loss: 1.7671e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.7387e-11 - val_loss: 1.7157e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7011e-11 - val_loss: 1.7353e-11
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.7204e-11 - val_loss: 1.7073e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7095e-11 - val_loss: 1.7172e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7490e-11 - val_loss: 1.8061e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8133e-11 - val_loss: 1.8381e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8354e-11 - val_loss: 1.8566e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8626e-11 - val_loss: 1.8954e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9187e-11 - val_loss: 1.9369e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9498e-11 - val_loss: 1.9238e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9142e-11 - val_loss: 1.8818e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8667e-11 - val_loss: 1.8604e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8411e-11 - val_loss: 1.8115e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8127e-11 - val_loss: 1.8029e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7885e-11 - val_loss: 1.7630e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7404e-11 - val_loss: 1.7305e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.6994e-11 - val_loss: 1.6426e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.6210e-11 - val_loss: 1.6061e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6079e-11 - val_loss: 1.6272e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6369e-11 - val_loss: 1.6388e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.6222e-11 - val_loss: 1.5969e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5823e-11 - val_loss: 1.5695e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5743e-11 - val_loss: 1.5694e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5581e-11 - val_loss: 1.5578e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5521e-11 - val_loss: 1.5565e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5633e-11 - val_loss: 1.6026e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6183e-11 - val_loss: 1.6275e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6329e-11 - val_loss: 1.6264e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6316e-11 - val_loss: 1.6594e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6419e-11 - val_loss: 1.6090e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5955e-11 - val_loss: 1.5722e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5396e-11 - val_loss: 1.5093e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5031e-11 - val_loss: 1.5035e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4862e-11 - val_loss: 1.4837e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4767e-11 - val_loss: 1.4707e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4896e-11 - val_loss: 1.4975e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5025e-11 - val_loss: 1.5222e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5197e-11 - val_loss: 1.4914e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4697e-11 - val_loss: 1.4447e-11
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4232e-11 - val_loss: 1.4161e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4047e-11 - val_loss: 1.3919e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3861e-11 - val_loss: 1.3906e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3733e-11 - val_loss: 1.3813e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3696e-11 - val_loss: 1.3461e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3369e-11 - val_loss: 1.3161e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3230e-11 - val_loss: 1.3287e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3092e-11 - val_loss: 1.2999e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2980e-11 - val_loss: 1.3054e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3281e-11 - val_loss: 1.3499e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3593e-11 - val_loss: 1.3788e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3944e-11 - val_loss: 1.4093e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4058e-11 - val_loss: 1.3996e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3892e-11 - val_loss: 1.3937e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3907e-11 - val_loss: 1.3792e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3739e-11 - val_loss: 1.3424e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3329e-11 - val_loss: 1.3396e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3502e-11 - val_loss: 1.3823e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3624e-11 - val_loss: 1.3661e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3686e-11 - val_loss: 1.3888e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3836e-11 - val_loss: 1.3846e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3776e-11 - val_loss: 1.3697e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3672e-11 - val_loss: 1.3624e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3673e-11 - val_loss: 1.4038e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4175e-11 - val_loss: 1.4554e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4470e-11 - val_loss: 1.4473e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4507e-11 - val_loss: 1.4543e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4397e-11 - val_loss: 1.4043e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3940e-11 - val_loss: 1.3685e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3694e-11 - val_loss: 1.3534e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3489e-11 - val_loss: 1.3360e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3342e-11 - val_loss: 1.3298e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3139e-11 - val_loss: 1.3111e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3204e-11 - val_loss: 1.3178e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3168e-11 - val_loss: 1.3051e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3066e-11 - val_loss: 1.3096e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3136e-11 - val_loss: 1.3111e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2887e-11 - val_loss: 1.2531e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2493e-11 - val_loss: 1.2402e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2160e-11 - val_loss: 1.1884e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1978e-11 - val_loss: 1.2179e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2113e-11 - val_loss: 1.1990e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1939e-11 - val_loss: 1.2132e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2182e-11 - val_loss: 1.2084e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2261e-11 - val_loss: 1.2580e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2416e-11 - val_loss: 1.2170e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2127e-11 - val_loss: 1.2231e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2260e-11 - val_loss: 1.2107e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2160e-11 - val_loss: 1.2449e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2429e-11 - val_loss: 1.2301e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2205e-11 - val_loss: 1.2170e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2226e-11 - val_loss: 1.2163e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1985e-11 - val_loss: 1.2027e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2132e-11 - val_loss: 1.2132e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2026e-11 - val_loss: 1.2095e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1829e-11 - val_loss: 1.1446e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1409e-11 - val_loss: 1.1349e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1470e-11 - val_loss: 1.1553e-11
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1411e-11 - val_loss: 1.1280e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1352e-11 - val_loss: 1.1342e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1210e-11 - val_loss: 1.1108e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1023e-11 - val_loss: 1.0904e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0865e-11 - val_loss: 1.0865e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0948e-11 - val_loss: 1.1206e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1038e-11 - val_loss: 1.0934e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0809e-11 - val_loss: 1.0690e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0755e-11 - val_loss: 1.0842e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0779e-11 - val_loss: 1.0717e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0845e-11 - val_loss: 1.0930e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0773e-11 - val_loss: 1.0792e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0780e-11 - val_loss: 1.0863e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0683e-11 - val_loss: 1.0332e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0309e-11 - val_loss: 1.0012e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9678e-12 - val_loss: 1.0046e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0012e-11 - val_loss: 1.0032e-11
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0058e-11 - val_loss: 9.9723e-12
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.9591e-12 - val_loss: 9.8727e-12
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.8098e-12 - val_loss: 9.6963e-12
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7689e-12 - val_loss: 9.9256e-12
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8817e-12 - val_loss: 9.7313e-12
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.5842e-12 - val_loss: 9.6772e-12
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7756e-12 - val_loss: 9.9570e-12
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.8402e-12 - val_loss: 9.6316e-12
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6534e-12 - val_loss: 9.7666e-12
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8945e-12 - val_loss: 9.9920e-12
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0041e-11 - val_loss: 1.0132e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0056e-11 - val_loss: 1.0076e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0092e-11 - val_loss: 1.0132e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9867e-12 - val_loss: 9.8144e-12
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8033e-12 - val_loss: 9.7485e-12
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6511e-12 - val_loss: 9.6324e-12
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6509e-12 - val_loss: 9.7138e-12
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.6987e-12 - val_loss: 9.5529e-12
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6505e-12 - val_loss: 9.8105e-12
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9706e-12 - val_loss: 1.0208e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0333e-11 - val_loss: 1.0521e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0475e-11 - val_loss: 1.0427e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0471e-11 - val_loss: 1.0597e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0612e-11 - val_loss: 1.0657e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0687e-11 - val_loss: 1.0725e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0645e-11 - val_loss: 1.0657e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0655e-11 - val_loss: 1.0649e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0552e-11 - val_loss: 1.0496e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0509e-11 - val_loss: 1.0440e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0540e-11 - val_loss: 1.0641e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0643e-11 - val_loss: 1.0708e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0712e-11 - val_loss: 1.0619e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0595e-11 - val_loss: 1.0638e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0694e-11 - val_loss: 1.0648e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0570e-11 - val_loss: 1.0557e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0510e-11 - val_loss: 1.0457e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0386e-11 - val_loss: 1.0459e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0476e-11 - val_loss: 1.0417e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0429e-11 - val_loss: 1.0453e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0422e-11 - val_loss: 1.0350e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0342e-11 - val_loss: 1.0421e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0380e-11 - val_loss: 1.0391e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0361e-11 - val_loss: 1.0459e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0535e-11 - val_loss: 1.0597e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0575e-11 - val_loss: 1.0578e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0586e-11 - val_loss: 1.0548e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0473e-11 - val_loss: 1.0407e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0461e-11 - val_loss: 1.0470e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0395e-11 - val_loss: 1.0395e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0300e-11 - val_loss: 1.0107e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0207e-11 - val_loss: 1.0260e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0182e-11 - val_loss: 1.0194e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0238e-11 - val_loss: 1.0251e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0159e-11 - val_loss: 1.0184e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0007e-11 - val_loss: 9.7094e-12
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.5694e-12 - val_loss: 9.4079e-12
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.3247e-12 - val_loss: 9.3313e-12
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3950e-12 - val_loss: 9.3930e-12
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.2578e-12 - val_loss: 9.1230e-12
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1448e-12 - val_loss: 9.3467e-12
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3244e-12 - val_loss: 9.3058e-12
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.1363e-12 - val_loss: 9.0306e-12
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1007e-12 - val_loss: 9.3331e-12
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4403e-12 - val_loss: 9.4629e-12
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4332e-12 - val_loss: 9.3861e-12
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2907e-12 - val_loss: 9.3083e-12
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3612e-12 - val_loss: 9.3308e-12
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3344e-12 - val_loss: 9.2903e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2563e-12 - val_loss: 9.3105e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3486e-12 - val_loss: 9.4419e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4579e-12 - val_loss: 9.2822e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2917e-12 - val_loss: 9.3859e-12
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3315e-12 - val_loss: 9.4557e-12
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4858e-12 - val_loss: 9.3069e-12
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3239e-12 - val_loss: 9.3593e-12
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3569e-12 - val_loss: 9.3895e-12
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2919e-12 - val_loss: 9.1676e-12
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2321e-12 - val_loss: 9.2029e-12
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2417e-12 - val_loss: 9.2658e-12
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2592e-12 - val_loss: 9.1848e-12
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2433e-12 - val_loss: 9.2839e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2870e-12 - val_loss: 9.3135e-12
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4003e-12 - val_loss: 9.3824e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2925e-12 - val_loss: 9.2057e-12
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2585e-12 - val_loss: 9.3098e-12
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3357e-12 - val_loss: 9.2822e-12
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1801e-12 - val_loss: 9.0686e-12
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1512e-12 - val_loss: 9.2643e-12
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3006e-12 - val_loss: 9.2866e-12
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1734e-12 - val_loss: 9.0340e-12
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0196e-12 - val_loss: 9.0510e-12
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1697e-12 - val_loss: 9.3780e-12
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2933e-12 - val_loss: 9.1200e-12
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1756e-12 - val_loss: 9.3066e-12
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3430e-12 - val_loss: 9.4096e-12
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5385e-12 - val_loss: 9.6036e-12
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4858e-12 - val_loss: 9.2413e-12
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2759e-12 - val_loss: 9.3993e-12
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4637e-12 - val_loss: 9.4464e-12
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3161e-12 - val_loss: 9.1751e-12
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1443e-12 - val_loss: 9.1429e-12
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2924e-12 - val_loss: 9.3719e-12
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3513e-12 - val_loss: 9.1880e-12
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2098e-12 - val_loss: 9.2335e-12
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2775e-12 - val_loss: 9.4408e-12
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4868e-12 - val_loss: 9.4468e-12
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4063e-12 - val_loss: 9.3321e-12
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2999e-12 - val_loss: 9.2831e-12
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3460e-12 - val_loss: 9.4789e-12
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3547e-12 - val_loss: 9.1489e-12
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.0304e-12 - val_loss: 8.8461e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9942e-12 - val_loss: 9.2746e-12
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2033e-12 - val_loss: 9.0312e-12
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.9313e-12 - val_loss: 8.7319e-12
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7957e-12 - val_loss: 8.9302e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0013e-12 - val_loss: 9.0378e-12
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9275e-12 - val_loss: 8.7602e-12
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8505e-12 - val_loss: 8.9335e-12
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9453e-12 - val_loss: 9.0311e-12
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0812e-12 - val_loss: 9.0887e-12
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0011e-12 - val_loss: 8.8520e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8607e-12 - val_loss: 9.0549e-12
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1328e-12 - val_loss: 9.1729e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1024e-12 - val_loss: 8.8791e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9043e-12 - val_loss: 8.8926e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0048e-12 - val_loss: 9.0693e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1025e-12 - val_loss: 9.1430e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0076e-12 - val_loss: 8.8237e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8609e-12 - val_loss: 8.9911e-12
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1413e-12 - val_loss: 9.1088e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0178e-12 - val_loss: 8.8693e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8213e-12 - val_loss: 8.7604e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8476e-12 - val_loss: 9.0481e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9678e-12 - val_loss: 8.8765e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9048e-12 - val_loss: 8.8873e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8479e-12 - val_loss: 8.8007e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8468e-12 - val_loss: 8.8990e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8311e-12 - val_loss: 8.7520e-12
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.6321e-12 - val_loss: 8.4180e-12
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.3966e-12 - val_loss: 8.3591e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4713e-12 - val_loss: 8.5309e-12
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.4083e-12 - val_loss: 8.2621e-12
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.2262e-12 - val_loss: 8.1310e-12
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2110e-12 - val_loss: 8.3237e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3264e-12 - val_loss: 8.2720e-12
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.2199e-12 - val_loss: 8.1210e-12
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.0632e-12 - val_loss: 8.1035e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1214e-12 - val_loss: 8.1355e-12
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.1694e-12 - val_loss: 8.0883e-12
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.0496e-12 - val_loss: 7.9693e-12
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0009e-12 - val_loss: 8.0079e-12
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.9498e-12 - val_loss: 7.8577e-12
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.8458e-12 - val_loss: 7.7310e-12
Epoch 497/512

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.6768e-12 - val_loss: 7.6498e-12
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.5748e-12 - val_loss: 7.4671e-12
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.4399e-12 - val_loss: 7.3978e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5455e-12 - val_loss: 7.6198e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6517e-12 - val_loss: 7.7202e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5981e-12 - val_loss: 7.3990e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4195e-12 - val_loss: 7.4513e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5138e-12 - val_loss: 7.5097e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5109e-12 - val_loss: 7.5295e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5217e-12 - val_loss: 7.4754e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4423e-12 - val_loss: 7.4805e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4522e-12 - val_loss: 7.4361e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4829e-12 - val_loss: 7.4504e-12
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.3475e-12 - val_loss: 7.2509e-12
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.1745e-12 - val_loss: 7.1984e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3550e-12 - val_loss: 7.4957e-12
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.379 | eve: 10.258 | bob: 9.296Epoch   0:   0% | abe: 9.330 | eve: 10.288 | bob: 9.254Epoch   0:   1% | abe: 9.297 | eve: 10.274 | bob: 9.226Epoch   0:   2% | abe: 9.281 | eve: 10.281 | bob: 9.216Epoch   0:   3% | abe: 9.244 | eve: 10.273 | bob: 9.184Epoch   0:   3% | abe: 9.229 | eve: 10.275 | bob: 9.174Epoch   0:   4% | abe: 9.201 | eve: 10.280 | bob: 9.149Epoch   0:   5% | abe: 9.190 | eve: 10.276 | bob: 9.141Epoch   0:   6% | abe: 9.183 | eve: 10.284 | bob: 9.137Epoch   0:   7% | abe: 9.173 | eve: 10.295 | bob: 9.129Epoch   0:   7% | abe: 9.169 | eve: 10.300 | bob: 9.128Epoch   0:   8% | abe: 9.160 | eve: 10.305 | bob: 9.120Epoch   0:   9% | abe: 9.149 | eve: 10.317 | bob: 9.110Epoch   0:  10% | abe: 9.143 | eve: 10.320 | bob: 9.104Epoch   0:  10% | abe: 9.137 | eve: 10.330 | bob: 9.100Epoch   0:  11% | abe: 9.131 | eve: 10.329 | bob: 9.094Epoch   0:  12% | abe: 9.126 | eve: 10.330 | bob: 9.090Epoch   0:  13% | abe: 9.125 | eve: 10.335 | bob: 9.089Epoch   0:  14% | abe: 9.125 | eve: 10.337 | bob: 9.089Epoch   0:  14% | abe: 9.120 | eve: 10.339 | bob: 9.084Epoch   0:  15% | abe: 9.119 | eve: 10.342 | bob: 9.084Epoch   0:  16% | abe: 9.114 | eve: 10.346 | bob: 9.078Epoch   0:  17% | abe: 9.110 | eve: 10.348 | bob: 9.074Epoch   0:  17% | abe: 9.107 | eve: 10.350 | bob: 9.071Epoch   0:  18% | abe: 9.106 | eve: 10.354 | bob: 9.071Epoch   0:  19% | abe: 9.105 | eve: 10.359 | bob: 9.070Epoch   0:  20% | abe: 9.105 | eve: 10.364 | bob: 9.069Epoch   0:  21% | abe: 9.102 | eve: 10.370 | bob: 9.067Epoch   0:  21% | abe: 9.102 | eve: 10.371 | bob: 9.066Epoch   0:  22% | abe: 9.101 | eve: 10.376 | bob: 9.065Epoch   0:  23% | abe: 9.099 | eve: 10.382 | bob: 9.063Epoch   0:  24% | abe: 9.098 | eve: 10.386 | bob: 9.062Epoch   0:  25% | abe: 9.098 | eve: 10.391 | bob: 9.063Epoch   0:  25% | abe: 9.098 | eve: 10.397 | bob: 9.062Epoch   0:  26% | abe: 9.094 | eve: 10.401 | bob: 9.059Epoch   0:  27% | abe: 9.093 | eve: 10.404 | bob: 9.057Epoch   0:  28% | abe: 9.092 | eve: 10.407 | bob: 9.056Epoch   0:  28% | abe: 9.092 | eve: 10.412 | bob: 9.055Epoch   0:  29% | abe: 9.090 | eve: 10.418 | bob: 9.054Epoch   0:  30% | abe: 9.088 | eve: 10.423 | bob: 9.052Epoch   0:  31% | abe: 9.087 | eve: 10.427 | bob: 9.051Epoch   0:  32% | abe: 9.086 | eve: 10.430 | bob: 9.050Epoch   0:  32% | abe: 9.087 | eve: 10.435 | bob: 9.050Epoch   0:  33% | abe: 9.086 | eve: 10.439 | bob: 9.050Epoch   0:  34% | abe: 9.085 | eve: 10.444 | bob: 9.048Epoch   0:  35% | abe: 9.083 | eve: 10.448 | bob: 9.046Epoch   0:  35% | abe: 9.082 | eve: 10.453 | bob: 9.045Epoch   0:  36% | abe: 9.081 | eve: 10.457 | bob: 9.044Epoch   0:  37% | abe: 9.080 | eve: 10.462 | bob: 9.043Epoch   0:  38% | abe: 9.080 | eve: 10.464 | bob: 9.043Epoch   0:  39% | abe: 9.079 | eve: 10.467 | bob: 9.042Epoch   0:  39% | abe: 9.080 | eve: 10.471 | bob: 9.042Epoch   0:  40% | abe: 9.079 | eve: 10.476 | bob: 9.042Epoch   0:  41% | abe: 9.080 | eve: 10.477 | bob: 9.042Epoch   0:  42% | abe: 9.080 | eve: 10.481 | bob: 9.042Epoch   0:  42% | abe: 9.080 | eve: 10.486 | bob: 9.042Epoch   0:  43% | abe: 9.079 | eve: 10.489 | bob: 9.041Epoch   0:  44% | abe: 9.078 | eve: 10.491 | bob: 9.040Epoch   0:  45% | abe: 9.077 | eve: 10.495 | bob: 9.039Epoch   0:  46% | abe: 9.077 | eve: 10.499 | bob: 9.038Epoch   0:  46% | abe: 9.076 | eve: 10.502 | bob: 9.038Epoch   0:  47% | abe: 9.076 | eve: 10.504 | bob: 9.037Epoch   0:  48% | abe: 9.076 | eve: 10.506 | bob: 9.037Epoch   0:  49% | abe: 9.075 | eve: 10.510 | bob: 9.037Epoch   0:  50% | abe: 9.074 | eve: 10.513 | bob: 9.035Epoch   0:  50% | abe: 9.074 | eve: 10.516 | bob: 9.035Epoch   0:  51% | abe: 9.073 | eve: 10.519 | bob: 9.034Epoch   0:  52% | abe: 9.073 | eve: 10.521 | bob: 9.033Epoch   0:  53% | abe: 9.072 | eve: 10.522 | bob: 9.033Epoch   0:  53% | abe: 9.072 | eve: 10.525 | bob: 9.032Epoch   0:  54% | abe: 9.073 | eve: 10.528 | bob: 9.033Epoch   0:  55% | abe: 9.072 | eve: 10.532 | bob: 9.032Epoch   0:  56% | abe: 9.071 | eve: 10.534 | bob: 9.031Epoch   0:  57% | abe: 9.070 | eve: 10.537 | bob: 9.030Epoch   0:  57% | abe: 9.070 | eve: 10.539 | bob: 9.029Epoch   0:  58% | abe: 9.069 | eve: 10.542 | bob: 9.029Epoch   0:  59% | abe: 9.068 | eve: 10.544 | bob: 9.028Epoch   0:  60% | abe: 9.068 | eve: 10.547 | bob: 9.027Epoch   0:  60% | abe: 9.069 | eve: 10.549 | bob: 9.028Epoch   0:  61% | abe: 9.068 | eve: 10.552 | bob: 9.027Epoch   0:  62% | abe: 9.068 | eve: 10.553 | bob: 9.027Epoch   0:  63% | abe: 9.067 | eve: 10.555 | bob: 9.026Epoch   0:  64% | abe: 9.067 | eve: 10.557 | bob: 9.026Epoch   0:  64% | abe: 9.067 | eve: 10.559 | bob: 9.025Epoch   0:  65% | abe: 9.067 | eve: 10.561 | bob: 9.026Epoch   0:  66% | abe: 9.066 | eve: 10.563 | bob: 9.025Epoch   0:  67% | abe: 9.066 | eve: 10.565 | bob: 9.024Epoch   0:  67% | abe: 9.065 | eve: 10.568 | bob: 9.024Epoch   0:  68% | abe: 9.065 | eve: 10.570 | bob: 9.024Epoch   0:  69% | abe: 9.065 | eve: 10.572 | bob: 9.024Epoch   0:  70% | abe: 9.065 | eve: 10.574 | bob: 9.023Epoch   0:  71% | abe: 9.065 | eve: 10.576 | bob: 9.023Epoch   0:  71% | abe: 9.065 | eve: 10.578 | bob: 9.022Epoch   0:  72% | abe: 9.064 | eve: 10.581 | bob: 9.022Epoch   0:  73% | abe: 9.063 | eve: 10.583 | bob: 9.021Epoch   0:  74% | abe: 9.063 | eve: 10.585 | bob: 9.020Epoch   0:  75% | abe: 9.063 | eve: 10.586 | bob: 9.020Epoch   0:  75% | abe: 9.062 | eve: 10.588 | bob: 9.020Epoch   0:  76% | abe: 9.062 | eve: 10.590 | bob: 9.019Epoch   0:  77% | abe: 9.061 | eve: 10.592 | bob: 9.019Epoch   0:  78% | abe: 9.062 | eve: 10.594 | bob: 9.019Epoch   0:  78% | abe: 9.061 | eve: 10.595 | bob: 9.018Epoch   0:  79% | abe: 9.061 | eve: 10.597 | bob: 9.018Epoch   0:  80% | abe: 9.060 | eve: 10.598 | bob: 9.017Epoch   0:  81% | abe: 9.060 | eve: 10.599 | bob: 9.016Epoch   0:  82% | abe: 9.060 | eve: 10.601 | bob: 9.016Epoch   0:  82% | abe: 9.060 | eve: 10.602 | bob: 9.016Epoch   0:  83% | abe: 9.059 | eve: 10.604 | bob: 9.016Epoch   0:  84% | abe: 9.060 | eve: 10.605 | bob: 9.016Epoch   0:  85% | abe: 9.060 | eve: 10.606 | bob: 9.016Epoch   0:  85% | abe: 9.059 | eve: 10.607 | bob: 9.015Epoch   0:  86% | abe: 9.059 | eve: 10.609 | bob: 9.015Epoch   0:  87% | abe: 9.058 | eve: 10.611 | bob: 9.014Epoch   0:  88% | abe: 9.059 | eve: 10.612 | bob: 9.015Epoch   0:  89% | abe: 9.059 | eve: 10.614 | bob: 9.015Epoch   0:  89% | abe: 9.059 | eve: 10.615 | bob: 9.015Epoch   0:  90% | abe: 9.059 | eve: 10.616 | bob: 9.015Epoch   0:  91% | abe: 9.059 | eve: 10.617 | bob: 9.014Epoch   0:  92% | abe: 9.058 | eve: 10.618 | bob: 9.014Epoch   0:  92% | abe: 9.058 | eve: 10.618 | bob: 9.013Epoch   0:  93% | abe: 9.058 | eve: 10.619 | bob: 9.013Epoch   0:  94% | abe: 9.058 | eve: 10.621 | bob: 9.013Epoch   0:  95% | abe: 9.058 | eve: 10.621 | bob: 9.013Epoch   0:  96% | abe: 9.058 | eve: 10.622 | bob: 9.013Epoch   0:  96% | abe: 9.057 | eve: 10.622 | bob: 9.013Epoch   0:  97% | abe: 9.057 | eve: 10.624 | bob: 9.012Epoch   0:  98% | abe: 9.057 | eve: 10.625 | bob: 9.012Epoch   0:  99% | abe: 9.057 | eve: 10.626 | bob: 9.012
New best Bob loss 9.011658318014042 at epoch 0
Epoch   1:   0% | abe: 9.007 | eve: 10.791 | bob: 8.954Epoch   1:   0% | abe: 9.034 | eve: 10.744 | bob: 8.982Epoch   1:   1% | abe: 9.037 | eve: 10.768 | bob: 8.985Epoch   1:   2% | abe: 9.047 | eve: 10.762 | bob: 8.996Epoch   1:   3% | abe: 9.046 | eve: 10.758 | bob: 8.994Epoch   1:   3% | abe: 9.037 | eve: 10.761 | bob: 8.985Epoch   1:   4% | abe: 9.039 | eve: 10.749 | bob: 8.988Epoch   1:   5% | abe: 9.030 | eve: 10.746 | bob: 8.978Epoch   1:   6% | abe: 9.024 | eve: 10.741 | bob: 8.972Epoch   1:   7% | abe: 9.024 | eve: 10.731 | bob: 8.972Epoch   1:   7% | abe: 9.024 | eve: 10.729 | bob: 8.973Epoch   1:   8% | abe: 9.024 | eve: 10.732 | bob: 8.972Epoch   1:   9% | abe: 9.023 | eve: 10.728 | bob: 8.971Epoch   1:  10% | abe: 9.020 | eve: 10.725 | bob: 8.969Epoch   1:  10% | abe: 9.025 | eve: 10.726 | bob: 8.973Epoch   1:  11% | abe: 9.025 | eve: 10.728 | bob: 8.974Epoch   1:  12% | abe: 9.028 | eve: 10.729 | bob: 8.976Epoch   1:  13% | abe: 9.024 | eve: 10.728 | bob: 8.973Epoch   1:  14% | abe: 9.021 | eve: 10.725 | bob: 8.970Epoch   1:  14% | abe: 9.021 | eve: 10.726 | bob: 8.969Epoch   1:  15% | abe: 9.021 | eve: 10.725 | bob: 8.970Epoch   1:  16% | abe: 9.020 | eve: 10.725 | bob: 8.969Epoch   1:  17% | abe: 9.022 | eve: 10.725 | bob: 8.970Epoch   1:  17% | abe: 9.024 | eve: 10.726 | bob: 8.973Epoch   1:  18% | abe: 9.023 | eve: 10.727 | bob: 8.972Epoch   1:  19% | abe: 9.021 | eve: 10.726 | bob: 8.970Epoch   1:  20% | abe: 9.021 | eve: 10.725 | bob: 8.969Epoch   1:  21% | abe: 9.019 | eve: 10.724 | bob: 8.968Epoch   1:  21% | abe: 9.022 | eve: 10.725 | bob: 8.971Epoch   1:  22% | abe: 9.021 | eve: 10.726 | bob: 8.970Epoch   1:  23% | abe: 9.020 | eve: 10.727 | bob: 8.969Epoch   1:  24% | abe: 9.021 | eve: 10.725 | bob: 8.970Epoch   1:  25% | abe: 9.019 | eve: 10.724 | bob: 8.968Epoch   1:  25% | abe: 9.017 | eve: 10.726 | bob: 8.966Epoch   1:  26% | abe: 9.017 | eve: 10.728 | bob: 8.966Epoch   1:  27% | abe: 9.018 | eve: 10.728 | bob: 8.967Epoch   1:  28% | abe: 9.018 | eve: 10.729 | bob: 8.967Epoch   1:  28% | abe: 9.019 | eve: 10.729 | bob: 8.968Epoch   1:  29% | abe: 9.019 | eve: 10.729 | bob: 8.968Epoch   1:  30% | abe: 9.018 | eve: 10.730 | bob: 8.967Epoch   1:  31% | abe: 9.017 | eve: 10.732 | bob: 8.966Epoch   1:  32% | abe: 9.017 | eve: 10.731 | bob: 8.966Epoch   1:  32% | abe: 9.017 | eve: 10.734 | bob: 8.966Epoch   1:  33% | abe: 9.019 | eve: 10.732 | bob: 8.968Epoch   1:  34% | abe: 9.018 | eve: 10.732 | bob: 8.967Epoch   1:  35% | abe: 9.017 | eve: 10.732 | bob: 8.966Epoch   1:  35% | abe: 9.018 | eve: 10.732 | bob: 8.967Epoch   1:  36% | abe: 9.019 | eve: 10.731 | bob: 8.968Epoch   1:  37% | abe: 9.019 | eve: 10.730 | bob: 8.968Epoch   1:  38% | abe: 9.019 | eve: 10.730 | bob: 8.968Epoch   1:  39% | abe: 9.020 | eve: 10.731 | bob: 8.969Epoch   1:  39% | abe: 9.021 | eve: 10.731 | bob: 8.970Epoch   1:  40% | abe: 9.021 | eve: 10.731 | bob: 8.970Epoch   1:  41% | abe: 9.019 | eve: 10.730 | bob: 8.968Epoch   1:  42% | abe: 9.018 | eve: 10.730 | bob: 8.968Epoch   1:  42% | abe: 9.019 | eve: 10.729 | bob: 8.968Epoch   1:  43% | abe: 9.018 | eve: 10.728 | bob: 8.968Epoch   1:  44% | abe: 9.018 | eve: 10.727 | bob: 8.968Epoch   1:  45% | abe: 9.019 | eve: 10.727 | bob: 8.968Epoch   1:  46% | abe: 9.020 | eve: 10.728 | bob: 8.969Epoch   1:  46% | abe: 9.020 | eve: 10.728 | bob: 8.969Epoch   1:  47% | abe: 9.019 | eve: 10.729 | bob: 8.968Epoch   1:  48% | abe: 9.020 | eve: 10.730 | bob: 8.970Epoch   1:  49% | abe: 9.019 | eve: 10.729 | bob: 8.969Epoch   1:  50% | abe: 9.019 | eve: 10.730 | bob: 8.969Epoch   1:  50% | abe: 9.019 | eve: 10.730 | bob: 8.968Epoch   1:  51% | abe: 9.018 | eve: 10.729 | bob: 8.967Epoch   1:  52% | abe: 9.019 | eve: 10.729 | bob: 8.968Epoch   1:  53% | abe: 9.019 | eve: 10.728 | bob: 8.968Epoch   1:  53% | abe: 9.018 | eve: 10.728 | bob: 8.967Epoch   1:  54% | abe: 9.018 | eve: 10.728 | bob: 8.967Epoch   1:  55% | abe: 9.017 | eve: 10.727 | bob: 8.966Epoch   1:  56% | abe: 9.017 | eve: 10.727 | bob: 8.966Epoch   1:  57% | abe: 9.017 | eve: 10.727 | bob: 8.966Epoch   1:  57% | abe: 9.017 | eve: 10.726 | bob: 8.966Epoch   1:  58% | abe: 9.017 | eve: 10.726 | bob: 8.966Epoch   1:  59% | abe: 9.017 | eve: 10.726 | bob: 8.966Epoch   1:  60% | abe: 9.017 | eve: 10.725 | bob: 8.966Epoch   1:  60% | abe: 9.017 | eve: 10.723 | bob: 8.966Epoch   1:  61% | abe: 9.017 | eve: 10.724 | bob: 8.965Epoch   1:  62% | abe: 9.016 | eve: 10.724 | bob: 8.965Epoch   1:  63% | abe: 9.016 | eve: 10.723 | bob: 8.964Epoch   1:  64% | abe: 9.016 | eve: 10.723 | bob: 8.965Epoch   1:  64% | abe: 9.016 | eve: 10.722 | bob: 8.964Epoch   1:  65% | abe: 9.016 | eve: 10.722 | bob: 8.964Epoch   1:  66% | abe: 9.016 | eve: 10.721 | bob: 8.964Epoch   1:  67% | abe: 9.016 | eve: 10.720 | bob: 8.964Epoch   1:  67% | abe: 9.015 | eve: 10.721 | bob: 8.964Epoch   1:  68% | abe: 9.014 | eve: 10.721 | bob: 8.963Epoch   1:  69% | abe: 9.014 | eve: 10.721 | bob: 8.963Epoch   1:  70% | abe: 9.014 | eve: 10.721 | bob: 8.962Epoch   1:  71% | abe: 9.013 | eve: 10.720 | bob: 8.961Epoch   1:  71% | abe: 9.014 | eve: 10.720 | bob: 8.962Epoch   1:  72% | abe: 9.013 | eve: 10.720 | bob: 8.961Epoch   1:  73% | abe: 9.013 | eve: 10.720 | bob: 8.961Epoch   1:  74% | abe: 9.013 | eve: 10.721 | bob: 8.961Epoch   1:  75% | abe: 9.012 | eve: 10.720 | bob: 8.960Epoch   1:  75% | abe: 9.012 | eve: 10.719 | bob: 8.960Epoch   1:  76% | abe: 9.012 | eve: 10.719 | bob: 8.960Epoch   1:  77% | abe: 9.012 | eve: 10.720 | bob: 8.960Epoch   1:  78% | abe: 9.012 | eve: 10.719 | bob: 8.960Epoch   1:  78% | abe: 9.012 | eve: 10.719 | bob: 8.960Epoch   1:  79% | abe: 9.013 | eve: 10.719 | bob: 8.960