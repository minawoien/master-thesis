WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-14 21:48:55.959120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-14 21:48:56.030597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:85:00.0
2024-04-14 21:48:56.031356: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:48:56.034685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:48:56.037424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:48:56.038351: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:48:56.042179: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:48:56.044430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:48:56.052728: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:48:56.062303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:48:56.062697: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-14 21:48:56.080454: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-14 21:48:56.084243: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4ec53e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:48:56.084301: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-14 21:48:56.373104: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2933d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:48:56.373186: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-14 21:48:56.379734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:85:00.0
2024-04-14 21:48:56.379838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:48:56.379858: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:48:56.379873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:48:56.379888: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:48:56.379902: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:48:56.379916: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:48:56.379932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:48:56.384142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:48:56.384197: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:48:56.387027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-14 21:48:56.387047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-14 21:48:56.387053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-14 21:48:56.391396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:85:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-14 21:48:59.867734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9218 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4579 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1464 - val_loss: 3.9144e-04
Epoch 4/512
448/448 - 0s - loss: 0.0224 - val_loss: 4.0186e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5939e-06
Epoch 6/512
448/448 - 0s - loss: 7.4157e-04 - val_loss: 5.8671e-06
Epoch 7/512
448/448 - 0s - loss: 5.3891e-04 - val_loss: 4.4188e-06
Epoch 8/512
448/448 - 0s - loss: 3.9882e-04 - val_loss: 3.1194e-06
Epoch 9/512
448/448 - 0s - loss: 2.7489e-04 - val_loss: 2.0215e-06
Epoch 10/512
448/448 - 0s - loss: 1.7300e-04 - val_loss: 1.1775e-06
Epoch 11/512
448/448 - 0s - loss: 9.7203e-05 - val_loss: 5.9934e-07
Epoch 12/512
448/448 - 0s - loss: 4.7358e-05 - val_loss: 2.5760e-07
Epoch 13/512
448/448 - 0s - loss: 1.9300e-05 - val_loss: 8.9355e-08
Epoch 14/512
448/448 - 0s - loss: 6.2792e-06 - val_loss: 2.3597e-08
Epoch 15/512
448/448 - 0s - loss: 1.5759e-06 - val_loss: 9.5786e-09
Epoch 16/512
448/448 - 0s - loss: 1.0995e-05 - val_loss: 3.0532e-06
Epoch 17/512
448/448 - 0s - loss: 0.0033 - val_loss: 3.3839e-05
Epoch 18/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.3829e-06
Epoch 19/512
448/448 - 0s - loss: 1.1087e-04 - val_loss: 1.4000e-06
Epoch 20/512
448/448 - 0s - loss: 3.2693e-04 - val_loss: 1.6013e-05
Epoch 21/512
448/448 - 0s - loss: 0.0025 - val_loss: 1.7476e-05
Epoch 22/512
448/448 - 0s - loss: 0.0011 - val_loss: 4.2623e-06
Epoch 23/512
448/448 - 0s - loss: 4.4037e-04 - val_loss: 7.5774e-06
Epoch 24/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.9690e-05
Epoch 25/512
448/448 - 0s - loss: 0.0016 - val_loss: 9.0677e-06
Epoch 26/512
448/448 - 0s - loss: 7.5109e-04 - val_loss: 7.1299e-06
Epoch 27/512
448/448 - 0s - loss: 8.5073e-04 - val_loss: 1.3057e-05
Epoch 28/512
448/448 - 0s - loss: 0.0013 - val_loss: 1.2218e-05
Epoch 29/512
448/448 - 0s - loss: 0.0010 - val_loss: 8.0987e-06
Epoch 30/512
448/448 - 0s - loss: 8.1616e-04 - val_loss: 9.8370e-06
Epoch 31/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1994e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 9.3161e-06
Epoch 33/512
448/448 - 0s - loss: 8.7728e-04 - val_loss: 8.7981e-06
Epoch 34/512
448/448 - 0s - loss: 9.1253e-04 - val_loss: 1.0696e-05
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.0136e-05
Epoch 36/512
448/448 - 0s - loss: 9.3256e-04 - val_loss: 8.7123e-06
Epoch 37/512
448/448 - 0s - loss: 8.6170e-04 - val_loss: 9.2357e-06
Epoch 38/512
448/448 - 0s - loss: 9.3393e-04 - val_loss: 9.7559e-06
Epoch 39/512
448/448 - 0s - loss: 9.3151e-04 - val_loss: 8.9281e-06
Epoch 40/512
448/448 - 0s - loss: 8.5722e-04 - val_loss: 8.8757e-06
Epoch 41/512
448/448 - 0s - loss: 8.7872e-04 - val_loss: 9.1417e-06
Epoch 42/512
448/448 - 0s - loss: 8.8112e-04 - val_loss: 8.8481e-06
Epoch 43/512
448/448 - 0s - loss: 8.5469e-04 - val_loss: 8.4238e-06
Epoch 44/512
448/448 - 0s - loss: 8.3156e-04 - val_loss: 8.4683e-06
Epoch 45/512
448/448 - 0s - loss: 8.4668e-04 - val_loss: 8.3085e-06
Epoch 46/512
448/448 - 0s - loss: 8.1458e-04 - val_loss: 8.3022e-06
Epoch 47/512
448/448 - 0s - loss: 8.1636e-04 - val_loss: 8.4063e-06
Epoch 48/512
448/448 - 0s - loss: 8.1994e-04 - val_loss: 7.9642e-06
Epoch 49/512
448/448 - 0s - loss: 7.7780e-04 - val_loss: 7.9719e-06
Epoch 50/512
448/448 - 0s - loss: 7.9454e-04 - val_loss: 7.9586e-06
Epoch 51/512
448/448 - 0s - loss: 7.7208e-04 - val_loss: 8.0384e-06
Epoch 52/512
448/448 - 0s - loss: 7.8276e-04 - val_loss: 7.7873e-06
Epoch 53/512
448/448 - 0s - loss: 7.5613e-04 - val_loss: 7.5366e-06
Epoch 54/512
448/448 - 0s - loss: 7.4509e-04 - val_loss: 7.5774e-06
Epoch 55/512
448/448 - 0s - loss: 7.5024e-04 - val_loss: 7.4348e-06
Epoch 56/512
448/448 - 0s - loss: 7.3367e-04 - val_loss: 7.3265e-06
Epoch 57/512
448/448 - 0s - loss: 7.2673e-04 - val_loss: 7.2716e-06
Epoch 58/512
448/448 - 0s - loss: 7.1946e-04 - val_loss: 7.2668e-06
Epoch 59/512
448/448 - 0s - loss: 7.1636e-04 - val_loss: 7.1505e-06
Epoch 60/512
448/448 - 0s - loss: 7.0402e-04 - val_loss: 6.9892e-06
Epoch 61/512
448/448 - 0s - loss: 6.9580e-04 - val_loss: 6.8488e-06
Epoch 62/512
448/448 - 0s - loss: 6.7996e-04 - val_loss: 7.0191e-06
Epoch 63/512
448/448 - 0s - loss: 6.9476e-04 - val_loss: 6.8761e-06
Epoch 64/512
448/448 - 0s - loss: 6.7590e-04 - val_loss: 6.5933e-06
Epoch 65/512
448/448 - 0s - loss: 6.5751e-04 - val_loss: 6.6401e-06
Epoch 66/512
448/448 - 0s - loss: 6.6419e-04 - val_loss: 6.6418e-06
Epoch 67/512
448/448 - 0s - loss: 6.5688e-04 - val_loss: 6.5255e-06
Epoch 68/512
448/448 - 0s - loss: 6.4587e-04 - val_loss: 6.3770e-06
Epoch 69/512
448/448 - 0s - loss: 6.3556e-04 - val_loss: 6.4956e-06
Epoch 70/512
448/448 - 0s - loss: 6.4433e-04 - val_loss: 6.3651e-06
Epoch 71/512
448/448 - 0s - loss: 6.2400e-04 - val_loss: 6.2479e-06
Epoch 72/512
448/448 - 0s - loss: 6.2050e-04 - val_loss: 6.2718e-06
Epoch 73/512
448/448 - 0s - loss: 6.1734e-04 - val_loss: 6.2535e-06
Epoch 74/512
448/448 - 0s - loss: 6.1635e-04 - val_loss: 6.0348e-06
Epoch 75/512
448/448 - 0s - loss: 5.9713e-04 - val_loss: 6.0346e-06
Epoch 76/512
448/448 - 0s - loss: 6.0003e-04 - val_loss: 6.0613e-06
Epoch 77/512
448/448 - 0s - loss: 5.9946e-04 - val_loss: 5.8204e-06
Epoch 78/512
448/448 - 0s - loss: 5.7689e-04 - val_loss: 5.8023e-06
Epoch 79/512
448/448 - 0s - loss: 5.8071e-04 - val_loss: 5.9388e-06
Epoch 80/512
448/448 - 0s - loss: 5.8862e-04 - val_loss: 5.7008e-06
Epoch 81/512
448/448 - 0s - loss: 5.5843e-04 - val_loss: 5.7311e-06
Epoch 82/512
448/448 - 0s - loss: 5.7176e-04 - val_loss: 5.6836e-06
Epoch 83/512
448/448 - 0s - loss: 5.5662e-04 - val_loss: 5.6584e-06
Epoch 84/512
448/448 - 0s - loss: 5.5775e-04 - val_loss: 5.5378e-06
Epoch 85/512
448/448 - 0s - loss: 5.4811e-04 - val_loss: 5.4449e-06
Epoch 86/512
448/448 - 0s - loss: 5.4564e-04 - val_loss: 5.3494e-06
Epoch 87/512
448/448 - 0s - loss: 5.2963e-04 - val_loss: 5.5682e-06
Epoch 88/512
448/448 - 0s - loss: 5.4880e-04 - val_loss: 5.5282e-06
Epoch 89/512
448/448 - 0s - loss: 5.3863e-04 - val_loss: 5.0757e-06
Epoch 90/512
448/448 - 0s - loss: 5.0623e-04 - val_loss: 5.2020e-06
Epoch 91/512
448/448 - 0s - loss: 5.2736e-04 - val_loss: 5.3177e-06
Epoch 92/512
448/448 - 0s - loss: 5.2269e-04 - val_loss: 5.1103e-06
Epoch 93/512
448/448 - 0s - loss: 5.0455e-04 - val_loss: 5.0560e-06
Epoch 94/512
448/448 - 0s - loss: 5.0408e-04 - val_loss: 5.1555e-06
Epoch 95/512
448/448 - 0s - loss: 5.1027e-04 - val_loss: 4.9449e-06
Epoch 96/512
448/448 - 0s - loss: 4.9045e-04 - val_loss: 4.9011e-06
Epoch 97/512
448/448 - 0s - loss: 4.8859e-04 - val_loss: 5.1229e-06
Epoch 98/512
448/448 - 0s - loss: 5.0476e-04 - val_loss: 4.8772e-06
Epoch 99/512
448/448 - 0s - loss: 4.7673e-04 - val_loss: 4.7375e-06
Epoch 100/512
448/448 - 0s - loss: 4.7734e-04 - val_loss: 4.7474e-06
Epoch 101/512
448/448 - 0s - loss: 4.7830e-04 - val_loss: 4.6974e-06
Epoch 102/512
448/448 - 0s - loss: 4.7267e-04 - val_loss: 4.5823e-06
Epoch 103/512
448/448 - 0s - loss: 4.5930e-04 - val_loss: 4.7278e-06
Epoch 104/512
448/448 - 0s - loss: 4.7400e-04 - val_loss: 4.5921e-06
Epoch 105/512
448/448 - 0s - loss: 4.5251e-04 - val_loss: 4.5712e-06
Epoch 106/512
448/448 - 0s - loss: 4.5797e-04 - val_loss: 4.5062e-06
Epoch 107/512
448/448 - 0s - loss: 4.4771e-04 - val_loss: 4.5134e-06
Epoch 108/512
448/448 - 0s - loss: 4.5080e-04 - val_loss: 4.4389e-06
Epoch 109/512
448/448 - 0s - loss: 4.3949e-04 - val_loss: 4.4315e-06
Epoch 110/512
448/448 - 0s - loss: 4.3935e-04 - val_loss: 4.4661e-06
Epoch 111/512
448/448 - 0s - loss: 4.4046e-04 - val_loss: 4.3003e-06
Epoch 112/512
448/448 - 0s - loss: 4.2472e-04 - val_loss: 4.2848e-06
Epoch 113/512
448/448 - 0s - loss: 4.2786e-04 - val_loss: 4.2964e-06
Epoch 114/512
448/448 - 0s - loss: 4.2598e-04 - val_loss: 4.2055e-06
Epoch 115/512
448/448 - 0s - loss: 4.1729e-04 - val_loss: 4.1391e-06
Epoch 116/512
448/448 - 0s - loss: 4.1504e-04 - val_loss: 4.0979e-06
Epoch 117/512
448/448 - 0s - loss: 4.1359e-04 - val_loss: 4.0350e-06
Epoch 118/512
448/448 - 0s - loss: 4.0783e-04 - val_loss: 3.9921e-06
Epoch 119/512
448/448 - 0s - loss: 3.9921e-04 - val_loss: 4.1023e-06
Epoch 120/512
448/448 - 0s - loss: 4.0656e-04 - val_loss: 4.1048e-06
Epoch 121/512
448/448 - 0s - loss: 4.0252e-04 - val_loss: 3.8999e-06
Epoch 122/512
448/448 - 0s - loss: 3.8651e-04 - val_loss: 3.8890e-06
Epoch 123/512
448/448 - 0s - loss: 3.8989e-04 - val_loss: 3.9309e-06
Epoch 124/512
448/448 - 0s - loss: 3.9132e-04 - val_loss: 3.8764e-06
Epoch 125/512
448/448 - 0s - loss: 3.7980e-04 - val_loss: 3.8196e-06
Epoch 126/512
448/448 - 0s - loss: 3.7925e-04 - val_loss: 3.8161e-06
Epoch 127/512
448/448 - 0s - loss: 3.7637e-04 - val_loss: 3.7784e-06
Epoch 128/512
448/448 - 0s - loss: 3.7399e-04 - val_loss: 3.6801e-06
Epoch 129/512
448/448 - 0s - loss: 3.6455e-04 - val_loss: 3.7278e-06
Epoch 130/512
448/448 - 0s - loss: 3.7022e-04 - val_loss: 3.6973e-06
Epoch 131/512
448/448 - 0s - loss: 3.6326e-04 - val_loss: 3.5915e-06
Epoch 132/512
448/448 - 0s - loss: 3.5765e-04 - val_loss: 3.5007e-06
Epoch 133/512
448/448 - 0s - loss: 3.4857e-04 - val_loss: 3.6619e-06
Epoch 134/512
448/448 - 0s - loss: 3.6115e-04 - val_loss: 3.6032e-06
Epoch 135/512
448/448 - 0s - loss: 3.5377e-04 - val_loss: 3.2966e-06
Epoch 136/512
448/448 - 0s - loss: 3.2962e-04 - val_loss: 3.4599e-06
Epoch 137/512
448/448 - 0s - loss: 3.5117e-04 - val_loss: 3.5458e-06
Epoch 138/512
448/448 - 0s - loss: 3.4592e-04 - val_loss: 3.3401e-06
Epoch 139/512
448/448 - 0s - loss: 3.2927e-04 - val_loss: 3.2710e-06
Epoch 140/512
448/448 - 0s - loss: 3.2913e-04 - val_loss: 3.3978e-06
Epoch 141/512
448/448 - 0s - loss: 3.3965e-04 - val_loss: 3.2535e-06
Epoch 142/512
448/448 - 0s - loss: 3.2146e-04 - val_loss: 3.1815e-06
Epoch 143/512
448/448 - 0s - loss: 3.2129e-04 - val_loss: 3.2312e-06
Epoch 144/512
448/448 - 0s - loss: 3.2400e-04 - val_loss: 3.1775e-06
Epoch 145/512
448/448 - 0s - loss: 3.1710e-04 - val_loss: 3.0979e-06
Epoch 146/512
448/448 - 0s - loss: 3.1033e-04 - val_loss: 3.1187e-06
Epoch 147/512
448/448 - 0s - loss: 3.1422e-04 - val_loss: 3.0825e-06
Epoch 148/512
448/448 - 0s - loss: 3.0598e-04 - val_loss: 3.0867e-06
Epoch 149/512
448/448 - 0s - loss: 3.0865e-04 - val_loss: 3.0235e-06
Epoch 150/512
448/448 - 0s - loss: 3.0031e-04 - val_loss: 2.9892e-06
Epoch 151/512
448/448 - 0s - loss: 3.0110e-04 - val_loss: 2.9362e-06
Epoch 152/512
448/448 - 0s - loss: 2.9474e-04 - val_loss: 2.9061e-06
Epoch 153/512
448/448 - 0s - loss: 2.9332e-04 - val_loss: 2.9177e-06
Epoch 154/512
448/448 - 0s - loss: 2.9233e-04 - val_loss: 2.8715e-06
Epoch 155/512
448/448 - 0s - loss: 2.8647e-04 - val_loss: 2.8449e-06
Epoch 156/512
448/448 - 0s - loss: 2.8526e-04 - val_loss: 2.8373e-06
Epoch 157/512
448/448 - 0s - loss: 2.8253e-04 - val_loss: 2.8259e-06
Epoch 158/512
448/448 - 0s - loss: 2.7906e-04 - val_loss: 2.7954e-06
Epoch 159/512
448/448 - 0s - loss: 2.7699e-04 - val_loss: 2.7326e-06
Epoch 160/512
448/448 - 0s - loss: 2.7249e-04 - val_loss: 2.7053e-06
Epoch 161/512
448/448 - 0s - loss: 2.6869e-04 - val_loss: 2.7518e-06
Epoch 162/512
448/448 - 0s - loss: 2.7302e-04 - val_loss: 2.6467e-06
Epoch 163/512
448/448 - 0s - loss: 2.6065e-04 - val_loss: 2.6517e-06
Epoch 164/512
448/448 - 0s - loss: 2.6515e-04 - val_loss: 2.6198e-06
Epoch 165/512
448/448 - 0s - loss: 2.6020e-04 - val_loss: 2.5438e-06
Epoch 166/512
448/448 - 0s - loss: 2.5402e-04 - val_loss: 2.5657e-06
Epoch 167/512
448/448 - 0s - loss: 2.5605e-04 - val_loss: 2.5525e-06
Epoch 168/512
448/448 - 0s - loss: 2.5585e-04 - val_loss: 2.3951e-06
Epoch 169/512
448/448 - 0s - loss: 2.4032e-04 - val_loss: 2.4660e-06
Epoch 170/512
448/448 - 0s - loss: 2.5119e-04 - val_loss: 2.5047e-06
Epoch 171/512
448/448 - 0s - loss: 2.4820e-04 - val_loss: 2.3358e-06
Epoch 172/512
448/448 - 0s - loss: 2.3353e-04 - val_loss: 2.3508e-06
Epoch 173/512
448/448 - 0s - loss: 2.3924e-04 - val_loss: 2.4301e-06
Epoch 174/512
448/448 - 0s - loss: 2.4094e-04 - val_loss: 2.3360e-06
Epoch 175/512
448/448 - 0s - loss: 2.3059e-04 - val_loss: 2.2680e-06
Epoch 176/512
448/448 - 0s - loss: 2.3007e-04 - val_loss: 2.2807e-06
Epoch 177/512
448/448 - 0s - loss: 2.2830e-04 - val_loss: 2.3231e-06
Epoch 178/512
448/448 - 0s - loss: 2.3006e-04 - val_loss: 2.2291e-06
Epoch 179/512
448/448 - 0s - loss: 2.2052e-04 - val_loss: 2.1809e-06
Epoch 180/512
448/448 - 0s - loss: 2.1907e-04 - val_loss: 2.2216e-06
Epoch 181/512
448/448 - 0s - loss: 2.2194e-04 - val_loss: 2.1514e-06
Epoch 182/512
448/448 - 0s - loss: 2.1520e-04 - val_loss: 2.0816e-06
Epoch 183/512
448/448 - 0s - loss: 2.0995e-04 - val_loss: 2.1274e-06
Epoch 184/512
448/448 - 0s - loss: 2.1429e-04 - val_loss: 2.1097e-06
Epoch 185/512
448/448 - 0s - loss: 2.0861e-04 - val_loss: 2.0502e-06
Epoch 186/512
448/448 - 0s - loss: 2.0464e-04 - val_loss: 2.0414e-06
Epoch 187/512
448/448 - 0s - loss: 2.0653e-04 - val_loss: 1.9610e-06
Epoch 188/512
448/448 - 0s - loss: 1.9657e-04 - val_loss: 1.9817e-06
Epoch 189/512
448/448 - 0s - loss: 2.0154e-04 - val_loss: 1.9754e-06
Epoch 190/512
448/448 - 0s - loss: 1.9717e-04 - val_loss: 1.9259e-06
Epoch 191/512
448/448 - 0s - loss: 1.9346e-04 - val_loss: 1.8902e-06
Epoch 192/512
448/448 - 0s - loss: 1.9073e-04 - val_loss: 1.9056e-06
Epoch 193/512
448/448 - 0s - loss: 1.9102e-04 - val_loss: 1.8710e-06
Epoch 194/512
448/448 - 0s - loss: 1.8625e-04 - val_loss: 1.8671e-06
Epoch 195/512
448/448 - 0s - loss: 1.8619e-04 - val_loss: 1.8370e-06
Epoch 196/512
448/448 - 0s - loss: 1.8182e-04 - val_loss: 1.8052e-06
Epoch 197/512
448/448 - 0s - loss: 1.8144e-04 - val_loss: 1.7428e-06
Epoch 198/512
448/448 - 0s - loss: 1.7536e-04 - val_loss: 1.7411e-06
Epoch 199/512
448/448 - 0s - loss: 1.7628e-04 - val_loss: 1.7570e-06
Epoch 200/512
448/448 - 0s - loss: 1.7584e-04 - val_loss: 1.7163e-06
Epoch 201/512
448/448 - 0s - loss: 1.7056e-04 - val_loss: 1.6688e-06
Epoch 202/512
448/448 - 0s - loss: 1.6773e-04 - val_loss: 1.6820e-06
Epoch 203/512
448/448 - 0s - loss: 1.6854e-04 - val_loss: 1.6548e-06
Epoch 204/512
448/448 - 0s - loss: 1.6454e-04 - val_loss: 1.6111e-06
Epoch 205/512
448/448 - 0s - loss: 1.6177e-04 - val_loss: 1.5724e-06
Epoch 206/512
448/448 - 0s - loss: 1.5925e-04 - val_loss: 1.5635e-06
Epoch 207/512
448/448 - 0s - loss: 1.5918e-04 - val_loss: 1.5208e-06
Epoch 208/512
448/448 - 0s - loss: 1.5382e-04 - val_loss: 1.5239e-06
Epoch 209/512
448/448 - 0s - loss: 1.5450e-04 - val_loss: 1.5183e-06
Epoch 210/512
448/448 - 0s - loss: 1.5147e-04 - val_loss: 1.5046e-06
Epoch 211/512
448/448 - 0s - loss: 1.5036e-04 - val_loss: 1.4673e-06
Epoch 212/512
448/448 - 0s - loss: 1.4628e-04 - val_loss: 1.4380e-06
Epoch 213/512
448/448 - 0s - loss: 1.4489e-04 - val_loss: 1.4405e-06
Epoch 214/512
448/448 - 0s - loss: 1.4376e-04 - val_loss: 1.4270e-06
Epoch 215/512
448/448 - 0s - loss: 1.4095e-04 - val_loss: 1.4106e-06
Epoch 216/512
448/448 - 0s - loss: 1.4140e-04 - val_loss: 1.3386e-06
Epoch 217/512
448/448 - 0s - loss: 1.3421e-04 - val_loss: 1.3215e-06
Epoch 218/512
448/448 - 0s - loss: 1.3393e-04 - val_loss: 1.3607e-06
Epoch 219/512
448/448 - 0s - loss: 1.3635e-04 - val_loss: 1.2985e-06
Epoch 220/512
448/448 - 0s - loss: 1.2899e-04 - val_loss: 1.2594e-06
Epoch 221/512
448/448 - 0s - loss: 1.2758e-04 - val_loss: 1.2680e-06
Epoch 222/512
448/448 - 0s - loss: 1.2730e-04 - val_loss: 1.2695e-06
Epoch 223/512
448/448 - 0s - loss: 1.2763e-04 - val_loss: 1.1729e-06
Epoch 224/512
448/448 - 0s - loss: 1.1845e-04 - val_loss: 1.1790e-06
Epoch 225/512
448/448 - 0s - loss: 1.2068e-04 - val_loss: 1.2293e-06
Epoch 226/512
448/448 - 0s - loss: 1.2190e-04 - val_loss: 1.2002e-06
Epoch 227/512
448/448 - 0s - loss: 1.1811e-04 - val_loss: 1.0974e-06
Epoch 228/512
448/448 - 0s - loss: 1.1117e-04 - val_loss: 1.0935e-06
Epoch 229/512
448/448 - 0s - loss: 1.1203e-04 - val_loss: 1.1559e-06
Epoch 230/512
448/448 - 0s - loss: 1.1602e-04 - val_loss: 1.0798e-06
Epoch 231/512
448/448 - 0s - loss: 1.0702e-04 - val_loss: 1.0198e-06
Epoch 232/512
448/448 - 0s - loss: 1.0462e-04 - val_loss: 1.0798e-06
Epoch 233/512
448/448 - 0s - loss: 1.0934e-04 - val_loss: 1.0444e-06
Epoch 234/512
448/448 - 0s - loss: 1.0344e-04 - val_loss: 9.7025e-07
Epoch 235/512
448/448 - 0s - loss: 9.8700e-05 - val_loss: 9.8761e-07
Epoch 236/512
448/448 - 0s - loss: 1.0117e-04 - val_loss: 9.9607e-07
Epoch 237/512
448/448 - 0s - loss: 9.9456e-05 - val_loss: 9.4320e-07
Epoch 238/512
448/448 - 0s - loss: 9.4436e-05 - val_loss: 9.3002e-07
Epoch 239/512
448/448 - 0s - loss: 9.4466e-05 - val_loss: 9.4532e-07
Epoch 240/512
448/448 - 0s - loss: 9.4756e-05 - val_loss: 9.0923e-07
Epoch 241/512
448/448 - 0s - loss: 9.0400e-05 - val_loss: 8.6502e-07
Epoch 242/512
448/448 - 0s - loss: 8.7475e-05 - val_loss: 8.7604e-07
Epoch 243/512
448/448 - 0s - loss: 8.8708e-05 - val_loss: 8.7859e-07
Epoch 244/512
448/448 - 0s - loss: 8.7567e-05 - val_loss: 8.2449e-07
Epoch 245/512
448/448 - 0s - loss: 8.2500e-05 - val_loss: 8.1069e-07
Epoch 246/512
448/448 - 0s - loss: 8.2903e-05 - val_loss: 8.0822e-07
Epoch 247/512
448/448 - 0s - loss: 8.1032e-05 - val_loss: 8.0057e-07
Epoch 248/512
448/448 - 0s - loss: 8.0578e-05 - val_loss: 7.5643e-07
Epoch 249/512
448/448 - 0s - loss: 7.6112e-05 - val_loss: 7.5861e-07
Epoch 250/512
448/448 - 0s - loss: 7.6718e-05 - val_loss: 7.5726e-07
Epoch 251/512
448/448 - 0s - loss: 7.5522e-05 - val_loss: 7.1045e-07
Epoch 252/512
448/448 - 0s - loss: 7.1799e-05 - val_loss: 6.9175e-07
Epoch 253/512
448/448 - 0s - loss: 7.0902e-05 - val_loss: 6.9985e-07
Epoch 254/512
448/448 - 0s - loss: 7.1225e-05 - val_loss: 6.6952e-07
Epoch 255/512
448/448 - 0s - loss: 6.7352e-05 - val_loss: 6.5601e-07
Epoch 256/512
448/448 - 0s - loss: 6.6780e-05 - val_loss: 6.5717e-07
Epoch 257/512
448/448 - 0s - loss: 6.6264e-05 - val_loss: 6.3977e-07
Epoch 258/512
448/448 - 0s - loss: 6.4118e-05 - val_loss: 6.0483e-07
Epoch 259/512
448/448 - 0s - loss: 6.0892e-05 - val_loss: 6.2127e-07
Epoch 260/512
448/448 - 0s - loss: 6.2918e-05 - val_loss: 6.0049e-07
Epoch 261/512
448/448 - 0s - loss: 5.9886e-05 - val_loss: 5.5232e-07
Epoch 262/512
448/448 - 0s - loss: 5.5699e-05 - val_loss: 5.8577e-07
Epoch 263/512
448/448 - 0s - loss: 5.9985e-05 - val_loss: 5.6304e-07
Epoch 264/512
448/448 - 0s - loss: 5.5717e-05 - val_loss: 5.0642e-07
Epoch 265/512
448/448 - 0s - loss: 5.1640e-05 - val_loss: 5.2612e-07
Epoch 266/512
448/448 - 0s - loss: 5.4497e-05 - val_loss: 5.3600e-07
Epoch 267/512
448/448 - 0s - loss: 5.3188e-05 - val_loss: 4.8541e-07
Epoch 268/512
448/448 - 0s - loss: 4.8411e-05 - val_loss: 4.8781e-07
Epoch 269/512
448/448 - 0s - loss: 5.0246e-05 - val_loss: 4.8671e-07
Epoch 270/512
448/448 - 0s - loss: 4.8558e-05 - val_loss: 4.6005e-07
Epoch 271/512
448/448 - 0s - loss: 4.6502e-05 - val_loss: 4.4488e-07
Epoch 272/512
448/448 - 0s - loss: 4.5497e-05 - val_loss: 4.4341e-07
Epoch 273/512
448/448 - 0s - loss: 4.5116e-05 - val_loss: 4.2457e-07
Epoch 274/512
448/448 - 0s - loss: 4.2611e-05 - val_loss: 4.2747e-07
Epoch 275/512
448/448 - 0s - loss: 4.3149e-05 - val_loss: 4.1737e-07
Epoch 276/512
448/448 - 0s - loss: 4.1614e-05 - val_loss: 3.8765e-07
Epoch 277/512
448/448 - 0s - loss: 3.9136e-05 - val_loss: 3.8014e-07
Epoch 278/512
448/448 - 0s - loss: 3.8637e-05 - val_loss: 3.9885e-07
Epoch 279/512
448/448 - 0s - loss: 4.0106e-05 - val_loss: 3.5820e-07
Epoch 280/512
448/448 - 0s - loss: 3.5529e-05 - val_loss: 3.4028e-07
Epoch 281/512
448/448 - 0s - loss: 3.5266e-05 - val_loss: 3.5433e-07
Epoch 282/512
448/448 - 0s - loss: 3.6126e-05 - val_loss: 3.4527e-07
Epoch 283/512
448/448 - 0s - loss: 3.4071e-05 - val_loss: 3.2612e-07
Epoch 284/512
448/448 - 0s - loss: 3.2804e-05 - val_loss: 3.1503e-07
Epoch 285/512
448/448 - 0s - loss: 3.2046e-05 - val_loss: 3.0915e-07
Epoch 286/512
448/448 - 0s - loss: 3.1451e-05 - val_loss: 3.0220e-07
Epoch 287/512
448/448 - 0s - loss: 3.0463e-05 - val_loss: 2.9113e-07
Epoch 288/512
448/448 - 0s - loss: 2.9410e-05 - val_loss: 2.8712e-07
Epoch 289/512
448/448 - 0s - loss: 2.9000e-05 - val_loss: 2.7212e-07
Epoch 290/512
448/448 - 0s - loss: 2.7397e-05 - val_loss: 2.6902e-07
Epoch 291/512
448/448 - 0s - loss: 2.7174e-05 - val_loss: 2.6612e-07
Epoch 292/512
448/448 - 0s - loss: 2.6565e-05 - val_loss: 2.5763e-07
Epoch 293/512
448/448 - 0s - loss: 2.5697e-05 - val_loss: 2.3746e-07
Epoch 294/512
448/448 - 0s - loss: 2.4115e-05 - val_loss: 2.3173e-07
Epoch 295/512
448/448 - 0s - loss: 2.4010e-05 - val_loss: 2.3002e-07
Epoch 296/512
448/448 - 0s - loss: 2.3393e-05 - val_loss: 2.2489e-07
Epoch 297/512
448/448 - 0s - loss: 2.2585e-05 - val_loss: 2.1833e-07
Epoch 298/512
448/448 - 0s - loss: 2.2117e-05 - val_loss: 2.0435e-07
Epoch 299/512
448/448 - 0s - loss: 2.0650e-05 - val_loss: 2.0340e-07
Epoch 300/512
448/448 - 0s - loss: 2.0805e-05 - val_loss: 1.9834e-07
Epoch 301/512
448/448 - 0s - loss: 2.0124e-05 - val_loss: 1.8207e-07
Epoch 302/512
448/448 - 0s - loss: 1.8541e-05 - val_loss: 1.8597e-07
Epoch 303/512
448/448 - 0s - loss: 1.9111e-05 - val_loss: 1.8302e-07
Epoch 304/512
448/448 - 0s - loss: 1.8218e-05 - val_loss: 1.6855e-07
Epoch 305/512
448/448 - 0s - loss: 1.7087e-05 - val_loss: 1.6397e-07
Epoch 306/512
448/448 - 0s - loss: 1.7013e-05 - val_loss: 1.5999e-07
Epoch 307/512
448/448 - 0s - loss: 1.6302e-05 - val_loss: 1.5754e-07
Epoch 308/512
448/448 - 0s - loss: 1.5909e-05 - val_loss: 1.5286e-07
Epoch 309/512
448/448 - 0s - loss: 1.5261e-05 - val_loss: 1.4830e-07
Epoch 310/512
448/448 - 0s - loss: 1.4863e-05 - val_loss: 1.4166e-07
Epoch 311/512
448/448 - 0s - loss: 1.4312e-05 - val_loss: 1.3386e-07
Epoch 312/512
448/448 - 0s - loss: 1.3703e-05 - val_loss: 1.2805e-07
Epoch 313/512
448/448 - 0s - loss: 1.3154e-05 - val_loss: 1.2961e-07
Epoch 314/512
448/448 - 0s - loss: 1.3248e-05 - val_loss: 1.2379e-07
Epoch 315/512
448/448 - 0s - loss: 1.2401e-05 - val_loss: 1.1526e-07
Epoch 316/512
448/448 - 0s - loss: 1.1761e-05 - val_loss: 1.1501e-07
Epoch 317/512
448/448 - 0s - loss: 1.1785e-05 - val_loss: 1.1243e-07
Epoch 318/512
448/448 - 0s - loss: 1.1361e-05 - val_loss: 1.0407e-07
Epoch 319/512
448/448 - 0s - loss: 1.0637e-05 - val_loss: 9.9576e-08
Epoch 320/512
448/448 - 0s - loss: 1.0298e-05 - val_loss: 1.0102e-07
Epoch 321/512
448/448 - 0s - loss: 1.0279e-05 - val_loss: 9.7854e-08
Epoch 322/512
448/448 - 0s - loss: 9.8255e-06 - val_loss: 8.8466e-08
Epoch 323/512
448/448 - 0s - loss: 9.0582e-06 - val_loss: 8.7514e-08
Epoch 324/512
448/448 - 0s - loss: 9.1124e-06 - val_loss: 8.7112e-08
Epoch 325/512
448/448 - 0s - loss: 8.9329e-06 - val_loss: 7.9432e-08
Epoch 326/512
448/448 - 0s - loss: 8.0326e-06 - val_loss: 7.9995e-08
Epoch 327/512
448/448 - 0s - loss: 8.2574e-06 - val_loss: 7.9022e-08
Epoch 328/512
448/448 - 0s - loss: 7.9261e-06 - val_loss: 7.2808e-08
Epoch 329/512
448/448 - 0s - loss: 7.3930e-06 - val_loss: 6.8047e-08
Epoch 330/512
448/448 - 0s - loss: 7.0664e-06 - val_loss: 6.9248e-08
Epoch 331/512
448/448 - 0s - loss: 7.1065e-06 - val_loss: 6.7746e-08
Epoch 332/512
448/448 - 0s - loss: 6.7977e-06 - val_loss: 6.2719e-08
Epoch 333/512
448/448 - 0s - loss: 6.3086e-06 - val_loss: 6.1571e-08
Epoch 334/512
448/448 - 0s - loss: 6.2899e-06 - val_loss: 5.9665e-08
Epoch 335/512
448/448 - 0s - loss: 6.0514e-06 - val_loss: 5.5644e-08
Epoch 336/512
448/448 - 0s - loss: 5.6557e-06 - val_loss: 5.3637e-08
Epoch 337/512
448/448 - 0s - loss: 5.5357e-06 - val_loss: 5.2389e-08
Epoch 338/512
448/448 - 0s - loss: 5.3602e-06 - val_loss: 4.9835e-08
Epoch 339/512
448/448 - 0s - loss: 5.0621e-06 - val_loss: 4.8853e-08
Epoch 340/512
448/448 - 0s - loss: 5.0005e-06 - val_loss: 4.5504e-08
Epoch 341/512
448/448 - 0s - loss: 4.6149e-06 - val_loss: 4.5108e-08
Epoch 342/512
448/448 - 0s - loss: 4.6430e-06 - val_loss: 4.3475e-08
Epoch 343/512
448/448 - 0s - loss: 4.3559e-06 - val_loss: 4.1470e-08
Epoch 344/512
448/448 - 0s - loss: 4.2110e-06 - val_loss: 3.8954e-08
Epoch 345/512
448/448 - 0s - loss: 4.0032e-06 - val_loss: 3.7088e-08
Epoch 346/512
448/448 - 0s - loss: 3.8109e-06 - val_loss: 3.7088e-08
Epoch 347/512
448/448 - 0s - loss: 3.7930e-06 - val_loss: 3.5682e-08
Epoch 348/512
448/448 - 0s - loss: 3.6215e-06 - val_loss: 3.2182e-08
Epoch 349/512
448/448 - 0s - loss: 3.2891e-06 - val_loss: 3.1692e-08
Epoch 350/512
448/448 - 0s - loss: 3.3291e-06 - val_loss: 3.1815e-08
Epoch 351/512
448/448 - 0s - loss: 3.2487e-06 - val_loss: 2.8826e-08
Epoch 352/512
448/448 - 0s - loss: 2.9406e-06 - val_loss: 2.7537e-08
Epoch 353/512
448/448 - 0s - loss: 2.8838e-06 - val_loss: 2.7752e-08
Epoch 354/512
448/448 - 0s - loss: 2.8532e-06 - val_loss: 2.6530e-08
Epoch 355/512
448/448 - 0s - loss: 2.7042e-06 - val_loss: 2.3699e-08
Epoch 356/512
448/448 - 0s - loss: 2.4607e-06 - val_loss: 2.3546e-08
Epoch 357/512
448/448 - 0s - loss: 2.4861e-06 - val_loss: 2.3811e-08
Epoch 358/512
448/448 - 0s - loss: 2.4328e-06 - val_loss: 2.1762e-08
Epoch 359/512
448/448 - 0s - loss: 2.2005e-06 - val_loss: 2.0859e-08
Epoch 360/512
448/448 - 0s - loss: 2.1727e-06 - val_loss: 2.0594e-08
Epoch 361/512
448/448 - 0s - loss: 2.1108e-06 - val_loss: 1.9234e-08
Epoch 362/512
448/448 - 0s - loss: 1.9831e-06 - val_loss: 1.7497e-08
Epoch 363/512
448/448 - 0s - loss: 1.8198e-06 - val_loss: 1.8127e-08
Epoch 364/512
448/448 - 0s - loss: 1.9039e-06 - val_loss: 1.7372e-08
Epoch 365/512
448/448 - 0s - loss: 1.7411e-06 - val_loss: 1.5896e-08
Epoch 366/512
448/448 - 0s - loss: 1.6507e-06 - val_loss: 1.5193e-08
Epoch 367/512
448/448 - 0s - loss: 1.5917e-06 - val_loss: 1.5079e-08
Epoch 368/512
448/448 - 0s - loss: 1.5664e-06 - val_loss: 1.4161e-08
Epoch 369/512
448/448 - 0s - loss: 1.4514e-06 - val_loss: 1.3420e-08
Epoch 370/512
448/448 - 0s - loss: 1.3951e-06 - val_loss: 1.3266e-08
Epoch 371/512
448/448 - 0s - loss: 1.3762e-06 - val_loss: 1.2302e-08
Epoch 372/512
448/448 - 0s - loss: 1.2660e-06 - val_loss: 1.1693e-08
Epoch 373/512
448/448 - 0s - loss: 1.2214e-06 - val_loss: 1.1558e-08
Epoch 374/512
448/448 - 0s - loss: 1.1963e-06 - val_loss: 1.1025e-08
Epoch 375/512
448/448 - 0s - loss: 1.1308e-06 - val_loss: 1.0254e-08
Epoch 376/512
448/448 - 0s - loss: 1.0563e-06 - val_loss: 1.0022e-08
Epoch 377/512
448/448 - 0s - loss: 1.0374e-06 - val_loss: 9.8925e-09
Epoch 378/512
448/448 - 0s - loss: 1.0080e-06 - val_loss: 9.1497e-09
Epoch 379/512
448/448 - 0s - loss: 9.2792e-07 - val_loss: 8.6040e-09
Epoch 380/512
448/448 - 0s - loss: 8.9257e-07 - val_loss: 8.3282e-09
Epoch 381/512
448/448 - 0s - loss: 8.7211e-07 - val_loss: 7.9317e-09
Epoch 382/512
448/448 - 0s - loss: 8.1796e-07 - val_loss: 7.5569e-09
Epoch 383/512
448/448 - 0s - loss: 7.8101e-07 - val_loss: 7.5038e-09
Epoch 384/512
448/448 - 0s - loss: 7.7443e-07 - val_loss: 6.8583e-09
Epoch 385/512
448/448 - 0s - loss: 7.0207e-07 - val_loss: 6.4291e-09
Epoch 386/512
448/448 - 0s - loss: 6.8119e-07 - val_loss: 6.2806e-09
Epoch 387/512
448/448 - 0s - loss: 6.5419e-07 - val_loss: 6.2080e-09
Epoch 388/512
448/448 - 0s - loss: 6.3734e-07 - val_loss: 5.8762e-09
Epoch 389/512
448/448 - 0s - loss: 6.0495e-07 - val_loss: 5.1884e-09
Epoch 390/512
448/448 - 0s - loss: 5.4149e-07 - val_loss: 5.2619e-09
Epoch 391/512
448/448 - 0s - loss: 5.5895e-07 - val_loss: 5.3036e-09
Epoch 392/512
448/448 - 0s - loss: 5.3981e-07 - val_loss: 4.6435e-09
Epoch 393/512
448/448 - 0s - loss: 4.7033e-07 - val_loss: 4.5123e-09
Epoch 394/512
448/448 - 0s - loss: 4.8005e-07 - val_loss: 4.4080e-09
Epoch 395/512
448/448 - 0s - loss: 4.5604e-07 - val_loss: 4.1382e-09
Epoch 396/512
448/448 - 0s - loss: 4.2574e-07 - val_loss: 3.9430e-09
Epoch 397/512
448/448 - 0s - loss: 4.1257e-07 - val_loss: 3.7900e-09
Epoch 398/512
448/448 - 0s - loss: 3.9317e-07 - val_loss: 3.6385e-09
Epoch 399/512
448/448 - 0s - loss: 3.7364e-07 - val_loss: 3.5225e-09
Epoch 400/512
448/448 - 0s - loss: 3.6077e-07 - val_loss: 3.3916e-09
Epoch 401/512
448/448 - 0s - loss: 3.4573e-07 - val_loss: 3.0639e-09
Epoch 402/512
448/448 - 0s - loss: 3.1738e-07 - val_loss: 2.8972e-09
Epoch 403/512
448/448 - 0s - loss: 3.0766e-07 - val_loss: 2.8895e-09
Epoch 404/512
448/448 - 0s - loss: 3.0109e-07 - val_loss: 2.7720e-09
Epoch 405/512
448/448 - 0s - loss: 2.8359e-07 - val_loss: 2.5588e-09
Epoch 406/512
448/448 - 0s - loss: 2.6528e-07 - val_loss: 2.4594e-09
Epoch 407/512
448/448 - 0s - loss: 2.5676e-07 - val_loss: 2.3551e-09
Epoch 408/512
448/448 - 0s - loss: 2.4403e-07 - val_loss: 2.2597e-09
Epoch 409/512
448/448 - 0s - loss: 2.3524e-07 - val_loss: 2.1318e-09
Epoch 410/512
448/448 - 0s - loss: 2.2008e-07 - val_loss: 2.0509e-09
Epoch 411/512
448/448 - 0s - loss: 2.1378e-07 - val_loss: 1.9199e-09
Epoch 412/512
448/448 - 0s - loss: 2.0034e-07 - val_loss: 1.8166e-09
Epoch 413/512
448/448 - 0s - loss: 1.8960e-07 - val_loss: 1.8019e-09
Epoch 414/512
448/448 - 0s - loss: 1.8754e-07 - val_loss: 1.7148e-09
Epoch 415/512
448/448 - 0s - loss: 1.7463e-07 - val_loss: 1.5796e-09
Epoch 416/512
448/448 - 0s - loss: 1.6406e-07 - val_loss: 1.5339e-09
Epoch 417/512
448/448 - 0s - loss: 1.5932e-07 - val_loss: 1.4863e-09
Epoch 418/512
448/448 - 0s - loss: 1.5347e-07 - val_loss: 1.3616e-09
Epoch 419/512
448/448 - 0s - loss: 1.4044e-07 - val_loss: 1.3076e-09
Epoch 420/512
448/448 - 0s - loss: 1.3759e-07 - val_loss: 1.2883e-09
Epoch 421/512
448/448 - 0s - loss: 1.3257e-07 - val_loss: 1.2070e-09
Epoch 422/512
448/448 - 0s - loss: 1.2349e-07 - val_loss: 1.1136e-09
Epoch 423/512
448/448 - 0s - loss: 1.1649e-07 - val_loss: 1.0745e-09
Epoch 424/512
448/448 - 0s - loss: 1.1309e-07 - val_loss: 1.0217e-09
Epoch 425/512
448/448 - 0s - loss: 1.0667e-07 - val_loss: 9.9926e-10
Epoch 426/512
448/448 - 0s - loss: 1.0381e-07 - val_loss: 9.3037e-10
Epoch 427/512
448/448 - 0s - loss: 9.6919e-08 - val_loss: 8.5325e-10
Epoch 428/512
448/448 - 0s - loss: 8.9371e-08 - val_loss: 8.6397e-10
Epoch 429/512
448/448 - 0s - loss: 9.1479e-08 - val_loss: 8.2308e-10
Epoch 430/512
448/448 - 0s - loss: 8.4036e-08 - val_loss: 7.5480e-10
Epoch 431/512
448/448 - 0s - loss: 7.8989e-08 - val_loss: 7.1787e-10
Epoch 432/512
448/448 - 0s - loss: 7.6077e-08 - val_loss: 6.9295e-10
Epoch 433/512
448/448 - 0s - loss: 7.2427e-08 - val_loss: 6.6650e-10
Epoch 434/512
448/448 - 0s - loss: 6.9591e-08 - val_loss: 6.3026e-10
Epoch 435/512
448/448 - 0s - loss: 6.5138e-08 - val_loss: 5.9202e-10
Epoch 436/512
448/448 - 0s - loss: 6.2023e-08 - val_loss: 5.7314e-10
Epoch 437/512
448/448 - 0s - loss: 5.9685e-08 - val_loss: 5.5600e-10
Epoch 438/512
448/448 - 0s - loss: 5.7044e-08 - val_loss: 5.3179e-10
Epoch 439/512
448/448 - 0s - loss: 5.4601e-08 - val_loss: 4.8292e-10
Epoch 440/512
448/448 - 0s - loss: 5.0013e-08 - val_loss: 4.6298e-10
Epoch 441/512
448/448 - 0s - loss: 4.9060e-08 - val_loss: 4.5327e-10
Epoch 442/512
448/448 - 0s - loss: 4.7222e-08 - val_loss: 4.3109e-10
Epoch 443/512
448/448 - 0s - loss: 4.3902e-08 - val_loss: 4.1181e-10
Epoch 444/512
448/448 - 0s - loss: 4.2360e-08 - val_loss: 3.9314e-10
Epoch 445/512
448/448 - 0s - loss: 4.0677e-08 - val_loss: 3.5343e-10
Epoch 446/512
448/448 - 0s - loss: 3.6955e-08 - val_loss: 3.3915e-10
Epoch 447/512
448/448 - 0s - loss: 3.6075e-08 - val_loss: 3.4701e-10
Epoch 448/512
448/448 - 0s - loss: 3.6143e-08 - val_loss: 3.2131e-10
Epoch 449/512
448/448 - 0s - loss: 3.2840e-08 - val_loss: 2.8668e-10
Epoch 450/512
448/448 - 0s - loss: 3.0008e-08 - val_loss: 2.8783e-10
Epoch 451/512
448/448 - 0s - loss: 3.0591e-08 - val_loss: 2.8048e-10
Epoch 452/512
448/448 - 0s - loss: 2.8950e-08 - val_loss: 2.5915e-10
Epoch 453/512
448/448 - 0s - loss: 2.6993e-08 - val_loss: 2.3931e-10
Epoch 454/512
448/448 - 0s - loss: 2.5192e-08 - val_loss: 2.3309e-10
Epoch 455/512
448/448 - 0s - loss: 2.4706e-08 - val_loss: 2.2825e-10
Epoch 456/512
448/448 - 0s - loss: 2.3676e-08 - val_loss: 2.2070e-10
Epoch 457/512
448/448 - 0s - loss: 2.2526e-08 - val_loss: 2.0664e-10
Epoch 458/512
448/448 - 0s - loss: 2.1270e-08 - val_loss: 1.9027e-10
Epoch 459/512
448/448 - 0s - loss: 1.9870e-08 - val_loss: 1.8401e-10
Epoch 460/512
448/448 - 0s - loss: 1.9265e-08 - val_loss: 1.8324e-10
Epoch 461/512
448/448 - 0s - loss: 1.9109e-08 - val_loss: 1.6575e-10
Epoch 462/512
448/448 - 0s - loss: 1.7132e-08 - val_loss: 1.5455e-10
Epoch 463/512
448/448 - 0s - loss: 1.6177e-08 - val_loss: 1.5592e-10
Epoch 464/512
448/448 - 0s - loss: 1.6331e-08 - val_loss: 1.5193e-10
Epoch 465/512
448/448 - 0s - loss: 1.5548e-08 - val_loss: 1.3790e-10
Epoch 466/512
448/448 - 0s - loss: 1.4085e-08 - val_loss: 1.3011e-10
Epoch 467/512
448/448 - 0s - loss: 1.3736e-08 - val_loss: 1.2696e-10
Epoch 468/512
448/448 - 0s - loss: 1.3213e-08 - val_loss: 1.2250e-10
Epoch 469/512
448/448 - 0s - loss: 1.2814e-08 - val_loss: 1.1524e-10
Epoch 470/512
448/448 - 0s - loss: 1.1901e-08 - val_loss: 1.0649e-10
Epoch 471/512
448/448 - 0s - loss: 1.1164e-08 - val_loss: 1.0632e-10
Epoch 472/512
448/448 - 0s - loss: 1.1195e-08 - val_loss: 1.0060e-10
Epoch 473/512
448/448 - 0s - loss: 1.0426e-08 - val_loss: 9.3981e-11
Epoch 474/512
448/448 - 0s - loss: 9.8281e-09 - val_loss: 8.9527e-11
Epoch 475/512
448/448 - 0s - loss: 9.2992e-09 - val_loss: 8.9525e-11
Epoch 476/512
448/448 - 0s - loss: 9.2803e-09 - val_loss: 8.6651e-11
Epoch 477/512
448/448 - 0s - loss: 8.8228e-09 - val_loss: 8.1245e-11
Epoch 478/512
448/448 - 0s - loss: 8.3185e-09 - val_loss: 7.4949e-11
Epoch 479/512
448/448 - 0s - loss: 7.7320e-09 - val_loss: 7.3206e-11
Epoch 480/512
448/448 - 0s - loss: 7.6608e-09 - val_loss: 7.0529e-11
Epoch 481/512
448/448 - 0s - loss: 7.2533e-09 - val_loss: 6.7603e-11
Epoch 482/512
448/448 - 0s - loss: 6.9406e-09 - val_loss: 6.4492e-11
Epoch 483/512
448/448 - 0s - loss: 6.6279e-09 - val_loss: 6.1981e-11
Epoch 484/512
448/448 - 0s - loss: 6.4228e-09 - val_loss: 5.8325e-11
Epoch 485/512
448/448 - 0s - loss: 5.9556e-09 - val_loss: 5.6853e-11
Epoch 486/512
448/448 - 0s - loss: 5.9058e-09 - val_loss: 5.4810e-11
Epoch 487/512
448/448 - 0s - loss: 5.5717e-09 - val_loss: 5.3259e-11
Epoch 488/512
448/448 - 0s - loss: 5.4587e-09 - val_loss: 4.9838e-11
Epoch 489/512
448/448 - 0s - loss: 5.0572e-09 - val_loss: 4.6581e-11
Epoch 490/512
448/448 - 0s - loss: 4.8061e-09 - val_loss: 4.5594e-11
Epoch 491/512
448/448 - 0s - loss: 4.7556e-09 - val_loss: 4.4351e-11
Epoch 492/512
448/448 - 0s - loss: 4.5681e-09 - val_loss: 4.2618e-11
Epoch 493/512
448/448 - 0s - loss: 4.3797e-09 - val_loss: 3.9797e-11
Epoch 494/512
448/448 - 0s - loss: 4.1173e-09 - val_loss: 3.8469e-11
Epoch 495/512
448/448 - 0s - loss: 3.9730e-09 - val_loss: 3.7263e-11
Epoch 496/512
448/448 - 0s - loss: 3.8036e-09 - val_loss: 3.6769e-11
Epoch 497/512
448/448 - 0s - loss: 3.7402e-09 - val_loss: 3.6032e-11
Epoch 498/512
448/448 - 0s - loss: 3.6476e-09 - val_loss: 3.3147e-11
Epoch 499/512
448/448 - 0s - loss: 3.3489e-09 - val_loss: 3.0864e-11
Epoch 500/512
448/448 - 0s - loss: 3.1912e-09 - val_loss: 3.0431e-11
Epoch 501/512
448/448 - 0s - loss: 3.1517e-09 - val_loss: 3.0330e-11
Epoch 502/512
448/448 - 0s - loss: 3.1368e-09 - val_loss: 2.8637e-11
Epoch 503/512
448/448 - 0s - loss: 2.9221e-09 - val_loss: 2.6190e-11
Epoch 504/512
448/448 - 0s - loss: 2.6591e-09 - val_loss: 2.6322e-11
Epoch 505/512
448/448 - 0s - loss: 2.7511e-09 - val_loss: 2.5867e-11
Epoch 506/512
448/448 - 0s - loss: 2.6595e-09 - val_loss: 2.4913e-11
Epoch 507/512
448/448 - 0s - loss: 2.5323e-09 - val_loss: 2.3278e-11
Epoch 508/512
448/448 - 0s - loss: 2.3916e-09 - val_loss: 2.2099e-11
Epoch 509/512
448/448 - 0s - loss: 2.2821e-09 - val_loss: 2.1802e-11
Epoch 510/512
448/448 - 0s - loss: 2.2664e-09 - val_loss: 2.1769e-11
Epoch 511/512
448/448 - 0s - loss: 2.2295e-09 - val_loss: 2.1038e-11
Epoch 512/512
448/448 - 0s - loss: 2.1390e-09 - val_loss: 1.9851e-11
2024-04-14 21:49:15.293872: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0226e-09 - val_loss: 2.1361e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2510e-09 - val_loss: 2.1587e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1446e-09 - val_loss: 1.8619e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8307e-09 - val_loss: 1.6540e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6808e-09 - val_loss: 1.6075e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6785e-09 - val_loss: 1.6719e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7449e-09 - val_loss: 1.7039e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7371e-09 - val_loss: 1.5975e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5965e-09 - val_loss: 1.4674e-09
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4881e-09 - val_loss: 1.3919e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4311e-09 - val_loss: 1.3868e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4384e-09 - val_loss: 1.3884e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4316e-09 - val_loss: 1.3583e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3652e-09 - val_loss: 1.2741e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2889e-09 - val_loss: 1.2194e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2380e-09 - val_loss: 1.1947e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2281e-09 - val_loss: 1.1809e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1922e-09 - val_loss: 1.1314e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1513e-09 - val_loss: 1.1008e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1203e-09 - val_loss: 1.0602e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0727e-09 - val_loss: 1.0237e-09
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0430e-09 - val_loss: 1.0073e-09
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0275e-09 - val_loss: 9.9511e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0114e-09 - val_loss: 9.6424e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7067e-10 - val_loss: 9.1700e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.2155e-10 - val_loss: 8.7770e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.9407e-10 - val_loss: 8.5681e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.7808e-10 - val_loss: 8.5315e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.7390e-10 - val_loss: 8.3030e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.4102e-10 - val_loss: 7.9129e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0317e-10 - val_loss: 7.8300e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.9916e-10 - val_loss: 7.6659e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.7386e-10 - val_loss: 7.4047e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5401e-10 - val_loss: 7.2860e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4179e-10 - val_loss: 7.1629e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2938e-10 - val_loss: 7.0153e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0396e-10 - val_loss: 6.7243e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7920e-10 - val_loss: 6.4183e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.6040e-10 - val_loss: 6.3658e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.4976e-10 - val_loss: 6.2607e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.3407e-10 - val_loss: 6.0980e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1855e-10 - val_loss: 5.9236e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0343e-10 - val_loss: 5.8119e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8792e-10 - val_loss: 5.7304e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8223e-10 - val_loss: 5.5797e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6456e-10 - val_loss: 5.4601e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5123e-10 - val_loss: 5.2562e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3477e-10 - val_loss: 5.1936e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1847e-10 - val_loss: 4.9416e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0557e-10 - val_loss: 4.9582e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0544e-10 - val_loss: 4.9741e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0965e-10 - val_loss: 4.9160e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0121e-10 - val_loss: 4.8278e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8589e-10 - val_loss: 4.6556e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6550e-10 - val_loss: 4.3552e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3732e-10 - val_loss: 4.2934e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3574e-10 - val_loss: 4.2161e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2893e-10 - val_loss: 4.2003e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2682e-10 - val_loss: 4.1474e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1743e-10 - val_loss: 3.9974e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0399e-10 - val_loss: 3.9544e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0046e-10 - val_loss: 3.8781e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9440e-10 - val_loss: 3.8284e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8671e-10 - val_loss: 3.7260e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7735e-10 - val_loss: 3.6810e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7113e-10 - val_loss: 3.5829e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5710e-10 - val_loss: 3.4190e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4780e-10 - val_loss: 3.4031e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4110e-10 - val_loss: 3.3497e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4021e-10 - val_loss: 3.3690e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4216e-10 - val_loss: 3.3965e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4369e-10 - val_loss: 3.2517e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2480e-10 - val_loss: 3.1809e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2012e-10 - val_loss: 2.9998e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0103e-10 - val_loss: 2.9718e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0316e-10 - val_loss: 2.9670e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0044e-10 - val_loss: 2.9643e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0349e-10 - val_loss: 2.9034e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9010e-10 - val_loss: 2.7895e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8447e-10 - val_loss: 2.7002e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6858e-10 - val_loss: 2.6458e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7020e-10 - val_loss: 2.6684e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7039e-10 - val_loss: 2.6311e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6781e-10 - val_loss: 2.6745e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6723e-10 - val_loss: 2.5623e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5906e-10 - val_loss: 2.5632e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5924e-10 - val_loss: 2.4822e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4705e-10 - val_loss: 2.3849e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3880e-10 - val_loss: 2.2974e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3371e-10 - val_loss: 2.2927e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3608e-10 - val_loss: 2.3795e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4112e-10 - val_loss: 2.3742e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3925e-10 - val_loss: 2.3669e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3787e-10 - val_loss: 2.2956e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2907e-10 - val_loss: 2.2193e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1930e-10 - val_loss: 2.1003e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1115e-10 - val_loss: 2.0588e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0838e-10 - val_loss: 2.0405e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0727e-10 - val_loss: 2.0730e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1099e-10 - val_loss: 2.0645e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0509e-10 - val_loss: 2.0231e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0451e-10 - val_loss: 1.9827e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0095e-10 - val_loss: 1.9542e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9842e-10 - val_loss: 1.9567e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9830e-10 - val_loss: 1.9305e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9086e-10 - val_loss: 1.7997e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8042e-10 - val_loss: 1.7661e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7968e-10 - val_loss: 1.7183e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7474e-10 - val_loss: 1.7811e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8081e-10 - val_loss: 1.7912e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8068e-10 - val_loss: 1.7201e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7360e-10 - val_loss: 1.6904e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7070e-10 - val_loss: 1.6592e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6646e-10 - val_loss: 1.6032e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6165e-10 - val_loss: 1.6339e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6684e-10 - val_loss: 1.6787e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6677e-10 - val_loss: 1.5714e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5723e-10 - val_loss: 1.5265e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5497e-10 - val_loss: 1.4995e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4985e-10 - val_loss: 1.4343e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4525e-10 - val_loss: 1.4824e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5158e-10 - val_loss: 1.4991e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5098e-10 - val_loss: 1.4758e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4794e-10 - val_loss: 1.4818e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5103e-10 - val_loss: 1.4649e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4475e-10 - val_loss: 1.3551e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3605e-10 - val_loss: 1.3678e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4145e-10 - val_loss: 1.4589e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4702e-10 - val_loss: 1.4096e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3990e-10 - val_loss: 1.3540e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3720e-10 - val_loss: 1.3739e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3682e-10 - val_loss: 1.3229e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3276e-10 - val_loss: 1.3037e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3398e-10 - val_loss: 1.3828e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4235e-10 - val_loss: 1.3739e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3619e-10 - val_loss: 1.2891e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2722e-10 - val_loss: 1.2329e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2654e-10 - val_loss: 1.2467e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2491e-10 - val_loss: 1.2062e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2167e-10 - val_loss: 1.1768e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1914e-10 - val_loss: 1.2166e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2329e-10 - val_loss: 1.2062e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2018e-10 - val_loss: 1.1743e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1848e-10 - val_loss: 1.1504e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1773e-10 - val_loss: 1.1899e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1925e-10 - val_loss: 1.1440e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1355e-10 - val_loss: 1.0893e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0980e-10 - val_loss: 1.1292e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1282e-10 - val_loss: 1.0862e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0960e-10 - val_loss: 1.1005e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1025e-10 - val_loss: 1.0615e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0635e-10 - val_loss: 1.0557e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0756e-10 - val_loss: 1.0915e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1099e-10 - val_loss: 1.0887e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1011e-10 - val_loss: 1.0805e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0944e-10 - val_loss: 1.0996e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0933e-10 - val_loss: 1.0220e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0150e-10 - val_loss: 9.8993e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9164e-11 - val_loss: 9.7920e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7331e-11 - val_loss: 9.1448e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1280e-11 - val_loss: 9.0561e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1530e-11 - val_loss: 9.5182e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7435e-11 - val_loss: 9.7754e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9015e-11 - val_loss: 9.6550e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5929e-11 - val_loss: 9.3742e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6357e-11 - val_loss: 9.8393e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8766e-11 - val_loss: 9.5586e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8000e-11 - val_loss: 9.6957e-11
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.3994e-11 - val_loss: 8.7206e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.8019e-11 - val_loss: 8.5558e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.6245e-11 - val_loss: 8.4507e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5338e-11 - val_loss: 8.6729e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9086e-11 - val_loss: 8.5597e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3073e-11 - val_loss: 7.8693e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9060e-11 - val_loss: 8.0044e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1523e-11 - val_loss: 8.1052e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2602e-11 - val_loss: 8.5242e-11
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3961e-11 - val_loss: 7.7838e-11
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.6600e-11 - val_loss: 7.5070e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5496e-11 - val_loss: 7.4929e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5448e-11 - val_loss: 7.4482e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6719e-11 - val_loss: 8.0320e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0258e-11 - val_loss: 7.4074e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.3348e-11 - val_loss: 7.4062e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5214e-11 - val_loss: 7.4662e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5010e-11 - val_loss: 7.6666e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8360e-11 - val_loss: 7.9547e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8010e-11 - val_loss: 7.1622e-11
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.9904e-11 - val_loss: 6.7890e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8950e-11 - val_loss: 7.0739e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3319e-11 - val_loss: 7.2474e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3705e-11 - val_loss: 7.4223e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2299e-11 - val_loss: 6.6307e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5458e-11 - val_loss: 6.2055e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2643e-11 - val_loss: 6.4317e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6965e-11 - val_loss: 6.9457e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1266e-11 - val_loss: 7.2401e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2950e-11 - val_loss: 6.9189e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7573e-11 - val_loss: 6.2826e-11
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1383e-11 - val_loss: 6.0872e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3000e-11 - val_loss: 6.5054e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7092e-11 - val_loss: 6.9264e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0669e-11 - val_loss: 7.0592e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8619e-11 - val_loss: 6.3287e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1751e-11 - val_loss: 5.7038e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6621e-11 - val_loss: 5.5090e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7076e-11 - val_loss: 5.8761e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0210e-11 - val_loss: 6.1520e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3147e-11 - val_loss: 6.2002e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0542e-11 - val_loss: 5.6007e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5890e-11 - val_loss: 5.4177e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4359e-11 - val_loss: 5.5537e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8419e-11 - val_loss: 6.2815e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5238e-11 - val_loss: 6.7779e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9369e-11 - val_loss: 7.0015e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8390e-11 - val_loss: 6.2777e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0947e-11 - val_loss: 5.6300e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5411e-11 - val_loss: 5.2669e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3635e-11 - val_loss: 5.7292e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8341e-11 - val_loss: 6.1686e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2708e-11 - val_loss: 6.2424e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0648e-11 - val_loss: 5.5988e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.4758e-11 - val_loss: 5.0465e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9687e-11 - val_loss: 4.8037e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8332e-11 - val_loss: 4.7573e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8612e-11 - val_loss: 5.3071e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5069e-11 - val_loss: 5.8944e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0482e-11 - val_loss: 6.2307e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1965e-11 - val_loss: 5.8112e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6837e-11 - val_loss: 5.2658e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0637e-11 - val_loss: 4.6765e-11
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6370e-11 - val_loss: 4.5244e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6106e-11 - val_loss: 4.8828e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0604e-11 - val_loss: 5.3587e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5683e-11 - val_loss: 5.9301e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9408e-11 - val_loss: 5.6817e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6004e-11 - val_loss: 5.1741e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0384e-11 - val_loss: 4.7817e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7085e-11 - val_loss: 4.3854e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4283e-11 - val_loss: 4.5601e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7707e-11 - val_loss: 5.1819e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3817e-11 - val_loss: 5.6710e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8058e-11 - val_loss: 5.5139e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3661e-11 - val_loss: 5.0189e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8978e-11 - val_loss: 4.6154e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5717e-11 - val_loss: 4.5283e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4613e-11 - val_loss: 4.3057e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2907e-11 - val_loss: 4.4118e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5984e-11 - val_loss: 4.8523e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0796e-11 - val_loss: 5.3972e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2737e-11 - val_loss: 4.9252e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7819e-11 - val_loss: 4.5447e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5206e-11 - val_loss: 4.3688e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3175e-11 - val_loss: 4.2406e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3391e-11 - val_loss: 4.2630e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2235e-11 - val_loss: 4.2054e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4305e-11 - val_loss: 4.7929e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9617e-11 - val_loss: 5.0584e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1135e-11 - val_loss: 4.8351e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6595e-11 - val_loss: 4.2762e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1674e-11 - val_loss: 3.9814e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9958e-11 - val_loss: 3.9424e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9905e-11 - val_loss: 4.0669e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0460e-11 - val_loss: 3.8700e-11
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7841e-11 - val_loss: 3.7938e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0904e-11 - val_loss: 4.5080e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6625e-11 - val_loss: 4.9052e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8254e-11 - val_loss: 4.4674e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3832e-11 - val_loss: 4.0967e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9892e-11 - val_loss: 3.6884e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6068e-11 - val_loss: 3.6199e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6360e-11 - val_loss: 3.5704e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6837e-11 - val_loss: 3.7471e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7625e-11 - val_loss: 3.8568e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0022e-11 - val_loss: 4.3420e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4570e-11 - val_loss: 4.6755e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7750e-11 - val_loss: 4.6151e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3685e-11 - val_loss: 3.8796e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8107e-11 - val_loss: 3.6314e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6079e-11 - val_loss: 3.4607e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5370e-11 - val_loss: 3.4824e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4289e-11 - val_loss: 3.4113e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3955e-11 - val_loss: 3.4475e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4578e-11 - val_loss: 3.4923e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5921e-11 - val_loss: 3.9205e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1591e-11 - val_loss: 4.4832e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6428e-11 - val_loss: 4.6248e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4141e-11 - val_loss: 4.0778e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9053e-11 - val_loss: 3.5308e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5227e-11 - val_loss: 3.3653e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3417e-11 - val_loss: 3.1597e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1516e-11 - val_loss: 3.1113e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2165e-11 - val_loss: 3.1961e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1589e-11 - val_loss: 3.2009e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3089e-11 - val_loss: 3.4560e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5269e-11 - val_loss: 3.7501e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9281e-11 - val_loss: 4.2152e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1230e-11 - val_loss: 3.7902e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7436e-11 - val_loss: 3.4843e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4506e-11 - val_loss: 3.3498e-11
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2686e-11 - val_loss: 3.0785e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0248e-11 - val_loss: 2.8286e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8612e-11 - val_loss: 2.8467e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8921e-11 - val_loss: 2.9433e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0309e-11 - val_loss: 3.1652e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2135e-11 - val_loss: 3.2269e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2735e-11 - val_loss: 3.4380e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5916e-11 - val_loss: 3.9163e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0143e-11 - val_loss: 3.8459e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7398e-11 - val_loss: 3.5068e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4505e-11 - val_loss: 3.2611e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2093e-11 - val_loss: 3.0832e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1192e-11 - val_loss: 3.0096e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9290e-11 - val_loss: 2.7497e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7858e-11 - val_loss: 2.7539e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8117e-11 - val_loss: 2.8459e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8554e-11 - val_loss: 2.8573e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8290e-11 - val_loss: 2.8518e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9319e-11 - val_loss: 2.9675e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9239e-11 - val_loss: 2.7630e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7417e-11 - val_loss: 2.8416e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8770e-11 - val_loss: 2.7652e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8545e-11 - val_loss: 2.9849e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0487e-11 - val_loss: 3.1228e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1155e-11 - val_loss: 3.1311e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2071e-11 - val_loss: 3.3237e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2731e-11 - val_loss: 3.0132e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9815e-11 - val_loss: 2.8292e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7861e-11 - val_loss: 2.7785e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7715e-11 - val_loss: 2.7257e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6922e-11 - val_loss: 2.6794e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6811e-11 - val_loss: 2.6276e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5972e-11 - val_loss: 2.5986e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6707e-11 - val_loss: 2.7952e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7889e-11 - val_loss: 2.9025e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9808e-11 - val_loss: 3.0402e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0317e-11 - val_loss: 2.8781e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8899e-11 - val_loss: 2.7578e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6778e-11 - val_loss: 2.4448e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4460e-11 - val_loss: 2.4636e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5349e-11 - val_loss: 2.5259e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5916e-11 - val_loss: 2.5412e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6047e-11 - val_loss: 2.6199e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6430e-11 - val_loss: 2.6844e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7226e-11 - val_loss: 2.6341e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6293e-11 - val_loss: 2.5049e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5010e-11 - val_loss: 2.5427e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5584e-11 - val_loss: 2.6057e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5014e-11 - val_loss: 2.0677e-11
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9354e-11 - val_loss: 1.7207e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6719e-11 - val_loss: 1.5248e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4659e-11 - val_loss: 1.5454e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6514e-11 - val_loss: 1.8717e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9367e-11 - val_loss: 2.0625e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0971e-11 - val_loss: 2.2156e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3027e-11 - val_loss: 2.5041e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5549e-11 - val_loss: 2.5503e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6228e-11 - val_loss: 2.7303e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7340e-11 - val_loss: 2.8483e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9035e-11 - val_loss: 2.8270e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7700e-11 - val_loss: 2.6061e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5815e-11 - val_loss: 2.4412e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4068e-11 - val_loss: 2.3785e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3185e-11 - val_loss: 2.1325e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1544e-11 - val_loss: 2.1266e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1483e-11 - val_loss: 2.0052e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9763e-11 - val_loss: 1.8785e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7399e-11 - val_loss: 1.6000e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5721e-11 - val_loss: 1.4401e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3897e-11 - val_loss: 1.4813e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5438e-11 - val_loss: 1.7219e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7875e-11 - val_loss: 1.8777e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9401e-11 - val_loss: 2.0553e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0317e-11 - val_loss: 2.0402e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1842e-11 - val_loss: 2.4659e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4853e-11 - val_loss: 2.5113e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4774e-11 - val_loss: 2.4690e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4279e-11 - val_loss: 2.2977e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3154e-11 - val_loss: 2.3471e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3311e-11 - val_loss: 2.4150e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4203e-11 - val_loss: 2.4391e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4301e-11 - val_loss: 2.3775e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4170e-11 - val_loss: 2.3663e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2903e-11 - val_loss: 2.1142e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0531e-11 - val_loss: 1.8067e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7103e-11 - val_loss: 1.5184e-11
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4040e-11 - val_loss: 1.2314e-11
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2321e-11 - val_loss: 1.1315e-11
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0834e-11 - val_loss: 1.0405e-11
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0475e-11 - val_loss: 1.0346e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9773e-12 - val_loss: 9.8315e-12
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0396e-11 - val_loss: 1.2934e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4476e-11 - val_loss: 1.7465e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8822e-11 - val_loss: 2.0251e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0614e-11 - val_loss: 2.1003e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1615e-11 - val_loss: 2.2325e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2755e-11 - val_loss: 2.3085e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2731e-11 - val_loss: 2.2593e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3369e-11 - val_loss: 2.4170e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3823e-11 - val_loss: 2.3458e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3278e-11 - val_loss: 2.3073e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2736e-11 - val_loss: 2.2384e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2175e-11 - val_loss: 2.1740e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1383e-11 - val_loss: 2.0808e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0388e-11 - val_loss: 1.9182e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9073e-11 - val_loss: 1.8234e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7380e-11 - val_loss: 1.5293e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4055e-11 - val_loss: 1.2600e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2440e-11 - val_loss: 1.2278e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1704e-11 - val_loss: 1.0279e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0271e-11 - val_loss: 1.0309e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0047e-11 - val_loss: 9.7699e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8305e-12 - val_loss: 9.8082e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8887e-12 - val_loss: 9.8470e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5813e-12 - val_loss: 1.0261e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1015e-11 - val_loss: 1.3512e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4860e-11 - val_loss: 1.6662e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7361e-11 - val_loss: 1.9003e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9705e-11 - val_loss: 2.0430e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0847e-11 - val_loss: 2.1739e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2104e-11 - val_loss: 2.2510e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2787e-11 - val_loss: 2.1644e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1137e-11 - val_loss: 2.0095e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0031e-11 - val_loss: 1.9100e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9438e-11 - val_loss: 1.9885e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9757e-11 - val_loss: 1.9080e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8716e-11 - val_loss: 1.8668e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8940e-11 - val_loss: 1.8223e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8462e-11 - val_loss: 1.8543e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8253e-11 - val_loss: 1.7542e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7219e-11 - val_loss: 1.6946e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6494e-11 - val_loss: 1.4610e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4015e-11 - val_loss: 1.2505e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2071e-11 - val_loss: 1.1555e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1290e-11 - val_loss: 1.0795e-11
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0348e-11 - val_loss: 9.5718e-12
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.2232e-12 - val_loss: 9.2129e-12
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3321e-12 - val_loss: 9.3728e-12
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6654e-12 - val_loss: 9.9438e-12
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9704e-12 - val_loss: 9.7706e-12
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5724e-12 - val_loss: 9.5034e-12
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8187e-12 - val_loss: 1.1047e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1617e-11 - val_loss: 1.3446e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3992e-11 - val_loss: 1.5506e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5756e-11 - val_loss: 1.5952e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6494e-11 - val_loss: 1.7276e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7288e-11 - val_loss: 1.7651e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7876e-11 - val_loss: 1.7192e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7072e-11 - val_loss: 1.6417e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6534e-11 - val_loss: 1.7167e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7024e-11 - val_loss: 1.6282e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6193e-11 - val_loss: 1.4737e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4923e-11 - val_loss: 1.4749e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4956e-11 - val_loss: 1.5045e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5158e-11 - val_loss: 1.4518e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4491e-11 - val_loss: 1.4066e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4431e-11 - val_loss: 1.4656e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4618e-11 - val_loss: 1.3963e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4012e-11 - val_loss: 1.4583e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4921e-11 - val_loss: 1.5316e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5530e-11 - val_loss: 1.6454e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6172e-11 - val_loss: 1.5385e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5257e-11 - val_loss: 1.3753e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2346e-11 - val_loss: 1.0181e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.2199e-12 - val_loss: 7.3517e-12
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.9320e-12 - val_loss: 6.4543e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5159e-12 - val_loss: 6.6253e-12
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.4622e-12 - val_loss: 5.8172e-12
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6814e-12 - val_loss: 5.2900e-12
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1403e-12 - val_loss: 4.6930e-12
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5567e-12 - val_loss: 4.2065e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1490e-12 - val_loss: 4.3271e-12
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2046e-12 - val_loss: 4.1042e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1804e-12 - val_loss: 4.3512e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2259e-12 - val_loss: 4.1312e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1815e-12 - val_loss: 4.3156e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6583e-12 - val_loss: 5.1381e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1801e-12 - val_loss: 5.4686e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6407e-12 - val_loss: 5.8330e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1040e-12 - val_loss: 7.1408e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0418e-12 - val_loss: 9.9292e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1142e-11 - val_loss: 1.3547e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4502e-11 - val_loss: 1.5597e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5719e-11 - val_loss: 1.5917e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6127e-11 - val_loss: 1.7142e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7266e-11 - val_loss: 1.6323e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6156e-11 - val_loss: 1.5384e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5603e-11 - val_loss: 1.5780e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6682e-11 - val_loss: 1.6962e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6454e-11 - val_loss: 1.5980e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5636e-11 - val_loss: 1.4984e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4687e-11 - val_loss: 1.4723e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4484e-11 - val_loss: 1.3656e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3619e-11 - val_loss: 1.3466e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3511e-11 - val_loss: 1.3959e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3970e-11 - val_loss: 1.4509e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4578e-11 - val_loss: 1.3939e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3826e-11 - val_loss: 1.4193e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4248e-11 - val_loss: 1.4091e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3813e-11 - val_loss: 1.2977e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2945e-11 - val_loss: 1.1381e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0224e-11 - val_loss: 8.2125e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6889e-12 - val_loss: 6.5504e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9790e-12 - val_loss: 5.0130e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0051e-12 - val_loss: 5.1016e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0986e-12 - val_loss: 4.3997e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1941e-12 - val_loss: 4.1050e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1614e-12 - val_loss: 4.2924e-12
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1164e-12 - val_loss: 3.4765e-12
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3227e-12 - val_loss: 3.1747e-12
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6070e-12 - val_loss: 4.4692e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5056e-12 - val_loss: 4.5834e-12
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0458 - val_loss: 7.5950e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.0501e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.0801e-04
Epoch 4/512
448/448 - 0s - loss: 0.0010 - val_loss: 2.0346e-04
Epoch 5/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.4349e-04
Epoch 6/512
448/448 - 0s - loss: 8.7935e-04 - val_loss: 1.3366e-04
Epoch 7/512
448/448 - 0s - loss: 5.7947e-04 - val_loss: 1.0348e-04
Epoch 8/512
448/448 - 0s - loss: 9.5200e-04 - val_loss: 4.0119e-05
Epoch 9/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.2876e-05
Epoch 10/512
448/448 - 0s - loss: 5.8966e-04 - val_loss: 1.9469e-05
Epoch 11/512
448/448 - 0s - loss: 6.1376e-04 - val_loss: 9.7428e-06
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0567e-05
Epoch 13/512
448/448 - 0s - loss: 5.9722e-04 - val_loss: 1.2144e-05
Epoch 14/512
448/448 - 0s - loss: 4.5074e-04 - val_loss: 1.4747e-05
Epoch 15/512
448/448 - 0s - loss: 8.2547e-04 - val_loss: 1.9201e-05
Epoch 16/512
448/448 - 0s - loss: 5.5576e-04 - val_loss: 2.0100e-05
Epoch 17/512
448/448 - 0s - loss: 3.5127e-04 - val_loss: 2.0614e-05
Epoch 18/512
448/448 - 0s - loss: 5.7108e-04 - val_loss: 2.2443e-05
Epoch 19/512
448/448 - 0s - loss: 4.6527e-04 - val_loss: 2.2497e-05
Epoch 20/512
448/448 - 0s - loss: 2.9435e-04 - val_loss: 2.1049e-05
Epoch 21/512
448/448 - 0s - loss: 3.9656e-04 - val_loss: 2.1084e-05
Epoch 22/512
448/448 - 0s - loss: 3.7646e-04 - val_loss: 2.0508e-05
Epoch 23/512
448/448 - 0s - loss: 2.3811e-04 - val_loss: 1.7971e-05
Epoch 24/512
448/448 - 0s - loss: 2.9364e-04 - val_loss: 1.7590e-05
Epoch 25/512
448/448 - 0s - loss: 2.8494e-04 - val_loss: 1.6011e-05
Epoch 26/512
448/448 - 0s - loss: 2.0666e-04 - val_loss: 1.3757e-05
Epoch 27/512
448/448 - 0s - loss: 2.1509e-04 - val_loss: 1.2550e-05
Epoch 28/512
448/448 - 0s - loss: 2.2029e-04 - val_loss: 1.1192e-05
Epoch 29/512
448/448 - 0s - loss: 1.7399e-04 - val_loss: 9.4078e-06
Epoch 30/512
448/448 - 0s - loss: 1.6988e-04 - val_loss: 8.1823e-06
Epoch 31/512
448/448 - 0s - loss: 1.7488e-04 - val_loss: 7.2667e-06
Epoch 32/512
448/448 - 0s - loss: 1.4757e-04 - val_loss: 6.5676e-06
Epoch 33/512
448/448 - 0s - loss: 1.4059e-04 - val_loss: 6.1237e-06
Epoch 34/512
448/448 - 0s - loss: 1.4074e-04 - val_loss: 5.9255e-06
Epoch 35/512
448/448 - 0s - loss: 1.3036e-04 - val_loss: 5.8638e-06
Epoch 36/512
448/448 - 0s - loss: 1.2192e-04 - val_loss: 5.8203e-06
Epoch 37/512
448/448 - 0s - loss: 1.1788e-04 - val_loss: 5.7551e-06
Epoch 38/512
448/448 - 0s - loss: 1.1260e-04 - val_loss: 5.6527e-06
Epoch 39/512
448/448 - 0s - loss: 1.0842e-04 - val_loss: 5.4686e-06
Epoch 40/512
448/448 - 0s - loss: 1.0354e-04 - val_loss: 5.2652e-06
Epoch 41/512
448/448 - 0s - loss: 9.8510e-05 - val_loss: 5.0982e-06
Epoch 42/512
448/448 - 0s - loss: 9.6299e-05 - val_loss: 4.9044e-06
Epoch 43/512
448/448 - 0s - loss: 9.3798e-05 - val_loss: 4.6039e-06
Epoch 44/512
448/448 - 0s - loss: 9.1068e-05 - val_loss: 4.3088e-06
Epoch 45/512
448/448 - 0s - loss: 8.4917e-05 - val_loss: 3.9164e-06
Epoch 46/512
448/448 - 0s - loss: 8.4726e-05 - val_loss: 3.7186e-06
Epoch 47/512
448/448 - 0s - loss: 8.4496e-05 - val_loss: 3.4815e-06
Epoch 48/512
448/448 - 0s - loss: 7.8046e-05 - val_loss: 3.1978e-06
Epoch 49/512
448/448 - 0s - loss: 7.7124e-05 - val_loss: 2.9956e-06
Epoch 50/512
448/448 - 0s - loss: 7.6511e-05 - val_loss: 2.7620e-06
Epoch 51/512
448/448 - 0s - loss: 7.4139e-05 - val_loss: 2.5646e-06
Epoch 52/512
448/448 - 0s - loss: 6.9757e-05 - val_loss: 2.3879e-06
Epoch 53/512
448/448 - 0s - loss: 7.0630e-05 - val_loss: 2.2911e-06
Epoch 54/512
448/448 - 0s - loss: 6.8415e-05 - val_loss: 2.0848e-06
Epoch 55/512
448/448 - 0s - loss: 6.7262e-05 - val_loss: 1.8928e-06
Epoch 56/512
448/448 - 0s - loss: 6.4259e-05 - val_loss: 1.7429e-06
Epoch 57/512
448/448 - 0s - loss: 6.3515e-05 - val_loss: 1.6667e-06
Epoch 58/512
448/448 - 0s - loss: 6.2629e-05 - val_loss: 1.5548e-06
Epoch 59/512
448/448 - 0s - loss: 6.0598e-05 - val_loss: 1.4536e-06
Epoch 60/512
448/448 - 0s - loss: 5.9513e-05 - val_loss: 1.3594e-06
Epoch 61/512
448/448 - 0s - loss: 5.7663e-05 - val_loss: 1.2676e-06
Epoch 62/512
448/448 - 0s - loss: 5.8604e-05 - val_loss: 1.1768e-06
Epoch 63/512
448/448 - 0s - loss: 5.5344e-05 - val_loss: 1.0731e-06
Epoch 64/512
448/448 - 0s - loss: 5.4556e-05 - val_loss: 1.0309e-06
Epoch 65/512
448/448 - 0s - loss: 5.3499e-05 - val_loss: 9.7617e-07
Epoch 66/512
448/448 - 0s - loss: 5.3985e-05 - val_loss: 9.1093e-07
Epoch 67/512
448/448 - 0s - loss: 5.1830e-05 - val_loss: 8.2967e-07
Epoch 68/512
448/448 - 0s - loss: 4.9497e-05 - val_loss: 7.8136e-07
Epoch 69/512
448/448 - 0s - loss: 5.1023e-05 - val_loss: 7.5126e-07
Epoch 70/512
448/448 - 0s - loss: 4.8916e-05 - val_loss: 7.0167e-07
Epoch 71/512
448/448 - 0s - loss: 4.7883e-05 - val_loss: 6.6613e-07
Epoch 72/512
448/448 - 0s - loss: 4.7613e-05 - val_loss: 6.3027e-07
Epoch 73/512
448/448 - 0s - loss: 4.5932e-05 - val_loss: 5.9769e-07
Epoch 74/512
448/448 - 0s - loss: 4.5999e-05 - val_loss: 5.7748e-07
Epoch 75/512
448/448 - 0s - loss: 4.6169e-05 - val_loss: 5.2245e-07
Epoch 76/512
448/448 - 0s - loss: 4.2791e-05 - val_loss: 4.9186e-07
Epoch 77/512
448/448 - 0s - loss: 4.3453e-05 - val_loss: 4.8631e-07
Epoch 78/512
448/448 - 0s - loss: 4.3373e-05 - val_loss: 4.6180e-07
Epoch 79/512
448/448 - 0s - loss: 4.1858e-05 - val_loss: 4.3775e-07
Epoch 80/512
448/448 - 0s - loss: 4.0909e-05 - val_loss: 4.1914e-07
Epoch 81/512
448/448 - 0s - loss: 4.1033e-05 - val_loss: 4.0309e-07
Epoch 82/512
448/448 - 0s - loss: 4.0163e-05 - val_loss: 3.8099e-07
Epoch 83/512
448/448 - 0s - loss: 3.9106e-05 - val_loss: 3.6562e-07
Epoch 84/512
448/448 - 0s - loss: 3.9281e-05 - val_loss: 3.4853e-07
Epoch 85/512
448/448 - 0s - loss: 3.7565e-05 - val_loss: 3.4004e-07
Epoch 86/512
448/448 - 0s - loss: 3.8233e-05 - val_loss: 3.2496e-07
Epoch 87/512
448/448 - 0s - loss: 3.7032e-05 - val_loss: 3.0754e-07
Epoch 88/512
448/448 - 0s - loss: 3.5930e-05 - val_loss: 2.9490e-07
Epoch 89/512
448/448 - 0s - loss: 3.5404e-05 - val_loss: 2.9276e-07
Epoch 90/512
448/448 - 0s - loss: 3.6084e-05 - val_loss: 2.8180e-07
Epoch 91/512
448/448 - 0s - loss: 3.4403e-05 - val_loss: 2.6758e-07
Epoch 92/512
448/448 - 0s - loss: 3.3580e-05 - val_loss: 2.6577e-07
Epoch 93/512
448/448 - 0s - loss: 3.4357e-05 - val_loss: 2.5374e-07
Epoch 94/512
448/448 - 0s - loss: 3.2577e-05 - val_loss: 2.4514e-07
Epoch 95/512
448/448 - 0s - loss: 3.3062e-05 - val_loss: 2.3684e-07
Epoch 96/512
448/448 - 0s - loss: 3.1739e-05 - val_loss: 2.2900e-07
Epoch 97/512
448/448 - 0s - loss: 3.1707e-05 - val_loss: 2.2273e-07
Epoch 98/512
448/448 - 0s - loss: 3.1284e-05 - val_loss: 2.1095e-07
Epoch 99/512
448/448 - 0s - loss: 3.0083e-05 - val_loss: 2.0852e-07
Epoch 100/512
448/448 - 0s - loss: 3.0351e-05 - val_loss: 2.0407e-07
Epoch 101/512
448/448 - 0s - loss: 2.9750e-05 - val_loss: 1.9538e-07
Epoch 102/512
448/448 - 0s - loss: 2.8738e-05 - val_loss: 1.9168e-07
Epoch 103/512
448/448 - 0s - loss: 2.8661e-05 - val_loss: 1.8856e-07
Epoch 104/512
448/448 - 0s - loss: 2.8507e-05 - val_loss: 1.7969e-07
Epoch 105/512
448/448 - 0s - loss: 2.7586e-05 - val_loss: 1.7134e-07
Epoch 106/512
448/448 - 0s - loss: 2.6987e-05 - val_loss: 1.7086e-07
Epoch 107/512
448/448 - 0s - loss: 2.6510e-05 - val_loss: 1.7344e-07
Epoch 108/512
448/448 - 0s - loss: 2.7076e-05 - val_loss: 1.6326e-07
Epoch 109/512
448/448 - 0s - loss: 2.5351e-05 - val_loss: 1.6007e-07
Epoch 110/512
448/448 - 0s - loss: 2.5370e-05 - val_loss: 1.5934e-07
Epoch 111/512
448/448 - 0s - loss: 2.5310e-05 - val_loss: 1.5676e-07
Epoch 112/512
448/448 - 0s - loss: 2.4406e-05 - val_loss: 1.5245e-07
Epoch 113/512
448/448 - 0s - loss: 2.4130e-05 - val_loss: 1.5037e-07
Epoch 114/512
448/448 - 0s - loss: 2.3668e-05 - val_loss: 1.4970e-07
Epoch 115/512
448/448 - 0s - loss: 2.3551e-05 - val_loss: 1.4337e-07
Epoch 116/512
448/448 - 0s - loss: 2.2590e-05 - val_loss: 1.4162e-07
Epoch 117/512
448/448 - 0s - loss: 2.2720e-05 - val_loss: 1.3904e-07
Epoch 118/512
448/448 - 0s - loss: 2.2085e-05 - val_loss: 1.3433e-07
Epoch 119/512
448/448 - 0s - loss: 2.1479e-05 - val_loss: 1.3135e-07
Epoch 120/512
448/448 - 0s - loss: 2.0983e-05 - val_loss: 1.3383e-07
Epoch 121/512
448/448 - 0s - loss: 2.1553e-05 - val_loss: 1.2680e-07
Epoch 122/512
448/448 - 0s - loss: 2.0056e-05 - val_loss: 1.2391e-07
Epoch 123/512
448/448 - 0s - loss: 2.0028e-05 - val_loss: 1.2387e-07
Epoch 124/512
448/448 - 0s - loss: 1.9905e-05 - val_loss: 1.2097e-07
Epoch 125/512
448/448 - 0s - loss: 1.9320e-05 - val_loss: 1.1617e-07
Epoch 126/512
448/448 - 0s - loss: 1.8839e-05 - val_loss: 1.1403e-07
Epoch 127/512
448/448 - 0s - loss: 1.8704e-05 - val_loss: 1.1100e-07
Epoch 128/512
448/448 - 0s - loss: 1.8149e-05 - val_loss: 1.0817e-07
Epoch 129/512
448/448 - 0s - loss: 1.7730e-05 - val_loss: 1.0792e-07
Epoch 130/512
448/448 - 0s - loss: 1.7511e-05 - val_loss: 1.0768e-07
Epoch 131/512
448/448 - 0s - loss: 1.7548e-05 - val_loss: 1.0167e-07
Epoch 132/512
448/448 - 0s - loss: 1.6466e-05 - val_loss: 9.7754e-08
Epoch 133/512
448/448 - 0s - loss: 1.6210e-05 - val_loss: 1.0004e-07
Epoch 134/512
448/448 - 0s - loss: 1.6537e-05 - val_loss: 9.6838e-08
Epoch 135/512
448/448 - 0s - loss: 1.5837e-05 - val_loss: 9.0517e-08
Epoch 136/512
448/448 - 0s - loss: 1.5025e-05 - val_loss: 9.0590e-08
Epoch 137/512
448/448 - 0s - loss: 1.5250e-05 - val_loss: 9.0218e-08
Epoch 138/512
448/448 - 0s - loss: 1.4947e-05 - val_loss: 8.7044e-08
Epoch 139/512
448/448 - 0s - loss: 1.4464e-05 - val_loss: 8.3129e-08
Epoch 140/512
448/448 - 0s - loss: 1.3935e-05 - val_loss: 8.2164e-08
Epoch 141/512
448/448 - 0s - loss: 1.3940e-05 - val_loss: 8.0106e-08
Epoch 142/512
448/448 - 0s - loss: 1.3409e-05 - val_loss: 7.9363e-08
Epoch 143/512
448/448 - 0s - loss: 1.3423e-05 - val_loss: 7.6016e-08
Epoch 144/512
448/448 - 0s - loss: 1.2788e-05 - val_loss: 7.3749e-08
Epoch 145/512
448/448 - 0s - loss: 1.2613e-05 - val_loss: 7.1981e-08
Epoch 146/512
448/448 - 0s - loss: 1.2317e-05 - val_loss: 7.0948e-08
Epoch 147/512
448/448 - 0s - loss: 1.2055e-05 - val_loss: 6.9758e-08
Epoch 148/512
448/448 - 0s - loss: 1.1899e-05 - val_loss: 6.6684e-08
Epoch 149/512
448/448 - 0s - loss: 1.1309e-05 - val_loss: 6.5195e-08
Epoch 150/512
448/448 - 0s - loss: 1.1249e-05 - val_loss: 6.4118e-08
Epoch 151/512
448/448 - 0s - loss: 1.1085e-05 - val_loss: 6.0333e-08
Epoch 152/512
448/448 - 0s - loss: 1.0353e-05 - val_loss: 6.0687e-08
Epoch 153/512
448/448 - 0s - loss: 1.0667e-05 - val_loss: 5.9189e-08
Epoch 154/512
448/448 - 0s - loss: 1.0068e-05 - val_loss: 5.6872e-08
Epoch 155/512
448/448 - 0s - loss: 9.8242e-06 - val_loss: 5.6013e-08
Epoch 156/512
448/448 - 0s - loss: 9.7193e-06 - val_loss: 5.4014e-08
Epoch 157/512
448/448 - 0s - loss: 9.3792e-06 - val_loss: 5.2258e-08
Epoch 158/512
448/448 - 0s - loss: 9.1201e-06 - val_loss: 5.0937e-08
Epoch 159/512
448/448 - 0s - loss: 8.8657e-06 - val_loss: 5.0150e-08
Epoch 160/512
448/448 - 0s - loss: 8.7828e-06 - val_loss: 4.8258e-08
Epoch 161/512
448/448 - 0s - loss: 8.3398e-06 - val_loss: 4.7430e-08
Epoch 162/512
448/448 - 0s - loss: 8.2734e-06 - val_loss: 4.6347e-08
Epoch 163/512
448/448 - 0s - loss: 8.0088e-06 - val_loss: 4.4902e-08
Epoch 164/512
448/448 - 0s - loss: 7.7403e-06 - val_loss: 4.3998e-08
Epoch 165/512
448/448 - 0s - loss: 7.6113e-06 - val_loss: 4.3061e-08
Epoch 166/512
448/448 - 0s - loss: 7.4307e-06 - val_loss: 4.1120e-08
Epoch 167/512
448/448 - 0s - loss: 7.0793e-06 - val_loss: 3.9721e-08
Epoch 168/512
448/448 - 0s - loss: 6.8855e-06 - val_loss: 3.9376e-08
Epoch 169/512
448/448 - 0s - loss: 6.8261e-06 - val_loss: 3.8362e-08
Epoch 170/512
448/448 - 0s - loss: 6.5658e-06 - val_loss: 3.6996e-08
Epoch 171/512
448/448 - 0s - loss: 6.3124e-06 - val_loss: 3.6608e-08
Epoch 172/512
448/448 - 0s - loss: 6.2735e-06 - val_loss: 3.5450e-08
Epoch 173/512
448/448 - 0s - loss: 5.9900e-06 - val_loss: 3.4361e-08
Epoch 174/512
448/448 - 0s - loss: 5.8284e-06 - val_loss: 3.3484e-08
Epoch 175/512
448/448 - 0s - loss: 5.6126e-06 - val_loss: 3.3431e-08
Epoch 176/512
448/448 - 0s - loss: 5.6409e-06 - val_loss: 3.1655e-08
Epoch 177/512
448/448 - 0s - loss: 5.2441e-06 - val_loss: 3.0389e-08
Epoch 178/512
448/448 - 0s - loss: 5.0961e-06 - val_loss: 3.0553e-08
Epoch 179/512
448/448 - 0s - loss: 5.0895e-06 - val_loss: 3.0014e-08
Epoch 180/512
448/448 - 0s - loss: 4.8904e-06 - val_loss: 2.8538e-08
Epoch 181/512
448/448 - 0s - loss: 4.6411e-06 - val_loss: 2.7658e-08
Epoch 182/512
448/448 - 0s - loss: 4.5457e-06 - val_loss: 2.7214e-08
Epoch 183/512
448/448 - 0s - loss: 4.4349e-06 - val_loss: 2.6556e-08
Epoch 184/512
448/448 - 0s - loss: 4.2846e-06 - val_loss: 2.5575e-08
Epoch 185/512
448/448 - 0s - loss: 4.0601e-06 - val_loss: 2.5547e-08
Epoch 186/512
448/448 - 0s - loss: 4.1110e-06 - val_loss: 2.4553e-08
Epoch 187/512
448/448 - 0s - loss: 3.8382e-06 - val_loss: 2.3396e-08
Epoch 188/512
448/448 - 0s - loss: 3.6570e-06 - val_loss: 2.3844e-08
Epoch 189/512
448/448 - 0s - loss: 3.7255e-06 - val_loss: 2.3275e-08
Epoch 190/512
448/448 - 0s - loss: 3.5272e-06 - val_loss: 2.1867e-08
Epoch 191/512
448/448 - 0s - loss: 3.2914e-06 - val_loss: 2.1727e-08
Epoch 192/512
448/448 - 0s - loss: 3.3403e-06 - val_loss: 2.1227e-08
Epoch 193/512
448/448 - 0s - loss: 3.1684e-06 - val_loss: 2.0367e-08
Epoch 194/512
448/448 - 0s - loss: 2.9994e-06 - val_loss: 2.0261e-08
Epoch 195/512
448/448 - 0s - loss: 3.0263e-06 - val_loss: 1.9449e-08
Epoch 196/512
448/448 - 0s - loss: 2.8218e-06 - val_loss: 1.8791e-08
Epoch 197/512
448/448 - 0s - loss: 2.7264e-06 - val_loss: 1.8676e-08
Epoch 198/512
448/448 - 0s - loss: 2.6842e-06 - val_loss: 1.8469e-08
Epoch 199/512
448/448 - 0s - loss: 2.6167e-06 - val_loss: 1.7310e-08
Epoch 200/512
448/448 - 0s - loss: 2.4180e-06 - val_loss: 1.6706e-08
Epoch 201/512
448/448 - 0s - loss: 2.3657e-06 - val_loss: 1.6773e-08
Epoch 202/512
448/448 - 0s - loss: 2.3275e-06 - val_loss: 1.6669e-08
Epoch 203/512
448/448 - 0s - loss: 2.2672e-06 - val_loss: 1.5739e-08
Epoch 204/512
448/448 - 0s - loss: 2.1026e-06 - val_loss: 1.5192e-08
Epoch 205/512
448/448 - 0s - loss: 2.0340e-06 - val_loss: 1.5399e-08
Epoch 206/512
448/448 - 0s - loss: 2.0572e-06 - val_loss: 1.4718e-08
Epoch 207/512
448/448 - 0s - loss: 1.8916e-06 - val_loss: 1.4036e-08
Epoch 208/512
448/448 - 0s - loss: 1.7934e-06 - val_loss: 1.4215e-08
Epoch 209/512
448/448 - 0s - loss: 1.8368e-06 - val_loss: 1.3870e-08
Epoch 210/512
448/448 - 0s - loss: 1.7087e-06 - val_loss: 1.3272e-08
Epoch 211/512
448/448 - 0s - loss: 1.6319e-06 - val_loss: 1.2986e-08
Epoch 212/512
448/448 - 0s - loss: 1.5896e-06 - val_loss: 1.2793e-08
Epoch 213/512
448/448 - 0s - loss: 1.5424e-06 - val_loss: 1.2353e-08
Epoch 214/512
448/448 - 0s - loss: 1.4600e-06 - val_loss: 1.1924e-08
Epoch 215/512
448/448 - 0s - loss: 1.3927e-06 - val_loss: 1.1914e-08
Epoch 216/512
448/448 - 0s - loss: 1.3788e-06 - val_loss: 1.1814e-08
Epoch 217/512
448/448 - 0s - loss: 1.3499e-06 - val_loss: 1.0859e-08
Epoch 218/512
448/448 - 0s - loss: 1.1855e-06 - val_loss: 1.0746e-08
Epoch 219/512
448/448 - 0s - loss: 1.2110e-06 - val_loss: 1.1070e-08
Epoch 220/512
448/448 - 0s - loss: 1.2248e-06 - val_loss: 1.0383e-08
Epoch 221/512
448/448 - 0s - loss: 1.0782e-06 - val_loss: 9.9653e-09
Epoch 222/512
448/448 - 0s - loss: 1.0492e-06 - val_loss: 1.0153e-08
Epoch 223/512
448/448 - 0s - loss: 1.0718e-06 - val_loss: 9.8748e-09
Epoch 224/512
448/448 - 0s - loss: 9.9466e-07 - val_loss: 9.3242e-09
Epoch 225/512
448/448 - 0s - loss: 9.2918e-07 - val_loss: 9.1355e-09
Epoch 226/512
448/448 - 0s - loss: 9.0705e-07 - val_loss: 9.1527e-09
Epoch 227/512
448/448 - 0s - loss: 8.9675e-07 - val_loss: 8.8526e-09
Epoch 228/512
448/448 - 0s - loss: 8.3683e-07 - val_loss: 8.4698e-09
Epoch 229/512
448/448 - 0s - loss: 7.8967e-07 - val_loss: 8.4774e-09
Epoch 230/512
448/448 - 0s - loss: 7.9325e-07 - val_loss: 8.2684e-09
Epoch 231/512
448/448 - 0s - loss: 7.4200e-07 - val_loss: 7.9322e-09
Epoch 232/512
448/448 - 0s - loss: 7.0371e-07 - val_loss: 7.8251e-09
Epoch 233/512
448/448 - 0s - loss: 6.8713e-07 - val_loss: 7.7220e-09
Epoch 234/512
448/448 - 0s - loss: 6.6259e-07 - val_loss: 7.4165e-09
Epoch 235/512
448/448 - 0s - loss: 6.1743e-07 - val_loss: 7.3120e-09
Epoch 236/512
448/448 - 0s - loss: 6.0559e-07 - val_loss: 7.2308e-09
Epoch 237/512
448/448 - 0s - loss: 5.8930e-07 - val_loss: 6.9277e-09
Epoch 238/512
448/448 - 0s - loss: 5.4429e-07 - val_loss: 6.7710e-09
Epoch 239/512
448/448 - 0s - loss: 5.2731e-07 - val_loss: 6.7324e-09
Epoch 240/512
448/448 - 0s - loss: 5.1542e-07 - val_loss: 6.5807e-09
Epoch 241/512
448/448 - 0s - loss: 4.8921e-07 - val_loss: 6.3250e-09
Epoch 242/512
448/448 - 0s - loss: 4.5542e-07 - val_loss: 6.3147e-09
Epoch 243/512
448/448 - 0s - loss: 4.6126e-07 - val_loss: 6.1017e-09
Epoch 244/512
448/448 - 0s - loss: 4.2164e-07 - val_loss: 5.9359e-09
Epoch 245/512
448/448 - 0s - loss: 4.0763e-07 - val_loss: 5.8287e-09
Epoch 246/512
448/448 - 0s - loss: 3.9106e-07 - val_loss: 5.7591e-09
Epoch 247/512
448/448 - 0s - loss: 3.7906e-07 - val_loss: 5.6386e-09
Epoch 248/512
448/448 - 0s - loss: 3.6166e-07 - val_loss: 5.4737e-09
Epoch 249/512
448/448 - 0s - loss: 3.3886e-07 - val_loss: 5.3867e-09
Epoch 250/512
448/448 - 0s - loss: 3.3582e-07 - val_loss: 5.2346e-09
Epoch 251/512
448/448 - 0s - loss: 3.1166e-07 - val_loss: 5.0902e-09
Epoch 252/512
448/448 - 0s - loss: 2.9638e-07 - val_loss: 5.0568e-09
Epoch 253/512
448/448 - 0s - loss: 2.9402e-07 - val_loss: 4.9606e-09
Epoch 254/512
448/448 - 0s - loss: 2.7691e-07 - val_loss: 4.7846e-09
Epoch 255/512
448/448 - 0s - loss: 2.5772e-07 - val_loss: 4.7427e-09
Epoch 256/512
448/448 - 0s - loss: 2.5674e-07 - val_loss: 4.6487e-09
Epoch 257/512
448/448 - 0s - loss: 2.4232e-07 - val_loss: 4.5045e-09
Epoch 258/512
448/448 - 0s - loss: 2.2690e-07 - val_loss: 4.4296e-09
Epoch 259/512
448/448 - 0s - loss: 2.2051e-07 - val_loss: 4.3806e-09
Epoch 260/512
448/448 - 0s - loss: 2.1306e-07 - val_loss: 4.2988e-09
Epoch 261/512
448/448 - 0s - loss: 2.0374e-07 - val_loss: 4.1552e-09
Epoch 262/512
448/448 - 0s - loss: 1.8986e-07 - val_loss: 4.0408e-09
Epoch 263/512
448/448 - 0s - loss: 1.8068e-07 - val_loss: 4.0206e-09
Epoch 264/512
448/448 - 0s - loss: 1.7856e-07 - val_loss: 3.9646e-09
Epoch 265/512
448/448 - 0s - loss: 1.7075e-07 - val_loss: 3.8284e-09
Epoch 266/512
448/448 - 0s - loss: 1.5649e-07 - val_loss: 3.7539e-09
Epoch 267/512
448/448 - 0s - loss: 1.5234e-07 - val_loss: 3.7325e-09
Epoch 268/512
448/448 - 0s - loss: 1.4822e-07 - val_loss: 3.6866e-09
Epoch 269/512
448/448 - 0s - loss: 1.4173e-07 - val_loss: 3.5989e-09
Epoch 270/512
448/448 - 0s - loss: 1.3525e-07 - val_loss: 3.4639e-09
Epoch 271/512
448/448 - 0s - loss: 1.2369e-07 - val_loss: 3.4186e-09
Epoch 272/512
448/448 - 0s - loss: 1.2143e-07 - val_loss: 3.4415e-09
Epoch 273/512
448/448 - 0s - loss: 1.2277e-07 - val_loss: 3.3260e-09
Epoch 274/512
448/448 - 0s - loss: 1.0954e-07 - val_loss: 3.2100e-09
Epoch 275/512
448/448 - 0s - loss: 1.0233e-07 - val_loss: 3.1974e-09
Epoch 276/512
448/448 - 0s - loss: 1.0397e-07 - val_loss: 3.1753e-09
Epoch 277/512
448/448 - 0s - loss: 1.0053e-07 - val_loss: 3.0575e-09
Epoch 278/512
448/448 - 0s - loss: 8.9067e-08 - val_loss: 3.0106e-09
Epoch 279/512
448/448 - 0s - loss: 8.8567e-08 - val_loss: 2.9986e-09
Epoch 280/512
448/448 - 0s - loss: 8.7263e-08 - val_loss: 2.9167e-09
Epoch 281/512
448/448 - 0s - loss: 7.9154e-08 - val_loss: 2.8551e-09
Epoch 282/512
448/448 - 0s - loss: 7.6576e-08 - val_loss: 2.8377e-09
Epoch 283/512
448/448 - 0s - loss: 7.5887e-08 - val_loss: 2.7817e-09
Epoch 284/512
448/448 - 0s - loss: 7.0496e-08 - val_loss: 2.7058e-09
Epoch 285/512
448/448 - 0s - loss: 6.5521e-08 - val_loss: 2.6848e-09
Epoch 286/512
448/448 - 0s - loss: 6.5630e-08 - val_loss: 2.6509e-09
Epoch 287/512
448/448 - 0s - loss: 6.2076e-08 - val_loss: 2.5870e-09
Epoch 288/512
448/448 - 0s - loss: 5.7662e-08 - val_loss: 2.5460e-09
Epoch 289/512
448/448 - 0s - loss: 5.5826e-08 - val_loss: 2.5188e-09
Epoch 290/512
448/448 - 0s - loss: 5.4362e-08 - val_loss: 2.4776e-09
Epoch 291/512
448/448 - 0s - loss: 5.1244e-08 - val_loss: 2.4237e-09
Epoch 292/512
448/448 - 0s - loss: 4.7989e-08 - val_loss: 2.4084e-09
Epoch 293/512
448/448 - 0s - loss: 4.7916e-08 - val_loss: 2.3740e-09
Epoch 294/512
448/448 - 0s - loss: 4.5159e-08 - val_loss: 2.3148e-09
Epoch 295/512
448/448 - 0s - loss: 4.1722e-08 - val_loss: 2.2919e-09
Epoch 296/512
448/448 - 0s - loss: 4.1548e-08 - val_loss: 2.2644e-09
Epoch 297/512
448/448 - 0s - loss: 3.9567e-08 - val_loss: 2.2210e-09
Epoch 298/512
448/448 - 0s - loss: 3.7213e-08 - val_loss: 2.1775e-09
Epoch 299/512
448/448 - 0s - loss: 3.5123e-08 - val_loss: 2.1617e-09
Epoch 300/512
448/448 - 0s - loss: 3.4765e-08 - val_loss: 2.1394e-09
Epoch 301/512
448/448 - 0s - loss: 3.3497e-08 - val_loss: 2.0898e-09
Epoch 302/512
448/448 - 0s - loss: 3.0250e-08 - val_loss: 2.0701e-09
Epoch 303/512
448/448 - 0s - loss: 3.0415e-08 - val_loss: 2.0437e-09
Epoch 304/512
448/448 - 0s - loss: 2.8504e-08 - val_loss: 2.0291e-09
Epoch 305/512
448/448 - 0s - loss: 2.8342e-08 - val_loss: 1.9919e-09
Epoch 306/512
448/448 - 0s - loss: 2.6255e-08 - val_loss: 1.9531e-09
Epoch 307/512
448/448 - 0s - loss: 2.4765e-08 - val_loss: 1.9236e-09
Epoch 308/512
448/448 - 0s - loss: 2.3705e-08 - val_loss: 1.9133e-09
Epoch 309/512
448/448 - 0s - loss: 2.3531e-08 - val_loss: 1.8956e-09
Epoch 310/512
448/448 - 0s - loss: 2.2745e-08 - val_loss: 1.8571e-09
Epoch 311/512
448/448 - 0s - loss: 2.0806e-08 - val_loss: 1.8278e-09
Epoch 312/512
448/448 - 0s - loss: 1.9803e-08 - val_loss: 1.8185e-09
Epoch 313/512
448/448 - 0s - loss: 2.0097e-08 - val_loss: 1.7991e-09
Epoch 314/512
448/448 - 0s - loss: 1.8993e-08 - val_loss: 1.7628e-09
Epoch 315/512
448/448 - 0s - loss: 1.7438e-08 - val_loss: 1.7429e-09
Epoch 316/512
448/448 - 0s - loss: 1.7086e-08 - val_loss: 1.7311e-09
Epoch 317/512
448/448 - 0s - loss: 1.6952e-08 - val_loss: 1.7091e-09
Epoch 318/512
448/448 - 0s - loss: 1.5949e-08 - val_loss: 1.6838e-09
Epoch 319/512
448/448 - 0s - loss: 1.5163e-08 - val_loss: 1.6623e-09
Epoch 320/512
448/448 - 0s - loss: 1.4424e-08 - val_loss: 1.6540e-09
Epoch 321/512
448/448 - 0s - loss: 1.4426e-08 - val_loss: 1.6331e-09
Epoch 322/512
448/448 - 0s - loss: 1.3607e-08 - val_loss: 1.6081e-09
Epoch 323/512
448/448 - 0s - loss: 1.2935e-08 - val_loss: 1.5881e-09
Epoch 324/512
448/448 - 0s - loss: 1.2336e-08 - val_loss: 1.5760e-09
Epoch 325/512
448/448 - 0s - loss: 1.2241e-08 - val_loss: 1.5584e-09
Epoch 326/512
448/448 - 0s - loss: 1.1657e-08 - val_loss: 1.5371e-09
Epoch 327/512
448/448 - 0s - loss: 1.1077e-08 - val_loss: 1.5166e-09
Epoch 328/512
448/448 - 0s - loss: 1.0408e-08 - val_loss: 1.5071e-09
Epoch 329/512
448/448 - 0s - loss: 1.0449e-08 - val_loss: 1.4974e-09
Epoch 330/512
448/448 - 0s - loss: 1.0377e-08 - val_loss: 1.4770e-09
Epoch 331/512
448/448 - 0s - loss: 9.5362e-09 - val_loss: 1.4550e-09
Epoch 332/512
448/448 - 0s - loss: 9.0663e-09 - val_loss: 1.4441e-09
Epoch 333/512
448/448 - 0s - loss: 8.9455e-09 - val_loss: 1.4323e-09
Epoch 334/512
448/448 - 0s - loss: 8.7559e-09 - val_loss: 1.4170e-09
Epoch 335/512
448/448 - 0s - loss: 8.3729e-09 - val_loss: 1.3991e-09
Epoch 336/512
448/448 - 0s - loss: 7.9028e-09 - val_loss: 1.3877e-09
Epoch 337/512
448/448 - 0s - loss: 7.7784e-09 - val_loss: 1.3761e-09
Epoch 338/512
448/448 - 0s - loss: 7.5892e-09 - val_loss: 1.3585e-09
Epoch 339/512
448/448 - 0s - loss: 7.1193e-09 - val_loss: 1.3465e-09
Epoch 340/512
448/448 - 0s - loss: 7.1175e-09 - val_loss: 1.3356e-09
Epoch 341/512
448/448 - 0s - loss: 6.7809e-09 - val_loss: 1.3216e-09
Epoch 342/512
448/448 - 0s - loss: 6.6538e-09 - val_loss: 1.3060e-09
Epoch 343/512
448/448 - 0s - loss: 6.2898e-09 - val_loss: 1.2943e-09
Epoch 344/512
448/448 - 0s - loss: 6.0607e-09 - val_loss: 1.2841e-09
Epoch 345/512
448/448 - 0s - loss: 6.0913e-09 - val_loss: 1.2694e-09
Epoch 346/512
448/448 - 0s - loss: 5.6737e-09 - val_loss: 1.2580e-09
Epoch 347/512
448/448 - 0s - loss: 5.5854e-09 - val_loss: 1.2464e-09
Epoch 348/512
448/448 - 0s - loss: 5.4116e-09 - val_loss: 1.2357e-09
Epoch 349/512
448/448 - 0s - loss: 5.2405e-09 - val_loss: 1.2258e-09
Epoch 350/512
448/448 - 0s - loss: 5.0848e-09 - val_loss: 1.2142e-09
Epoch 351/512
448/448 - 0s - loss: 4.9814e-09 - val_loss: 1.2042e-09
Epoch 352/512
448/448 - 0s - loss: 4.8344e-09 - val_loss: 1.1910e-09
Epoch 353/512
448/448 - 0s - loss: 4.5750e-09 - val_loss: 1.1810e-09
Epoch 354/512
448/448 - 0s - loss: 4.4811e-09 - val_loss: 1.1743e-09
Epoch 355/512
448/448 - 0s - loss: 4.5094e-09 - val_loss: 1.1639e-09
Epoch 356/512
448/448 - 0s - loss: 4.3243e-09 - val_loss: 1.1522e-09
Epoch 357/512
448/448 - 0s - loss: 4.1620e-09 - val_loss: 1.1403e-09
Epoch 358/512
448/448 - 0s - loss: 3.9725e-09 - val_loss: 1.1331e-09
Epoch 359/512
448/448 - 0s - loss: 4.0109e-09 - val_loss: 1.1222e-09
Epoch 360/512
448/448 - 0s - loss: 3.7831e-09 - val_loss: 1.1118e-09
Epoch 361/512
448/448 - 0s - loss: 3.7180e-09 - val_loss: 1.1040e-09
Epoch 362/512
448/448 - 0s - loss: 3.6617e-09 - val_loss: 1.0960e-09
Epoch 363/512
448/448 - 0s - loss: 3.5686e-09 - val_loss: 1.0879e-09
Epoch 364/512
448/448 - 0s - loss: 3.5054e-09 - val_loss: 1.0782e-09
Epoch 365/512
448/448 - 0s - loss: 3.3745e-09 - val_loss: 1.0657e-09
Epoch 366/512
448/448 - 0s - loss: 3.1769e-09 - val_loss: 1.0589e-09
Epoch 367/512
448/448 - 0s - loss: 3.1993e-09 - val_loss: 1.0541e-09
Epoch 368/512
448/448 - 0s - loss: 3.2206e-09 - val_loss: 1.0457e-09
Epoch 369/512
448/448 - 0s - loss: 3.1050e-09 - val_loss: 1.0336e-09
Epoch 370/512
448/448 - 0s - loss: 2.9035e-09 - val_loss: 1.0251e-09
Epoch 371/512
448/448 - 0s - loss: 2.8547e-09 - val_loss: 1.0190e-09
Epoch 372/512
448/448 - 0s - loss: 2.8721e-09 - val_loss: 1.0117e-09
Epoch 373/512
448/448 - 0s - loss: 2.8004e-09 - val_loss: 1.0068e-09
Epoch 374/512
448/448 - 0s - loss: 2.7954e-09 - val_loss: 9.9564e-10
Epoch 375/512
448/448 - 0s - loss: 2.6369e-09 - val_loss: 9.8691e-10
Epoch 376/512
448/448 - 0s - loss: 2.5382e-09 - val_loss: 9.7797e-10
Epoch 377/512
448/448 - 0s - loss: 2.4670e-09 - val_loss: 9.7242e-10
Epoch 378/512
448/448 - 0s - loss: 2.4837e-09 - val_loss: 9.6863e-10
Epoch 379/512
448/448 - 0s - loss: 2.4965e-09 - val_loss: 9.6206e-10
Epoch 380/512
448/448 - 0s - loss: 2.4389e-09 - val_loss: 9.5397e-10
Epoch 381/512
448/448 - 0s - loss: 2.3303e-09 - val_loss: 9.4298e-10
Epoch 382/512
448/448 - 0s - loss: 2.1670e-09 - val_loss: 9.3614e-10
Epoch 383/512
448/448 - 0s - loss: 2.1722e-09 - val_loss: 9.3090e-10
Epoch 384/512
448/448 - 0s - loss: 2.1776e-09 - val_loss: 9.2635e-10
Epoch 385/512
448/448 - 0s - loss: 2.1637e-09 - val_loss: 9.1982e-10
Epoch 386/512
448/448 - 0s - loss: 2.1258e-09 - val_loss: 9.1215e-10
Epoch 387/512
448/448 - 0s - loss: 2.0538e-09 - val_loss: 9.0591e-10
Epoch 388/512
448/448 - 0s - loss: 2.0069e-09 - val_loss: 8.9973e-10
Epoch 389/512
448/448 - 0s - loss: 1.9935e-09 - val_loss: 8.9450e-10
Epoch 390/512
448/448 - 0s - loss: 1.9669e-09 - val_loss: 8.8787e-10
Epoch 391/512
448/448 - 0s - loss: 1.8915e-09 - val_loss: 8.8048e-10
Epoch 392/512
448/448 - 0s - loss: 1.8297e-09 - val_loss: 8.7572e-10
Epoch 393/512
448/448 - 0s - loss: 1.8449e-09 - val_loss: 8.6996e-10
Epoch 394/512
448/448 - 0s - loss: 1.8146e-09 - val_loss: 8.6412e-10
Epoch 395/512
448/448 - 0s - loss: 1.7768e-09 - val_loss: 8.5804e-10
Epoch 396/512
448/448 - 0s - loss: 1.7450e-09 - val_loss: 8.5258e-10
Epoch 397/512
448/448 - 0s - loss: 1.6978e-09 - val_loss: 8.4628e-10
Epoch 398/512
448/448 - 0s - loss: 1.6758e-09 - val_loss: 8.4106e-10
Epoch 399/512
448/448 - 0s - loss: 1.6307e-09 - val_loss: 8.3627e-10
Epoch 400/512
448/448 - 0s - loss: 1.6122e-09 - val_loss: 8.3066e-10
Epoch 401/512
448/448 - 0s - loss: 1.5862e-09 - val_loss: 8.2549e-10
Epoch 402/512
448/448 - 0s - loss: 1.5661e-09 - val_loss: 8.2114e-10
Epoch 403/512
448/448 - 0s - loss: 1.5617e-09 - val_loss: 8.1567e-10
Epoch 404/512
448/448 - 0s - loss: 1.5271e-09 - val_loss: 8.0928e-10
Epoch 405/512
448/448 - 0s - loss: 1.4782e-09 - val_loss: 8.0544e-10
Epoch 406/512
448/448 - 0s - loss: 1.4580e-09 - val_loss: 7.9912e-10
Epoch 407/512
448/448 - 0s - loss: 1.4288e-09 - val_loss: 7.9438e-10
Epoch 408/512
448/448 - 0s - loss: 1.4097e-09 - val_loss: 7.9034e-10
Epoch 409/512
448/448 - 0s - loss: 1.4040e-09 - val_loss: 7.8547e-10
Epoch 410/512
448/448 - 0s - loss: 1.3784e-09 - val_loss: 7.8112e-10
Epoch 411/512
448/448 - 0s - loss: 1.3687e-09 - val_loss: 7.7692e-10
Epoch 412/512
448/448 - 0s - loss: 1.3535e-09 - val_loss: 7.7068e-10
Epoch 413/512
448/448 - 0s - loss: 1.2851e-09 - val_loss: 7.6690e-10
Epoch 414/512
448/448 - 0s - loss: 1.2905e-09 - val_loss: 7.6167e-10
Epoch 415/512
448/448 - 0s - loss: 1.2586e-09 - val_loss: 7.5676e-10
Epoch 416/512
448/448 - 0s - loss: 1.2261e-09 - val_loss: 7.5259e-10
Epoch 417/512
448/448 - 0s - loss: 1.2397e-09 - val_loss: 7.4955e-10
Epoch 418/512
448/448 - 0s - loss: 1.2390e-09 - val_loss: 7.4536e-10
Epoch 419/512
448/448 - 0s - loss: 1.2202e-09 - val_loss: 7.4013e-10
Epoch 420/512
448/448 - 0s - loss: 1.1853e-09 - val_loss: 7.3623e-10
Epoch 421/512
448/448 - 0s - loss: 1.1770e-09 - val_loss: 7.3196e-10
Epoch 422/512
448/448 - 0s - loss: 1.1537e-09 - val_loss: 7.2687e-10
Epoch 423/512
448/448 - 0s - loss: 1.1262e-09 - val_loss: 7.2340e-10
Epoch 424/512
448/448 - 0s - loss: 1.1177e-09 - val_loss: 7.1985e-10
Epoch 425/512
448/448 - 0s - loss: 1.1130e-09 - val_loss: 7.1570e-10
Epoch 426/512
448/448 - 0s - loss: 1.0977e-09 - val_loss: 7.1179e-10
Epoch 427/512
448/448 - 0s - loss: 1.0866e-09 - val_loss: 7.0822e-10
Epoch 428/512
448/448 - 0s - loss: 1.0674e-09 - val_loss: 7.0461e-10
Epoch 429/512
448/448 - 0s - loss: 1.0638e-09 - val_loss: 7.0056e-10
Epoch 430/512
448/448 - 0s - loss: 1.0359e-09 - val_loss: 6.9741e-10
Epoch 431/512
448/448 - 0s - loss: 1.0358e-09 - val_loss: 6.9264e-10
Epoch 432/512
448/448 - 0s - loss: 1.0039e-09 - val_loss: 6.8811e-10
Epoch 433/512
448/448 - 0s - loss: 9.8457e-10 - val_loss: 6.8445e-10
Epoch 434/512
448/448 - 0s - loss: 9.8412e-10 - val_loss: 6.8190e-10
Epoch 435/512
448/448 - 0s - loss: 9.7699e-10 - val_loss: 6.7935e-10
Epoch 436/512
448/448 - 0s - loss: 9.9007e-10 - val_loss: 6.7567e-10
Epoch 437/512
448/448 - 0s - loss: 9.8139e-10 - val_loss: 6.7087e-10
Epoch 438/512
448/448 - 0s - loss: 9.3322e-10 - val_loss: 6.6658e-10
Epoch 439/512
448/448 - 0s - loss: 9.1670e-10 - val_loss: 6.6320e-10
Epoch 440/512
448/448 - 0s - loss: 9.0548e-10 - val_loss: 6.6066e-10
Epoch 441/512
448/448 - 0s - loss: 9.0608e-10 - val_loss: 6.5763e-10
Epoch 442/512
448/448 - 0s - loss: 8.9866e-10 - val_loss: 6.5508e-10
Epoch 443/512
448/448 - 0s - loss: 9.1799e-10 - val_loss: 6.5142e-10
Epoch 444/512
448/448 - 0s - loss: 8.9401e-10 - val_loss: 6.4797e-10
Epoch 445/512
448/448 - 0s - loss: 8.8364e-10 - val_loss: 6.4421e-10
Epoch 446/512
448/448 - 0s - loss: 8.4098e-10 - val_loss: 6.4068e-10
Epoch 447/512
448/448 - 0s - loss: 8.4073e-10 - val_loss: 6.3763e-10
Epoch 448/512
448/448 - 0s - loss: 8.3560e-10 - val_loss: 6.3537e-10
Epoch 449/512
448/448 - 0s - loss: 8.3773e-10 - val_loss: 6.3108e-10
Epoch 450/512
448/448 - 0s - loss: 8.0798e-10 - val_loss: 6.2931e-10
Epoch 451/512
448/448 - 0s - loss: 8.3181e-10 - val_loss: 6.2635e-10
Epoch 452/512
448/448 - 0s - loss: 8.0516e-10 - val_loss: 6.2259e-10
Epoch 453/512
448/448 - 0s - loss: 7.9868e-10 - val_loss: 6.1981e-10
Epoch 454/512
448/448 - 0s - loss: 7.8619e-10 - val_loss: 6.1696e-10
Epoch 455/512
448/448 - 0s - loss: 7.8664e-10 - val_loss: 6.1344e-10
Epoch 456/512
448/448 - 0s - loss: 7.7680e-10 - val_loss: 6.1068e-10
Epoch 457/512
448/448 - 0s - loss: 7.6027e-10 - val_loss: 6.0777e-10
Epoch 458/512
448/448 - 0s - loss: 7.5051e-10 - val_loss: 6.0457e-10
Epoch 459/512
448/448 - 0s - loss: 7.4527e-10 - val_loss: 6.0263e-10
Epoch 460/512
448/448 - 0s - loss: 7.4754e-10 - val_loss: 6.0090e-10
Epoch 461/512
448/448 - 0s - loss: 7.5576e-10 - val_loss: 5.9768e-10
Epoch 462/512
448/448 - 0s - loss: 7.3350e-10 - val_loss: 5.9442e-10
Epoch 463/512
448/448 - 0s - loss: 7.2329e-10 - val_loss: 5.9016e-10
Epoch 464/512
448/448 - 0s - loss: 7.0332e-10 - val_loss: 5.8780e-10
Epoch 465/512
448/448 - 0s - loss: 6.9395e-10 - val_loss: 5.8569e-10
Epoch 466/512
448/448 - 0s - loss: 7.0387e-10 - val_loss: 5.8419e-10
Epoch 467/512
448/448 - 0s - loss: 7.0830e-10 - val_loss: 5.8254e-10
Epoch 468/512
448/448 - 0s - loss: 7.1324e-10 - val_loss: 5.7970e-10
Epoch 469/512
448/448 - 0s - loss: 7.1569e-10 - val_loss: 5.7699e-10
Epoch 470/512
448/448 - 0s - loss: 6.9110e-10 - val_loss: 5.7293e-10
Epoch 471/512
448/448 - 0s - loss: 6.5135e-10 - val_loss: 5.6961e-10
Epoch 472/512
448/448 - 0s - loss: 6.4000e-10 - val_loss: 5.6721e-10
Epoch 473/512
448/448 - 0s - loss: 6.3869e-10 - val_loss: 5.6592e-10
Epoch 474/512
448/448 - 0s - loss: 6.4642e-10 - val_loss: 5.6413e-10
Epoch 475/512
448/448 - 0s - loss: 6.7127e-10 - val_loss: 5.6156e-10
Epoch 476/512
448/448 - 0s - loss: 6.5769e-10 - val_loss: 5.5822e-10
Epoch 477/512
448/448 - 0s - loss: 6.3229e-10 - val_loss: 5.5524e-10
Epoch 478/512
448/448 - 0s - loss: 6.1225e-10 - val_loss: 5.5302e-10
Epoch 479/512
448/448 - 0s - loss: 6.0898e-10 - val_loss: 5.5084e-10
Epoch 480/512
448/448 - 0s - loss: 6.2081e-10 - val_loss: 5.4875e-10
Epoch 481/512
448/448 - 0s - loss: 6.1783e-10 - val_loss: 5.4704e-10
Epoch 482/512
448/448 - 0s - loss: 6.1183e-10 - val_loss: 5.4435e-10
Epoch 483/512
448/448 - 0s - loss: 6.0972e-10 - val_loss: 5.4264e-10
Epoch 484/512
448/448 - 0s - loss: 6.0096e-10 - val_loss: 5.3987e-10
Epoch 485/512
448/448 - 0s - loss: 6.0899e-10 - val_loss: 5.3821e-10
Epoch 486/512
448/448 - 0s - loss: 5.9943e-10 - val_loss: 5.3545e-10
Epoch 487/512
448/448 - 0s - loss: 5.8473e-10 - val_loss: 5.3211e-10
Epoch 488/512
448/448 - 0s - loss: 5.6273e-10 - val_loss: 5.2973e-10
Epoch 489/512
448/448 - 0s - loss: 5.5004e-10 - val_loss: 5.2778e-10
Epoch 490/512
448/448 - 0s - loss: 5.5245e-10 - val_loss: 5.2609e-10
Epoch 491/512
448/448 - 0s - loss: 5.7407e-10 - val_loss: 5.2481e-10
Epoch 492/512
448/448 - 0s - loss: 5.7491e-10 - val_loss: 5.2235e-10
Epoch 493/512
448/448 - 0s - loss: 5.6081e-10 - val_loss: 5.2060e-10
Epoch 494/512
448/448 - 0s - loss: 5.4725e-10 - val_loss: 5.1763e-10
Epoch 495/512
448/448 - 0s - loss: 5.3756e-10 - val_loss: 5.1525e-10
Epoch 496/512
448/448 - 0s - loss: 5.3502e-10 - val_loss: 5.1450e-10
Epoch 497/512
448/448 - 0s - loss: 5.4004e-10 - val_loss: 5.1257e-10
Epoch 498/512
448/448 - 0s - loss: 5.4471e-10 - val_loss: 5.0990e-10
Epoch 499/512
448/448 - 0s - loss: 5.3346e-10 - val_loss: 5.0802e-10
Epoch 500/512
448/448 - 0s - loss: 5.3500e-10 - val_loss: 5.0617e-10
Epoch 501/512
448/448 - 0s - loss: 5.3112e-10 - val_loss: 5.0361e-10
Epoch 502/512
448/448 - 0s - loss: 5.0963e-10 - val_loss: 5.0148e-10
Epoch 503/512
448/448 - 0s - loss: 5.0608e-10 - val_loss: 4.9970e-10
Epoch 504/512
448/448 - 0s - loss: 5.0520e-10 - val_loss: 4.9787e-10
Epoch 505/512
448/448 - 0s - loss: 5.0906e-10 - val_loss: 4.9615e-10
Epoch 506/512
448/448 - 0s - loss: 5.0229e-10 - val_loss: 4.9423e-10
Epoch 507/512
448/448 - 0s - loss: 5.0008e-10 - val_loss: 4.9224e-10
Epoch 508/512
448/448 - 0s - loss: 4.8976e-10 - val_loss: 4.9074e-10
Epoch 509/512
448/448 - 0s - loss: 4.8130e-10 - val_loss: 4.8825e-10
Epoch 510/512
448/448 - 0s - loss: 4.8873e-10 - val_loss: 4.8701e-10
Epoch 511/512
448/448 - 0s - loss: 4.9301e-10 - val_loss: 4.8600e-10
Epoch 512/512
448/448 - 0s - loss: 4.9355e-10 - val_loss: 4.8412e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4094e-08 - val_loss: 2.6518e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5567e-08 - val_loss: 1.6225e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.8234e-10 - val_loss: 3.5256e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.2409e-10 - val_loss: 3.5180e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9731e-10 - val_loss: 1.1618e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4730e-09 - val_loss: 7.1314e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6396e-09 - val_loss: 7.8954e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0816e-09 - val_loss: 2.5901e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1464e-09 - val_loss: 1.5521e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6800e-09 - val_loss: 2.1369e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8053e-09 - val_loss: 4.2186e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9867e-09 - val_loss: 4.9809e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6808e-09 - val_loss: 3.2450e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9815e-09 - val_loss: 2.3269e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3713e-09 - val_loss: 2.4276e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7234e-09 - val_loss: 3.1008e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4152e-09 - val_loss: 3.4330e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4658e-09 - val_loss: 2.9590e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8906e-09 - val_loss: 2.4771e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5119e-09 - val_loss: 2.3776e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5203e-09 - val_loss: 2.5641e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7068e-09 - val_loss: 2.6730e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7537e-09 - val_loss: 2.5397e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5715e-09 - val_loss: 2.3283e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3642e-09 - val_loss: 2.2044e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2832e-09 - val_loss: 2.2141e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3044e-09 - val_loss: 2.2333e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3098e-09 - val_loss: 2.1698e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2211e-09 - val_loss: 2.0795e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1280e-09 - val_loss: 2.0069e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0672e-09 - val_loss: 1.9771e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0295e-09 - val_loss: 1.9395e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9996e-09 - val_loss: 1.8740e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9241e-09 - val_loss: 1.8278e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8861e-09 - val_loss: 1.8049e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8617e-09 - val_loss: 1.7595e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7993e-09 - val_loss: 1.7096e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7569e-09 - val_loss: 1.6747e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7225e-09 - val_loss: 1.6230e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6587e-09 - val_loss: 1.5997e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6464e-09 - val_loss: 1.5834e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6195e-09 - val_loss: 1.5325e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5662e-09 - val_loss: 1.4779e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5205e-09 - val_loss: 1.4542e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4883e-09 - val_loss: 1.4295e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4745e-09 - val_loss: 1.4168e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4506e-09 - val_loss: 1.3829e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4114e-09 - val_loss: 1.3371e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3732e-09 - val_loss: 1.3063e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3346e-09 - val_loss: 1.2727e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3095e-09 - val_loss: 1.2685e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3063e-09 - val_loss: 1.2496e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2799e-09 - val_loss: 1.2167e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2387e-09 - val_loss: 1.1673e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1940e-09 - val_loss: 1.1604e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1913e-09 - val_loss: 1.1554e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1846e-09 - val_loss: 1.1356e-09
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1641e-09 - val_loss: 1.1066e-09
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1299e-09 - val_loss: 1.0801e-09
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1051e-09 - val_loss: 1.0524e-09
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0774e-09 - val_loss: 1.0328e-09
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0565e-09 - val_loss: 1.0235e-09
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0482e-09 - val_loss: 1.0086e-09
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0320e-09 - val_loss: 9.7888e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9569e-10 - val_loss: 9.5813e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8025e-10 - val_loss: 9.4806e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6992e-10 - val_loss: 9.4231e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6253e-10 - val_loss: 9.1972e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3872e-10 - val_loss: 9.0830e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2718e-10 - val_loss: 8.8878e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0537e-10 - val_loss: 8.6910e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8540e-10 - val_loss: 8.4858e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6323e-10 - val_loss: 8.3796e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5926e-10 - val_loss: 8.4042e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5909e-10 - val_loss: 8.2050e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3496e-10 - val_loss: 8.0413e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1915e-10 - val_loss: 7.8843e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0435e-10 - val_loss: 7.7380e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8859e-10 - val_loss: 7.6119e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7940e-10 - val_loss: 7.5409e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7213e-10 - val_loss: 7.4861e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6372e-10 - val_loss: 7.3109e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4194e-10 - val_loss: 7.1103e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2490e-10 - val_loss: 6.9573e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1155e-10 - val_loss: 6.9545e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1227e-10 - val_loss: 6.8975e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9999e-10 - val_loss: 6.7256e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8496e-10 - val_loss: 6.6076e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7237e-10 - val_loss: 6.5267e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6368e-10 - val_loss: 6.4039e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5735e-10 - val_loss: 6.4388e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5735e-10 - val_loss: 6.4073e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5145e-10 - val_loss: 6.2618e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3726e-10 - val_loss: 6.1277e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2262e-10 - val_loss: 5.9723e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0661e-10 - val_loss: 5.9000e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0333e-10 - val_loss: 5.8620e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9824e-10 - val_loss: 5.7869e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8910e-10 - val_loss: 5.7447e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8577e-10 - val_loss: 5.6282e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7055e-10 - val_loss: 5.4875e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6087e-10 - val_loss: 5.4945e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5948e-10 - val_loss: 5.4859e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5628e-10 - val_loss: 5.4206e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5080e-10 - val_loss: 5.3277e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4109e-10 - val_loss: 5.1962e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2712e-10 - val_loss: 5.1054e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1788e-10 - val_loss: 5.0059e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0865e-10 - val_loss: 4.9853e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0813e-10 - val_loss: 4.9645e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0292e-10 - val_loss: 4.8445e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9069e-10 - val_loss: 4.7300e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8313e-10 - val_loss: 4.7533e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8469e-10 - val_loss: 4.7746e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8277e-10 - val_loss: 4.6900e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7422e-10 - val_loss: 4.5875e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6440e-10 - val_loss: 4.4816e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5303e-10 - val_loss: 4.4214e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5018e-10 - val_loss: 4.4348e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5055e-10 - val_loss: 4.4195e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4967e-10 - val_loss: 4.4214e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4860e-10 - val_loss: 4.3612e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4045e-10 - val_loss: 4.2393e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3066e-10 - val_loss: 4.2572e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3210e-10 - val_loss: 4.1909e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2300e-10 - val_loss: 4.1036e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1649e-10 - val_loss: 4.0690e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0968e-10 - val_loss: 3.9599e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0328e-10 - val_loss: 4.0080e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0806e-10 - val_loss: 3.9531e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0078e-10 - val_loss: 3.8932e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9524e-10 - val_loss: 3.8441e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8967e-10 - val_loss: 3.8079e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8787e-10 - val_loss: 3.8158e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8600e-10 - val_loss: 3.7272e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8033e-10 - val_loss: 3.7286e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7881e-10 - val_loss: 3.6843e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7153e-10 - val_loss: 3.5907e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6369e-10 - val_loss: 3.6082e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6630e-10 - val_loss: 3.5909e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6407e-10 - val_loss: 3.5384e-10
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.5755e-10 - val_loss: 3.4990e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.5386e-10 - val_loss: 3.4511e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.5075e-10 - val_loss: 3.4174e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.4531e-10 - val_loss: 3.3678e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3997e-10 - val_loss: 3.2919e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3277e-10 - val_loss: 3.2654e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3171e-10 - val_loss: 3.2543e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2940e-10 - val_loss: 3.2561e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3138e-10 - val_loss: 3.2681e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3092e-10 - val_loss: 3.2311e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.2773e-10 - val_loss: 3.1873e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.1972e-10 - val_loss: 3.1290e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.1612e-10 - val_loss: 3.0577e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.0979e-10 - val_loss: 3.0390e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.0605e-10 - val_loss: 2.9793e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.0276e-10 - val_loss: 2.9754e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.0207e-10 - val_loss: 2.9725e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.9913e-10 - val_loss: 2.9284e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9642e-10 - val_loss: 2.9368e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9790e-10 - val_loss: 2.9404e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.9828e-10 - val_loss: 2.8964e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.9334e-10 - val_loss: 2.8553e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8896e-10 - val_loss: 2.8203e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8376e-10 - val_loss: 2.7717e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8067e-10 - val_loss: 2.7930e-10
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8263e-10 - val_loss: 2.7766e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8344e-10 - val_loss: 2.7905e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8110e-10 - val_loss: 2.7341e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.7617e-10 - val_loss: 2.6888e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.7172e-10 - val_loss: 2.6526e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6783e-10 - val_loss: 2.6267e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6365e-10 - val_loss: 2.5936e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6179e-10 - val_loss: 2.6101e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6393e-10 - val_loss: 2.5919e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6253e-10 - val_loss: 2.5756e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6016e-10 - val_loss: 2.5570e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5937e-10 - val_loss: 2.5480e-10
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5629e-10 - val_loss: 2.4967e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5203e-10 - val_loss: 2.4482e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4851e-10 - val_loss: 2.4137e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4340e-10 - val_loss: 2.4160e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4600e-10 - val_loss: 2.4437e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4745e-10 - val_loss: 2.4695e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5019e-10 - val_loss: 2.4521e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4743e-10 - val_loss: 2.4060e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4080e-10 - val_loss: 2.3342e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3503e-10 - val_loss: 2.2987e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3276e-10 - val_loss: 2.2959e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3161e-10 - val_loss: 2.2914e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3092e-10 - val_loss: 2.2756e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2938e-10 - val_loss: 2.2578e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2893e-10 - val_loss: 2.2872e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3135e-10 - val_loss: 2.2893e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3030e-10 - val_loss: 2.2641e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2737e-10 - val_loss: 2.2273e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2483e-10 - val_loss: 2.2257e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2370e-10 - val_loss: 2.1630e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1804e-10 - val_loss: 2.1449e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1671e-10 - val_loss: 2.1388e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1458e-10 - val_loss: 2.1096e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1469e-10 - val_loss: 2.1511e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1774e-10 - val_loss: 2.1725e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1911e-10 - val_loss: 2.1608e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1747e-10 - val_loss: 2.1277e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1435e-10 - val_loss: 2.1037e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1228e-10 - val_loss: 2.0690e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0922e-10 - val_loss: 2.0689e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0855e-10 - val_loss: 2.0677e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0708e-10 - val_loss: 2.0086e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0174e-10 - val_loss: 1.9621e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9676e-10 - val_loss: 1.9411e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9725e-10 - val_loss: 1.9494e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9736e-10 - val_loss: 1.9698e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9883e-10 - val_loss: 1.9643e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9981e-10 - val_loss: 1.9892e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0039e-10 - val_loss: 1.9836e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9923e-10 - val_loss: 1.9799e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9767e-10 - val_loss: 1.8882e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9074e-10 - val_loss: 1.8727e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8825e-10 - val_loss: 1.8491e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8620e-10 - val_loss: 1.8390e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8587e-10 - val_loss: 1.8259e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8447e-10 - val_loss: 1.8038e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8274e-10 - val_loss: 1.8246e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8354e-10 - val_loss: 1.8058e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8379e-10 - val_loss: 1.8320e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8426e-10 - val_loss: 1.8170e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8201e-10 - val_loss: 1.7757e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7951e-10 - val_loss: 1.7736e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7811e-10 - val_loss: 1.7688e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7735e-10 - val_loss: 1.7488e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7653e-10 - val_loss: 1.7412e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7601e-10 - val_loss: 1.7462e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7689e-10 - val_loss: 1.7535e-10
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7668e-10 - val_loss: 1.7467e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7647e-10 - val_loss: 1.7127e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7131e-10 - val_loss: 1.6651e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6819e-10 - val_loss: 1.6608e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6691e-10 - val_loss: 1.6556e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6669e-10 - val_loss: 1.6363e-10
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6644e-10 - val_loss: 1.6703e-10
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6858e-10 - val_loss: 1.6520e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6611e-10 - val_loss: 1.6507e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6583e-10 - val_loss: 1.6338e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6484e-10 - val_loss: 1.6335e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6466e-10 - val_loss: 1.6333e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6546e-10 - val_loss: 1.6317e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6264e-10 - val_loss: 1.5962e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6069e-10 - val_loss: 1.5864e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5972e-10 - val_loss: 1.5743e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5806e-10 - val_loss: 1.5476e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5452e-10 - val_loss: 1.5219e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5319e-10 - val_loss: 1.5262e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5426e-10 - val_loss: 1.5329e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5419e-10 - val_loss: 1.5433e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5516e-10 - val_loss: 1.5441e-10
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5608e-10 - val_loss: 1.5450e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5533e-10 - val_loss: 1.5323e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5344e-10 - val_loss: 1.5153e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5200e-10 - val_loss: 1.4965e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5008e-10 - val_loss: 1.4878e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5050e-10 - val_loss: 1.4820e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4922e-10 - val_loss: 1.4863e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4884e-10 - val_loss: 1.4683e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4725e-10 - val_loss: 1.4595e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4645e-10 - val_loss: 1.4401e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4482e-10 - val_loss: 1.4206e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4236e-10 - val_loss: 1.4109e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4251e-10 - val_loss: 1.4317e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4502e-10 - val_loss: 1.4536e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4578e-10 - val_loss: 1.4321e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4460e-10 - val_loss: 1.4145e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4198e-10 - val_loss: 1.4070e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4112e-10 - val_loss: 1.3800e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3934e-10 - val_loss: 1.3827e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3932e-10 - val_loss: 1.3675e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3813e-10 - val_loss: 1.3874e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4004e-10 - val_loss: 1.3716e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3803e-10 - val_loss: 1.3565e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3777e-10 - val_loss: 1.3756e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3777e-10 - val_loss: 1.3633e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3750e-10 - val_loss: 1.3694e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3749e-10 - val_loss: 1.3473e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3564e-10 - val_loss: 1.3515e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3624e-10 - val_loss: 1.3402e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3468e-10 - val_loss: 1.3508e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3608e-10 - val_loss: 1.3448e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3446e-10 - val_loss: 1.3072e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3139e-10 - val_loss: 1.3032e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3102e-10 - val_loss: 1.3006e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3018e-10 - val_loss: 1.2854e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2925e-10 - val_loss: 1.3044e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3041e-10 - val_loss: 1.2887e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2934e-10 - val_loss: 1.2680e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2794e-10 - val_loss: 1.2694e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2825e-10 - val_loss: 1.2850e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2939e-10 - val_loss: 1.2796e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2799e-10 - val_loss: 1.2641e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2731e-10 - val_loss: 1.2525e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2577e-10 - val_loss: 1.2511e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2677e-10 - val_loss: 1.2473e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2515e-10 - val_loss: 1.2496e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2647e-10 - val_loss: 1.2651e-10
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2739e-10 - val_loss: 1.2526e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2424e-10 - val_loss: 1.2373e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2441e-10 - val_loss: 1.2317e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2293e-10 - val_loss: 1.1961e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2036e-10 - val_loss: 1.2008e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2211e-10 - val_loss: 1.2137e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2180e-10 - val_loss: 1.2071e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2130e-10 - val_loss: 1.2086e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2166e-10 - val_loss: 1.1904e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2019e-10 - val_loss: 1.1926e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2065e-10 - val_loss: 1.1991e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1992e-10 - val_loss: 1.1672e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1665e-10 - val_loss: 1.1542e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1707e-10 - val_loss: 1.1526e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1601e-10 - val_loss: 1.1467e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1465e-10 - val_loss: 1.1184e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1200e-10 - val_loss: 1.1032e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1025e-10 - val_loss: 1.0897e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1023e-10 - val_loss: 1.1105e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1183e-10 - val_loss: 1.1136e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1252e-10 - val_loss: 1.1206e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1335e-10 - val_loss: 1.1361e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1406e-10 - val_loss: 1.1333e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1440e-10 - val_loss: 1.1316e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1421e-10 - val_loss: 1.1422e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1487e-10 - val_loss: 1.1199e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1177e-10 - val_loss: 1.0955e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1052e-10 - val_loss: 1.1174e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1222e-10 - val_loss: 1.1080e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1018e-10 - val_loss: 1.0628e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0649e-10 - val_loss: 1.0604e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0690e-10 - val_loss: 1.0730e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0803e-10 - val_loss: 1.0791e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0843e-10 - val_loss: 1.0729e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0816e-10 - val_loss: 1.0852e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0900e-10 - val_loss: 1.0847e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0914e-10 - val_loss: 1.0761e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0923e-10 - val_loss: 1.1051e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1186e-10 - val_loss: 1.0994e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1011e-10 - val_loss: 1.0803e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0823e-10 - val_loss: 1.0846e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0977e-10 - val_loss: 1.1043e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1063e-10 - val_loss: 1.0833e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0812e-10 - val_loss: 1.0698e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0730e-10 - val_loss: 1.0559e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0576e-10 - val_loss: 1.0312e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0369e-10 - val_loss: 1.0285e-10
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0331e-10 - val_loss: 1.0213e-10
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0287e-10 - val_loss: 1.0210e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0335e-10 - val_loss: 1.0490e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0558e-10 - val_loss: 1.0587e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0652e-10 - val_loss: 1.0548e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0579e-10 - val_loss: 1.0545e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0604e-10 - val_loss: 1.0466e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0596e-10 - val_loss: 1.0556e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0552e-10 - val_loss: 1.0343e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0364e-10 - val_loss: 1.0216e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0255e-10 - val_loss: 9.9664e-11
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.9299e-11 - val_loss: 9.8991e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0018e-10 - val_loss: 1.0012e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0099e-10 - val_loss: 1.0149e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0149e-10 - val_loss: 9.9546e-11
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.9374e-11 - val_loss: 9.8485e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9877e-11 - val_loss: 1.0051e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0097e-10 - val_loss: 9.8508e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.7946e-11 - val_loss: 9.6239e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.6420e-11 - val_loss: 9.6193e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7549e-11 - val_loss: 9.8678e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9023e-11 - val_loss: 9.7576e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8211e-11 - val_loss: 9.6438e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.6139e-11 - val_loss: 9.5258e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.5489e-11 - val_loss: 9.4067e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4427e-11 - val_loss: 9.4254e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4804e-11 - val_loss: 9.4279e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.3991e-11 - val_loss: 9.3672e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4657e-11 - val_loss: 9.4352e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4748e-11 - val_loss: 9.4310e-11
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.4830e-11 - val_loss: 9.3199e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.3468e-11 - val_loss: 9.2310e-11
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.2271e-11 - val_loss: 9.0203e-11
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.0992e-11 - val_loss: 8.9777e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0821e-11 - val_loss: 9.0171e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1422e-11 - val_loss: 9.2006e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2758e-11 - val_loss: 9.1850e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1373e-11 - val_loss: 8.9843e-11
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.9739e-11 - val_loss: 8.9089e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0288e-11 - val_loss: 9.0310e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1179e-11 - val_loss: 9.0877e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.0129e-11 - val_loss: 8.7962e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.7769e-11 - val_loss: 8.6298e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6884e-11 - val_loss: 8.7107e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8087e-11 - val_loss: 8.7711e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7469e-11 - val_loss: 8.6670e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7546e-11 - val_loss: 8.7483e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7569e-11 - val_loss: 8.7393e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8146e-11 - val_loss: 8.9204e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9372e-11 - val_loss: 8.8967e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8658e-11 - val_loss: 8.7524e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7743e-11 - val_loss: 8.7091e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7834e-11 - val_loss: 8.8492e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8799e-11 - val_loss: 8.7939e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8727e-11 - val_loss: 8.8465e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8441e-11 - val_loss: 8.6656e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.6309e-11 - val_loss: 8.5274e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5543e-11 - val_loss: 8.6648e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7775e-11 - val_loss: 8.8175e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8023e-11 - val_loss: 8.5877e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.6093e-11 - val_loss: 8.5078e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.5307e-11 - val_loss: 8.4411e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5266e-11 - val_loss: 8.6731e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7566e-11 - val_loss: 8.7525e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7770e-11 - val_loss: 8.7853e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7419e-11 - val_loss: 8.4637e-11
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.4346e-11 - val_loss: 8.3954e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4859e-11 - val_loss: 8.5651e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6781e-11 - val_loss: 8.6070e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6422e-11 - val_loss: 8.5808e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6294e-11 - val_loss: 8.5730e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5648e-11 - val_loss: 8.5590e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6370e-11 - val_loss: 8.6281e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6723e-11 - val_loss: 8.5926e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.5354e-11 - val_loss: 8.3844e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.4092e-11 - val_loss: 8.3290e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.2970e-11 - val_loss: 8.2593e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.3091e-11 - val_loss: 8.2317e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.2388e-11 - val_loss: 8.2118e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2558e-11 - val_loss: 8.2369e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.2592e-11 - val_loss: 8.1637e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.1335e-11 - val_loss: 7.9451e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.9641e-11 - val_loss: 7.9364e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9948e-11 - val_loss: 8.0867e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1204e-11 - val_loss: 8.1342e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2066e-11 - val_loss: 8.2041e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1744e-11 - val_loss: 8.0495e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 8.0113e-11 - val_loss: 7.7816e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.7733e-11 - val_loss: 7.6487e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7316e-11 - val_loss: 7.7342e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8355e-11 - val_loss: 7.9145e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9760e-11 - val_loss: 7.9698e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9962e-11 - val_loss: 7.8865e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8747e-11 - val_loss: 7.7376e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8092e-11 - val_loss: 7.9067e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9038e-11 - val_loss: 7.8229e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8525e-11 - val_loss: 7.7872e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7912e-11 - val_loss: 7.7459e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7903e-11 - val_loss: 7.6798e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.6955e-11 - val_loss: 7.6221e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.5744e-11 - val_loss: 7.4757e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5473e-11 - val_loss: 7.5886e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6632e-11 - val_loss: 7.6560e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7439e-11 - val_loss: 7.7755e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8411e-11 - val_loss: 7.7677e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7639e-11 - val_loss: 7.5983e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6508e-11 - val_loss: 7.5946e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6230e-11 - val_loss: 7.5953e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6397e-11 - val_loss: 7.6074e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6258e-11 - val_loss: 7.5852e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.5388e-11 - val_loss: 7.3953e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.4043e-11 - val_loss: 7.3560e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4145e-11 - val_loss: 7.4712e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4997e-11 - val_loss: 7.5008e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5584e-11 - val_loss: 7.6133e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6539e-11 - val_loss: 7.5717e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5755e-11 - val_loss: 7.3956e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.3817e-11 - val_loss: 7.2451e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3008e-11 - val_loss: 7.3462e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3493e-11 - val_loss: 7.2997e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2854e-11 - val_loss: 7.2847e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4024e-11 - val_loss: 7.4357e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4252e-11 - val_loss: 7.3156e-11
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.3060e-11 - val_loss: 7.2377e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.2548e-11 - val_loss: 7.1825e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.1997e-11 - val_loss: 7.0593e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.0812e-11 - val_loss: 6.9915e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 7.0101e-11 - val_loss: 6.9075e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.8910e-11 - val_loss: 6.6892e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.6718e-11 - val_loss: 6.5687e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6307e-11 - val_loss: 6.6983e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7575e-11 - val_loss: 6.8661e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9612e-11 - val_loss: 7.1066e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1423e-11 - val_loss: 7.1707e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2355e-11 - val_loss: 7.2053e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2295e-11 - val_loss: 7.1814e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1657e-11 - val_loss: 7.0606e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0745e-11 - val_loss: 6.9946e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0187e-11 - val_loss: 7.1135e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1646e-11 - val_loss: 7.0978e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1086e-11 - val_loss: 7.0737e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0898e-11 - val_loss: 6.9320e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8766e-11 - val_loss: 6.7292e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7407e-11 - val_loss: 6.7293e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7539e-11 - val_loss: 6.7656e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8447e-11 - val_loss: 6.9359e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9554e-11 - val_loss: 6.9413e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9752e-11 - val_loss: 6.9888e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0527e-11 - val_loss: 7.0594e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9708e-11 - val_loss: 6.8285e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8128e-11 - val_loss: 6.6747e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.6292e-11 - val_loss: 6.5424e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.5323e-11 - val_loss: 6.5352e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6105e-11 - val_loss: 6.6881e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8207e-11 - val_loss: 6.8794e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9117e-11 - val_loss: 6.8425e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8287e-11 - val_loss: 6.7525e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7914e-11 - val_loss: 6.7254e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6808e-11 - val_loss: 6.5749e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.5332e-11 - val_loss: 6.5308e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 6.4934e-11 - val_loss: 6.4636e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.300 | eve: 9.724 | bob: 9.184Epoch   0:   0% | abe: 9.276 | eve: 9.731 | bob: 9.168Epoch   0:   1% | abe: 9.273 | eve: 9.727 | bob: 9.172Epoch   0:   2% | abe: 9.234 | eve: 9.721 | bob: 9.139Epoch   0:   2% | abe: 9.231 | eve: 9.714 | bob: 9.141Epoch   0:   3% | abe: 9.228 | eve: 9.715 | bob: 9.142Epoch   0:   4% | abe: 9.223 | eve: 9.711 | bob: 9.140Epoch   0:   4% | abe: 9.218 | eve: 9.717 | bob: 9.138Epoch   0:   5% | abe: 9.205 | eve: 9.724 | bob: 9.126Epoch   0:   6% | abe: 9.184 | eve: 9.723 | bob: 9.108Epoch   0:   6% | abe: 9.174 | eve: 9.723 | bob: 9.100Epoch   0:   7% | abe: 9.169 | eve: 9.729 | bob: 9.096Epoch   0:   8% | abe: 9.165 | eve: 9.729 | bob: 9.094Epoch   0:   8% | abe: 9.158 | eve: 9.732 | bob: 9.088Epoch   0:   9% | abe: 9.154 | eve: 9.738 | bob: 9.086Epoch   0:  10% | abe: 9.145 | eve: 9.737 | bob: 9.078Epoch   0:  10% | abe: 9.142 | eve: 9.737 | bob: 9.076Epoch   0:  11% | abe: 9.138 | eve: 9.738 | bob: 9.072Epoch   0:  12% | abe: 9.130 | eve: 9.739 | bob: 9.066Epoch   0:  13% | abe: 9.127 | eve: 9.744 | bob: 9.062Epoch   0:  13% | abe: 9.125 | eve: 9.746 | bob: 9.061Epoch   0:  14% | abe: 9.122 | eve: 9.752 | bob: 9.059Epoch   0:  15% | abe: 9.119 | eve: 9.752 | bob: 9.055Epoch   0:  15% | abe: 9.115 | eve: 9.759 | bob: 9.051Epoch   0:  16% | abe: 9.112 | eve: 9.760 | bob: 9.048Epoch   0:  17% | abe: 9.107 | eve: 9.760 | bob: 9.043Epoch   0:  17% | abe: 9.105 | eve: 9.762 | bob: 9.042Epoch   0:  18% | abe: 9.105 | eve: 9.760 | bob: 9.042Epoch   0:  19% | abe: 9.105 | eve: 9.761 | bob: 9.042Epoch   0:  19% | abe: 9.106 | eve: 9.763 | bob: 9.043Epoch   0:  20% | abe: 9.103 | eve: 9.766 | bob: 9.039Epoch   0:  21% | abe: 9.101 | eve: 9.766 | bob: 9.037Epoch   0:  21% | abe: 9.101 | eve: 9.769 | bob: 9.037Epoch   0:  22% | abe: 9.098 | eve: 9.775 | bob: 9.034Epoch   0:  23% | abe: 9.096 | eve: 9.777 | bob: 9.032Epoch   0:  23% | abe: 9.095 | eve: 9.779 | bob: 9.030Epoch   0:  24% | abe: 9.094 | eve: 9.780 | bob: 9.029Epoch   0:  25% | abe: 9.093 | eve: 9.783 | bob: 9.028Epoch   0:  26% | abe: 9.091 | eve: 9.786 | bob: 9.026Epoch   0:  26% | abe: 9.092 | eve: 9.787 | bob: 9.027Epoch   0:  27% | abe: 9.091 | eve: 9.787 | bob: 9.025Epoch   0:  28% | abe: 9.090 | eve: 9.789 | bob: 9.024Epoch   0:  28% | abe: 9.088 | eve: 9.792 | bob: 9.023Epoch   0:  29% | abe: 9.088 | eve: 9.792 | bob: 9.022Epoch   0:  30% | abe: 9.086 | eve: 9.795 | bob: 9.020Epoch   0:  30% | abe: 9.084 | eve: 9.796 | bob: 9.018Epoch   0:  31% | abe: 9.084 | eve: 9.798 | bob: 9.018Epoch   0:  32% | abe: 9.084 | eve: 9.799 | bob: 9.018Epoch   0:  32% | abe: 9.082 | eve: 9.801 | bob: 9.015Epoch   0:  33% | abe: 9.081 | eve: 9.803 | bob: 9.014Epoch   0:  34% | abe: 9.079 | eve: 9.804 | bob: 9.012Epoch   0:  34% | abe: 9.079 | eve: 9.806 | bob: 9.012Epoch   0:  35% | abe: 9.077 | eve: 9.807 | bob: 9.010Epoch   0:  36% | abe: 9.076 | eve: 9.808 | bob: 9.009Epoch   0:  36% | abe: 9.075 | eve: 9.808 | bob: 9.007Epoch   0:  37% | abe: 9.074 | eve: 9.811 | bob: 9.006Epoch   0:  38% | abe: 9.074 | eve: 9.811 | bob: 9.005Epoch   0:  39% | abe: 9.073 | eve: 9.811 | bob: 9.005Epoch   0:  39% | abe: 9.073 | eve: 9.812 | bob: 9.004Epoch   0:  40% | abe: 9.072 | eve: 9.814 | bob: 9.003Epoch   0:  41% | abe: 9.072 | eve: 9.814 | bob: 9.003Epoch   0:  41% | abe: 9.071 | eve: 9.815 | bob: 9.002Epoch   0:  42% | abe: 9.070 | eve: 9.815 | bob: 9.001Epoch   0:  43% | abe: 9.069 | eve: 9.816 | bob: 9.000Epoch   0:  43% | abe: 9.069 | eve: 9.818 | bob: 9.000Epoch   0:  44% | abe: 9.068 | eve: 9.819 | bob: 8.999Epoch   0:  45% | abe: 9.067 | eve: 9.820 | bob: 8.998Epoch   0:  45% | abe: 9.066 | eve: 9.820 | bob: 8.996Epoch   0:  46% | abe: 9.065 | eve: 9.822 | bob: 8.995Epoch   0:  47% | abe: 9.064 | eve: 9.823 | bob: 8.994Epoch   0:  47% | abe: 9.063 | eve: 9.823 | bob: 8.993Epoch   0:  48% | abe: 9.062 | eve: 9.825 | bob: 8.992Epoch   0:  49% | abe: 9.061 | eve: 9.826 | bob: 8.991Epoch   0:  50% | abe: 9.061 | eve: 9.827 | bob: 8.990Epoch   0:  50% | abe: 9.060 | eve: 9.829 | bob: 8.989Epoch   0:  51% | abe: 9.060 | eve: 9.829 | bob: 8.989Epoch   0:  52% | abe: 9.059 | eve: 9.829 | bob: 8.988Epoch   0:  52% | abe: 9.058 | eve: 9.830 | bob: 8.987Epoch   0:  53% | abe: 9.058 | eve: 9.831 | bob: 8.987Epoch   0:  54% | abe: 9.057 | eve: 9.830 | bob: 8.985Epoch   0:  54% | abe: 9.057 | eve: 9.832 | bob: 8.986Epoch   0:  55% | abe: 9.056 | eve: 9.832 | bob: 8.985Epoch   0:  56% | abe: 9.057 | eve: 9.833 | bob: 8.985Epoch   0:  56% | abe: 9.056 | eve: 9.834 | bob: 8.985Epoch   0:  57% | abe: 9.056 | eve: 9.834 | bob: 8.984Epoch   0:  58% | abe: 9.056 | eve: 9.834 | bob: 8.984Epoch   0:  58% | abe: 9.056 | eve: 9.835 | bob: 8.984Epoch   0:  59% | abe: 9.054 | eve: 9.836 | bob: 8.982Epoch   0:  60% | abe: 9.054 | eve: 9.836 | bob: 8.982Epoch   0:  60% | abe: 9.055 | eve: 9.836 | bob: 8.982Epoch   0:  61% | abe: 9.053 | eve: 9.836 | bob: 8.980Epoch   0:  62% | abe: 9.052 | eve: 9.837 | bob: 8.980Epoch   0:  63% | abe: 9.052 | eve: 9.837 | bob: 8.979Epoch   0:  63% | abe: 9.052 | eve: 9.837 | bob: 8.979Epoch   0:  64% | abe: 9.050 | eve: 9.839 | bob: 8.977Epoch   0:  65% | abe: 9.050 | eve: 9.839 | bob: 8.977Epoch   0:  65% | abe: 9.049 | eve: 9.839 | bob: 8.976Epoch   0:  66% | abe: 9.049 | eve: 9.839 | bob: 8.976Epoch   0:  67% | abe: 9.049 | eve: 9.841 | bob: 8.976Epoch   0:  67% | abe: 9.049 | eve: 9.840 | bob: 8.975Epoch   0:  68% | abe: 9.048 | eve: 9.840 | bob: 8.975Epoch   0:  69% | abe: 9.048 | eve: 9.840 | bob: 8.974Epoch   0:  69% | abe: 9.046 | eve: 9.841 | bob: 8.973Epoch   0:  70% | abe: 9.046 | eve: 9.841 | bob: 8.972Epoch   0:  71% | abe: 9.046 | eve: 9.842 | bob: 8.972Epoch   0:  71% | abe: 9.045 | eve: 9.842 | bob: 8.971Epoch   0:  72% | abe: 9.044 | eve: 9.842 | bob: 8.970Epoch   0:  73% | abe: 9.043 | eve: 9.844 | bob: 8.969Epoch   0:  73% | abe: 9.043 | eve: 9.844 | bob: 8.968Epoch   0:  74% | abe: 9.043 | eve: 9.844 | bob: 8.968Epoch   0:  75% | abe: 9.043 | eve: 9.844 | bob: 8.968Epoch   0:  76% | abe: 9.042 | eve: 9.844 | bob: 8.967Epoch   0:  76% | abe: 9.042 | eve: 9.844 | bob: 8.967Epoch   0:  77% | abe: 9.041 | eve: 9.845 | bob: 8.966Epoch   0:  78% | abe: 9.041 | eve: 9.845 | bob: 8.966Epoch   0:  78% | abe: 9.040 | eve: 9.846 | bob: 8.965Epoch   0:  79% | abe: 9.040 | eve: 9.846 | bob: 8.965Epoch   0:  80% | abe: 9.039 | eve: 9.847 | bob: 8.964Epoch   0:  80% | abe: 9.039 | eve: 9.847 | bob: 8.964Epoch   0:  81% | abe: 9.039 | eve: 9.847 | bob: 8.964Epoch   0:  82% | abe: 9.038 | eve: 9.848 | bob: 8.963Epoch   0:  82% | abe: 9.038 | eve: 9.848 | bob: 8.962Epoch   0:  83% | abe: 9.037 | eve: 9.848 | bob: 8.961Epoch   0:  84% | abe: 9.036 | eve: 9.848 | bob: 8.961Epoch   0:  84% | abe: 9.036 | eve: 9.849 | bob: 8.960Epoch   0:  85% | abe: 9.036 | eve: 9.850 | bob: 8.960Epoch   0:  86% | abe: 9.035 | eve: 9.850 | bob: 8.959Epoch   0:  86% | abe: 9.035 | eve: 9.850 | bob: 8.959Epoch   0:  87% | abe: 9.036 | eve: 9.850 | bob: 8.960Epoch   0:  88% | abe: 9.035 | eve: 9.850 | bob: 8.959Epoch   0:  89% | abe: 9.035 | eve: 9.850 | bob: 8.959Epoch   0:  89% | abe: 9.034 | eve: 9.851 | bob: 8.958Epoch   0:  90% | abe: 9.034 | eve: 9.851 | bob: 8.958Epoch   0:  91% | abe: 9.033 | eve: 9.852 | bob: 8.957Epoch   0:  91% | abe: 9.032 | eve: 9.852 | bob: 8.956Epoch   0:  92% | abe: 9.032 | eve: 9.853 | bob: 8.955Epoch   0:  93% | abe: 9.031 | eve: 9.853 | bob: 8.955Epoch   0:  93% | abe: 9.031 | eve: 9.853 | bob: 8.955Epoch   0:  94% | abe: 9.031 | eve: 9.854 | bob: 8.954Epoch   0:  95% | abe: 9.030 | eve: 9.855 | bob: 8.954Epoch   0:  95% | abe: 9.030 | eve: 9.855 | bob: 8.953Epoch   0:  96% | abe: 9.030 | eve: 9.855 | bob: 8.953Epoch   0:  97% | abe: 9.029 | eve: 9.856 | bob: 8.952Epoch   0:  97% | abe: 9.029 | eve: 9.856 | bob: 8.952Epoch   0:  98% | abe: 9.029 | eve: 9.856 | bob: 8.952Epoch   0:  99% | abe: 9.029 | eve: 9.856 | bob: 8.951
New best Bob loss 8.95127871152152 at epoch 0
Epoch   1:   0% | abe: 8.979 | eve: 9.824 | bob: 8.892Epoch   1:   0% | abe: 8.959 | eve: 9.855 | bob: 8.872Epoch   1:   1% | abe: 8.990 | eve: 9.850 | bob: 8.904Epoch   1:   2% | abe: 8.985 | eve: 9.838 | bob: 8.899Epoch   1:   2% | abe: 8.978 | eve: 9.842 | bob: 8.892Epoch   1:   3% | abe: 8.973 | eve: 9.843 | bob: 8.886Epoch   1:   4% | abe: 8.968 | eve: 9.837 | bob: 8.881Epoch   1:   4% | abe: 8.955 | eve: 9.852 | bob: 8.867Epoch   1:   5% | abe: 8.954 | eve: 9.860 | bob: 8.866Epoch   1:   6% | abe: 8.957 | eve: 9.855 | bob: 8.869Epoch   1:   6% | abe: 8.957 | eve: 9.853 | bob: 8.868Epoch   1:   7% | abe: 8.953 | eve: 9.858 | bob: 8.865Epoch   1:   8% | abe: 8.950 | eve: 9.859 | bob: 8.862Epoch   1:   8% | abe: 8.956 | eve: 9.858 | bob: 8.868Epoch   1:   9% | abe: 8.958 | eve: 9.860 | bob: 8.870Epoch   1:  10% | abe: 8.961 | eve: 9.859 | bob: 8.873Epoch   1:  10% | abe: 8.958 | eve: 9.862 | bob: 8.870Epoch   1:  11% | abe: 8.959 | eve: 9.864 | bob: 8.871Epoch   1:  12% | abe: 8.959 | eve: 9.866 | bob: 8.871Epoch   1:  13% | abe: 8.957 | eve: 9.865 | bob: 8.869Epoch   1:  13% | abe: 8.955 | eve: 9.864 | bob: 8.867Epoch   1:  14% | abe: 8.951 | eve: 9.862 | bob: 8.863Epoch   1:  15% | abe: 8.949 | eve: 9.862 | bob: 8.862Epoch   1:  15% | abe: 8.948 | eve: 9.867 | bob: 8.860Epoch   1:  16% | abe: 8.948 | eve: 9.867 | bob: 8.861Epoch   1:  17% | abe: 8.947 | eve: 9.865 | bob: 8.859Epoch   1:  17% | abe: 8.946 | eve: 9.865 | bob: 8.858Epoch   1:  18% | abe: 8.946 | eve: 9.866 | bob: 8.858Epoch   1:  19% | abe: 8.945 | eve: 9.870 | bob: 8.857Epoch   1:  19% | abe: 8.945 | eve: 9.867 | bob: 8.857Epoch   1:  20% | abe: 8.944 | eve: 9.864 | bob: 8.856Epoch   1:  21% | abe: 8.945 | eve: 9.863 | bob: 8.858Epoch   1:  21% | abe: 8.946 | eve: 9.863 | bob: 8.858Epoch   1:  22% | abe: 8.946 | eve: 9.863 | bob: 8.858Epoch   1:  23% | abe: 8.945 | eve: 9.863 | bob: 8.857Epoch   1:  23% | abe: 8.944 | eve: 9.864 | bob: 8.857Epoch   1:  24% | abe: 8.945 | eve: 9.864 | bob: 8.857Epoch   1:  25% | abe: 8.943 | eve: 9.865 | bob: 8.855Epoch   1:  26% | abe: 8.942 | eve: 9.865 | bob: 8.855Epoch   1:  26% | abe: 8.941 | eve: 9.865 | bob: 8.854Epoch   1:  27% | abe: 8.941 | eve: 9.867 | bob: 8.854Epoch   1:  28% | abe: 8.941 | eve: 9.868 | bob: 8.853Epoch   1:  28% | abe: 8.941 | eve: 9.869 | bob: 8.853Epoch   1:  29% | abe: 8.942 | eve: 9.868 | bob: 8.854Epoch   1:  30% | abe: 8.942 | eve: 9.870 | bob: 8.855Epoch   1:  30% | abe: 8.941 | eve: 9.870 | bob: 8.854Epoch   1:  31% | abe: 8.941 | eve: 9.870 | bob: 8.854Epoch   1:  32% | abe: 8.943 | eve: 9.870 | bob: 8.856Epoch   1:  32% | abe: 8.944 | eve: 9.870 | bob: 8.857Epoch   1:  33% | abe: 8.944 | eve: 9.870 | bob: 8.857Epoch   1:  34% | abe: 8.945 | eve: 9.870 | bob: 8.858Epoch   1:  34% | abe: 8.945 | eve: 9.869 | bob: 8.858Epoch   1:  35% | abe: 8.945 | eve: 9.869 | bob: 8.858Epoch   1:  36% | abe: 8.945 | eve: 9.869 | bob: 8.858Epoch   1:  36% | abe: 8.944 | eve: 9.868 | bob: 8.858Epoch   1:  37% | abe: 8.944 | eve: 9.869 | bob: 8.858Epoch   1:  38% | abe: 8.944 | eve: 9.870 | bob: 8.857Epoch   1:  39% | abe: 8.943 | eve: 9.871 | bob: 8.856Epoch   1:  39% | abe: 8.943 | eve: 9.872 | bob: 8.857Epoch   1:  40% | abe: 8.944 | eve: 9.871 | bob: 8.857Epoch   1:  41% | abe: 8.943 | eve: 9.872 | bob: 8.856Epoch   1:  41% | abe: 8.943 | eve: 9.872 | bob: 8.856Epoch   1:  42% | abe: 8.942 | eve: 9.872 | bob: 8.855Epoch   1:  43% | abe: 8.941 | eve: 9.872 | bob: 8.854Epoch   1:  43% | abe: 8.941 | eve: 9.872 | bob: 8.854Epoch   1:  44% | abe: 8.940 | eve: 9.872 | bob: 8.853Epoch   1:  45% | abe: 8.940 | eve: 9.873 | bob: 8.853Epoch   1:  45% | abe: 8.939 | eve: 9.873 | bob: 8.852Epoch   1:  46% | abe: 8.938 | eve: 9.872 | bob: 8.851Epoch   1:  47% | abe: 8.937 | eve: 9.871 | bob: 8.850Epoch   1:  47% | abe: 8.936 | eve: 9.870 | bob: 8.849Epoch   1:  48% | abe: 8.935 | eve: 9.870 | bob: 8.848Epoch   1:  49% | abe: 8.935 | eve: 9.869 | bob: 8.848Epoch   1:  50% | abe: 8.934 | eve: 9.868 | bob: 8.847Epoch   1:  50% | abe: 8.934 | eve: 9.869 | bob: 8.846Epoch   1:  51% | abe: 8.933 | eve: 9.870 | bob: 8.846Epoch   1:  52% | abe: 8.932 | eve: 9.870 | bob: 8.845Epoch   1:  52% | abe: 8.933 | eve: 9.870 | bob: 8.846Epoch   1:  53% | abe: 8.932 | eve: 9.870 | bob: 8.845Epoch   1:  54% | abe: 8.932 | eve: 9.870 | bob: 8.844Epoch   1:  54% | abe: 8.932 | eve: 9.870 | bob: 8.845Epoch   1:  55% | abe: 8.931 | eve: 9.871 | bob: 8.844Epoch   1:  56% | abe: 8.930 | eve: 9.872 | bob: 8.843Epoch   1:  56% | abe: 8.930 | eve: 9.872 | bob: 8.843Epoch   1:  57% | abe: 8.930 | eve: 9.873 | bob: 8.843Epoch   1:  58% | abe: 8.930 | eve: 9.873 | bob: 8.843Epoch   1:  58% | abe: 8.929 | eve: 9.873 | bob: 8.842Epoch   1:  59% | abe: 8.929 | eve: 9.872 | bob: 8.842Epoch   1:  60% | abe: 8.928 | eve: 9.871 | bob: 8.841Epoch   1:  60% | abe: 8.927 | eve: 9.871 | bob: 8.840Epoch   1:  61% | abe: 8.926 | eve: 9.870 | bob: 8.840Epoch   1:  62% | abe: 8.926 | eve: 9.870 | bob: 8.839Epoch   1:  63% | abe: 8.925 | eve: 9.870 | bob: 8.838Epoch   1:  63% | abe: 8.925 | eve: 9.869 | bob: 8.838Epoch   1:  64% | abe: 8.925 | eve: 9.869 | bob: 8.838Epoch   1:  65% | abe: 8.924 | eve: 9.869 | bob: 8.838Epoch   1:  65% | abe: 8.924 | eve: 9.870 | bob: 8.837Epoch   1:  66% | abe: 8.923 | eve: 9.870 | bob: 8.836Epoch   1:  67% | abe: 8.923 | eve: 9.869 | bob: 8.836Epoch   1:  67% | abe: 8.922 | eve: 9.869 | bob: 8.835Epoch   1:  68% | abe: 8.922 | eve: 9.870 | bob: 8.836Epoch   1:  69% | abe: 8.922 | eve: 9.870 | bob: 8.835Epoch   1:  69% | abe: 8.922 | eve: 9.869 | bob: 8.835Epoch   1:  70% | abe: 8.922 | eve: 9.869 | bob: 8.835Epoch   1:  71% | abe: 8.921 | eve: 9.869 | bob: 8.835Epoch   1:  71% | abe: 8.921 | eve: 9.869 | bob: 8.835Epoch   1:  72% | abe: 8.921 | eve: 9.870 | bob: 8.835Epoch   1:  73% | abe: 8.921 | eve: 9.870 | bob: 8.834Epoch   1:  73% | abe: 8.920 | eve: 9.870 | bob: 8.834Epoch   1:  74% | abe: 8.920 | eve: 9.871 | bob: 8.834Epoch   1:  75% | abe: 8.920 | eve: 9.871 | bob: 8.834Epoch   1:  76% | abe: 8.920 | eve: 9.871 | bob: 8.834Epoch   1:  76% | abe: 8.919 | eve: 9.871 | bob: 8.833Epoch   1:  77% | abe: 8.919 | eve: 9.870 | bob: 8.833Epoch   1:  78% | abe: 8.919 | eve: 9.871 | bob: 8.832Epoch   1:  78% | abe: 8.918 | eve: 9.870 | bob: 8.832Epoch   1:  79% | abe: 8.918 | eve: 9.870 | bob: 8.831Epoch   1:  80% | abe: 8.917 | eve: 9.871 | bob: 8.831Epoch   1:  80% | abe: 8.917 | eve: 9.871 | bob: 8.830Epoch   1:  81% | abe: 8.917 | eve: 9.871 | bob: 8.830Epoch   1:  82% | abe: 8.917 | eve: 9.871 | bob: 8.830Epoch   1:  82% | abe: 8.916 | eve: 9.872 | bob: 8.830Epoch   1:  83% | abe: 8.915 | eve: 9.872 | bob: 8.829Epoch   1:  84% | abe: 8.915 | eve: 9.872 | bob: 8.829Epoch   1:  84% | abe: 8.914 | eve: 9.871 | bob: 8.828Epoch   1:  85% | abe: 8.914 | eve: 9.871 | bob: 8.828Epoch   1:  86% | abe: 8.914 | eve: 9.871 | bob: 8.828Epoch   1:  86% | abe: 8.913 | eve: 9.871 | bob: 8.827Epoch   1:  87% | abe: 8.913 | eve: 9.870 | bob: 8.827Epoch   1:  88% | abe: 8.912 | eve: 9.871 | bob: 8.826Epoch   1:  89% | abe: 8.912 | eve: 9.870 | bob: 8.826Epoch   1:  89% | abe: 8.911 | eve: 9.870 | bob: 8.825Epoch   1:  90% | abe: 8.911 | eve: 9.870 | bob: 8.825Epoch   1:  91% | abe: 8.911 | eve: 9.870 | bob: 8.825Epoch   1:  91% | abe: 8.910 | eve: 9.870 | bob: 8.824Epoch   1:  92% | abe: 8.909 | eve: 9.870 | bob: 8.823Epoch   1:  93% | abe: 8.909 | eve: 9.869 | bob: 8.823Epoch   1:  93% | abe: 8.909 | eve: 9.869 | bob: 8.823Epoch   1:  94% | abe: 8.908 | eve: 9.870 | bob: 8.822Epoch   1:  95% | abe: 8.908 | eve: 9.870 | bob: 8.822Epoch   1:  95% | abe: 8.908 | eve: 9.870 | bob: 8.822Epoch   1:  96% | abe: 8.907 | eve: 9.870 | bob: 8.821Epoch   1:  97% | abe: 8.907 | eve: 9.869 | bob: 8.821Epoch   1:  97% | abe: 8.907 | eve: 9.869 | bob: 8.821Epoch   1:  98% | abe: 8.907 | eve: 9.870 | bob: 8.821Epoch   1:  99% | abe: 8.907 | eve: 9.870 | bob: 8.821
New best Bob loss 8.820719342668761 at epoch 1
Training complete.
cipher1 + cipher2
[[0.4281829  0.90977067 1.2596829  ... 0.45541823 0.6829063  1.3769659 ]
 [0.42808318 1.0049965  1.1346428  ... 0.5075529  0.7927533  1.3353778 ]
 [0.4678369  1.0993191  1.2016386  ... 0.54163337 0.74520874 1.378211  ]
 ...
 [0.42353964 1.0001693  1.1706618  ... 0.5569831  0.74946797 1.3529817 ]
 [0.46595743 1.0225189  1.0494548  ... 0.4112166  0.7911054  1.3689778 ]
 [0.5771459  1.1237401  1.2072124  ... 0.4617106  0.8428389  1.296839  ]]
HO addition:
[[0.4281711  0.9097403  1.2596534  ... 0.45540604 0.6828904  1.3769268 ]
 [0.4280721  1.0049655  1.134615   ... 0.50754166 0.7927356  1.3353431 ]
 [0.46782327 1.0992861  1.2016143  ... 0.5416216  0.7451925  1.3781735 ]
 ...
 [0.42352873 1.000137   1.170638   ... 0.5569682  0.749451   1.3529428 ]
 [0.4659437  1.022483   1.049431   ... 0.41120505 0.7910866  1.3689396 ]
 [0.57713395 1.1237085  1.2071835  ... 0.46169928 0.8428174  1.2968005 ]]
cipher1 * cipher2
[[0.04571041 0.18659417 0.38947257 ... 0.0518389  0.11421524 0.47011843]
 [0.04577952 0.2424689  0.3195018  ... 0.06192936 0.15171957 0.44555256]
 [0.05387133 0.294012   0.33248234 ... 0.06985126 0.13223758 0.4743058 ]
 ...
 [0.044802   0.23280084 0.31614643 ... 0.07753734 0.13620706 0.45259714]
 [0.05325427 0.22219782 0.2676097  ... 0.04204558 0.15405904 0.46648914]
 [0.07756506 0.31402895 0.35995755 ... 0.05289288 0.17725739 0.41149732]]
HO multiplication
[[0.04569374 0.18658598 0.38947403 ... 0.05182244 0.11420092 0.4701257 ]
 [0.0457629  0.24246341 0.31949925 ... 0.06191325 0.15170713 0.44555718]
 [0.05385491 0.29400975 0.33248186 ... 0.0698354  0.13222428 0.47431237]
 ...
 [0.04478535 0.23279557 0.31614485 ... 0.07752155 0.13619381 0.4526037 ]
 [0.05323785 0.22219405 0.2676041  ... 0.04202886 0.15404665 0.4664958 ]
 [0.07754957 0.3140269  0.3599574  ... 0.05287643 0.1772463  0.41150248]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 100.00%
Bob decrypted addition: [[1.0003493  1.045133   1.052587   ... 0.9307274  0.9530409  0.9279845 ]
 [0.98545873 0.9886118  0.9331797  ... 0.94160074 0.8938994  0.9078046 ]
 [0.9853282  1.0035682  0.97320455 ... 0.91463935 0.9725371  0.9485557 ]
 ...
 [0.9651339  1.045349   0.99537164 ... 0.8979044  0.9769221  0.92883146]
 [0.98897505 1.0297763  1.0256721  ... 0.9243697  0.9580633  0.9171712 ]
 [0.95553416 0.9625586  1.000607   ... 0.92631996 0.9999908  0.9255048 ]]
Bob decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits addition: 3551
Total number of bits addition: 7168
Decryption accuracy addition: 49.539620535714285%
Bob decrypted multiplication: [[0.83992136 0.8475767  0.83575046 ... 0.84083176 0.85485953 0.9525527 ]
 [0.8387289  0.8345737  0.8288927  ... 0.85328215 0.8315238  0.9266731 ]
 [0.83277357 0.8627501  0.8294859  ... 0.8461027  0.8535868  0.97569716]
 ...
 [0.8260806  0.876053   0.830863   ... 0.8335502  0.86632824 0.95140415]
 [0.82671136 0.8881548  0.8336266  ... 0.8448565  0.856517   0.9411272 ]
 [0.82693493 0.8700957  0.8298381  ... 0.8379793  0.8944861  0.9379608 ]]
Bob decrypted bits multiplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits multiplication: 1818
Total number of bits multiplication: 7168
Decryption accuracy multiplication: 25.362723214285715%
Eve decrypted addition: [[1.001823   1.1478535  0.9515359  ... 0.9460473  1.1232373  1.0535688 ]
 [0.9813873  1.1408304  0.94738775 ... 0.950904   1.1136235  1.0651052 ]
 [0.96208346 1.1626488  0.923469   ... 0.9560528  1.1195903  1.0438349 ]
 ...
 [0.97759265 1.1088114  0.9882582  ... 0.99901795 1.1282623  1.0655286 ]
 [0.97889954 1.1721057  0.90637213 ... 0.9236293  1.1188847  1.0494927 ]
 [0.9479149  1.1226469  0.9605668  ... 0.976118   1.1274613  1.064775  ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 3551
Total number of bits addition: 7168
Decryption accuracy by Eve addition: 49.539620535714285%
Eve decrypted mulitplication: [[1.0437794  1.1206907  0.95679545 ... 1.0060612  1.0880115  1.0158228 ]
 [0.99752975 1.1072692  0.94213414 ... 0.98932934 1.0733435  1.0303303 ]
 [0.9846213  1.1286904  0.9245449  ... 0.98959804 1.0884554  1.0210865 ]
 ...
 [1.0503066  1.0812724  0.98732644 ... 1.0212277  1.1117053  1.0267612 ]
 [1.026788   1.1395634  0.8954684  ... 0.9508485  1.0900129  1.022475  ]
 [0.97010016 1.099007   0.93438876 ... 0.98617774 1.1065234  1.0407516 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 1818
Total number of bits mulitplication: 7168
Decryption accuracy by Eve mulitplication: 25.362723214285715%
Bob decrypted P1: [[0.889731   0.84509355 0.8760095  ... 0.874099   0.8754787  0.9526054 ]
 [0.869759   0.8326659  0.84065044 ... 0.9053637  0.8370785  0.92898315]
 [0.8697869  0.8377499  0.84499896 ... 0.8634229  0.8949666  0.9829989 ]
 ...
 [0.83992845 0.87008727 0.8446293  ... 0.84172    0.895966   0.95638436]
 [0.84583116 0.87681615 0.85995805 ... 0.8682494  0.8779637  0.9388102 ]
 [0.84848315 0.84642667 0.84057355 ... 0.8699969  0.9284908  0.9357752 ]]
Bob decrypted bits P1: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P1: 3593
Total number of bits P1: 7168
Decryption accuracy P1: 50.12555803571429%
Bob decrypted P2: [[0.9114742  0.8578575  0.91234523 ... 0.8856113  0.88600904 0.95439124]
 [0.8897543  0.8387087  0.851143   ... 0.9115415  0.84712183 0.929234  ]
 [0.89563423 0.83747995 0.8655046  ... 0.86370194 0.92193913 0.96786135]
 ...
 [0.8535934  0.8879256  0.85326034 ... 0.846505   0.9082703  0.9472388 ]
 [0.8576829  0.8776261  0.87576413 ... 0.8775486  0.89019716 0.944838  ]
 [0.85705733 0.8496866  0.8526546  ... 0.883198   0.94470143 0.9430905 ]]
Bob decrypted bits P2: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P2: 3594
Total number of bits P2: 7168
Decryption accuracy P2: 50.13950892857143%
