WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-12 10:04:20.642273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-12 10:04:20.750855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:04:20.751863: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:04:20.754940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:04:20.757997: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:04:20.758969: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:04:20.763215: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:04:20.765420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:04:20.772245: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:04:20.783997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:04:20.784726: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-12 10:04:20.804369: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-12 10:04:20.807190: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3448ec0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:04:20.807271: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-12 10:04:21.067964: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2cc2050 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:04:21.068051: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-12 10:04:21.084432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:04:21.084590: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:04:21.084618: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:04:21.084639: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:04:21.084660: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:04:21.084681: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:04:21.084701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:04:21.084724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:04:21.097296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:04:21.097735: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:04:21.103389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-12 10:04:21.103501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-12 10:04:21.103518: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-12 10:04:21.122963: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-12 10:04:25.091612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 10 samples, validate on 10 samples
Epoch 1/512
10/10 - 1s - loss: 1.1010 - val_loss: 0.0081
Epoch 2/512
10/10 - 0s - loss: 0.8115 - val_loss: 0.0062
Epoch 3/512
10/10 - 0s - loss: 0.6175 - val_loss: 0.0045
Epoch 4/512
10/10 - 0s - loss: 0.4538 - val_loss: 0.0033
Epoch 5/512
10/10 - 0s - loss: 0.3325 - val_loss: 0.0024
Epoch 6/512
10/10 - 0s - loss: 0.2425 - val_loss: 0.0017
Epoch 7/512
10/10 - 0s - loss: 0.1719 - val_loss: 0.0011
Epoch 8/512
10/10 - 0s - loss: 0.1149 - val_loss: 7.0432e-04
Epoch 9/512
10/10 - 0s - loss: 0.0704 - val_loss: 3.8941e-04
Epoch 10/512
10/10 - 0s - loss: 0.0389 - val_loss: 1.9415e-04
Epoch 11/512
10/10 - 0s - loss: 0.0194 - val_loss: 8.8091e-05
Epoch 12/512
10/10 - 0s - loss: 0.0088 - val_loss: 3.6737e-05
Epoch 13/512
10/10 - 0s - loss: 0.0037 - val_loss: 1.4175e-05
Epoch 14/512
10/10 - 0s - loss: 0.0014 - val_loss: 5.0779e-06
Epoch 15/512
10/10 - 0s - loss: 5.0779e-04 - val_loss: 1.6979e-06
Epoch 16/512
10/10 - 0s - loss: 1.6979e-04 - val_loss: 5.4067e-07
Epoch 17/512
10/10 - 0s - loss: 5.4067e-05 - val_loss: 1.7615e-07
Epoch 18/512
10/10 - 0s - loss: 1.7615e-05 - val_loss: 7.0064e-08
Epoch 19/512
10/10 - 0s - loss: 7.0064e-06 - val_loss: 4.0546e-08
Epoch 20/512
10/10 - 0s - loss: 4.0546e-06 - val_loss: 3.1515e-08
Epoch 21/512
10/10 - 0s - loss: 3.1515e-06 - val_loss: 2.7442e-08
Epoch 22/512
10/10 - 0s - loss: 2.7442e-06 - val_loss: 2.4511e-08
Epoch 23/512
10/10 - 0s - loss: 2.4511e-06 - val_loss: 2.1904e-08
Epoch 24/512
10/10 - 0s - loss: 2.1904e-06 - val_loss: 1.9477e-08
Epoch 25/512
10/10 - 0s - loss: 1.9477e-06 - val_loss: 1.7207e-08
Epoch 26/512
10/10 - 0s - loss: 1.7207e-06 - val_loss: 1.5098e-08
Epoch 27/512
10/10 - 0s - loss: 1.5098e-06 - val_loss: 1.3151e-08
Epoch 28/512
10/10 - 0s - loss: 1.3151e-06 - val_loss: 1.1367e-08
Epoch 29/512
10/10 - 0s - loss: 1.1367e-06 - val_loss: 9.7448e-09
Epoch 30/512
10/10 - 0s - loss: 9.7449e-07 - val_loss: 8.2825e-09
Epoch 31/512
10/10 - 0s - loss: 8.2826e-07 - val_loss: 6.9753e-09
Epoch 32/512
10/10 - 0s - loss: 6.9753e-07 - val_loss: 5.8173e-09
Epoch 33/512
10/10 - 0s - loss: 5.8173e-07 - val_loss: 4.8022e-09
Epoch 34/512
10/10 - 0s - loss: 4.8023e-07 - val_loss: 3.9210e-09
Epoch 35/512
10/10 - 0s - loss: 3.9210e-07 - val_loss: 3.1651e-09
Epoch 36/512
10/10 - 0s - loss: 3.1651e-07 - val_loss: 2.5234e-09
Epoch 37/512
10/10 - 0s - loss: 2.5234e-07 - val_loss: 1.9861e-09
Epoch 38/512
10/10 - 0s - loss: 1.9861e-07 - val_loss: 1.5419e-09
Epoch 39/512
10/10 - 0s - loss: 1.5419e-07 - val_loss: 1.1795e-09
Epoch 40/512
10/10 - 0s - loss: 1.1795e-07 - val_loss: 8.8835e-10
Epoch 41/512
10/10 - 0s - loss: 8.8836e-08 - val_loss: 6.5818e-10
Epoch 42/512
10/10 - 0s - loss: 6.5817e-08 - val_loss: 4.7908e-10
Epoch 43/512
10/10 - 0s - loss: 4.7908e-08 - val_loss: 3.4225e-10
Epoch 44/512
10/10 - 0s - loss: 3.4225e-08 - val_loss: 2.3976e-10
Epoch 45/512
10/10 - 0s - loss: 2.3977e-08 - val_loss: 1.6443e-10
Epoch 46/512
10/10 - 0s - loss: 1.6442e-08 - val_loss: 1.1021e-10
Epoch 47/512
10/10 - 0s - loss: 1.1022e-08 - val_loss: 7.2126e-11
Epoch 48/512
10/10 - 0s - loss: 7.2125e-09 - val_loss: 4.6024e-11
Epoch 49/512
10/10 - 0s - loss: 4.6025e-09 - val_loss: 2.8579e-11
Epoch 50/512
10/10 - 0s - loss: 2.8578e-09 - val_loss: 1.7207e-11
Epoch 51/512
10/10 - 0s - loss: 1.7206e-09 - val_loss: 1.0059e-11
Epoch 52/512
10/10 - 0s - loss: 1.0059e-09 - val_loss: 5.6790e-12
Epoch 53/512
10/10 - 0s - loss: 5.6795e-10 - val_loss: 3.1254e-12
Epoch 54/512
10/10 - 0s - loss: 3.1253e-10 - val_loss: 1.8512e-12
Epoch 55/512
10/10 - 0s - loss: 1.8512e-10 - val_loss: 2.2719e-12
Epoch 56/512
10/10 - 0s - loss: 2.2722e-10 - val_loss: 1.1220e-11
Epoch 57/512
10/10 - 0s - loss: 1.1220e-09 - val_loss: 9.2854e-11
Epoch 58/512
10/10 - 0s - loss: 9.2855e-09 - val_loss: 9.1175e-10
Epoch 59/512
10/10 - 0s - loss: 9.1175e-08 - val_loss: 1.0289e-08
Epoch 60/512
10/10 - 0s - loss: 1.0289e-06 - val_loss: 1.3291e-07
Epoch 61/512
10/10 - 0s - loss: 1.3291e-05 - val_loss: 1.9430e-06
Epoch 62/512
10/10 - 0s - loss: 1.9430e-04 - val_loss: 2.8220e-05
Epoch 63/512
10/10 - 0s - loss: 0.0028 - val_loss: 1.6102e-04
Epoch 64/512
10/10 - 0s - loss: 0.0161 - val_loss: 1.0598e-04
Epoch 65/512
10/10 - 0s - loss: 0.0106 - val_loss: 1.9979e-05
Epoch 66/512
10/10 - 0s - loss: 0.0020 - val_loss: 6.0326e-06
Epoch 67/512
10/10 - 0s - loss: 6.0326e-04 - val_loss: 1.9781e-06
Epoch 68/512
10/10 - 0s - loss: 1.9781e-04 - val_loss: 8.9970e-07
Epoch 69/512
10/10 - 0s - loss: 8.9970e-05 - val_loss: 5.0145e-07
Epoch 70/512
10/10 - 0s - loss: 5.0145e-05 - val_loss: 3.6128e-07
Epoch 71/512
10/10 - 0s - loss: 3.6128e-05 - val_loss: 3.1807e-07
Epoch 72/512
10/10 - 0s - loss: 3.1807e-05 - val_loss: 3.4953e-07
Epoch 73/512
10/10 - 0s - loss: 3.4953e-05 - val_loss: 4.6160e-07
Epoch 74/512
10/10 - 0s - loss: 4.6160e-05 - val_loss: 7.4377e-07
Epoch 75/512
10/10 - 0s - loss: 7.4377e-05 - val_loss: 1.4063e-06
Epoch 76/512
10/10 - 0s - loss: 1.4063e-04 - val_loss: 3.1602e-06
Epoch 77/512
10/10 - 0s - loss: 3.1602e-04 - val_loss: 7.8355e-06
Epoch 78/512
10/10 - 0s - loss: 7.8355e-04 - val_loss: 2.0720e-05
Epoch 79/512
10/10 - 0s - loss: 0.0021 - val_loss: 4.3625e-05
Epoch 80/512
10/10 - 0s - loss: 0.0044 - val_loss: 6.2896e-05
Epoch 81/512
10/10 - 0s - loss: 0.0063 - val_loss: 4.1515e-05
Epoch 82/512
10/10 - 0s - loss: 0.0042 - val_loss: 2.6652e-05
Epoch 83/512
10/10 - 0s - loss: 0.0027 - val_loss: 1.2811e-05
Epoch 84/512
10/10 - 0s - loss: 0.0013 - val_loss: 7.9444e-06
Epoch 85/512
10/10 - 0s - loss: 7.9444e-04 - val_loss: 5.0587e-06
Epoch 86/512
10/10 - 0s - loss: 5.0587e-04 - val_loss: 4.1276e-06
Epoch 87/512
10/10 - 0s - loss: 4.1276e-04 - val_loss: 3.7210e-06
Epoch 88/512
10/10 - 0s - loss: 3.7210e-04 - val_loss: 4.1584e-06
Epoch 89/512
10/10 - 0s - loss: 4.1584e-04 - val_loss: 5.0980e-06
Epoch 90/512
10/10 - 0s - loss: 5.0980e-04 - val_loss: 7.4271e-06
Epoch 91/512
10/10 - 0s - loss: 7.4271e-04 - val_loss: 1.1080e-05
Epoch 92/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.7970e-05
Epoch 93/512
10/10 - 0s - loss: 0.0018 - val_loss: 2.4736e-05
Epoch 94/512
10/10 - 0s - loss: 0.0025 - val_loss: 3.1885e-05
Epoch 95/512
10/10 - 0s - loss: 0.0032 - val_loss: 2.8223e-05
Epoch 96/512
10/10 - 0s - loss: 0.0028 - val_loss: 2.4692e-05
Epoch 97/512
10/10 - 0s - loss: 0.0025 - val_loss: 1.6612e-05
Epoch 98/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.2959e-05
Epoch 99/512
10/10 - 0s - loss: 0.0013 - val_loss: 9.4397e-06
Epoch 100/512
10/10 - 0s - loss: 9.4397e-04 - val_loss: 8.3314e-06
Epoch 101/512
10/10 - 0s - loss: 8.3314e-04 - val_loss: 7.4612e-06
Epoch 102/512
10/10 - 0s - loss: 7.4612e-04 - val_loss: 7.9939e-06
Epoch 103/512
10/10 - 0s - loss: 7.9939e-04 - val_loss: 8.7145e-06
Epoch 104/512
10/10 - 0s - loss: 8.7145e-04 - val_loss: 1.0936e-05
Epoch 105/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.3173e-05
Epoch 106/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.7103e-05
Epoch 107/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.9087e-05
Epoch 108/512
10/10 - 0s - loss: 0.0019 - val_loss: 2.1634e-05
Epoch 109/512
10/10 - 0s - loss: 0.0022 - val_loss: 1.9580e-05
Epoch 110/512
10/10 - 0s - loss: 0.0020 - val_loss: 1.8519e-05
Epoch 111/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.4770e-05
Epoch 112/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.3187e-05
Epoch 113/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.0901e-05
Epoch 114/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0405e-05
Epoch 115/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.6616e-06
Epoch 116/512
10/10 - 0s - loss: 9.6616e-04 - val_loss: 1.0304e-05
Epoch 117/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0678e-05
Epoch 118/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.2352e-05
Epoch 119/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.3327e-05
Epoch 120/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.5417e-05
Epoch 121/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.5716e-05
Epoch 122/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.6774e-05
Epoch 123/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.5418e-05
Epoch 124/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.5083e-05
Epoch 125/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.3119e-05
Epoch 126/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2540e-05
Epoch 127/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.1168e-05
Epoch 128/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1115e-05
Epoch 129/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0569e-05
Epoch 130/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1183e-05
Epoch 131/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1262e-05
Epoch 132/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.2380e-05
Epoch 133/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2633e-05
Epoch 134/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.3753e-05
Epoch 135/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.3532e-05
Epoch 136/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.4064e-05
Epoch 137/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.3140e-05
Epoch 138/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.3087e-05
Epoch 139/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.1946e-05
Epoch 140/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1806e-05
Epoch 141/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.0957e-05
Epoch 142/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1099e-05
Epoch 143/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0688e-05
Epoch 144/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1186e-05
Epoch 145/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1089e-05
Epoch 146/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1800e-05
Epoch 147/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1730e-05
Epoch 148/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2358e-05
Epoch 149/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2020e-05
Epoch 150/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2333e-05
Epoch 151/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1691e-05
Epoch 152/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1746e-05
Epoch 153/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1032e-05
Epoch 154/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1066e-05
Epoch 155/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0514e-05
Epoch 156/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0702e-05
Epoch 157/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0379e-05
Epoch 158/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0746e-05
Epoch 159/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0574e-05
Epoch 160/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1019e-05
Epoch 161/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0836e-05
Epoch 162/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1203e-05
Epoch 163/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0882e-05
Epoch 164/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1083e-05
Epoch 165/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0629e-05
Epoch 166/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0713e-05
Epoch 167/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0243e-05
Epoch 168/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0328e-05
Epoch 169/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.9507e-06
Epoch 170/512
10/10 - 0s - loss: 9.9507e-04 - val_loss: 1.0118e-05
Epoch 171/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.8622e-06
Epoch 172/512
10/10 - 0s - loss: 9.8622e-04 - val_loss: 1.0114e-05
Epoch 173/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.9291e-06
Epoch 174/512
10/10 - 0s - loss: 9.9291e-04 - val_loss: 1.0202e-05
Epoch 175/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0004e-05
Epoch 176/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0221e-05
Epoch 177/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.9546e-06
Epoch 178/512
10/10 - 0s - loss: 9.9546e-04 - val_loss: 1.0084e-05
Epoch 179/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.7618e-06
Epoch 180/512
10/10 - 0s - loss: 9.7618e-04 - val_loss: 9.8370e-06
Epoch 181/512
10/10 - 0s - loss: 9.8370e-04 - val_loss: 9.5182e-06
Epoch 182/512
10/10 - 0s - loss: 9.5182e-04 - val_loss: 9.5986e-06
Epoch 183/512
10/10 - 0s - loss: 9.5986e-04 - val_loss: 9.3346e-06
Epoch 184/512
10/10 - 0s - loss: 9.3346e-04 - val_loss: 9.4571e-06
Epoch 185/512
10/10 - 0s - loss: 9.4571e-04 - val_loss: 9.2575e-06
Epoch 186/512
10/10 - 0s - loss: 9.2575e-04 - val_loss: 9.4167e-06
Epoch 187/512
10/10 - 0s - loss: 9.4167e-04 - val_loss: 9.2515e-06
Epoch 188/512
10/10 - 0s - loss: 9.2515e-04 - val_loss: 9.4111e-06
Epoch 189/512
10/10 - 0s - loss: 9.4111e-04 - val_loss: 9.2376e-06
Epoch 190/512
10/10 - 0s - loss: 9.2376e-04 - val_loss: 9.3605e-06
Epoch 191/512
10/10 - 0s - loss: 9.3605e-04 - val_loss: 9.1549e-06
Epoch 192/512
10/10 - 0s - loss: 9.1549e-04 - val_loss: 9.2309e-06
Epoch 193/512
10/10 - 0s - loss: 9.2309e-04 - val_loss: 9.0039e-06
Epoch 194/512
10/10 - 0s - loss: 9.0038e-04 - val_loss: 9.0536e-06
Epoch 195/512
10/10 - 0s - loss: 9.0536e-04 - val_loss: 8.8355e-06
Epoch 196/512
10/10 - 0s - loss: 8.8355e-04 - val_loss: 8.8892e-06
Epoch 197/512
10/10 - 0s - loss: 8.8892e-04 - val_loss: 8.7041e-06
Epoch 198/512
10/10 - 0s - loss: 8.7041e-04 - val_loss: 8.7779e-06
Epoch 199/512
10/10 - 0s - loss: 8.7779e-04 - val_loss: 8.6278e-06
Epoch 200/512
10/10 - 0s - loss: 8.6278e-04 - val_loss: 8.7157e-06
Epoch 201/512
10/10 - 0s - loss: 8.7157e-04 - val_loss: 8.5833e-06
Epoch 202/512
10/10 - 0s - loss: 8.5833e-04 - val_loss: 8.6652e-06
Epoch 203/512
10/10 - 0s - loss: 8.6652e-04 - val_loss: 8.5291e-06
Epoch 204/512
10/10 - 0s - loss: 8.5291e-04 - val_loss: 8.5877e-06
Epoch 205/512
10/10 - 0s - loss: 8.5877e-04 - val_loss: 8.4382e-06
Epoch 206/512
10/10 - 0s - loss: 8.4382e-04 - val_loss: 8.4711e-06
Epoch 207/512
10/10 - 0s - loss: 8.4711e-04 - val_loss: 8.3151e-06
Epoch 208/512
10/10 - 0s - loss: 8.3151e-04 - val_loss: 8.3346e-06
Epoch 209/512
10/10 - 0s - loss: 8.3346e-04 - val_loss: 8.1876e-06
Epoch 210/512
10/10 - 0s - loss: 8.1876e-04 - val_loss: 8.2088e-06
Epoch 211/512
10/10 - 0s - loss: 8.2088e-04 - val_loss: 8.0821e-06
Epoch 212/512
10/10 - 0s - loss: 8.0821e-04 - val_loss: 8.1119e-06
Epoch 213/512
10/10 - 0s - loss: 8.1119e-04 - val_loss: 8.0050e-06
Epoch 214/512
10/10 - 0s - loss: 8.0050e-04 - val_loss: 8.0388e-06
Epoch 215/512
10/10 - 0s - loss: 8.0388e-04 - val_loss: 7.9420e-06
Epoch 216/512
10/10 - 0s - loss: 7.9420e-04 - val_loss: 7.9688e-06
Epoch 217/512
10/10 - 0s - loss: 7.9688e-04 - val_loss: 7.8715e-06
Epoch 218/512
10/10 - 0s - loss: 7.8715e-04 - val_loss: 7.8836e-06
Epoch 219/512
10/10 - 0s - loss: 7.8836e-04 - val_loss: 7.7817e-06
Epoch 220/512
10/10 - 0s - loss: 7.7817e-04 - val_loss: 7.7792e-06
Epoch 221/512
10/10 - 0s - loss: 7.7792e-04 - val_loss: 7.6770e-06
Epoch 222/512
10/10 - 0s - loss: 7.6770e-04 - val_loss: 7.6670e-06
Epoch 223/512
10/10 - 0s - loss: 7.6670e-04 - val_loss: 7.5727e-06
Epoch 224/512
10/10 - 0s - loss: 7.5727e-04 - val_loss: 7.5630e-06
Epoch 225/512
10/10 - 0s - loss: 7.5630e-04 - val_loss: 7.4815e-06
Epoch 226/512
10/10 - 0s - loss: 7.4815e-04 - val_loss: 7.4745e-06
Epoch 227/512
10/10 - 0s - loss: 7.4745e-04 - val_loss: 7.4048e-06
Epoch 228/512
10/10 - 0s - loss: 7.4048e-04 - val_loss: 7.3975e-06
Epoch 229/512
10/10 - 0s - loss: 7.3975e-04 - val_loss: 7.3342e-06
Epoch 230/512
10/10 - 0s - loss: 7.3342e-04 - val_loss: 7.3211e-06
Epoch 231/512
10/10 - 0s - loss: 7.3211e-04 - val_loss: 7.2588e-06
Epoch 232/512
10/10 - 0s - loss: 7.2588e-04 - val_loss: 7.2361e-06
Epoch 233/512
10/10 - 0s - loss: 7.2361e-04 - val_loss: 7.1733e-06
Epoch 234/512
10/10 - 0s - loss: 7.1733e-04 - val_loss: 7.1420e-06
Epoch 235/512
10/10 - 0s - loss: 7.1420e-04 - val_loss: 7.0814e-06
Epoch 236/512
10/10 - 0s - loss: 7.0814e-04 - val_loss: 7.0457e-06
Epoch 237/512
10/10 - 0s - loss: 7.0457e-04 - val_loss: 6.9913e-06
Epoch 238/512
10/10 - 0s - loss: 6.9913e-04 - val_loss: 6.9548e-06
Epoch 239/512
10/10 - 0s - loss: 6.9548e-04 - val_loss: 6.9088e-06
Epoch 240/512
10/10 - 0s - loss: 6.9088e-04 - val_loss: 6.8726e-06
Epoch 241/512
10/10 - 0s - loss: 6.8726e-04 - val_loss: 6.8340e-06
Epoch 242/512
10/10 - 0s - loss: 6.8340e-04 - val_loss: 6.7960e-06
Epoch 243/512
10/10 - 0s - loss: 6.7960e-04 - val_loss: 6.7617e-06
Epoch 244/512
10/10 - 0s - loss: 6.7617e-04 - val_loss: 6.7191e-06
Epoch 245/512
10/10 - 0s - loss: 6.7191e-04 - val_loss: 6.6867e-06
Epoch 246/512
10/10 - 0s - loss: 6.6867e-04 - val_loss: 6.6379e-06
Epoch 247/512
10/10 - 0s - loss: 6.6379e-04 - val_loss: 6.6067e-06
Epoch 248/512
10/10 - 0s - loss: 6.6067e-04 - val_loss: 6.5528e-06
Epoch 249/512
10/10 - 0s - loss: 6.5528e-04 - val_loss: 6.5243e-06
Epoch 250/512
10/10 - 0s - loss: 6.5243e-04 - val_loss: 6.4674e-06
Epoch 251/512
10/10 - 0s - loss: 6.4674e-04 - val_loss: 6.4437e-06
Epoch 252/512
10/10 - 0s - loss: 6.4437e-04 - val_loss: 6.3856e-06
Epoch 253/512
10/10 - 0s - loss: 6.3856e-04 - val_loss: 6.3675e-06
Epoch 254/512
10/10 - 0s - loss: 6.3675e-04 - val_loss: 6.3087e-06
Epoch 255/512
10/10 - 0s - loss: 6.3087e-04 - val_loss: 6.2954e-06
Epoch 256/512
10/10 - 0s - loss: 6.2954e-04 - val_loss: 6.2346e-06
Epoch 257/512
10/10 - 0s - loss: 6.2346e-04 - val_loss: 6.2247e-06
Epoch 258/512
10/10 - 0s - loss: 6.2247e-04 - val_loss: 6.1606e-06
Epoch 259/512
10/10 - 0s - loss: 6.1606e-04 - val_loss: 6.1526e-06
Epoch 260/512
10/10 - 0s - loss: 6.1526e-04 - val_loss: 6.0845e-06
Epoch 261/512
10/10 - 0s - loss: 6.0845e-04 - val_loss: 6.0782e-06
Epoch 262/512
10/10 - 0s - loss: 6.0782e-04 - val_loss: 6.0068e-06
Epoch 263/512
10/10 - 0s - loss: 6.0068e-04 - val_loss: 6.0031e-06
Epoch 264/512
10/10 - 0s - loss: 6.0031e-04 - val_loss: 5.9296e-06
Epoch 265/512
10/10 - 0s - loss: 5.9296e-04 - val_loss: 5.9294e-06
Epoch 266/512
10/10 - 0s - loss: 5.9294e-04 - val_loss: 5.8548e-06
Epoch 267/512
10/10 - 0s - loss: 5.8548e-04 - val_loss: 5.8586e-06
Epoch 268/512
10/10 - 0s - loss: 5.8586e-04 - val_loss: 5.7830e-06
Epoch 269/512
10/10 - 0s - loss: 5.7830e-04 - val_loss: 5.7902e-06
Epoch 270/512
10/10 - 0s - loss: 5.7902e-04 - val_loss: 5.7129e-06
Epoch 271/512
10/10 - 0s - loss: 5.7129e-04 - val_loss: 5.7226e-06
Epoch 272/512
10/10 - 0s - loss: 5.7226e-04 - val_loss: 5.6429e-06
Epoch 273/512
10/10 - 0s - loss: 5.6429e-04 - val_loss: 5.6543e-06
Epoch 274/512
10/10 - 0s - loss: 5.6543e-04 - val_loss: 5.5720e-06
Epoch 275/512
10/10 - 0s - loss: 5.5720e-04 - val_loss: 5.5851e-06
Epoch 276/512
10/10 - 0s - loss: 5.5851e-04 - val_loss: 5.5007e-06
Epoch 277/512
10/10 - 0s - loss: 5.5007e-04 - val_loss: 5.5160e-06
Epoch 278/512
10/10 - 0s - loss: 5.5160e-04 - val_loss: 5.4302e-06
Epoch 279/512
10/10 - 0s - loss: 5.4302e-04 - val_loss: 5.4482e-06
Epoch 280/512
10/10 - 0s - loss: 5.4482e-04 - val_loss: 5.3616e-06
Epoch 281/512
10/10 - 0s - loss: 5.3616e-04 - val_loss: 5.3824e-06
Epoch 282/512
10/10 - 0s - loss: 5.3824e-04 - val_loss: 5.2948e-06
Epoch 283/512
10/10 - 0s - loss: 5.2948e-04 - val_loss: 5.3179e-06
Epoch 284/512
10/10 - 0s - loss: 5.3179e-04 - val_loss: 5.2290e-06
Epoch 285/512
10/10 - 0s - loss: 5.2290e-04 - val_loss: 5.2540e-06
Epoch 286/512
10/10 - 0s - loss: 5.2540e-04 - val_loss: 5.1635e-06
Epoch 287/512
10/10 - 0s - loss: 5.1635e-04 - val_loss: 5.1899e-06
Epoch 288/512
10/10 - 0s - loss: 5.1899e-04 - val_loss: 5.0977e-06
Epoch 289/512
10/10 - 0s - loss: 5.0977e-04 - val_loss: 5.1257e-06
Epoch 290/512
10/10 - 0s - loss: 5.1257e-04 - val_loss: 5.0323e-06
Epoch 291/512
10/10 - 0s - loss: 5.0323e-04 - val_loss: 5.0619e-06
Epoch 292/512
10/10 - 0s - loss: 5.0619e-04 - val_loss: 4.9676e-06
Epoch 293/512
10/10 - 0s - loss: 4.9676e-04 - val_loss: 4.9993e-06
Epoch 294/512
10/10 - 0s - loss: 4.9993e-04 - val_loss: 4.9042e-06
Epoch 295/512
10/10 - 0s - loss: 4.9042e-04 - val_loss: 4.9378e-06
Epoch 296/512
10/10 - 0s - loss: 4.9378e-04 - val_loss: 4.8421e-06
Epoch 297/512
10/10 - 0s - loss: 4.8421e-04 - val_loss: 4.8774e-06
Epoch 298/512
10/10 - 0s - loss: 4.8774e-04 - val_loss: 4.7809e-06
Epoch 299/512
10/10 - 0s - loss: 4.7809e-04 - val_loss: 4.8176e-06
Epoch 300/512
10/10 - 0s - loss: 4.8176e-04 - val_loss: 4.7200e-06
Epoch 301/512
10/10 - 0s - loss: 4.7200e-04 - val_loss: 4.7580e-06
Epoch 302/512
10/10 - 0s - loss: 4.7580e-04 - val_loss: 4.6594e-06
Epoch 303/512
10/10 - 0s - loss: 4.6594e-04 - val_loss: 4.6985e-06
Epoch 304/512
10/10 - 0s - loss: 4.6985e-04 - val_loss: 4.5991e-06
Epoch 305/512
10/10 - 0s - loss: 4.5991e-04 - val_loss: 4.6395e-06
Epoch 306/512
10/10 - 0s - loss: 4.6395e-04 - val_loss: 4.5396e-06
Epoch 307/512
10/10 - 0s - loss: 4.5396e-04 - val_loss: 4.5813e-06
Epoch 308/512
10/10 - 0s - loss: 4.5813e-04 - val_loss: 4.4810e-06
Epoch 309/512
10/10 - 0s - loss: 4.4810e-04 - val_loss: 4.5241e-06
Epoch 310/512
10/10 - 0s - loss: 4.5241e-04 - val_loss: 4.4234e-06
Epoch 311/512
10/10 - 0s - loss: 4.4234e-04 - val_loss: 4.4678e-06
Epoch 312/512
10/10 - 0s - loss: 4.4678e-04 - val_loss: 4.3667e-06
Epoch 313/512
10/10 - 0s - loss: 4.3667e-04 - val_loss: 4.4122e-06
Epoch 314/512
10/10 - 0s - loss: 4.4122e-04 - val_loss: 4.3104e-06
Epoch 315/512
10/10 - 0s - loss: 4.3104e-04 - val_loss: 4.3567e-06
Epoch 316/512
10/10 - 0s - loss: 4.3567e-04 - val_loss: 4.2544e-06
Epoch 317/512
10/10 - 0s - loss: 4.2544e-04 - val_loss: 4.3015e-06
Epoch 318/512
10/10 - 0s - loss: 4.3015e-04 - val_loss: 4.1988e-06
Epoch 319/512
10/10 - 0s - loss: 4.1988e-04 - val_loss: 4.2469e-06
Epoch 320/512
10/10 - 0s - loss: 4.2469e-04 - val_loss: 4.1440e-06
Epoch 321/512
10/10 - 0s - loss: 4.1440e-04 - val_loss: 4.1931e-06
Epoch 322/512
10/10 - 0s - loss: 4.1931e-04 - val_loss: 4.0901e-06
Epoch 323/512
10/10 - 0s - loss: 4.0901e-04 - val_loss: 4.1402e-06
Epoch 324/512
10/10 - 0s - loss: 4.1402e-04 - val_loss: 4.0370e-06
Epoch 325/512
10/10 - 0s - loss: 4.0370e-04 - val_loss: 4.0878e-06
Epoch 326/512
10/10 - 0s - loss: 4.0878e-04 - val_loss: 3.9844e-06
Epoch 327/512
10/10 - 0s - loss: 3.9844e-04 - val_loss: 4.0359e-06
Epoch 328/512
10/10 - 0s - loss: 4.0359e-04 - val_loss: 3.9323e-06
Epoch 329/512
10/10 - 0s - loss: 3.9323e-04 - val_loss: 3.9846e-06
Epoch 330/512
10/10 - 0s - loss: 3.9846e-04 - val_loss: 3.8807e-06
Epoch 331/512
10/10 - 0s - loss: 3.8807e-04 - val_loss: 3.9336e-06
Epoch 332/512
10/10 - 0s - loss: 3.9336e-04 - val_loss: 3.8297e-06
Epoch 333/512
10/10 - 0s - loss: 3.8297e-04 - val_loss: 3.8832e-06
Epoch 334/512
10/10 - 0s - loss: 3.8832e-04 - val_loss: 3.7792e-06
Epoch 335/512
10/10 - 0s - loss: 3.7792e-04 - val_loss: 3.8332e-06
Epoch 336/512
10/10 - 0s - loss: 3.8332e-04 - val_loss: 3.7294e-06
Epoch 337/512
10/10 - 0s - loss: 3.7294e-04 - val_loss: 3.7841e-06
Epoch 338/512
10/10 - 0s - loss: 3.7841e-04 - val_loss: 3.6804e-06
Epoch 339/512
10/10 - 0s - loss: 3.6804e-04 - val_loss: 3.7357e-06
Epoch 340/512
10/10 - 0s - loss: 3.7357e-04 - val_loss: 3.6319e-06
Epoch 341/512
10/10 - 0s - loss: 3.6319e-04 - val_loss: 3.6877e-06
Epoch 342/512
10/10 - 0s - loss: 3.6877e-04 - val_loss: 3.5839e-06
Epoch 343/512
10/10 - 0s - loss: 3.5839e-04 - val_loss: 3.6401e-06
Epoch 344/512
10/10 - 0s - loss: 3.6401e-04 - val_loss: 3.5364e-06
Epoch 345/512
10/10 - 0s - loss: 3.5364e-04 - val_loss: 3.5929e-06
Epoch 346/512
10/10 - 0s - loss: 3.5929e-04 - val_loss: 3.4893e-06
Epoch 347/512
10/10 - 0s - loss: 3.4893e-04 - val_loss: 3.5463e-06
Epoch 348/512
10/10 - 0s - loss: 3.5463e-04 - val_loss: 3.4429e-06
Epoch 349/512
10/10 - 0s - loss: 3.4429e-04 - val_loss: 3.5003e-06
Epoch 350/512
10/10 - 0s - loss: 3.5003e-04 - val_loss: 3.3972e-06
Epoch 351/512
10/10 - 0s - loss: 3.3972e-04 - val_loss: 3.4549e-06
Epoch 352/512
10/10 - 0s - loss: 3.4549e-04 - val_loss: 3.3520e-06
Epoch 353/512
10/10 - 0s - loss: 3.3520e-04 - val_loss: 3.4101e-06
Epoch 354/512
10/10 - 0s - loss: 3.4101e-04 - val_loss: 3.3074e-06
Epoch 355/512
10/10 - 0s - loss: 3.3074e-04 - val_loss: 3.3657e-06
Epoch 356/512
10/10 - 0s - loss: 3.3657e-04 - val_loss: 3.2632e-06
Epoch 357/512
10/10 - 0s - loss: 3.2632e-04 - val_loss: 3.3217e-06
Epoch 358/512
10/10 - 0s - loss: 3.3217e-04 - val_loss: 3.2194e-06
Epoch 359/512
10/10 - 0s - loss: 3.2194e-04 - val_loss: 3.2781e-06
Epoch 360/512
10/10 - 0s - loss: 3.2781e-04 - val_loss: 3.1762e-06
Epoch 361/512
10/10 - 0s - loss: 3.1762e-04 - val_loss: 3.2351e-06
Epoch 362/512
10/10 - 0s - loss: 3.2351e-04 - val_loss: 3.1335e-06
Epoch 363/512
10/10 - 0s - loss: 3.1335e-04 - val_loss: 3.1926e-06
Epoch 364/512
10/10 - 0s - loss: 3.1926e-04 - val_loss: 3.0913e-06
Epoch 365/512
10/10 - 0s - loss: 3.0913e-04 - val_loss: 3.1507e-06
Epoch 366/512
10/10 - 0s - loss: 3.1507e-04 - val_loss: 3.0498e-06
Epoch 367/512
10/10 - 0s - loss: 3.0498e-04 - val_loss: 3.1092e-06
Epoch 368/512
10/10 - 0s - loss: 3.1092e-04 - val_loss: 3.0086e-06
Epoch 369/512
10/10 - 0s - loss: 3.0086e-04 - val_loss: 3.0682e-06
Epoch 370/512
10/10 - 0s - loss: 3.0682e-04 - val_loss: 2.9681e-06
Epoch 371/512
10/10 - 0s - loss: 2.9681e-04 - val_loss: 3.0278e-06
Epoch 372/512
10/10 - 0s - loss: 3.0278e-04 - val_loss: 2.9280e-06
Epoch 373/512
10/10 - 0s - loss: 2.9280e-04 - val_loss: 2.9877e-06
Epoch 374/512
10/10 - 0s - loss: 2.9877e-04 - val_loss: 2.8882e-06
Epoch 375/512
10/10 - 0s - loss: 2.8882e-04 - val_loss: 2.9480e-06
Epoch 376/512
10/10 - 0s - loss: 2.9480e-04 - val_loss: 2.8490e-06
Epoch 377/512
10/10 - 0s - loss: 2.8490e-04 - val_loss: 2.9088e-06
Epoch 378/512
10/10 - 0s - loss: 2.9088e-04 - val_loss: 2.8102e-06
Epoch 379/512
10/10 - 0s - loss: 2.8102e-04 - val_loss: 2.8701e-06
Epoch 380/512
10/10 - 0s - loss: 2.8701e-04 - val_loss: 2.7719e-06
Epoch 381/512
10/10 - 0s - loss: 2.7719e-04 - val_loss: 2.8318e-06
Epoch 382/512
10/10 - 0s - loss: 2.8318e-04 - val_loss: 2.7341e-06
Epoch 383/512
10/10 - 0s - loss: 2.7341e-04 - val_loss: 2.7940e-06
Epoch 384/512
10/10 - 0s - loss: 2.7940e-04 - val_loss: 2.6969e-06
Epoch 385/512
10/10 - 0s - loss: 2.6969e-04 - val_loss: 2.7568e-06
Epoch 386/512
10/10 - 0s - loss: 2.7568e-04 - val_loss: 2.6601e-06
Epoch 387/512
10/10 - 0s - loss: 2.6601e-04 - val_loss: 2.7199e-06
Epoch 388/512
10/10 - 0s - loss: 2.7199e-04 - val_loss: 2.6237e-06
Epoch 389/512
10/10 - 0s - loss: 2.6237e-04 - val_loss: 2.6833e-06
Epoch 390/512
10/10 - 0s - loss: 2.6833e-04 - val_loss: 2.5875e-06
Epoch 391/512
10/10 - 0s - loss: 2.5875e-04 - val_loss: 2.6471e-06
Epoch 392/512
10/10 - 0s - loss: 2.6471e-04 - val_loss: 2.5519e-06
Epoch 393/512
10/10 - 0s - loss: 2.5519e-04 - val_loss: 2.6115e-06
Epoch 394/512
10/10 - 0s - loss: 2.6115e-04 - val_loss: 2.5168e-06
Epoch 395/512
10/10 - 0s - loss: 2.5168e-04 - val_loss: 2.5763e-06
Epoch 396/512
10/10 - 0s - loss: 2.5763e-04 - val_loss: 2.4822e-06
Epoch 397/512
10/10 - 0s - loss: 2.4822e-04 - val_loss: 2.5416e-06
Epoch 398/512
10/10 - 0s - loss: 2.5416e-04 - val_loss: 2.4480e-06
Epoch 399/512
10/10 - 0s - loss: 2.4480e-04 - val_loss: 2.5073e-06
Epoch 400/512
10/10 - 0s - loss: 2.5073e-04 - val_loss: 2.4142e-06
Epoch 401/512
10/10 - 0s - loss: 2.4142e-04 - val_loss: 2.4733e-06
Epoch 402/512
10/10 - 0s - loss: 2.4733e-04 - val_loss: 2.3807e-06
Epoch 403/512
10/10 - 0s - loss: 2.3807e-04 - val_loss: 2.4396e-06
Epoch 404/512
10/10 - 0s - loss: 2.4396e-04 - val_loss: 2.3476e-06
Epoch 405/512
10/10 - 0s - loss: 2.3476e-04 - val_loss: 2.4064e-06
Epoch 406/512
10/10 - 0s - loss: 2.4064e-04 - val_loss: 2.3149e-06
Epoch 407/512
10/10 - 0s - loss: 2.3149e-04 - val_loss: 2.3736e-06
Epoch 408/512
10/10 - 0s - loss: 2.3736e-04 - val_loss: 2.2827e-06
Epoch 409/512
10/10 - 0s - loss: 2.2827e-04 - val_loss: 2.3412e-06
Epoch 410/512
10/10 - 0s - loss: 2.3412e-04 - val_loss: 2.2509e-06
Epoch 411/512
10/10 - 0s - loss: 2.2509e-04 - val_loss: 2.3092e-06
Epoch 412/512
10/10 - 0s - loss: 2.3092e-04 - val_loss: 2.2195e-06
Epoch 413/512
10/10 - 0s - loss: 2.2195e-04 - val_loss: 2.2777e-06
Epoch 414/512
10/10 - 0s - loss: 2.2777e-04 - val_loss: 2.1885e-06
Epoch 415/512
10/10 - 0s - loss: 2.1885e-04 - val_loss: 2.2464e-06
Epoch 416/512
10/10 - 0s - loss: 2.2464e-04 - val_loss: 2.1578e-06
Epoch 417/512
10/10 - 0s - loss: 2.1578e-04 - val_loss: 2.2155e-06
Epoch 418/512
10/10 - 0s - loss: 2.2155e-04 - val_loss: 2.1275e-06
Epoch 419/512
10/10 - 0s - loss: 2.1275e-04 - val_loss: 2.1850e-06
Epoch 420/512
10/10 - 0s - loss: 2.1850e-04 - val_loss: 2.0976e-06
Epoch 421/512
10/10 - 0s - loss: 2.0976e-04 - val_loss: 2.1548e-06
Epoch 422/512
10/10 - 0s - loss: 2.1548e-04 - val_loss: 2.0680e-06
Epoch 423/512
10/10 - 0s - loss: 2.0680e-04 - val_loss: 2.1251e-06
Epoch 424/512
10/10 - 0s - loss: 2.1251e-04 - val_loss: 2.0389e-06
Epoch 425/512
10/10 - 0s - loss: 2.0389e-04 - val_loss: 2.0957e-06
Epoch 426/512
10/10 - 0s - loss: 2.0957e-04 - val_loss: 2.0102e-06
Epoch 427/512
10/10 - 0s - loss: 2.0102e-04 - val_loss: 2.0667e-06
Epoch 428/512
10/10 - 0s - loss: 2.0667e-04 - val_loss: 1.9817e-06
Epoch 429/512
10/10 - 0s - loss: 1.9817e-04 - val_loss: 2.0379e-06
Epoch 430/512
10/10 - 0s - loss: 2.0379e-04 - val_loss: 1.9535e-06
Epoch 431/512
10/10 - 0s - loss: 1.9535e-04 - val_loss: 2.0095e-06
Epoch 432/512
10/10 - 0s - loss: 2.0095e-04 - val_loss: 1.9258e-06
Epoch 433/512
10/10 - 0s - loss: 1.9258e-04 - val_loss: 1.9816e-06
Epoch 434/512
10/10 - 0s - loss: 1.9816e-04 - val_loss: 1.8985e-06
Epoch 435/512
10/10 - 0s - loss: 1.8985e-04 - val_loss: 1.9540e-06
Epoch 436/512
10/10 - 0s - loss: 1.9540e-04 - val_loss: 1.8715e-06
Epoch 437/512
10/10 - 0s - loss: 1.8715e-04 - val_loss: 1.9268e-06
Epoch 438/512
10/10 - 0s - loss: 1.9268e-04 - val_loss: 1.8448e-06
Epoch 439/512
10/10 - 0s - loss: 1.8448e-04 - val_loss: 1.8997e-06
Epoch 440/512
10/10 - 0s - loss: 1.8997e-04 - val_loss: 1.8183e-06
Epoch 441/512
10/10 - 0s - loss: 1.8183e-04 - val_loss: 1.8729e-06
Epoch 442/512
10/10 - 0s - loss: 1.8729e-04 - val_loss: 1.7921e-06
Epoch 443/512
10/10 - 0s - loss: 1.7921e-04 - val_loss: 1.8465e-06
Epoch 444/512
10/10 - 0s - loss: 1.8465e-04 - val_loss: 1.7664e-06
Epoch 445/512
10/10 - 0s - loss: 1.7664e-04 - val_loss: 1.8205e-06
Epoch 446/512
10/10 - 0s - loss: 1.8205e-04 - val_loss: 1.7411e-06
Epoch 447/512
10/10 - 0s - loss: 1.7411e-04 - val_loss: 1.7949e-06
Epoch 448/512
10/10 - 0s - loss: 1.7949e-04 - val_loss: 1.7161e-06
Epoch 449/512
10/10 - 0s - loss: 1.7161e-04 - val_loss: 1.7696e-06
Epoch 450/512
10/10 - 0s - loss: 1.7696e-04 - val_loss: 1.6914e-06
Epoch 451/512
10/10 - 0s - loss: 1.6914e-04 - val_loss: 1.7446e-06
Epoch 452/512
10/10 - 0s - loss: 1.7446e-04 - val_loss: 1.6670e-06
Epoch 453/512
10/10 - 0s - loss: 1.6670e-04 - val_loss: 1.7198e-06
Epoch 454/512
10/10 - 0s - loss: 1.7198e-04 - val_loss: 1.6427e-06
Epoch 455/512
10/10 - 0s - loss: 1.6427e-04 - val_loss: 1.6953e-06
Epoch 456/512
10/10 - 0s - loss: 1.6953e-04 - val_loss: 1.6189e-06
Epoch 457/512
10/10 - 0s - loss: 1.6189e-04 - val_loss: 1.6711e-06
Epoch 458/512
10/10 - 0s - loss: 1.6711e-04 - val_loss: 1.5954e-06
Epoch 459/512
10/10 - 0s - loss: 1.5954e-04 - val_loss: 1.6473e-06
Epoch 460/512
10/10 - 0s - loss: 1.6473e-04 - val_loss: 1.5722e-06
Epoch 461/512
10/10 - 0s - loss: 1.5722e-04 - val_loss: 1.6238e-06
Epoch 462/512
10/10 - 0s - loss: 1.6238e-04 - val_loss: 1.5493e-06
Epoch 463/512
10/10 - 0s - loss: 1.5493e-04 - val_loss: 1.6006e-06
Epoch 464/512
10/10 - 0s - loss: 1.6006e-04 - val_loss: 1.5266e-06
Epoch 465/512
10/10 - 0s - loss: 1.5266e-04 - val_loss: 1.5775e-06
Epoch 466/512
10/10 - 0s - loss: 1.5775e-04 - val_loss: 1.5042e-06
Epoch 467/512
10/10 - 0s - loss: 1.5042e-04 - val_loss: 1.5548e-06
Epoch 468/512
10/10 - 0s - loss: 1.5548e-04 - val_loss: 1.4822e-06
Epoch 469/512
10/10 - 0s - loss: 1.4822e-04 - val_loss: 1.5325e-06
Epoch 470/512
10/10 - 0s - loss: 1.5325e-04 - val_loss: 1.4605e-06
Epoch 471/512
10/10 - 0s - loss: 1.4605e-04 - val_loss: 1.5104e-06
Epoch 472/512
10/10 - 0s - loss: 1.5104e-04 - val_loss: 1.4390e-06
Epoch 473/512
10/10 - 0s - loss: 1.4390e-04 - val_loss: 1.4886e-06
Epoch 474/512
10/10 - 0s - loss: 1.4886e-04 - val_loss: 1.4177e-06
Epoch 475/512
10/10 - 0s - loss: 1.4177e-04 - val_loss: 1.4670e-06
Epoch 476/512
10/10 - 0s - loss: 1.4670e-04 - val_loss: 1.3968e-06
Epoch 477/512
10/10 - 0s - loss: 1.3968e-04 - val_loss: 1.4456e-06
Epoch 478/512
10/10 - 0s - loss: 1.4456e-04 - val_loss: 1.3760e-06
Epoch 479/512
10/10 - 0s - loss: 1.3760e-04 - val_loss: 1.4246e-06
Epoch 480/512
10/10 - 0s - loss: 1.4246e-04 - val_loss: 1.3556e-06
Epoch 481/512
10/10 - 0s - loss: 1.3556e-04 - val_loss: 1.4038e-06
Epoch 482/512
10/10 - 0s - loss: 1.4038e-04 - val_loss: 1.3355e-06
Epoch 483/512
10/10 - 0s - loss: 1.3355e-04 - val_loss: 1.3834e-06
Epoch 484/512
10/10 - 0s - loss: 1.3834e-04 - val_loss: 1.3156e-06
Epoch 485/512
10/10 - 0s - loss: 1.3156e-04 - val_loss: 1.3632e-06
Epoch 486/512
10/10 - 0s - loss: 1.3632e-04 - val_loss: 1.2961e-06
Epoch 487/512
10/10 - 0s - loss: 1.2961e-04 - val_loss: 1.3433e-06
Epoch 488/512
10/10 - 0s - loss: 1.3433e-04 - val_loss: 1.2767e-06
Epoch 489/512
10/10 - 0s - loss: 1.2767e-04 - val_loss: 1.3236e-06
Epoch 490/512
10/10 - 0s - loss: 1.3236e-04 - val_loss: 1.2575e-06
Epoch 491/512
10/10 - 0s - loss: 1.2575e-04 - val_loss: 1.3040e-06
Epoch 492/512
10/10 - 0s - loss: 1.3040e-04 - val_loss: 1.2386e-06
Epoch 493/512
10/10 - 0s - loss: 1.2386e-04 - val_loss: 1.2847e-06
Epoch 494/512
10/10 - 0s - loss: 1.2847e-04 - val_loss: 1.2199e-06
Epoch 495/512
10/10 - 0s - loss: 1.2199e-04 - val_loss: 1.2657e-06
Epoch 496/512
10/10 - 0s - loss: 1.2657e-04 - val_loss: 1.2016e-06
Epoch 497/512
10/10 - 0s - loss: 1.2016e-04 - val_loss: 1.2470e-06
Epoch 498/512
10/10 - 0s - loss: 1.2470e-04 - val_loss: 1.1835e-06
Epoch 499/512
10/10 - 0s - loss: 1.1835e-04 - val_loss: 1.2286e-06
Epoch 500/512
10/10 - 0s - loss: 1.2286e-04 - val_loss: 1.1656e-06
Epoch 501/512
10/10 - 0s - loss: 1.1656e-04 - val_loss: 1.2103e-06
Epoch 502/512
10/10 - 0s - loss: 1.2103e-04 - val_loss: 1.1479e-06
Epoch 503/512
10/10 - 0s - loss: 1.1479e-04 - val_loss: 1.1923e-06
Epoch 504/512
10/10 - 0s - loss: 1.1923e-04 - val_loss: 1.1305e-06
Epoch 505/512
10/10 - 0s - loss: 1.1305e-04 - val_loss: 1.1745e-06
Epoch 506/512
10/10 - 0s - loss: 1.1745e-04 - val_loss: 1.1132e-06
Epoch 507/512
10/10 - 0s - loss: 1.1132e-04 - val_loss: 1.1569e-06
Epoch 508/512
10/10 - 0s - loss: 1.1569e-04 - val_loss: 1.0962e-06
Epoch 509/512
10/10 - 0s - loss: 1.0962e-04 - val_loss: 1.1395e-06
Epoch 510/512
10/10 - 0s - loss: 1.1395e-04 - val_loss: 1.0795e-06
Epoch 511/512
10/10 - 0s - loss: 1.0795e-04 - val_loss: 1.1225e-06
Epoch 512/512
10/10 - 0s - loss: 1.1225e-04 - val_loss: 1.0630e-06
2024-04-12 10:04:28.741166: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 10 samples, validate on 10 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00014, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1180e-04 - val_loss: 1.4124e-04
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00014
10/10 - 0s - loss: 1.4102e-04 - val_loss: 1.5292e-04
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00014
10/10 - 0s - loss: 1.5273e-04 - val_loss: 1.6812e-04
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00014
10/10 - 0s - loss: 1.6786e-04 - val_loss: 1.4945e-04
Epoch 5/512

Epoch 00005: val_loss improved from 0.00014 to 0.00014, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4927e-04 - val_loss: 1.3921e-04
Epoch 6/512

Epoch 00006: val_loss improved from 0.00014 to 0.00011, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3899e-04 - val_loss: 1.1057e-04
Epoch 7/512

Epoch 00007: val_loss improved from 0.00011 to 0.00010, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1044e-04 - val_loss: 9.8543e-05
Epoch 8/512

Epoch 00008: val_loss improved from 0.00010 to 0.00008, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.8381e-05 - val_loss: 8.1593e-05
Epoch 9/512

Epoch 00009: val_loss improved from 0.00008 to 0.00008, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.1506e-05 - val_loss: 7.8346e-05
Epoch 10/512

Epoch 00010: val_loss improved from 0.00008 to 0.00007, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.8214e-05 - val_loss: 7.3104e-05
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.3029e-05 - val_loss: 7.8786e-05
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.8653e-05 - val_loss: 8.2160e-05
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.2076e-05 - val_loss: 9.6190e-05
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.6029e-05 - val_loss: 1.0413e-04
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.0402e-04 - val_loss: 1.2144e-04
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.2124e-04 - val_loss: 1.2289e-04
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.2276e-04 - val_loss: 1.3093e-04
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.3072e-04 - val_loss: 1.1774e-04
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.1763e-04 - val_loss: 1.1404e-04
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.1385e-04 - val_loss: 9.6431e-05
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.6342e-05 - val_loss: 9.1407e-05
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.1248e-05 - val_loss: 7.9663e-05
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.9594e-05 - val_loss: 7.9437e-05
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.9296e-05 - val_loss: 7.4872e-05
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.4810e-05 - val_loss: 8.0431e-05
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.0288e-05 - val_loss: 8.1178e-05
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.1111e-05 - val_loss: 9.1458e-05
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.1296e-05 - val_loss: 9.3641e-05
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.3563e-05 - val_loss: 1.0435e-04
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.0417e-04 - val_loss: 1.0168e-04
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.0160e-04 - val_loss: 1.0683e-04
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00007
10/10 - 0s - loss: 1.0665e-04 - val_loss: 9.6950e-05
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.6873e-05 - val_loss: 9.6416e-05
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.6243e-05 - val_loss: 8.4694e-05
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.4632e-05 - val_loss: 8.3510e-05
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.3356e-05 - val_loss: 7.5063e-05
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.5012e-05 - val_loss: 7.6651e-05
Epoch 38/512

Epoch 00038: val_loss improved from 0.00007 to 0.00007, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.6507e-05 - val_loss: 7.2508e-05
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.2460e-05 - val_loss: 7.7626e-05
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.7480e-05 - val_loss: 7.6474e-05
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.6424e-05 - val_loss: 8.4046e-05
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.3889e-05 - val_loss: 8.3048e-05
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.2993e-05 - val_loss: 9.0200e-05
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00007
10/10 - 0s - loss: 9.0033e-05 - val_loss: 8.6064e-05
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.6007e-05 - val_loss: 8.9995e-05
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.9827e-05 - val_loss: 8.2263e-05
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.2212e-05 - val_loss: 8.3349e-05
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00007
10/10 - 0s - loss: 8.3190e-05 - val_loss: 7.4933e-05
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.4890e-05 - val_loss: 7.5776e-05
Epoch 50/512

Epoch 00050: val_loss improved from 0.00007 to 0.00007, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.5629e-05 - val_loss: 6.9316e-05
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00007
10/10 - 0s - loss: 6.9279e-05 - val_loss: 7.1812e-05
Epoch 52/512

Epoch 00052: val_loss improved from 0.00007 to 0.00007, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.1671e-05 - val_loss: 6.7890e-05
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00007
10/10 - 0s - loss: 6.7856e-05 - val_loss: 7.2447e-05
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.2305e-05 - val_loss: 7.0113e-05
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.0077e-05 - val_loss: 7.5867e-05
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.5719e-05 - val_loss: 7.3285e-05
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.3248e-05 - val_loss: 7.8496e-05
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.8342e-05 - val_loss: 7.4019e-05
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.3982e-05 - val_loss: 7.7384e-05
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.7231e-05 - val_loss: 7.1106e-05
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00007
10/10 - 0s - loss: 7.1073e-05 - val_loss: 7.3015e-05
Epoch 62/512

Epoch 00062: val_loss improved from 0.00007 to 0.00007, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.2869e-05 - val_loss: 6.6553e-05
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00007
10/10 - 0s - loss: 6.6525e-05 - val_loss: 6.8412e-05
Epoch 64/512

Epoch 00064: val_loss improved from 0.00007 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.8274e-05 - val_loss: 6.3159e-05
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.3135e-05 - val_loss: 6.6011e-05
Epoch 66/512

Epoch 00066: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.5876e-05 - val_loss: 6.2236e-05
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.2213e-05 - val_loss: 6.6253e-05
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.6117e-05 - val_loss: 6.3292e-05
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.3269e-05 - val_loss: 6.7857e-05
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.7718e-05 - val_loss: 6.4588e-05
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.4565e-05 - val_loss: 6.8691e-05
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.8550e-05 - val_loss: 6.4322e-05
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.4300e-05 - val_loss: 6.7368e-05
Epoch 74/512

Epoch 00074: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.7229e-05 - val_loss: 6.2109e-05
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.2090e-05 - val_loss: 6.4403e-05
Epoch 76/512

Epoch 00076: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.4269e-05 - val_loss: 5.9166e-05
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.9149e-05 - val_loss: 6.1484e-05
Epoch 78/512

Epoch 00078: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.1355e-05 - val_loss: 5.7005e-05
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.6991e-05 - val_loss: 5.9914e-05
Epoch 80/512

Epoch 00080: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.9787e-05 - val_loss: 5.6286e-05
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.6273e-05 - val_loss: 5.9831e-05
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.9704e-05 - val_loss: 5.6606e-05
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.6594e-05 - val_loss: 6.0375e-05
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00006
10/10 - 0s - loss: 6.0247e-05 - val_loss: 5.6905e-05
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.6893e-05 - val_loss: 6.0339e-05
Epoch 86/512

Epoch 00086: val_loss improved from 0.00006 to 0.00006, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.0211e-05 - val_loss: 5.6253e-05
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00006
10/10 - 0s - loss: 5.6242e-05 - val_loss: 5.9076e-05
Epoch 88/512

Epoch 00088: val_loss improved from 0.00006 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.8950e-05 - val_loss: 5.4559e-05
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.4549e-05 - val_loss: 5.6986e-05
Epoch 90/512

Epoch 00090: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.6863e-05 - val_loss: 5.2564e-05
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.2557e-05 - val_loss: 5.5035e-05
Epoch 92/512

Epoch 00092: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.4916e-05 - val_loss: 5.1090e-05
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.1084e-05 - val_loss: 5.3902e-05
Epoch 94/512

Epoch 00094: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.3784e-05 - val_loss: 5.0447e-05
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.0442e-05 - val_loss: 5.3593e-05
Epoch 96/512

Epoch 00096: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.3476e-05 - val_loss: 5.0337e-05
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.0332e-05 - val_loss: 5.3557e-05
Epoch 98/512

Epoch 00098: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.3439e-05 - val_loss: 5.0135e-05
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00005
10/10 - 0s - loss: 5.0131e-05 - val_loss: 5.3122e-05
Epoch 100/512

Epoch 00100: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.3005e-05 - val_loss: 4.9366e-05
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00005
10/10 - 0s - loss: 4.9362e-05 - val_loss: 5.1997e-05
Epoch 102/512

Epoch 00102: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.1882e-05 - val_loss: 4.8049e-05
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00005
10/10 - 0s - loss: 4.8047e-05 - val_loss: 5.0470e-05
Epoch 104/512

Epoch 00104: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.0358e-05 - val_loss: 4.6627e-05
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00005
10/10 - 0s - loss: 4.6626e-05 - val_loss: 4.9082e-05
Epoch 106/512

Epoch 00106: val_loss improved from 0.00005 to 0.00005, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.8972e-05 - val_loss: 4.5540e-05
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00005
10/10 - 0s - loss: 4.5540e-05 - val_loss: 4.8186e-05
Epoch 108/512

Epoch 00108: val_loss improved from 0.00005 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.8077e-05 - val_loss: 4.4930e-05
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.4931e-05 - val_loss: 4.7741e-05
Epoch 110/512

Epoch 00110: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.7634e-05 - val_loss: 4.4587e-05
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.4588e-05 - val_loss: 4.7407e-05
Epoch 112/512

Epoch 00112: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.7300e-05 - val_loss: 4.4155e-05
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.4157e-05 - val_loss: 4.6815e-05
Epoch 114/512

Epoch 00114: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.6709e-05 - val_loss: 4.3393e-05
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.3396e-05 - val_loss: 4.5842e-05
Epoch 116/512

Epoch 00116: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.5738e-05 - val_loss: 4.2346e-05
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.2349e-05 - val_loss: 4.4677e-05
Epoch 118/512

Epoch 00118: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.4575e-05 - val_loss: 4.1274e-05
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.1278e-05 - val_loss: 4.3625e-05
Epoch 120/512

Epoch 00120: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.3525e-05 - val_loss: 4.0418e-05
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00004
10/10 - 0s - loss: 4.0422e-05 - val_loss: 4.2869e-05
Epoch 122/512

Epoch 00122: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.2770e-05 - val_loss: 3.9832e-05
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.9836e-05 - val_loss: 4.2359e-05
Epoch 124/512

Epoch 00124: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.2261e-05 - val_loss: 3.9382e-05
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.9388e-05 - val_loss: 4.1890e-05
Epoch 126/512

Epoch 00126: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.1793e-05 - val_loss: 3.8862e-05
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.8868e-05 - val_loss: 4.1260e-05
Epoch 128/512

Epoch 00128: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.1164e-05 - val_loss: 3.8152e-05
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.8158e-05 - val_loss: 4.0422e-05
Epoch 130/512

Epoch 00130: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.0328e-05 - val_loss: 3.7299e-05
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.7305e-05 - val_loss: 3.9495e-05
Epoch 132/512

Epoch 00132: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.9403e-05 - val_loss: 3.6450e-05
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.6457e-05 - val_loss: 3.8658e-05
Epoch 134/512

Epoch 00134: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.8567e-05 - val_loss: 3.5746e-05
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.5754e-05 - val_loss: 3.8000e-05
Epoch 136/512

Epoch 00136: val_loss improved from 0.00004 to 0.00004, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.7911e-05 - val_loss: 3.5193e-05
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00004
10/10 - 0s - loss: 3.5201e-05 - val_loss: 3.7473e-05
Epoch 138/512

Epoch 00138: val_loss improved from 0.00004 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.7385e-05 - val_loss: 3.4705e-05
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.4712e-05 - val_loss: 3.6955e-05
Epoch 140/512

Epoch 00140: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.6868e-05 - val_loss: 3.4167e-05
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.4175e-05 - val_loss: 3.6342e-05
Epoch 142/512

Epoch 00142: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.6256e-05 - val_loss: 3.3527e-05
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.3536e-05 - val_loss: 3.5621e-05
Epoch 144/512

Epoch 00144: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.5536e-05 - val_loss: 3.2818e-05
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.2826e-05 - val_loss: 3.4861e-05
Epoch 146/512

Epoch 00146: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.4778e-05 - val_loss: 3.2122e-05
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.2131e-05 - val_loss: 3.4166e-05
Epoch 148/512

Epoch 00148: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.4084e-05 - val_loss: 3.1518e-05
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.1527e-05 - val_loss: 3.3578e-05
Epoch 150/512

Epoch 00150: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.3497e-05 - val_loss: 3.1002e-05
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.1011e-05 - val_loss: 3.3063e-05
Epoch 152/512

Epoch 00152: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.2983e-05 - val_loss: 3.0515e-05
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.0525e-05 - val_loss: 3.2547e-05
Epoch 154/512

Epoch 00154: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.2469e-05 - val_loss: 3.0003e-05
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00003
10/10 - 0s - loss: 3.0013e-05 - val_loss: 3.1979e-05
Epoch 156/512

Epoch 00156: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.1902e-05 - val_loss: 2.9432e-05
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.9442e-05 - val_loss: 3.1351e-05
Epoch 158/512

Epoch 00158: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.1275e-05 - val_loss: 2.8829e-05
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.8839e-05 - val_loss: 3.0713e-05
Epoch 160/512

Epoch 00160: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.0639e-05 - val_loss: 2.8244e-05
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.8255e-05 - val_loss: 3.0117e-05
Epoch 162/512

Epoch 00162: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.0044e-05 - val_loss: 2.7711e-05
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.7722e-05 - val_loss: 2.9585e-05
Epoch 164/512

Epoch 00164: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.9513e-05 - val_loss: 2.7234e-05
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.7244e-05 - val_loss: 2.9100e-05
Epoch 166/512

Epoch 00166: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.9029e-05 - val_loss: 2.6779e-05
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.6790e-05 - val_loss: 2.8616e-05
Epoch 168/512

Epoch 00168: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.8546e-05 - val_loss: 2.6307e-05
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.6318e-05 - val_loss: 2.8102e-05
Epoch 170/512

Epoch 00170: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.8034e-05 - val_loss: 2.5803e-05
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.5814e-05 - val_loss: 2.7555e-05
Epoch 172/512

Epoch 00172: val_loss improved from 0.00003 to 0.00003, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.7487e-05 - val_loss: 2.5283e-05
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00003
10/10 - 0s - loss: 2.5294e-05 - val_loss: 2.7006e-05
Epoch 174/512

Epoch 00174: val_loss improved from 0.00003 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.6939e-05 - val_loss: 2.4777e-05
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.4788e-05 - val_loss: 2.6486e-05
Epoch 176/512

Epoch 00176: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.6421e-05 - val_loss: 2.4310e-05
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.4321e-05 - val_loss: 2.6012e-05
Epoch 178/512

Epoch 00178: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.5948e-05 - val_loss: 2.3879e-05
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.3890e-05 - val_loss: 2.5567e-05
Epoch 180/512

Epoch 00180: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.5504e-05 - val_loss: 2.3459e-05
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.3470e-05 - val_loss: 2.5119e-05
Epoch 182/512

Epoch 00182: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.5057e-05 - val_loss: 2.3027e-05
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.3038e-05 - val_loss: 2.4651e-05
Epoch 184/512

Epoch 00184: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.4590e-05 - val_loss: 2.2577e-05
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.2588e-05 - val_loss: 2.4167e-05
Epoch 186/512

Epoch 00186: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.4107e-05 - val_loss: 2.2123e-05
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.2134e-05 - val_loss: 2.3691e-05
Epoch 188/512

Epoch 00188: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.3632e-05 - val_loss: 2.1688e-05
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.1699e-05 - val_loss: 2.3244e-05
Epoch 190/512

Epoch 00190: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.3186e-05 - val_loss: 2.1282e-05
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.1293e-05 - val_loss: 2.2822e-05
Epoch 192/512

Epoch 00192: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.2765e-05 - val_loss: 2.0890e-05
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.0901e-05 - val_loss: 2.2411e-05
Epoch 194/512

Epoch 00194: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.2355e-05 - val_loss: 2.0505e-05
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.0515e-05 - val_loss: 2.2001e-05
Epoch 196/512

Epoch 00196: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.1946e-05 - val_loss: 2.0118e-05
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00002
10/10 - 0s - loss: 2.0129e-05 - val_loss: 2.1587e-05
Epoch 198/512

Epoch 00198: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.1533e-05 - val_loss: 1.9724e-05
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.9735e-05 - val_loss: 2.1165e-05
Epoch 200/512

Epoch 00200: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.1112e-05 - val_loss: 1.9330e-05
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.9340e-05 - val_loss: 2.0750e-05
Epoch 202/512

Epoch 00202: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.0698e-05 - val_loss: 1.8948e-05
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.8959e-05 - val_loss: 2.0353e-05
Epoch 204/512

Epoch 00204: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.0302e-05 - val_loss: 1.8585e-05
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.8595e-05 - val_loss: 1.9973e-05
Epoch 206/512

Epoch 00206: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.9923e-05 - val_loss: 1.8235e-05
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.8245e-05 - val_loss: 1.9607e-05
Epoch 208/512

Epoch 00208: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.9557e-05 - val_loss: 1.7892e-05
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.7902e-05 - val_loss: 1.9240e-05
Epoch 210/512

Epoch 00210: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.9192e-05 - val_loss: 1.7546e-05
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.7557e-05 - val_loss: 1.8870e-05
Epoch 212/512

Epoch 00212: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.8823e-05 - val_loss: 1.7198e-05
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.7209e-05 - val_loss: 1.8500e-05
Epoch 214/512

Epoch 00214: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.8453e-05 - val_loss: 1.6854e-05
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.6864e-05 - val_loss: 1.8137e-05
Epoch 216/512

Epoch 00216: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.8091e-05 - val_loss: 1.6520e-05
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.6530e-05 - val_loss: 1.7786e-05
Epoch 218/512

Epoch 00218: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7741e-05 - val_loss: 1.6199e-05
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.6209e-05 - val_loss: 1.7448e-05
Epoch 220/512

Epoch 00220: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7404e-05 - val_loss: 1.5885e-05
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.5895e-05 - val_loss: 1.7116e-05
Epoch 222/512

Epoch 00222: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7073e-05 - val_loss: 1.5578e-05
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.5588e-05 - val_loss: 1.6791e-05
Epoch 224/512

Epoch 00224: val_loss improved from 0.00002 to 0.00002, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.6748e-05 - val_loss: 1.5274e-05
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00002
10/10 - 0s - loss: 1.5284e-05 - val_loss: 1.6466e-05
Epoch 226/512

Epoch 00226: val_loss improved from 0.00002 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.6424e-05 - val_loss: 1.4968e-05
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.4978e-05 - val_loss: 1.6139e-05
Epoch 228/512

Epoch 00228: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.6098e-05 - val_loss: 1.4666e-05
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.4676e-05 - val_loss: 1.5819e-05
Epoch 230/512

Epoch 00230: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5779e-05 - val_loss: 1.4372e-05
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.4381e-05 - val_loss: 1.5509e-05
Epoch 232/512

Epoch 00232: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5469e-05 - val_loss: 1.4088e-05
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.4098e-05 - val_loss: 1.5211e-05
Epoch 234/512

Epoch 00234: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5172e-05 - val_loss: 1.3814e-05
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.3823e-05 - val_loss: 1.4919e-05
Epoch 236/512

Epoch 00236: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4881e-05 - val_loss: 1.3542e-05
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.3551e-05 - val_loss: 1.4628e-05
Epoch 238/512

Epoch 00238: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4590e-05 - val_loss: 1.3270e-05
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.3279e-05 - val_loss: 1.4337e-05
Epoch 240/512

Epoch 00240: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4301e-05 - val_loss: 1.3001e-05
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.3010e-05 - val_loss: 1.4050e-05
Epoch 242/512

Epoch 00242: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4014e-05 - val_loss: 1.2737e-05
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.2746e-05 - val_loss: 1.3771e-05
Epoch 244/512

Epoch 00244: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3736e-05 - val_loss: 1.2481e-05
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.2490e-05 - val_loss: 1.3501e-05
Epoch 246/512

Epoch 00246: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3466e-05 - val_loss: 1.2232e-05
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.2241e-05 - val_loss: 1.3236e-05
Epoch 248/512

Epoch 00248: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3202e-05 - val_loss: 1.1988e-05
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.1996e-05 - val_loss: 1.2975e-05
Epoch 250/512

Epoch 00250: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2942e-05 - val_loss: 1.1746e-05
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.1755e-05 - val_loss: 1.2717e-05
Epoch 252/512

Epoch 00252: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2684e-05 - val_loss: 1.1507e-05
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.1515e-05 - val_loss: 1.2461e-05
Epoch 254/512

Epoch 00254: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2429e-05 - val_loss: 1.1271e-05
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.1280e-05 - val_loss: 1.2210e-05
Epoch 256/512

Epoch 00256: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2179e-05 - val_loss: 1.1041e-05
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.1049e-05 - val_loss: 1.1965e-05
Epoch 258/512

Epoch 00258: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1934e-05 - val_loss: 1.0816e-05
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.0824e-05 - val_loss: 1.1726e-05
Epoch 260/512

Epoch 00260: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1696e-05 - val_loss: 1.0598e-05
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.0606e-05 - val_loss: 1.1494e-05
Epoch 262/512

Epoch 00262: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1465e-05 - val_loss: 1.0384e-05
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.0392e-05 - val_loss: 1.1265e-05
Epoch 264/512

Epoch 00264: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1236e-05 - val_loss: 1.0171e-05
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00001
10/10 - 0s - loss: 1.0179e-05 - val_loss: 1.1037e-05
Epoch 266/512

Epoch 00266: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1009e-05 - val_loss: 9.9609e-06
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00001
10/10 - 0s - loss: 9.9687e-06 - val_loss: 1.0810e-05
Epoch 268/512

Epoch 00268: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0783e-05 - val_loss: 9.7526e-06
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00001
10/10 - 0s - loss: 9.7603e-06 - val_loss: 1.0589e-05
Epoch 270/512

Epoch 00270: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0561e-05 - val_loss: 9.5504e-06
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00001
10/10 - 0s - loss: 9.5579e-06 - val_loss: 1.0374e-05
Epoch 272/512

Epoch 00272: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0347e-05 - val_loss: 9.3550e-06
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00001
10/10 - 0s - loss: 9.3625e-06 - val_loss: 1.0167e-05
Epoch 274/512

Epoch 00274: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0141e-05 - val_loss: 9.1662e-06
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00001
10/10 - 0s - loss: 9.1736e-06 - val_loss: 9.9639e-06
Epoch 276/512

Epoch 00276: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.9382e-06 - val_loss: 8.9779e-06
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.9851e-06 - val_loss: 9.7612e-06
Epoch 278/512

Epoch 00278: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.7360e-06 - val_loss: 8.7901e-06
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.7972e-06 - val_loss: 9.5584e-06
Epoch 280/512

Epoch 00280: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.5337e-06 - val_loss: 8.6040e-06
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.6110e-06 - val_loss: 9.3594e-06
Epoch 282/512

Epoch 00282: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.3352e-06 - val_loss: 8.4229e-06
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.4298e-06 - val_loss: 9.1666e-06
Epoch 284/512

Epoch 00284: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.1430e-06 - val_loss: 8.2487e-06
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.2555e-06 - val_loss: 8.9828e-06
Epoch 286/512

Epoch 00286: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.9596e-06 - val_loss: 8.0816e-06
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00001
10/10 - 0s - loss: 8.0883e-06 - val_loss: 8.8018e-06
Epoch 288/512

Epoch 00288: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.7791e-06 - val_loss: 7.9131e-06
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.9197e-06 - val_loss: 8.6198e-06
Epoch 290/512

Epoch 00290: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.5976e-06 - val_loss: 7.7455e-06
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.7520e-06 - val_loss: 8.4387e-06
Epoch 292/512

Epoch 00292: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.4169e-06 - val_loss: 7.5792e-06
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.5855e-06 - val_loss: 8.2610e-06
Epoch 294/512

Epoch 00294: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.2397e-06 - val_loss: 7.4198e-06
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.4261e-06 - val_loss: 8.0912e-06
Epoch 296/512

Epoch 00296: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.0703e-06 - val_loss: 7.2646e-06
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.2707e-06 - val_loss: 7.9250e-06
Epoch 298/512

Epoch 00298: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.9045e-06 - val_loss: 7.1138e-06
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00001
10/10 - 0s - loss: 7.1199e-06 - val_loss: 7.7632e-06
Epoch 300/512

Epoch 00300: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.7432e-06 - val_loss: 6.9648e-06
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.9708e-06 - val_loss: 7.6015e-06
Epoch 302/512

Epoch 00302: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.5818e-06 - val_loss: 6.8154e-06
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.8213e-06 - val_loss: 7.4393e-06
Epoch 304/512

Epoch 00304: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.4201e-06 - val_loss: 6.6681e-06
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.6738e-06 - val_loss: 7.2820e-06
Epoch 306/512

Epoch 00306: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.2632e-06 - val_loss: 6.5262e-06
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.5319e-06 - val_loss: 7.1307e-06
Epoch 308/512

Epoch 00308: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.1123e-06 - val_loss: 6.3891e-06
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.3946e-06 - val_loss: 6.9839e-06
Epoch 310/512

Epoch 00310: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.9659e-06 - val_loss: 6.2553e-06
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.2607e-06 - val_loss: 6.8385e-06
Epoch 312/512

Epoch 00312: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.8209e-06 - val_loss: 6.1216e-06
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00001
10/10 - 0s - loss: 6.1269e-06 - val_loss: 6.6935e-06
Epoch 314/512

Epoch 00314: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.6762e-06 - val_loss: 5.9893e-06
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.9946e-06 - val_loss: 6.5507e-06
Epoch 316/512

Epoch 00316: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.5338e-06 - val_loss: 5.8589e-06
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.8640e-06 - val_loss: 6.4101e-06
Epoch 318/512

Epoch 00318: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.3935e-06 - val_loss: 5.7323e-06
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.7374e-06 - val_loss: 6.2752e-06
Epoch 320/512

Epoch 00320: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.2590e-06 - val_loss: 5.6110e-06
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.6159e-06 - val_loss: 6.1450e-06
Epoch 322/512

Epoch 00322: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.1291e-06 - val_loss: 5.4923e-06
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.4972e-06 - val_loss: 6.0159e-06
Epoch 324/512

Epoch 00324: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.0004e-06 - val_loss: 5.3740e-06
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.3788e-06 - val_loss: 5.8879e-06
Epoch 326/512

Epoch 00326: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.8727e-06 - val_loss: 5.2578e-06
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.2626e-06 - val_loss: 5.7627e-06
Epoch 328/512

Epoch 00328: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.7478e-06 - val_loss: 5.1442e-06
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.1489e-06 - val_loss: 5.6390e-06
Epoch 330/512

Epoch 00330: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.6245e-06 - val_loss: 5.0311e-06
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00001
10/10 - 0s - loss: 5.0357e-06 - val_loss: 5.5166e-06
Epoch 332/512

Epoch 00332: val_loss improved from 0.00001 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.5024e-06 - val_loss: 4.9206e-06
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.9251e-06 - val_loss: 5.3973e-06
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.3833e-06 - val_loss: 4.8128e-06
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.8171e-06 - val_loss: 5.2815e-06
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.2678e-06 - val_loss: 4.7093e-06
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.7136e-06 - val_loss: 5.1708e-06
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.1575e-06 - val_loss: 4.6094e-06
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.6136e-06 - val_loss: 5.0625e-06
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.0495e-06 - val_loss: 4.5102e-06
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.5143e-06 - val_loss: 4.9536e-06
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.9408e-06 - val_loss: 4.4103e-06
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.4143e-06 - val_loss: 4.8446e-06
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.8321e-06 - val_loss: 4.3119e-06
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.3159e-06 - val_loss: 4.7385e-06
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.7262e-06 - val_loss: 4.2163e-06
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.2202e-06 - val_loss: 4.6348e-06
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.6228e-06 - val_loss: 4.1229e-06
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.1267e-06 - val_loss: 4.5345e-06
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.5228e-06 - val_loss: 4.0338e-06
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.0375e-06 - val_loss: 4.4389e-06
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.4274e-06 - val_loss: 3.9473e-06
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.9510e-06 - val_loss: 4.3442e-06
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.3330e-06 - val_loss: 3.8603e-06
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.8639e-06 - val_loss: 4.2488e-06
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.2378e-06 - val_loss: 3.7736e-06
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.7772e-06 - val_loss: 4.1547e-06
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.1440e-06 - val_loss: 3.6894e-06
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.6928e-06 - val_loss: 4.0629e-06
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.0524e-06 - val_loss: 3.6064e-06
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.6098e-06 - val_loss: 3.9733e-06
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.9631e-06 - val_loss: 3.5271e-06
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.5304e-06 - val_loss: 3.8881e-06
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.8781e-06 - val_loss: 3.4498e-06
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.4531e-06 - val_loss: 3.8035e-06
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.7937e-06 - val_loss: 3.3736e-06
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3768e-06 - val_loss: 3.7204e-06
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.7108e-06 - val_loss: 3.2983e-06
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3015e-06 - val_loss: 3.6380e-06
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.6287e-06 - val_loss: 3.2240e-06
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.2271e-06 - val_loss: 3.5574e-06
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.5482e-06 - val_loss: 3.1514e-06
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.1544e-06 - val_loss: 3.4785e-06
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.4695e-06 - val_loss: 3.0815e-06
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0844e-06 - val_loss: 3.4025e-06
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.3938e-06 - val_loss: 3.0126e-06
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0154e-06 - val_loss: 3.3269e-06
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.3183e-06 - val_loss: 2.9445e-06
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9474e-06 - val_loss: 3.2536e-06
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.2453e-06 - val_loss: 2.8789e-06
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8817e-06 - val_loss: 3.1816e-06
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.1734e-06 - val_loss: 2.8143e-06
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8170e-06 - val_loss: 3.1112e-06
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.1032e-06 - val_loss: 2.7510e-06
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.7536e-06 - val_loss: 3.0421e-06
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 3.0343e-06 - val_loss: 2.6888e-06
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6913e-06 - val_loss: 2.9739e-06
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.9662e-06 - val_loss: 2.6277e-06
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6303e-06 - val_loss: 2.9067e-06
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.8993e-06 - val_loss: 2.5672e-06
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5696e-06 - val_loss: 2.8413e-06
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.8340e-06 - val_loss: 2.5091e-06
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5116e-06 - val_loss: 2.7778e-06
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.7706e-06 - val_loss: 2.4523e-06
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4547e-06 - val_loss: 2.7160e-06
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.7090e-06 - val_loss: 2.3972e-06
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3995e-06 - val_loss: 2.6560e-06
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.6492e-06 - val_loss: 2.3439e-06
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3462e-06 - val_loss: 2.5978e-06
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.5911e-06 - val_loss: 2.2912e-06
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2934e-06 - val_loss: 2.5396e-06
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.5331e-06 - val_loss: 2.2383e-06
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2405e-06 - val_loss: 2.4808e-06
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.4745e-06 - val_loss: 2.1859e-06
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1880e-06 - val_loss: 2.4233e-06
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.4171e-06 - val_loss: 2.1344e-06
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1365e-06 - val_loss: 2.3678e-06
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.3617e-06 - val_loss: 2.0860e-06
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0880e-06 - val_loss: 2.3154e-06
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.3094e-06 - val_loss: 2.0395e-06
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0415e-06 - val_loss: 2.2647e-06
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.2589e-06 - val_loss: 1.9938e-06
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9958e-06 - val_loss: 2.2138e-06
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.2081e-06 - val_loss: 1.9475e-06
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9494e-06 - val_loss: 2.1624e-06
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.1569e-06 - val_loss: 1.9018e-06
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9036e-06 - val_loss: 2.1129e-06
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.1075e-06 - val_loss: 1.8582e-06
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8600e-06 - val_loss: 2.0647e-06
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.0594e-06 - val_loss: 1.8148e-06
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8166e-06 - val_loss: 2.0175e-06
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 2.0123e-06 - val_loss: 1.7731e-06
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7749e-06 - val_loss: 1.9718e-06
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.9668e-06 - val_loss: 1.7329e-06
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7346e-06 - val_loss: 1.9278e-06
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.9228e-06 - val_loss: 1.6932e-06
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6949e-06 - val_loss: 1.8839e-06
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.8791e-06 - val_loss: 1.6540e-06
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6557e-06 - val_loss: 1.8405e-06
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.8358e-06 - val_loss: 1.6149e-06
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6165e-06 - val_loss: 1.7970e-06
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7924e-06 - val_loss: 1.5765e-06
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5781e-06 - val_loss: 1.7553e-06
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7508e-06 - val_loss: 1.5400e-06
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5415e-06 - val_loss: 1.7158e-06
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.7114e-06 - val_loss: 1.5050e-06
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5065e-06 - val_loss: 1.6770e-06
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.6727e-06 - val_loss: 1.4703e-06
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4718e-06 - val_loss: 1.6388e-06
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.6346e-06 - val_loss: 1.4362e-06
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4376e-06 - val_loss: 1.6006e-06
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5965e-06 - val_loss: 1.4016e-06
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4030e-06 - val_loss: 1.5625e-06
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5585e-06 - val_loss: 1.3685e-06
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3699e-06 - val_loss: 1.5263e-06
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.5224e-06 - val_loss: 1.3366e-06
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3379e-06 - val_loss: 1.4914e-06
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4876e-06 - val_loss: 1.3057e-06
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3070e-06 - val_loss: 1.4575e-06
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4538e-06 - val_loss: 1.2758e-06
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2770e-06 - val_loss: 1.4244e-06
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.4207e-06 - val_loss: 1.2459e-06
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2472e-06 - val_loss: 1.3908e-06
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3873e-06 - val_loss: 1.2158e-06
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2170e-06 - val_loss: 1.3576e-06
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3541e-06 - val_loss: 1.1867e-06
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1879e-06 - val_loss: 1.3259e-06
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.3225e-06 - val_loss: 1.1590e-06
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1602e-06 - val_loss: 1.2954e-06
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2921e-06 - val_loss: 1.1321e-06
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1332e-06 - val_loss: 1.2654e-06
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2622e-06 - val_loss: 1.1054e-06
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1065e-06 - val_loss: 1.2361e-06
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2329e-06 - val_loss: 1.0793e-06
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0803e-06 - val_loss: 1.2070e-06
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.2040e-06 - val_loss: 1.0534e-06
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0545e-06 - val_loss: 1.1783e-06
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1753e-06 - val_loss: 1.0284e-06
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0294e-06 - val_loss: 1.1510e-06
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1481e-06 - val_loss: 1.0045e-06
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0055e-06 - val_loss: 1.1245e-06
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.1216e-06 - val_loss: 9.8073e-07
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.8173e-07 - val_loss: 1.0978e-06
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0950e-06 - val_loss: 9.5705e-07
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.5803e-07 - val_loss: 1.0718e-06
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0691e-06 - val_loss: 9.3438e-07
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.3533e-07 - val_loss: 1.0465e-06
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0439e-06 - val_loss: 9.1168e-07
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.1261e-07 - val_loss: 1.0213e-06
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 1.0187e-06 - val_loss: 8.8982e-07
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.9073e-07 - val_loss: 9.9732e-07
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.9478e-07 - val_loss: 8.6878e-07
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.6967e-07 - val_loss: 9.7416e-07
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.7168e-07 - val_loss: 8.4833e-07
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.4921e-07 - val_loss: 9.5144e-07
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.4903e-07 - val_loss: 8.2835e-07
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.2920e-07 - val_loss: 9.2934e-07
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.2698e-07 - val_loss: 8.0884e-07
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.0968e-07 - val_loss: 9.0743e-07
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 9.0513e-07 - val_loss: 7.8921e-07
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.9002e-07 - val_loss: 8.8528e-07
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.8303e-07 - val_loss: 7.6977e-07
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.7057e-07 - val_loss: 8.6428e-07
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.6208e-07 - val_loss: 7.5152e-07
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.5230e-07 - val_loss: 8.4374e-07
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.4159e-07 - val_loss: 7.3346e-07
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.3422e-07 - val_loss: 8.2365e-07
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.2156e-07 - val_loss: 7.1570e-07
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.1644e-07 - val_loss: 8.0427e-07
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.0222e-07 - val_loss: 6.9903e-07
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.9976e-07 - val_loss: 7.8560e-07
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.8360e-07 - val_loss: 6.8223e-07
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.8295e-07 - val_loss: 7.6679e-07
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.6484e-07 - val_loss: 6.6580e-07
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.6650e-07 - val_loss: 7.4821e-07
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.4631e-07 - val_loss: 6.4929e-07
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.4997e-07 - val_loss: 7.3009e-07
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.2823e-07 - val_loss: 6.3374e-07
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.3441e-07 - val_loss: 7.1305e-07
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.1123e-07 - val_loss: 6.1865e-07
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.1930e-07 - val_loss: 6.9590e-07
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_addition_weights.h5
10/10 - 0s - loss: 6.9413e-07 - val_loss: 6.0357e-07
Train on 10 samples, validate on 10 samples
Epoch 1/512
10/10 - 0s - loss: 1.1831 - val_loss: 4.0030
Epoch 2/512
10/10 - 0s - loss: 3.9669 - val_loss: 0.1320
Epoch 3/512
10/10 - 0s - loss: 0.0938 - val_loss: 0.1041
Epoch 4/512
10/10 - 0s - loss: 0.0853 - val_loss: 0.0904
Epoch 5/512
10/10 - 0s - loss: 0.0817 - val_loss: 0.0815
Epoch 6/512
10/10 - 0s - loss: 0.0791 - val_loss: 0.0746
Epoch 7/512
10/10 - 0s - loss: 0.0767 - val_loss: 0.0685
Epoch 8/512
10/10 - 0s - loss: 0.0742 - val_loss: 0.0627
Epoch 9/512
10/10 - 0s - loss: 0.0715 - val_loss: 0.0571
Epoch 10/512
10/10 - 0s - loss: 0.0687 - val_loss: 0.0513
Epoch 11/512
10/10 - 0s - loss: 0.0657 - val_loss: 0.0455
Epoch 12/512
10/10 - 0s - loss: 0.0624 - val_loss: 0.0396
Epoch 13/512
10/10 - 0s - loss: 0.0592 - val_loss: 0.0341
Epoch 14/512
10/10 - 0s - loss: 0.0550 - val_loss: 0.0282
Epoch 15/512
10/10 - 0s - loss: 0.0504 - val_loss: 0.0222
Epoch 16/512
10/10 - 0s - loss: 0.0450 - val_loss: 0.0164
Epoch 17/512
10/10 - 0s - loss: 0.0402 - val_loss: 0.0122
Epoch 18/512
10/10 - 0s - loss: 0.0360 - val_loss: 0.0099
Epoch 19/512
10/10 - 0s - loss: 0.0335 - val_loss: 0.0091
Epoch 20/512
10/10 - 0s - loss: 0.0318 - val_loss: 0.0089
Epoch 21/512
10/10 - 0s - loss: 0.0306 - val_loss: 0.0091
Epoch 22/512
10/10 - 0s - loss: 0.0302 - val_loss: 0.0096
Epoch 23/512
10/10 - 0s - loss: 0.0300 - val_loss: 0.0098
Epoch 24/512
10/10 - 0s - loss: 0.0298 - val_loss: 0.0168
Epoch 25/512
10/10 - 0s - loss: 0.0349 - val_loss: 0.0115
Epoch 26/512
10/10 - 0s - loss: 0.0286 - val_loss: 0.0114
Epoch 27/512
10/10 - 0s - loss: 0.0281 - val_loss: 0.0108
Epoch 28/512
10/10 - 0s - loss: 0.0278 - val_loss: 0.0102
Epoch 29/512
10/10 - 0s - loss: 0.0275 - val_loss: 0.0104
Epoch 30/512
10/10 - 0s - loss: 0.0274 - val_loss: 0.0167
Epoch 31/512
10/10 - 0s - loss: 0.0334 - val_loss: 0.0079
Epoch 32/512
10/10 - 0s - loss: 0.0277 - val_loss: 0.0085
Epoch 33/512
10/10 - 0s - loss: 0.0272 - val_loss: 0.0089
Epoch 34/512
10/10 - 0s - loss: 0.0268 - val_loss: 0.0088
Epoch 35/512
10/10 - 0s - loss: 0.0265 - val_loss: 0.0089
Epoch 36/512
10/10 - 0s - loss: 0.0261 - val_loss: 0.0089
Epoch 37/512
10/10 - 0s - loss: 0.0257 - val_loss: 0.0085
Epoch 38/512
10/10 - 0s - loss: 0.0252 - val_loss: 0.0085
Epoch 39/512
10/10 - 0s - loss: 0.0247 - val_loss: 0.0083
Epoch 40/512
10/10 - 0s - loss: 0.0242 - val_loss: 0.0081
Epoch 41/512
10/10 - 0s - loss: 0.0236 - val_loss: 0.0076
Epoch 42/512
10/10 - 0s - loss: 0.0229 - val_loss: 0.0076
Epoch 43/512
10/10 - 0s - loss: 0.0222 - val_loss: 0.0073
Epoch 44/512
10/10 - 0s - loss: 0.0214 - val_loss: 0.0070
Epoch 45/512
10/10 - 0s - loss: 0.0206 - val_loss: 0.0067
Epoch 46/512
10/10 - 0s - loss: 0.0196 - val_loss: 0.0064
Epoch 47/512
10/10 - 0s - loss: 0.0190 - val_loss: 0.0061
Epoch 48/512
10/10 - 0s - loss: 0.0175 - val_loss: 0.0057
Epoch 49/512
10/10 - 0s - loss: 0.0162 - val_loss: 0.0045
Epoch 50/512
10/10 - 0s - loss: 0.0149 - val_loss: 0.0053
Epoch 51/512
10/10 - 0s - loss: 0.0121 - val_loss: 0.0043
Epoch 52/512
10/10 - 0s - loss: 0.0106 - val_loss: 0.0048
Epoch 53/512
10/10 - 0s - loss: 0.0093 - val_loss: 0.0042
Epoch 54/512
10/10 - 0s - loss: 0.0081 - val_loss: 0.0051
Epoch 55/512
10/10 - 0s - loss: 0.0070 - val_loss: 0.0043
Epoch 56/512
10/10 - 0s - loss: 0.0061 - val_loss: 0.0063
Epoch 57/512
10/10 - 0s - loss: 0.0060 - val_loss: 0.0046
Epoch 58/512
10/10 - 0s - loss: 0.0148 - val_loss: 0.0075
Epoch 59/512
10/10 - 0s - loss: 0.0207 - val_loss: 0.0045
Epoch 60/512
10/10 - 0s - loss: 0.0089 - val_loss: 0.0063
Epoch 61/512
10/10 - 0s - loss: 0.0068 - val_loss: 0.0046
Epoch 62/512
10/10 - 0s - loss: 0.0066 - val_loss: 0.0101
Epoch 63/512
10/10 - 0s - loss: 0.0078 - val_loss: 0.0068
Epoch 64/512
10/10 - 0s - loss: 0.0092 - val_loss: 0.0202
Epoch 65/512
10/10 - 0s - loss: 0.0136 - val_loss: 0.0086
Epoch 66/512
10/10 - 0s - loss: 0.0126 - val_loss: 0.0109
Epoch 67/512
10/10 - 0s - loss: 0.0072 - val_loss: 0.0029
Epoch 68/512
10/10 - 0s - loss: 0.0037 - val_loss: 0.0031
Epoch 69/512
10/10 - 0s - loss: 0.0031 - val_loss: 0.0032
Epoch 70/512
10/10 - 0s - loss: 0.0027 - val_loss: 0.0035
Epoch 71/512
10/10 - 0s - loss: 0.0023 - val_loss: 0.0035
Epoch 72/512
10/10 - 0s - loss: 0.0022 - val_loss: 0.0039
Epoch 73/512
10/10 - 0s - loss: 0.0086 - val_loss: 0.0275
Epoch 74/512
10/10 - 0s - loss: 0.0745 - val_loss: 0.0630
Epoch 75/512
10/10 - 0s - loss: 0.0869 - val_loss: 0.0515
Epoch 76/512
10/10 - 0s - loss: 0.0722 - val_loss: 0.0069
Epoch 77/512
10/10 - 0s - loss: 0.0294 - val_loss: 0.0070
Epoch 78/512
10/10 - 0s - loss: 0.0269 - val_loss: 0.0071
Epoch 79/512
10/10 - 0s - loss: 0.0253 - val_loss: 0.0070
Epoch 80/512
10/10 - 0s - loss: 0.0241 - val_loss: 0.0068
Epoch 81/512
10/10 - 0s - loss: 0.0231 - val_loss: 0.0065
Epoch 82/512
10/10 - 0s - loss: 0.0224 - val_loss: 0.0060
Epoch 83/512
10/10 - 0s - loss: 0.0215 - val_loss: 0.0062
Epoch 84/512
10/10 - 0s - loss: 0.0209 - val_loss: 0.0054
Epoch 85/512
10/10 - 0s - loss: 0.0196 - val_loss: 0.0036
Epoch 86/512
10/10 - 0s - loss: 0.0173 - val_loss: 0.0029
Epoch 87/512
10/10 - 0s - loss: 0.0161 - val_loss: 0.0024
Epoch 88/512
10/10 - 0s - loss: 0.0150 - val_loss: 0.0017
Epoch 89/512
10/10 - 0s - loss: 0.0154 - val_loss: 0.0015
Epoch 90/512
10/10 - 0s - loss: 0.0133 - val_loss: 0.0016
Epoch 91/512
10/10 - 0s - loss: 0.0119 - val_loss: 0.0013
Epoch 92/512
10/10 - 0s - loss: 0.0105 - val_loss: 9.7664e-04
Epoch 93/512
10/10 - 0s - loss: 0.0093 - val_loss: 8.6301e-04
Epoch 94/512
10/10 - 0s - loss: 0.0082 - val_loss: 8.4796e-04
Epoch 95/512
10/10 - 0s - loss: 0.0072 - val_loss: 9.0711e-04
Epoch 96/512
10/10 - 0s - loss: 0.0061 - val_loss: 0.0010
Epoch 97/512
10/10 - 0s - loss: 0.0053 - val_loss: 0.0012
Epoch 98/512
10/10 - 0s - loss: 0.0046 - val_loss: 0.0014
Epoch 99/512
10/10 - 0s - loss: 0.0040 - val_loss: 0.0016
Epoch 100/512
10/10 - 0s - loss: 0.0036 - val_loss: 0.0017
Epoch 101/512
10/10 - 0s - loss: 0.0033 - val_loss: 0.0018
Epoch 102/512
10/10 - 0s - loss: 0.0031 - val_loss: 0.0018
Epoch 103/512
10/10 - 0s - loss: 0.0029 - val_loss: 0.0019
Epoch 104/512
10/10 - 0s - loss: 0.0028 - val_loss: 0.0019
Epoch 105/512
10/10 - 0s - loss: 0.0027 - val_loss: 0.0022
Epoch 106/512
10/10 - 0s - loss: 0.0028 - val_loss: 0.0047
Epoch 107/512
10/10 - 0s - loss: 0.0060 - val_loss: 0.0077
Epoch 108/512
10/10 - 0s - loss: 0.0078 - val_loss: 0.0014
Epoch 109/512
10/10 - 0s - loss: 0.0069 - val_loss: 0.0016
Epoch 110/512
10/10 - 0s - loss: 0.0080 - val_loss: 0.0017
Epoch 111/512
10/10 - 0s - loss: 0.0073 - val_loss: 0.0021
Epoch 112/512
10/10 - 0s - loss: 0.0051 - val_loss: 0.0019
Epoch 113/512
10/10 - 0s - loss: 0.0049 - val_loss: 0.0023
Epoch 114/512
10/10 - 0s - loss: 0.0042 - val_loss: 0.0020
Epoch 115/512
10/10 - 0s - loss: 0.0039 - val_loss: 0.0023
Epoch 116/512
10/10 - 0s - loss: 0.0037 - val_loss: 0.0018
Epoch 117/512
10/10 - 0s - loss: 0.0032 - val_loss: 0.0020
Epoch 118/512
10/10 - 0s - loss: 0.0032 - val_loss: 0.0016
Epoch 119/512
10/10 - 0s - loss: 0.0035 - val_loss: 0.0018
Epoch 120/512
10/10 - 0s - loss: 0.0038 - val_loss: 0.0013
Epoch 121/512
10/10 - 0s - loss: 0.0046 - val_loss: 0.0017
Epoch 122/512
10/10 - 0s - loss: 0.0045 - val_loss: 0.0011
Epoch 123/512
10/10 - 0s - loss: 0.0049 - val_loss: 0.0015
Epoch 124/512
10/10 - 0s - loss: 0.0037 - val_loss: 0.0010
Epoch 125/512
10/10 - 0s - loss: 0.0036 - val_loss: 0.0016
Epoch 126/512
10/10 - 0s - loss: 0.0031 - val_loss: 0.0013
Epoch 127/512
10/10 - 0s - loss: 0.0037 - val_loss: 0.0018
Epoch 128/512
10/10 - 0s - loss: 0.0032 - val_loss: 7.6592e-04
Epoch 129/512
10/10 - 0s - loss: 0.0038 - val_loss: 0.0013
Epoch 130/512
10/10 - 0s - loss: 0.0034 - val_loss: 8.3847e-04
Epoch 131/512
10/10 - 0s - loss: 0.0038 - val_loss: 0.0014
Epoch 132/512
10/10 - 0s - loss: 0.0035 - val_loss: 9.5625e-04
Epoch 133/512
10/10 - 0s - loss: 0.0044 - val_loss: 0.0017
Epoch 134/512
10/10 - 0s - loss: 0.0035 - val_loss: 8.7132e-04
Epoch 135/512
10/10 - 0s - loss: 0.0035 - val_loss: 9.4003e-04
Epoch 136/512
10/10 - 0s - loss: 0.0028 - val_loss: 7.0329e-04
Epoch 137/512
10/10 - 0s - loss: 0.0030 - val_loss: 0.0012
Epoch 138/512
10/10 - 0s - loss: 0.0034 - val_loss: 5.5661e-04
Epoch 139/512
10/10 - 0s - loss: 0.0046 - val_loss: 0.0012
Epoch 140/512
10/10 - 0s - loss: 0.0042 - val_loss: 0.0011
Epoch 141/512
10/10 - 0s - loss: 0.0062 - val_loss: 9.4120e-04
Epoch 142/512
10/10 - 0s - loss: 0.0074 - val_loss: 4.6350e-04
Epoch 143/512
10/10 - 0s - loss: 0.0055 - val_loss: 4.8053e-04
Epoch 144/512
10/10 - 0s - loss: 0.0022 - val_loss: 6.7792e-04
Epoch 145/512
10/10 - 0s - loss: 0.0021 - val_loss: 8.9790e-04
Epoch 146/512
10/10 - 0s - loss: 0.0023 - val_loss: 0.0013
Epoch 147/512
10/10 - 0s - loss: 0.0027 - val_loss: 4.8032e-04
Epoch 148/512
10/10 - 0s - loss: 0.0023 - val_loss: 7.8089e-04
Epoch 149/512
10/10 - 0s - loss: 0.0020 - val_loss: 5.3258e-04
Epoch 150/512
10/10 - 0s - loss: 0.0020 - val_loss: 7.5166e-04
Epoch 151/512
10/10 - 0s - loss: 0.0023 - val_loss: 4.1284e-04
Epoch 152/512
10/10 - 0s - loss: 0.0030 - val_loss: 8.1066e-04
Epoch 153/512
10/10 - 0s - loss: 0.0040 - val_loss: 2.0688e-04
Epoch 154/512
10/10 - 0s - loss: 0.0063 - val_loss: 6.0636e-04
Epoch 155/512
10/10 - 0s - loss: 0.0040 - val_loss: 5.9915e-04
Epoch 156/512
10/10 - 0s - loss: 0.0042 - val_loss: 4.1735e-04
Epoch 157/512
10/10 - 0s - loss: 0.0023 - val_loss: 7.5716e-04
Epoch 158/512
10/10 - 0s - loss: 0.0024 - val_loss: 0.0012
Epoch 159/512
10/10 - 0s - loss: 0.0027 - val_loss: 2.4846e-04
Epoch 160/512
10/10 - 0s - loss: 0.0032 - val_loss: 0.0014
Epoch 161/512
10/10 - 0s - loss: 0.0035 - val_loss: 1.4278e-04
Epoch 162/512
10/10 - 0s - loss: 0.0062 - val_loss: 7.7391e-04
Epoch 163/512
10/10 - 0s - loss: 0.0053 - val_loss: 1.1217e-04
Epoch 164/512
10/10 - 0s - loss: 0.0064 - val_loss: 4.1979e-04
Epoch 165/512
10/10 - 0s - loss: 0.0028 - val_loss: 2.4890e-04
Epoch 166/512
10/10 - 0s - loss: 0.0024 - val_loss: 4.2813e-04
Epoch 167/512
10/10 - 0s - loss: 0.0019 - val_loss: 3.0548e-04
Epoch 168/512
10/10 - 0s - loss: 0.0019 - val_loss: 5.2122e-04
Epoch 169/512
10/10 - 0s - loss: 0.0019 - val_loss: 4.1637e-04
Epoch 170/512
10/10 - 0s - loss: 0.0027 - val_loss: 5.1329e-04
Epoch 171/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.3812e-04
Epoch 172/512
10/10 - 0s - loss: 0.0028 - val_loss: 4.9901e-04
Epoch 173/512
10/10 - 0s - loss: 0.0027 - val_loss: 1.8900e-04
Epoch 174/512
10/10 - 0s - loss: 0.0037 - val_loss: 4.8462e-04
Epoch 175/512
10/10 - 0s - loss: 0.0031 - val_loss: 3.8786e-04
Epoch 176/512
10/10 - 0s - loss: 0.0040 - val_loss: 6.0723e-04
Epoch 177/512
10/10 - 0s - loss: 0.0025 - val_loss: 2.8201e-04
Epoch 178/512
10/10 - 0s - loss: 0.0029 - val_loss: 3.9148e-04
Epoch 179/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.1831e-04
Epoch 180/512
10/10 - 0s - loss: 0.0024 - val_loss: 4.7117e-04
Epoch 181/512
10/10 - 0s - loss: 0.0025 - val_loss: 1.8161e-04
Epoch 182/512
10/10 - 0s - loss: 0.0035 - val_loss: 3.9134e-04
Epoch 183/512
10/10 - 0s - loss: 0.0028 - val_loss: 1.6664e-04
Epoch 184/512
10/10 - 0s - loss: 0.0033 - val_loss: 3.4373e-04
Epoch 185/512
10/10 - 0s - loss: 0.0024 - val_loss: 3.9443e-04
Epoch 186/512
10/10 - 0s - loss: 0.0027 - val_loss: 0.0010
Epoch 187/512
10/10 - 0s - loss: 0.0025 - val_loss: 5.9608e-05
Epoch 188/512
10/10 - 0s - loss: 0.0031 - val_loss: 4.1156e-04
Epoch 189/512
10/10 - 0s - loss: 0.0025 - val_loss: 1.2038e-04
Epoch 190/512
10/10 - 0s - loss: 0.0033 - val_loss: 3.6324e-04
Epoch 191/512
10/10 - 0s - loss: 0.0029 - val_loss: 1.1212e-04
Epoch 192/512
10/10 - 0s - loss: 0.0037 - val_loss: 2.8648e-04
Epoch 193/512
10/10 - 0s - loss: 0.0024 - val_loss: 1.5711e-04
Epoch 194/512
10/10 - 0s - loss: 0.0023 - val_loss: 2.8968e-04
Epoch 195/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.8157e-04
Epoch 196/512
10/10 - 0s - loss: 0.0019 - val_loss: 3.4205e-04
Epoch 197/512
10/10 - 0s - loss: 0.0018 - val_loss: 3.2735e-04
Epoch 198/512
10/10 - 0s - loss: 0.0022 - val_loss: 4.8337e-04
Epoch 199/512
10/10 - 0s - loss: 0.0020 - val_loss: 1.3544e-04
Epoch 200/512
10/10 - 0s - loss: 0.0027 - val_loss: 3.1574e-04
Epoch 201/512
10/10 - 0s - loss: 0.0025 - val_loss: 1.1715e-04
Epoch 202/512
10/10 - 0s - loss: 0.0034 - val_loss: 2.9812e-04
Epoch 203/512
10/10 - 0s - loss: 0.0026 - val_loss: 1.8819e-04
Epoch 204/512
10/10 - 0s - loss: 0.0029 - val_loss: 2.3786e-04
Epoch 205/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.7197e-04
Epoch 206/512
10/10 - 0s - loss: 0.0020 - val_loss: 3.1054e-04
Epoch 207/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.8888e-04
Epoch 208/512
10/10 - 0s - loss: 0.0021 - val_loss: 3.0553e-04
Epoch 209/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.2899e-04
Epoch 210/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.7602e-04
Epoch 211/512
10/10 - 0s - loss: 0.0023 - val_loss: 9.4924e-05
Epoch 212/512
10/10 - 0s - loss: 0.0033 - val_loss: 2.3019e-04
Epoch 213/512
10/10 - 0s - loss: 0.0024 - val_loss: 1.3327e-04
Epoch 214/512
10/10 - 0s - loss: 0.0026 - val_loss: 2.5996e-04
Epoch 215/512
10/10 - 0s - loss: 0.0019 - val_loss: 4.7165e-04
Epoch 216/512
10/10 - 0s - loss: 0.0023 - val_loss: 2.3784e-04
Epoch 217/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.7591e-04
Epoch 218/512
10/10 - 0s - loss: 0.0015 - val_loss: 2.7843e-04
Epoch 219/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.1059e-04
Epoch 220/512
10/10 - 0s - loss: 0.0025 - val_loss: 2.5581e-04
Epoch 221/512
10/10 - 0s - loss: 0.0026 - val_loss: 7.4980e-05
Epoch 222/512
10/10 - 0s - loss: 0.0038 - val_loss: 1.9685e-04
Epoch 223/512
10/10 - 0s - loss: 0.0023 - val_loss: 1.5664e-04
Epoch 224/512
10/10 - 0s - loss: 0.0021 - val_loss: 2.0906e-04
Epoch 225/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.9257e-04
Epoch 226/512
10/10 - 0s - loss: 0.0016 - val_loss: 2.5543e-04
Epoch 227/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.4795e-04
Epoch 228/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.1672e-04
Epoch 229/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.0067e-04
Epoch 230/512
10/10 - 0s - loss: 0.0024 - val_loss: 2.1849e-04
Epoch 231/512
10/10 - 0s - loss: 0.0023 - val_loss: 9.1348e-05
Epoch 232/512
10/10 - 0s - loss: 0.0030 - val_loss: 1.7263e-04
Epoch 233/512
10/10 - 0s - loss: 0.0020 - val_loss: 1.1400e-04
Epoch 234/512
10/10 - 0s - loss: 0.0019 - val_loss: 3.2733e-04
Epoch 235/512
10/10 - 0s - loss: 0.0017 - val_loss: 5.8449e-04
Epoch 236/512
10/10 - 0s - loss: 0.0022 - val_loss: 9.6445e-05
Epoch 237/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.5871e-04
Epoch 238/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.4474e-04
Epoch 239/512
10/10 - 0s - loss: 0.0014 - val_loss: 8.3520e-05
Epoch 240/512
10/10 - 0s - loss: 0.0030 - val_loss: 2.1642e-04
Epoch 241/512
10/10 - 0s - loss: 0.0026 - val_loss: 7.7010e-05
Epoch 242/512
10/10 - 0s - loss: 0.0031 - val_loss: 1.6594e-04
Epoch 243/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.0415e-04
Epoch 244/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.7545e-04
Epoch 245/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.2280e-04
Epoch 246/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.8736e-04
Epoch 247/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2175e-04
Epoch 248/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.9713e-04
Epoch 249/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.0812e-04
Epoch 250/512
10/10 - 0s - loss: 0.0020 - val_loss: 1.9171e-04
Epoch 251/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.1882e-04
Epoch 252/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.1623e-04
Epoch 253/512
10/10 - 0s - loss: 0.0018 - val_loss: 3.8798e-04
Epoch 254/512
10/10 - 0s - loss: 0.0022 - val_loss: 1.6131e-04
Epoch 255/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.2344e-04
Epoch 256/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.9623e-04
Epoch 257/512
10/10 - 0s - loss: 0.0014 - val_loss: 9.5272e-05
Epoch 258/512
10/10 - 0s - loss: 0.0021 - val_loss: 1.8515e-04
Epoch 259/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.0504e-04
Epoch 260/512
10/10 - 0s - loss: 0.0023 - val_loss: 1.6481e-04
Epoch 261/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.2391e-04
Epoch 262/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.3718e-04
Epoch 263/512
10/10 - 0s - loss: 0.0014 - val_loss: 3.4272e-04
Epoch 264/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.5131e-04
Epoch 265/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2025e-04
Epoch 266/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.9011e-04
Epoch 267/512
10/10 - 0s - loss: 0.0016 - val_loss: 8.1756e-05
Epoch 268/512
10/10 - 0s - loss: 0.0026 - val_loss: 1.7086e-04
Epoch 269/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.2422e-04
Epoch 270/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.6173e-04
Epoch 271/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.1923e-04
Epoch 272/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.6611e-04
Epoch 273/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1968e-04
Epoch 274/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.7363e-04
Epoch 275/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.1542e-04
Epoch 276/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.7147e-04
Epoch 277/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.2269e-04
Epoch 278/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.8566e-04
Epoch 279/512
10/10 - 0s - loss: 0.0015 - val_loss: 2.4556e-04
Epoch 280/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.0277e-04
Epoch 281/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2848e-04
Epoch 282/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.7551e-04
Epoch 283/512
10/10 - 0s - loss: 0.0013 - val_loss: 8.5852e-05
Epoch 284/512
10/10 - 0s - loss: 0.0022 - val_loss: 1.5564e-04
Epoch 285/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.2409e-04
Epoch 286/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.5666e-04
Epoch 287/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2973e-04
Epoch 288/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.7462e-04
Epoch 289/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.7160e-04
Epoch 290/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.6274e-04
Epoch 291/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.0745e-04
Epoch 292/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.5306e-04
Epoch 293/512
10/10 - 0s - loss: 0.0014 - val_loss: 2.2438e-04
Epoch 294/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.9877e-04
Epoch 295/512
10/10 - 0s - loss: 0.0011 - val_loss: 3.9338e-04
Epoch 296/512
10/10 - 0s - loss: 0.0014 - val_loss: 3.7255e-04
Epoch 297/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.9438e-04
Epoch 298/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.5364e-04
Epoch 299/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1163e-04
Epoch 300/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.5779e-04
Epoch 301/512
10/10 - 0s - loss: 8.7699e-04 - val_loss: 1.5428e-04
Epoch 302/512
10/10 - 0s - loss: 8.0526e-04 - val_loss: 1.6754e-04
Epoch 303/512
10/10 - 0s - loss: 8.0052e-04 - val_loss: 1.5910e-04
Epoch 304/512
10/10 - 0s - loss: 8.3858e-04 - val_loss: 1.7733e-04
Epoch 305/512
10/10 - 0s - loss: 9.3607e-04 - val_loss: 1.5728e-04
Epoch 306/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.9226e-04
Epoch 307/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.4264e-04
Epoch 308/512
10/10 - 0s - loss: 0.0023 - val_loss: 1.7839e-04
Epoch 309/512
10/10 - 0s - loss: 0.0020 - val_loss: 1.3015e-04
Epoch 310/512
10/10 - 0s - loss: 0.0019 - val_loss: 1.7421e-04
Epoch 311/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.8564e-04
Epoch 312/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.6209e-04
Epoch 313/512
10/10 - 0s - loss: 8.9768e-04 - val_loss: 1.4817e-04
Epoch 314/512
10/10 - 0s - loss: 8.7843e-04 - val_loss: 1.8356e-04
Epoch 315/512
10/10 - 0s - loss: 8.5056e-04 - val_loss: 1.7194e-04
Epoch 316/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.7586e-04
Epoch 317/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.1981e-04
Epoch 318/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.8186e-04
Epoch 319/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.0446e-04
Epoch 320/512
10/10 - 0s - loss: 0.0039 - val_loss: 1.5044e-04
Epoch 321/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.8323e-04
Epoch 322/512
10/10 - 0s - loss: 8.0896e-04 - val_loss: 6.8561e-04
Epoch 323/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.8586e-04
Epoch 324/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.9039e-04
Epoch 325/512
10/10 - 0s - loss: 8.1571e-04 - val_loss: 2.8364e-04
Epoch 326/512
10/10 - 0s - loss: 7.7733e-04 - val_loss: 2.7829e-04
Epoch 327/512
10/10 - 0s - loss: 7.5226e-04 - val_loss: 2.6641e-04
Epoch 328/512
10/10 - 0s - loss: 7.3303e-04 - val_loss: 2.6744e-04
Epoch 329/512
10/10 - 0s - loss: 7.1858e-04 - val_loss: 2.5701e-04
Epoch 330/512
10/10 - 0s - loss: 7.1130e-04 - val_loss: 2.6692e-04
Epoch 331/512
10/10 - 0s - loss: 7.2174e-04 - val_loss: 2.4790e-04
Epoch 332/512
10/10 - 0s - loss: 7.9119e-04 - val_loss: 2.7941e-04
Epoch 333/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.1762e-04
Epoch 334/512
10/10 - 0s - loss: 0.0018 - val_loss: 2.8858e-04
Epoch 335/512
10/10 - 0s - loss: 0.0026 - val_loss: 1.7900e-04
Epoch 336/512
10/10 - 0s - loss: 0.0024 - val_loss: 2.3859e-04
Epoch 337/512
10/10 - 0s - loss: 0.0014 - val_loss: 2.0356e-04
Epoch 338/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.4679e-04
Epoch 339/512
10/10 - 0s - loss: 7.9988e-04 - val_loss: 2.2556e-04
Epoch 340/512
10/10 - 0s - loss: 7.2626e-04 - val_loss: 2.6041e-04
Epoch 341/512
10/10 - 0s - loss: 6.8572e-04 - val_loss: 2.4201e-04
Epoch 342/512
10/10 - 0s - loss: 6.7546e-04 - val_loss: 2.7751e-04
Epoch 343/512
10/10 - 0s - loss: 6.7966e-04 - val_loss: 2.5197e-04
Epoch 344/512
10/10 - 0s - loss: 7.1930e-04 - val_loss: 2.9954e-04
Epoch 345/512
10/10 - 0s - loss: 8.0110e-04 - val_loss: 2.5059e-04
Epoch 346/512
10/10 - 0s - loss: 9.9780e-04 - val_loss: 3.2281e-04
Epoch 347/512
10/10 - 0s - loss: 0.0013 - val_loss: 2.3552e-04
Epoch 348/512
10/10 - 0s - loss: 0.0017 - val_loss: 3.2392e-04
Epoch 349/512
10/10 - 0s - loss: 0.0017 - val_loss: 2.3691e-04
Epoch 350/512
10/10 - 0s - loss: 0.0016 - val_loss: 3.0684e-04
Epoch 351/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.5770e-04
Epoch 352/512
10/10 - 0s - loss: 0.0010 - val_loss: 3.0978e-04
Epoch 353/512
10/10 - 0s - loss: 8.6164e-04 - val_loss: 2.7440e-04
Epoch 354/512
10/10 - 0s - loss: 8.3696e-04 - val_loss: 3.1696e-04
Epoch 355/512
10/10 - 0s - loss: 7.7470e-04 - val_loss: 2.7897e-04
Epoch 356/512
10/10 - 0s - loss: 8.2697e-04 - val_loss: 3.2677e-04
Epoch 357/512
10/10 - 0s - loss: 8.4268e-04 - val_loss: 2.7924e-04
Epoch 358/512
10/10 - 0s - loss: 9.9302e-04 - val_loss: 3.3049e-04
Epoch 359/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.7357e-04
Epoch 360/512
10/10 - 0s - loss: 0.0013 - val_loss: 3.2539e-04
Epoch 361/512
10/10 - 0s - loss: 0.0013 - val_loss: 2.7360e-04
Epoch 362/512
10/10 - 0s - loss: 0.0014 - val_loss: 3.1542e-04
Epoch 363/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.7789e-04
Epoch 364/512
10/10 - 0s - loss: 0.0012 - val_loss: 3.1371e-04
Epoch 365/512
10/10 - 0s - loss: 9.7512e-04 - val_loss: 2.8431e-04
Epoch 366/512
10/10 - 0s - loss: 9.7836e-04 - val_loss: 3.1128e-04
Epoch 367/512
10/10 - 0s - loss: 8.6034e-04 - val_loss: 2.7869e-04
Epoch 368/512
10/10 - 0s - loss: 9.1487e-04 - val_loss: 3.1813e-04
Epoch 369/512
10/10 - 0s - loss: 8.9140e-04 - val_loss: 2.8346e-04
Epoch 370/512
10/10 - 0s - loss: 0.0010 - val_loss: 3.0919e-04
Epoch 371/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.6720e-04
Epoch 372/512
10/10 - 0s - loss: 0.0012 - val_loss: 3.0930e-04
Epoch 373/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.7469e-04
Epoch 374/512
10/10 - 0s - loss: 0.0013 - val_loss: 2.9735e-04
Epoch 375/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.6162e-04
Epoch 376/512
10/10 - 0s - loss: 0.0012 - val_loss: 3.0307e-04
Epoch 377/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.7622e-04
Epoch 378/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.9032e-04
Epoch 379/512
10/10 - 0s - loss: 8.9333e-04 - val_loss: 2.5628e-04
Epoch 380/512
10/10 - 0s - loss: 9.4987e-04 - val_loss: 2.8267e-04
Epoch 381/512
10/10 - 0s - loss: 9.1020e-04 - val_loss: 2.5963e-04
Epoch 382/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.9619e-04
Epoch 383/512
10/10 - 0s - loss: 9.9782e-04 - val_loss: 2.6322e-04
Epoch 384/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.8260e-04
Epoch 385/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.4208e-04
Epoch 386/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.7411e-04
Epoch 387/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.5122e-04
Epoch 388/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.7958e-04
Epoch 389/512
10/10 - 0s - loss: 9.9036e-04 - val_loss: 2.5160e-04
Epoch 390/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.6746e-04
Epoch 391/512
10/10 - 0s - loss: 9.0781e-04 - val_loss: 2.3503e-04
Epoch 392/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.6660e-04
Epoch 393/512
10/10 - 0s - loss: 9.2328e-04 - val_loss: 2.4451e-04
Epoch 394/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.6316e-04
Epoch 395/512
10/10 - 0s - loss: 9.5437e-04 - val_loss: 2.3250e-04
Epoch 396/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.6101e-04
Epoch 397/512
10/10 - 0s - loss: 9.9468e-04 - val_loss: 2.3407e-04
Epoch 398/512
10/10 - 0s - loss: 0.0012 - val_loss: 2.4925e-04
Epoch 399/512
10/10 - 0s - loss: 9.6927e-04 - val_loss: 2.2133e-04
Epoch 400/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.4729e-04
Epoch 401/512
10/10 - 0s - loss: 9.4138e-04 - val_loss: 2.2750e-04
Epoch 402/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.4713e-04
Epoch 403/512
10/10 - 0s - loss: 8.9773e-04 - val_loss: 2.2325e-04
Epoch 404/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.4293e-04
Epoch 405/512
10/10 - 0s - loss: 8.8459e-04 - val_loss: 2.1835e-04
Epoch 406/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.3869e-04
Epoch 407/512
10/10 - 0s - loss: 9.0435e-04 - val_loss: 2.1543e-04
Epoch 408/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.3306e-04
Epoch 409/512
10/10 - 0s - loss: 9.2266e-04 - val_loss: 2.1053e-04
Epoch 410/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.2979e-04
Epoch 411/512
10/10 - 0s - loss: 9.1947e-04 - val_loss: 2.0993e-04
Epoch 412/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.2673e-04
Epoch 413/512
10/10 - 0s - loss: 8.8609e-04 - val_loss: 2.0722e-04
Epoch 414/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.2488e-04
Epoch 415/512
10/10 - 0s - loss: 8.6090e-04 - val_loss: 2.0596e-04
Epoch 416/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.2116e-04
Epoch 417/512
10/10 - 0s - loss: 8.5508e-04 - val_loss: 2.0139e-04
Epoch 418/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.1792e-04
Epoch 419/512
10/10 - 0s - loss: 8.7206e-04 - val_loss: 1.9945e-04
Epoch 420/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.1350e-04
Epoch 421/512
10/10 - 0s - loss: 8.7885e-04 - val_loss: 1.9569e-04
Epoch 422/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.1068e-04
Epoch 423/512
10/10 - 0s - loss: 8.6994e-04 - val_loss: 1.9485e-04
Epoch 424/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.0850e-04
Epoch 425/512
10/10 - 0s - loss: 8.4652e-04 - val_loss: 1.9275e-04
Epoch 426/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.0665e-04
Epoch 427/512
10/10 - 0s - loss: 8.3226e-04 - val_loss: 1.9092e-04
Epoch 428/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.0325e-04
Epoch 429/512
10/10 - 0s - loss: 8.3056e-04 - val_loss: 1.8737e-04
Epoch 430/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.9961e-04
Epoch 431/512
10/10 - 0s - loss: 8.3868e-04 - val_loss: 1.8497e-04
Epoch 432/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.9607e-04
Epoch 433/512
10/10 - 0s - loss: 8.4119e-04 - val_loss: 1.8224e-04
Epoch 434/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.9362e-04
Epoch 435/512
10/10 - 0s - loss: 8.3283e-04 - val_loss: 1.8072e-04
Epoch 436/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.9222e-04
Epoch 437/512
10/10 - 0s - loss: 8.1618e-04 - val_loss: 1.7937e-04
Epoch 438/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.9036e-04
Epoch 439/512
10/10 - 0s - loss: 8.0111e-04 - val_loss: 1.7728e-04
Epoch 440/512
10/10 - 0s - loss: 9.9602e-04 - val_loss: 1.8683e-04
Epoch 441/512
10/10 - 0s - loss: 7.9478e-04 - val_loss: 1.7410e-04
Epoch 442/512
10/10 - 0s - loss: 9.9359e-04 - val_loss: 1.8297e-04
Epoch 443/512
10/10 - 0s - loss: 8.0113e-04 - val_loss: 1.7122e-04
Epoch 444/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.7917e-04
Epoch 445/512
10/10 - 0s - loss: 8.0805e-04 - val_loss: 1.6811e-04
Epoch 446/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.7647e-04
Epoch 447/512
10/10 - 0s - loss: 8.0485e-04 - val_loss: 1.6625e-04
Epoch 448/512
10/10 - 0s - loss: 9.9241e-04 - val_loss: 1.7619e-04
Epoch 449/512
10/10 - 0s - loss: 7.8728e-04 - val_loss: 1.6599e-04
Epoch 450/512
10/10 - 0s - loss: 9.8519e-04 - val_loss: 1.7706e-04
Epoch 451/512
10/10 - 0s - loss: 7.6166e-04 - val_loss: 1.6588e-04
Epoch 452/512
10/10 - 0s - loss: 9.8284e-04 - val_loss: 1.7112e-04
Epoch 453/512
10/10 - 0s - loss: 7.3005e-04 - val_loss: 1.6028e-04
Epoch 454/512
10/10 - 0s - loss: 9.2894e-04 - val_loss: 1.6723e-04
Epoch 455/512
10/10 - 0s - loss: 7.4810e-04 - val_loss: 1.5778e-04
Epoch 456/512
10/10 - 0s - loss: 9.8999e-04 - val_loss: 1.6191e-04
Epoch 457/512
10/10 - 0s - loss: 7.8481e-04 - val_loss: 1.5611e-04
Epoch 458/512
10/10 - 0s - loss: 9.4714e-04 - val_loss: 1.5346e-04
Epoch 459/512
10/10 - 0s - loss: 7.9315e-04 - val_loss: 1.6924e-04
Epoch 460/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.5588e-04
Epoch 461/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.9690e-04
Epoch 462/512
10/10 - 0s - loss: 9.1265e-04 - val_loss: 2.0926e-04
Epoch 463/512
10/10 - 0s - loss: 7.5123e-04 - val_loss: 1.8914e-04
Epoch 464/512
10/10 - 0s - loss: 7.4409e-04 - val_loss: 1.7658e-04
Epoch 465/512
10/10 - 0s - loss: 6.8905e-04 - val_loss: 1.4187e-04
Epoch 466/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.4164e-04
Epoch 467/512
10/10 - 0s - loss: 8.1673e-04 - val_loss: 1.6467e-04
Epoch 468/512
10/10 - 0s - loss: 7.7622e-04 - val_loss: 1.5575e-04
Epoch 469/512
10/10 - 0s - loss: 6.9622e-04 - val_loss: 1.6068e-04
Epoch 470/512
10/10 - 0s - loss: 7.2230e-04 - val_loss: 1.5854e-04
Epoch 471/512
10/10 - 0s - loss: 7.1869e-04 - val_loss: 1.4891e-04
Epoch 472/512
10/10 - 0s - loss: 9.2837e-04 - val_loss: 1.4604e-04
Epoch 473/512
10/10 - 0s - loss: 9.6556e-04 - val_loss: 1.3546e-04
Epoch 474/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.2150e-04
Epoch 475/512
10/10 - 0s - loss: 9.4197e-04 - val_loss: 2.3586e-04
Epoch 476/512
10/10 - 0s - loss: 8.3203e-04 - val_loss: 1.5396e-04
Epoch 477/512
10/10 - 0s - loss: 7.1857e-04 - val_loss: 1.7467e-04
Epoch 478/512
10/10 - 0s - loss: 5.6946e-04 - val_loss: 1.6746e-04
Epoch 479/512
10/10 - 0s - loss: 5.4417e-04 - val_loss: 1.4839e-04
Epoch 480/512
10/10 - 0s - loss: 6.7499e-04 - val_loss: 1.4790e-04
Epoch 481/512
10/10 - 0s - loss: 7.4579e-04 - val_loss: 1.2591e-04
Epoch 482/512
10/10 - 0s - loss: 0.0017 - val_loss: 1.2124e-04
Epoch 483/512
10/10 - 0s - loss: 9.7544e-04 - val_loss: 1.7119e-04
Epoch 484/512
10/10 - 0s - loss: 7.7434e-04 - val_loss: 1.4019e-04
Epoch 485/512
10/10 - 0s - loss: 6.3548e-04 - val_loss: 1.5313e-04
Epoch 486/512
10/10 - 0s - loss: 5.6052e-04 - val_loss: 1.4705e-04
Epoch 487/512
10/10 - 0s - loss: 5.2406e-04 - val_loss: 1.5021e-04
Epoch 488/512
10/10 - 0s - loss: 5.6068e-04 - val_loss: 1.4485e-04
Epoch 489/512
10/10 - 0s - loss: 6.0512e-04 - val_loss: 1.5087e-04
Epoch 490/512
10/10 - 0s - loss: 7.6422e-04 - val_loss: 1.4124e-04
Epoch 491/512
10/10 - 0s - loss: 9.0169e-04 - val_loss: 1.5488e-04
Epoch 492/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.3625e-04
Epoch 493/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.5283e-04
Epoch 494/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.3444e-04
Epoch 495/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.4245e-04
Epoch 496/512
10/10 - 0s - loss: 9.5441e-04 - val_loss: 1.3353e-04
Epoch 497/512
10/10 - 0s - loss: 7.1184e-04 - val_loss: 1.3804e-04
Epoch 498/512
10/10 - 0s - loss: 7.0317e-04 - val_loss: 1.3160e-04
Epoch 499/512
10/10 - 0s - loss: 6.0256e-04 - val_loss: 1.3374e-04
Epoch 500/512
10/10 - 0s - loss: 7.0344e-04 - val_loss: 1.2760e-04
Epoch 501/512
10/10 - 0s - loss: 7.6202e-04 - val_loss: 7.0683e-05
Epoch 502/512
10/10 - 0s - loss: 7.8466e-04 - val_loss: 8.7115e-05
Epoch 503/512
10/10 - 0s - loss: 7.8904e-04 - val_loss: 9.2991e-05
Epoch 504/512
10/10 - 0s - loss: 5.9724e-04 - val_loss: 9.8183e-05
Epoch 505/512
10/10 - 0s - loss: 5.8224e-04 - val_loss: 8.8224e-05
Epoch 506/512
10/10 - 0s - loss: 5.9606e-04 - val_loss: 9.4870e-05
Epoch 507/512
10/10 - 0s - loss: 6.6543e-04 - val_loss: 8.7982e-05
Epoch 508/512
10/10 - 0s - loss: 9.0498e-04 - val_loss: 1.1651e-04
Epoch 509/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.3126e-04
Epoch 510/512
10/10 - 0s - loss: 0.0021 - val_loss: 1.2072e-04
Epoch 511/512
10/10 - 0s - loss: 0.0019 - val_loss: 6.2750e-05
Epoch 512/512
10/10 - 0s - loss: 0.0013 - val_loss: 7.3143e-05
Train on 10 samples, validate on 10 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00057, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 9.5120e-05 - val_loss: 5.7149e-04
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00057
10/10 - 0s - loss: 5.7111e-04 - val_loss: 0.0031
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00057
10/10 - 0s - loss: 0.0031 - val_loss: 0.0054
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00057
10/10 - 0s - loss: 0.0054 - val_loss: 7.7289e-04
Epoch 5/512

Epoch 00005: val_loss improved from 0.00057 to 0.00048, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 7.7354e-04 - val_loss: 4.7903e-04
Epoch 6/512

Epoch 00006: val_loss improved from 0.00048 to 0.00020, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.7884e-04 - val_loss: 1.9870e-04
Epoch 7/512

Epoch 00007: val_loss improved from 0.00020 to 0.00014, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9900e-04 - val_loss: 1.4199e-04
Epoch 8/512

Epoch 00008: val_loss improved from 0.00014 to 0.00010, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4192e-04 - val_loss: 9.6609e-05
Epoch 9/512

Epoch 00009: val_loss improved from 0.00010 to 0.00009, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test-2/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 9.6813e-05 - val_loss: 9.4600e-05
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.4551e-05 - val_loss: 9.5018e-05
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.5207e-05 - val_loss: 1.2874e-04
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00009
10/10 - 0s - loss: 1.2868e-04 - val_loss: 1.7640e-04
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00009
10/10 - 0s - loss: 1.7662e-04 - val_loss: 3.1494e-04
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.1484e-04 - val_loss: 5.0454e-04
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0485e-04 - val_loss: 9.8311e-04
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.8304e-04 - val_loss: 0.0012
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00009
10/10 - 0s - loss: 0.0012 - val_loss: 0.0017
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00009
10/10 - 0s - loss: 0.0017 - val_loss: 9.5175e-04
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.5185e-04 - val_loss: 9.2062e-04
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.2087e-04 - val_loss: 4.7999e-04
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7998e-04 - val_loss: 4.3366e-04
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3387e-04 - val_loss: 2.9478e-04
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00009
10/10 - 0s - loss: 2.9475e-04 - val_loss: 3.0827e-04
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.0848e-04 - val_loss: 2.7512e-04
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00009
10/10 - 0s - loss: 2.7508e-04 - val_loss: 3.5020e-04
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5045e-04 - val_loss: 3.7793e-04
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7786e-04 - val_loss: 5.5484e-04
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5520e-04 - val_loss: 6.1160e-04
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.1147e-04 - val_loss: 8.9008e-04
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00009
10/10 - 0s - loss: 8.9063e-04 - val_loss: 7.8956e-04
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00009
10/10 - 0s - loss: 7.8936e-04 - val_loss: 9.7246e-04
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00009
10/10 - 0s - loss: 9.7317e-04 - val_loss: 6.7007e-04
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.6985e-04 - val_loss: 7.0893e-04
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00009
10/10 - 0s - loss: 7.0963e-04 - val_loss: 4.8202e-04
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8182e-04 - val_loss: 5.0347e-04
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0414e-04 - val_loss: 3.9193e-04
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9176e-04 - val_loss: 4.4576e-04
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4644e-04 - val_loss: 4.0093e-04
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0075e-04 - val_loss: 5.0444e-04
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0519e-04 - val_loss: 4.8926e-04
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8904e-04 - val_loss: 6.4569e-04
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.4658e-04 - val_loss: 6.0171e-04
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.0143e-04 - val_loss: 7.6177e-04
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00009
10/10 - 0s - loss: 7.6280e-04 - val_loss: 6.2743e-04
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.2713e-04 - val_loss: 7.2624e-04
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00009
10/10 - 0s - loss: 7.2734e-04 - val_loss: 5.5239e-04
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5213e-04 - val_loss: 6.0636e-04
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.0746e-04 - val_loss: 4.7286e-04
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7265e-04 - val_loss: 5.2662e-04
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2772e-04 - val_loss: 4.4409e-04
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4391e-04 - val_loss: 5.2083e-04
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2197e-04 - val_loss: 4.6944e-04
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6924e-04 - val_loss: 5.7510e-04
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.7632e-04 - val_loss: 5.2547e-04
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2524e-04 - val_loss: 6.4526e-04
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.4659e-04 - val_loss: 5.6422e-04
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.6397e-04 - val_loss: 6.6681e-04
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.6822e-04 - val_loss: 5.5112e-04
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5088e-04 - val_loss: 6.2432e-04
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.2576e-04 - val_loss: 5.0809e-04
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0790e-04 - val_loss: 5.6845e-04
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.6990e-04 - val_loss: 4.7652e-04
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7637e-04 - val_loss: 5.4302e-04
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4449e-04 - val_loss: 4.7508e-04
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7494e-04 - val_loss: 5.5628e-04
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5780e-04 - val_loss: 4.9851e-04
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9836e-04 - val_loss: 5.9090e-04
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.9249e-04 - val_loss: 5.2507e-04
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2491e-04 - val_loss: 6.1483e-04
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.1650e-04 - val_loss: 5.3112e-04
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3097e-04 - val_loss: 6.0668e-04
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00009
10/10 - 0s - loss: 6.0839e-04 - val_loss: 5.1408e-04
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1396e-04 - val_loss: 5.7767e-04
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.7940e-04 - val_loss: 4.9229e-04
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9220e-04 - val_loss: 5.5417e-04
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5592e-04 - val_loss: 4.8287e-04
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8280e-04 - val_loss: 5.5077e-04
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5255e-04 - val_loss: 4.8959e-04
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8953e-04 - val_loss: 5.6439e-04
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.6621e-04 - val_loss: 5.0392e-04
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0386e-04 - val_loss: 5.7998e-04
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.8186e-04 - val_loss: 5.1233e-04
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1227e-04 - val_loss: 5.8246e-04
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.8438e-04 - val_loss: 5.0787e-04
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0783e-04 - val_loss: 5.7013e-04
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.7207e-04 - val_loss: 4.9587e-04
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9586e-04 - val_loss: 5.5435e-04
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5631e-04 - val_loss: 4.8673e-04
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8674e-04 - val_loss: 5.4661e-04
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4858e-04 - val_loss: 4.8622e-04
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8625e-04 - val_loss: 5.4964e-04
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5165e-04 - val_loss: 4.9244e-04
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9247e-04 - val_loss: 5.5755e-04
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5959e-04 - val_loss: 4.9851e-04
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9855e-04 - val_loss: 5.6147e-04
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.6354e-04 - val_loss: 4.9867e-04
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9872e-04 - val_loss: 5.5716e-04
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5926e-04 - val_loss: 4.9303e-04
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9310e-04 - val_loss: 5.4809e-04
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.5021e-04 - val_loss: 4.8652e-04
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8661e-04 - val_loss: 5.4099e-04
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4312e-04 - val_loss: 4.8375e-04
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8386e-04 - val_loss: 5.3960e-04
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4175e-04 - val_loss: 4.8549e-04
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8560e-04 - val_loss: 5.4251e-04
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4469e-04 - val_loss: 4.8880e-04
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8893e-04 - val_loss: 5.4517e-04
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4737e-04 - val_loss: 4.8996e-04
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9010e-04 - val_loss: 5.4396e-04
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4618e-04 - val_loss: 4.8762e-04
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8777e-04 - val_loss: 5.3914e-04
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.4138e-04 - val_loss: 4.8358e-04
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8374e-04 - val_loss: 5.3391e-04
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3616e-04 - val_loss: 4.8069e-04
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8087e-04 - val_loss: 5.3122e-04
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3348e-04 - val_loss: 4.8034e-04
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8052e-04 - val_loss: 5.3142e-04
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3370e-04 - val_loss: 4.8162e-04
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8182e-04 - val_loss: 5.3252e-04
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3482e-04 - val_loss: 4.8248e-04
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8268e-04 - val_loss: 5.3214e-04
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3446e-04 - val_loss: 4.8154e-04
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8175e-04 - val_loss: 5.2956e-04
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.3189e-04 - val_loss: 4.7915e-04
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7938e-04 - val_loss: 5.2602e-04
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2836e-04 - val_loss: 4.7681e-04
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7705e-04 - val_loss: 5.2332e-04
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2567e-04 - val_loss: 4.7568e-04
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7593e-04 - val_loss: 5.2229e-04
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2465e-04 - val_loss: 4.7575e-04
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7601e-04 - val_loss: 5.2224e-04
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2461e-04 - val_loss: 4.7603e-04
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7630e-04 - val_loss: 5.2184e-04
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2422e-04 - val_loss: 4.7553e-04
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7580e-04 - val_loss: 5.2028e-04
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2268e-04 - val_loss: 4.7408e-04
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7436e-04 - val_loss: 5.1788e-04
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.2028e-04 - val_loss: 4.7232e-04
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7261e-04 - val_loss: 5.1559e-04
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1800e-04 - val_loss: 4.7103e-04
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7133e-04 - val_loss: 5.1412e-04
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1654e-04 - val_loss: 4.7048e-04
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7078e-04 - val_loss: 5.1339e-04
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1582e-04 - val_loss: 4.7026e-04
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7058e-04 - val_loss: 5.1275e-04
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1518e-04 - val_loss: 4.6980e-04
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7012e-04 - val_loss: 5.1159e-04
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1403e-04 - val_loss: 4.6880e-04
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6913e-04 - val_loss: 5.0986e-04
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1231e-04 - val_loss: 4.6747e-04
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6781e-04 - val_loss: 5.0799e-04
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.1044e-04 - val_loss: 4.6626e-04
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6660e-04 - val_loss: 5.0646e-04
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0892e-04 - val_loss: 4.6543e-04
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6578e-04 - val_loss: 5.0540e-04
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0787e-04 - val_loss: 4.6490e-04
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6526e-04 - val_loss: 5.0454e-04
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0701e-04 - val_loss: 4.6435e-04
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6471e-04 - val_loss: 5.0349e-04
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0597e-04 - val_loss: 4.6352e-04
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6389e-04 - val_loss: 5.0212e-04
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0460e-04 - val_loss: 4.6246e-04
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6283e-04 - val_loss: 5.0057e-04
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0305e-04 - val_loss: 4.6137e-04
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6175e-04 - val_loss: 4.9913e-04
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0161e-04 - val_loss: 4.6046e-04
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6085e-04 - val_loss: 4.9795e-04
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00009
10/10 - 0s - loss: 5.0043e-04 - val_loss: 4.5975e-04
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6014e-04 - val_loss: 4.9694e-04
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9943e-04 - val_loss: 4.5909e-04
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5948e-04 - val_loss: 4.9590e-04
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9840e-04 - val_loss: 4.5832e-04
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5872e-04 - val_loss: 4.9470e-04
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9719e-04 - val_loss: 4.5740e-04
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5780e-04 - val_loss: 4.9337e-04
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9586e-04 - val_loss: 4.5642e-04
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5682e-04 - val_loss: 4.9204e-04
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9454e-04 - val_loss: 4.5550e-04
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5591e-04 - val_loss: 4.9084e-04
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9333e-04 - val_loss: 4.5470e-04
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5511e-04 - val_loss: 4.8976e-04
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9226e-04 - val_loss: 4.5396e-04
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5438e-04 - val_loss: 4.8871e-04
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9121e-04 - val_loss: 4.5319e-04
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5361e-04 - val_loss: 4.8759e-04
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.9009e-04 - val_loss: 4.5234e-04
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5276e-04 - val_loss: 4.8638e-04
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8888e-04 - val_loss: 4.5142e-04
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5185e-04 - val_loss: 4.8515e-04
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8765e-04 - val_loss: 4.5053e-04
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5096e-04 - val_loss: 4.8397e-04
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8647e-04 - val_loss: 4.4969e-04
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5013e-04 - val_loss: 4.8287e-04
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8537e-04 - val_loss: 4.4890e-04
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4934e-04 - val_loss: 4.8181e-04
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8431e-04 - val_loss: 4.4812e-04
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4856e-04 - val_loss: 4.8073e-04
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8323e-04 - val_loss: 4.4729e-04
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4773e-04 - val_loss: 4.7960e-04
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8210e-04 - val_loss: 4.4643e-04
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4687e-04 - val_loss: 4.7845e-04
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.8094e-04 - val_loss: 4.4555e-04
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4600e-04 - val_loss: 4.7731e-04
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7980e-04 - val_loss: 4.4471e-04
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4515e-04 - val_loss: 4.7621e-04
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7870e-04 - val_loss: 4.4389e-04
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4434e-04 - val_loss: 4.7515e-04
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7764e-04 - val_loss: 4.4310e-04
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4355e-04 - val_loss: 4.7409e-04
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7658e-04 - val_loss: 4.4228e-04
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4273e-04 - val_loss: 4.7301e-04
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7550e-04 - val_loss: 4.4144e-04
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4189e-04 - val_loss: 4.7191e-04
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7439e-04 - val_loss: 4.4059e-04
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4104e-04 - val_loss: 4.7081e-04
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7329e-04 - val_loss: 4.3975e-04
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4020e-04 - val_loss: 4.6974e-04
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7221e-04 - val_loss: 4.3892e-04
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3938e-04 - val_loss: 4.6869e-04
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7116e-04 - val_loss: 4.3812e-04
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3857e-04 - val_loss: 4.6764e-04
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.7011e-04 - val_loss: 4.3730e-04
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3776e-04 - val_loss: 4.6659e-04
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6906e-04 - val_loss: 4.3648e-04
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3694e-04 - val_loss: 4.6554e-04
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6800e-04 - val_loss: 4.3565e-04
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3611e-04 - val_loss: 4.6447e-04
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6693e-04 - val_loss: 4.3481e-04
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3527e-04 - val_loss: 4.6342e-04
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6587e-04 - val_loss: 4.3399e-04
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3445e-04 - val_loss: 4.6239e-04
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6484e-04 - val_loss: 4.3318e-04
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3364e-04 - val_loss: 4.6136e-04
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6381e-04 - val_loss: 4.3237e-04
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3284e-04 - val_loss: 4.6034e-04
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6278e-04 - val_loss: 4.3156e-04
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3202e-04 - val_loss: 4.5931e-04
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6175e-04 - val_loss: 4.3074e-04
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3120e-04 - val_loss: 4.5828e-04
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.6071e-04 - val_loss: 4.2992e-04
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3038e-04 - val_loss: 4.5725e-04
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5968e-04 - val_loss: 4.2910e-04
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2956e-04 - val_loss: 4.5624e-04
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5866e-04 - val_loss: 4.2829e-04
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2876e-04 - val_loss: 4.5523e-04
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5765e-04 - val_loss: 4.2749e-04
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2795e-04 - val_loss: 4.5423e-04
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5664e-04 - val_loss: 4.2669e-04
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2715e-04 - val_loss: 4.5322e-04
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5564e-04 - val_loss: 4.2588e-04
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2634e-04 - val_loss: 4.5222e-04
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5463e-04 - val_loss: 4.2507e-04
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2553e-04 - val_loss: 4.5122e-04
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5362e-04 - val_loss: 4.2426e-04
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2472e-04 - val_loss: 4.5022e-04
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5262e-04 - val_loss: 4.2346e-04
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2392e-04 - val_loss: 4.4923e-04
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5163e-04 - val_loss: 4.2266e-04
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2312e-04 - val_loss: 4.4825e-04
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.5064e-04 - val_loss: 4.2186e-04
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2232e-04 - val_loss: 4.4727e-04
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4966e-04 - val_loss: 4.2107e-04
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2153e-04 - val_loss: 4.4629e-04
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4867e-04 - val_loss: 4.2027e-04
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2073e-04 - val_loss: 4.4531e-04
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4768e-04 - val_loss: 4.1947e-04
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1993e-04 - val_loss: 4.4434e-04
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4671e-04 - val_loss: 4.1868e-04
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1913e-04 - val_loss: 4.4337e-04
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4573e-04 - val_loss: 4.1789e-04
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1834e-04 - val_loss: 4.4241e-04
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4476e-04 - val_loss: 4.1710e-04
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1755e-04 - val_loss: 4.4145e-04
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4380e-04 - val_loss: 4.1631e-04
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1677e-04 - val_loss: 4.4049e-04
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4284e-04 - val_loss: 4.1553e-04
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1598e-04 - val_loss: 4.3954e-04
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4187e-04 - val_loss: 4.1474e-04
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1519e-04 - val_loss: 4.3858e-04
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.4091e-04 - val_loss: 4.1396e-04
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1441e-04 - val_loss: 4.3763e-04
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3996e-04 - val_loss: 4.1317e-04
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1362e-04 - val_loss: 4.3669e-04
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3901e-04 - val_loss: 4.1240e-04
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1284e-04 - val_loss: 4.3575e-04
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3806e-04 - val_loss: 4.1162e-04
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1207e-04 - val_loss: 4.3481e-04
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3712e-04 - val_loss: 4.1085e-04
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1129e-04 - val_loss: 4.3387e-04
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3618e-04 - val_loss: 4.1007e-04
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1052e-04 - val_loss: 4.3294e-04
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3524e-04 - val_loss: 4.0930e-04
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0974e-04 - val_loss: 4.3201e-04
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3430e-04 - val_loss: 4.0853e-04
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0897e-04 - val_loss: 4.3108e-04
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3337e-04 - val_loss: 4.0776e-04
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0820e-04 - val_loss: 4.3016e-04
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3244e-04 - val_loss: 4.0699e-04
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0743e-04 - val_loss: 4.2924e-04
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3152e-04 - val_loss: 4.0623e-04
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0666e-04 - val_loss: 4.2833e-04
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.3059e-04 - val_loss: 4.0547e-04
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0590e-04 - val_loss: 4.2741e-04
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2967e-04 - val_loss: 4.0470e-04
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0514e-04 - val_loss: 4.2650e-04
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2876e-04 - val_loss: 4.0394e-04
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0438e-04 - val_loss: 4.2559e-04
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2784e-04 - val_loss: 4.0319e-04
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0362e-04 - val_loss: 4.2469e-04
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2693e-04 - val_loss: 4.0243e-04
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0286e-04 - val_loss: 4.2379e-04
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2602e-04 - val_loss: 4.0168e-04
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0211e-04 - val_loss: 4.2289e-04
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2512e-04 - val_loss: 4.0093e-04
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0135e-04 - val_loss: 4.2200e-04
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2422e-04 - val_loss: 4.0018e-04
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0060e-04 - val_loss: 4.2110e-04
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2332e-04 - val_loss: 3.9943e-04
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9985e-04 - val_loss: 4.2021e-04
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2242e-04 - val_loss: 3.9868e-04
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9910e-04 - val_loss: 4.1933e-04
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2153e-04 - val_loss: 3.9794e-04
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9836e-04 - val_loss: 4.1845e-04
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.2064e-04 - val_loss: 3.9720e-04
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9762e-04 - val_loss: 4.1757e-04
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1976e-04 - val_loss: 3.9646e-04
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9687e-04 - val_loss: 4.1669e-04
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1887e-04 - val_loss: 3.9572e-04
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9613e-04 - val_loss: 4.1581e-04
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1799e-04 - val_loss: 3.9498e-04
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9539e-04 - val_loss: 4.1494e-04
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1711e-04 - val_loss: 3.9425e-04
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9466e-04 - val_loss: 4.1407e-04
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1624e-04 - val_loss: 3.9352e-04
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9393e-04 - val_loss: 4.1321e-04
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1537e-04 - val_loss: 3.9279e-04
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9319e-04 - val_loss: 4.1235e-04
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1450e-04 - val_loss: 3.9206e-04
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9246e-04 - val_loss: 4.1149e-04
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1364e-04 - val_loss: 3.9134e-04
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9174e-04 - val_loss: 4.1063e-04
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1277e-04 - val_loss: 3.9061e-04
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9101e-04 - val_loss: 4.0977e-04
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1191e-04 - val_loss: 3.8988e-04
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9028e-04 - val_loss: 4.0892e-04
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1105e-04 - val_loss: 3.8917e-04
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8956e-04 - val_loss: 4.0807e-04
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.1020e-04 - val_loss: 3.8845e-04
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8884e-04 - val_loss: 4.0723e-04
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0935e-04 - val_loss: 3.8773e-04
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8812e-04 - val_loss: 4.0639e-04
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0850e-04 - val_loss: 3.8702e-04
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8741e-04 - val_loss: 4.0555e-04
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0765e-04 - val_loss: 3.8631e-04
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8669e-04 - val_loss: 4.0471e-04
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0681e-04 - val_loss: 3.8560e-04
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8598e-04 - val_loss: 4.0388e-04
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0597e-04 - val_loss: 3.8489e-04
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8528e-04 - val_loss: 4.0305e-04
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0513e-04 - val_loss: 3.8419e-04
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8457e-04 - val_loss: 4.0222e-04
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0430e-04 - val_loss: 3.8348e-04
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8386e-04 - val_loss: 4.0139e-04
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0346e-04 - val_loss: 3.8278e-04
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8316e-04 - val_loss: 4.0057e-04
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0264e-04 - val_loss: 3.8208e-04
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8246e-04 - val_loss: 3.9975e-04
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0181e-04 - val_loss: 3.8139e-04
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8176e-04 - val_loss: 3.9894e-04
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0099e-04 - val_loss: 3.8069e-04
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8106e-04 - val_loss: 3.9812e-04
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00009
10/10 - 0s - loss: 4.0017e-04 - val_loss: 3.7999e-04
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8036e-04 - val_loss: 3.9731e-04
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9935e-04 - val_loss: 3.7930e-04
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7967e-04 - val_loss: 3.9650e-04
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9853e-04 - val_loss: 3.7862e-04
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7898e-04 - val_loss: 3.9570e-04
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9772e-04 - val_loss: 3.7793e-04
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7829e-04 - val_loss: 3.9490e-04
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9692e-04 - val_loss: 3.7725e-04
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7761e-04 - val_loss: 3.9410e-04
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9611e-04 - val_loss: 3.7656e-04
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7692e-04 - val_loss: 3.9330e-04
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9531e-04 - val_loss: 3.7588e-04
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7624e-04 - val_loss: 3.9250e-04
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9451e-04 - val_loss: 3.7520e-04
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7556e-04 - val_loss: 3.9171e-04
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9371e-04 - val_loss: 3.7453e-04
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7488e-04 - val_loss: 3.9093e-04
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9292e-04 - val_loss: 3.7386e-04
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7421e-04 - val_loss: 3.9014e-04
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9213e-04 - val_loss: 3.7318e-04
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7353e-04 - val_loss: 3.8936e-04
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9134e-04 - val_loss: 3.7251e-04
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7286e-04 - val_loss: 3.8858e-04
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.9055e-04 - val_loss: 3.7185e-04
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7219e-04 - val_loss: 3.8780e-04
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8977e-04 - val_loss: 3.7118e-04
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7152e-04 - val_loss: 3.8703e-04
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8899e-04 - val_loss: 3.7052e-04
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7086e-04 - val_loss: 3.8626e-04
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8821e-04 - val_loss: 3.6986e-04
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7019e-04 - val_loss: 3.8549e-04
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8743e-04 - val_loss: 3.6920e-04
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6953e-04 - val_loss: 3.8473e-04
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8666e-04 - val_loss: 3.6854e-04
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6887e-04 - val_loss: 3.8396e-04
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8590e-04 - val_loss: 3.6789e-04
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6822e-04 - val_loss: 3.8320e-04
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8513e-04 - val_loss: 3.6724e-04
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6756e-04 - val_loss: 3.8245e-04
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8437e-04 - val_loss: 3.6659e-04
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6691e-04 - val_loss: 3.8169e-04
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8361e-04 - val_loss: 3.6594e-04
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6626e-04 - val_loss: 3.8094e-04
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8285e-04 - val_loss: 3.6529e-04
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6561e-04 - val_loss: 3.8019e-04
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8209e-04 - val_loss: 3.6465e-04
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6497e-04 - val_loss: 3.7945e-04
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8134e-04 - val_loss: 3.6401e-04
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6432e-04 - val_loss: 3.7871e-04
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.8060e-04 - val_loss: 3.6337e-04
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6368e-04 - val_loss: 3.7797e-04
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7985e-04 - val_loss: 3.6273e-04
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6305e-04 - val_loss: 3.7723e-04
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7911e-04 - val_loss: 3.6210e-04
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6241e-04 - val_loss: 3.7650e-04
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7837e-04 - val_loss: 3.6147e-04
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6177e-04 - val_loss: 3.7576e-04
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7763e-04 - val_loss: 3.6084e-04
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6114e-04 - val_loss: 3.7504e-04
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7689e-04 - val_loss: 3.6021e-04
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6051e-04 - val_loss: 3.7431e-04
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7616e-04 - val_loss: 3.5958e-04
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5988e-04 - val_loss: 3.7359e-04
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7544e-04 - val_loss: 3.5896e-04
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5926e-04 - val_loss: 3.7287e-04
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7471e-04 - val_loss: 3.5834e-04
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5864e-04 - val_loss: 3.7215e-04
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7399e-04 - val_loss: 3.5772e-04
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5801e-04 - val_loss: 3.7144e-04
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7327e-04 - val_loss: 3.5710e-04
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5739e-04 - val_loss: 3.7073e-04
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7255e-04 - val_loss: 3.5649e-04
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5678e-04 - val_loss: 3.7002e-04
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7184e-04 - val_loss: 3.5588e-04
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5617e-04 - val_loss: 3.6932e-04
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7113e-04 - val_loss: 3.5527e-04
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5556e-04 - val_loss: 3.6862e-04
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.7042e-04 - val_loss: 3.5466e-04
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5494e-04 - val_loss: 3.6791e-04
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6971e-04 - val_loss: 3.5406e-04
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5434e-04 - val_loss: 3.6722e-04
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6901e-04 - val_loss: 3.5345e-04
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5373e-04 - val_loss: 3.6652e-04
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6831e-04 - val_loss: 3.5285e-04
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5313e-04 - val_loss: 3.6583e-04
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6762e-04 - val_loss: 3.5226e-04
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5253e-04 - val_loss: 3.6515e-04
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6692e-04 - val_loss: 3.5166e-04
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5193e-04 - val_loss: 3.6446e-04
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6623e-04 - val_loss: 3.5107e-04
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5134e-04 - val_loss: 3.6378e-04
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6555e-04 - val_loss: 3.5047e-04
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5074e-04 - val_loss: 3.6310e-04
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6486e-04 - val_loss: 3.4988e-04
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5015e-04 - val_loss: 3.6242e-04
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6417e-04 - val_loss: 3.4930e-04
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4956e-04 - val_loss: 3.6175e-04
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6350e-04 - val_loss: 3.4871e-04
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4897e-04 - val_loss: 3.6108e-04
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6282e-04 - val_loss: 3.4813e-04
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4839e-04 - val_loss: 3.6042e-04
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6215e-04 - val_loss: 3.4755e-04
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4781e-04 - val_loss: 3.5975e-04
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6148e-04 - val_loss: 3.4697e-04
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4723e-04 - val_loss: 3.5909e-04
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6081e-04 - val_loss: 3.4640e-04
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4665e-04 - val_loss: 3.5842e-04
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.6014e-04 - val_loss: 3.4582e-04
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4607e-04 - val_loss: 3.5777e-04
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5948e-04 - val_loss: 3.4525e-04
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4550e-04 - val_loss: 3.5712e-04
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5883e-04 - val_loss: 3.4469e-04
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4493e-04 - val_loss: 3.5647e-04
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5818e-04 - val_loss: 3.4412e-04
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4437e-04 - val_loss: 3.5582e-04
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5752e-04 - val_loss: 3.4356e-04
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4380e-04 - val_loss: 3.5518e-04
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5687e-04 - val_loss: 3.4299e-04
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4323e-04 - val_loss: 3.5453e-04
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5622e-04 - val_loss: 3.4243e-04
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4267e-04 - val_loss: 3.5389e-04
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5558e-04 - val_loss: 3.4187e-04
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4211e-04 - val_loss: 3.5326e-04
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5494e-04 - val_loss: 3.4132e-04
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4155e-04 - val_loss: 3.5263e-04
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5430e-04 - val_loss: 3.4077e-04
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4100e-04 - val_loss: 3.5200e-04
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5367e-04 - val_loss: 3.4022e-04
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4045e-04 - val_loss: 3.5137e-04
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5303e-04 - val_loss: 3.3967e-04
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.3990e-04 - val_loss: 3.5075e-04
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5240e-04 - val_loss: 3.3912e-04
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.3935e-04 - val_loss: 3.5012e-04
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5177e-04 - val_loss: 3.3858e-04
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.3880e-04 - val_loss: 3.4950e-04
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5115e-04 - val_loss: 3.3804e-04
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.3826e-04 - val_loss: 3.4889e-04
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.5053e-04 - val_loss: 3.3750e-04
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.3772e-04 - val_loss: 3.4828e-04
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00009
10/10 - 0s - loss: 3.4991e-04 - val_loss: 3.3696e-04
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 10.769 | eve: 9.060 | bob: 10.490Epoch   0:   0% | abe: 10.544 | eve: 9.150 | bob: 10.276Epoch   0:   1% | abe: 10.698 | eve: 9.313 | bob: 10.433Epoch   0:   2% | abe: 10.565 | eve: 9.217 | bob: 10.316Epoch   0:   3% | abe: 10.443 | eve: 9.265 | bob: 10.208Epoch   0:   3% | abe: 10.427 | eve: 9.285 | bob: 10.200Epoch   0:   4% | abe: 10.428 | eve: 9.368 | bob: 10.220Epoch   0:   5% | abe: 10.368 | eve: 9.372 | bob: 10.161Epoch   0:   6% | abe: 10.272 | eve: 9.370 | bob: 10.064Epoch   0:   7% | abe: 10.208 | eve: 9.371 | bob: 10.004Epoch   0:   7% | abe: 10.145 | eve: 9.341 | bob: 9.943Epoch   0:   8% | abe: 10.128 | eve: 9.341 | bob: 9.933Epoch   0:   9% | abe: 10.075 | eve: 9.330 | bob: 9.887Epoch   0:  10% | abe: 10.032 | eve: 9.327 | bob: 9.850Epoch   0:  10% | abe: 10.005 | eve: 9.311 | bob: 9.825Epoch   0:  11% | abe: 9.949 | eve: 9.346 | bob: 9.773Epoch   0:  12% | abe: 9.914 | eve: 9.377 | bob: 9.744Epoch   0:  13% | abe: 9.876 | eve: 9.390 | bob: 9.713Epoch   0:  14% | abe: 9.841 | eve: 9.390 | bob: 9.684Epoch   0:  14% | abe: 9.813 | eve: 9.358 | bob: 9.660Epoch   0:  15% | abe: 9.807 | eve: 9.332 | bob: 9.658Epoch   0:  16% | abe: 9.788 | eve: 9.352 | bob: 9.643Epoch   0:  17% | abe: 9.765 | eve: 9.347 | bob: 9.627Epoch   0:  17% | abe: 9.748 | eve: 9.325 | bob: 9.615Epoch   0:  18% | abe: 9.734 | eve: 9.308 | bob: 9.607Epoch   0:  19% | abe: 9.702 | eve: 9.317 | bob: 9.582Epoch   0:  20% | abe: 9.680 | eve: 9.295 | bob: 9.563Epoch   0:  21% | abe: 9.667 | eve: 9.283 | bob: 9.555Epoch   0:  21% | abe: 9.647 | eve: 9.283 | bob: 9.538Epoch   0:  22% | abe: 9.627 | eve: 9.275 | bob: 9.522Epoch   0:  23% | abe: 9.623 | eve: 9.250 | bob: 9.522Epoch   0:  24% | abe: 9.607 | eve: 9.264 | bob: 9.510Epoch   0:  25% | abe: 9.593 | eve: 9.273 | bob: 9.497Epoch   0:  25% | abe: 9.586 | eve: 9.269 | bob: 9.493Epoch   0:  26% | abe: 9.573 | eve: 9.264 | bob: 9.484Epoch   0:  27% | abe: 9.574 | eve: 9.258 | bob: 9.487Epoch   0:  28% | abe: 9.562 | eve: 9.240 | bob: 9.478Epoch   0:  28% | abe: 9.550 | eve: 9.249 | bob: 9.468Epoch   0:  29% | abe: 9.536 | eve: 9.250 | bob: 9.457Epoch   0:  30% | abe: 9.528 | eve: 9.250 | bob: 9.449Epoch   0:  31% | abe: 9.521 | eve: 9.241 | bob: 9.443Epoch   0:  32% | abe: 9.513 | eve: 9.225 | bob: 9.437Epoch   0:  32% | abe: 9.497 | eve: 9.227 | bob: 9.422Epoch   0:  33% | abe: 9.490 | eve: 9.226 | bob: 9.416Epoch   0:  34% | abe: 9.480 | eve: 9.224 | bob: 9.408Epoch   0:  35% | abe: 9.468 | eve: 9.226 | bob: 9.397Epoch   0:  35% | abe: 9.456 | eve: 9.222 | bob: 9.385Epoch   0:  36% | abe: 9.448 | eve: 9.227 | bob: 9.378Epoch   0:  37% | abe: 9.442 | eve: 9.221 | bob: 9.373Epoch   0:  38% | abe: 9.438 | eve: 9.209 | bob: 9.370Epoch   0:  39% | abe: 9.428 | eve: 9.204 | bob: 9.361Epoch   0:  39% | abe: 9.417 | eve: 9.211 | bob: 9.351Epoch   0:  40% | abe: 9.409 | eve: 9.215 | bob: 9.344Epoch   0:  41% | abe: 9.402 | eve: 9.221 | bob: 9.337Epoch   0:  42% | abe: 9.398 | eve: 9.233 | bob: 9.334Epoch   0:  42% | abe: 9.392 | eve: 9.221 | bob: 9.330Epoch   0:  43% | abe: 9.391 | eve: 9.212 | bob: 9.330Epoch   0:  44% | abe: 9.383 | eve: 9.215 | bob: 9.323Epoch   0:  45% | abe: 9.381 | eve: 9.212 | bob: 9.322Epoch   0:  46% | abe: 9.373 | eve: 9.216 | bob: 9.316Epoch   0:  46% | abe: 9.368 | eve: 9.217 | bob: 9.312Epoch   0:  47% | abe: 9.364 | eve: 9.216 | bob: 9.309Epoch   0:  48% | abe: 9.356 | eve: 9.213 | bob: 9.302Epoch   0:  49% | abe: 9.354 | eve: 9.219 | bob: 9.301Epoch   0:  50% | abe: 9.353 | eve: 9.212 | bob: 9.300Epoch   0:  50% | abe: 9.346 | eve: 9.203 | bob: 9.294Epoch   0:  51% | abe: 9.342 | eve: 9.215 | bob: 9.291Epoch   0:  52% | abe: 9.336 | eve: 9.229 | bob: 9.286Epoch   0:  53% | abe: 9.333 | eve: 9.231 | bob: 9.284Epoch   0:  53% | abe: 9.333 | eve: 9.236 | bob: 9.284Epoch   0:  54% | abe: 9.329 | eve: 9.239 | bob: 9.281Epoch   0:  55% | abe: 9.326 | eve: 9.244 | bob: 9.279Epoch   0:  56% | abe: 9.316 | eve: 9.243 | bob: 9.269Epoch   0:  57% | abe: 9.314 | eve: 9.245 | bob: 9.267Epoch   0:  57% | abe: 9.309 | eve: 9.239 | bob: 9.262Epoch   0:  58% | abe: 9.307 | eve: 9.244 | bob: 9.260Epoch   0:  59% | abe: 9.303 | eve: 9.246 | bob: 9.256Epoch   0:  60% | abe: 9.301 | eve: 9.249 | bob: 9.256Epoch   0:  60% | abe: 9.299 | eve: 9.249 | bob: 9.254Epoch   0:  61% | abe: 9.296 | eve: 9.250 | bob: 9.251Epoch   0:  62% | abe: 9.295 | eve: 9.244 | bob: 9.251Epoch   0:  63% | abe: 9.297 | eve: 9.236 | bob: 9.254Epoch   0:  64% | abe: 9.293 | eve: 9.236 | bob: 9.250Epoch   0:  64% | abe: 9.289 | eve: 9.233 | bob: 9.246Epoch   0:  65% | abe: 9.288 | eve: 9.235 | bob: 9.246Epoch   0:  66% | abe: 9.282 | eve: 9.239 | bob: 9.239Epoch   0:  67% | abe: 9.278 | eve: 9.239 | bob: 9.236Epoch   0:  67% | abe: 9.273 | eve: 9.251 | bob: 9.231Epoch   0:  68% | abe: 9.269 | eve: 9.256 | bob: 9.227Epoch   0:  69% | abe: 9.266 | eve: 9.258 | bob: 9.225Epoch   0:  70% | abe: 9.266 | eve: 9.256 | bob: 9.224Epoch   0:  71% | abe: 9.262 | eve: 9.257 | bob: 9.221Epoch   0:  71% | abe: 9.261 | eve: 9.251 | bob: 9.220Epoch   0:  72% | abe: 9.258 | eve: 9.248 | bob: 9.217Epoch   0:  73% | abe: 9.251 | eve: 9.249 | bob: 9.210Epoch   0:  74% | abe: 9.250 | eve: 9.243 | bob: 9.209Epoch   0:  75% | abe: 9.249 | eve: 9.244 | bob: 9.208Epoch   0:  75% | abe: 9.249 | eve: 9.242 | bob: 9.207Epoch   0:  76% | abe: 9.250 | eve: 9.240 | bob: 9.208Epoch   0:  77% | abe: 9.248 | eve: 9.237 | bob: 9.207Epoch   0:  78% | abe: 9.248 | eve: 9.239 | bob: 9.207Epoch   0:  78% | abe: 9.247 | eve: 9.240 | bob: 9.206Epoch   0:  79% | abe: 9.245 | eve: 9.236 | bob: 9.205Epoch   0:  80% | abe: 9.242 | eve: 9.237 | bob: 9.202Epoch   0:  81% | abe: 9.241 | eve: 9.240 | bob: 9.200Epoch   0:  82% | abe: 9.241 | eve: 9.241 | bob: 9.201Epoch   0:  82% | abe: 9.239 | eve: 9.248 | bob: 9.199Epoch   0:  83% | abe: 9.241 | eve: 9.244 | bob: 9.201Epoch   0:  84% | abe: 9.240 | eve: 9.239 | bob: 9.201Epoch   0:  85% | abe: 9.238 | eve: 9.235 | bob: 9.199Epoch   0:  85% | abe: 9.236 | eve: 9.238 | bob: 9.197Epoch   0:  86% | abe: 9.237 | eve: 9.235 | bob: 9.198Epoch   0:  87% | abe: 9.239 | eve: 9.234 | bob: 9.201Epoch   0:  88% | abe: 9.237 | eve: 9.234 | bob: 9.198Epoch   0:  89% | abe: 9.235 | eve: 9.237 | bob: 9.197Epoch   0:  89% | abe: 9.233 | eve: 9.236 | bob: 9.194Epoch   0:  90% | abe: 9.231 | eve: 9.232 | bob: 9.193Epoch   0:  91% | abe: 9.230 | eve: 9.228 | bob: 9.192Epoch   0:  92% | abe: 9.228 | eve: 9.223 | bob: 9.189Epoch   0:  92% | abe: 9.228 | eve: 9.223 | bob: 9.190Epoch   0:  93% | abe: 9.227 | eve: 9.224 | bob: 9.189Epoch   0:  94% | abe: 9.226 | eve: 9.226 | bob: 9.188Epoch   0:  95% | abe: 9.228 | eve: 9.227 | bob: 9.190Epoch   0:  96% | abe: 9.227 | eve: 9.227 | bob: 9.189Epoch   0:  96% | abe: 9.228 | eve: 9.224 | bob: 9.191Epoch   0:  97% | abe: 9.226 | eve: 9.224 | bob: 9.190Epoch   0:  98% | abe: 9.224 | eve: 9.225 | bob: 9.187Epoch   0:  99% | abe: 9.224 | eve: 9.229 | bob: 9.189
New best Bob loss 9.188528863876126 at epoch 0
Epoch   1:   0% | abe: 8.994 | eve: 9.270 | bob: 9.007Epoch   1:   0% | abe: 9.096 | eve: 9.176 | bob: 9.116Epoch   1:   1% | abe: 9.064 | eve: 9.390 | bob: 9.069Epoch   1:   2% | abe: 9.051 | eve: 9.189 | bob: 9.053Epoch   1:   3% | abe: 9.056 | eve: 9.194 | bob: 9.064Epoch   1:   3% | abe: 9.051 | eve: 9.120 | bob: 9.060Epoch   1:   4% | abe: 9.036 | eve: 9.124 | bob: 9.045Epoch   1:   5% | abe: 9.081 | eve: 9.145 | bob: 9.090Epoch   1:   6% | abe: 9.077 | eve: 9.169 | bob: 9.090Epoch   1:   7% | abe: 9.082 | eve: 9.164 | bob: 9.094Epoch   1:   7% | abe: 9.113 | eve: 9.206 | bob: 9.129Epoch   1:   8% | abe: 9.106 | eve: 9.305 | bob: 9.118Epoch   1:   9% | abe: 9.099 | eve: 9.302 | bob: 9.110Epoch   1:  10% | abe: 9.107 | eve: 9.304 | bob: 9.117Epoch   1:  10% | abe: 9.120 | eve: 9.341 | bob: 9.131Epoch   1:  11% | abe: 9.124 | eve: 9.355 | bob: 9.134Epoch   1:  12% | abe: 9.125 | eve: 9.345 | bob: 9.134Epoch   1:  13% | abe: 9.138 | eve: 9.312 | bob: 9.146Epoch   1:  14% | abe: 9.134 | eve: 9.310 | bob: 9.144Epoch   1:  14% | abe: 9.137 | eve: 9.316 | bob: 9.146Epoch   1:  15% | abe: 9.132 | eve: 9.306 | bob: 9.143Epoch   1:  16% | abe: 9.133 | eve: 9.323 | bob: 9.143Epoch   1:  17% | abe: 9.134 | eve: 9.296 | bob: 9.144Epoch   1:  17% | abe: 9.123 | eve: 9.285 | bob: 9.134Epoch   1:  18% | abe: 9.112 | eve: 9.295 | bob: 9.122Epoch   1:  19% | abe: 9.123 | eve: 9.285 | bob: 9.134Epoch   1:  20% | abe: 9.126 | eve: 9.303 | bob: 9.139Epoch   1:  21% | abe: 9.135 | eve: 9.288 | bob: 9.151Epoch   1:  21% | abe: 9.139 | eve: 9.264 | bob: 9.155Epoch   1:  22% | abe: 9.131 | eve: 9.266 | bob: 9.148Epoch   1:  23% | abe: 9.133 | eve: 9.267 | bob: 9.150Epoch   1:  24% | abe: 9.145 | eve: 9.273 | bob: 9.162Epoch   1:  25% | abe: 9.136 | eve: 9.263 | bob: 9.152Epoch   1:  25% | abe: 9.140 | eve: 9.260 | bob: 9.157Epoch   1:  26% | abe: 9.142 | eve: 9.260 | bob: 9.159Epoch   1:  27% | abe: 9.134 | eve: 9.259 | bob: 9.152Epoch   1:  28% | abe: 9.136 | eve: 9.266 | bob: 9.154Epoch   1:  28% | abe: 9.147 | eve: 9.268 | bob: 9.165Epoch   1:  29% | abe: 9.146 | eve: 9.272 | bob: 9.165Epoch   1:  30% | abe: 9.140 | eve: 9.270 | bob: 9.160Epoch   1:  31% | abe: 9.144 | eve: 9.265 | bob: 9.164Epoch   1:  32% | abe: 9.140 | eve: 9.270 | bob: 9.159Epoch   1:  32% | abe: 9.139 | eve: 9.271 | bob: 9.158Epoch   1:  33% | abe: 9.142 | eve: 9.272 | bob: 9.161Epoch   1:  34% | abe: 9.144 | eve: 9.278 | bob: 9.163Epoch   1:  35% | abe: 9.147 | eve: 9.272 | bob: 9.166Epoch   1:  35% | abe: 9.142 | eve: 9.256 | bob: 9.162Epoch   1:  36% | abe: 9.133 | eve: 9.237 | bob: 9.153Epoch   1:  37% | abe: 9.134 | eve: 9.225 | bob: 9.155Epoch   1:  38% | abe: 9.135 | eve: 9.228 | bob: 9.155Epoch   1:  39% | abe: 9.134 | eve: 9.227 | bob: 9.155Epoch   1:  39% | abe: 9.130 | eve: 9.236 | bob: 9.150Epoch   1:  40% | abe: 9.129 | eve: 9.236 | bob: 9.149Epoch   1:  41% | abe: 9.124 | eve: 9.237 | bob: 9.145Epoch   1:  42% | abe: 9.127 | eve: 9.231 | bob: 9.148Epoch   1:  42% | abe: 9.123 | eve: 9.221 | bob: 9.143Epoch   1:  43% | abe: 9.120 | eve: 9.229 | bob: 9.140Epoch   1:  44% | abe: 9.117 | eve: 9.231 | bob: 9.137Epoch   1:  45% | abe: 9.117 | eve: 9.228 | bob: 9.137Epoch   1:  46% | abe: 9.118 | eve: 9.228 | bob: 9.138Epoch   1:  46% | abe: 9.120 | eve: 9.229 | bob: 9.140Epoch   1:  47% | abe: 9.121 | eve: 9.233 | bob: 9.141Epoch   1:  48% | abe: 9.122 | eve: 9.234 | bob: 9.142Epoch   1:  49% | abe: 9.124 | eve: 9.227 | bob: 9.144Epoch   1:  50% | abe: 9.118 | eve: 9.222 | bob: 9.138Epoch   1:  50% | abe: 9.116 | eve: 9.219 | bob: 9.136Epoch   1:  51% | abe: 9.114 | eve: 9.222 | bob: 9.134Epoch   1:  52% | abe: 9.113 | eve: 9.219 | bob: 9.132Epoch   1:  53% | abe: 9.110 | eve: 9.219 | bob: 9.130Epoch   1:  53% | abe: 9.114 | eve: 9.224 | bob: 9.133Epoch   1:  54% | abe: 9.111 | eve: 9.230 | bob: 9.130Epoch   1:  55% | abe: 9.106 | eve: 9.229 | bob: 9.125Epoch   1:  56% | abe: 9.105 | eve: 9.234 | bob: 9.124Epoch   1:  57% | abe: 9.104 | eve: 9.233 | bob: 9.123Epoch   1:  57% | abe: 9.107 | eve: 9.236 | bob: 9.126Epoch   1:  58% | abe: 9.105 | eve: 9.235 | bob: 9.124Epoch   1:  59% | abe: 9.104 | eve: 9.239 | bob: 9.123Epoch   1:  60% | abe: 9.105 | eve: 9.238 | bob: 9.123Epoch   1:  60% | abe: 9.106 | eve: 9.235 | bob: 9.125Epoch   1:  61% | abe: 9.105 | eve: 9.232 | bob: 9.124Epoch   1:  62% | abe: 9.108 | eve: 9.232 | bob: 9.126Epoch   1:  63% | abe: 9.106 | eve: 9.229 | bob: 9.124Epoch   1:  64% | abe: 9.102 | eve: 9.223 | bob: 9.121Epoch   1:  64% | abe: 9.102 | eve: 9.218 | bob: 9.121Epoch   1:  65% | abe: 9.101 | eve: 9.218 | bob: 9.119Epoch   1:  66% | abe: 9.097 | eve: 9.216 | bob: 9.116Epoch   1:  67% | abe: 9.094 | eve: 9.221 | bob: 9.112Epoch   1:  67% | abe: 9.093 | eve: 9.220 | bob: 9.111Epoch   1:  68% | abe: 9.090 | eve: 9.218 | bob: 9.108Epoch   1:  69% | abe: 9.089 | eve: 9.216 | bob: 9.106Epoch   1:  70% | abe: 9.086 | eve: 9.218 | bob: 9.103Epoch   1:  71% | abe: 9.085 | eve: 9.217 | bob: 9.102Epoch   1:  71% | abe: 9.086 | eve: 9.218 | bob: 9.103Epoch   1:  72% | abe: 9.087 | eve: 9.222 | bob: 9.103Epoch   1:  73% | abe: 9.088 | eve: 9.219 | bob: 9.105Epoch   1:  74% | abe: 9.088 | eve: 9.223 | bob: 9.105Epoch   1:  75% | abe: 9.086 | eve: 9.220 | bob: 9.104Epoch   1:  75% | abe: 9.088 | eve: 9.223 | bob: 9.105Epoch   1:  76% | abe: 9.094 | eve: 9.230 | bob: 9.112Epoch   1:  77% | abe: 9.095 | eve: 9.226 | bob: 9.112Epoch   1:  78% | abe: 9.094 | eve: 9.229 | bob: 9.112Epoch   1:  78% | abe: 9.094 | eve: 9.227 | bob: 9.112Epoch   1:  79% | abe: 9.094 | eve: 9.227 | bob: 9.112Epoch   1:  80% | abe: 9.093 | eve: 9.228 | bob: 9.111Epoch   1:  81% | abe: 9.091 | eve: 9.225 | bob: 9.108Epoch   1:  82% | abe: 9.091 | eve: 9.224 | bob: 9.108Epoch   1:  82% | abe: 9.090 | eve: 9.225 | bob: 9.107Epoch   1:  83% | abe: 9.088 | eve: 9.225 | bob: 9.105Epoch   1:  84% | abe: 9.089 | eve: 9.223 | bob: 9.106Epoch   1:  85% | abe: 9.085 | eve: 9.221 | bob: 9.103Epoch   1:  85% | abe: 9.085 | eve: 9.217 | bob: 9.102Epoch   1:  86% | abe: 9.085 | eve: 9.219 | bob: 9.103Epoch   1:  87% | abe: 9.085 | eve: 9.216 | bob: 9.103Epoch   1:  88% | abe: 9.085 | eve: 9.216 | bob: 9.102Epoch   1:  89% | abe: 9.083 | eve: 9.216 | bob: 9.101Epoch   1:  89% | abe: 9.082 | eve: 9.214 | bob: 9.099Epoch   1:  90% | abe: 9.081 | eve: 9.210 | bob: 9.099Epoch   1:  91% | abe: 9.078 | eve: 9.211 | bob: 9.096Epoch   1:  92% | abe: 9.080 | eve: 9.212 | bob: 9.098Epoch   1:  92% | abe: 9.078 | eve: 9.213 | bob: 9.096Epoch   1:  93% | abe: 9.078 | eve: 9.216 | bob: 9.096Epoch   1:  94% | abe: 9.075 | eve: 9.218 | bob: 9.093Epoch   1:  95% | abe: 9.074 | eve: 9.214 | bob: 9.092Epoch   1:  96% | abe: 9.074 | eve: 9.216 | bob: 9.092Epoch   1:  96% | abe: 9.073 | eve: 9.219 | bob: 9.091Epoch   1:  97% | abe: 9.072 | eve: 9.217 | bob: 9.091Epoch   1:  98% | abe: 9.073 | eve: 9.217 | bob: 9.091Epoch   1:  99% | abe: 9.073 | eve: 9.219 | bob: 9.092
New best Bob loss 9.0917851880542 at epoch 1
Epoch   2:   0% | abe: 9.262 | eve: 9.157 | bob: 9.278Epoch   2:   0% | abe: 9.286 | eve: 9.227 | bob: 9.313Epoch   2:   1% | abe: 9.229 | eve: 9.043 | bob: 9.244Epoch   2:   2% | abe: 9.120 | eve: 9.029 | bob: 9.132Epoch   2:   3% | abe: 9.148 | eve: 8.957 | bob: 9.170Epoch   2:   3% | abe: 9.185 | eve: 8.979 | bob: 9.217Epoch   2:   4% | abe: 9.188 | eve: 9.021 | bob: 9.221Epoch   2:   5% | abe: 9.198 | eve: 9.101 | bob: 9.229Epoch   2:   6% | abe: 9.187 | eve: 9.086 | bob: 9.220Epoch   2:   7% | abe: 9.176 | eve: 9.101 | bob: 9.206Epoch   2:   7% | abe: 9.177 | eve: 9.140 | bob: 9.203Epoch   2:   8% | abe: 9.140 | eve: 9.197 | bob: 9.164Epoch   2:   9% | abe: 9.132 | eve: 9.208 | bob: 9.158Epoch   2:  10% | abe: 9.126 | eve: 9.195 | bob: 9.154Epoch   2:  10% | abe: 9.105 | eve: 9.198 | bob: 9.133Epoch   2:  11% | abe: 9.106 | eve: 9.204 | bob: 9.135Epoch   2:  12% | abe: 9.113 | eve: 9.175 | bob: 9.142Epoch   2:  13% | abe: 9.117 | eve: 9.173 | bob: 9.147Epoch   2:  14% | abe: 9.114 | eve: 9.176 | bob: 9.141Epoch   2:  14% | abe: 9.107 | eve: 9.169 | bob: 9.133Epoch   2:  15% | abe: 9.112 | eve: 9.188 | bob: 9.139Epoch   2:  16% | abe: 9.114 | eve: 9.186 | bob: 9.141Epoch   2:  17% | abe: 9.124 | eve: 9.180 | bob: 9.153Epoch   2:  17% | abe: 9.127 | eve: 9.191 | bob: 9.157Epoch   2:  18% | abe: 9.116 | eve: 9.176 | bob: 9.145Epoch   2:  19% | abe: 9.116 | eve: 9.191 | bob: 9.145Epoch   2:  20% | abe: 9.116 | eve: 9.220 | bob: 9.145Epoch   2:  21% | abe: 9.124 | eve: 9.226 | bob: 9.152Epoch   2:  21% | abe: 9.122 | eve: 9.230 | bob: 9.151Epoch   2:  22% | abe: 9.115 | eve: 9.217 | bob: 9.145Epoch   2:  23% | abe: 9.111 | eve: 9.213 | bob: 9.142Epoch   2:  24% | abe: 9.114 | eve: 9.207 | bob: 9.145Epoch   2:  25% | abe: 9.106 | eve: 9.210 | bob: 9.136Epoch   2:  25% | abe: 9.107 | eve: 9.178 | bob: 9.139Epoch   2:  26% | abe: 9.100 | eve: 9.171 | bob: 9.131Epoch   2:  27% | abe: 9.098 | eve: 9.177 | bob: 9.129Epoch   2:  28% | abe: 9.093 | eve: 9.173 | bob: 9.125Epoch   2:  28% | abe: 9.087 | eve: 9.161 | bob: 9.119Epoch   2:  29% | abe: 9.087 | eve: 9.172 | bob: 9.120Epoch   2:  30% | abe: 9.081 | eve: 9.180 | bob: 9.113Epoch   2:  31% | abe: 9.075 | eve: 9.188 | bob: 9.107Epoch   2:  32% | abe: 9.065 | eve: 9.193 | bob: 9.097Epoch   2:  32% | abe: 9.068 | eve: 9.195 | bob: 9.100Epoch   2:  33% | abe: 9.068 | eve: 9.200 | bob: 9.100Epoch   2:  34% | abe: 9.062 | eve: 9.203 | bob: 9.093Epoch   2:  35% | abe: 9.064 | eve: 9.199 | bob: 9.095Epoch   2:  35% | abe: 9.066 | eve: 9.197 | bob: 9.097Epoch   2:  36% | abe: 9.064 | eve: 9.198 | bob: 9.095Epoch   2:  37% | abe: 9.064 | eve: 9.193 | bob: 9.095Epoch   2:  38% | abe: 9.066 | eve: 9.182 | bob: 9.097Epoch   2:  39% | abe: 9.069 | eve: 9.173 | bob: 9.100Epoch   2:  39% | abe: 9.068 | eve: 9.170 | bob: 9.100Epoch   2:  40% | abe: 9.068 | eve: 9.167 | bob: 9.099Epoch   2:  41% | abe: 9.064 | eve: 9.164 | bob: 9.095Epoch   2:  42% | abe: 9.061 | eve: 9.171 | bob: 9.091Epoch   2:  42% | abe: 9.056 | eve: 9.173 | bob: 9.086Epoch   2:  43% | abe: 9.052 | eve: 9.180 | bob: 9.083Epoch   2:  44% | abe: 9.053 | eve: 9.174 | bob: 9.083Epoch   2:  45% | abe: 9.050 | eve: 9.170 | bob: 9.081Epoch   2:  46% | abe: 9.054 | eve: 9.173 | bob: 9.085Epoch   2:  46% | abe: 9.054 | eve: 9.165 | bob: 9.086Epoch   2:  47% | abe: 9.050 | eve: 9.164 | bob: 9.081Epoch   2:  48% | abe: 9.051 | eve: 9.170 | bob: 9.083Epoch   2:  49% | abe: 9.047 | eve: 9.170 | bob: 9.078Epoch   2:  50% | abe: 9.044 | eve: 9.174 | bob: 9.075Epoch   2:  50% | abe: 9.042 | eve: 9.176 | bob: 9.074Epoch   2:  51% | abe: 9.042 | eve: 9.177 | bob: 9.074Epoch   2:  52% | abe: 9.042 | eve: 9.184 | bob: 9.074Epoch   2:  53% | abe: 9.043 | eve: 9.183 | bob: 9.075Epoch   2:  53% | abe: 9.042 | eve: 9.188 | bob: 9.073Epoch   2:  54% | abe: 9.044 | eve: 9.176 | bob: 9.076Epoch   2:  55% | abe: 9.043 | eve: 9.181 | bob: 9.074Epoch   2:  56% | abe: 9.045 | eve: 9.187 | bob: 9.077Epoch   2:  57% | abe: 9.046 | eve: 9.188 | bob: 9.078Epoch   2:  57% | abe: 9.046 | eve: 9.191 | bob: 9.078Epoch   2:  58% | abe: 9.049 | eve: 9.195 | bob: 9.080Epoch   2:  59% | abe: 9.048 | eve: 9.201 | bob: 9.078Epoch   2:  60% | abe: 9.047 | eve: 9.201 | bob: 9.077Epoch   2:  60% | abe: 9.044 | eve: 9.208 | bob: 9.074Epoch   2:  61% | abe: 9.044 | eve: 9.203 | bob: 9.074Epoch   2:  62% | abe: 9.041 | eve: 9.200 | bob: 9.071Epoch   2:  63% | abe: 9.041 | eve: 9.195 | bob: 9.071Epoch   2:  64% | abe: 9.039 | eve: 9.193 | bob: 9.069Epoch   2:  64% | abe: 9.039 | eve: 9.193 | bob: 9.069Epoch   2:  65% | abe: 9.039 | eve: 9.186 | bob: 9.069Epoch   2:  66% | abe: 9.037 | eve: 9.188 | bob: 9.067Epoch   2:  67% | abe: 9.040 | eve: 9.182 | bob: 9.071Epoch   2:  67% | abe: 9.040 | eve: 9.181 | bob: 9.071Epoch   2:  68% | abe: 9.040 | eve: 9.184 | bob: 9.072Epoch   2:  69% | abe: 9.039 | eve: 9.180 | bob: 9.071Epoch   2:  70% | abe: 9.040 | eve: 9.180 | bob: 9.071Epoch   2:  71% | abe: 9.038 | eve: 9.177 | bob: 9.070Epoch   2:  71% | abe: 9.037 | eve: 9.173 | bob: 9.068Epoch   2:  72% | abe: 9.038 | eve: 9.171 | bob: 9.070Epoch   2:  73% | abe: 9.037 | eve: 9.168 | bob: 9.069Epoch   2:  74% | abe: 9.038 | eve: 9.172 | bob: 9.070Epoch   2:  75% | abe: 9.041 | eve: 9.172 | bob: 9.073Epoch   2:  75% | abe: 9.042 | eve: 9.173 | bob: 9.074Epoch   2:  76% | abe: 9.042 | eve: 9.180 | bob: 9.074Epoch   2:  77% | abe: 9.043 | eve: 9.185 | bob: 9.075Epoch   2:  78% | abe: 9.045 | eve: 9.181 | bob: 9.078Epoch   2:  78% | abe: 9.042 | eve: 9.182 | bob: 9.075Epoch   2:  79% | abe: 9.040 | eve: 9.184 | bob: 9.072Epoch   2:  80% | abe: 9.042 | eve: 9.184 | bob: 9.073Epoch   2:  81% | abe: 9.041 | eve: 9.181 | bob: 9.073Epoch   2:  82% | abe: 9.044 | eve: 9.181 | bob: 9.076Epoch   2:  82% | abe: 9.042 | eve: 9.180 | bob: 9.074Epoch   2:  83% | abe: 9.043 | eve: 9.184 | bob: 9.075Epoch   2:  84% | abe: 9.041 | eve: 9.179 | bob: 9.072Epoch   2:  85% | abe: 9.042 | eve: 9.177 | bob: 9.074Epoch   2:  85% | abe: 9.039 | eve: 9.175 | bob: 9.071Epoch   2:  86% | abe: 9.038 | eve: 9.175 | bob: 9.070Epoch   2:  87% | abe: 9.038 | eve: 9.171 | bob: 9.071Epoch   2:  88% | abe: 9.038 | eve: 9.172 | bob: 9.071Epoch   2:  89% | abe: 9.039 | eve: 9.172 | bob: 9.071Epoch   2:  89% | abe: 9.039 | eve: 9.168 | bob: 9.072Epoch   2:  90% | abe: 9.040 | eve: 9.173 | bob: 9.072Epoch   2:  91% | abe: 9.038 | eve: 9.168 | bob: 9.071Epoch   2:  92% | abe: 9.037 | eve: 9.166 | bob: 9.069Epoch   2:  92% | abe: 9.036 | eve: 9.167 | bob: 9.068Epoch   2:  93% | abe: 9.037 | eve: 9.165 | bob: 9.069Epoch   2:  94% | abe: 9.037 | eve: 9.164 | bob: 9.070Epoch   2:  95% | abe: 9.036 | eve: 9.164 | bob: 9.069Epoch   2:  96% | abe: 9.036 | eve: 9.163 | bob: 9.069Epoch   2:  96% | abe: 9.034 | eve: 9.164 | bob: 9.068Epoch   2:  97% | abe: 9.032 | eve: 9.160 | bob: 9.065Epoch   2:  98% | abe: 9.032 | eve: 9.162 | bob: 9.065Epoch   2:  99% | abe: 9.030 | eve: 9.169 | bob: 9.062
New best Bob loss 9.062222775281407 at epoch 2
Training complete.
cipher1 + cipher2
[[1.0142887  1.1061478  0.9316945  ... 1.0382173  1.0401328  0.9467286 ]
 [1.0483471  1.1128271  0.9750691  ... 1.0086455  1.0976533  0.9580916 ]
 [1.0718721  1.0225263  0.9347712  ... 1.0370314  1.0947373  0.9506935 ]
 ...
 [1.0538938  1.1167147  0.9504709  ... 1.0323153  1.0777017  0.9540485 ]
 [1.0449761  1.0688703  0.9362937  ... 1.0041769  1.1021171  0.9555435 ]
 [1.0386624  1.1068401  0.9684169  ... 0.9753804  1.0909141  0.99698603]]
HO addition:
[[1.0146918  1.1066419  0.9321793  ... 1.0386441  1.0407237  0.94720113]
 [1.048776   1.1132753  0.975566   ... 1.0090784  1.0982372  0.9585334 ]
 [1.0723228  1.0229832  0.935333   ... 1.0374686  1.0953084  0.9511403 ]
 ...
 [1.0543135  1.1171974  0.9509601  ... 1.0327407  1.078277   0.9545116 ]
 [1.0453849  1.0693283  0.93679214 ... 1.0046009  1.1027     0.95599383]
 [1.0390537  1.1073071  0.9689644  ... 0.975804   1.0914482  0.9974755 ]]
cipher1 * cipher2
[[0.25412884 0.30490386 0.21691458 ... 0.26705384 0.26906124 0.22407342]
 [0.2721832  0.3062719  0.23766641 ... 0.2528915  0.3008682  0.22908942]
 [0.28516513 0.26055318 0.21594115 ... 0.26698047 0.29945612 0.22572063]
 ...
 [0.2744229  0.310131   0.22579558 ... 0.2640842  0.28999063 0.22749496]
 [0.26932287 0.2839572  0.21890795 ... 0.25036013 0.30338535 0.22804935]
 [0.26506117 0.30414957 0.23333636 ... 0.23667403 0.29748425 0.24847083]]
HO multiplication
[[0.27498296 0.30816287 0.24222682 ... 0.2834293  0.2784614  0.24814455]
 [0.28723893 0.31189743 0.25753567 ... 0.2721405  0.3019054  0.2533492 ]
 [0.29578978 0.27668357 0.23928027 ... 0.28272253 0.3012055  0.25052616]
 ...
 [0.28956744 0.31265184 0.24881344 ... 0.281223   0.29419047 0.25114936]
 [0.28640917 0.29444596 0.24326584 ... 0.27071473 0.3037556  0.25214523]
 [0.2843797  0.30911264 0.25273275 ... 0.2601566  0.3009523  0.2660073 ]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 2.34%
Bob decrypted addition: [[0.7217069  0.6702143  0.72980815 0.97340375 0.67694557 0.8965976
  0.66470075 0.8991631  0.6546997  0.7080096  0.6841241  0.6838956
  0.825718   0.9124267  0.999912   0.94770294]
 [0.7359592  0.6636665  0.7236328  0.8566495  0.6524458  0.83723116
  0.6812242  0.9110767  0.65878737 0.83025247 0.6700701  0.7134072
  0.819695   0.88297254 0.88463783 0.9756924 ]
 [0.78538775 0.673975   0.6983887  0.8790495  0.6721715  0.7484484
  0.68659323 0.9052046  0.675787   0.8034405  0.6840938  0.8102681
  0.69102836 0.84877944 0.8871597  0.9772916 ]
 [0.7138488  0.6864884  0.7231654  0.967777   0.65353316 0.8683522
  0.6794276  0.685152   0.64734423 0.69421935 0.69314253 0.6735877
  0.7771994  0.8476616  0.945289   0.96261257]
 [0.8038365  0.6678508  0.695508   0.7360567  0.65190345 0.98284394
  0.6639553  0.913098   0.6579337  0.69977355 0.755775   0.67546606
  0.73797715 0.83138406 0.8446046  0.9882062 ]
 [0.7289659  0.66966987 0.74673504 0.78796154 0.6980139  0.92533386
  0.67787725 0.9730849  0.6732252  0.71629846 0.68051946 0.67992103
  0.7209145  0.9388168  0.6718059  1.0444466 ]
 [0.83920014 0.66050375 0.7004869  0.889378   0.6996695  0.798246
  0.67220414 0.8628173  0.6526698  0.75066185 0.6621687  0.6984032
  0.70296955 0.8939338  0.7545832  1.0170604 ]
 [0.82387066 0.6819917  0.7131462  0.8977013  0.68288755 1.0247309
  0.70381963 0.94744253 0.6635292  0.69303286 0.6767334  0.6866492
  0.79333085 0.7450572  0.9361754  0.96821296]
 [0.74082136 0.6599082  0.8118379  0.7026005  0.64986384 1.0100886
  0.70456797 0.7606138  0.63622665 0.71001816 0.6781558  0.6853759
  0.91393024 0.79666543 0.8724998  0.9780093 ]
 [0.7282882  0.66384125 0.7210375  0.814384   0.6426448  0.90380377
  0.6618973  0.7500518  0.6398245  0.7032255  0.7279824  0.68876046
  0.96377695 0.80212635 0.7997111  0.9989045 ]]
Bob decrypted bits addition: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits addition: 69
Total number of bits addition: 160
Decryption accuracy addition: 43.125%
Bob decrypted multiplication: [[0.8449096  0.67581093 0.92041    1.0310014  0.6863735  0.9183812
  0.68198276 0.9386108  0.6787338  0.7870317  0.7424824  0.69908786
  0.88631994 0.87664896 1.0249705  0.94349784]
 [0.84194416 0.6690644  0.9224434  0.886533   0.66268635 0.8654515
  0.71291363 0.96418196 0.6909807  0.9940775  0.72703564 0.7490047
  0.88448155 0.88244545 0.9053099  0.9660818 ]
 [0.87823135 0.70814323 0.8476184  0.92881864 0.71465814 0.7455574
  0.7248452  0.95020527 0.853861   0.8975417  0.7697568  0.85810906
  0.73850673 0.835031   0.9154342  0.96830267]
 [0.8060371  0.72675633 0.9319571  1.1064112  0.66861606 0.8784034
  0.7123195  0.697505   0.6614027  0.7447745  0.7775264  0.67678154
  0.8276801  0.8345622  0.98155874 0.95399994]
 [0.89149487 0.6950362  0.8293015  0.78022695 0.66409206 1.0058949
  0.6745728  0.9877383  0.71882814 0.7412073  0.9258323  0.6769997
  0.7887379  0.80520016 0.874781   0.9780239 ]
 [0.8201562  0.686255   0.9418691  0.8135892  0.7536624  0.9322042
  0.7121765  1.0019664  0.6984302  0.8044823  0.73709506 0.688972
  0.76779866 0.98600876 0.66172457 1.0371486 ]
 [0.9107141  0.6716515  0.8468392  0.9261677  0.76631904 0.7837902
  0.69382465 0.9181723  0.72137237 0.8515874  0.67620444 0.7268702
  0.73434854 0.87135154 0.7766666  1.0046649 ]
 [0.92032486 0.73725903 0.8573115  0.9025519  0.73819077 1.0527225
  0.74995875 0.99736583 0.6924475  0.72771704 0.7241435  0.700667
  0.8618319  0.7455485  0.95999837 0.96020573]
 [0.8657613  0.6587329  1.0194815  0.7172576  0.6598977  1.0889239
  0.7556968  0.8244636  0.6514346  0.7893263  0.7394607  0.6935327
  0.97371143 0.834819   0.8841939  0.96843415]
 [0.8488656  0.66347396 0.91862994 0.8208567  0.64806986 0.916554
  0.66817605 0.79741776 0.6628473  0.7696088  0.84975773 0.7052026
  1.0025084  0.81144625 0.8258231  0.9861557 ]]
Bob decrypted bits multiplication: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits multiplication: 45
Total number of bits multiplication: 160
Decryption accuracy multiplication: 28.125%
Eve decrypted addition: [[0.94804156 0.9810107  0.91616845 0.9937742  0.99609977 1.0212514
  0.9545849  1.0292516  1.0330296  1.032896   0.9649619  1.1094667
  0.9998564  0.93826205 0.96172917 1.0189416 ]
 [0.95942026 0.96939737 0.91897416 0.9913244  1.0069896  1.0066164
  0.9794024  1.0138863  1.0629351  1.0212188  0.94906676 1.1214023
  0.9973329  0.93324673 0.9634981  1.0215085 ]
 [0.9560185  0.9771339  0.91606474 0.9998337  0.98910034 1.0202811
  0.96530914 1.0175906  1.0508229  1.0191042  0.95242816 1.1055077
  1.0078967  0.93741316 0.95939374 1.0221311 ]
 [0.95293415 0.98040223 0.91121495 1.0043325  0.9763159  1.0367028
  0.938468   1.0335455  1.036292   1.0142463  0.975166   1.1041781
  1.0081962  0.9259223  0.97499436 1.0161521 ]
 [0.9559266  0.97634757 0.91584486 1.0011536  0.9846624  1.032797
  0.9461157  1.0160356  1.0653793  1.0123578  0.9721119  1.1118693
  1.0047617  0.94507444 0.95988584 1.0137066 ]
 [0.96740776 0.96773225 0.91014946 0.9999768  0.99753916 1.0254668
  0.952783   1.0263431  1.0507883  1.0406808  0.9393807  1.1008759
  1.0123116  0.93085486 0.9655319  1.0134394 ]
 [0.95752203 0.978525   0.9152339  0.9908373  0.9983392  1.0230155
  0.9520123  1.0244561  1.0432159  1.0294881  0.9601969  1.1097009
  1.0056193  0.93757814 0.95721203 1.0200918 ]
 [0.95996106 0.969554   0.91086644 1.0027752  0.99780965 1.0246949
  0.9531092  1.0171199  1.0593954  1.0217746  0.94702274 1.1004313
  1.0148882  0.942729   0.94501555 1.0208892 ]
 [0.96262634 0.9618002  0.92630696 0.99679273 0.9999092  1.0355844
  0.93941766 1.0202852  1.0689214  0.9991292  0.96954095 1.1144128
  1.0035311  0.94110984 0.96138537 1.015524  ]
 [0.96677786 0.97276694 0.90818924 1.0153846  0.97564864 1.0150691
  0.9610338  1.0140213  1.0585726  1.039613   0.93853587 1.1003042
  1.0104395  0.93392366 0.9592441  1.0235374 ]]
Eve decrypted bits addition: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits by Eve addition: 69
Total number of bits addition: 160
Decryption accuracy by Eve addition: 43.125%
Eve decrypted mulitplication: [[0.9428729  1.0026459  0.93275607 0.9882879  0.98446804 1.029671
  0.9293849  1.0465798  0.999836   1.0049608  0.9912818  1.0875217
  1.0035731  0.9535925  0.9693774  1.0288289 ]
 [0.9582714  0.98021567 0.9399686  0.9955192  0.989112   1.0159479
  0.97881484 1.0116916  1.0506561  0.9689902  0.9804405  1.1028733
  1.0009052  0.9596489  0.954144   1.0336553 ]
 [0.96229076 0.98615956 0.9188637  1.0051842  0.9750005  1.0249066
  0.96107453 1.0113161  1.0396948  0.99085754 0.9726233  1.088319
  1.0206425  0.96154416 0.94234204 1.026224  ]
 [0.9532046  0.9874571  0.9192881  1.0067164  0.9588512  1.047652
  0.9248005  1.0433606  1.025925   0.97525644 1.0008023  1.0744691
  1.0254701  0.95578486 0.9672669  1.0109011 ]
 [0.96000934 0.98988503 0.92446816 1.0041738  0.9702388  1.0538083
  0.923613   1.0077373  1.0687257  0.98327816 0.99638814 1.0808947
  1.027358   0.9714351  0.9547518  1.0073133 ]
 [0.9673333  0.9773568  0.9133402  1.0020001  0.9827337  1.0345291
  0.9301929  1.0295502  1.0423714  1.021966   0.97065336 1.0756474
  1.032018   0.94996756 0.9634776  1.010594  ]
 [0.9617946  1.0011592  0.93250716 0.97800595 0.97980917 1.0323018
  0.9298342  1.0357968  1.0200962  0.99079996 0.98943377 1.0894347
  1.0199971  0.9542047  0.9552987  1.0242836 ]
 [0.9695613  0.9788484  0.9181333  1.0091374  0.98382944 1.030392
  0.94397557 1.0148768  1.0489837  0.97957027 0.9857631  1.0650727
  1.0358384  0.9665425  0.92983663 1.0223535 ]
 [0.9794227  0.97577506 0.94073635 0.99402535 0.98906046 1.0478961
  0.9186087  1.032251   1.0480747  0.963793   0.9927255  1.1004004
  1.0094264  0.9576814  0.95716053 1.021723  ]
 [0.9828405  0.9899162  0.9139444  1.020878   0.95772195 1.0210242
  0.94494736 1.0122263  1.0533751  1.0038545  0.9714836  1.0616552
  1.0338584  0.9559074  0.95365655 1.0321797 ]]
Eve decrypted bits mulitplication: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 45
Total number of bits mulitplication: 160
Decryption accuracy by Eve mulitplication: 28.125%
Bob decrypted P1: [[0.79363436 0.67371166 0.86846334 1.0129153  0.68487144 0.9231441
  0.6719973  0.93945926 0.6700317  0.75899714 0.7136378  0.69495237
  0.86772484 0.88052833 1.0151392  0.9452305 ]
 [0.80464345 0.6659163  0.8578681  0.88539433 0.6602291  0.871562
  0.6947893  0.9574541  0.67762494 0.9478437  0.6979285  0.7431288
  0.8673881  0.87116456 0.9011472  0.9690872 ]
 [0.8491896  0.69364595 0.79672545 0.9191431  0.69537795 0.76073825
  0.70524037 0.9455118  0.7596433  0.8717391  0.7266847  0.85994405
  0.72654617 0.82615066 0.9084916  0.97131044]
 [0.773065   0.7100365  0.8599543  1.0768664  0.6631521  0.88759106
  0.69363993 0.6990137  0.6511281  0.72683966 0.73755693 0.6763866
  0.81803954 0.83291036 0.9696398  0.9567705 ]
 [0.8643862  0.68163836 0.77646947 0.76523334 0.655995   1.0056715
  0.6688386  0.96964586 0.6879938  0.7263876  0.86262727 0.6784212
  0.77423346 0.8021399  0.86516136 0.9817482 ]
 [0.7886701  0.677698   0.8960396  0.8107498  0.7316335  0.9387147
  0.69264066 0.9978252  0.6878751  0.77183294 0.7104064  0.6881656
  0.75267553 0.9552093  0.6635493  1.0397596 ]
 [0.88974285 0.6666857  0.7945171  0.9182516  0.73596346 0.80891407
  0.6814977  0.9118308  0.68690383 0.8199263  0.66767514 0.72030497
  0.72262585 0.86526895 0.7708355  1.0084943 ]
 [0.8886979  0.7135304  0.82430476 0.9016163  0.71236366 1.0406944
  0.7251384  0.9889993  0.6795256  0.7166107  0.6990372  0.6987518
  0.8411886  0.73700386 0.95798594 0.96222615]
 [0.82153106 0.65814936 0.9786793  0.7118575  0.6529641  1.0683117
  0.72849715 0.8087648  0.6448914  0.76390815 0.7059337  0.69388217
  0.95897824 0.80797136 0.88643676 0.97105527]
 [0.7996253  0.6635456  0.8529836  0.82509583 0.6473722  0.92875046
  0.6647203  0.79144764 0.6500906  0.7471323  0.80063033 0.7020403
  0.99202853 0.7945429  0.8216446  0.9899547 ]]
Bob decrypted bits P1: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits P1: 74
Total number of bits P1: 160
Decryption accuracy P1: 46.25%
Bob decrypted P2: [[0.8043198  0.67044437 0.83892536 0.99429107 0.68089163 0.89113206
  0.6752293  0.9060591  0.6638119  0.755994   0.71862733 0.6920072
  0.86751467 0.8901474  1.012074   0.9455379 ]
 [0.80633545 0.66507316 0.83305776 0.8505602  0.65646774 0.8337493
  0.7045842  0.92634267 0.67217064 0.9486939  0.69999826 0.7326528
  0.86025935 0.88274884 0.89388376 0.9702589 ]
 [0.85404086 0.69117594 0.7734613  0.8909189  0.6910304  0.73869765
  0.71058404 0.91499096 0.7575165  0.8725553  0.73365086 0.83368015
  0.7164849  0.84502363 0.89898115 0.97256374]
 [0.7727622  0.70650613 0.839047   1.0357037  0.65849054 0.8565283
  0.7032429  0.6825105  0.6548766  0.7248639  0.74269426 0.67537236
  0.8008315  0.8349103  0.9617957  0.9583043 ]
 [0.8690944  0.683544   0.7600806  0.7526484  0.65819484 0.98108745
  0.6701808  0.9392257  0.68236697 0.72497    0.867613   0.6747005
  0.7648499  0.81642926 0.8581164  0.9827175 ]
 [0.7919407  0.678453   0.8669582  0.7837188  0.731308   0.9143089
  0.70105743 0.9835616  0.685658   0.7720039  0.7097981  0.6841742
  0.7463187  0.9716144  0.66167235 1.0408627 ]
 [0.89240414 0.6667198  0.76719636 0.89800125 0.7306756  0.77505326
  0.6871588  0.8793448  0.68114096 0.816755   0.66858184 0.7141633
  0.7205689  0.8853431  0.7619886  1.0109792 ]
 [0.8928261  0.71626145 0.7867539  0.89439124 0.7070826  1.0365349
  0.73854053 0.963671   0.6736077  0.7143563  0.70311785 0.6936928
  0.8336184  0.7459419  0.945247   0.9642752 ]
 [0.8203096  0.65708596 0.95921993 0.70286393 0.6557523  1.0427922
  0.7399036  0.7775779  0.6411905  0.7559663  0.7116637  0.6898613
  0.9532619  0.82744837 0.8705596  0.9732736 ]
 [0.80737996 0.6605518  0.8260369  0.80006856 0.6434046  0.8875721
  0.66627073 0.7623016  0.64993703 0.74271786 0.8125157  0.69609404
  0.9911632  0.8116381  0.8089957  0.99208796]]
Bob decrypted bits P2: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits P2: 85
Total number of bits P2: 160
Decryption accuracy P2: 53.125%
