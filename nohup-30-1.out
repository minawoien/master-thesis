WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-11 17:32:32.524021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-11 17:32:32.658567: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-11 17:32:32.659420: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:32:32.661661: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 17:32:32.663381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 17:32:32.664059: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 17:32:32.666355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 17:32:32.668482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 17:32:32.676824: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 17:32:32.683730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 17:32:32.684137: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-11 17:32:32.697739: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-11 17:32:32.700630: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4b5d800 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-11 17:32:32.700656: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-11 17:32:33.043695: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4526ca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-11 17:32:33.043760: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-11 17:32:33.055349: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-11 17:32:33.055477: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:32:33.055515: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 17:32:33.055545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 17:32:33.055575: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 17:32:33.055625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 17:32:33.055683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 17:32:33.055751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 17:32:33.065185: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 17:32:33.065272: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:32:33.075389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-11 17:32:33.075418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-11 17:32:33.075444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-11 17:32:33.082210: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-11 17:32:36.657955: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.9597 - val_loss: 0.0090
Epoch 2/512
512/512 - 0s - loss: 0.7463 - val_loss: 0.0037
Epoch 3/512
512/512 - 0s - loss: 0.2144 - val_loss: 4.9587e-04
Epoch 4/512
512/512 - 0s - loss: 0.0244 - val_loss: 4.2558e-05
Epoch 5/512
512/512 - 0s - loss: 0.0028 - val_loss: 1.6025e-05
Epoch 6/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.0128e-05
Epoch 7/512
512/512 - 0s - loss: 8.3829e-04 - val_loss: 5.7777e-06
Epoch 8/512
512/512 - 0s - loss: 4.5767e-04 - val_loss: 2.8627e-06
Epoch 9/512
512/512 - 0s - loss: 2.1509e-04 - val_loss: 1.1892e-06
Epoch 10/512
512/512 - 0s - loss: 8.3845e-05 - val_loss: 3.9522e-07
Epoch 11/512
512/512 - 0s - loss: 2.5825e-05 - val_loss: 9.8856e-08
Epoch 12/512
512/512 - 0s - loss: 5.9078e-06 - val_loss: 1.7393e-08
Epoch 13/512
512/512 - 0s - loss: 1.1159e-06 - val_loss: 2.4212e-08
Epoch 14/512
512/512 - 0s - loss: 1.0877e-04 - val_loss: 2.0306e-05
Epoch 15/512
512/512 - 0s - loss: 0.0042 - val_loss: 1.3443e-05
Epoch 16/512
512/512 - 0s - loss: 6.3777e-04 - val_loss: 2.1261e-06
Epoch 17/512
512/512 - 0s - loss: 2.5768e-04 - val_loss: 5.8428e-06
Epoch 18/512
512/512 - 0s - loss: 0.0015 - val_loss: 3.5189e-05
Epoch 19/512
512/512 - 0s - loss: 0.0025 - val_loss: 1.0596e-05
Epoch 20/512
512/512 - 0s - loss: 7.6323e-04 - val_loss: 6.7968e-06
Epoch 21/512
512/512 - 0s - loss: 9.3665e-04 - val_loss: 1.7665e-05
Epoch 22/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.8017e-05
Epoch 23/512
512/512 - 0s - loss: 0.0013 - val_loss: 9.1722e-06
Epoch 24/512
512/512 - 0s - loss: 9.1982e-04 - val_loss: 1.1862e-05
Epoch 25/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.7092e-05
Epoch 26/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.1729e-05
Epoch 27/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0529e-05
Epoch 28/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.3931e-05
Epoch 29/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2656e-05
Epoch 30/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0503e-05
Epoch 31/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.1816e-05
Epoch 32/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.2265e-05
Epoch 33/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0584e-05
Epoch 34/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0428e-05
Epoch 35/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.1161e-05
Epoch 36/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0385e-05
Epoch 37/512
512/512 - 0s - loss: 9.7889e-04 - val_loss: 9.7980e-06
Epoch 38/512
512/512 - 0s - loss: 9.5216e-04 - val_loss: 1.0325e-05
Epoch 39/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8136e-06
Epoch 40/512
512/512 - 0s - loss: 9.2037e-04 - val_loss: 9.1899e-06
Epoch 41/512
512/512 - 0s - loss: 9.0412e-04 - val_loss: 9.4359e-06
Epoch 42/512
512/512 - 0s - loss: 9.1652e-04 - val_loss: 9.4467e-06
Epoch 43/512
512/512 - 0s - loss: 8.9436e-04 - val_loss: 8.8269e-06
Epoch 44/512
512/512 - 0s - loss: 8.4542e-04 - val_loss: 8.6410e-06
Epoch 45/512
512/512 - 0s - loss: 8.4597e-04 - val_loss: 8.6927e-06
Epoch 46/512
512/512 - 0s - loss: 8.4345e-04 - val_loss: 8.3189e-06
Epoch 47/512
512/512 - 0s - loss: 7.9732e-04 - val_loss: 8.0567e-06
Epoch 48/512
512/512 - 0s - loss: 7.8848e-04 - val_loss: 8.1482e-06
Epoch 49/512
512/512 - 0s - loss: 7.9167e-04 - val_loss: 7.8206e-06
Epoch 50/512
512/512 - 0s - loss: 7.4735e-04 - val_loss: 7.5372e-06
Epoch 51/512
512/512 - 0s - loss: 7.3546e-04 - val_loss: 7.6935e-06
Epoch 52/512
512/512 - 0s - loss: 7.4368e-04 - val_loss: 7.3920e-06
Epoch 53/512
512/512 - 0s - loss: 7.0706e-04 - val_loss: 7.0538e-06
Epoch 54/512
512/512 - 0s - loss: 6.8689e-04 - val_loss: 6.9940e-06
Epoch 55/512
512/512 - 0s - loss: 6.7815e-04 - val_loss: 7.0645e-06
Epoch 56/512
512/512 - 0s - loss: 6.7735e-04 - val_loss: 6.7798e-06
Epoch 57/512
512/512 - 0s - loss: 6.5004e-04 - val_loss: 6.4108e-06
Epoch 58/512
512/512 - 0s - loss: 6.2537e-04 - val_loss: 6.4218e-06
Epoch 59/512
512/512 - 0s - loss: 6.3137e-04 - val_loss: 6.3205e-06
Epoch 60/512
512/512 - 0s - loss: 6.0753e-04 - val_loss: 6.0862e-06
Epoch 61/512
512/512 - 0s - loss: 5.9221e-04 - val_loss: 6.0093e-06
Epoch 62/512
512/512 - 0s - loss: 5.8550e-04 - val_loss: 5.8183e-06
Epoch 63/512
512/512 - 0s - loss: 5.6807e-04 - val_loss: 5.6676e-06
Epoch 64/512
512/512 - 0s - loss: 5.5603e-04 - val_loss: 5.5455e-06
Epoch 65/512
512/512 - 0s - loss: 5.4338e-04 - val_loss: 5.4572e-06
Epoch 66/512
512/512 - 0s - loss: 5.3106e-04 - val_loss: 5.3671e-06
Epoch 67/512
512/512 - 0s - loss: 5.2380e-04 - val_loss: 5.1985e-06
Epoch 68/512
512/512 - 0s - loss: 5.0783e-04 - val_loss: 5.0213e-06
Epoch 69/512
512/512 - 0s - loss: 4.9419e-04 - val_loss: 4.9451e-06
Epoch 70/512
512/512 - 0s - loss: 4.8614e-04 - val_loss: 4.9003e-06
Epoch 71/512
512/512 - 0s - loss: 4.8061e-04 - val_loss: 4.6702e-06
Epoch 72/512
512/512 - 0s - loss: 4.5750e-04 - val_loss: 4.5689e-06
Epoch 73/512
512/512 - 0s - loss: 4.5060e-04 - val_loss: 4.5960e-06
Epoch 74/512
512/512 - 0s - loss: 4.5564e-04 - val_loss: 4.3254e-06
Epoch 75/512
512/512 - 0s - loss: 4.2277e-04 - val_loss: 4.2026e-06
Epoch 76/512
512/512 - 0s - loss: 4.1925e-04 - val_loss: 4.2902e-06
Epoch 77/512
512/512 - 0s - loss: 4.2268e-04 - val_loss: 4.1515e-06
Epoch 78/512
512/512 - 0s - loss: 4.0616e-04 - val_loss: 3.8580e-06
Epoch 79/512
512/512 - 0s - loss: 3.8390e-04 - val_loss: 3.8383e-06
Epoch 80/512
512/512 - 0s - loss: 3.8527e-04 - val_loss: 3.9712e-06
Epoch 81/512
512/512 - 0s - loss: 3.8795e-04 - val_loss: 3.7388e-06
Epoch 82/512
512/512 - 0s - loss: 3.6233e-04 - val_loss: 3.5270e-06
Epoch 83/512
512/512 - 0s - loss: 3.5378e-04 - val_loss: 3.5569e-06
Epoch 84/512
512/512 - 0s - loss: 3.5373e-04 - val_loss: 3.5433e-06
Epoch 85/512
512/512 - 0s - loss: 3.4892e-04 - val_loss: 3.3264e-06
Epoch 86/512
512/512 - 0s - loss: 3.2698e-04 - val_loss: 3.2326e-06
Epoch 87/512
512/512 - 0s - loss: 3.2532e-04 - val_loss: 3.2814e-06
Epoch 88/512
512/512 - 0s - loss: 3.2481e-04 - val_loss: 3.1482e-06
Epoch 89/512
512/512 - 0s - loss: 3.0706e-04 - val_loss: 3.0427e-06
Epoch 90/512
512/512 - 0s - loss: 3.0235e-04 - val_loss: 3.0267e-06
Epoch 91/512
512/512 - 0s - loss: 2.9969e-04 - val_loss: 2.9353e-06
Epoch 92/512
512/512 - 0s - loss: 2.8912e-04 - val_loss: 2.7950e-06
Epoch 93/512
512/512 - 0s - loss: 2.8049e-04 - val_loss: 2.7322e-06
Epoch 94/512
512/512 - 0s - loss: 2.7297e-04 - val_loss: 2.7140e-06
Epoch 95/512
512/512 - 0s - loss: 2.7215e-04 - val_loss: 2.6282e-06
Epoch 96/512
512/512 - 0s - loss: 2.6185e-04 - val_loss: 2.4995e-06
Epoch 97/512
512/512 - 0s - loss: 2.4870e-04 - val_loss: 2.5334e-06
Epoch 98/512
512/512 - 0s - loss: 2.5499e-04 - val_loss: 2.4853e-06
Epoch 99/512
512/512 - 0s - loss: 2.4415e-04 - val_loss: 2.3073e-06
Epoch 100/512
512/512 - 0s - loss: 2.3133e-04 - val_loss: 2.2716e-06
Epoch 101/512
512/512 - 0s - loss: 2.3104e-04 - val_loss: 2.2660e-06
Epoch 102/512
512/512 - 0s - loss: 2.2602e-04 - val_loss: 2.1977e-06
Epoch 103/512
512/512 - 0s - loss: 2.1632e-04 - val_loss: 2.1662e-06
Epoch 104/512
512/512 - 0s - loss: 2.1594e-04 - val_loss: 2.0941e-06
Epoch 105/512
512/512 - 0s - loss: 2.0844e-04 - val_loss: 1.9911e-06
Epoch 106/512
512/512 - 0s - loss: 1.9823e-04 - val_loss: 1.9813e-06
Epoch 107/512
512/512 - 0s - loss: 1.9911e-04 - val_loss: 1.9551e-06
Epoch 108/512
512/512 - 0s - loss: 1.9553e-04 - val_loss: 1.8037e-06
Epoch 109/512
512/512 - 0s - loss: 1.7917e-04 - val_loss: 1.8085e-06
Epoch 110/512
512/512 - 0s - loss: 1.8468e-04 - val_loss: 1.8274e-06
Epoch 111/512
512/512 - 0s - loss: 1.8132e-04 - val_loss: 1.6737e-06
Epoch 112/512
512/512 - 0s - loss: 1.6804e-04 - val_loss: 1.5859e-06
Epoch 113/512
512/512 - 0s - loss: 1.6422e-04 - val_loss: 1.6282e-06
Epoch 114/512
512/512 - 0s - loss: 1.6709e-04 - val_loss: 1.5864e-06
Epoch 115/512
512/512 - 0s - loss: 1.5737e-04 - val_loss: 1.4926e-06
Epoch 116/512
512/512 - 0s - loss: 1.5108e-04 - val_loss: 1.4875e-06
Epoch 117/512
512/512 - 0s - loss: 1.5094e-04 - val_loss: 1.4662e-06
Epoch 118/512
512/512 - 0s - loss: 1.4642e-04 - val_loss: 1.4023e-06
Epoch 119/512
512/512 - 0s - loss: 1.3991e-04 - val_loss: 1.3509e-06
Epoch 120/512
512/512 - 0s - loss: 1.3710e-04 - val_loss: 1.3247e-06
Epoch 121/512
512/512 - 0s - loss: 1.3366e-04 - val_loss: 1.2819e-06
Epoch 122/512
512/512 - 0s - loss: 1.3017e-04 - val_loss: 1.2249e-06
Epoch 123/512
512/512 - 0s - loss: 1.2350e-04 - val_loss: 1.2015e-06
Epoch 124/512
512/512 - 0s - loss: 1.2254e-04 - val_loss: 1.1870e-06
Epoch 125/512
512/512 - 0s - loss: 1.1964e-04 - val_loss: 1.1230e-06
Epoch 126/512
512/512 - 0s - loss: 1.1346e-04 - val_loss: 1.0833e-06
Epoch 127/512
512/512 - 0s - loss: 1.0997e-04 - val_loss: 1.0937e-06
Epoch 128/512
512/512 - 0s - loss: 1.1106e-04 - val_loss: 1.0432e-06
Epoch 129/512
512/512 - 0s - loss: 1.0367e-04 - val_loss: 9.8132e-07
Epoch 130/512
512/512 - 0s - loss: 1.0025e-04 - val_loss: 9.7522e-07
Epoch 131/512
512/512 - 0s - loss: 9.9414e-05 - val_loss: 9.5115e-07
Epoch 132/512
512/512 - 0s - loss: 9.5514e-05 - val_loss: 9.0908e-07
Epoch 133/512
512/512 - 0s - loss: 9.2243e-05 - val_loss: 8.8073e-07
Epoch 134/512
512/512 - 0s - loss: 8.9198e-05 - val_loss: 8.5875e-07
Epoch 135/512
512/512 - 0s - loss: 8.7090e-05 - val_loss: 8.3242e-07
Epoch 136/512
512/512 - 0s - loss: 8.4173e-05 - val_loss: 8.0091e-07
Epoch 137/512
512/512 - 0s - loss: 8.1267e-05 - val_loss: 7.6435e-07
Epoch 138/512
512/512 - 0s - loss: 7.8001e-05 - val_loss: 7.5252e-07
Epoch 139/512
512/512 - 0s - loss: 7.6760e-05 - val_loss: 7.3420e-07
Epoch 140/512
512/512 - 0s - loss: 7.3769e-05 - val_loss: 7.0418e-07
Epoch 141/512
512/512 - 0s - loss: 7.1409e-05 - val_loss: 6.6958e-07
Epoch 142/512
512/512 - 0s - loss: 6.8684e-05 - val_loss: 6.4237e-07
Epoch 143/512
512/512 - 0s - loss: 6.6064e-05 - val_loss: 6.3622e-07
Epoch 144/512
512/512 - 0s - loss: 6.5068e-05 - val_loss: 6.1930e-07
Epoch 145/512
512/512 - 0s - loss: 6.2556e-05 - val_loss: 5.8645e-07
Epoch 146/512
512/512 - 0s - loss: 5.9668e-05 - val_loss: 5.6262e-07
Epoch 147/512
512/512 - 0s - loss: 5.7866e-05 - val_loss: 5.5574e-07
Epoch 148/512
512/512 - 0s - loss: 5.6649e-05 - val_loss: 5.4095e-07
Epoch 149/512
512/512 - 0s - loss: 5.4716e-05 - val_loss: 5.0868e-07
Epoch 150/512
512/512 - 0s - loss: 5.1674e-05 - val_loss: 4.9185e-07
Epoch 151/512
512/512 - 0s - loss: 5.0840e-05 - val_loss: 4.7688e-07
Epoch 152/512
512/512 - 0s - loss: 4.8800e-05 - val_loss: 4.5790e-07
Epoch 153/512
512/512 - 0s - loss: 4.6822e-05 - val_loss: 4.4256e-07
Epoch 154/512
512/512 - 0s - loss: 4.5480e-05 - val_loss: 4.3088e-07
Epoch 155/512
512/512 - 0s - loss: 4.3956e-05 - val_loss: 4.1633e-07
Epoch 156/512
512/512 - 0s - loss: 4.2550e-05 - val_loss: 3.9199e-07
Epoch 157/512
512/512 - 0s - loss: 4.0156e-05 - val_loss: 3.7745e-07
Epoch 158/512
512/512 - 0s - loss: 3.9122e-05 - val_loss: 3.7396e-07
Epoch 159/512
512/512 - 0s - loss: 3.8409e-05 - val_loss: 3.5924e-07
Epoch 160/512
512/512 - 0s - loss: 3.6637e-05 - val_loss: 3.3294e-07
Epoch 161/512
512/512 - 0s - loss: 3.4243e-05 - val_loss: 3.2736e-07
Epoch 162/512
512/512 - 0s - loss: 3.4250e-05 - val_loss: 3.2148e-07
Epoch 163/512
512/512 - 0s - loss: 3.2882e-05 - val_loss: 3.0353e-07
Epoch 164/512
512/512 - 0s - loss: 3.0807e-05 - val_loss: 2.9417e-07
Epoch 165/512
512/512 - 0s - loss: 3.0524e-05 - val_loss: 2.8209e-07
Epoch 166/512
512/512 - 0s - loss: 2.9137e-05 - val_loss: 2.6864e-07
Epoch 167/512
512/512 - 0s - loss: 2.7616e-05 - val_loss: 2.5958e-07
Epoch 168/512
512/512 - 0s - loss: 2.6803e-05 - val_loss: 2.5788e-07
Epoch 169/512
512/512 - 0s - loss: 2.6110e-05 - val_loss: 2.4790e-07
Epoch 170/512
512/512 - 0s - loss: 2.5243e-05 - val_loss: 2.2810e-07
Epoch 171/512
512/512 - 0s - loss: 2.3304e-05 - val_loss: 2.2089e-07
Epoch 172/512
512/512 - 0s - loss: 2.3088e-05 - val_loss: 2.1823e-07
Epoch 173/512
512/512 - 0s - loss: 2.2475e-05 - val_loss: 2.0498e-07
Epoch 174/512
512/512 - 0s - loss: 2.1045e-05 - val_loss: 1.9375e-07
Epoch 175/512
512/512 - 0s - loss: 2.0201e-05 - val_loss: 1.8988e-07
Epoch 176/512
512/512 - 0s - loss: 1.9851e-05 - val_loss: 1.8161e-07
Epoch 177/512
512/512 - 0s - loss: 1.8584e-05 - val_loss: 1.7488e-07
Epoch 178/512
512/512 - 0s - loss: 1.8252e-05 - val_loss: 1.6651e-07
Epoch 179/512
512/512 - 0s - loss: 1.7092e-05 - val_loss: 1.6081e-07
Epoch 180/512
512/512 - 0s - loss: 1.6761e-05 - val_loss: 1.5733e-07
Epoch 181/512
512/512 - 0s - loss: 1.6067e-05 - val_loss: 1.4976e-07
Epoch 182/512
512/512 - 0s - loss: 1.5255e-05 - val_loss: 1.4300e-07
Epoch 183/512
512/512 - 0s - loss: 1.4767e-05 - val_loss: 1.3600e-07
Epoch 184/512
512/512 - 0s - loss: 1.4029e-05 - val_loss: 1.3052e-07
Epoch 185/512
512/512 - 0s - loss: 1.3568e-05 - val_loss: 1.2590e-07
Epoch 186/512
512/512 - 0s - loss: 1.2949e-05 - val_loss: 1.2061e-07
Epoch 187/512
512/512 - 0s - loss: 1.2504e-05 - val_loss: 1.1502e-07
Epoch 188/512
512/512 - 0s - loss: 1.1860e-05 - val_loss: 1.0971e-07
Epoch 189/512
512/512 - 0s - loss: 1.1383e-05 - val_loss: 1.0623e-07
Epoch 190/512
512/512 - 0s - loss: 1.1030e-05 - val_loss: 1.0122e-07
Epoch 191/512
512/512 - 0s - loss: 1.0471e-05 - val_loss: 9.6531e-08
Epoch 192/512
512/512 - 0s - loss: 9.9641e-06 - val_loss: 9.3054e-08
Epoch 193/512
512/512 - 0s - loss: 9.6394e-06 - val_loss: 9.0284e-08
Epoch 194/512
512/512 - 0s - loss: 9.3147e-06 - val_loss: 8.4882e-08
Epoch 195/512
512/512 - 0s - loss: 8.7744e-06 - val_loss: 7.9789e-08
Epoch 196/512
512/512 - 0s - loss: 8.3633e-06 - val_loss: 7.8288e-08
Epoch 197/512
512/512 - 0s - loss: 8.1155e-06 - val_loss: 7.5960e-08
Epoch 198/512
512/512 - 0s - loss: 7.8856e-06 - val_loss: 7.0060e-08
Epoch 199/512
512/512 - 0s - loss: 7.2571e-06 - val_loss: 6.6022e-08
Epoch 200/512
512/512 - 0s - loss: 7.0333e-06 - val_loss: 6.5467e-08
Epoch 201/512
512/512 - 0s - loss: 6.8271e-06 - val_loss: 6.3259e-08
Epoch 202/512
512/512 - 0s - loss: 6.5374e-06 - val_loss: 5.9081e-08
Epoch 203/512
512/512 - 0s - loss: 6.1299e-06 - val_loss: 5.6004e-08
Epoch 204/512
512/512 - 0s - loss: 5.8814e-06 - val_loss: 5.5350e-08
Epoch 205/512
512/512 - 0s - loss: 5.7902e-06 - val_loss: 5.1713e-08
Epoch 206/512
512/512 - 0s - loss: 5.3357e-06 - val_loss: 4.8348e-08
Epoch 207/512
512/512 - 0s - loss: 5.0890e-06 - val_loss: 4.8245e-08
Epoch 208/512
512/512 - 0s - loss: 5.0774e-06 - val_loss: 4.6099e-08
Epoch 209/512
512/512 - 0s - loss: 4.6841e-06 - val_loss: 4.2903e-08
Epoch 210/512
512/512 - 0s - loss: 4.4801e-06 - val_loss: 4.1001e-08
Epoch 211/512
512/512 - 0s - loss: 4.3143e-06 - val_loss: 3.9642e-08
Epoch 212/512
512/512 - 0s - loss: 4.1136e-06 - val_loss: 3.8024e-08
Epoch 213/512
512/512 - 0s - loss: 3.9648e-06 - val_loss: 3.6002e-08
Epoch 214/512
512/512 - 0s - loss: 3.7571e-06 - val_loss: 3.3879e-08
Epoch 215/512
512/512 - 0s - loss: 3.5563e-06 - val_loss: 3.2313e-08
Epoch 216/512
512/512 - 0s - loss: 3.3841e-06 - val_loss: 3.1871e-08
Epoch 217/512
512/512 - 0s - loss: 3.3450e-06 - val_loss: 2.9929e-08
Epoch 218/512
512/512 - 0s - loss: 3.0672e-06 - val_loss: 2.8152e-08
Epoch 219/512
512/512 - 0s - loss: 2.9701e-06 - val_loss: 2.7162e-08
Epoch 220/512
512/512 - 0s - loss: 2.8268e-06 - val_loss: 2.6377e-08
Epoch 221/512
512/512 - 0s - loss: 2.7503e-06 - val_loss: 2.4690e-08
Epoch 222/512
512/512 - 0s - loss: 2.5447e-06 - val_loss: 2.3268e-08
Epoch 223/512
512/512 - 0s - loss: 2.4529e-06 - val_loss: 2.2837e-08
Epoch 224/512
512/512 - 0s - loss: 2.3947e-06 - val_loss: 2.1286e-08
Epoch 225/512
512/512 - 0s - loss: 2.2056e-06 - val_loss: 1.9960e-08
Epoch 226/512
512/512 - 0s - loss: 2.1183e-06 - val_loss: 1.9762e-08
Epoch 227/512
512/512 - 0s - loss: 2.0767e-06 - val_loss: 1.8457e-08
Epoch 228/512
512/512 - 0s - loss: 1.9243e-06 - val_loss: 1.7133e-08
Epoch 229/512
512/512 - 0s - loss: 1.8124e-06 - val_loss: 1.7050e-08
Epoch 230/512
512/512 - 0s - loss: 1.7900e-06 - val_loss: 1.6586e-08
Epoch 231/512
512/512 - 0s - loss: 1.7148e-06 - val_loss: 1.4936e-08
Epoch 232/512
512/512 - 0s - loss: 1.5522e-06 - val_loss: 1.4088e-08
Epoch 233/512
512/512 - 0s - loss: 1.5147e-06 - val_loss: 1.4203e-08
Epoch 234/512
512/512 - 0s - loss: 1.4901e-06 - val_loss: 1.3414e-08
Epoch 235/512
512/512 - 0s - loss: 1.3791e-06 - val_loss: 1.2165e-08
Epoch 236/512
512/512 - 0s - loss: 1.2865e-06 - val_loss: 1.1908e-08
Epoch 237/512
512/512 - 0s - loss: 1.2730e-06 - val_loss: 1.1633e-08
Epoch 238/512
512/512 - 0s - loss: 1.2105e-06 - val_loss: 1.0675e-08
Epoch 239/512
512/512 - 0s - loss: 1.1158e-06 - val_loss: 1.0147e-08
Epoch 240/512
512/512 - 0s - loss: 1.0796e-06 - val_loss: 9.9952e-09
Epoch 241/512
512/512 - 0s - loss: 1.0449e-06 - val_loss: 9.4358e-09
Epoch 242/512
512/512 - 0s - loss: 9.8156e-07 - val_loss: 8.8397e-09
Epoch 243/512
512/512 - 0s - loss: 9.3228e-07 - val_loss: 8.3471e-09
Epoch 244/512
512/512 - 0s - loss: 8.8182e-07 - val_loss: 8.0369e-09
Epoch 245/512
512/512 - 0s - loss: 8.4894e-07 - val_loss: 7.7559e-09
Epoch 246/512
512/512 - 0s - loss: 8.1427e-07 - val_loss: 7.2153e-09
Epoch 247/512
512/512 - 0s - loss: 7.5236e-07 - val_loss: 6.9209e-09
Epoch 248/512
512/512 - 0s - loss: 7.2882e-07 - val_loss: 6.7966e-09
Epoch 249/512
512/512 - 0s - loss: 7.1441e-07 - val_loss: 6.2180e-09
Epoch 250/512
512/512 - 0s - loss: 6.4103e-07 - val_loss: 5.8505e-09
Epoch 251/512
512/512 - 0s - loss: 6.3106e-07 - val_loss: 5.6882e-09
Epoch 252/512
512/512 - 0s - loss: 5.9751e-07 - val_loss: 5.4123e-09
Epoch 253/512
512/512 - 0s - loss: 5.7263e-07 - val_loss: 5.0697e-09
Epoch 254/512
512/512 - 0s - loss: 5.2994e-07 - val_loss: 4.9190e-09
Epoch 255/512
512/512 - 0s - loss: 5.2049e-07 - val_loss: 4.6973e-09
Epoch 256/512
512/512 - 0s - loss: 4.9061e-07 - val_loss: 4.3288e-09
Epoch 257/512
512/512 - 0s - loss: 4.5780e-07 - val_loss: 4.1451e-09
Epoch 258/512
512/512 - 0s - loss: 4.3907e-07 - val_loss: 4.0681e-09
Epoch 259/512
512/512 - 0s - loss: 4.2811e-07 - val_loss: 3.8428e-09
Epoch 260/512
512/512 - 0s - loss: 3.9545e-07 - val_loss: 3.5851e-09
Epoch 261/512
512/512 - 0s - loss: 3.7779e-07 - val_loss: 3.4216e-09
Epoch 262/512
512/512 - 0s - loss: 3.6416e-07 - val_loss: 3.2470e-09
Epoch 263/512
512/512 - 0s - loss: 3.4140e-07 - val_loss: 3.0609e-09
Epoch 264/512
512/512 - 0s - loss: 3.2788e-07 - val_loss: 2.8595e-09
Epoch 265/512
512/512 - 0s - loss: 3.0311e-07 - val_loss: 2.7926e-09
Epoch 266/512
512/512 - 0s - loss: 2.9641e-07 - val_loss: 2.7450e-09
Epoch 267/512
512/512 - 0s - loss: 2.8733e-07 - val_loss: 2.4778e-09
Epoch 268/512
512/512 - 0s - loss: 2.5877e-07 - val_loss: 2.3215e-09
Epoch 269/512
512/512 - 0s - loss: 2.5095e-07 - val_loss: 2.2917e-09
Epoch 270/512
512/512 - 0s - loss: 2.4171e-07 - val_loss: 2.2130e-09
Epoch 271/512
512/512 - 0s - loss: 2.3067e-07 - val_loss: 2.0468e-09
Epoch 272/512
512/512 - 0s - loss: 2.1538e-07 - val_loss: 1.9208e-09
Epoch 273/512
512/512 - 0s - loss: 2.0383e-07 - val_loss: 1.8575e-09
Epoch 274/512
512/512 - 0s - loss: 1.9752e-07 - val_loss: 1.7844e-09
Epoch 275/512
512/512 - 0s - loss: 1.8748e-07 - val_loss: 1.6335e-09
Epoch 276/512
512/512 - 0s - loss: 1.7186e-07 - val_loss: 1.5817e-09
Epoch 277/512
512/512 - 0s - loss: 1.6940e-07 - val_loss: 1.5619e-09
Epoch 278/512
512/512 - 0s - loss: 1.6285e-07 - val_loss: 1.4388e-09
Epoch 279/512
512/512 - 0s - loss: 1.5014e-07 - val_loss: 1.3241e-09
Epoch 280/512
512/512 - 0s - loss: 1.4111e-07 - val_loss: 1.2954e-09
Epoch 281/512
512/512 - 0s - loss: 1.3824e-07 - val_loss: 1.2587e-09
Epoch 282/512
512/512 - 0s - loss: 1.3273e-07 - val_loss: 1.1345e-09
Epoch 283/512
512/512 - 0s - loss: 1.1916e-07 - val_loss: 1.0874e-09
Epoch 284/512
512/512 - 0s - loss: 1.1760e-07 - val_loss: 1.0796e-09
Epoch 285/512
512/512 - 0s - loss: 1.1285e-07 - val_loss: 1.0252e-09
Epoch 286/512
512/512 - 0s - loss: 1.0693e-07 - val_loss: 9.4348e-10
Epoch 287/512
512/512 - 0s - loss: 9.8732e-08 - val_loss: 8.9345e-10
Epoch 288/512
512/512 - 0s - loss: 9.5824e-08 - val_loss: 8.5801e-10
Epoch 289/512
512/512 - 0s - loss: 9.1402e-08 - val_loss: 8.1167e-10
Epoch 290/512
512/512 - 0s - loss: 8.6734e-08 - val_loss: 7.5877e-10
Epoch 291/512
512/512 - 0s - loss: 8.0221e-08 - val_loss: 7.3067e-10
Epoch 292/512
512/512 - 0s - loss: 7.8750e-08 - val_loss: 7.1080e-10
Epoch 293/512
512/512 - 0s - loss: 7.5062e-08 - val_loss: 6.4873e-10
Epoch 294/512
512/512 - 0s - loss: 6.9250e-08 - val_loss: 6.1886e-10
Epoch 295/512
512/512 - 0s - loss: 6.6768e-08 - val_loss: 6.0344e-10
Epoch 296/512
512/512 - 0s - loss: 6.4019e-08 - val_loss: 5.7117e-10
Epoch 297/512
512/512 - 0s - loss: 6.0683e-08 - val_loss: 5.3551e-10
Epoch 298/512
512/512 - 0s - loss: 5.6785e-08 - val_loss: 5.1499e-10
Epoch 299/512
512/512 - 0s - loss: 5.4589e-08 - val_loss: 5.0270e-10
Epoch 300/512
512/512 - 0s - loss: 5.2792e-08 - val_loss: 4.7353e-10
Epoch 301/512
512/512 - 0s - loss: 4.9481e-08 - val_loss: 4.4081e-10
Epoch 302/512
512/512 - 0s - loss: 4.6553e-08 - val_loss: 4.1850e-10
Epoch 303/512
512/512 - 0s - loss: 4.4327e-08 - val_loss: 4.0566e-10
Epoch 304/512
512/512 - 0s - loss: 4.3200e-08 - val_loss: 3.8281e-10
Epoch 305/512
512/512 - 0s - loss: 4.0015e-08 - val_loss: 3.6838e-10
Epoch 306/512
512/512 - 0s - loss: 3.8919e-08 - val_loss: 3.5146e-10
Epoch 307/512
512/512 - 0s - loss: 3.7022e-08 - val_loss: 3.2591e-10
Epoch 308/512
512/512 - 0s - loss: 3.4528e-08 - val_loss: 3.0501e-10
Epoch 309/512
512/512 - 0s - loss: 3.2549e-08 - val_loss: 3.0299e-10
Epoch 310/512
512/512 - 0s - loss: 3.2477e-08 - val_loss: 2.9076e-10
Epoch 311/512
512/512 - 0s - loss: 3.0593e-08 - val_loss: 2.6205e-10
Epoch 312/512
512/512 - 0s - loss: 2.7834e-08 - val_loss: 2.4923e-10
Epoch 313/512
512/512 - 0s - loss: 2.6966e-08 - val_loss: 2.5002e-10
Epoch 314/512
512/512 - 0s - loss: 2.6894e-08 - val_loss: 2.3642e-10
Epoch 315/512
512/512 - 0s - loss: 2.4890e-08 - val_loss: 2.1824e-10
Epoch 316/512
512/512 - 0s - loss: 2.3040e-08 - val_loss: 2.1203e-10
Epoch 317/512
512/512 - 0s - loss: 2.2771e-08 - val_loss: 2.0709e-10
Epoch 318/512
512/512 - 0s - loss: 2.2221e-08 - val_loss: 1.9137e-10
Epoch 319/512
512/512 - 0s - loss: 1.9929e-08 - val_loss: 1.7985e-10
Epoch 320/512
512/512 - 0s - loss: 1.9250e-08 - val_loss: 1.8057e-10
Epoch 321/512
512/512 - 0s - loss: 1.9207e-08 - val_loss: 1.7462e-10
Epoch 322/512
512/512 - 0s - loss: 1.8001e-08 - val_loss: 1.6112e-10
Epoch 323/512
512/512 - 0s - loss: 1.6895e-08 - val_loss: 1.5107e-10
Epoch 324/512
512/512 - 0s - loss: 1.6039e-08 - val_loss: 1.4723e-10
Epoch 325/512
512/512 - 0s - loss: 1.5730e-08 - val_loss: 1.4214e-10
Epoch 326/512
512/512 - 0s - loss: 1.5040e-08 - val_loss: 1.3348e-10
Epoch 327/512
512/512 - 0s - loss: 1.3969e-08 - val_loss: 1.2725e-10
Epoch 328/512
512/512 - 0s - loss: 1.3530e-08 - val_loss: 1.2513e-10
Epoch 329/512
512/512 - 0s - loss: 1.3289e-08 - val_loss: 1.1944e-10
Epoch 330/512
512/512 - 0s - loss: 1.2554e-08 - val_loss: 1.1045e-10
Epoch 331/512
512/512 - 0s - loss: 1.1612e-08 - val_loss: 1.0663e-10
Epoch 332/512
512/512 - 0s - loss: 1.1419e-08 - val_loss: 1.0467e-10
Epoch 333/512
512/512 - 0s - loss: 1.1075e-08 - val_loss: 1.0005e-10
Epoch 334/512
512/512 - 0s - loss: 1.0469e-08 - val_loss: 9.3945e-11
Epoch 335/512
512/512 - 0s - loss: 9.9914e-09 - val_loss: 8.9413e-11
Epoch 336/512
512/512 - 0s - loss: 9.3916e-09 - val_loss: 8.7615e-11
Epoch 337/512
512/512 - 0s - loss: 9.3888e-09 - val_loss: 8.5361e-11
Epoch 338/512
512/512 - 0s - loss: 8.9110e-09 - val_loss: 8.1937e-11
Epoch 339/512
512/512 - 0s - loss: 8.5868e-09 - val_loss: 7.7526e-11
Epoch 340/512
512/512 - 0s - loss: 8.1765e-09 - val_loss: 7.2597e-11
Epoch 341/512
512/512 - 0s - loss: 7.6547e-09 - val_loss: 7.0437e-11
Epoch 342/512
512/512 - 0s - loss: 7.4684e-09 - val_loss: 6.9121e-11
Epoch 343/512
512/512 - 0s - loss: 7.3144e-09 - val_loss: 6.6688e-11
Epoch 344/512
512/512 - 0s - loss: 6.9219e-09 - val_loss: 6.4483e-11
Epoch 345/512
512/512 - 0s - loss: 6.7820e-09 - val_loss: 6.0965e-11
Epoch 346/512
512/512 - 0s - loss: 6.4132e-09 - val_loss: 5.8016e-11
Epoch 347/512
512/512 - 0s - loss: 6.0423e-09 - val_loss: 5.6274e-11
Epoch 348/512
512/512 - 0s - loss: 5.9937e-09 - val_loss: 5.4949e-11
Epoch 349/512
512/512 - 0s - loss: 5.8128e-09 - val_loss: 5.2829e-11
Epoch 350/512
512/512 - 0s - loss: 5.4752e-09 - val_loss: 5.0514e-11
Epoch 351/512
512/512 - 0s - loss: 5.2735e-09 - val_loss: 4.9180e-11
Epoch 352/512
512/512 - 0s - loss: 5.1860e-09 - val_loss: 4.7136e-11
Epoch 353/512
512/512 - 0s - loss: 4.9168e-09 - val_loss: 4.5490e-11
Epoch 354/512
512/512 - 0s - loss: 4.7584e-09 - val_loss: 4.4514e-11
Epoch 355/512
512/512 - 0s - loss: 4.6323e-09 - val_loss: 4.2977e-11
Epoch 356/512
512/512 - 0s - loss: 4.5057e-09 - val_loss: 4.0713e-11
Epoch 357/512
512/512 - 0s - loss: 4.2430e-09 - val_loss: 3.9423e-11
Epoch 358/512
512/512 - 0s - loss: 4.1190e-09 - val_loss: 3.8545e-11
Epoch 359/512
512/512 - 0s - loss: 4.0308e-09 - val_loss: 3.7348e-11
Epoch 360/512
512/512 - 0s - loss: 3.8879e-09 - val_loss: 3.5726e-11
Epoch 361/512
512/512 - 0s - loss: 3.7256e-09 - val_loss: 3.4490e-11
Epoch 362/512
512/512 - 0s - loss: 3.6043e-09 - val_loss: 3.3872e-11
Epoch 363/512
512/512 - 0s - loss: 3.5107e-09 - val_loss: 3.2889e-11
Epoch 364/512
512/512 - 0s - loss: 3.4307e-09 - val_loss: 3.1710e-11
Epoch 365/512
512/512 - 0s - loss: 3.2920e-09 - val_loss: 3.0515e-11
Epoch 366/512
512/512 - 0s - loss: 3.1973e-09 - val_loss: 2.9308e-11
Epoch 367/512
512/512 - 0s - loss: 3.0180e-09 - val_loss: 2.8410e-11
Epoch 368/512
512/512 - 0s - loss: 2.9731e-09 - val_loss: 2.7656e-11
Epoch 369/512
512/512 - 0s - loss: 2.8713e-09 - val_loss: 2.6970e-11
Epoch 370/512
512/512 - 0s - loss: 2.8097e-09 - val_loss: 2.6240e-11
Epoch 371/512
512/512 - 0s - loss: 2.7509e-09 - val_loss: 2.5102e-11
Epoch 372/512
512/512 - 0s - loss: 2.6014e-09 - val_loss: 2.4659e-11
Epoch 373/512
512/512 - 0s - loss: 2.5323e-09 - val_loss: 2.4009e-11
Epoch 374/512
512/512 - 0s - loss: 2.4962e-09 - val_loss: 2.3339e-11
Epoch 375/512
512/512 - 0s - loss: 2.4211e-09 - val_loss: 2.2368e-11
Epoch 376/512
512/512 - 0s - loss: 2.3424e-09 - val_loss: 2.1882e-11
Epoch 377/512
512/512 - 0s - loss: 2.2905e-09 - val_loss: 2.1439e-11
Epoch 378/512
512/512 - 0s - loss: 2.2122e-09 - val_loss: 2.0928e-11
Epoch 379/512
512/512 - 0s - loss: 2.1848e-09 - val_loss: 2.0183e-11
Epoch 380/512
512/512 - 0s - loss: 2.0806e-09 - val_loss: 1.9333e-11
Epoch 381/512
512/512 - 0s - loss: 2.0129e-09 - val_loss: 1.9020e-11
Epoch 382/512
512/512 - 0s - loss: 1.9850e-09 - val_loss: 1.8568e-11
Epoch 383/512
512/512 - 0s - loss: 1.9292e-09 - val_loss: 1.8280e-11
Epoch 384/512
512/512 - 0s - loss: 1.9224e-09 - val_loss: 1.7376e-11
Epoch 385/512
512/512 - 0s - loss: 1.7951e-09 - val_loss: 1.6830e-11
Epoch 386/512
512/512 - 0s - loss: 1.7623e-09 - val_loss: 1.6954e-11
Epoch 387/512
512/512 - 0s - loss: 1.7671e-09 - val_loss: 1.6681e-11
Epoch 388/512
512/512 - 0s - loss: 1.7148e-09 - val_loss: 1.6198e-11
Epoch 389/512
512/512 - 0s - loss: 1.6673e-09 - val_loss: 1.5373e-11
Epoch 390/512
512/512 - 0s - loss: 1.5805e-09 - val_loss: 1.5006e-11
Epoch 391/512
512/512 - 0s - loss: 1.5492e-09 - val_loss: 1.4515e-11
Epoch 392/512
512/512 - 0s - loss: 1.5116e-09 - val_loss: 1.4294e-11
Epoch 393/512
512/512 - 0s - loss: 1.4834e-09 - val_loss: 1.4070e-11
Epoch 394/512
512/512 - 0s - loss: 1.4548e-09 - val_loss: 1.3697e-11
Epoch 395/512
512/512 - 0s - loss: 1.4146e-09 - val_loss: 1.3383e-11
Epoch 396/512
512/512 - 0s - loss: 1.3940e-09 - val_loss: 1.2976e-11
Epoch 397/512
512/512 - 0s - loss: 1.3416e-09 - val_loss: 1.2765e-11
Epoch 398/512
512/512 - 0s - loss: 1.3255e-09 - val_loss: 1.2549e-11
Epoch 399/512
512/512 - 0s - loss: 1.3091e-09 - val_loss: 1.2253e-11
Epoch 400/512
512/512 - 0s - loss: 1.2675e-09 - val_loss: 1.1851e-11
Epoch 401/512
512/512 - 0s - loss: 1.2119e-09 - val_loss: 1.1579e-11
Epoch 402/512
512/512 - 0s - loss: 1.1906e-09 - val_loss: 1.1379e-11
Epoch 403/512
512/512 - 0s - loss: 1.1764e-09 - val_loss: 1.1262e-11
Epoch 404/512
512/512 - 0s - loss: 1.1594e-09 - val_loss: 1.0970e-11
Epoch 405/512
512/512 - 0s - loss: 1.1371e-09 - val_loss: 1.0631e-11
Epoch 406/512
512/512 - 0s - loss: 1.0986e-09 - val_loss: 1.0250e-11
Epoch 407/512
512/512 - 0s - loss: 1.0389e-09 - val_loss: 1.0023e-11
Epoch 408/512
512/512 - 0s - loss: 1.0492e-09 - val_loss: 1.0055e-11
Epoch 409/512
512/512 - 0s - loss: 1.0410e-09 - val_loss: 9.8293e-12
Epoch 410/512
512/512 - 0s - loss: 1.0179e-09 - val_loss: 9.6271e-12
Epoch 411/512
512/512 - 0s - loss: 9.8517e-10 - val_loss: 9.3627e-12
Epoch 412/512
512/512 - 0s - loss: 9.5165e-10 - val_loss: 9.1841e-12
Epoch 413/512
512/512 - 0s - loss: 9.5348e-10 - val_loss: 9.1427e-12
Epoch 414/512
512/512 - 0s - loss: 9.4805e-10 - val_loss: 8.9055e-12
Epoch 415/512
512/512 - 0s - loss: 9.1483e-10 - val_loss: 8.6969e-12
Epoch 416/512
512/512 - 0s - loss: 8.9115e-10 - val_loss: 8.3274e-12
Epoch 417/512
512/512 - 0s - loss: 8.4498e-10 - val_loss: 8.1681e-12
Epoch 418/512
512/512 - 0s - loss: 8.4261e-10 - val_loss: 8.2244e-12
Epoch 419/512
512/512 - 0s - loss: 8.5669e-10 - val_loss: 8.1721e-12
Epoch 420/512
512/512 - 0s - loss: 8.4128e-10 - val_loss: 8.0292e-12
Epoch 421/512
512/512 - 0s - loss: 8.2503e-10 - val_loss: 7.7725e-12
Epoch 422/512
512/512 - 0s - loss: 7.8388e-10 - val_loss: 7.5274e-12
Epoch 423/512
512/512 - 0s - loss: 7.6389e-10 - val_loss: 7.4119e-12
Epoch 424/512
512/512 - 0s - loss: 7.6573e-10 - val_loss: 7.2429e-12
Epoch 425/512
512/512 - 0s - loss: 7.4070e-10 - val_loss: 7.1328e-12
Epoch 426/512
512/512 - 0s - loss: 7.3728e-10 - val_loss: 7.1329e-12
Epoch 427/512
512/512 - 0s - loss: 7.3488e-10 - val_loss: 6.9572e-12
Epoch 428/512
512/512 - 0s - loss: 7.0496e-10 - val_loss: 6.7209e-12
Epoch 429/512
512/512 - 0s - loss: 6.9842e-10 - val_loss: 6.6147e-12
Epoch 430/512
512/512 - 0s - loss: 6.8512e-10 - val_loss: 6.5742e-12
Epoch 431/512
512/512 - 0s - loss: 6.7799e-10 - val_loss: 6.4103e-12
Epoch 432/512
512/512 - 0s - loss: 6.5772e-10 - val_loss: 6.2972e-12
Epoch 433/512
512/512 - 0s - loss: 6.5705e-10 - val_loss: 6.2515e-12
Epoch 434/512
512/512 - 0s - loss: 6.4592e-10 - val_loss: 6.1260e-12
Epoch 435/512
512/512 - 0s - loss: 6.2313e-10 - val_loss: 6.0667e-12
Epoch 436/512
512/512 - 0s - loss: 6.1121e-10 - val_loss: 5.8220e-12
Epoch 437/512
512/512 - 0s - loss: 5.9738e-10 - val_loss: 5.8542e-12
Epoch 438/512
512/512 - 0s - loss: 5.9561e-10 - val_loss: 5.7267e-12
Epoch 439/512
512/512 - 0s - loss: 5.7476e-10 - val_loss: 5.4631e-12
Epoch 440/512
512/512 - 0s - loss: 5.6548e-10 - val_loss: 5.4996e-12
Epoch 441/512
512/512 - 0s - loss: 5.6549e-10 - val_loss: 5.4863e-12
Epoch 442/512
512/512 - 0s - loss: 5.6151e-10 - val_loss: 5.2990e-12
Epoch 443/512
512/512 - 0s - loss: 5.3810e-10 - val_loss: 5.1659e-12
Epoch 444/512
512/512 - 0s - loss: 5.2906e-10 - val_loss: 5.1083e-12
Epoch 445/512
512/512 - 0s - loss: 5.2669e-10 - val_loss: 5.0365e-12
Epoch 446/512
512/512 - 0s - loss: 5.1272e-10 - val_loss: 4.8850e-12
Epoch 447/512
512/512 - 0s - loss: 5.0485e-10 - val_loss: 4.9469e-12
Epoch 448/512
512/512 - 0s - loss: 5.0506e-10 - val_loss: 4.9342e-12
Epoch 449/512
512/512 - 0s - loss: 5.0255e-10 - val_loss: 4.8407e-12
Epoch 450/512
512/512 - 0s - loss: 4.9644e-10 - val_loss: 4.6694e-12
Epoch 451/512
512/512 - 0s - loss: 4.7889e-10 - val_loss: 4.5904e-12
Epoch 452/512
512/512 - 0s - loss: 4.6236e-10 - val_loss: 4.4721e-12
Epoch 453/512
512/512 - 0s - loss: 4.5886e-10 - val_loss: 4.4503e-12
Epoch 454/512
512/512 - 0s - loss: 4.5113e-10 - val_loss: 4.4205e-12
Epoch 455/512
512/512 - 0s - loss: 4.5046e-10 - val_loss: 4.4346e-12
Epoch 456/512
512/512 - 0s - loss: 4.5807e-10 - val_loss: 4.4291e-12
Epoch 457/512
512/512 - 0s - loss: 4.5193e-10 - val_loss: 4.2191e-12
Epoch 458/512
512/512 - 0s - loss: 4.2828e-10 - val_loss: 4.0524e-12
Epoch 459/512
512/512 - 0s - loss: 4.1272e-10 - val_loss: 3.9517e-12
Epoch 460/512
512/512 - 0s - loss: 4.0174e-10 - val_loss: 3.9628e-12
Epoch 461/512
512/512 - 0s - loss: 4.0562e-10 - val_loss: 4.0054e-12
Epoch 462/512
512/512 - 0s - loss: 4.1138e-10 - val_loss: 3.9754e-12
Epoch 463/512
512/512 - 0s - loss: 3.9969e-10 - val_loss: 3.8624e-12
Epoch 464/512
512/512 - 0s - loss: 3.9733e-10 - val_loss: 3.7952e-12
Epoch 465/512
512/512 - 0s - loss: 3.8952e-10 - val_loss: 3.7880e-12
Epoch 466/512
512/512 - 0s - loss: 3.8857e-10 - val_loss: 3.6917e-12
Epoch 467/512
512/512 - 0s - loss: 3.7576e-10 - val_loss: 3.5758e-12
Epoch 468/512
512/512 - 0s - loss: 3.6476e-10 - val_loss: 3.5354e-12
Epoch 469/512
512/512 - 0s - loss: 3.6305e-10 - val_loss: 3.5146e-12
Epoch 470/512
512/512 - 0s - loss: 3.5679e-10 - val_loss: 3.4847e-12
Epoch 471/512
512/512 - 0s - loss: 3.5921e-10 - val_loss: 3.3842e-12
Epoch 472/512
512/512 - 0s - loss: 3.5028e-10 - val_loss: 3.4390e-12
Epoch 473/512
512/512 - 0s - loss: 3.4834e-10 - val_loss: 3.3424e-12
Epoch 474/512
512/512 - 0s - loss: 3.3901e-10 - val_loss: 3.2919e-12
Epoch 475/512
512/512 - 0s - loss: 3.3663e-10 - val_loss: 3.1609e-12
Epoch 476/512
512/512 - 0s - loss: 3.2286e-10 - val_loss: 3.1189e-12
Epoch 477/512
512/512 - 0s - loss: 3.2333e-10 - val_loss: 3.1859e-12
Epoch 478/512
512/512 - 0s - loss: 3.2309e-10 - val_loss: 3.0968e-12
Epoch 479/512
512/512 - 0s - loss: 3.1495e-10 - val_loss: 3.0942e-12
Epoch 480/512
512/512 - 0s - loss: 3.1963e-10 - val_loss: 3.0696e-12
Epoch 481/512
512/512 - 0s - loss: 3.1082e-10 - val_loss: 3.0112e-12
Epoch 482/512
512/512 - 0s - loss: 3.0463e-10 - val_loss: 2.9361e-12
Epoch 483/512
512/512 - 0s - loss: 2.9862e-10 - val_loss: 2.9050e-12
Epoch 484/512
512/512 - 0s - loss: 2.9949e-10 - val_loss: 2.9229e-12
Epoch 485/512
512/512 - 0s - loss: 2.9750e-10 - val_loss: 2.8845e-12
Epoch 486/512
512/512 - 0s - loss: 2.9124e-10 - val_loss: 2.8519e-12
Epoch 487/512
512/512 - 0s - loss: 2.9063e-10 - val_loss: 2.8137e-12
Epoch 488/512
512/512 - 0s - loss: 2.8806e-10 - val_loss: 2.7520e-12
Epoch 489/512
512/512 - 0s - loss: 2.8193e-10 - val_loss: 2.8091e-12
Epoch 490/512
512/512 - 0s - loss: 2.8115e-10 - val_loss: 2.7352e-12
Epoch 491/512
512/512 - 0s - loss: 2.8181e-10 - val_loss: 2.6557e-12
Epoch 492/512
512/512 - 0s - loss: 2.7486e-10 - val_loss: 2.6490e-12
Epoch 493/512
512/512 - 0s - loss: 2.6712e-10 - val_loss: 2.6313e-12
Epoch 494/512
512/512 - 0s - loss: 2.6894e-10 - val_loss: 2.5648e-12
Epoch 495/512
512/512 - 0s - loss: 2.5923e-10 - val_loss: 2.5342e-12
Epoch 496/512
512/512 - 0s - loss: 2.5839e-10 - val_loss: 2.5279e-12
Epoch 497/512
512/512 - 0s - loss: 2.5987e-10 - val_loss: 2.5068e-12
Epoch 498/512
512/512 - 0s - loss: 2.5547e-10 - val_loss: 2.4699e-12
Epoch 499/512
512/512 - 0s - loss: 2.5178e-10 - val_loss: 2.3873e-12
Epoch 500/512
512/512 - 0s - loss: 2.4492e-10 - val_loss: 2.4199e-12
Epoch 501/512
512/512 - 0s - loss: 2.4491e-10 - val_loss: 2.3504e-12
Epoch 502/512
512/512 - 0s - loss: 2.4276e-10 - val_loss: 2.2903e-12
Epoch 503/512
512/512 - 0s - loss: 2.3524e-10 - val_loss: 2.3548e-12
Epoch 504/512
512/512 - 0s - loss: 2.3778e-10 - val_loss: 2.2577e-12
Epoch 505/512
512/512 - 0s - loss: 2.3139e-10 - val_loss: 2.2441e-12
Epoch 506/512
512/512 - 0s - loss: 2.2678e-10 - val_loss: 2.2600e-12
Epoch 507/512
512/512 - 0s - loss: 2.3064e-10 - val_loss: 2.2397e-12
Epoch 508/512
512/512 - 0s - loss: 2.3010e-10 - val_loss: 2.2735e-12
Epoch 509/512
512/512 - 0s - loss: 2.2900e-10 - val_loss: 2.1867e-12
Epoch 510/512
512/512 - 0s - loss: 2.1953e-10 - val_loss: 2.1307e-12
Epoch 511/512
512/512 - 0s - loss: 2.2002e-10 - val_loss: 2.1798e-12
Epoch 512/512
512/512 - 0s - loss: 2.2123e-10 - val_loss: 2.1122e-12
2024-04-11 17:32:53.961522: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7107e-10 - val_loss: 9.6227e-10
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2931e-09 - val_loss: 1.3968e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2447e-09 - val_loss: 8.1085e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6679e-10 - val_loss: 4.3712e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9184e-10 - val_loss: 3.1903e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1876e-10 - val_loss: 3.1966e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5648e-10 - val_loss: 4.1252e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7266e-10 - val_loss: 5.4373e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0300e-10 - val_loss: 6.3022e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5064e-10 - val_loss: 5.9693e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8958e-10 - val_loss: 5.2410e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1417e-10 - val_loss: 4.5776e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5120e-10 - val_loss: 4.2240e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3642e-10 - val_loss: 4.2953e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4972e-10 - val_loss: 4.5524e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7673e-10 - val_loss: 4.7406e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9031e-10 - val_loss: 4.7231e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8522e-10 - val_loss: 4.6092e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6587e-10 - val_loss: 4.4286e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4975e-10 - val_loss: 4.3253e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4015e-10 - val_loss: 4.1486e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2438e-10 - val_loss: 4.0591e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1703e-10 - val_loss: 4.0584e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1754e-10 - val_loss: 4.0539e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0997e-10 - val_loss: 4.0076e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1042e-10 - val_loss: 3.9480e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0389e-10 - val_loss: 3.9172e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0099e-10 - val_loss: 3.9244e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0164e-10 - val_loss: 3.8555e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9523e-10 - val_loss: 3.8545e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9467e-10 - val_loss: 3.8245e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8770e-10 - val_loss: 3.6626e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6758e-10 - val_loss: 3.4741e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5219e-10 - val_loss: 3.3983e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4990e-10 - val_loss: 3.3648e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4519e-10 - val_loss: 3.3361e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4398e-10 - val_loss: 3.4266e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5268e-10 - val_loss: 3.4619e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5416e-10 - val_loss: 3.4223e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4228e-10 - val_loss: 3.2580e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2948e-10 - val_loss: 3.1112e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1604e-10 - val_loss: 3.0802e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1972e-10 - val_loss: 3.1448e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2292e-10 - val_loss: 3.1211e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2089e-10 - val_loss: 3.1329e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1875e-10 - val_loss: 3.0820e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1030e-10 - val_loss: 2.9799e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0059e-10 - val_loss: 2.8770e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9170e-10 - val_loss: 2.7984e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8660e-10 - val_loss: 2.7919e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8331e-10 - val_loss: 2.7255e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8207e-10 - val_loss: 2.8141e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9352e-10 - val_loss: 2.8502e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9179e-10 - val_loss: 2.8162e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8421e-10 - val_loss: 2.7764e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7989e-10 - val_loss: 2.6505e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6742e-10 - val_loss: 2.6035e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6584e-10 - val_loss: 2.5894e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6592e-10 - val_loss: 2.6193e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6462e-10 - val_loss: 2.5795e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6407e-10 - val_loss: 2.5415e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6030e-10 - val_loss: 2.4799e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4947e-10 - val_loss: 2.4007e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4346e-10 - val_loss: 2.3743e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4501e-10 - val_loss: 2.3954e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4247e-10 - val_loss: 2.3648e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4113e-10 - val_loss: 2.3458e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3802e-10 - val_loss: 2.2620e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3007e-10 - val_loss: 2.2340e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2249e-10 - val_loss: 2.1491e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2250e-10 - val_loss: 2.2347e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2776e-10 - val_loss: 2.2313e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3084e-10 - val_loss: 2.2420e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2770e-10 - val_loss: 2.2359e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3173e-10 - val_loss: 2.2665e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2992e-10 - val_loss: 2.2173e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1690e-10 - val_loss: 2.0198e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0831e-10 - val_loss: 1.9998e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0082e-10 - val_loss: 1.9856e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0141e-10 - val_loss: 1.9621e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9919e-10 - val_loss: 1.9945e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0331e-10 - val_loss: 1.9973e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0654e-10 - val_loss: 2.0486e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0686e-10 - val_loss: 1.9727e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9759e-10 - val_loss: 1.8652e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9145e-10 - val_loss: 1.8581e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8994e-10 - val_loss: 1.8263e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8623e-10 - val_loss: 1.8263e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8601e-10 - val_loss: 1.7922e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8323e-10 - val_loss: 1.8335e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8952e-10 - val_loss: 1.8667e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9131e-10 - val_loss: 1.8674e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8736e-10 - val_loss: 1.7795e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8036e-10 - val_loss: 1.7735e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7917e-10 - val_loss: 1.7147e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7195e-10 - val_loss: 1.7106e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7822e-10 - val_loss: 1.7321e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7338e-10 - val_loss: 1.7027e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7590e-10 - val_loss: 1.7236e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7152e-10 - val_loss: 1.6248e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6532e-10 - val_loss: 1.6103e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6127e-10 - val_loss: 1.5547e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5654e-10 - val_loss: 1.4847e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5164e-10 - val_loss: 1.5284e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5835e-10 - val_loss: 1.5791e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6430e-10 - val_loss: 1.6297e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6644e-10 - val_loss: 1.6288e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6447e-10 - val_loss: 1.6254e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6322e-10 - val_loss: 1.5658e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5878e-10 - val_loss: 1.5337e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5323e-10 - val_loss: 1.4873e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5225e-10 - val_loss: 1.4994e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5153e-10 - val_loss: 1.4331e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4564e-10 - val_loss: 1.4459e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4748e-10 - val_loss: 1.4057e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3984e-10 - val_loss: 1.3532e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3927e-10 - val_loss: 1.4034e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4336e-10 - val_loss: 1.3675e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3861e-10 - val_loss: 1.3846e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4367e-10 - val_loss: 1.4234e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4093e-10 - val_loss: 1.3305e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3383e-10 - val_loss: 1.3055e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3409e-10 - val_loss: 1.3274e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3218e-10 - val_loss: 1.2586e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2763e-10 - val_loss: 1.2718e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2878e-10 - val_loss: 1.2623e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2927e-10 - val_loss: 1.2729e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2945e-10 - val_loss: 1.2980e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3255e-10 - val_loss: 1.2970e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3103e-10 - val_loss: 1.2841e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3020e-10 - val_loss: 1.3071e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3137e-10 - val_loss: 1.2686e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2725e-10 - val_loss: 1.2182e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2279e-10 - val_loss: 1.1479e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1322e-10 - val_loss: 1.0892e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1207e-10 - val_loss: 1.1302e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1647e-10 - val_loss: 1.1340e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1480e-10 - val_loss: 1.1378e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1876e-10 - val_loss: 1.2147e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2429e-10 - val_loss: 1.2231e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2060e-10 - val_loss: 1.1564e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1850e-10 - val_loss: 1.1739e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2082e-10 - val_loss: 1.2011e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1788e-10 - val_loss: 1.0867e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0779e-10 - val_loss: 1.0654e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0959e-10 - val_loss: 1.1154e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1222e-10 - val_loss: 1.0715e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0691e-10 - val_loss: 1.0320e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0647e-10 - val_loss: 1.1077e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1640e-10 - val_loss: 1.1647e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1498e-10 - val_loss: 1.0818e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1028e-10 - val_loss: 1.1131e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1450e-10 - val_loss: 1.1427e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1316e-10 - val_loss: 1.0568e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0501e-10 - val_loss: 1.0043e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0162e-10 - val_loss: 1.0244e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0252e-10 - val_loss: 9.8189e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7856e-11 - val_loss: 9.6227e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9244e-11 - val_loss: 9.7598e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0025e-10 - val_loss: 1.0037e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0179e-10 - val_loss: 9.9451e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0121e-10 - val_loss: 9.9764e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0103e-10 - val_loss: 1.0051e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0282e-10 - val_loss: 9.9483e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.9547e-11 - val_loss: 9.6028e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7546e-11 - val_loss: 9.6286e-11
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6292e-11 - val_loss: 9.0635e-11
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9253e-11 - val_loss: 8.6891e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9564e-11 - val_loss: 9.1576e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4789e-11 - val_loss: 9.5532e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6044e-11 - val_loss: 9.4083e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2241e-11 - val_loss: 8.9293e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2144e-11 - val_loss: 9.1638e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3237e-11 - val_loss: 9.1838e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4222e-11 - val_loss: 9.2566e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0692e-11 - val_loss: 8.3753e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5255e-11 - val_loss: 8.6540e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8971e-11 - val_loss: 8.9799e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2398e-11 - val_loss: 9.3266e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1126e-11 - val_loss: 8.4287e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4481e-11 - val_loss: 8.3246e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6865e-11 - val_loss: 8.7625e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0768e-11 - val_loss: 8.8641e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5193e-11 - val_loss: 7.9957e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9664e-11 - val_loss: 7.8249e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8577e-11 - val_loss: 7.9419e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3297e-11 - val_loss: 8.5229e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7600e-11 - val_loss: 8.5237e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4052e-11 - val_loss: 8.0176e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8944e-11 - val_loss: 7.6505e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9832e-11 - val_loss: 8.2775e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4677e-11 - val_loss: 8.6168e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4174e-11 - val_loss: 7.8348e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8036e-11 - val_loss: 7.6689e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7269e-11 - val_loss: 7.4258e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7840e-11 - val_loss: 8.1484e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3428e-11 - val_loss: 8.1083e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9950e-11 - val_loss: 7.6499e-11
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6101e-11 - val_loss: 7.2610e-11
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3528e-11 - val_loss: 7.2353e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4002e-11 - val_loss: 7.2790e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3677e-11 - val_loss: 7.1902e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2403e-11 - val_loss: 7.1267e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2638e-11 - val_loss: 7.1951e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3140e-11 - val_loss: 7.2953e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4949e-11 - val_loss: 7.7989e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1541e-11 - val_loss: 7.9420e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7754e-11 - val_loss: 7.3714e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4221e-11 - val_loss: 7.2605e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2937e-11 - val_loss: 7.0515e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6800e-11 - val_loss: 6.1375e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0440e-11 - val_loss: 5.7812e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8252e-11 - val_loss: 6.0351e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4513e-11 - val_loss: 6.8884e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2039e-11 - val_loss: 7.1955e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2605e-11 - val_loss: 7.0601e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9525e-11 - val_loss: 6.7933e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8769e-11 - val_loss: 6.7779e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8533e-11 - val_loss: 6.8674e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8399e-11 - val_loss: 6.7212e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8130e-11 - val_loss: 6.8125e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7827e-11 - val_loss: 6.3604e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2318e-11 - val_loss: 5.8576e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7721e-11 - val_loss: 5.8856e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2003e-11 - val_loss: 6.5548e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7400e-11 - val_loss: 6.8439e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1687e-11 - val_loss: 7.2063e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1928e-11 - val_loss: 6.6650e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4176e-11 - val_loss: 5.9139e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0551e-11 - val_loss: 6.2324e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3458e-11 - val_loss: 6.4008e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5512e-11 - val_loss: 6.5573e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5809e-11 - val_loss: 6.2868e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1083e-11 - val_loss: 5.7068e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6304e-11 - val_loss: 5.4618e-11
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5551e-11 - val_loss: 5.3443e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6117e-11 - val_loss: 6.0708e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2762e-11 - val_loss: 6.4395e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5487e-11 - val_loss: 6.5829e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3726e-11 - val_loss: 5.8615e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8639e-11 - val_loss: 5.6150e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5082e-11 - val_loss: 5.3802e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3945e-11 - val_loss: 5.5624e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8886e-11 - val_loss: 6.1344e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4024e-11 - val_loss: 6.6133e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6418e-11 - val_loss: 6.2512e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0588e-11 - val_loss: 5.5231e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4000e-11 - val_loss: 5.1395e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1647e-11 - val_loss: 4.9965e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7830e-11 - val_loss: 4.4100e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2779e-11 - val_loss: 4.0539e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1148e-11 - val_loss: 4.3214e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5917e-11 - val_loss: 4.7814e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9463e-11 - val_loss: 5.1820e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3887e-11 - val_loss: 5.5354e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5649e-11 - val_loss: 5.4718e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7108e-11 - val_loss: 5.9399e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1360e-11 - val_loss: 6.2247e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3226e-11 - val_loss: 6.2316e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9545e-11 - val_loss: 5.4563e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3370e-11 - val_loss: 5.1082e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1070e-11 - val_loss: 4.9672e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0999e-11 - val_loss: 5.1665e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0653e-11 - val_loss: 4.6086e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5069e-11 - val_loss: 4.2034e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0300e-11 - val_loss: 3.9448e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1938e-11 - val_loss: 4.5392e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7812e-11 - val_loss: 4.9593e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1645e-11 - val_loss: 5.2931e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3406e-11 - val_loss: 5.2718e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1937e-11 - val_loss: 4.9823e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9289e-11 - val_loss: 4.6832e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5703e-11 - val_loss: 4.3334e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2747e-11 - val_loss: 4.3686e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6436e-11 - val_loss: 4.8636e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0850e-11 - val_loss: 5.2140e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1483e-11 - val_loss: 5.0403e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0583e-11 - val_loss: 5.0441e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1668e-11 - val_loss: 5.0441e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0250e-11 - val_loss: 4.4931e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2448e-11 - val_loss: 3.8916e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8599e-11 - val_loss: 3.7394e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7922e-11 - val_loss: 3.9115e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2108e-11 - val_loss: 4.5437e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7377e-11 - val_loss: 4.8642e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9606e-11 - val_loss: 4.8995e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9806e-11 - val_loss: 5.0741e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1528e-11 - val_loss: 4.9299e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6899e-11 - val_loss: 4.2515e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1518e-11 - val_loss: 3.8497e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8785e-11 - val_loss: 3.8742e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9602e-11 - val_loss: 4.0573e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2234e-11 - val_loss: 4.3892e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5624e-11 - val_loss: 4.7118e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8498e-11 - val_loss: 4.8142e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9240e-11 - val_loss: 4.8838e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5843e-11 - val_loss: 4.1436e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0902e-11 - val_loss: 3.8016e-11
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7812e-11 - val_loss: 3.5897e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6712e-11 - val_loss: 3.7491e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7859e-11 - val_loss: 3.9898e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1291e-11 - val_loss: 4.2870e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5763e-11 - val_loss: 4.7736e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9237e-11 - val_loss: 4.8803e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7627e-11 - val_loss: 4.3673e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1829e-11 - val_loss: 3.7833e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7771e-11 - val_loss: 3.5891e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6114e-11 - val_loss: 3.6106e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6344e-11 - val_loss: 3.5662e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7614e-11 - val_loss: 4.0149e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2285e-11 - val_loss: 4.4359e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5848e-11 - val_loss: 4.6881e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6924e-11 - val_loss: 4.6127e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6426e-11 - val_loss: 4.2215e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0254e-11 - val_loss: 3.7072e-11
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6479e-11 - val_loss: 3.5557e-11
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5299e-11 - val_loss: 3.4951e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5344e-11 - val_loss: 3.3671e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3915e-11 - val_loss: 3.4905e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6458e-11 - val_loss: 3.9501e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2218e-11 - val_loss: 4.4414e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5124e-11 - val_loss: 4.5259e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4715e-11 - val_loss: 4.1245e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9584e-11 - val_loss: 3.6942e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6112e-11 - val_loss: 3.3800e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4426e-11 - val_loss: 3.4123e-11
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4519e-11 - val_loss: 3.3288e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3894e-11 - val_loss: 3.4242e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4938e-11 - val_loss: 3.4713e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6459e-11 - val_loss: 3.9288e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0978e-11 - val_loss: 4.2560e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3421e-11 - val_loss: 4.4523e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4920e-11 - val_loss: 4.0022e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8543e-11 - val_loss: 3.6442e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5852e-11 - val_loss: 3.4109e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3488e-11 - val_loss: 3.1992e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2670e-11 - val_loss: 3.3570e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3474e-11 - val_loss: 3.2768e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3653e-11 - val_loss: 3.4396e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3402e-11 - val_loss: 3.0236e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9127e-11 - val_loss: 2.7312e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7254e-11 - val_loss: 2.5476e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5471e-11 - val_loss: 2.5142e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4684e-11 - val_loss: 2.6208e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8063e-11 - val_loss: 3.1099e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2819e-11 - val_loss: 3.4008e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5154e-11 - val_loss: 3.6727e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6818e-11 - val_loss: 3.5778e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5467e-11 - val_loss: 3.4623e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4731e-11 - val_loss: 3.4436e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5017e-11 - val_loss: 3.4633e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6225e-11 - val_loss: 3.8505e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0044e-11 - val_loss: 4.1705e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2969e-11 - val_loss: 4.3906e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3852e-11 - val_loss: 3.9771e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8155e-11 - val_loss: 3.5745e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5095e-11 - val_loss: 3.3599e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2608e-11 - val_loss: 3.1283e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1353e-11 - val_loss: 2.9980e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0851e-11 - val_loss: 3.2059e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2971e-11 - val_loss: 3.3230e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2840e-11 - val_loss: 3.1888e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0501e-11 - val_loss: 2.7809e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7002e-11 - val_loss: 2.5422e-11
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5427e-11 - val_loss: 2.4858e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4335e-11 - val_loss: 2.3427e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4657e-11 - val_loss: 2.7306e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9315e-11 - val_loss: 3.1670e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2860e-11 - val_loss: 3.2611e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2758e-11 - val_loss: 3.2637e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3694e-11 - val_loss: 3.4504e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4005e-11 - val_loss: 3.2209e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1427e-11 - val_loss: 3.0905e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1735e-11 - val_loss: 3.2665e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4997e-11 - val_loss: 3.7760e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9267e-11 - val_loss: 4.0042e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1269e-11 - val_loss: 4.1983e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9872e-11 - val_loss: 3.5019e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4648e-11 - val_loss: 3.2593e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2908e-11 - val_loss: 3.1752e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1196e-11 - val_loss: 3.0779e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1629e-11 - val_loss: 3.1590e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1327e-11 - val_loss: 3.1035e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1258e-11 - val_loss: 3.1187e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1017e-11 - val_loss: 2.8975e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7945e-11 - val_loss: 2.6023e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6225e-11 - val_loss: 2.5317e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4433e-11 - val_loss: 2.4065e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4647e-11 - val_loss: 2.5003e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4944e-11 - val_loss: 2.5214e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6646e-11 - val_loss: 2.8088e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9478e-11 - val_loss: 3.0729e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1291e-11 - val_loss: 3.2340e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3558e-11 - val_loss: 3.4127e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3809e-11 - val_loss: 3.2834e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3034e-11 - val_loss: 3.2679e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1746e-11 - val_loss: 3.1103e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1673e-11 - val_loss: 3.1803e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9672e-11 - val_loss: 2.6514e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5667e-11 - val_loss: 2.4564e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4173e-11 - val_loss: 2.3151e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3497e-11 - val_loss: 2.3873e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4730e-11 - val_loss: 2.5680e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7353e-11 - val_loss: 2.9818e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1482e-11 - val_loss: 3.2602e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3000e-11 - val_loss: 3.3921e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4562e-11 - val_loss: 3.2650e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2277e-11 - val_loss: 3.0494e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0100e-11 - val_loss: 2.9347e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8414e-11 - val_loss: 2.5689e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4309e-11 - val_loss: 2.2479e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2091e-11 - val_loss: 2.0449e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0377e-11 - val_loss: 2.0052e-11
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0158e-11 - val_loss: 1.9617e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0009e-11 - val_loss: 2.0489e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1186e-11 - val_loss: 2.1389e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2094e-11 - val_loss: 2.2871e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3310e-11 - val_loss: 2.4356e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6244e-11 - val_loss: 2.8136e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9137e-11 - val_loss: 2.9992e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0246e-11 - val_loss: 3.0010e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9796e-11 - val_loss: 2.8909e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9250e-11 - val_loss: 2.8791e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8316e-11 - val_loss: 2.8080e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8940e-11 - val_loss: 2.9165e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7184e-11 - val_loss: 2.3969e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2974e-11 - val_loss: 2.1980e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2350e-11 - val_loss: 2.2309e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1948e-11 - val_loss: 2.1281e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1093e-11 - val_loss: 2.0354e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9818e-11 - val_loss: 1.8987e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9505e-11 - val_loss: 2.0051e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0379e-11 - val_loss: 2.0687e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0328e-11 - val_loss: 2.0245e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0875e-11 - val_loss: 2.2584e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4295e-11 - val_loss: 2.6416e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7784e-11 - val_loss: 2.9255e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0660e-11 - val_loss: 3.1136e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0366e-11 - val_loss: 2.9312e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9289e-11 - val_loss: 2.7538e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5567e-11 - val_loss: 2.2705e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2411e-11 - val_loss: 2.1224e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0717e-11 - val_loss: 2.0639e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0411e-11 - val_loss: 1.9942e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9639e-11 - val_loss: 1.9057e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9094e-11 - val_loss: 1.9370e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0113e-11 - val_loss: 2.0951e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1436e-11 - val_loss: 2.2054e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2671e-11 - val_loss: 2.3965e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5647e-11 - val_loss: 2.6780e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7525e-11 - val_loss: 2.8474e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9079e-11 - val_loss: 2.8268e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7935e-11 - val_loss: 2.7554e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7963e-11 - val_loss: 2.6665e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6253e-11 - val_loss: 2.4175e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3182e-11 - val_loss: 2.1593e-11
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0357e-11 - val_loss: 1.8547e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8952e-11 - val_loss: 1.9393e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9065e-11 - val_loss: 1.8266e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8326e-11 - val_loss: 1.8348e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8505e-11 - val_loss: 1.9128e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9773e-11 - val_loss: 2.0414e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0326e-11 - val_loss: 1.9360e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9087e-11 - val_loss: 1.9212e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9803e-11 - val_loss: 2.0375e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0414e-11 - val_loss: 1.9387e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9258e-11 - val_loss: 1.8832e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9291e-11 - val_loss: 2.1607e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3395e-11 - val_loss: 2.4999e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5808e-11 - val_loss: 2.6747e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7301e-11 - val_loss: 2.7340e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7099e-11 - val_loss: 2.5256e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3358e-11 - val_loss: 2.0630e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9986e-11 - val_loss: 1.9516e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9687e-11 - val_loss: 1.8661e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8627e-11 - val_loss: 1.8221e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8503e-11 - val_loss: 1.8800e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9183e-11 - val_loss: 1.8933e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8955e-11 - val_loss: 1.8666e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8822e-11 - val_loss: 1.8704e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9011e-11 - val_loss: 1.9624e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9619e-11 - val_loss: 1.9073e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8998e-11 - val_loss: 1.8005e-11
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8160e-11 - val_loss: 1.7517e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7346e-11 - val_loss: 1.7715e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8253e-11 - val_loss: 1.8608e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9601e-11 - val_loss: 2.1324e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2508e-11 - val_loss: 2.3001e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3353e-11 - val_loss: 2.4167e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4831e-11 - val_loss: 2.4795e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3550e-11 - val_loss: 2.1284e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0756e-11 - val_loss: 1.9688e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9115e-11 - val_loss: 1.8301e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8432e-11 - val_loss: 1.8311e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7817e-11 - val_loss: 1.6922e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7363e-11 - val_loss: 1.8289e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8728e-11 - val_loss: 1.8810e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8659e-11 - val_loss: 1.7356e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7384e-11 - val_loss: 1.7511e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7859e-11 - val_loss: 1.8256e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8628e-11 - val_loss: 1.8510e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9094e-11 - val_loss: 1.9596e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9741e-11 - val_loss: 1.8978e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8897e-11 - val_loss: 1.8522e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8293e-11 - val_loss: 1.6263e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5534e-11 - val_loss: 1.4231e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3683e-11 - val_loss: 1.2959e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3018e-11 - val_loss: 1.3017e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2961e-11 - val_loss: 1.2976e-11
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2636e-11 - val_loss: 1.2032e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2004e-11 - val_loss: 1.1382e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1379e-11 - val_loss: 1.1247e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 129.2301 - val_loss: 0.0836
Epoch 2/512
512/512 - 0s - loss: 0.0589 - val_loss: 0.0675
Epoch 3/512
512/512 - 0s - loss: 0.0541 - val_loss: 0.0550
Epoch 4/512
512/512 - 0s - loss: 0.0500 - val_loss: 0.0463
Epoch 5/512
512/512 - 0s - loss: 0.0415 - val_loss: 0.0321
Epoch 6/512
512/512 - 0s - loss: 0.0493 - val_loss: 0.0340
Epoch 7/512
512/512 - 0s - loss: 0.0412 - val_loss: 0.0334
Epoch 8/512
512/512 - 0s - loss: 0.0347 - val_loss: 0.0301
Epoch 9/512
512/512 - 0s - loss: 0.0278 - val_loss: 0.0273
Epoch 10/512
512/512 - 0s - loss: 0.0250 - val_loss: 0.0256
Epoch 11/512
512/512 - 0s - loss: 0.0236 - val_loss: 0.0246
Epoch 12/512
512/512 - 0s - loss: 0.0233 - val_loss: 0.0231
Epoch 13/512
512/512 - 0s - loss: 0.0218 - val_loss: 0.0219
Epoch 14/512
512/512 - 0s - loss: 0.0212 - val_loss: 0.0206
Epoch 15/512
512/512 - 0s - loss: 0.0224 - val_loss: 0.0197
Epoch 16/512
512/512 - 0s - loss: 0.0210 - val_loss: 0.0189
Epoch 17/512
512/512 - 0s - loss: 0.0193 - val_loss: 0.0176
Epoch 18/512
512/512 - 0s - loss: 0.0214 - val_loss: 0.0165
Epoch 19/512
512/512 - 0s - loss: 0.0169 - val_loss: 0.0140
Epoch 20/512
512/512 - 0s - loss: 0.0122 - val_loss: 0.0020
Epoch 21/512
512/512 - 0s - loss: 0.0465 - val_loss: 0.0190
Epoch 22/512
512/512 - 0s - loss: 0.0192 - val_loss: 0.0171
Epoch 23/512
512/512 - 0s - loss: 0.0166 - val_loss: 0.0150
Epoch 24/512
512/512 - 0s - loss: 0.0128 - val_loss: 0.0133
Epoch 25/512
512/512 - 0s - loss: 0.0113 - val_loss: 0.0117
Epoch 26/512
512/512 - 0s - loss: 0.0163 - val_loss: 0.0104
Epoch 27/512
512/512 - 0s - loss: 0.0102 - val_loss: 0.0092
Epoch 28/512
512/512 - 0s - loss: 0.0088 - val_loss: 0.0081
Epoch 29/512
512/512 - 0s - loss: 0.0085 - val_loss: 0.0071
Epoch 30/512
512/512 - 0s - loss: 0.0076 - val_loss: 0.0063
Epoch 31/512
512/512 - 0s - loss: 0.0079 - val_loss: 0.0055
Epoch 32/512
512/512 - 0s - loss: 0.0069 - val_loss: 0.0049
Epoch 33/512
512/512 - 0s - loss: 0.0053 - val_loss: 0.0041
Epoch 34/512
512/512 - 0s - loss: 0.0046 - val_loss: 0.0035
Epoch 35/512
512/512 - 0s - loss: 0.0059 - val_loss: 0.0030
Epoch 36/512
512/512 - 0s - loss: 0.0051 - val_loss: 0.0026
Epoch 37/512
512/512 - 0s - loss: 0.0040 - val_loss: 0.0022
Epoch 38/512
512/512 - 0s - loss: 0.0037 - val_loss: 0.0018
Epoch 39/512
512/512 - 0s - loss: 0.0043 - val_loss: 0.0016
Epoch 40/512
512/512 - 0s - loss: 0.0074 - val_loss: 8.7816e-04
Epoch 41/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.2674e-04
Epoch 42/512
512/512 - 0s - loss: 0.0013 - val_loss: 4.1308e-04
Epoch 43/512
512/512 - 0s - loss: 0.0014 - val_loss: 6.0157e-04
Epoch 44/512
512/512 - 0s - loss: 0.0077 - val_loss: 0.0062
Epoch 45/512
512/512 - 0s - loss: 0.0066 - val_loss: 0.0019
Epoch 46/512
512/512 - 0s - loss: 0.0021 - val_loss: 9.0425e-04
Epoch 47/512
512/512 - 0s - loss: 0.0024 - val_loss: 6.0511e-04
Epoch 48/512
512/512 - 0s - loss: 0.0046 - val_loss: 3.8088e-04
Epoch 49/512
512/512 - 0s - loss: 0.0032 - val_loss: 2.8363e-04
Epoch 50/512
512/512 - 0s - loss: 0.0027 - val_loss: 2.3477e-04
Epoch 51/512
512/512 - 0s - loss: 0.0033 - val_loss: 2.0092e-04
Epoch 52/512
512/512 - 0s - loss: 0.0033 - val_loss: 1.7026e-04
Epoch 53/512
512/512 - 0s - loss: 0.0030 - val_loss: 1.5696e-04
Epoch 54/512
512/512 - 0s - loss: 0.0032 - val_loss: 1.3751e-04
Epoch 55/512
512/512 - 0s - loss: 0.0031 - val_loss: 7.0530e-05
Epoch 56/512
512/512 - 0s - loss: 0.0031 - val_loss: 9.9206e-05
Epoch 57/512
512/512 - 0s - loss: 0.0036 - val_loss: 4.0668e-04
Epoch 58/512
512/512 - 0s - loss: 0.0041 - val_loss: 1.1598e-04
Epoch 59/512
512/512 - 0s - loss: 0.0021 - val_loss: 8.4931e-05
Epoch 60/512
512/512 - 0s - loss: 0.0026 - val_loss: 1.4854e-04
Epoch 61/512
512/512 - 0s - loss: 0.0037 - val_loss: 9.3335e-05
Epoch 62/512
512/512 - 0s - loss: 0.0027 - val_loss: 9.8499e-05
Epoch 63/512
512/512 - 0s - loss: 0.0025 - val_loss: 1.1105e-04
Epoch 64/512
512/512 - 0s - loss: 0.0028 - val_loss: 9.1926e-05
Epoch 65/512
512/512 - 0s - loss: 0.0026 - val_loss: 9.8175e-05
Epoch 66/512
512/512 - 0s - loss: 0.0024 - val_loss: 1.1195e-04
Epoch 67/512
512/512 - 0s - loss: 0.0025 - val_loss: 8.2940e-05
Epoch 68/512
512/512 - 0s - loss: 0.0024 - val_loss: 1.1380e-04
Epoch 69/512
512/512 - 0s - loss: 0.0023 - val_loss: 1.7411e-04
Epoch 70/512
512/512 - 0s - loss: 0.0022 - val_loss: 5.5835e-05
Epoch 71/512
512/512 - 0s - loss: 0.0023 - val_loss: 2.3030e-05
Epoch 72/512
512/512 - 0s - loss: 0.0022 - val_loss: 4.5775e-05
Epoch 73/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.8766e-04
Epoch 74/512
512/512 - 0s - loss: 0.0023 - val_loss: 2.2931e-05
Epoch 75/512
512/512 - 0s - loss: 0.0019 - val_loss: 2.2457e-05
Epoch 76/512
512/512 - 0s - loss: 0.0020 - val_loss: 8.5024e-05
Epoch 77/512
512/512 - 0s - loss: 0.0026 - val_loss: 1.3111e-04
Epoch 78/512
512/512 - 0s - loss: 0.0014 - val_loss: 3.8914e-05
Epoch 79/512
512/512 - 0s - loss: 0.0019 - val_loss: 3.9130e-05
Epoch 80/512
512/512 - 0s - loss: 0.0024 - val_loss: 6.2447e-05
Epoch 81/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.5153e-05
Epoch 82/512
512/512 - 0s - loss: 0.0018 - val_loss: 9.2053e-05
Epoch 83/512
512/512 - 0s - loss: 0.0021 - val_loss: 4.0827e-05
Epoch 84/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.6633e-05
Epoch 85/512
512/512 - 0s - loss: 0.0018 - val_loss: 5.9457e-05
Epoch 86/512
512/512 - 0s - loss: 0.0017 - val_loss: 3.8004e-05
Epoch 87/512
512/512 - 0s - loss: 0.0021 - val_loss: 2.0990e-05
Epoch 88/512
512/512 - 0s - loss: 0.0021 - val_loss: 3.2827e-05
Epoch 89/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.0794e-05
Epoch 90/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.6203e-05
Epoch 91/512
512/512 - 0s - loss: 8.1715e-04 - val_loss: 5.9308e-05
Epoch 92/512
512/512 - 0s - loss: 0.0031 - val_loss: 1.3371e-04
Epoch 93/512
512/512 - 0s - loss: 0.0021 - val_loss: 3.6003e-05
Epoch 94/512
512/512 - 0s - loss: 0.0011 - val_loss: 5.1445e-05
Epoch 95/512
512/512 - 0s - loss: 0.0018 - val_loss: 2.1624e-04
Epoch 96/512
512/512 - 0s - loss: 0.0019 - val_loss: 5.5356e-05
Epoch 97/512
512/512 - 0s - loss: 0.0017 - val_loss: 3.3601e-05
Epoch 98/512
512/512 - 0s - loss: 0.0014 - val_loss: 2.9112e-05
Epoch 99/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.9465e-05
Epoch 100/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.0801e-05
Epoch 101/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.7782e-05
Epoch 102/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.3873e-05
Epoch 103/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.4889e-05
Epoch 104/512
512/512 - 0s - loss: 0.0022 - val_loss: 8.3725e-05
Epoch 105/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.5677e-05
Epoch 106/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.2143e-05
Epoch 107/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.3473e-05
Epoch 108/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.1521e-05
Epoch 109/512
512/512 - 0s - loss: 0.0015 - val_loss: 6.6273e-05
Epoch 110/512
512/512 - 0s - loss: 0.0016 - val_loss: 7.1339e-05
Epoch 111/512
512/512 - 0s - loss: 0.0016 - val_loss: 4.3877e-05
Epoch 112/512
512/512 - 0s - loss: 0.0016 - val_loss: 5.1169e-05
Epoch 113/512
512/512 - 0s - loss: 0.0018 - val_loss: 2.6341e-05
Epoch 114/512
512/512 - 0s - loss: 0.0013 - val_loss: 6.9571e-05
Epoch 115/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.4483e-05
Epoch 116/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.8684e-05
Epoch 117/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.8403e-05
Epoch 118/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.0985e-04
Epoch 119/512
512/512 - 0s - loss: 0.0013 - val_loss: 3.7933e-05
Epoch 120/512
512/512 - 0s - loss: 0.0024 - val_loss: 8.9901e-06
Epoch 121/512
512/512 - 0s - loss: 8.6947e-04 - val_loss: 9.7768e-06
Epoch 122/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.3807e-05
Epoch 123/512
512/512 - 0s - loss: 0.0018 - val_loss: 2.2197e-05
Epoch 124/512
512/512 - 0s - loss: 0.0012 - val_loss: 3.4163e-05
Epoch 125/512
512/512 - 0s - loss: 0.0015 - val_loss: 7.8949e-06
Epoch 126/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.2122e-05
Epoch 127/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.5402e-05
Epoch 128/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.2017e-05
Epoch 129/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.4819e-05
Epoch 130/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.3335e-05
Epoch 131/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.5468e-05
Epoch 132/512
512/512 - 0s - loss: 0.0013 - val_loss: 3.3971e-05
Epoch 133/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.5017e-05
Epoch 134/512
512/512 - 0s - loss: 0.0014 - val_loss: 3.3658e-05
Epoch 135/512
512/512 - 0s - loss: 0.0015 - val_loss: 5.9946e-06
Epoch 136/512
512/512 - 0s - loss: 5.7288e-04 - val_loss: 1.0319e-05
Epoch 137/512
512/512 - 0s - loss: 0.0021 - val_loss: 1.9614e-05
Epoch 138/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.2135e-05
Epoch 139/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.5941e-05
Epoch 140/512
512/512 - 0s - loss: 8.6153e-04 - val_loss: 1.7655e-05
Epoch 141/512
512/512 - 0s - loss: 5.9435e-04 - val_loss: 1.2054e-05
Epoch 142/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.7508e-05
Epoch 143/512
512/512 - 0s - loss: 0.0026 - val_loss: 1.5811e-04
Epoch 144/512
512/512 - 0s - loss: 7.7164e-04 - val_loss: 2.2746e-05
Epoch 145/512
512/512 - 0s - loss: 5.2947e-04 - val_loss: 1.1870e-05
Epoch 146/512
512/512 - 0s - loss: 0.0013 - val_loss: 3.6005e-05
Epoch 147/512
512/512 - 0s - loss: 0.0018 - val_loss: 1.6651e-05
Epoch 148/512
512/512 - 0s - loss: 7.8841e-04 - val_loss: 1.3752e-05
Epoch 149/512
512/512 - 0s - loss: 0.0015 - val_loss: 3.8744e-05
Epoch 150/512
512/512 - 0s - loss: 0.0022 - val_loss: 1.4580e-04
Epoch 151/512
512/512 - 0s - loss: 6.7966e-04 - val_loss: 2.7486e-05
Epoch 152/512
512/512 - 0s - loss: 0.0010 - val_loss: 4.8217e-04
Epoch 153/512
512/512 - 0s - loss: 0.0019 - val_loss: 5.9807e-05
Epoch 154/512
512/512 - 0s - loss: 5.9484e-04 - val_loss: 4.0480e-05
Epoch 155/512
512/512 - 0s - loss: 5.2173e-04 - val_loss: 3.9341e-05
Epoch 156/512
512/512 - 0s - loss: 0.0025 - val_loss: 2.1710e-04
Epoch 157/512
512/512 - 0s - loss: 9.6284e-04 - val_loss: 1.1160e-04
Epoch 158/512
512/512 - 0s - loss: 0.0012 - val_loss: 3.4943e-05
Epoch 159/512
512/512 - 0s - loss: 8.0592e-04 - val_loss: 3.0901e-05
Epoch 160/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.8550e-05
Epoch 161/512
512/512 - 0s - loss: 8.2178e-04 - val_loss: 4.0146e-05
Epoch 162/512
512/512 - 0s - loss: 0.0016 - val_loss: 4.9955e-05
Epoch 163/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.4189e-05
Epoch 164/512
512/512 - 0s - loss: 9.3879e-04 - val_loss: 2.3652e-05
Epoch 165/512
512/512 - 0s - loss: 0.0018 - val_loss: 2.3380e-05
Epoch 166/512
512/512 - 0s - loss: 0.0012 - val_loss: 3.4749e-05
Epoch 167/512
512/512 - 0s - loss: 8.3860e-04 - val_loss: 9.4211e-05
Epoch 168/512
512/512 - 0s - loss: 5.8926e-04 - val_loss: 2.9455e-05
Epoch 169/512
512/512 - 0s - loss: 0.0016 - val_loss: 6.0059e-05
Epoch 170/512
512/512 - 0s - loss: 0.0022 - val_loss: 2.0228e-05
Epoch 171/512
512/512 - 0s - loss: 6.5872e-04 - val_loss: 1.4559e-05
Epoch 172/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.0626e-05
Epoch 173/512
512/512 - 0s - loss: 0.0018 - val_loss: 1.8957e-05
Epoch 174/512
512/512 - 0s - loss: 0.0016 - val_loss: 7.3644e-05
Epoch 175/512
512/512 - 0s - loss: 8.8430e-04 - val_loss: 4.0118e-05
Epoch 176/512
512/512 - 0s - loss: 7.3612e-04 - val_loss: 3.8023e-05
Epoch 177/512
512/512 - 0s - loss: 0.0012 - val_loss: 3.6989e-05
Epoch 178/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.1015e-04
Epoch 179/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.1635e-05
Epoch 180/512
512/512 - 0s - loss: 5.8304e-04 - val_loss: 3.6064e-05
Epoch 181/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.6462e-04
Epoch 182/512
512/512 - 0s - loss: 6.7775e-04 - val_loss: 5.8879e-05
Epoch 183/512
512/512 - 0s - loss: 8.0822e-04 - val_loss: 8.0335e-05
Epoch 184/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.9013e-04
Epoch 185/512
512/512 - 0s - loss: 9.9842e-04 - val_loss: 1.2401e-04
Epoch 186/512
512/512 - 0s - loss: 9.1603e-04 - val_loss: 1.0736e-04
Epoch 187/512
512/512 - 0s - loss: 0.0030 - val_loss: 6.1670e-04
Epoch 188/512
512/512 - 0s - loss: 0.0013 - val_loss: 2.4323e-04
Epoch 189/512
512/512 - 0s - loss: 9.4609e-04 - val_loss: 2.3016e-04
Epoch 190/512
512/512 - 0s - loss: 9.4498e-04 - val_loss: 1.5723e-04
Epoch 191/512
512/512 - 0s - loss: 8.6738e-04 - val_loss: 2.2853e-04
Epoch 192/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.1046e-04
Epoch 193/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.1506e-04
Epoch 194/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.5488e-04
Epoch 195/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.8905e-04
Epoch 196/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.7233e-04
Epoch 197/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.8619e-04
Epoch 198/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.6446e-04
Epoch 199/512
512/512 - 0s - loss: 9.5656e-04 - val_loss: 1.2359e-04
Epoch 200/512
512/512 - 0s - loss: 0.0013 - val_loss: 2.1140e-04
Epoch 201/512
512/512 - 0s - loss: 7.7930e-04 - val_loss: 1.1022e-04
Epoch 202/512
512/512 - 0s - loss: 0.0016 - val_loss: 5.7730e-05
Epoch 203/512
512/512 - 0s - loss: 0.0018 - val_loss: 1.0196e-04
Epoch 204/512
512/512 - 0s - loss: 0.0011 - val_loss: 3.9855e-05
Epoch 205/512
512/512 - 0s - loss: 7.2440e-04 - val_loss: 6.2723e-05
Epoch 206/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.0077e-05
Epoch 207/512
512/512 - 0s - loss: 0.0017 - val_loss: 4.4920e-05
Epoch 208/512
512/512 - 0s - loss: 8.1795e-04 - val_loss: 5.9068e-05
Epoch 209/512
512/512 - 0s - loss: 0.0015 - val_loss: 3.5873e-05
Epoch 210/512
512/512 - 0s - loss: 9.8834e-04 - val_loss: 9.7722e-06
Epoch 211/512
512/512 - 0s - loss: 4.8383e-04 - val_loss: 1.9102e-05
Epoch 212/512
512/512 - 0s - loss: 0.0022 - val_loss: 2.2052e-05
Epoch 213/512
512/512 - 0s - loss: 8.8971e-04 - val_loss: 5.8317e-06
Epoch 214/512
512/512 - 0s - loss: 6.2823e-04 - val_loss: 1.6999e-05
Epoch 215/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.5920e-05
Epoch 216/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1607e-05
Epoch 217/512
512/512 - 0s - loss: 8.0199e-04 - val_loss: 1.5091e-05
Epoch 218/512
512/512 - 0s - loss: 0.0012 - val_loss: 2.2346e-05
Epoch 219/512
512/512 - 0s - loss: 0.0012 - val_loss: 8.0906e-06
Epoch 220/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.3765e-05
Epoch 221/512
512/512 - 0s - loss: 0.0012 - val_loss: 7.5397e-06
Epoch 222/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1152e-05
Epoch 223/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.3206e-06
Epoch 224/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.9720e-06
Epoch 225/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.6878e-06
Epoch 226/512
512/512 - 0s - loss: 9.4478e-04 - val_loss: 8.9616e-06
Epoch 227/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2893e-05
Epoch 228/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0129e-05
Epoch 229/512
512/512 - 0s - loss: 9.0298e-04 - val_loss: 8.0608e-06
Epoch 230/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0991e-05
Epoch 231/512
512/512 - 0s - loss: 0.0012 - val_loss: 8.2426e-06
Epoch 232/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.7237e-06
Epoch 233/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.5706e-06
Epoch 234/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1563e-05
Epoch 235/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.6950e-06
Epoch 236/512
512/512 - 0s - loss: 5.0126e-04 - val_loss: 5.2247e-06
Epoch 237/512
512/512 - 0s - loss: 0.0012 - val_loss: 2.4843e-05
Epoch 238/512
512/512 - 0s - loss: 0.0018 - val_loss: 6.9828e-06
Epoch 239/512
512/512 - 0s - loss: 6.7866e-04 - val_loss: 5.4619e-06
Epoch 240/512
512/512 - 0s - loss: 4.3214e-04 - val_loss: 5.2841e-06
Epoch 241/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.9866e-05
Epoch 242/512
512/512 - 0s - loss: 0.0019 - val_loss: 3.7205e-06
Epoch 243/512
512/512 - 0s - loss: 5.8072e-04 - val_loss: 4.1930e-06
Epoch 244/512
512/512 - 0s - loss: 9.0095e-04 - val_loss: 1.1506e-05
Epoch 245/512
512/512 - 0s - loss: 0.0016 - val_loss: 5.4013e-06
Epoch 246/512
512/512 - 0s - loss: 9.0894e-04 - val_loss: 1.9932e-05
Epoch 247/512
512/512 - 0s - loss: 6.8938e-04 - val_loss: 4.5864e-06
Epoch 248/512
512/512 - 0s - loss: 0.0016 - val_loss: 9.3560e-06
Epoch 249/512
512/512 - 0s - loss: 0.0013 - val_loss: 4.0293e-06
Epoch 250/512
512/512 - 0s - loss: 6.7976e-04 - val_loss: 4.7570e-06
Epoch 251/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.5932e-06
Epoch 252/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.7148e-06
Epoch 253/512
512/512 - 0s - loss: 8.8273e-04 - val_loss: 5.1863e-06
Epoch 254/512
512/512 - 0s - loss: 0.0010 - val_loss: 7.4044e-06
Epoch 255/512
512/512 - 0s - loss: 0.0012 - val_loss: 6.1134e-06
Epoch 256/512
512/512 - 0s - loss: 9.7762e-04 - val_loss: 5.9773e-06
Epoch 257/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.6115e-06
Epoch 258/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.8712e-06
Epoch 259/512
512/512 - 0s - loss: 0.0014 - val_loss: 3.4635e-06
Epoch 260/512
512/512 - 0s - loss: 8.1241e-04 - val_loss: 2.3402e-06
Epoch 261/512
512/512 - 0s - loss: 5.4277e-04 - val_loss: 3.1120e-05
Epoch 262/512
512/512 - 0s - loss: 7.8380e-04 - val_loss: 8.1964e-06
Epoch 263/512
512/512 - 0s - loss: 8.6662e-04 - val_loss: 6.0533e-06
Epoch 264/512
512/512 - 0s - loss: 0.0017 - val_loss: 5.6912e-06
Epoch 265/512
512/512 - 0s - loss: 8.9192e-04 - val_loss: 6.3148e-06
Epoch 266/512
512/512 - 0s - loss: 8.5513e-04 - val_loss: 8.6358e-06
Epoch 267/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.2567e-05
Epoch 268/512
512/512 - 0s - loss: 0.0014 - val_loss: 8.6976e-06
Epoch 269/512
512/512 - 0s - loss: 7.8114e-04 - val_loss: 8.2722e-06
Epoch 270/512
512/512 - 0s - loss: 9.7627e-04 - val_loss: 1.7028e-05
Epoch 271/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1813e-05
Epoch 272/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.5343e-06
Epoch 273/512
512/512 - 0s - loss: 7.7330e-04 - val_loss: 9.8603e-06
Epoch 274/512
512/512 - 0s - loss: 9.1064e-04 - val_loss: 1.1379e-05
Epoch 275/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0115e-05
Epoch 276/512
512/512 - 0s - loss: 8.7879e-04 - val_loss: 9.6765e-06
Epoch 277/512
512/512 - 0s - loss: 8.7839e-04 - val_loss: 3.1291e-05
Epoch 278/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.4592e-05
Epoch 279/512
512/512 - 0s - loss: 6.5149e-04 - val_loss: 1.3983e-05
Epoch 280/512
512/512 - 0s - loss: 7.8129e-04 - val_loss: 1.8079e-05
Epoch 281/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.6863e-05
Epoch 282/512
512/512 - 0s - loss: 9.7292e-04 - val_loss: 1.5000e-05
Epoch 283/512
512/512 - 0s - loss: 8.6649e-04 - val_loss: 1.7527e-05
Epoch 284/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.6874e-05
Epoch 285/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.5506e-05
Epoch 286/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.2342e-05
Epoch 287/512
512/512 - 0s - loss: 8.3161e-04 - val_loss: 1.3943e-05
Epoch 288/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.6641e-05
Epoch 289/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.8639e-05
Epoch 290/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.4332e-05
Epoch 291/512
512/512 - 0s - loss: 8.7879e-04 - val_loss: 1.9235e-05
Epoch 292/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.7578e-05
Epoch 293/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.6807e-05
Epoch 294/512
512/512 - 0s - loss: 8.5171e-04 - val_loss: 1.8577e-05
Epoch 295/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.9874e-05
Epoch 296/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.9609e-05
Epoch 297/512
512/512 - 0s - loss: 5.7128e-04 - val_loss: 2.1130e-05
Epoch 298/512
512/512 - 0s - loss: 0.0010 - val_loss: 2.3485e-05
Epoch 299/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.7516e-05
Epoch 300/512
512/512 - 0s - loss: 7.0578e-04 - val_loss: 1.7755e-05
Epoch 301/512
512/512 - 0s - loss: 6.7811e-04 - val_loss: 2.7773e-05
Epoch 302/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.8870e-05
Epoch 303/512
512/512 - 0s - loss: 0.0010 - val_loss: 2.2454e-05
Epoch 304/512
512/512 - 0s - loss: 7.1530e-04 - val_loss: 2.5802e-05
Epoch 305/512
512/512 - 0s - loss: 8.3681e-04 - val_loss: 3.4164e-05
Epoch 306/512
512/512 - 0s - loss: 0.0016 - val_loss: 2.3318e-05
Epoch 307/512
512/512 - 0s - loss: 7.4506e-04 - val_loss: 5.1334e-05
Epoch 308/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.3653e-05
Epoch 309/512
512/512 - 0s - loss: 4.2001e-04 - val_loss: 1.4643e-05
Epoch 310/512
512/512 - 0s - loss: 5.5031e-04 - val_loss: 2.8749e-05
Epoch 311/512
512/512 - 0s - loss: 0.0016 - val_loss: 2.4593e-05
Epoch 312/512
512/512 - 0s - loss: 8.5190e-04 - val_loss: 1.9957e-05
Epoch 313/512
512/512 - 0s - loss: 7.0466e-04 - val_loss: 2.4050e-05
Epoch 314/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.7324e-05
Epoch 315/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.3001e-05
Epoch 316/512
512/512 - 0s - loss: 8.6470e-04 - val_loss: 2.7395e-05
Epoch 317/512
512/512 - 0s - loss: 9.5945e-04 - val_loss: 2.8785e-05
Epoch 318/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.5996e-05
Epoch 319/512
512/512 - 0s - loss: 6.5344e-04 - val_loss: 3.7007e-05
Epoch 320/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.2170e-05
Epoch 321/512
512/512 - 0s - loss: 0.0013 - val_loss: 3.0338e-05
Epoch 322/512
512/512 - 0s - loss: 6.9417e-04 - val_loss: 3.2363e-05
Epoch 323/512
512/512 - 0s - loss: 9.5183e-04 - val_loss: 3.7265e-05
Epoch 324/512
512/512 - 0s - loss: 0.0014 - val_loss: 3.9432e-05
Epoch 325/512
512/512 - 0s - loss: 7.5623e-04 - val_loss: 3.5698e-05
Epoch 326/512
512/512 - 0s - loss: 7.8598e-04 - val_loss: 4.1874e-05
Epoch 327/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.1592e-05
Epoch 328/512
512/512 - 0s - loss: 9.8526e-04 - val_loss: 3.5324e-05
Epoch 329/512
512/512 - 0s - loss: 8.9572e-04 - val_loss: 3.6019e-05
Epoch 330/512
512/512 - 0s - loss: 0.0011 - val_loss: 4.3545e-05
Epoch 331/512
512/512 - 0s - loss: 8.7249e-04 - val_loss: 3.4269e-05
Epoch 332/512
512/512 - 0s - loss: 6.8337e-04 - val_loss: 3.8045e-05
Epoch 333/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.6763e-05
Epoch 334/512
512/512 - 0s - loss: 0.0011 - val_loss: 5.5993e-05
Epoch 335/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.8000e-05
Epoch 336/512
512/512 - 0s - loss: 6.8398e-04 - val_loss: 6.8737e-05
Epoch 337/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.8711e-05
Epoch 338/512
512/512 - 0s - loss: 0.0012 - val_loss: 5.8698e-05
Epoch 339/512
512/512 - 0s - loss: 8.9465e-04 - val_loss: 5.6731e-05
Epoch 340/512
512/512 - 0s - loss: 9.2359e-04 - val_loss: 5.7249e-05
Epoch 341/512
512/512 - 0s - loss: 9.7901e-04 - val_loss: 5.7977e-05
Epoch 342/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.8403e-05
Epoch 343/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.1368e-05
Epoch 344/512
512/512 - 0s - loss: 7.9729e-04 - val_loss: 5.3575e-05
Epoch 345/512
512/512 - 0s - loss: 0.0011 - val_loss: 5.1931e-05
Epoch 346/512
512/512 - 0s - loss: 8.3995e-04 - val_loss: 3.2164e-05
Epoch 347/512
512/512 - 0s - loss: 6.4404e-04 - val_loss: 5.3119e-05
Epoch 348/512
512/512 - 0s - loss: 0.0014 - val_loss: 4.8866e-05
Epoch 349/512
512/512 - 0s - loss: 8.9086e-04 - val_loss: 4.0758e-05
Epoch 350/512
512/512 - 0s - loss: 7.0343e-04 - val_loss: 4.2889e-05
Epoch 351/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.0524e-05
Epoch 352/512
512/512 - 0s - loss: 6.2279e-04 - val_loss: 5.0217e-05
Epoch 353/512
512/512 - 0s - loss: 9.6769e-04 - val_loss: 4.2543e-05
Epoch 354/512
512/512 - 0s - loss: 7.3128e-04 - val_loss: 1.0937e-04
Epoch 355/512
512/512 - 0s - loss: 5.5779e-04 - val_loss: 1.0089e-04
Epoch 356/512
512/512 - 0s - loss: 0.0020 - val_loss: 9.0850e-05
Epoch 357/512
512/512 - 0s - loss: 5.3063e-04 - val_loss: 9.6321e-05
Epoch 358/512
512/512 - 0s - loss: 4.6947e-04 - val_loss: 9.2040e-05
Epoch 359/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.4054e-05
Epoch 360/512
512/512 - 0s - loss: 0.0010 - val_loss: 8.8842e-05
Epoch 361/512
512/512 - 0s - loss: 6.2166e-04 - val_loss: 8.8667e-05
Epoch 362/512
512/512 - 0s - loss: 8.6030e-04 - val_loss: 8.3942e-05
Epoch 363/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.3210e-05
Epoch 364/512
512/512 - 0s - loss: 7.9236e-04 - val_loss: 8.3840e-05
Epoch 365/512
512/512 - 0s - loss: 8.2380e-04 - val_loss: 8.1181e-05
Epoch 366/512
512/512 - 0s - loss: 9.8130e-04 - val_loss: 7.9246e-05
Epoch 367/512
512/512 - 0s - loss: 8.9623e-04 - val_loss: 7.9215e-05
Epoch 368/512
512/512 - 0s - loss: 8.4064e-04 - val_loss: 7.7293e-05
Epoch 369/512
512/512 - 0s - loss: 9.2338e-04 - val_loss: 7.2923e-05
Epoch 370/512
512/512 - 0s - loss: 9.4092e-04 - val_loss: 7.2155e-05
Epoch 371/512
512/512 - 0s - loss: 8.8015e-04 - val_loss: 7.0344e-05
Epoch 372/512
512/512 - 0s - loss: 9.3858e-04 - val_loss: 6.8348e-05
Epoch 373/512
512/512 - 0s - loss: 9.4055e-04 - val_loss: 6.5350e-05
Epoch 374/512
512/512 - 0s - loss: 9.5467e-04 - val_loss: 6.4610e-05
Epoch 375/512
512/512 - 0s - loss: 9.3447e-04 - val_loss: 6.2888e-05
Epoch 376/512
512/512 - 0s - loss: 9.5470e-04 - val_loss: 7.5234e-05
Epoch 377/512
512/512 - 0s - loss: 8.7388e-04 - val_loss: 7.1019e-05
Epoch 378/512
512/512 - 0s - loss: 0.0010 - val_loss: 6.2812e-05
Epoch 379/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.1666e-05
Epoch 380/512
512/512 - 0s - loss: 8.5465e-04 - val_loss: 5.8307e-05
Epoch 381/512
512/512 - 0s - loss: 8.3733e-04 - val_loss: 7.0755e-05
Epoch 382/512
512/512 - 0s - loss: 0.0010 - val_loss: 6.3561e-05
Epoch 383/512
512/512 - 0s - loss: 0.0010 - val_loss: 6.9273e-05
Epoch 384/512
512/512 - 0s - loss: 8.7519e-04 - val_loss: 6.8414e-05
Epoch 385/512
512/512 - 0s - loss: 9.8123e-04 - val_loss: 6.6000e-05
Epoch 386/512
512/512 - 0s - loss: 9.8530e-04 - val_loss: 6.5391e-05
Epoch 387/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.7146e-05
Epoch 388/512
512/512 - 0s - loss: 7.9364e-04 - val_loss: 5.9311e-05
Epoch 389/512
512/512 - 0s - loss: 8.8492e-04 - val_loss: 5.7740e-05
Epoch 390/512
512/512 - 0s - loss: 0.0010 - val_loss: 5.7534e-05
Epoch 391/512
512/512 - 0s - loss: 9.4770e-04 - val_loss: 6.0655e-05
Epoch 392/512
512/512 - 0s - loss: 9.9426e-04 - val_loss: 4.4175e-05
Epoch 393/512
512/512 - 0s - loss: 9.6072e-04 - val_loss: 5.5531e-05
Epoch 394/512
512/512 - 0s - loss: 5.7585e-04 - val_loss: 5.8454e-05
Epoch 395/512
512/512 - 0s - loss: 9.5590e-04 - val_loss: 5.0663e-05
Epoch 396/512
512/512 - 0s - loss: 0.0012 - val_loss: 5.6644e-05
Epoch 397/512
512/512 - 0s - loss: 7.3381e-04 - val_loss: 5.7852e-05
Epoch 398/512
512/512 - 0s - loss: 9.1244e-04 - val_loss: 1.2850e-04
Epoch 399/512
512/512 - 0s - loss: 6.4856e-04 - val_loss: 1.1227e-04
Epoch 400/512
512/512 - 0s - loss: 0.0014 - val_loss: 9.5357e-05
Epoch 401/512
512/512 - 0s - loss: 9.0270e-04 - val_loss: 1.0112e-04
Epoch 402/512
512/512 - 0s - loss: 6.6095e-04 - val_loss: 9.6832e-05
Epoch 403/512
512/512 - 0s - loss: 0.0010 - val_loss: 8.5936e-05
Epoch 404/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.7681e-05
Epoch 405/512
512/512 - 0s - loss: 8.5545e-04 - val_loss: 7.6873e-05
Epoch 406/512
512/512 - 0s - loss: 8.4767e-04 - val_loss: 7.6187e-05
Epoch 407/512
512/512 - 0s - loss: 9.3379e-04 - val_loss: 7.5728e-05
Epoch 408/512
512/512 - 0s - loss: 9.3864e-04 - val_loss: 7.6300e-05
Epoch 409/512
512/512 - 0s - loss: 9.0365e-04 - val_loss: 8.0344e-05
Epoch 410/512
512/512 - 0s - loss: 9.0532e-04 - val_loss: 7.6922e-05
Epoch 411/512
512/512 - 0s - loss: 9.6737e-04 - val_loss: 7.5666e-05
Epoch 412/512
512/512 - 0s - loss: 9.4660e-04 - val_loss: 7.5096e-05
Epoch 413/512
512/512 - 0s - loss: 8.8667e-04 - val_loss: 7.4412e-05
Epoch 414/512
512/512 - 0s - loss: 9.6934e-04 - val_loss: 8.3493e-05
Epoch 415/512
512/512 - 0s - loss: 9.0316e-04 - val_loss: 8.3916e-05
Epoch 416/512
512/512 - 0s - loss: 9.0442e-04 - val_loss: 7.1557e-05
Epoch 417/512
512/512 - 0s - loss: 0.0010 - val_loss: 7.2139e-05
Epoch 418/512
512/512 - 0s - loss: 8.8211e-04 - val_loss: 6.6947e-05
Epoch 419/512
512/512 - 0s - loss: 8.6548e-04 - val_loss: 6.9516e-05
Epoch 420/512
512/512 - 0s - loss: 0.0010 - val_loss: 7.5646e-05
Epoch 421/512
512/512 - 0s - loss: 8.4858e-04 - val_loss: 7.5340e-05
Epoch 422/512
512/512 - 0s - loss: 8.6105e-04 - val_loss: 7.2549e-05
Epoch 423/512
512/512 - 0s - loss: 9.3012e-04 - val_loss: 6.5929e-05
Epoch 424/512
512/512 - 0s - loss: 8.6100e-04 - val_loss: 1.0082e-04
Epoch 425/512
512/512 - 0s - loss: 8.0469e-04 - val_loss: 8.1266e-05
Epoch 426/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.6857e-05
Epoch 427/512
512/512 - 0s - loss: 9.4822e-04 - val_loss: 8.4728e-05
Epoch 428/512
512/512 - 0s - loss: 8.5368e-04 - val_loss: 8.1755e-05
Epoch 429/512
512/512 - 0s - loss: 9.6426e-04 - val_loss: 8.0237e-05
Epoch 430/512
512/512 - 0s - loss: 9.1729e-04 - val_loss: 8.0596e-05
Epoch 431/512
512/512 - 0s - loss: 8.7924e-04 - val_loss: 7.9754e-05
Epoch 432/512
512/512 - 0s - loss: 9.5948e-04 - val_loss: 7.6474e-05
Epoch 433/512
512/512 - 0s - loss: 9.3213e-04 - val_loss: 7.8125e-05
Epoch 434/512
512/512 - 0s - loss: 8.3820e-04 - val_loss: 7.8738e-05
Epoch 435/512
512/512 - 0s - loss: 9.4855e-04 - val_loss: 7.3467e-05
Epoch 436/512
512/512 - 0s - loss: 9.7419e-04 - val_loss: 7.4468e-05
Epoch 437/512
512/512 - 0s - loss: 8.5568e-04 - val_loss: 7.3965e-05
Epoch 438/512
512/512 - 0s - loss: 8.9202e-04 - val_loss: 7.4081e-05
Epoch 439/512
512/512 - 0s - loss: 9.2766e-04 - val_loss: 5.6683e-05
Epoch 440/512
512/512 - 0s - loss: 9.8309e-04 - val_loss: 6.5404e-05
Epoch 441/512
512/512 - 0s - loss: 6.8069e-04 - val_loss: 7.2747e-05
Epoch 442/512
512/512 - 0s - loss: 8.9233e-04 - val_loss: 6.3395e-05
Epoch 443/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.4983e-05
Epoch 444/512
512/512 - 0s - loss: 7.9181e-04 - val_loss: 6.9182e-05
Epoch 445/512
512/512 - 0s - loss: 8.1531e-04 - val_loss: 6.3933e-05
Epoch 446/512
512/512 - 0s - loss: 0.0010 - val_loss: 4.9653e-05
Epoch 447/512
512/512 - 0s - loss: 9.6448e-04 - val_loss: 6.0147e-05
Epoch 448/512
512/512 - 0s - loss: 5.7725e-04 - val_loss: 5.5811e-05
Epoch 449/512
512/512 - 0s - loss: 9.7532e-04 - val_loss: 5.1338e-05
Epoch 450/512
512/512 - 0s - loss: 0.0011 - val_loss: 5.7926e-05
Epoch 451/512
512/512 - 0s - loss: 6.5553e-04 - val_loss: 7.6801e-05
Epoch 452/512
512/512 - 0s - loss: 8.6275e-04 - val_loss: 6.2818e-05
Epoch 453/512
512/512 - 0s - loss: 0.0012 - val_loss: 6.3771e-05
Epoch 454/512
512/512 - 0s - loss: 7.1774e-04 - val_loss: 6.9941e-05
Epoch 455/512
512/512 - 0s - loss: 7.7317e-04 - val_loss: 6.2264e-05
Epoch 456/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.2558e-05
Epoch 457/512
512/512 - 0s - loss: 8.4337e-04 - val_loss: 7.5337e-05
Epoch 458/512
512/512 - 0s - loss: 8.6222e-04 - val_loss: 7.1365e-05
Epoch 459/512
512/512 - 0s - loss: 9.4553e-04 - val_loss: 6.9589e-05
Epoch 460/512
512/512 - 0s - loss: 8.9864e-04 - val_loss: 6.8587e-05
Epoch 461/512
512/512 - 0s - loss: 8.7876e-04 - val_loss: 6.5469e-05
Epoch 462/512
512/512 - 0s - loss: 9.0647e-04 - val_loss: 6.6975e-05
Epoch 463/512
512/512 - 0s - loss: 8.7101e-04 - val_loss: 6.7371e-05
Epoch 464/512
512/512 - 0s - loss: 9.1899e-04 - val_loss: 6.5001e-05
Epoch 465/512
512/512 - 0s - loss: 8.6445e-04 - val_loss: 6.5300e-05
Epoch 466/512
512/512 - 0s - loss: 6.7873e-04 - val_loss: 1.3285e-04
Epoch 467/512
512/512 - 0s - loss: 6.9454e-04 - val_loss: 1.2118e-04
Epoch 468/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.1273e-04
Epoch 469/512
512/512 - 0s - loss: 6.3680e-04 - val_loss: 1.1298e-04
Epoch 470/512
512/512 - 0s - loss: 6.3697e-04 - val_loss: 1.0699e-04
Epoch 471/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.9634e-05
Epoch 472/512
512/512 - 0s - loss: 9.0230e-04 - val_loss: 1.0079e-04
Epoch 473/512
512/512 - 0s - loss: 7.3480e-04 - val_loss: 9.8007e-05
Epoch 474/512
512/512 - 0s - loss: 9.3547e-04 - val_loss: 9.2120e-05
Epoch 475/512
512/512 - 0s - loss: 9.5240e-04 - val_loss: 9.0681e-05
Epoch 476/512
512/512 - 0s - loss: 8.1523e-04 - val_loss: 8.9301e-05
Epoch 477/512
512/512 - 0s - loss: 8.8672e-04 - val_loss: 8.9532e-05
Epoch 478/512
512/512 - 0s - loss: 9.2757e-04 - val_loss: 8.6882e-05
Epoch 479/512
512/512 - 0s - loss: 8.9758e-04 - val_loss: 8.7421e-05
Epoch 480/512
512/512 - 0s - loss: 8.5322e-04 - val_loss: 8.4365e-05
Epoch 481/512
512/512 - 0s - loss: 9.5639e-04 - val_loss: 7.7507e-05
Epoch 482/512
512/512 - 0s - loss: 8.5041e-04 - val_loss: 6.6652e-05
Epoch 483/512
512/512 - 0s - loss: 9.0182e-04 - val_loss: 7.2968e-05
Epoch 484/512
512/512 - 0s - loss: 7.8804e-04 - val_loss: 7.2754e-05
Epoch 485/512
512/512 - 0s - loss: 8.7694e-04 - val_loss: 7.1304e-05
Epoch 486/512
512/512 - 0s - loss: 9.4588e-04 - val_loss: 7.0195e-05
Epoch 487/512
512/512 - 0s - loss: 8.6709e-04 - val_loss: 6.9269e-05
Epoch 488/512
512/512 - 0s - loss: 8.4736e-04 - val_loss: 6.8642e-05
Epoch 489/512
512/512 - 0s - loss: 8.8726e-04 - val_loss: 6.8732e-05
Epoch 490/512
512/512 - 0s - loss: 8.8279e-04 - val_loss: 6.6973e-05
Epoch 491/512
512/512 - 0s - loss: 8.7791e-04 - val_loss: 6.7481e-05
Epoch 492/512
512/512 - 0s - loss: 8.5225e-04 - val_loss: 6.6506e-05
Epoch 493/512
512/512 - 0s - loss: 8.8495e-04 - val_loss: 6.5013e-05
Epoch 494/512
512/512 - 0s - loss: 8.8513e-04 - val_loss: 6.5561e-05
Epoch 495/512
512/512 - 0s - loss: 8.4503e-04 - val_loss: 6.5785e-05
Epoch 496/512
512/512 - 0s - loss: 8.9193e-04 - val_loss: 6.5279e-05
Epoch 497/512
512/512 - 0s - loss: 8.8575e-04 - val_loss: 6.5014e-05
Epoch 498/512
512/512 - 0s - loss: 8.5524e-04 - val_loss: 6.4192e-05
Epoch 499/512
512/512 - 0s - loss: 7.5241e-04 - val_loss: 8.7633e-05
Epoch 500/512
512/512 - 0s - loss: 9.2356e-04 - val_loss: 7.6564e-05
Epoch 501/512
512/512 - 0s - loss: 0.0010 - val_loss: 7.5955e-05
Epoch 502/512
512/512 - 0s - loss: 7.5859e-04 - val_loss: 5.6051e-05
Epoch 503/512
512/512 - 0s - loss: 9.0255e-04 - val_loss: 6.4152e-05
Epoch 504/512
512/512 - 0s - loss: 6.9485e-04 - val_loss: 1.2470e-04
Epoch 505/512
512/512 - 0s - loss: 5.3475e-04 - val_loss: 1.1464e-04
Epoch 506/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.0082e-04
Epoch 507/512
512/512 - 0s - loss: 8.1215e-04 - val_loss: 1.0621e-04
Epoch 508/512
512/512 - 0s - loss: 5.4732e-04 - val_loss: 1.0191e-04
Epoch 509/512
512/512 - 0s - loss: 9.4978e-04 - val_loss: 9.5055e-05
Epoch 510/512
512/512 - 0s - loss: 9.7843e-04 - val_loss: 9.4771e-05
Epoch 511/512
512/512 - 0s - loss: 7.1042e-04 - val_loss: 9.3265e-05
Epoch 512/512
512/512 - 0s - loss: 8.3013e-04 - val_loss: 8.8411e-05
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00029, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 0.0036 - val_loss: 2.9273e-04
Epoch 2/512

Epoch 00002: val_loss improved from 0.00029 to 0.00015, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8321e-04 - val_loss: 1.5489e-04
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00015
512/512 - 0s - loss: 2.9087e-04 - val_loss: 0.0010
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0017 - val_loss: 0.0021
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0012 - val_loss: 7.4428e-04
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.0133e-04 - val_loss: 0.0010
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0011 - val_loss: 0.0015
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0012 - val_loss: 0.0011
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3047e-04 - val_loss: 0.0011
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9893e-04 - val_loss: 0.0011
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9847e-04 - val_loss: 0.0011
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9126e-04 - val_loss: 0.0011
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9806e-04 - val_loss: 0.0011
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9507e-04 - val_loss: 0.0011
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.8570e-04 - val_loss: 0.0011
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.9135e-04 - val_loss: 0.0011
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.8937e-04 - val_loss: 0.0011
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.8484e-04 - val_loss: 0.0011
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.8413e-04 - val_loss: 0.0011
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.7571e-04 - val_loss: 0.0011
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.7919e-04 - val_loss: 0.0011
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.7957e-04 - val_loss: 0.0011
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.6990e-04 - val_loss: 0.0011
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.7110e-04 - val_loss: 0.0011
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.6752e-04 - val_loss: 0.0011
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.7133e-04 - val_loss: 0.0011
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.6319e-04 - val_loss: 0.0011
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.6463e-04 - val_loss: 0.0011
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.6138e-04 - val_loss: 0.0011
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.5439e-04 - val_loss: 0.0011
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.5711e-04 - val_loss: 0.0011
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.5408e-04 - val_loss: 0.0011
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.4768e-04 - val_loss: 0.0011
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.5263e-04 - val_loss: 0.0011
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.4599e-04 - val_loss: 0.0011
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.4380e-04 - val_loss: 0.0010
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3601e-04 - val_loss: 0.0011
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.4435e-04 - val_loss: 0.0010
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3630e-04 - val_loss: 0.0010
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3112e-04 - val_loss: 0.0010
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3524e-04 - val_loss: 0.0010
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.2884e-04 - val_loss: 0.0010
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.2657e-04 - val_loss: 0.0010
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.2450e-04 - val_loss: 0.0010
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1942e-04 - val_loss: 0.0010
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1890e-04 - val_loss: 0.0010
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1501e-04 - val_loss: 0.0010
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1401e-04 - val_loss: 0.0010
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.0695e-04 - val_loss: 0.0010
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1072e-04 - val_loss: 0.0010
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.0681e-04 - val_loss: 9.8992e-04
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.9363e-04 - val_loss: 9.9589e-04
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.0015e-04 - val_loss: 0.0010
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.0405e-04 - val_loss: 9.8919e-04
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.8808e-04 - val_loss: 9.7964e-04
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.8713e-04 - val_loss: 9.8902e-04
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.9238e-04 - val_loss: 9.8014e-04
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7695e-04 - val_loss: 9.7467e-04
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.9198e-04 - val_loss: 9.7713e-04
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7088e-04 - val_loss: 9.5644e-04
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7109e-04 - val_loss: 9.6415e-04
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7331e-04 - val_loss: 9.7806e-04
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7571e-04 - val_loss: 8.9066e-04
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.4466e-04 - val_loss: 9.9153e-04
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.0061e-04 - val_loss: 9.7191e-04
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.1028e-04 - val_loss: 6.8893e-04
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.6015e-04 - val_loss: 0.0011
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.8375e-04 - val_loss: 9.8847e-04
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.3432e-04 - val_loss: 8.7409e-04
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.0895e-04 - val_loss: 9.2609e-04
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.6176e-04 - val_loss: 9.6572e-04
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.5032e-04 - val_loss: 9.1271e-04
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.2435e-04 - val_loss: 8.4709e-04
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.1237e-04 - val_loss: 9.5208e-04
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.6905e-04 - val_loss: 9.3704e-04
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.4613e-04 - val_loss: 8.8547e-04
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.9638e-04 - val_loss: 8.6705e-04
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.9056e-04 - val_loss: 9.3375e-04
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.3659e-04 - val_loss: 0.0011
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.6459e-04 - val_loss: 6.7017e-04
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.7999e-04 - val_loss: 7.4630e-04
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.8972e-04 - val_loss: 0.0012
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.7698e-04 - val_loss: 6.1097e-04
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.2584e-04 - val_loss: 0.0014
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0011 - val_loss: 8.1602e-04
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.5163e-04 - val_loss: 6.3376e-04
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.8918e-04 - val_loss: 9.7132e-04
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.1733e-04 - val_loss: 9.4670e-04
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.7220e-04 - val_loss: 7.0182e-04
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.0668e-04 - val_loss: 8.8503e-04
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.1495e-04 - val_loss: 8.5562e-04
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.9241e-04 - val_loss: 8.5932e-04
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.8604e-04 - val_loss: 9.9471e-04
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.8435e-04 - val_loss: 6.9760e-04
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.0498e-04 - val_loss: 5.9364e-04
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.7561e-04 - val_loss: 0.0013
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.5965e-04 - val_loss: 4.6899e-04
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.4454e-04 - val_loss: 7.5672e-04
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.6664e-04 - val_loss: 8.6011e-04
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00015
512/512 - 0s - loss: 9.3308e-04 - val_loss: 0.0011
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.5205e-04 - val_loss: 8.0301e-04
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.2887e-04 - val_loss: 8.9629e-04
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.7010e-04 - val_loss: 7.6135e-04
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.9632e-04 - val_loss: 7.8668e-04
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.9747e-04 - val_loss: 9.0087e-04
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.2060e-04 - val_loss: 6.7442e-04
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.5029e-04 - val_loss: 7.8691e-04
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.8321e-04 - val_loss: 9.5909e-04
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.3983e-04 - val_loss: 7.5086e-04
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.5179e-04 - val_loss: 6.8549e-04
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.2292e-04 - val_loss: 8.7918e-04
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.8041e-04 - val_loss: 5.8843e-04
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.3939e-04 - val_loss: 0.0011
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.2620e-04 - val_loss: 5.6552e-04
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.9340e-04 - val_loss: 8.9747e-04
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.6799e-04 - val_loss: 6.0941e-04
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.4627e-04 - val_loss: 0.0010
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6720e-04 - val_loss: 3.5739e-04
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.1663e-04 - val_loss: 6.8501e-04
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.4736e-04 - val_loss: 0.0010
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.8414e-04 - val_loss: 3.2581e-04
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.0524e-04 - val_loss: 5.4858e-04
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.4604e-04 - val_loss: 6.5284e-04
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.2825e-04 - val_loss: 0.0011
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00015
512/512 - 0s - loss: 8.5212e-04 - val_loss: 5.9686e-04
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.0698e-04 - val_loss: 5.5126e-04
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.0782e-04 - val_loss: 8.2078e-04
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.5136e-04 - val_loss: 7.1143e-04
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.2361e-04 - val_loss: 6.1194e-04
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.0327e-04 - val_loss: 7.4308e-04
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.3855e-04 - val_loss: 7.7665e-04
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6423e-04 - val_loss: 6.2097e-04
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.8746e-04 - val_loss: 6.5866e-04
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6469e-04 - val_loss: 7.7563e-04
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6631e-04 - val_loss: 6.3644e-04
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.0810e-04 - val_loss: 6.7553e-04
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.5430e-04 - val_loss: 7.0150e-04
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.1191e-04 - val_loss: 6.0644e-04
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6356e-04 - val_loss: 9.8414e-04
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.7509e-04 - val_loss: 5.3692e-04
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.3569e-04 - val_loss: 6.9300e-04
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.1329e-04 - val_loss: 7.8342e-04
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6874e-04 - val_loss: 6.1601e-04
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.8226e-04 - val_loss: 6.6329e-04
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.4828e-04 - val_loss: 7.2195e-04
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6155e-04 - val_loss: 6.3572e-04
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.6021e-04 - val_loss: 5.7129e-04
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.9916e-04 - val_loss: 7.2719e-04
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.1382e-04 - val_loss: 9.1776e-04
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6222e-04 - val_loss: 2.9891e-04
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00015
512/512 - 0s - loss: 1.9773e-04 - val_loss: 2.0625e-04
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00015
512/512 - 0s - loss: 2.4941e-04 - val_loss: 4.7289e-04
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.2676e-04 - val_loss: 5.5324e-04
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.0114e-04 - val_loss: 8.0134e-04
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.9914e-04 - val_loss: 5.4542e-04
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.5743e-04 - val_loss: 4.0542e-04
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.2676e-04 - val_loss: 5.5372e-04
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.2971e-04 - val_loss: 3.9327e-04
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.4216e-04 - val_loss: 2.7625e-04
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00015
512/512 - 0s - loss: 3.5558e-04 - val_loss: 8.0098e-04
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00015
512/512 - 0s - loss: 0.0011 - val_loss: 7.9725e-04
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.0442e-04 - val_loss: 3.4320e-04
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00015
512/512 - 0s - loss: 3.3052e-04 - val_loss: 6.3409e-04
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00015
512/512 - 0s - loss: 7.8088e-04 - val_loss: 8.1704e-04
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.9642e-04 - val_loss: 4.2908e-04
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00015
512/512 - 0s - loss: 4.1514e-04 - val_loss: 4.9179e-04
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00015
512/512 - 0s - loss: 5.6596e-04 - val_loss: 7.1372e-04
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.3100e-04 - val_loss: 3.1952e-04
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00015
512/512 - 0s - loss: 2.4873e-04 - val_loss: 2.1880e-04
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00015
512/512 - 0s - loss: 2.6503e-04 - val_loss: 5.0454e-04
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00015
512/512 - 0s - loss: 6.6960e-04 - val_loss: 5.3419e-04
Epoch 187/512

Epoch 00187: val_loss improved from 0.00015 to 0.00010, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0926e-04 - val_loss: 1.0081e-04
Epoch 188/512

Epoch 00188: val_loss improved from 0.00010 to 0.00010, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.0645e-05 - val_loss: 9.9030e-05
Epoch 189/512

Epoch 00189: val_loss improved from 0.00010 to 0.00009, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0397e-04 - val_loss: 9.4235e-05
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00009
512/512 - 0s - loss: 1.9802e-04 - val_loss: 8.6459e-04
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00009
512/512 - 0s - loss: 7.6455e-04 - val_loss: 3.1690e-04
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00009
512/512 - 0s - loss: 2.4423e-04 - val_loss: 1.8905e-04
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00009
512/512 - 0s - loss: 2.1958e-04 - val_loss: 3.3705e-04
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00009
512/512 - 0s - loss: 4.5692e-04 - val_loss: 3.6462e-04
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00009
512/512 - 0s - loss: 2.5755e-04 - val_loss: 3.1278e-04
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00009
512/512 - 0s - loss: 4.7181e-04 - val_loss: 7.6420e-04
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00009
512/512 - 0s - loss: 5.5141e-04 - val_loss: 2.3439e-04
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00009
512/512 - 0s - loss: 3.2467e-04 - val_loss: 7.0405e-04
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00009
512/512 - 0s - loss: 7.5288e-04 - val_loss: 4.1837e-04
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00009
512/512 - 0s - loss: 2.3010e-04 - val_loss: 1.3077e-04
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00009
512/512 - 0s - loss: 4.7455e-04 - val_loss: 0.0015
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00009
512/512 - 0s - loss: 9.5774e-04 - val_loss: 2.0938e-04
Epoch 203/512

Epoch 00203: val_loss improved from 0.00009 to 0.00008, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2939e-04 - val_loss: 8.2838e-05
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00008
512/512 - 0s - loss: 9.5278e-05 - val_loss: 1.9132e-04
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00008
512/512 - 0s - loss: 3.7148e-04 - val_loss: 7.3135e-04
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00008
512/512 - 0s - loss: 5.3530e-04 - val_loss: 1.6251e-04
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00008
512/512 - 0s - loss: 1.2024e-04 - val_loss: 1.1095e-04
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00008
512/512 - 0s - loss: 1.4799e-04 - val_loss: 4.7240e-04
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00008
512/512 - 0s - loss: 9.3707e-04 - val_loss: 5.2795e-04
Epoch 210/512

Epoch 00210: val_loss improved from 0.00008 to 0.00008, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6983e-04 - val_loss: 7.7754e-05
Epoch 211/512

Epoch 00211: val_loss improved from 0.00008 to 0.00007, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-30/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.2007e-05 - val_loss: 7.3441e-05
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.0386e-04 - val_loss: 2.8106e-04
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.7048e-04 - val_loss: 5.6619e-04
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.8375e-04 - val_loss: 2.9391e-04
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.4689e-04 - val_loss: 8.7878e-04
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.8888e-04 - val_loss: 9.4133e-05
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00007
512/512 - 0s - loss: 7.9835e-05 - val_loss: 7.4893e-05
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00007
512/512 - 0s - loss: 9.7187e-05 - val_loss: 2.5934e-04
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00007
512/512 - 0s - loss: 6.8231e-04 - val_loss: 7.0798e-04
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7324e-04 - val_loss: 1.2708e-04
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.1413e-04 - val_loss: 1.2325e-04
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9983e-04 - val_loss: 4.3792e-04
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.7976e-04 - val_loss: 3.7872e-04
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4137e-04 - val_loss: 1.5472e-04
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.8909e-04 - val_loss: 2.5527e-04
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.6145e-04 - val_loss: 4.0548e-04
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.8049e-04 - val_loss: 3.0920e-04
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6677e-04 - val_loss: 3.3682e-04
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7371e-04 - val_loss: 2.8372e-04
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7502e-04 - val_loss: 3.5719e-04
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2926e-04 - val_loss: 2.4089e-04
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5446e-04 - val_loss: 4.1179e-04
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2912e-04 - val_loss: 2.0797e-04
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.3872e-04 - val_loss: 3.3323e-04
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5831e-04 - val_loss: 3.2270e-04
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.3874e-04 - val_loss: 6.6942e-04
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7912e-04 - val_loss: 1.4564e-04
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.2625e-04 - val_loss: 1.4284e-04
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5789e-04 - val_loss: 6.1975e-04
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00007
512/512 - 0s - loss: 7.1807e-04 - val_loss: 7.2378e-04
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7455e-04 - val_loss: 1.0996e-04
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00007
512/512 - 0s - loss: 9.2326e-05 - val_loss: 8.1123e-05
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.1949e-04 - val_loss: 3.5088e-04
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00007
512/512 - 0s - loss: 8.3141e-04 - val_loss: 3.7782e-04
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9241e-04 - val_loss: 1.0783e-04
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.1782e-04 - val_loss: 2.5215e-04
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.1294e-04 - val_loss: 4.1146e-04
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6045e-04 - val_loss: 1.8491e-04
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9768e-04 - val_loss: 2.7586e-04
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5111e-04 - val_loss: 3.7604e-04
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.5457e-04 - val_loss: 7.6745e-04
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.8684e-04 - val_loss: 1.1746e-04
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.0673e-04 - val_loss: 1.1167e-04
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.7989e-04 - val_loss: 3.6826e-04
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.6458e-04 - val_loss: 4.0633e-04
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7283e-04 - val_loss: 3.0061e-04
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4126e-04 - val_loss: 2.4840e-04
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7472e-04 - val_loss: 3.1179e-04
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4265e-04 - val_loss: 5.7397e-04
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.4608e-04 - val_loss: 2.1611e-04
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.6530e-04 - val_loss: 1.5921e-04
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.3779e-04 - val_loss: 4.9166e-04
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00007
512/512 - 0s - loss: 7.5528e-04 - val_loss: 4.1948e-04
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.0793e-04 - val_loss: 7.4083e-05
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00007
512/512 - 0s - loss: 7.6967e-05 - val_loss: 1.0242e-04
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.1061e-04 - val_loss: 5.6524e-04
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.4283e-04 - val_loss: 2.6754e-04
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9695e-04 - val_loss: 1.6668e-04
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.1105e-04 - val_loss: 3.0934e-04
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00007
512/512 - 0s - loss: 7.3149e-04 - val_loss: 9.0088e-04
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.2401e-04 - val_loss: 1.8996e-04
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3096e-04 - val_loss: 5.8139e-04
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.9719e-04 - val_loss: 2.0755e-04
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.8083e-04 - val_loss: 1.6894e-04
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9329e-04 - val_loss: 2.5274e-04
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1625e-04 - val_loss: 4.0347e-04
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3986e-04 - val_loss: 2.5207e-04
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.3287e-04 - val_loss: 2.2107e-04
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.4130e-04 - val_loss: 5.4205e-04
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5783e-04 - val_loss: 1.9365e-04
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9678e-04 - val_loss: 2.2528e-04
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8880e-04 - val_loss: 3.8835e-04
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.6411e-04 - val_loss: 4.5327e-04
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5152e-04 - val_loss: 2.2023e-04
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.0407e-04 - val_loss: 2.1588e-04
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8157e-04 - val_loss: 3.7937e-04
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.6624e-04 - val_loss: 4.0185e-04
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1440e-04 - val_loss: 5.3248e-04
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.7754e-04 - val_loss: 2.6647e-04
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.0548e-04 - val_loss: 1.6597e-04
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9261e-04 - val_loss: 2.8685e-04
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4953e-04 - val_loss: 3.2759e-04
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3585e-04 - val_loss: 2.0724e-04
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.4095e-04 - val_loss: 5.1095e-04
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9563e-04 - val_loss: 1.3982e-04
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.2627e-04 - val_loss: 2.0601e-04
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.0009e-04 - val_loss: 5.8561e-04
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.9599e-04 - val_loss: 2.7417e-04
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1108e-04 - val_loss: 2.6776e-04
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.3981e-04 - val_loss: 2.2339e-04
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5716e-04 - val_loss: 4.0895e-04
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.2128e-04 - val_loss: 3.9232e-04
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4265e-04 - val_loss: 1.2265e-04
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.5769e-04 - val_loss: 2.4995e-04
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4970e-04 - val_loss: 4.4771e-04
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.2134e-04 - val_loss: 3.8415e-04
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4896e-04 - val_loss: 1.5148e-04
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.6893e-04 - val_loss: 2.6649e-04
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5054e-04 - val_loss: 4.3397e-04
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7148e-04 - val_loss: 2.4202e-04
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.1827e-04 - val_loss: 2.1841e-04
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8645e-04 - val_loss: 3.6111e-04
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.9881e-04 - val_loss: 3.0361e-04
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8483e-04 - val_loss: 4.3357e-04
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.7693e-04 - val_loss: 2.8280e-04
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.0859e-04 - val_loss: 5.8431e-04
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.6462e-04 - val_loss: 2.1102e-04
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.0510e-04 - val_loss: 2.5595e-04
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1386e-04 - val_loss: 2.8895e-04
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4815e-04 - val_loss: 2.1721e-04
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4522e-04 - val_loss: 3.3474e-04
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5096e-04 - val_loss: 3.2935e-04
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9222e-04 - val_loss: 2.4641e-04
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4937e-04 - val_loss: 3.7092e-04
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00007
512/512 - 0s - loss: 5.3071e-04 - val_loss: 3.3588e-04
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2496e-04 - val_loss: 1.3863e-04
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.5042e-04 - val_loss: 2.3311e-04
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4488e-04 - val_loss: 5.0074e-04
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00007
512/512 - 0s - loss: 4.2962e-04 - val_loss: 2.2918e-04
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.0499e-04 - val_loss: 2.0735e-04
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5577e-04 - val_loss: 3.5174e-04
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4886e-04 - val_loss: 3.0136e-04
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4693e-04 - val_loss: 4.5784e-04
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3471e-04 - val_loss: 2.5562e-04
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3631e-04 - val_loss: 2.7787e-04
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2802e-04 - val_loss: 1.9363e-04
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2679e-04 - val_loss: 3.1789e-04
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2282e-04 - val_loss: 3.0895e-04
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9253e-04 - val_loss: 2.9661e-04
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9045e-04 - val_loss: 1.9638e-04
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3328e-04 - val_loss: 4.5261e-04
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3352e-04 - val_loss: 2.1186e-04
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00007
512/512 - 0s - loss: 1.9806e-04 - val_loss: 2.2402e-04
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8095e-04 - val_loss: 3.5053e-04
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4451e-04 - val_loss: 3.5740e-04
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.0829e-04 - val_loss: 2.1443e-04
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2726e-04 - val_loss: 2.6249e-04
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1302e-04 - val_loss: 3.8412e-04
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4755e-04 - val_loss: 3.3775e-04
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8557e-04 - val_loss: 2.4167e-04
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4779e-04 - val_loss: 2.6793e-04
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.0375e-04 - val_loss: 3.4268e-04
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1651e-04 - val_loss: 4.3488e-04
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5641e-04 - val_loss: 2.5207e-04
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2644e-04 - val_loss: 1.6591e-04
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2166e-04 - val_loss: 5.0550e-04
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1149e-04 - val_loss: 2.1601e-04
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5909e-04 - val_loss: 3.3611e-04
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2617e-04 - val_loss: 2.8001e-04
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6108e-04 - val_loss: 2.4128e-04
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8895e-04 - val_loss: 3.7393e-04
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2572e-04 - val_loss: 2.5784e-04
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5418e-04 - val_loss: 2.5585e-04
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7822e-04 - val_loss: 3.0690e-04
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1476e-04 - val_loss: 3.0167e-04
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7885e-04 - val_loss: 2.7376e-04
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8078e-04 - val_loss: 3.0767e-04
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2963e-04 - val_loss: 3.6960e-04
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6664e-04 - val_loss: 2.6302e-04
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8857e-04 - val_loss: 3.2991e-04
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2501e-04 - val_loss: 2.6873e-04
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4313e-04 - val_loss: 2.5252e-04
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5510e-04 - val_loss: 3.2955e-04
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.2539e-04 - val_loss: 3.7756e-04
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1894e-04 - val_loss: 2.2684e-04
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.2516e-04 - val_loss: 2.3580e-04
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1494e-04 - val_loss: 4.1819e-04
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.3848e-04 - val_loss: 2.2465e-04
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.1085e-04 - val_loss: 2.1580e-04
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6167e-04 - val_loss: 3.8241e-04
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.5935e-04 - val_loss: 2.7985e-04
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6665e-04 - val_loss: 2.6830e-04
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7554e-04 - val_loss: 2.8924e-04
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7973e-04 - val_loss: 2.7028e-04
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8468e-04 - val_loss: 3.0578e-04
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.0273e-04 - val_loss: 2.5568e-04
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5767e-04 - val_loss: 2.7634e-04
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7595e-04 - val_loss: 3.6313e-04
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4586e-04 - val_loss: 2.7866e-04
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4811e-04 - val_loss: 2.3104e-04
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5903e-04 - val_loss: 3.5479e-04
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.4757e-04 - val_loss: 2.6708e-04
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5001e-04 - val_loss: 2.3911e-04
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.4983e-04 - val_loss: 2.8952e-04
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00007
512/512 - 0s - loss: 3.1035e-04 - val_loss: 3.1224e-04
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9156e-04 - val_loss: 2.2582e-04
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6434e-04 - val_loss: 3.2631e-04
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9654e-04 - val_loss: 2.5747e-04
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5824e-04 - val_loss: 2.7286e-04
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9515e-04 - val_loss: 2.9077e-04
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8440e-04 - val_loss: 3.0794e-04
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8239e-04 - val_loss: 2.4996e-04
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5552e-04 - val_loss: 2.7711e-04
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.9103e-04 - val_loss: 3.0141e-04
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8813e-04 - val_loss: 2.7483e-04
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7167e-04 - val_loss: 2.6012e-04
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7978e-04 - val_loss: 2.5917e-04
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7603e-04 - val_loss: 3.1407e-04
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8583e-04 - val_loss: 2.5501e-04
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6674e-04 - val_loss: 2.8573e-04
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8681e-04 - val_loss: 2.9002e-04
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7289e-04 - val_loss: 2.6342e-04
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7604e-04 - val_loss: 2.8577e-04
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7477e-04 - val_loss: 2.6712e-04
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7909e-04 - val_loss: 2.7566e-04
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7305e-04 - val_loss: 2.8013e-04
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7771e-04 - val_loss: 2.8414e-04
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7879e-04 - val_loss: 2.7196e-04
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7020e-04 - val_loss: 2.6093e-04
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7203e-04 - val_loss: 2.8910e-04
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6951e-04 - val_loss: 2.7492e-04
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8692e-04 - val_loss: 2.9411e-04
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6757e-04 - val_loss: 2.6792e-04
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7708e-04 - val_loss: 2.8529e-04
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.8204e-04 - val_loss: 2.6748e-04
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6276e-04 - val_loss: 2.6523e-04
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7168e-04 - val_loss: 2.7661e-04
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7980e-04 - val_loss: 2.7996e-04
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7409e-04 - val_loss: 2.6739e-04
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6631e-04 - val_loss: 2.6485e-04
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7286e-04 - val_loss: 2.7159e-04
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7484e-04 - val_loss: 2.8196e-04
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7301e-04 - val_loss: 2.5852e-04
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6575e-04 - val_loss: 2.6833e-04
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7821e-04 - val_loss: 2.7937e-04
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6880e-04 - val_loss: 2.6051e-04
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6754e-04 - val_loss: 2.7593e-04
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7347e-04 - val_loss: 2.7142e-04
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7126e-04 - val_loss: 2.5260e-04
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6592e-04 - val_loss: 2.8794e-04
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7792e-04 - val_loss: 2.5948e-04
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6408e-04 - val_loss: 2.6521e-04
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7067e-04 - val_loss: 2.6847e-04
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6867e-04 - val_loss: 2.6567e-04
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7213e-04 - val_loss: 2.7000e-04
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6936e-04 - val_loss: 2.6712e-04
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6521e-04 - val_loss: 2.6652e-04
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6474e-04 - val_loss: 2.6661e-04
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7361e-04 - val_loss: 2.7525e-04
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7271e-04 - val_loss: 2.6570e-04
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6092e-04 - val_loss: 2.6356e-04
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7044e-04 - val_loss: 2.7087e-04
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7068e-04 - val_loss: 2.6140e-04
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6084e-04 - val_loss: 2.6487e-04
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6965e-04 - val_loss: 2.7468e-04
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7192e-04 - val_loss: 2.6291e-04
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6466e-04 - val_loss: 2.6297e-04
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6463e-04 - val_loss: 2.6662e-04
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6932e-04 - val_loss: 2.6784e-04
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6738e-04 - val_loss: 2.6333e-04
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6114e-04 - val_loss: 2.5731e-04
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6643e-04 - val_loss: 2.7124e-04
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7340e-04 - val_loss: 2.6887e-04
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6032e-04 - val_loss: 2.5286e-04
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6143e-04 - val_loss: 2.6886e-04
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.7165e-04 - val_loss: 2.6826e-04
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6453e-04 - val_loss: 2.5805e-04
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6007e-04 - val_loss: 2.6284e-04
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6589e-04 - val_loss: 2.6734e-04
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6678e-04 - val_loss: 2.6292e-04
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6369e-04 - val_loss: 2.6236e-04
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6429e-04 - val_loss: 2.6117e-04
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6240e-04 - val_loss: 2.6619e-04
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6595e-04 - val_loss: 2.6225e-04
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6324e-04 - val_loss: 2.6057e-04
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6132e-04 - val_loss: 2.6892e-04
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6487e-04 - val_loss: 2.6215e-04
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6401e-04 - val_loss: 2.6536e-04
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6450e-04 - val_loss: 2.5918e-04
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6124e-04 - val_loss: 2.6262e-04
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6416e-04 - val_loss: 2.6458e-04
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6358e-04 - val_loss: 2.6103e-04
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6474e-04 - val_loss: 2.6087e-04
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5916e-04 - val_loss: 2.5918e-04
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6079e-04 - val_loss: 2.6260e-04
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6481e-04 - val_loss: 2.6305e-04
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6253e-04 - val_loss: 2.6028e-04
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5978e-04 - val_loss: 2.5652e-04
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6225e-04 - val_loss: 2.6423e-04
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6342e-04 - val_loss: 2.5629e-04
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5566e-04 - val_loss: 2.6164e-04
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6475e-04 - val_loss: 2.6038e-04
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6193e-04 - val_loss: 2.5959e-04
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5898e-04 - val_loss: 2.5840e-04
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6044e-04 - val_loss: 2.5979e-04
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6199e-04 - val_loss: 2.6291e-04
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6267e-04 - val_loss: 2.5759e-04
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5829e-04 - val_loss: 2.5677e-04
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5961e-04 - val_loss: 2.5948e-04
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6232e-04 - val_loss: 2.5897e-04
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6082e-04 - val_loss: 2.6034e-04
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6033e-04 - val_loss: 2.5607e-04
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5712e-04 - val_loss: 2.5776e-04
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5881e-04 - val_loss: 2.6128e-04
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.6340e-04 - val_loss: 2.5811e-04
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5842e-04 - val_loss: 2.5515e-04
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5690e-04 - val_loss: 2.5713e-04
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5966e-04 - val_loss: 2.5943e-04
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5972e-04 - val_loss: 2.5693e-04
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5863e-04 - val_loss: 2.5566e-04
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5768e-04 - val_loss: 2.5793e-04
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00007
512/512 - 0s - loss: 2.5843e-04 - val_loss: 2.5787e-04
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.853 | eve: 9.553 | bob: 9.628Epoch   0:   0% | abe: 9.795 | eve: 9.544 | bob: 9.577Epoch   0:   1% | abe: 9.751 | eve: 9.530 | bob: 9.541Epoch   0:   2% | abe: 9.737 | eve: 9.537 | bob: 9.535Epoch   0:   3% | abe: 9.697 | eve: 9.528 | bob: 9.506Epoch   0:   3% | abe: 9.676 | eve: 9.527 | bob: 9.495Epoch   0:   4% | abe: 9.640 | eve: 9.530 | bob: 9.469Epoch   0:   5% | abe: 9.619 | eve: 9.526 | bob: 9.458Epoch   0:   6% | abe: 9.604 | eve: 9.533 | bob: 9.452Epoch   0:   7% | abe: 9.588 | eve: 9.542 | bob: 9.443Epoch   0:   7% | abe: 9.576 | eve: 9.545 | bob: 9.438Epoch   0:   8% | abe: 9.560 | eve: 9.545 | bob: 9.428Epoch   0:   9% | abe: 9.541 | eve: 9.555 | bob: 9.414Epoch   0:  10% | abe: 9.527 | eve: 9.555 | bob: 9.406Epoch   0:  10% | abe: 9.515 | eve: 9.563 | bob: 9.397Epoch   0:  11% | abe: 9.500 | eve: 9.558 | bob: 9.387Epoch   0:  12% | abe: 9.486 | eve: 9.556 | bob: 9.376Epoch   0:  13% | abe: 9.477 | eve: 9.556 | bob: 9.371Epoch   0:  14% | abe: 9.470 | eve: 9.555 | bob: 9.366Epoch   0:  14% | abe: 9.457 | eve: 9.553 | bob: 9.356Epoch   0:  15% | abe: 9.449 | eve: 9.552 | bob: 9.351Epoch   0:  16% | abe: 9.436 | eve: 9.554 | bob: 9.340Epoch   0:  17% | abe: 9.425 | eve: 9.551 | bob: 9.331Epoch   0:  17% | abe: 9.414 | eve: 9.549 | bob: 9.322Epoch   0:  18% | abe: 9.407 | eve: 9.547 | bob: 9.317Epoch   0:  19% | abe: 9.399 | eve: 9.549 | bob: 9.311Epoch   0:  20% | abe: 9.392 | eve: 9.549 | bob: 9.305Epoch   0:  21% | abe: 9.383 | eve: 9.551 | bob: 9.297Epoch   0:  21% | abe: 9.375 | eve: 9.549 | bob: 9.291Epoch   0:  22% | abe: 9.368 | eve: 9.548 | bob: 9.285Epoch   0:  23% | abe: 9.359 | eve: 9.552 | bob: 9.278Epoch   0:  24% | abe: 9.352 | eve: 9.551 | bob: 9.272Epoch   0:  25% | abe: 9.347 | eve: 9.552 | bob: 9.268Epoch   0:  25% | abe: 9.341 | eve: 9.554 | bob: 9.262Epoch   0:  26% | abe: 9.332 | eve: 9.555 | bob: 9.255Epoch   0:  27% | abe: 9.326 | eve: 9.553 | bob: 9.249Epoch   0:  28% | abe: 9.319 | eve: 9.553 | bob: 9.243Epoch   0:  28% | abe: 9.314 | eve: 9.553 | bob: 9.239Epoch   0:  29% | abe: 9.308 | eve: 9.555 | bob: 9.234Epoch   0:  30% | abe: 9.301 | eve: 9.555 | bob: 9.228Epoch   0:  31% | abe: 9.296 | eve: 9.556 | bob: 9.224Epoch   0:  32% | abe: 9.291 | eve: 9.555 | bob: 9.219Epoch   0:  32% | abe: 9.288 | eve: 9.556 | bob: 9.217Epoch   0:  33% | abe: 9.283 | eve: 9.557 | bob: 9.213Epoch   0:  34% | abe: 9.278 | eve: 9.557 | bob: 9.208Epoch   0:  35% | abe: 9.272 | eve: 9.557 | bob: 9.203Epoch   0:  35% | abe: 9.268 | eve: 9.558 | bob: 9.200Epoch   0:  36% | abe: 9.264 | eve: 9.557 | bob: 9.196Epoch   0:  37% | abe: 9.260 | eve: 9.559 | bob: 9.193Epoch   0:  38% | abe: 9.257 | eve: 9.558 | bob: 9.190Epoch   0:  39% | abe: 9.253 | eve: 9.558 | bob: 9.187Epoch   0:  39% | abe: 9.251 | eve: 9.559 | bob: 9.185Epoch   0:  40% | abe: 9.248 | eve: 9.560 | bob: 9.182Epoch   0:  41% | abe: 9.246 | eve: 9.559 | bob: 9.181Epoch   0:  42% | abe: 9.244 | eve: 9.559 | bob: 9.179Epoch   0:  42% | abe: 9.242 | eve: 9.560 | bob: 9.177Epoch   0:  43% | abe: 9.238 | eve: 9.560 | bob: 9.174Epoch   0:  44% | abe: 9.235 | eve: 9.559 | bob: 9.171Epoch   0:  45% | abe: 9.232 | eve: 9.559 | bob: 9.169Epoch   0:  46% | abe: 9.230 | eve: 9.561 | bob: 9.166Epoch   0:  46% | abe: 9.227 | eve: 9.561 | bob: 9.164Epoch   0:  47% | abe: 9.225 | eve: 9.561 | bob: 9.162Epoch   0:  48% | abe: 9.223 | eve: 9.561 | bob: 9.160Epoch   0:  49% | abe: 9.221 | eve: 9.561 | bob: 9.159Epoch   0:  50% | abe: 9.218 | eve: 9.562 | bob: 9.156Epoch   0:  50% | abe: 9.216 | eve: 9.562 | bob: 9.154Epoch   0:  51% | abe: 9.214 | eve: 9.562 | bob: 9.152Epoch   0:  52% | abe: 9.211 | eve: 9.561 | bob: 9.150Epoch   0:  53% | abe: 9.210 | eve: 9.560 | bob: 9.148Epoch   0:  53% | abe: 9.208 | eve: 9.562 | bob: 9.147Epoch   0:  54% | abe: 9.207 | eve: 9.562 | bob: 9.146Epoch   0:  55% | abe: 9.205 | eve: 9.563 | bob: 9.144Epoch   0:  56% | abe: 9.203 | eve: 9.563 | bob: 9.142Epoch   0:  57% | abe: 9.201 | eve: 9.564 | bob: 9.140Epoch   0:  57% | abe: 9.199 | eve: 9.564 | bob: 9.138Epoch   0:  58% | abe: 9.197 | eve: 9.564 | bob: 9.137Epoch   0:  59% | abe: 9.195 | eve: 9.564 | bob: 9.135Epoch   0:  60% | abe: 9.193 | eve: 9.565 | bob: 9.133Epoch   0:  60% | abe: 9.193 | eve: 9.565 | bob: 9.133Epoch   0:  61% | abe: 9.191 | eve: 9.566 | bob: 9.131Epoch   0:  62% | abe: 9.190 | eve: 9.566 | bob: 9.130Epoch   0:  63% | abe: 9.188 | eve: 9.565 | bob: 9.129Epoch   0:  64% | abe: 9.187 | eve: 9.565 | bob: 9.128Epoch   0:  64% | abe: 9.186 | eve: 9.564 | bob: 9.126Epoch   0:  65% | abe: 9.185 | eve: 9.565 | bob: 9.126Epoch   0:  66% | abe: 9.183 | eve: 9.565 | bob: 9.124Epoch   0:  67% | abe: 9.182 | eve: 9.565 | bob: 9.123Epoch   0:  67% | abe: 9.180 | eve: 9.566 | bob: 9.122Epoch   0:  68% | abe: 9.180 | eve: 9.566 | bob: 9.121Epoch   0:  69% | abe: 9.179 | eve: 9.567 | bob: 9.120Epoch   0:  70% | abe: 9.178 | eve: 9.567 | bob: 9.119Epoch   0:  71% | abe: 9.177 | eve: 9.567 | bob: 9.118Epoch   0:  71% | abe: 9.175 | eve: 9.568 | bob: 9.117Epoch   0:  72% | abe: 9.174 | eve: 9.569 | bob: 9.115Epoch   0:  73% | abe: 9.172 | eve: 9.569 | bob: 9.114Epoch   0:  74% | abe: 9.171 | eve: 9.570 | bob: 9.113Epoch   0:  75% | abe: 9.170 | eve: 9.570 | bob: 9.112Epoch   0:  75% | abe: 9.169 | eve: 9.570 | bob: 9.111Epoch   0:  76% | abe: 9.168 | eve: 9.570 | bob: 9.110Epoch   0:  77% | abe: 9.167 | eve: 9.570 | bob: 9.109Epoch   0:  78% | abe: 9.166 | eve: 9.571 | bob: 9.109Epoch   0:  78% | abe: 9.165 | eve: 9.571 | bob: 9.108Epoch   0:  79% | abe: 9.164 | eve: 9.572 | bob: 9.107Epoch   0:  80% | abe: 9.163 | eve: 9.571 | bob: 9.106Epoch   0:  81% | abe: 9.162 | eve: 9.571 | bob: 9.105Epoch   0:  82% | abe: 9.161 | eve: 9.571 | bob: 9.104Epoch   0:  82% | abe: 9.161 | eve: 9.571 | bob: 9.103Epoch   0:  83% | abe: 9.160 | eve: 9.572 | bob: 9.103Epoch   0:  84% | abe: 9.159 | eve: 9.572 | bob: 9.102Epoch   0:  85% | abe: 9.159 | eve: 9.572 | bob: 9.102Epoch   0:  85% | abe: 9.158 | eve: 9.572 | bob: 9.101Epoch   0:  86% | abe: 9.157 | eve: 9.573 | bob: 9.100Epoch   0:  87% | abe: 9.156 | eve: 9.573 | bob: 9.099Epoch   0:  88% | abe: 9.156 | eve: 9.574 | bob: 9.099Epoch   0:  89% | abe: 9.155 | eve: 9.574 | bob: 9.099Epoch   0:  89% | abe: 9.155 | eve: 9.575 | bob: 9.098Epoch   0:  90% | abe: 9.155 | eve: 9.575 | bob: 9.098Epoch   0:  91% | abe: 9.154 | eve: 9.575 | bob: 9.097Epoch   0:  92% | abe: 9.153 | eve: 9.575 | bob: 9.096Epoch   0:  92% | abe: 9.152 | eve: 9.575 | bob: 9.095Epoch   0:  93% | abe: 9.151 | eve: 9.575 | bob: 9.095Epoch   0:  94% | abe: 9.151 | eve: 9.575 | bob: 9.095Epoch   0:  95% | abe: 9.151 | eve: 9.575 | bob: 9.095Epoch   0:  96% | abe: 9.151 | eve: 9.575 | bob: 9.095Epoch   0:  96% | abe: 9.150 | eve: 9.575 | bob: 9.094Epoch   0:  97% | abe: 9.149 | eve: 9.575 | bob: 9.093Epoch   0:  98% | abe: 9.148 | eve: 9.576 | bob: 9.092Epoch   0:  99% | abe: 9.147 | eve: 9.576 | bob: 9.092
New best Bob loss 9.091698417890711 at epoch 0
Epoch   1:   0% | abe: 9.042 | eve: 9.654 | bob: 8.991Epoch   1:   0% | abe: 9.074 | eve: 9.610 | bob: 9.025Epoch   1:   1% | abe: 9.077 | eve: 9.623 | bob: 9.029Epoch   1:   2% | abe: 9.088 | eve: 9.618 | bob: 9.039Epoch   1:   3% | abe: 9.087 | eve: 9.608 | bob: 9.038Epoch   1:   3% | abe: 9.080 | eve: 9.609 | bob: 9.031Epoch   1:   4% | abe: 9.083 | eve: 9.598 | bob: 9.034Epoch   1:   5% | abe: 9.073 | eve: 9.594 | bob: 9.024