WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-09 17:44:04.951672: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-09 17:44:05.210931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-09 17:44:05.211846: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 17:44:05.214481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-09 17:44:05.216636: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-09 17:44:05.217494: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-09 17:44:05.221184: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-09 17:44:05.224686: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-09 17:44:05.234231: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-09 17:44:05.244594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-09 17:44:05.245620: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-09 17:44:05.287350: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-09 17:44:05.291508: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x39309b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-09 17:44:05.291560: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-09 17:44:06.011402: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x13b20d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-09 17:44:06.011490: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-09 17:44:06.015122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-09 17:44:06.015312: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 17:44:06.015387: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-09 17:44:06.015425: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-09 17:44:06.015464: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-09 17:44:06.015506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-09 17:44:06.015545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-09 17:44:06.015588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-09 17:44:06.021569: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-09 17:44:06.021854: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 17:44:06.029072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-09 17:44:06.029566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-09 17:44:06.029659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-09 17:44:06.037898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-09 17:44:11.457808: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.8247 - val_loss: 0.0047
Epoch 2/512
512/512 - 0s - loss: 0.3060 - val_loss: 0.0012
Epoch 3/512
512/512 - 0s - loss: 0.0721 - val_loss: 2.4994e-04
Epoch 4/512
512/512 - 0s - loss: 0.0171 - val_loss: 9.8292e-05
Epoch 5/512
512/512 - 0s - loss: 0.0085 - val_loss: 6.6796e-05
Epoch 6/512
512/512 - 0s - loss: 0.0059 - val_loss: 4.5832e-05
Epoch 7/512
512/512 - 0s - loss: 0.0039 - val_loss: 2.8711e-05
Epoch 8/512
512/512 - 0s - loss: 0.0024 - val_loss: 1.6019e-05
Epoch 9/512
512/512 - 0s - loss: 0.0013 - val_loss: 7.7398e-06
Epoch 10/512
512/512 - 0s - loss: 5.7635e-04 - val_loss: 3.1190e-06
Epoch 11/512
512/512 - 0s - loss: 2.1762e-04 - val_loss: 9.9808e-07
Epoch 12/512
512/512 - 0s - loss: 6.4406e-05 - val_loss: 2.3780e-07
Epoch 13/512
512/512 - 0s - loss: 1.3988e-05 - val_loss: 3.8699e-08
Epoch 14/512
512/512 - 0s - loss: 2.0470e-06 - val_loss: 3.8686e-09
Epoch 15/512
512/512 - 0s - loss: 2.5423e-07 - val_loss: 1.3741e-08
Epoch 16/512
512/512 - 0s - loss: 1.6632e-04 - val_loss: 4.3990e-05
Epoch 17/512
512/512 - 0s - loss: 0.0055 - val_loss: 7.0398e-06
Epoch 18/512
512/512 - 0s - loss: 3.3688e-04 - val_loss: 1.0005e-06
Epoch 19/512
512/512 - 0s - loss: 1.2939e-04 - val_loss: 3.1034e-06
Epoch 20/512
512/512 - 0s - loss: 0.0012 - val_loss: 3.3433e-05
Epoch 21/512
512/512 - 0s - loss: 0.0028 - val_loss: 8.9173e-06
Epoch 22/512
512/512 - 0s - loss: 6.1980e-04 - val_loss: 4.3889e-06
Epoch 23/512
512/512 - 0s - loss: 6.6053e-04 - val_loss: 1.2933e-05
Epoch 24/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.7142e-05
Epoch 25/512
512/512 - 0s - loss: 0.0013 - val_loss: 7.6493e-06
Epoch 26/512
512/512 - 0s - loss: 7.8232e-04 - val_loss: 9.1742e-06
Epoch 27/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.5027e-05
Epoch 28/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.0723e-05
Epoch 29/512
512/512 - 0s - loss: 9.9173e-04 - val_loss: 8.7664e-06
Epoch 30/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.1715e-05
Epoch 31/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.1807e-05
Epoch 32/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.4502e-06
Epoch 33/512
512/512 - 0s - loss: 9.8241e-04 - val_loss: 1.0030e-05
Epoch 34/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1056e-05
Epoch 35/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.9056e-06
Epoch 36/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.4309e-06
Epoch 37/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0109e-05
Epoch 38/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0144e-05
Epoch 39/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.1625e-06
Epoch 40/512
512/512 - 0s - loss: 9.4212e-04 - val_loss: 9.3257e-06
Epoch 41/512
512/512 - 0s - loss: 9.9313e-04 - val_loss: 9.7083e-06
Epoch 42/512
512/512 - 0s - loss: 9.8782e-04 - val_loss: 9.2967e-06
Epoch 43/512
512/512 - 0s - loss: 9.4501e-04 - val_loss: 8.9990e-06
Epoch 44/512
512/512 - 0s - loss: 9.3249e-04 - val_loss: 9.1320e-06
Epoch 45/512
512/512 - 0s - loss: 9.4231e-04 - val_loss: 9.0132e-06
Epoch 46/512
512/512 - 0s - loss: 9.2774e-04 - val_loss: 8.7160e-06
Epoch 47/512
512/512 - 0s - loss: 8.9523e-04 - val_loss: 8.7220e-06
Epoch 48/512
512/512 - 0s - loss: 9.0454e-04 - val_loss: 8.8015e-06
Epoch 49/512
512/512 - 0s - loss: 9.0182e-04 - val_loss: 8.4750e-06
Epoch 50/512
512/512 - 0s - loss: 8.6118e-04 - val_loss: 8.3938e-06
Epoch 51/512
512/512 - 0s - loss: 8.6707e-04 - val_loss: 8.6400e-06
Epoch 52/512
512/512 - 0s - loss: 8.7883e-04 - val_loss: 8.3310e-06
Epoch 53/512
512/512 - 0s - loss: 8.4163e-04 - val_loss: 8.0810e-06
Epoch 54/512
512/512 - 0s - loss: 8.3163e-04 - val_loss: 8.1665e-06
Epoch 55/512
512/512 - 0s - loss: 8.3164e-04 - val_loss: 8.2862e-06
Epoch 56/512
512/512 - 0s - loss: 8.3566e-04 - val_loss: 8.0652e-06
Epoch 57/512
512/512 - 0s - loss: 8.1278e-04 - val_loss: 7.7466e-06
Epoch 58/512
512/512 - 0s - loss: 7.9234e-04 - val_loss: 7.8586e-06
Epoch 59/512
512/512 - 0s - loss: 8.0813e-04 - val_loss: 7.8104e-06
Epoch 60/512
512/512 - 0s - loss: 7.8701e-04 - val_loss: 7.6651e-06
Epoch 61/512
512/512 - 0s - loss: 7.7930e-04 - val_loss: 7.6358e-06
Epoch 62/512
512/512 - 0s - loss: 7.7560e-04 - val_loss: 7.5004e-06
Epoch 63/512
512/512 - 0s - loss: 7.6507e-04 - val_loss: 7.4355e-06
Epoch 64/512
512/512 - 0s - loss: 7.5850e-04 - val_loss: 7.3511e-06
Epoch 65/512
512/512 - 0s - loss: 7.4853e-04 - val_loss: 7.3279e-06
Epoch 66/512
512/512 - 0s - loss: 7.4250e-04 - val_loss: 7.3411e-06
Epoch 67/512
512/512 - 0s - loss: 7.4324e-04 - val_loss: 7.1941e-06
Epoch 68/512
512/512 - 0s - loss: 7.2831e-04 - val_loss: 7.0510e-06
Epoch 69/512
512/512 - 0s - loss: 7.1936e-04 - val_loss: 7.0611e-06
Epoch 70/512
512/512 - 0s - loss: 7.1790e-04 - val_loss: 7.0725e-06
Epoch 71/512
512/512 - 0s - loss: 7.1705e-04 - val_loss: 6.8545e-06
Epoch 72/512
512/512 - 0s - loss: 6.9452e-04 - val_loss: 6.8349e-06
Epoch 73/512
512/512 - 0s - loss: 6.9426e-04 - val_loss: 6.9605e-06
Epoch 74/512
512/512 - 0s - loss: 7.0994e-04 - val_loss: 6.6370e-06
Epoch 75/512
512/512 - 0s - loss: 6.7002e-04 - val_loss: 6.5889e-06
Epoch 76/512
512/512 - 0s - loss: 6.7516e-04 - val_loss: 6.7962e-06
Epoch 77/512
512/512 - 0s - loss: 6.8622e-04 - val_loss: 6.6561e-06
Epoch 78/512
512/512 - 0s - loss: 6.7032e-04 - val_loss: 6.3432e-06
Epoch 79/512
512/512 - 0s - loss: 6.4791e-04 - val_loss: 6.4174e-06
Epoch 80/512
512/512 - 0s - loss: 6.5695e-04 - val_loss: 6.6718e-06
Epoch 81/512
512/512 - 0s - loss: 6.6721e-04 - val_loss: 6.4175e-06
Epoch 82/512
512/512 - 0s - loss: 6.3863e-04 - val_loss: 6.2102e-06
Epoch 83/512
512/512 - 0s - loss: 6.3459e-04 - val_loss: 6.3081e-06
Epoch 84/512
512/512 - 0s - loss: 6.3856e-04 - val_loss: 6.3675e-06
Epoch 85/512
512/512 - 0s - loss: 6.4061e-04 - val_loss: 6.1227e-06
Epoch 86/512
512/512 - 0s - loss: 6.1474e-04 - val_loss: 6.0723e-06
Epoch 87/512
512/512 - 0s - loss: 6.1999e-04 - val_loss: 6.2061e-06
Epoch 88/512
512/512 - 0s - loss: 6.2423e-04 - val_loss: 6.0680e-06
Epoch 89/512
512/512 - 0s - loss: 6.0406e-04 - val_loss: 6.0204e-06
Epoch 90/512
512/512 - 0s - loss: 6.0621e-04 - val_loss: 6.0524e-06
Epoch 91/512
512/512 - 0s - loss: 6.0684e-04 - val_loss: 5.9583e-06
Epoch 92/512
512/512 - 0s - loss: 5.9615e-04 - val_loss: 5.8139e-06
Epoch 93/512
512/512 - 0s - loss: 5.9161e-04 - val_loss: 5.7822e-06
Epoch 94/512
512/512 - 0s - loss: 5.8329e-04 - val_loss: 5.8181e-06
Epoch 95/512
512/512 - 0s - loss: 5.9025e-04 - val_loss: 5.7496e-06
Epoch 96/512
512/512 - 0s - loss: 5.8008e-04 - val_loss: 5.5886e-06
Epoch 97/512
512/512 - 0s - loss: 5.6162e-04 - val_loss: 5.7572e-06
Epoch 98/512
512/512 - 0s - loss: 5.8359e-04 - val_loss: 5.7268e-06
Epoch 99/512
512/512 - 0s - loss: 5.6829e-04 - val_loss: 5.4503e-06
Epoch 100/512
512/512 - 0s - loss: 5.5173e-04 - val_loss: 5.4763e-06
Epoch 101/512
512/512 - 0s - loss: 5.5913e-04 - val_loss: 5.5301e-06
Epoch 102/512
512/512 - 0s - loss: 5.5556e-04 - val_loss: 5.4893e-06
Epoch 103/512
512/512 - 0s - loss: 5.4447e-04 - val_loss: 5.5356e-06
Epoch 104/512
512/512 - 0s - loss: 5.5329e-04 - val_loss: 5.4243e-06
Epoch 105/512
512/512 - 0s - loss: 5.4207e-04 - val_loss: 5.2689e-06
Epoch 106/512
512/512 - 0s - loss: 5.2748e-04 - val_loss: 5.3651e-06
Epoch 107/512
512/512 - 0s - loss: 5.4013e-04 - val_loss: 5.3792e-06
Epoch 108/512
512/512 - 0s - loss: 5.3877e-04 - val_loss: 5.0677e-06
Epoch 109/512
512/512 - 0s - loss: 5.0563e-04 - val_loss: 5.2101e-06
Epoch 110/512
512/512 - 0s - loss: 5.3129e-04 - val_loss: 5.3347e-06
Epoch 111/512
512/512 - 0s - loss: 5.2943e-04 - val_loss: 5.0052e-06
Epoch 112/512
512/512 - 0s - loss: 5.0442e-04 - val_loss: 4.8756e-06
Epoch 113/512
512/512 - 0s - loss: 5.0321e-04 - val_loss: 5.0625e-06
Epoch 114/512
512/512 - 0s - loss: 5.1751e-04 - val_loss: 5.0292e-06
Epoch 115/512
512/512 - 0s - loss: 5.0017e-04 - val_loss: 4.8914e-06
Epoch 116/512
512/512 - 0s - loss: 4.9386e-04 - val_loss: 4.9565e-06
Epoch 117/512
512/512 - 0s - loss: 4.9933e-04 - val_loss: 4.9498e-06
Epoch 118/512
512/512 - 0s - loss: 4.9348e-04 - val_loss: 4.8858e-06
Epoch 119/512
512/512 - 0s - loss: 4.8702e-04 - val_loss: 4.8323e-06
Epoch 120/512
512/512 - 0s - loss: 4.8597e-04 - val_loss: 4.7868e-06
Epoch 121/512
512/512 - 0s - loss: 4.8035e-04 - val_loss: 4.7539e-06
Epoch 122/512
512/512 - 0s - loss: 4.8170e-04 - val_loss: 4.6793e-06
Epoch 123/512
512/512 - 0s - loss: 4.6867e-04 - val_loss: 4.6783e-06
Epoch 124/512
512/512 - 0s - loss: 4.7226e-04 - val_loss: 4.6937e-06
Epoch 125/512
512/512 - 0s - loss: 4.7064e-04 - val_loss: 4.5913e-06
Epoch 126/512
512/512 - 0s - loss: 4.6152e-04 - val_loss: 4.5411e-06
Epoch 127/512
512/512 - 0s - loss: 4.5552e-04 - val_loss: 4.6478e-06
Epoch 128/512
512/512 - 0s - loss: 4.6713e-04 - val_loss: 4.5499e-06
Epoch 129/512
512/512 - 0s - loss: 4.4991e-04 - val_loss: 4.4320e-06
Epoch 130/512
512/512 - 0s - loss: 4.4761e-04 - val_loss: 4.4766e-06
Epoch 131/512
512/512 - 0s - loss: 4.4973e-04 - val_loss: 4.4497e-06
Epoch 132/512
512/512 - 0s - loss: 4.4333e-04 - val_loss: 4.3956e-06
Epoch 133/512
512/512 - 0s - loss: 4.4123e-04 - val_loss: 4.3628e-06
Epoch 134/512
512/512 - 0s - loss: 4.3566e-04 - val_loss: 4.3409e-06
Epoch 135/512
512/512 - 0s - loss: 4.3484e-04 - val_loss: 4.3261e-06
Epoch 136/512
512/512 - 0s - loss: 4.3242e-04 - val_loss: 4.2841e-06
Epoch 137/512
512/512 - 0s - loss: 4.2840e-04 - val_loss: 4.1818e-06
Epoch 138/512
512/512 - 0s - loss: 4.2013e-04 - val_loss: 4.2134e-06
Epoch 139/512
512/512 - 0s - loss: 4.2384e-04 - val_loss: 4.2307e-06
Epoch 140/512
512/512 - 0s - loss: 4.1914e-04 - val_loss: 4.1680e-06
Epoch 141/512
512/512 - 0s - loss: 4.1677e-04 - val_loss: 4.0648e-06
Epoch 142/512
512/512 - 0s - loss: 4.1034e-04 - val_loss: 3.9929e-06
Epoch 143/512
512/512 - 0s - loss: 4.0483e-04 - val_loss: 4.0751e-06
Epoch 144/512
512/512 - 0s - loss: 4.0995e-04 - val_loss: 4.0832e-06
Epoch 145/512
512/512 - 0s - loss: 4.0529e-04 - val_loss: 3.9626e-06
Epoch 146/512
512/512 - 0s - loss: 3.9630e-04 - val_loss: 3.9115e-06
Epoch 147/512
512/512 - 0s - loss: 3.9541e-04 - val_loss: 3.9599e-06
Epoch 148/512
512/512 - 0s - loss: 3.9622e-04 - val_loss: 3.9611e-06
Epoch 149/512
512/512 - 0s - loss: 3.9384e-04 - val_loss: 3.8652e-06
Epoch 150/512
512/512 - 0s - loss: 3.8547e-04 - val_loss: 3.8256e-06
Epoch 151/512
512/512 - 0s - loss: 3.8652e-04 - val_loss: 3.7904e-06
Epoch 152/512
512/512 - 0s - loss: 3.8085e-04 - val_loss: 3.7701e-06
Epoch 153/512
512/512 - 0s - loss: 3.7873e-04 - val_loss: 3.7657e-06
Epoch 154/512
512/512 - 0s - loss: 3.7772e-04 - val_loss: 3.7465e-06
Epoch 155/512
512/512 - 0s - loss: 3.7404e-04 - val_loss: 3.7316e-06
Epoch 156/512
512/512 - 0s - loss: 3.7394e-04 - val_loss: 3.6355e-06
Epoch 157/512
512/512 - 0s - loss: 3.6448e-04 - val_loss: 3.5968e-06
Epoch 158/512
512/512 - 0s - loss: 3.6379e-04 - val_loss: 3.6504e-06
Epoch 159/512
512/512 - 0s - loss: 3.6682e-04 - val_loss: 3.6290e-06
Epoch 160/512
512/512 - 0s - loss: 3.6249e-04 - val_loss: 3.4859e-06
Epoch 161/512
512/512 - 0s - loss: 3.5012e-04 - val_loss: 3.5242e-06
Epoch 162/512
512/512 - 0s - loss: 3.5840e-04 - val_loss: 3.5473e-06
Epoch 163/512
512/512 - 0s - loss: 3.5435e-04 - val_loss: 3.4693e-06
Epoch 164/512
512/512 - 0s - loss: 3.4415e-04 - val_loss: 3.4734e-06
Epoch 165/512
512/512 - 0s - loss: 3.5131e-04 - val_loss: 3.4151e-06
Epoch 166/512
512/512 - 0s - loss: 3.4375e-04 - val_loss: 3.3643e-06
Epoch 167/512
512/512 - 0s - loss: 3.3862e-04 - val_loss: 3.3836e-06
Epoch 168/512
512/512 - 0s - loss: 3.3902e-04 - val_loss: 3.4370e-06
Epoch 169/512
512/512 - 0s - loss: 3.3820e-04 - val_loss: 3.4081e-06
Epoch 170/512
512/512 - 0s - loss: 3.3858e-04 - val_loss: 3.2563e-06
Epoch 171/512
512/512 - 0s - loss: 3.2427e-04 - val_loss: 3.2729e-06
Epoch 172/512
512/512 - 0s - loss: 3.3145e-04 - val_loss: 3.3134e-06
Epoch 173/512
512/512 - 0s - loss: 3.3100e-04 - val_loss: 3.2086e-06
Epoch 174/512
512/512 - 0s - loss: 3.2121e-04 - val_loss: 3.1558e-06
Epoch 175/512
512/512 - 0s - loss: 3.1972e-04 - val_loss: 3.1976e-06
Epoch 176/512
512/512 - 0s - loss: 3.2378e-04 - val_loss: 3.1380e-06
Epoch 177/512
512/512 - 0s - loss: 3.1255e-04 - val_loss: 3.1375e-06
Epoch 178/512
512/512 - 0s - loss: 3.1830e-04 - val_loss: 3.1047e-06
Epoch 179/512
512/512 - 0s - loss: 3.0836e-04 - val_loss: 3.0981e-06
Epoch 180/512
512/512 - 0s - loss: 3.1217e-04 - val_loss: 3.1196e-06
Epoch 181/512
512/512 - 0s - loss: 3.0837e-04 - val_loss: 3.0784e-06
Epoch 182/512
512/512 - 0s - loss: 3.0496e-04 - val_loss: 3.0570e-06
Epoch 183/512
512/512 - 0s - loss: 3.0537e-04 - val_loss: 2.9804e-06
Epoch 184/512
512/512 - 0s - loss: 2.9744e-04 - val_loss: 2.9816e-06
Epoch 185/512
512/512 - 0s - loss: 3.0064e-04 - val_loss: 2.9869e-06
Epoch 186/512
512/512 - 0s - loss: 2.9661e-04 - val_loss: 2.9472e-06
Epoch 187/512
512/512 - 0s - loss: 2.9493e-04 - val_loss: 2.9171e-06
Epoch 188/512
512/512 - 0s - loss: 2.9149e-04 - val_loss: 2.8949e-06
Epoch 189/512
512/512 - 0s - loss: 2.8961e-04 - val_loss: 2.8874e-06
Epoch 190/512
512/512 - 0s - loss: 2.8921e-04 - val_loss: 2.8554e-06
Epoch 191/512
512/512 - 0s - loss: 2.8535e-04 - val_loss: 2.8394e-06
Epoch 192/512
512/512 - 0s - loss: 2.8356e-04 - val_loss: 2.8189e-06
Epoch 193/512
512/512 - 0s - loss: 2.8065e-04 - val_loss: 2.8230e-06
Epoch 194/512
512/512 - 0s - loss: 2.8138e-04 - val_loss: 2.7931e-06
Epoch 195/512
512/512 - 0s - loss: 2.7872e-04 - val_loss: 2.7224e-06
Epoch 196/512
512/512 - 0s - loss: 2.7294e-04 - val_loss: 2.7245e-06
Epoch 197/512
512/512 - 0s - loss: 2.7270e-04 - val_loss: 2.7576e-06
Epoch 198/512
512/512 - 0s - loss: 2.7710e-04 - val_loss: 2.6535e-06
Epoch 199/512
512/512 - 0s - loss: 2.6480e-04 - val_loss: 2.6201e-06
Epoch 200/512
512/512 - 0s - loss: 2.6734e-04 - val_loss: 2.6704e-06
Epoch 201/512
512/512 - 0s - loss: 2.6689e-04 - val_loss: 2.6542e-06
Epoch 202/512
512/512 - 0s - loss: 2.6473e-04 - val_loss: 2.5972e-06
Epoch 203/512
512/512 - 0s - loss: 2.6006e-04 - val_loss: 2.5734e-06
Epoch 204/512
512/512 - 0s - loss: 2.5884e-04 - val_loss: 2.6210e-06
Epoch 205/512
512/512 - 0s - loss: 2.6255e-04 - val_loss: 2.5384e-06
Epoch 206/512
512/512 - 0s - loss: 2.5315e-04 - val_loss: 2.4794e-06
Epoch 207/512
512/512 - 0s - loss: 2.5064e-04 - val_loss: 2.5603e-06
Epoch 208/512
512/512 - 0s - loss: 2.5783e-04 - val_loss: 2.5340e-06
Epoch 209/512
512/512 - 0s - loss: 2.4843e-04 - val_loss: 2.4791e-06
Epoch 210/512
512/512 - 0s - loss: 2.4869e-04 - val_loss: 2.4562e-06
Epoch 211/512
512/512 - 0s - loss: 2.4683e-04 - val_loss: 2.4426e-06
Epoch 212/512
512/512 - 0s - loss: 2.4354e-04 - val_loss: 2.4600e-06
Epoch 213/512
512/512 - 0s - loss: 2.4608e-04 - val_loss: 2.4329e-06
Epoch 214/512
512/512 - 0s - loss: 2.4228e-04 - val_loss: 2.3716e-06
Epoch 215/512
512/512 - 0s - loss: 2.3815e-04 - val_loss: 2.3422e-06
Epoch 216/512
512/512 - 0s - loss: 2.3454e-04 - val_loss: 2.4141e-06
Epoch 217/512
512/512 - 0s - loss: 2.4303e-04 - val_loss: 2.3497e-06
Epoch 218/512
512/512 - 0s - loss: 2.2988e-04 - val_loss: 2.3017e-06
Epoch 219/512
512/512 - 0s - loss: 2.3292e-04 - val_loss: 2.3325e-06
Epoch 220/512
512/512 - 0s - loss: 2.3156e-04 - val_loss: 2.3307e-06
Epoch 221/512
512/512 - 0s - loss: 2.3254e-04 - val_loss: 2.2489e-06
Epoch 222/512
512/512 - 0s - loss: 2.2318e-04 - val_loss: 2.2460e-06
Epoch 223/512
512/512 - 0s - loss: 2.2676e-04 - val_loss: 2.2858e-06
Epoch 224/512
512/512 - 0s - loss: 2.2814e-04 - val_loss: 2.1992e-06
Epoch 225/512
512/512 - 0s - loss: 2.1773e-04 - val_loss: 2.1939e-06
Epoch 226/512
512/512 - 0s - loss: 2.2220e-04 - val_loss: 2.2449e-06
Epoch 227/512
512/512 - 0s - loss: 2.2309e-04 - val_loss: 2.1391e-06
Epoch 228/512
512/512 - 0s - loss: 2.1442e-04 - val_loss: 2.0997e-06
Epoch 229/512
512/512 - 0s - loss: 2.1311e-04 - val_loss: 2.1990e-06
Epoch 230/512
512/512 - 0s - loss: 2.1841e-04 - val_loss: 2.1882e-06
Epoch 231/512
512/512 - 0s - loss: 2.1524e-04 - val_loss: 2.0724e-06
Epoch 232/512
512/512 - 0s - loss: 2.0711e-04 - val_loss: 2.0519e-06
Epoch 233/512
512/512 - 0s - loss: 2.0796e-04 - val_loss: 2.1307e-06
Epoch 234/512
512/512 - 0s - loss: 2.1230e-04 - val_loss: 2.1006e-06
Epoch 235/512
512/512 - 0s - loss: 2.0682e-04 - val_loss: 1.9963e-06
Epoch 236/512
512/512 - 0s - loss: 2.0049e-04 - val_loss: 2.0253e-06
Epoch 237/512
512/512 - 0s - loss: 2.0545e-04 - val_loss: 2.0518e-06
Epoch 238/512
512/512 - 0s - loss: 2.0383e-04 - val_loss: 1.9819e-06
Epoch 239/512
512/512 - 0s - loss: 1.9727e-04 - val_loss: 1.9684e-06
Epoch 240/512
512/512 - 0s - loss: 1.9821e-04 - val_loss: 2.0063e-06
Epoch 241/512
512/512 - 0s - loss: 1.9864e-04 - val_loss: 1.9897e-06
Epoch 242/512
512/512 - 0s - loss: 1.9627e-04 - val_loss: 1.9363e-06
Epoch 243/512
512/512 - 0s - loss: 1.9371e-04 - val_loss: 1.8858e-06
Epoch 244/512
512/512 - 0s - loss: 1.8995e-04 - val_loss: 1.9104e-06
Epoch 245/512
512/512 - 0s - loss: 1.9189e-04 - val_loss: 1.9290e-06
Epoch 246/512
512/512 - 0s - loss: 1.9205e-04 - val_loss: 1.8603e-06
Epoch 247/512
512/512 - 0s - loss: 1.8367e-04 - val_loss: 1.8672e-06
Epoch 248/512
512/512 - 0s - loss: 1.8684e-04 - val_loss: 1.9060e-06
Epoch 249/512
512/512 - 0s - loss: 1.8963e-04 - val_loss: 1.8111e-06
Epoch 250/512
512/512 - 0s - loss: 1.7759e-04 - val_loss: 1.8021e-06
Epoch 251/512
512/512 - 0s - loss: 1.8416e-04 - val_loss: 1.8094e-06
Epoch 252/512
512/512 - 0s - loss: 1.7952e-04 - val_loss: 1.7865e-06
Epoch 253/512
512/512 - 0s - loss: 1.7934e-04 - val_loss: 1.7704e-06
Epoch 254/512
512/512 - 0s - loss: 1.7581e-04 - val_loss: 1.7762e-06
Epoch 255/512
512/512 - 0s - loss: 1.7694e-04 - val_loss: 1.7563e-06
Epoch 256/512
512/512 - 0s - loss: 1.7435e-04 - val_loss: 1.7120e-06
Epoch 257/512
512/512 - 0s - loss: 1.7140e-04 - val_loss: 1.7162e-06
Epoch 258/512
512/512 - 0s - loss: 1.7172e-04 - val_loss: 1.7119e-06
Epoch 259/512
512/512 - 0s - loss: 1.6962e-04 - val_loss: 1.7087e-06
Epoch 260/512
512/512 - 0s - loss: 1.6802e-04 - val_loss: 1.6981e-06
Epoch 261/512
512/512 - 0s - loss: 1.6816e-04 - val_loss: 1.6544e-06
Epoch 262/512
512/512 - 0s - loss: 1.6476e-04 - val_loss: 1.6234e-06
Epoch 263/512
512/512 - 0s - loss: 1.6266e-04 - val_loss: 1.6308e-06
Epoch 264/512
512/512 - 0s - loss: 1.6487e-04 - val_loss: 1.5812e-06
Epoch 265/512
512/512 - 0s - loss: 1.5748e-04 - val_loss: 1.5845e-06
Epoch 266/512
512/512 - 0s - loss: 1.5894e-04 - val_loss: 1.6381e-06
Epoch 267/512
512/512 - 0s - loss: 1.6218e-04 - val_loss: 1.5680e-06
Epoch 268/512
512/512 - 0s - loss: 1.5432e-04 - val_loss: 1.5069e-06
Epoch 269/512
512/512 - 0s - loss: 1.5327e-04 - val_loss: 1.5307e-06
Epoch 270/512
512/512 - 0s - loss: 1.5271e-04 - val_loss: 1.5679e-06
Epoch 271/512
512/512 - 0s - loss: 1.5500e-04 - val_loss: 1.5198e-06
Epoch 272/512
512/512 - 0s - loss: 1.4963e-04 - val_loss: 1.4702e-06
Epoch 273/512
512/512 - 0s - loss: 1.4722e-04 - val_loss: 1.4779e-06
Epoch 274/512
512/512 - 0s - loss: 1.4796e-04 - val_loss: 1.5004e-06
Epoch 275/512
512/512 - 0s - loss: 1.4872e-04 - val_loss: 1.4326e-06
Epoch 276/512
512/512 - 0s - loss: 1.4168e-04 - val_loss: 1.4148e-06
Epoch 277/512
512/512 - 0s - loss: 1.4213e-04 - val_loss: 1.4585e-06
Epoch 278/512
512/512 - 0s - loss: 1.4452e-04 - val_loss: 1.4373e-06
Epoch 279/512
512/512 - 0s - loss: 1.4033e-04 - val_loss: 1.3693e-06
Epoch 280/512
512/512 - 0s - loss: 1.3640e-04 - val_loss: 1.3715e-06
Epoch 281/512
512/512 - 0s - loss: 1.3764e-04 - val_loss: 1.3898e-06
Epoch 282/512
512/512 - 0s - loss: 1.3870e-04 - val_loss: 1.3150e-06
Epoch 283/512
512/512 - 0s - loss: 1.3005e-04 - val_loss: 1.3262e-06
Epoch 284/512
512/512 - 0s - loss: 1.3454e-04 - val_loss: 1.3512e-06
Epoch 285/512
512/512 - 0s - loss: 1.3133e-04 - val_loss: 1.3291e-06
Epoch 286/512
512/512 - 0s - loss: 1.3091e-04 - val_loss: 1.2975e-06
Epoch 287/512
512/512 - 0s - loss: 1.2795e-04 - val_loss: 1.2662e-06
Epoch 288/512
512/512 - 0s - loss: 1.2709e-04 - val_loss: 1.2465e-06
Epoch 289/512
512/512 - 0s - loss: 1.2483e-04 - val_loss: 1.2457e-06
Epoch 290/512
512/512 - 0s - loss: 1.2572e-04 - val_loss: 1.2150e-06
Epoch 291/512
512/512 - 0s - loss: 1.2032e-04 - val_loss: 1.2216e-06
Epoch 292/512
512/512 - 0s - loss: 1.2307e-04 - val_loss: 1.2192e-06
Epoch 293/512
512/512 - 0s - loss: 1.2064e-04 - val_loss: 1.1502e-06
Epoch 294/512
512/512 - 0s - loss: 1.1595e-04 - val_loss: 1.1601e-06
Epoch 295/512
512/512 - 0s - loss: 1.1748e-04 - val_loss: 1.1731e-06
Epoch 296/512
512/512 - 0s - loss: 1.1644e-04 - val_loss: 1.1471e-06
Epoch 297/512
512/512 - 0s - loss: 1.1405e-04 - val_loss: 1.1151e-06
Epoch 298/512
512/512 - 0s - loss: 1.1143e-04 - val_loss: 1.1195e-06
Epoch 299/512
512/512 - 0s - loss: 1.1124e-04 - val_loss: 1.1270e-06
Epoch 300/512
512/512 - 0s - loss: 1.1073e-04 - val_loss: 1.1043e-06
Epoch 301/512
512/512 - 0s - loss: 1.0843e-04 - val_loss: 1.0738e-06
Epoch 302/512
512/512 - 0s - loss: 1.0637e-04 - val_loss: 1.0602e-06
Epoch 303/512
512/512 - 0s - loss: 1.0515e-04 - val_loss: 1.0593e-06
Epoch 304/512
512/512 - 0s - loss: 1.0545e-04 - val_loss: 1.0259e-06
Epoch 305/512
512/512 - 0s - loss: 1.0034e-04 - val_loss: 1.0418e-06
Epoch 306/512
512/512 - 0s - loss: 1.0349e-04 - val_loss: 1.0307e-06
Epoch 307/512
512/512 - 0s - loss: 1.0053e-04 - val_loss: 9.7262e-07
Epoch 308/512
512/512 - 0s - loss: 9.6946e-05 - val_loss: 9.5995e-07
Epoch 309/512
512/512 - 0s - loss: 9.6236e-05 - val_loss: 9.9337e-07
Epoch 310/512
512/512 - 0s - loss: 9.8465e-05 - val_loss: 9.5827e-07
Epoch 311/512
512/512 - 0s - loss: 9.3865e-05 - val_loss: 9.0347e-07
Epoch 312/512
512/512 - 0s - loss: 9.1239e-05 - val_loss: 9.1675e-07
Epoch 313/512
512/512 - 0s - loss: 9.2463e-05 - val_loss: 9.2337e-07
Epoch 314/512
512/512 - 0s - loss: 9.1172e-05 - val_loss: 8.9242e-07
Epoch 315/512
512/512 - 0s - loss: 8.8762e-05 - val_loss: 8.6511e-07
Epoch 316/512
512/512 - 0s - loss: 8.6016e-05 - val_loss: 8.8136e-07
Epoch 317/512
512/512 - 0s - loss: 8.7854e-05 - val_loss: 8.7353e-07
Epoch 318/512
512/512 - 0s - loss: 8.6638e-05 - val_loss: 8.0969e-07
Epoch 319/512
512/512 - 0s - loss: 7.9860e-05 - val_loss: 8.1882e-07
Epoch 320/512
512/512 - 0s - loss: 8.2973e-05 - val_loss: 8.6177e-07
Epoch 321/512
512/512 - 0s - loss: 8.3751e-05 - val_loss: 8.1114e-07
Epoch 322/512
512/512 - 0s - loss: 7.7745e-05 - val_loss: 7.7773e-07
Epoch 323/512
512/512 - 0s - loss: 7.8210e-05 - val_loss: 7.8901e-07
Epoch 324/512
512/512 - 0s - loss: 7.7921e-05 - val_loss: 7.7905e-07
Epoch 325/512
512/512 - 0s - loss: 7.6351e-05 - val_loss: 7.4393e-07
Epoch 326/512
512/512 - 0s - loss: 7.3343e-05 - val_loss: 7.4450e-07
Epoch 327/512
512/512 - 0s - loss: 7.3861e-05 - val_loss: 7.4177e-07
Epoch 328/512
512/512 - 0s - loss: 7.2743e-05 - val_loss: 7.1798e-07
Epoch 329/512
512/512 - 0s - loss: 7.0224e-05 - val_loss: 6.9705e-07
Epoch 330/512
512/512 - 0s - loss: 6.9846e-05 - val_loss: 6.9500e-07
Epoch 331/512
512/512 - 0s - loss: 6.8347e-05 - val_loss: 6.8472e-07
Epoch 332/512
512/512 - 0s - loss: 6.7865e-05 - val_loss: 6.6074e-07
Epoch 333/512
512/512 - 0s - loss: 6.5146e-05 - val_loss: 6.5395e-07
Epoch 334/512
512/512 - 0s - loss: 6.4911e-05 - val_loss: 6.5257e-07
Epoch 335/512
512/512 - 0s - loss: 6.4691e-05 - val_loss: 6.2245e-07
Epoch 336/512
512/512 - 0s - loss: 6.1055e-05 - val_loss: 6.1952e-07
Epoch 337/512
512/512 - 0s - loss: 6.2033e-05 - val_loss: 6.1834e-07
Epoch 338/512
512/512 - 0s - loss: 6.0206e-05 - val_loss: 6.0907e-07
Epoch 339/512
512/512 - 0s - loss: 5.9564e-05 - val_loss: 5.8623e-07
Epoch 340/512
512/512 - 0s - loss: 5.7912e-05 - val_loss: 5.6077e-07
Epoch 341/512
512/512 - 0s - loss: 5.5724e-05 - val_loss: 5.6712e-07
Epoch 342/512
512/512 - 0s - loss: 5.6690e-05 - val_loss: 5.6248e-07
Epoch 343/512
512/512 - 0s - loss: 5.5080e-05 - val_loss: 5.3147e-07
Epoch 344/512
512/512 - 0s - loss: 5.1930e-05 - val_loss: 5.3956e-07
Epoch 345/512
512/512 - 0s - loss: 5.3977e-05 - val_loss: 5.3569e-07
Epoch 346/512
512/512 - 0s - loss: 5.2032e-05 - val_loss: 4.9379e-07
Epoch 347/512
512/512 - 0s - loss: 4.8218e-05 - val_loss: 5.0499e-07
Epoch 348/512
512/512 - 0s - loss: 5.1060e-05 - val_loss: 5.0431e-07
Epoch 349/512
512/512 - 0s - loss: 4.8896e-05 - val_loss: 4.6611e-07
Epoch 350/512
512/512 - 0s - loss: 4.5614e-05 - val_loss: 4.7018e-07
Epoch 351/512
512/512 - 0s - loss: 4.7089e-05 - val_loss: 4.7967e-07
Epoch 352/512
512/512 - 0s - loss: 4.6838e-05 - val_loss: 4.4168e-07
Epoch 353/512
512/512 - 0s - loss: 4.2905e-05 - val_loss: 4.3341e-07
Epoch 354/512
512/512 - 0s - loss: 4.3167e-05 - val_loss: 4.5368e-07
Epoch 355/512
512/512 - 0s - loss: 4.4122e-05 - val_loss: 4.3221e-07
Epoch 356/512
512/512 - 0s - loss: 4.1638e-05 - val_loss: 3.9610e-07
Epoch 357/512
512/512 - 0s - loss: 3.9243e-05 - val_loss: 4.0593e-07
Epoch 358/512
512/512 - 0s - loss: 4.0621e-05 - val_loss: 4.1320e-07
Epoch 359/512
512/512 - 0s - loss: 3.9914e-05 - val_loss: 3.8189e-07
Epoch 360/512
512/512 - 0s - loss: 3.7162e-05 - val_loss: 3.6700e-07
Epoch 361/512
512/512 - 0s - loss: 3.6704e-05 - val_loss: 3.8361e-07
Epoch 362/512
512/512 - 0s - loss: 3.7523e-05 - val_loss: 3.7169e-07
Epoch 363/512
512/512 - 0s - loss: 3.5560e-05 - val_loss: 3.4517e-07
Epoch 364/512
512/512 - 0s - loss: 3.4005e-05 - val_loss: 3.4533e-07
Epoch 365/512
512/512 - 0s - loss: 3.4340e-05 - val_loss: 3.4461e-07
Epoch 366/512
512/512 - 0s - loss: 3.3492e-05 - val_loss: 3.2792e-07
Epoch 367/512
512/512 - 0s - loss: 3.1746e-05 - val_loss: 3.2441e-07
Epoch 368/512
512/512 - 0s - loss: 3.2303e-05 - val_loss: 3.1580e-07
Epoch 369/512
512/512 - 0s - loss: 3.0600e-05 - val_loss: 3.0341e-07
Epoch 370/512
512/512 - 0s - loss: 2.9859e-05 - val_loss: 3.0238e-07
Epoch 371/512
512/512 - 0s - loss: 2.9914e-05 - val_loss: 2.9212e-07
Epoch 372/512
512/512 - 0s - loss: 2.8270e-05 - val_loss: 2.8489e-07
Epoch 373/512
512/512 - 0s - loss: 2.7898e-05 - val_loss: 2.8628e-07
Epoch 374/512
512/512 - 0s - loss: 2.7927e-05 - val_loss: 2.7158e-07
Epoch 375/512
512/512 - 0s - loss: 2.6342e-05 - val_loss: 2.5902e-07
Epoch 376/512
512/512 - 0s - loss: 2.5734e-05 - val_loss: 2.5953e-07
Epoch 377/512
512/512 - 0s - loss: 2.5637e-05 - val_loss: 2.5300e-07
Epoch 378/512
512/512 - 0s - loss: 2.4483e-05 - val_loss: 2.4494e-07
Epoch 379/512
512/512 - 0s - loss: 2.4190e-05 - val_loss: 2.3743e-07
Epoch 380/512
512/512 - 0s - loss: 2.3201e-05 - val_loss: 2.3512e-07
Epoch 381/512
512/512 - 0s - loss: 2.3036e-05 - val_loss: 2.2922e-07
Epoch 382/512
512/512 - 0s - loss: 2.2304e-05 - val_loss: 2.2029e-07
Epoch 383/512
512/512 - 0s - loss: 2.1506e-05 - val_loss: 2.1608e-07
Epoch 384/512
512/512 - 0s - loss: 2.1587e-05 - val_loss: 2.0389e-07
Epoch 385/512
512/512 - 0s - loss: 1.9951e-05 - val_loss: 1.9991e-07
Epoch 386/512
512/512 - 0s - loss: 1.9912e-05 - val_loss: 2.0547e-07
Epoch 387/512
512/512 - 0s - loss: 2.0114e-05 - val_loss: 1.9463e-07
Epoch 388/512
512/512 - 0s - loss: 1.8623e-05 - val_loss: 1.8463e-07
Epoch 389/512
512/512 - 0s - loss: 1.8277e-05 - val_loss: 1.8642e-07
Epoch 390/512
512/512 - 0s - loss: 1.8151e-05 - val_loss: 1.8335e-07
Epoch 391/512
512/512 - 0s - loss: 1.7587e-05 - val_loss: 1.7285e-07
Epoch 392/512
512/512 - 0s - loss: 1.6858e-05 - val_loss: 1.6676e-07
Epoch 393/512
512/512 - 0s - loss: 1.6415e-05 - val_loss: 1.6594e-07
Epoch 394/512
512/512 - 0s - loss: 1.6268e-05 - val_loss: 1.5952e-07
Epoch 395/512
512/512 - 0s - loss: 1.5466e-05 - val_loss: 1.5459e-07
Epoch 396/512
512/512 - 0s - loss: 1.5333e-05 - val_loss: 1.4998e-07
Epoch 397/512
512/512 - 0s - loss: 1.4686e-05 - val_loss: 1.4537e-07
Epoch 398/512
512/512 - 0s - loss: 1.4377e-05 - val_loss: 1.4181e-07
Epoch 399/512
512/512 - 0s - loss: 1.3992e-05 - val_loss: 1.3572e-07
Epoch 400/512
512/512 - 0s - loss: 1.3353e-05 - val_loss: 1.3514e-07
Epoch 401/512
512/512 - 0s - loss: 1.3236e-05 - val_loss: 1.3373e-07
Epoch 402/512
512/512 - 0s - loss: 1.2911e-05 - val_loss: 1.2811e-07
Epoch 403/512
512/512 - 0s - loss: 1.2359e-05 - val_loss: 1.2201e-07
Epoch 404/512
512/512 - 0s - loss: 1.1981e-05 - val_loss: 1.2090e-07
Epoch 405/512
512/512 - 0s - loss: 1.1882e-05 - val_loss: 1.1603e-07
Epoch 406/512
512/512 - 0s - loss: 1.1307e-05 - val_loss: 1.1040e-07
Epoch 407/512
512/512 - 0s - loss: 1.0779e-05 - val_loss: 1.1217e-07
Epoch 408/512
512/512 - 0s - loss: 1.1112e-05 - val_loss: 1.0714e-07
Epoch 409/512
512/512 - 0s - loss: 1.0312e-05 - val_loss: 9.9216e-08
Epoch 410/512
512/512 - 0s - loss: 9.8235e-06 - val_loss: 1.0008e-07
Epoch 411/512
512/512 - 0s - loss: 9.9346e-06 - val_loss: 1.0019e-07
Epoch 412/512
512/512 - 0s - loss: 9.5703e-06 - val_loss: 9.4821e-08
Epoch 413/512
512/512 - 0s - loss: 9.1648e-06 - val_loss: 9.0384e-08
Epoch 414/512
512/512 - 0s - loss: 8.9164e-06 - val_loss: 8.7370e-08
Epoch 415/512
512/512 - 0s - loss: 8.5265e-06 - val_loss: 8.7947e-08
Epoch 416/512
512/512 - 0s - loss: 8.5951e-06 - val_loss: 8.5001e-08
Epoch 417/512
512/512 - 0s - loss: 8.0860e-06 - val_loss: 8.0089e-08
Epoch 418/512
512/512 - 0s - loss: 7.7884e-06 - val_loss: 7.9635e-08
Epoch 419/512
512/512 - 0s - loss: 7.8142e-06 - val_loss: 7.6116e-08
Epoch 420/512
512/512 - 0s - loss: 7.3129e-06 - val_loss: 7.2801e-08
Epoch 421/512
512/512 - 0s - loss: 7.1844e-06 - val_loss: 7.1682e-08
Epoch 422/512
512/512 - 0s - loss: 6.9266e-06 - val_loss: 7.0887e-08
Epoch 423/512
512/512 - 0s - loss: 6.8202e-06 - val_loss: 6.8223e-08
Epoch 424/512
512/512 - 0s - loss: 6.6427e-06 - val_loss: 6.2942e-08
Epoch 425/512
512/512 - 0s - loss: 6.0852e-06 - val_loss: 6.2443e-08
Epoch 426/512
512/512 - 0s - loss: 6.1892e-06 - val_loss: 6.3848e-08
Epoch 427/512
512/512 - 0s - loss: 6.1300e-06 - val_loss: 5.8384e-08
Epoch 428/512
512/512 - 0s - loss: 5.5810e-06 - val_loss: 5.5258e-08
Epoch 429/512
512/512 - 0s - loss: 5.5551e-06 - val_loss: 5.5732e-08
Epoch 430/512
512/512 - 0s - loss: 5.4236e-06 - val_loss: 5.4281e-08
Epoch 431/512
512/512 - 0s - loss: 5.2610e-06 - val_loss: 5.1493e-08
Epoch 432/512
512/512 - 0s - loss: 4.9845e-06 - val_loss: 4.9737e-08
Epoch 433/512
512/512 - 0s - loss: 4.9380e-06 - val_loss: 4.7833e-08
Epoch 434/512
512/512 - 0s - loss: 4.6630e-06 - val_loss: 4.6509e-08
Epoch 435/512
512/512 - 0s - loss: 4.5125e-06 - val_loss: 4.7491e-08
Epoch 436/512
512/512 - 0s - loss: 4.5664e-06 - val_loss: 4.4661e-08
Epoch 437/512
512/512 - 0s - loss: 4.2396e-06 - val_loss: 4.1382e-08
Epoch 438/512
512/512 - 0s - loss: 4.0502e-06 - val_loss: 4.1550e-08
Epoch 439/512
512/512 - 0s - loss: 4.0475e-06 - val_loss: 4.1123e-08
Epoch 440/512
512/512 - 0s - loss: 3.9420e-06 - val_loss: 3.8361e-08
Epoch 441/512
512/512 - 0s - loss: 3.6824e-06 - val_loss: 3.6636e-08
Epoch 442/512
512/512 - 0s - loss: 3.6185e-06 - val_loss: 3.5993e-08
Epoch 443/512
512/512 - 0s - loss: 3.4854e-06 - val_loss: 3.5317e-08
Epoch 444/512
512/512 - 0s - loss: 3.4116e-06 - val_loss: 3.4086e-08
Epoch 445/512
512/512 - 0s - loss: 3.2898e-06 - val_loss: 3.1997e-08
Epoch 446/512
512/512 - 0s - loss: 3.0822e-06 - val_loss: 3.1738e-08
Epoch 447/512
512/512 - 0s - loss: 3.0969e-06 - val_loss: 3.1238e-08
Epoch 448/512
512/512 - 0s - loss: 2.9792e-06 - val_loss: 2.9391e-08
Epoch 449/512
512/512 - 0s - loss: 2.8128e-06 - val_loss: 2.8300e-08
Epoch 450/512
512/512 - 0s - loss: 2.7638e-06 - val_loss: 2.7702e-08
Epoch 451/512
512/512 - 0s - loss: 2.6762e-06 - val_loss: 2.6398e-08
Epoch 452/512
512/512 - 0s - loss: 2.5255e-06 - val_loss: 2.5993e-08
Epoch 453/512
512/512 - 0s - loss: 2.5096e-06 - val_loss: 2.5305e-08
Epoch 454/512
512/512 - 0s - loss: 2.4089e-06 - val_loss: 2.4177e-08
Epoch 455/512
512/512 - 0s - loss: 2.3049e-06 - val_loss: 2.3149e-08
Epoch 456/512
512/512 - 0s - loss: 2.2438e-06 - val_loss: 2.2019e-08
Epoch 457/512
512/512 - 0s - loss: 2.1392e-06 - val_loss: 2.1340e-08
Epoch 458/512
512/512 - 0s - loss: 2.0809e-06 - val_loss: 2.1204e-08
Epoch 459/512
512/512 - 0s - loss: 2.0420e-06 - val_loss: 2.0082e-08
Epoch 460/512
512/512 - 0s - loss: 1.9118e-06 - val_loss: 1.9294e-08
Epoch 461/512
512/512 - 0s - loss: 1.8820e-06 - val_loss: 1.8923e-08
Epoch 462/512
512/512 - 0s - loss: 1.8242e-06 - val_loss: 1.7850e-08
Epoch 463/512
512/512 - 0s - loss: 1.7134e-06 - val_loss: 1.7517e-08
Epoch 464/512
512/512 - 0s - loss: 1.7063e-06 - val_loss: 1.7067e-08
Epoch 465/512
512/512 - 0s - loss: 1.6292e-06 - val_loss: 1.6089e-08
Epoch 466/512
512/512 - 0s - loss: 1.5477e-06 - val_loss: 1.5643e-08
Epoch 467/512
512/512 - 0s - loss: 1.5166e-06 - val_loss: 1.5372e-08
Epoch 468/512
512/512 - 0s - loss: 1.4714e-06 - val_loss: 1.4641e-08
Epoch 469/512
512/512 - 0s - loss: 1.4039e-06 - val_loss: 1.3856e-08
Epoch 470/512
512/512 - 0s - loss: 1.3225e-06 - val_loss: 1.3944e-08
Epoch 471/512
512/512 - 0s - loss: 1.3693e-06 - val_loss: 1.3043e-08
Epoch 472/512
512/512 - 0s - loss: 1.2402e-06 - val_loss: 1.1953e-08
Epoch 473/512
512/512 - 0s - loss: 1.1690e-06 - val_loss: 1.2393e-08
Epoch 474/512
512/512 - 0s - loss: 1.2188e-06 - val_loss: 1.2237e-08
Epoch 475/512
512/512 - 0s - loss: 1.1475e-06 - val_loss: 1.0871e-08
Epoch 476/512
512/512 - 0s - loss: 1.0425e-06 - val_loss: 1.0705e-08
Epoch 477/512
512/512 - 0s - loss: 1.0678e-06 - val_loss: 1.0791e-08
Epoch 478/512
512/512 - 0s - loss: 1.0261e-06 - val_loss: 1.0070e-08
Epoch 479/512
512/512 - 0s - loss: 9.6158e-07 - val_loss: 9.6853e-09
Epoch 480/512
512/512 - 0s - loss: 9.4443e-07 - val_loss: 9.4277e-09
Epoch 481/512
512/512 - 0s - loss: 9.0142e-07 - val_loss: 9.1611e-09
Epoch 482/512
512/512 - 0s - loss: 8.8161e-07 - val_loss: 8.7849e-09
Epoch 483/512
512/512 - 0s - loss: 8.4503e-07 - val_loss: 8.2335e-09
Epoch 484/512
512/512 - 0s - loss: 7.9611e-07 - val_loss: 7.9978e-09
Epoch 485/512
512/512 - 0s - loss: 7.8498e-07 - val_loss: 7.8791e-09
Epoch 486/512
512/512 - 0s - loss: 7.5421e-07 - val_loss: 7.5714e-09
Epoch 487/512
512/512 - 0s - loss: 7.2086e-07 - val_loss: 7.3209e-09
Epoch 488/512
512/512 - 0s - loss: 6.9828e-07 - val_loss: 7.0702e-09
Epoch 489/512
512/512 - 0s - loss: 6.7453e-07 - val_loss: 6.7556e-09
Epoch 490/512
512/512 - 0s - loss: 6.4096e-07 - val_loss: 6.5897e-09
Epoch 491/512
512/512 - 0s - loss: 6.3454e-07 - val_loss: 6.2161e-09
Epoch 492/512
512/512 - 0s - loss: 5.9003e-07 - val_loss: 5.9568e-09
Epoch 493/512
512/512 - 0s - loss: 5.7415e-07 - val_loss: 5.8941e-09
Epoch 494/512
512/512 - 0s - loss: 5.6828e-07 - val_loss: 5.5338e-09
Epoch 495/512
512/512 - 0s - loss: 5.2843e-07 - val_loss: 5.2392e-09
Epoch 496/512
512/512 - 0s - loss: 5.0783e-07 - val_loss: 5.1832e-09
Epoch 497/512
512/512 - 0s - loss: 5.0289e-07 - val_loss: 5.0530e-09
Epoch 498/512
512/512 - 0s - loss: 4.8136e-07 - val_loss: 4.6901e-09
Epoch 499/512
512/512 - 0s - loss: 4.5005e-07 - val_loss: 4.5445e-09
Epoch 500/512
512/512 - 0s - loss: 4.4009e-07 - val_loss: 4.5541e-09
Epoch 501/512
512/512 - 0s - loss: 4.3805e-07 - val_loss: 4.2570e-09
Epoch 502/512
512/512 - 0s - loss: 4.0214e-07 - val_loss: 3.9392e-09
Epoch 503/512
512/512 - 0s - loss: 3.8418e-07 - val_loss: 4.0021e-09
Epoch 504/512
512/512 - 0s - loss: 3.8860e-07 - val_loss: 3.8586e-09
Epoch 505/512
512/512 - 0s - loss: 3.6488e-07 - val_loss: 3.5706e-09
Epoch 506/512
512/512 - 0s - loss: 3.4249e-07 - val_loss: 3.4956e-09
Epoch 507/512
512/512 - 0s - loss: 3.3997e-07 - val_loss: 3.4457e-09
Epoch 508/512
512/512 - 0s - loss: 3.2866e-07 - val_loss: 3.2192e-09
Epoch 509/512
512/512 - 0s - loss: 3.0514e-07 - val_loss: 3.0910e-09
Epoch 510/512
512/512 - 0s - loss: 3.0096e-07 - val_loss: 3.0447e-09
Epoch 511/512
512/512 - 0s - loss: 2.9196e-07 - val_loss: 2.8459e-09
Epoch 512/512
512/512 - 0s - loss: 2.7066e-07 - val_loss: 2.7576e-09
2024-04-09 17:44:29.616601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0809e-07 - val_loss: 1.7589e-07
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9371e-07 - val_loss: 2.6587e-07
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8990e-07 - val_loss: 3.0026e-07
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4619e-07 - val_loss: 1.8923e-07
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7535e-07 - val_loss: 1.9069e-07
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1012e-07 - val_loss: 2.5375e-07
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3648e-07 - val_loss: 2.0805e-07
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8052e-07 - val_loss: 1.6582e-07
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6752e-07 - val_loss: 1.9365e-07
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9636e-07 - val_loss: 2.0213e-07
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8031e-07 - val_loss: 1.6101e-07
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5065e-07 - val_loss: 1.5567e-07
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5813e-07 - val_loss: 1.7368e-07
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6460e-07 - val_loss: 1.5751e-07
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4336e-07 - val_loss: 1.3756e-07
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3383e-07 - val_loss: 1.4289e-07
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4051e-07 - val_loss: 1.4422e-07
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3372e-07 - val_loss: 1.2781e-07
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2014e-07 - val_loss: 1.2148e-07
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1892e-07 - val_loss: 1.2472e-07
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1914e-07 - val_loss: 1.1829e-07
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1020e-07 - val_loss: 1.0815e-07
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0346e-07 - val_loss: 1.0658e-07
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0327e-07 - val_loss: 1.0575e-07
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.9682e-08 - val_loss: 9.8127e-08
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2583e-08 - val_loss: 9.3208e-08
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9681e-08 - val_loss: 9.2047e-08
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7837e-08 - val_loss: 8.8094e-08
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2985e-08 - val_loss: 8.2790e-08
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8947e-08 - val_loss: 8.0440e-08
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6956e-08 - val_loss: 7.8171e-08
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4158e-08 - val_loss: 7.4090e-08
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0168e-08 - val_loss: 7.0495e-08
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7297e-08 - val_loss: 6.8646e-08
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5457e-08 - val_loss: 6.6098e-08
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2586e-08 - val_loss: 6.2501e-08
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9365e-08 - val_loss: 6.0084e-08
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7475e-08 - val_loss: 5.8459e-08
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5456e-08 - val_loss: 5.5531e-08
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2603e-08 - val_loss: 5.3041e-08
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0538e-08 - val_loss: 5.1316e-08
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8945e-08 - val_loss: 4.9278e-08
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6711e-08 - val_loss: 4.6799e-08
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4484e-08 - val_loss: 4.5086e-08
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2963e-08 - val_loss: 4.3579e-08
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1412e-08 - val_loss: 4.1646e-08
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9446e-08 - val_loss: 3.9692e-08
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7830e-08 - val_loss: 3.8269e-08
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6439e-08 - val_loss: 3.6820e-08
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4928e-08 - val_loss: 3.5160e-08
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3433e-08 - val_loss: 3.3757e-08
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2080e-08 - val_loss: 3.2404e-08
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0815e-08 - val_loss: 3.1089e-08
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9545e-08 - val_loss: 2.9760e-08
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8257e-08 - val_loss: 2.8499e-08
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7095e-08 - val_loss: 2.7469e-08
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6134e-08 - val_loss: 2.6366e-08
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5022e-08 - val_loss: 2.5205e-08
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3953e-08 - val_loss: 2.4219e-08
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3061e-08 - val_loss: 2.3210e-08
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2036e-08 - val_loss: 2.2263e-08
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1191e-08 - val_loss: 2.1390e-08
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0319e-08 - val_loss: 2.0418e-08
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9412e-08 - val_loss: 1.9559e-08
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8709e-08 - val_loss: 1.8960e-08
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8026e-08 - val_loss: 1.8195e-08
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7236e-08 - val_loss: 1.7304e-08
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6465e-08 - val_loss: 1.6579e-08
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5786e-08 - val_loss: 1.5922e-08
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5212e-08 - val_loss: 1.5431e-08
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4685e-08 - val_loss: 1.4757e-08
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4014e-08 - val_loss: 1.4026e-08
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3346e-08 - val_loss: 1.3480e-08
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2892e-08 - val_loss: 1.3070e-08
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2455e-08 - val_loss: 1.2551e-08
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1934e-08 - val_loss: 1.1977e-08
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1372e-08 - val_loss: 1.1405e-08
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0913e-08 - val_loss: 1.1107e-08
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0644e-08 - val_loss: 1.0740e-08
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0193e-08 - val_loss: 1.0176e-08
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6606e-09 - val_loss: 9.6307e-09
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2268e-09 - val_loss: 9.3675e-09
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0028e-09 - val_loss: 9.1317e-09
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7023e-09 - val_loss: 8.6866e-09
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2574e-09 - val_loss: 8.2752e-09
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9429e-09 - val_loss: 8.0234e-09
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6852e-09 - val_loss: 7.7607e-09
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3941e-09 - val_loss: 7.3890e-09
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0420e-09 - val_loss: 7.0472e-09
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7536e-09 - val_loss: 6.8935e-09
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6094e-09 - val_loss: 6.6764e-09
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3730e-09 - val_loss: 6.3919e-09
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0877e-09 - val_loss: 6.0806e-09
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7898e-09 - val_loss: 5.7786e-09
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5445e-09 - val_loss: 5.6224e-09
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4170e-09 - val_loss: 5.4943e-09
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2686e-09 - val_loss: 5.2856e-09
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0433e-09 - val_loss: 5.0471e-09
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8436e-09 - val_loss: 4.8508e-09
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6534e-09 - val_loss: 4.6811e-09
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4895e-09 - val_loss: 4.5140e-09
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3159e-09 - val_loss: 4.3245e-09
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1717e-09 - val_loss: 4.2291e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0628e-09 - val_loss: 4.0660e-09
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8989e-09 - val_loss: 3.8918e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7220e-09 - val_loss: 3.7089e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5809e-09 - val_loss: 3.6237e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5102e-09 - val_loss: 3.5526e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4192e-09 - val_loss: 3.3971e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2535e-09 - val_loss: 3.2666e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1347e-09 - val_loss: 3.1453e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0277e-09 - val_loss: 3.0519e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9569e-09 - val_loss: 2.9657e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8596e-09 - val_loss: 2.8851e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7657e-09 - val_loss: 2.7573e-09
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6556e-09 - val_loss: 2.6543e-09
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5444e-09 - val_loss: 2.5385e-09
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4556e-09 - val_loss: 2.4891e-09
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4200e-09 - val_loss: 2.4394e-09
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3590e-09 - val_loss: 2.3710e-09
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2714e-09 - val_loss: 2.2538e-09
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1732e-09 - val_loss: 2.1650e-09
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1031e-09 - val_loss: 2.0965e-09
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0320e-09 - val_loss: 2.0516e-09
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9976e-09 - val_loss: 2.0033e-09
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9361e-09 - val_loss: 1.9392e-09
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8813e-09 - val_loss: 1.8796e-09
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8174e-09 - val_loss: 1.8025e-09
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7298e-09 - val_loss: 1.7395e-09
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6933e-09 - val_loss: 1.7120e-09
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6619e-09 - val_loss: 1.6685e-09
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6137e-09 - val_loss: 1.6133e-09
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5540e-09 - val_loss: 1.5457e-09
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5019e-09 - val_loss: 1.5064e-09
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4604e-09 - val_loss: 1.4737e-09
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4331e-09 - val_loss: 1.4425e-09
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4008e-09 - val_loss: 1.3944e-09
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3528e-09 - val_loss: 1.3527e-09
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3169e-09 - val_loss: 1.3146e-09
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2728e-09 - val_loss: 1.2825e-09
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2474e-09 - val_loss: 1.2427e-09
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2108e-09 - val_loss: 1.2090e-09
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1858e-09 - val_loss: 1.1944e-09
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1477e-09 - val_loss: 1.1420e-09
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1074e-09 - val_loss: 1.0954e-09
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0692e-09 - val_loss: 1.0843e-09
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0581e-09 - val_loss: 1.0701e-09
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0441e-09 - val_loss: 1.0375e-09
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0111e-09 - val_loss: 1.0120e-09
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7886e-10 - val_loss: 9.8491e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5401e-10 - val_loss: 9.4419e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2001e-10 - val_loss: 9.2385e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0321e-10 - val_loss: 9.0920e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8972e-10 - val_loss: 8.9375e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7048e-10 - val_loss: 8.7267e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4752e-10 - val_loss: 8.4308e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2355e-10 - val_loss: 8.2410e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0228e-10 - val_loss: 8.0290e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8708e-10 - val_loss: 7.8815e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6643e-10 - val_loss: 7.6202e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4691e-10 - val_loss: 7.4595e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2200e-10 - val_loss: 7.2085e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1033e-10 - val_loss: 7.1202e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9636e-10 - val_loss: 7.1005e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9649e-10 - val_loss: 6.9213e-10
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7250e-10 - val_loss: 6.7006e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5170e-10 - val_loss: 6.4970e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3225e-10 - val_loss: 6.3347e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2392e-10 - val_loss: 6.3274e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1954e-10 - val_loss: 6.2303e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1102e-10 - val_loss: 6.1217e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9072e-10 - val_loss: 5.8649e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7470e-10 - val_loss: 5.7052e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5609e-10 - val_loss: 5.5897e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5110e-10 - val_loss: 5.5284e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3402e-10 - val_loss: 5.2530e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2344e-10 - val_loss: 5.3211e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1673e-10 - val_loss: 5.0655e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0284e-10 - val_loss: 5.0837e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9639e-10 - val_loss: 4.9764e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9045e-10 - val_loss: 4.8782e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7495e-10 - val_loss: 4.7617e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6536e-10 - val_loss: 4.6482e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5325e-10 - val_loss: 4.4937e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4098e-10 - val_loss: 4.3606e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2983e-10 - val_loss: 4.3839e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3278e-10 - val_loss: 4.3592e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2601e-10 - val_loss: 4.2511e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1685e-10 - val_loss: 4.2121e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0958e-10 - val_loss: 3.9960e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9492e-10 - val_loss: 4.0211e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9773e-10 - val_loss: 4.0210e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9134e-10 - val_loss: 3.8959e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8259e-10 - val_loss: 3.8534e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7568e-10 - val_loss: 3.6803e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5684e-10 - val_loss: 3.5563e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5197e-10 - val_loss: 3.5283e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4942e-10 - val_loss: 3.5201e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5220e-10 - val_loss: 3.6020e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5258e-10 - val_loss: 3.4336e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3090e-10 - val_loss: 3.2605e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2089e-10 - val_loss: 3.1642e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0814e-10 - val_loss: 3.0983e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1004e-10 - val_loss: 3.2021e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1790e-10 - val_loss: 3.1851e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1068e-10 - val_loss: 3.1074e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0571e-10 - val_loss: 3.0853e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0000e-10 - val_loss: 2.9798e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9136e-10 - val_loss: 2.9000e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8026e-10 - val_loss: 2.7690e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7290e-10 - val_loss: 2.7594e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7495e-10 - val_loss: 2.8206e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7994e-10 - val_loss: 2.8247e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7748e-10 - val_loss: 2.7764e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7413e-10 - val_loss: 2.7528e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6389e-10 - val_loss: 2.5346e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4938e-10 - val_loss: 2.5070e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4872e-10 - val_loss: 2.4744e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4205e-10 - val_loss: 2.3944e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3621e-10 - val_loss: 2.4094e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3915e-10 - val_loss: 2.4330e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3840e-10 - val_loss: 2.3485e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2878e-10 - val_loss: 2.3079e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3144e-10 - val_loss: 2.3688e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3300e-10 - val_loss: 2.2861e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2135e-10 - val_loss: 2.1630e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1697e-10 - val_loss: 2.2232e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2036e-10 - val_loss: 2.1829e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1345e-10 - val_loss: 2.0918e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0461e-10 - val_loss: 2.0497e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0716e-10 - val_loss: 2.1315e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1036e-10 - val_loss: 2.0780e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0209e-10 - val_loss: 1.9742e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9715e-10 - val_loss: 2.0242e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0073e-10 - val_loss: 2.0117e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9673e-10 - val_loss: 1.9342e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8795e-10 - val_loss: 1.8725e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8873e-10 - val_loss: 1.9454e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9107e-10 - val_loss: 1.8889e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8460e-10 - val_loss: 1.8335e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8077e-10 - val_loss: 1.7987e-10
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8022e-10 - val_loss: 1.8151e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7852e-10 - val_loss: 1.7533e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7300e-10 - val_loss: 1.7264e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7066e-10 - val_loss: 1.6861e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6653e-10 - val_loss: 1.6673e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6574e-10 - val_loss: 1.6661e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6646e-10 - val_loss: 1.6971e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6803e-10 - val_loss: 1.6420e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6137e-10 - val_loss: 1.6035e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5795e-10 - val_loss: 1.6086e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5784e-10 - val_loss: 1.5552e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5207e-10 - val_loss: 1.4315e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3846e-10 - val_loss: 1.3920e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4007e-10 - val_loss: 1.4345e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4440e-10 - val_loss: 1.4901e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5057e-10 - val_loss: 1.5102e-10
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4876e-10 - val_loss: 1.5066e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4985e-10 - val_loss: 1.5261e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5140e-10 - val_loss: 1.5230e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4905e-10 - val_loss: 1.4544e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3877e-10 - val_loss: 1.3185e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2765e-10 - val_loss: 1.2788e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2949e-10 - val_loss: 1.3372e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3290e-10 - val_loss: 1.3672e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3596e-10 - val_loss: 1.3391e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3094e-10 - val_loss: 1.2742e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2748e-10 - val_loss: 1.3087e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3225e-10 - val_loss: 1.3609e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3502e-10 - val_loss: 1.3226e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2856e-10 - val_loss: 1.2423e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1874e-10 - val_loss: 1.1517e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1455e-10 - val_loss: 1.1818e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1948e-10 - val_loss: 1.2344e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2336e-10 - val_loss: 1.2150e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1808e-10 - val_loss: 1.1246e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1195e-10 - val_loss: 1.1370e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1672e-10 - val_loss: 1.2069e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2006e-10 - val_loss: 1.2230e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1953e-10 - val_loss: 1.1270e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0855e-10 - val_loss: 1.0449e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0151e-10 - val_loss: 9.9879e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9294e-11 - val_loss: 1.0379e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0594e-10 - val_loss: 1.0975e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0728e-10 - val_loss: 1.0633e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0524e-10 - val_loss: 1.0426e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0247e-10 - val_loss: 1.0065e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0035e-10 - val_loss: 1.0483e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0750e-10 - val_loss: 1.1266e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1110e-10 - val_loss: 1.0928e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0530e-10 - val_loss: 1.0139e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6657e-11 - val_loss: 9.3492e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2293e-11 - val_loss: 9.2058e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3887e-11 - val_loss: 9.8011e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7609e-11 - val_loss: 9.7067e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5729e-11 - val_loss: 9.4854e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3398e-11 - val_loss: 9.1583e-11
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0847e-11 - val_loss: 8.9604e-11
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8880e-11 - val_loss: 8.9505e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3037e-11 - val_loss: 1.0019e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9793e-11 - val_loss: 9.8145e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4350e-11 - val_loss: 9.0487e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7358e-11 - val_loss: 8.4936e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3722e-11 - val_loss: 8.3284e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3824e-11 - val_loss: 8.6466e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8913e-11 - val_loss: 9.0169e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7923e-11 - val_loss: 8.7066e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4634e-11 - val_loss: 8.1614e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0278e-11 - val_loss: 8.1499e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1385e-11 - val_loss: 8.2371e-11
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1338e-11 - val_loss: 7.9480e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8406e-11 - val_loss: 8.0434e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0643e-11 - val_loss: 8.1019e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9938e-11 - val_loss: 7.8837e-11
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7046e-11 - val_loss: 7.7306e-11
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6534e-11 - val_loss: 7.6938e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6809e-11 - val_loss: 7.7729e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7105e-11 - val_loss: 7.4571e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3761e-11 - val_loss: 7.4488e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4285e-11 - val_loss: 7.5325e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4404e-11 - val_loss: 7.3449e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3584e-11 - val_loss: 7.4587e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4831e-11 - val_loss: 7.4676e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3559e-11 - val_loss: 7.2558e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0373e-11 - val_loss: 6.6678e-11
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4750e-11 - val_loss: 6.5954e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6333e-11 - val_loss: 6.8034e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8482e-11 - val_loss: 7.0785e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1006e-11 - val_loss: 7.2054e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1969e-11 - val_loss: 7.2623e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2011e-11 - val_loss: 7.3580e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2420e-11 - val_loss: 6.8652e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5287e-11 - val_loss: 6.4011e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4762e-11 - val_loss: 6.4964e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4651e-11 - val_loss: 6.5297e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5405e-11 - val_loss: 6.6307e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6893e-11 - val_loss: 6.6786e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6231e-11 - val_loss: 6.6946e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6707e-11 - val_loss: 6.4620e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1255e-11 - val_loss: 5.5908e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4095e-11 - val_loss: 5.5002e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6669e-11 - val_loss: 6.0798e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2741e-11 - val_loss: 6.4681e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4339e-11 - val_loss: 6.3878e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3817e-11 - val_loss: 6.4165e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4416e-11 - val_loss: 6.5846e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3562e-11 - val_loss: 5.9010e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5910e-11 - val_loss: 5.1750e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0304e-11 - val_loss: 5.1132e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2206e-11 - val_loss: 5.6008e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8190e-11 - val_loss: 6.0872e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2454e-11 - val_loss: 6.5518e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5461e-11 - val_loss: 6.6242e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4746e-11 - val_loss: 6.1966e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8522e-11 - val_loss: 5.2647e-11
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0167e-11 - val_loss: 4.6706e-11
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5366e-11 - val_loss: 4.3370e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3557e-11 - val_loss: 4.7376e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8970e-11 - val_loss: 5.0764e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2786e-11 - val_loss: 5.6536e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7985e-11 - val_loss: 5.9773e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0619e-11 - val_loss: 6.2747e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1716e-11 - val_loss: 5.8639e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6131e-11 - val_loss: 5.1096e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7129e-11 - val_loss: 4.3948e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3145e-11 - val_loss: 4.2376e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3517e-11 - val_loss: 4.6593e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8454e-11 - val_loss: 5.2379e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4195e-11 - val_loss: 5.6385e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6557e-11 - val_loss: 5.9372e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9449e-11 - val_loss: 5.9650e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7162e-11 - val_loss: 5.2827e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9195e-11 - val_loss: 4.4505e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2799e-11 - val_loss: 4.2104e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1352e-11 - val_loss: 4.1138e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1157e-11 - val_loss: 4.3301e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4936e-11 - val_loss: 4.8390e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0054e-11 - val_loss: 5.2226e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1767e-11 - val_loss: 5.1494e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0753e-11 - val_loss: 5.0220e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8056e-11 - val_loss: 4.3924e-11
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1761e-11 - val_loss: 3.8136e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7219e-11 - val_loss: 3.5952e-11
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5596e-11 - val_loss: 3.4979e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4365e-11 - val_loss: 3.5389e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6104e-11 - val_loss: 3.7532e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1371e-11 - val_loss: 4.7295e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9420e-11 - val_loss: 5.2258e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1746e-11 - val_loss: 5.2147e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1481e-11 - val_loss: 5.1165e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1564e-11 - val_loss: 5.0434e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7127e-11 - val_loss: 4.2317e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9926e-11 - val_loss: 3.5906e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4943e-11 - val_loss: 3.4089e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3528e-11 - val_loss: 3.3025e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3195e-11 - val_loss: 3.4039e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3890e-11 - val_loss: 3.3880e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6544e-11 - val_loss: 4.1701e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3421e-11 - val_loss: 4.6447e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7982e-11 - val_loss: 4.9904e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9537e-11 - val_loss: 5.0403e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9749e-11 - val_loss: 4.5524e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2851e-11 - val_loss: 3.8594e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5803e-11 - val_loss: 3.2365e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1350e-11 - val_loss: 2.9474e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8761e-11 - val_loss: 2.8052e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8044e-11 - val_loss: 2.7231e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7666e-11 - val_loss: 2.9257e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0014e-11 - val_loss: 3.3898e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5808e-11 - val_loss: 4.0311e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3056e-11 - val_loss: 4.6491e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7231e-11 - val_loss: 4.7944e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7513e-11 - val_loss: 4.7484e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6534e-11 - val_loss: 4.3575e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0717e-11 - val_loss: 3.6997e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5028e-11 - val_loss: 3.2920e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1626e-11 - val_loss: 3.0659e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0037e-11 - val_loss: 2.9051e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9317e-11 - val_loss: 3.0281e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0751e-11 - val_loss: 3.1923e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2523e-11 - val_loss: 3.5014e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6746e-11 - val_loss: 3.9654e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1036e-11 - val_loss: 4.3103e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4067e-11 - val_loss: 4.3683e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2847e-11 - val_loss: 4.3049e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0943e-11 - val_loss: 3.7795e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5755e-11 - val_loss: 3.2621e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1095e-11 - val_loss: 2.9901e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8788e-11 - val_loss: 2.7895e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7547e-11 - val_loss: 2.7617e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6877e-11 - val_loss: 2.6671e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7147e-11 - val_loss: 2.8227e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8536e-11 - val_loss: 2.8949e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9046e-11 - val_loss: 2.9765e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2650e-11 - val_loss: 3.7099e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8600e-11 - val_loss: 4.2467e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3015e-11 - val_loss: 4.2472e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2006e-11 - val_loss: 4.1157e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9452e-11 - val_loss: 3.6887e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5244e-11 - val_loss: 3.2779e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1235e-11 - val_loss: 2.9062e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8291e-11 - val_loss: 2.7192e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5800e-11 - val_loss: 2.5157e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4669e-11 - val_loss: 2.4375e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4843e-11 - val_loss: 2.5004e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5237e-11 - val_loss: 2.5597e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4961e-11 - val_loss: 2.5002e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5671e-11 - val_loss: 2.7073e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7369e-11 - val_loss: 2.7587e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9407e-11 - val_loss: 3.2201e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3668e-11 - val_loss: 3.7335e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7809e-11 - val_loss: 3.9687e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0024e-11 - val_loss: 4.0305e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8673e-11 - val_loss: 3.5583e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1921e-11 - val_loss: 2.8489e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7064e-11 - val_loss: 2.4920e-11
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4059e-11 - val_loss: 2.3020e-11
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2631e-11 - val_loss: 2.1798e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1614e-11 - val_loss: 2.1689e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2107e-11 - val_loss: 2.2276e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2537e-11 - val_loss: 2.3514e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3950e-11 - val_loss: 2.5473e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6320e-11 - val_loss: 2.7646e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8161e-11 - val_loss: 2.8488e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7981e-11 - val_loss: 3.0276e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1614e-11 - val_loss: 3.4480e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6267e-11 - val_loss: 3.8797e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9481e-11 - val_loss: 4.1137e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1094e-11 - val_loss: 3.9704e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6993e-11 - val_loss: 3.1054e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8292e-11 - val_loss: 2.3897e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1697e-11 - val_loss: 1.8968e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9093e-11 - val_loss: 1.9202e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9103e-11 - val_loss: 1.9570e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9723e-11 - val_loss: 2.0550e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0759e-11 - val_loss: 2.1478e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2108e-11 - val_loss: 2.2640e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2057e-11 - val_loss: 2.1703e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1360e-11 - val_loss: 2.0223e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0665e-11 - val_loss: 2.1047e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1232e-11 - val_loss: 2.1448e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1774e-11 - val_loss: 2.2093e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2719e-11 - val_loss: 2.4036e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6230e-11 - val_loss: 2.9753e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2117e-11 - val_loss: 3.4891e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5209e-11 - val_loss: 3.6190e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4848e-11 - val_loss: 3.1673e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9642e-11 - val_loss: 2.7078e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6414e-11 - val_loss: 2.4744e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3017e-11 - val_loss: 2.1599e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0423e-11 - val_loss: 1.9417e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8898e-11 - val_loss: 1.9484e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9621e-11 - val_loss: 1.9895e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9484e-11 - val_loss: 1.9461e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9531e-11 - val_loss: 1.9579e-11
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9043e-11 - val_loss: 1.8628e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8635e-11 - val_loss: 1.9217e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0021e-11 - val_loss: 2.1526e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2097e-11 - val_loss: 2.2926e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1991e-11 - val_loss: 2.1023e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1208e-11 - val_loss: 2.2016e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1443e-11 - val_loss: 2.1595e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1905e-11 - val_loss: 2.1711e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3093e-11 - val_loss: 2.6494e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8413e-11 - val_loss: 3.0990e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1409e-11 - val_loss: 3.1310e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9418e-11 - val_loss: 2.6459e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5832e-11 - val_loss: 2.5091e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4429e-11 - val_loss: 2.3311e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2237e-11 - val_loss: 2.0723e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9950e-11 - val_loss: 1.9495e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8819e-11 - val_loss: 1.8297e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.1458 - val_loss: 0.0709
Epoch 2/512
512/512 - 0s - loss: 0.0528 - val_loss: 0.0410
Epoch 3/512
512/512 - 0s - loss: 0.0401 - val_loss: 0.0256
Epoch 4/512
512/512 - 0s - loss: 0.0330 - val_loss: 0.0164
Epoch 5/512
512/512 - 0s - loss: 0.0284 - val_loss: 0.0108
Epoch 6/512
512/512 - 0s - loss: 0.0250 - val_loss: 0.0074
Epoch 7/512
512/512 - 0s - loss: 0.0224 - val_loss: 0.0052
Epoch 8/512
512/512 - 0s - loss: 0.0202 - val_loss: 0.0036
Epoch 9/512
512/512 - 0s - loss: 0.0182 - val_loss: 0.0022
Epoch 10/512
512/512 - 0s - loss: 0.0162 - val_loss: 0.0011
Epoch 11/512
512/512 - 0s - loss: 0.0143 - val_loss: 4.9074e-04
Epoch 12/512
512/512 - 0s - loss: 0.0125 - val_loss: 6.7674e-04
Epoch 13/512
512/512 - 0s - loss: 0.0107 - val_loss: 0.0019
Epoch 14/512
512/512 - 0s - loss: 0.0093 - val_loss: 0.0043
Epoch 15/512
512/512 - 0s - loss: 0.0081 - val_loss: 0.0068
Epoch 16/512
512/512 - 0s - loss: 0.0076 - val_loss: 0.0081
Epoch 17/512
512/512 - 0s - loss: 0.0062 - val_loss: 0.0094
Epoch 18/512
512/512 - 0s - loss: 0.0048 - val_loss: 0.0097
Epoch 19/512
512/512 - 0s - loss: 0.0039 - val_loss: 0.0093
Epoch 20/512
512/512 - 0s - loss: 0.0032 - val_loss: 0.0085
Epoch 21/512
512/512 - 0s - loss: 0.0028 - val_loss: 0.0067
Epoch 22/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0069
Epoch 23/512
512/512 - 0s - loss: 0.0022 - val_loss: 0.0069
Epoch 24/512
512/512 - 0s - loss: 0.0019 - val_loss: 0.0063
Epoch 25/512
512/512 - 0s - loss: 0.0019 - val_loss: 0.0055
Epoch 26/512
512/512 - 0s - loss: 0.0018 - val_loss: 0.0054
Epoch 27/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0054
Epoch 28/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0049
Epoch 29/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0044
Epoch 30/512
512/512 - 0s - loss: 9.6062e-04 - val_loss: 0.0042
Epoch 31/512
512/512 - 0s - loss: 7.4597e-04 - val_loss: 0.0041
Epoch 32/512
512/512 - 0s - loss: 5.9271e-04 - val_loss: 0.0039
Epoch 33/512
512/512 - 0s - loss: 5.7106e-04 - val_loss: 0.0037
Epoch 34/512
512/512 - 0s - loss: 5.5366e-04 - val_loss: 0.0031
Epoch 35/512
512/512 - 0s - loss: 3.8790e-04 - val_loss: 0.0028
Epoch 36/512
512/512 - 0s - loss: 3.3689e-04 - val_loss: 0.0024
Epoch 37/512
512/512 - 0s - loss: 3.2950e-04 - val_loss: 0.0020
Epoch 38/512
512/512 - 0s - loss: 2.6084e-04 - val_loss: 0.0017
Epoch 39/512
512/512 - 0s - loss: 2.1911e-04 - val_loss: 0.0015
Epoch 40/512
512/512 - 0s - loss: 2.1785e-04 - val_loss: 0.0012
Epoch 41/512
512/512 - 0s - loss: 1.8836e-04 - val_loss: 0.0011
Epoch 42/512
512/512 - 0s - loss: 1.6466e-04 - val_loss: 9.2995e-04
Epoch 43/512
512/512 - 0s - loss: 1.6000e-04 - val_loss: 8.0400e-04
Epoch 44/512
512/512 - 0s - loss: 1.5216e-04 - val_loss: 6.8411e-04
Epoch 45/512
512/512 - 0s - loss: 1.3480e-04 - val_loss: 5.8917e-04
Epoch 46/512
512/512 - 0s - loss: 1.2722e-04 - val_loss: 5.2016e-04
Epoch 47/512
512/512 - 0s - loss: 1.2628e-04 - val_loss: 4.4247e-04
Epoch 48/512
512/512 - 0s - loss: 1.1618e-04 - val_loss: 3.9238e-04
Epoch 49/512
512/512 - 0s - loss: 1.1047e-04 - val_loss: 3.3376e-04
Epoch 50/512
512/512 - 0s - loss: 1.0542e-04 - val_loss: 2.9693e-04
Epoch 51/512
512/512 - 0s - loss: 1.0444e-04 - val_loss: 2.5880e-04
Epoch 52/512
512/512 - 0s - loss: 9.6495e-05 - val_loss: 2.2583e-04
Epoch 53/512
512/512 - 0s - loss: 9.2164e-05 - val_loss: 2.0097e-04
Epoch 54/512
512/512 - 0s - loss: 9.3183e-05 - val_loss: 1.7371e-04
Epoch 55/512
512/512 - 0s - loss: 8.6478e-05 - val_loss: 1.5666e-04
Epoch 56/512
512/512 - 0s - loss: 8.1822e-05 - val_loss: 1.4001e-04
Epoch 57/512
512/512 - 0s - loss: 8.2667e-05 - val_loss: 1.2158e-04
Epoch 58/512
512/512 - 0s - loss: 7.8635e-05 - val_loss: 1.1062e-04
Epoch 59/512
512/512 - 0s - loss: 7.3150e-05 - val_loss: 1.0060e-04
Epoch 60/512
512/512 - 0s - loss: 7.4279e-05 - val_loss: 8.9190e-05
Epoch 61/512
512/512 - 0s - loss: 7.1335e-05 - val_loss: 8.2497e-05
Epoch 62/512
512/512 - 0s - loss: 6.5804e-05 - val_loss: 7.5888e-05
Epoch 63/512
512/512 - 0s - loss: 6.6805e-05 - val_loss: 6.9736e-05
Epoch 64/512
512/512 - 0s - loss: 6.4689e-05 - val_loss: 6.5685e-05
Epoch 65/512
512/512 - 0s - loss: 6.0038e-05 - val_loss: 6.2716e-05
Epoch 66/512
512/512 - 0s - loss: 6.0085e-05 - val_loss: 6.0487e-05
Epoch 67/512
512/512 - 0s - loss: 5.7963e-05 - val_loss: 5.8420e-05
Epoch 68/512
512/512 - 0s - loss: 5.5283e-05 - val_loss: 5.7464e-05
Epoch 69/512
512/512 - 0s - loss: 5.3931e-05 - val_loss: 5.7578e-05
Epoch 70/512
512/512 - 0s - loss: 5.3017e-05 - val_loss: 5.8103e-05
Epoch 71/512
512/512 - 0s - loss: 4.9919e-05 - val_loss: 5.8684e-05
Epoch 72/512
512/512 - 0s - loss: 4.8333e-05 - val_loss: 6.0517e-05
Epoch 73/512
512/512 - 0s - loss: 4.7586e-05 - val_loss: 6.2191e-05
Epoch 74/512
512/512 - 0s - loss: 4.5417e-05 - val_loss: 6.4661e-05
Epoch 75/512
512/512 - 0s - loss: 4.3332e-05 - val_loss: 6.7775e-05
Epoch 76/512
512/512 - 0s - loss: 4.2373e-05 - val_loss: 7.2720e-05
Epoch 77/512
512/512 - 0s - loss: 4.1779e-05 - val_loss: 7.6909e-05
Epoch 78/512
512/512 - 0s - loss: 3.8918e-05 - val_loss: 7.8534e-05
Epoch 79/512
512/512 - 0s - loss: 3.7165e-05 - val_loss: 8.5619e-05
Epoch 80/512
512/512 - 0s - loss: 3.6851e-05 - val_loss: 9.0281e-05
Epoch 81/512
512/512 - 0s - loss: 3.5019e-05 - val_loss: 9.3902e-05
Epoch 82/512
512/512 - 0s - loss: 3.3380e-05 - val_loss: 1.0059e-04
Epoch 83/512
512/512 - 0s - loss: 3.2257e-05 - val_loss: 1.0679e-04
Epoch 84/512
512/512 - 0s - loss: 3.0622e-05 - val_loss: 1.1357e-04
Epoch 85/512
512/512 - 0s - loss: 2.9093e-05 - val_loss: 1.1834e-04
Epoch 86/512
512/512 - 0s - loss: 2.7940e-05 - val_loss: 1.2668e-04
Epoch 87/512
512/512 - 0s - loss: 2.6953e-05 - val_loss: 1.3197e-04
Epoch 88/512
512/512 - 0s - loss: 2.5494e-05 - val_loss: 1.3937e-04
Epoch 89/512
512/512 - 0s - loss: 2.3464e-05 - val_loss: 1.4401e-04
Epoch 90/512
512/512 - 0s - loss: 2.2461e-05 - val_loss: 1.5475e-04
Epoch 91/512
512/512 - 0s - loss: 2.1669e-05 - val_loss: 1.6009e-04
Epoch 92/512
512/512 - 0s - loss: 2.0196e-05 - val_loss: 1.6490e-04
Epoch 93/512
512/512 - 0s - loss: 1.8649e-05 - val_loss: 1.6888e-04
Epoch 94/512
512/512 - 0s - loss: 1.7647e-05 - val_loss: 1.7689e-04
Epoch 95/512
512/512 - 0s - loss: 1.6913e-05 - val_loss: 1.8185e-04
Epoch 96/512
512/512 - 0s - loss: 1.5427e-05 - val_loss: 1.7977e-04
Epoch 97/512
512/512 - 0s - loss: 1.4171e-05 - val_loss: 1.8494e-04
Epoch 98/512
512/512 - 0s - loss: 1.3502e-05 - val_loss: 1.8892e-04
Epoch 99/512
512/512 - 0s - loss: 1.2551e-05 - val_loss: 1.8841e-04
Epoch 100/512
512/512 - 0s - loss: 1.1531e-05 - val_loss: 1.8887e-04
Epoch 101/512
512/512 - 0s - loss: 1.0701e-05 - val_loss: 1.8950e-04
Epoch 102/512
512/512 - 0s - loss: 1.0050e-05 - val_loss: 1.9027e-04
Epoch 103/512
512/512 - 0s - loss: 9.2567e-06 - val_loss: 1.8691e-04
Epoch 104/512
512/512 - 0s - loss: 8.4881e-06 - val_loss: 1.8525e-04
Epoch 105/512
512/512 - 0s - loss: 7.9589e-06 - val_loss: 1.8378e-04
Epoch 106/512
512/512 - 0s - loss: 7.4322e-06 - val_loss: 1.8191e-04
Epoch 107/512
512/512 - 0s - loss: 6.8646e-06 - val_loss: 1.7789e-04
Epoch 108/512
512/512 - 0s - loss: 6.3441e-06 - val_loss: 1.7434e-04
Epoch 109/512
512/512 - 0s - loss: 5.9244e-06 - val_loss: 1.7224e-04
Epoch 110/512
512/512 - 0s - loss: 5.5425e-06 - val_loss: 1.6668e-04
Epoch 111/512
512/512 - 0s - loss: 5.1244e-06 - val_loss: 1.6295e-04
Epoch 112/512
512/512 - 0s - loss: 4.7847e-06 - val_loss: 1.6022e-04
Epoch 113/512
512/512 - 0s - loss: 4.5292e-06 - val_loss: 1.5609e-04
Epoch 114/512
512/512 - 0s - loss: 4.1496e-06 - val_loss: 1.4988e-04
Epoch 115/512
512/512 - 0s - loss: 3.9124e-06 - val_loss: 1.4813e-04
Epoch 116/512
512/512 - 0s - loss: 3.6888e-06 - val_loss: 1.4322e-04
Epoch 117/512
512/512 - 0s - loss: 3.4597e-06 - val_loss: 1.4024e-04
Epoch 118/512
512/512 - 0s - loss: 3.2226e-06 - val_loss: 1.3209e-04
Epoch 119/512
512/512 - 0s - loss: 3.0023e-06 - val_loss: 1.2963e-04
Epoch 120/512
512/512 - 0s - loss: 2.8717e-06 - val_loss: 1.3019e-04
Epoch 121/512
512/512 - 0s - loss: 2.7391e-06 - val_loss: 1.2279e-04
Epoch 122/512
512/512 - 0s - loss: 2.5343e-06 - val_loss: 1.1788e-04
Epoch 123/512
512/512 - 0s - loss: 2.3995e-06 - val_loss: 1.1696e-04
Epoch 124/512
512/512 - 0s - loss: 2.2929e-06 - val_loss: 1.1219e-04
Epoch 125/512
512/512 - 0s - loss: 2.1777e-06 - val_loss: 1.0869e-04
Epoch 126/512
512/512 - 0s - loss: 2.0517e-06 - val_loss: 1.0428e-04
Epoch 127/512
512/512 - 0s - loss: 1.9361e-06 - val_loss: 1.0127e-04
Epoch 128/512
512/512 - 0s - loss: 1.8533e-06 - val_loss: 9.9318e-05
Epoch 129/512
512/512 - 0s - loss: 1.7821e-06 - val_loss: 9.7857e-05
Epoch 130/512
512/512 - 0s - loss: 1.6892e-06 - val_loss: 9.2382e-05
Epoch 131/512
512/512 - 0s - loss: 1.6034e-06 - val_loss: 8.9179e-05
Epoch 132/512
512/512 - 0s - loss: 1.5300e-06 - val_loss: 8.7083e-05
Epoch 133/512
512/512 - 0s - loss: 1.4801e-06 - val_loss: 8.5705e-05
Epoch 134/512
512/512 - 0s - loss: 1.4174e-06 - val_loss: 8.1933e-05
Epoch 135/512
512/512 - 0s - loss: 1.3449e-06 - val_loss: 7.8264e-05
Epoch 136/512
512/512 - 0s - loss: 1.2900e-06 - val_loss: 7.6716e-05
Epoch 137/512
512/512 - 0s - loss: 1.2466e-06 - val_loss: 7.5721e-05
Epoch 138/512
512/512 - 0s - loss: 1.2136e-06 - val_loss: 7.5358e-05
Epoch 139/512
512/512 - 0s - loss: 1.1617e-06 - val_loss: 6.9331e-05
Epoch 140/512
512/512 - 0s - loss: 1.1093e-06 - val_loss: 6.7486e-05
Epoch 141/512
512/512 - 0s - loss: 1.0708e-06 - val_loss: 6.7150e-05
Epoch 142/512
512/512 - 0s - loss: 1.0442e-06 - val_loss: 6.7977e-05
Epoch 143/512
512/512 - 0s - loss: 1.0184e-06 - val_loss: 6.3881e-05
Epoch 144/512
512/512 - 0s - loss: 9.6961e-07 - val_loss: 5.9465e-05
Epoch 145/512
512/512 - 0s - loss: 9.3806e-07 - val_loss: 5.6831e-05
Epoch 146/512
512/512 - 0s - loss: 9.1406e-07 - val_loss: 5.3282e-05
Epoch 147/512
512/512 - 0s - loss: 8.9487e-07 - val_loss: 5.2486e-05
Epoch 148/512
512/512 - 0s - loss: 8.6252e-07 - val_loss: 5.1542e-05
Epoch 149/512
512/512 - 0s - loss: 8.3660e-07 - val_loss: 5.1265e-05
Epoch 150/512
512/512 - 0s - loss: 8.1459e-07 - val_loss: 4.9019e-05
Epoch 151/512
512/512 - 0s - loss: 7.9380e-07 - val_loss: 5.0956e-05
Epoch 152/512
512/512 - 0s - loss: 7.7140e-07 - val_loss: 5.1563e-05
Epoch 153/512
512/512 - 0s - loss: 7.5347e-07 - val_loss: 5.1263e-05
Epoch 154/512
512/512 - 0s - loss: 7.3741e-07 - val_loss: 5.0474e-05
Epoch 155/512
512/512 - 0s - loss: 7.1955e-07 - val_loss: 4.8415e-05
Epoch 156/512
512/512 - 0s - loss: 7.0218e-07 - val_loss: 4.8829e-05
Epoch 157/512
512/512 - 0s - loss: 6.8791e-07 - val_loss: 4.7134e-05
Epoch 158/512
512/512 - 0s - loss: 6.6893e-07 - val_loss: 4.4531e-05
Epoch 159/512
512/512 - 0s - loss: 6.5450e-07 - val_loss: 4.5332e-05
Epoch 160/512
512/512 - 0s - loss: 6.4186e-07 - val_loss: 4.4812e-05
Epoch 161/512
512/512 - 0s - loss: 6.3054e-07 - val_loss: 4.5183e-05
Epoch 162/512
512/512 - 0s - loss: 6.1791e-07 - val_loss: 4.1889e-05
Epoch 163/512
512/512 - 0s - loss: 6.0233e-07 - val_loss: 4.0324e-05
Epoch 164/512
512/512 - 0s - loss: 5.9107e-07 - val_loss: 4.0157e-05
Epoch 165/512
512/512 - 0s - loss: 5.7989e-07 - val_loss: 3.8845e-05
Epoch 166/512
512/512 - 0s - loss: 5.6941e-07 - val_loss: 3.7659e-05
Epoch 167/512
512/512 - 0s - loss: 5.6063e-07 - val_loss: 3.6847e-05
Epoch 168/512
512/512 - 0s - loss: 5.5188e-07 - val_loss: 3.6526e-05
Epoch 169/512
512/512 - 0s - loss: 5.3948e-07 - val_loss: 3.7261e-05
Epoch 170/512
512/512 - 0s - loss: 5.3056e-07 - val_loss: 3.8139e-05
Epoch 171/512
512/512 - 0s - loss: 5.2139e-07 - val_loss: 3.7336e-05
Epoch 172/512
512/512 - 0s - loss: 5.1450e-07 - val_loss: 3.7139e-05
Epoch 173/512
512/512 - 0s - loss: 5.0503e-07 - val_loss: 3.5072e-05
Epoch 174/512
512/512 - 0s - loss: 4.9632e-07 - val_loss: 3.4760e-05
Epoch 175/512
512/512 - 0s - loss: 4.8912e-07 - val_loss: 3.3142e-05
Epoch 176/512
512/512 - 0s - loss: 4.8430e-07 - val_loss: 3.2996e-05
Epoch 177/512
512/512 - 0s - loss: 4.7426e-07 - val_loss: 3.3526e-05
Epoch 178/512
512/512 - 0s - loss: 4.6747e-07 - val_loss: 3.4569e-05
Epoch 179/512
512/512 - 0s - loss: 4.6134e-07 - val_loss: 3.3747e-05
Epoch 180/512
512/512 - 0s - loss: 4.5397e-07 - val_loss: 3.2470e-05
Epoch 181/512
512/512 - 0s - loss: 4.4768e-07 - val_loss: 3.1610e-05
Epoch 182/512
512/512 - 0s - loss: 4.4189e-07 - val_loss: 3.3293e-05
Epoch 183/512
512/512 - 0s - loss: 4.3592e-07 - val_loss: 3.1908e-05
Epoch 184/512
512/512 - 0s - loss: 4.3036e-07 - val_loss: 2.9696e-05
Epoch 185/512
512/512 - 0s - loss: 4.2521e-07 - val_loss: 3.0073e-05
Epoch 186/512
512/512 - 0s - loss: 4.1838e-07 - val_loss: 3.0169e-05
Epoch 187/512
512/512 - 0s - loss: 4.1308e-07 - val_loss: 3.0871e-05
Epoch 188/512
512/512 - 0s - loss: 4.0960e-07 - val_loss: 3.0907e-05
Epoch 189/512
512/512 - 0s - loss: 4.0298e-07 - val_loss: 2.8864e-05
Epoch 190/512
512/512 - 0s - loss: 3.9806e-07 - val_loss: 2.9426e-05
Epoch 191/512
512/512 - 0s - loss: 3.9321e-07 - val_loss: 2.9707e-05
Epoch 192/512
512/512 - 0s - loss: 3.8954e-07 - val_loss: 2.9406e-05
Epoch 193/512
512/512 - 0s - loss: 3.8449e-07 - val_loss: 2.9350e-05
Epoch 194/512
512/512 - 0s - loss: 3.8056e-07 - val_loss: 2.8417e-05
Epoch 195/512
512/512 - 0s - loss: 3.7544e-07 - val_loss: 2.7063e-05
Epoch 196/512
512/512 - 0s - loss: 3.7129e-07 - val_loss: 2.7298e-05
Epoch 197/512
512/512 - 0s - loss: 3.6758e-07 - val_loss: 2.8127e-05
Epoch 198/512
512/512 - 0s - loss: 3.6332e-07 - val_loss: 2.6582e-05
Epoch 199/512
512/512 - 0s - loss: 3.5934e-07 - val_loss: 2.7181e-05
Epoch 200/512
512/512 - 0s - loss: 3.5534e-07 - val_loss: 2.5964e-05
Epoch 201/512
512/512 - 0s - loss: 3.5204e-07 - val_loss: 2.5483e-05
Epoch 202/512
512/512 - 0s - loss: 3.4852e-07 - val_loss: 2.6287e-05
Epoch 203/512
512/512 - 0s - loss: 3.4452e-07 - val_loss: 2.5791e-05
Epoch 204/512
512/512 - 0s - loss: 3.4139e-07 - val_loss: 2.4957e-05
Epoch 205/512
512/512 - 0s - loss: 3.3784e-07 - val_loss: 2.5328e-05
Epoch 206/512
512/512 - 0s - loss: 3.3434e-07 - val_loss: 2.5294e-05
Epoch 207/512
512/512 - 0s - loss: 3.3147e-07 - val_loss: 2.5518e-05
Epoch 208/512
512/512 - 0s - loss: 3.2859e-07 - val_loss: 2.5201e-05
Epoch 209/512
512/512 - 0s - loss: 3.2515e-07 - val_loss: 2.5149e-05
Epoch 210/512
512/512 - 0s - loss: 3.2209e-07 - val_loss: 2.3917e-05
Epoch 211/512
512/512 - 0s - loss: 3.1877e-07 - val_loss: 2.4619e-05
Epoch 212/512
512/512 - 0s - loss: 3.1606e-07 - val_loss: 2.4218e-05
Epoch 213/512
512/512 - 0s - loss: 3.1316e-07 - val_loss: 2.3711e-05
Epoch 214/512
512/512 - 0s - loss: 3.1036e-07 - val_loss: 2.3673e-05
Epoch 215/512
512/512 - 0s - loss: 3.0763e-07 - val_loss: 2.2879e-05
Epoch 216/512
512/512 - 0s - loss: 3.0492e-07 - val_loss: 2.2905e-05
Epoch 217/512
512/512 - 0s - loss: 3.0246e-07 - val_loss: 2.2611e-05
Epoch 218/512
512/512 - 0s - loss: 2.9971e-07 - val_loss: 2.2620e-05
Epoch 219/512
512/512 - 0s - loss: 2.9727e-07 - val_loss: 2.2515e-05
Epoch 220/512
512/512 - 0s - loss: 2.9468e-07 - val_loss: 2.2907e-05
Epoch 221/512
512/512 - 0s - loss: 2.9240e-07 - val_loss: 2.2560e-05
Epoch 222/512
512/512 - 0s - loss: 2.8985e-07 - val_loss: 2.1949e-05
Epoch 223/512
512/512 - 0s - loss: 2.8759e-07 - val_loss: 2.1769e-05
Epoch 224/512
512/512 - 0s - loss: 2.8522e-07 - val_loss: 2.1702e-05
Epoch 225/512
512/512 - 0s - loss: 2.8311e-07 - val_loss: 2.1595e-05
Epoch 226/512
512/512 - 0s - loss: 2.8072e-07 - val_loss: 2.1925e-05
Epoch 227/512
512/512 - 0s - loss: 2.7857e-07 - val_loss: 2.1076e-05
Epoch 228/512
512/512 - 0s - loss: 2.7673e-07 - val_loss: 2.0802e-05
Epoch 229/512
512/512 - 0s - loss: 2.7440e-07 - val_loss: 2.1209e-05
Epoch 230/512
512/512 - 0s - loss: 2.7233e-07 - val_loss: 2.0770e-05
Epoch 231/512
512/512 - 0s - loss: 2.7021e-07 - val_loss: 2.0542e-05
Epoch 232/512
512/512 - 0s - loss: 2.6846e-07 - val_loss: 2.0295e-05
Epoch 233/512
512/512 - 0s - loss: 2.6642e-07 - val_loss: 2.0757e-05
Epoch 234/512
512/512 - 0s - loss: 2.6455e-07 - val_loss: 2.1063e-05
Epoch 235/512
512/512 - 0s - loss: 2.6314e-07 - val_loss: 2.0766e-05
Epoch 236/512
512/512 - 0s - loss: 2.6060e-07 - val_loss: 2.0490e-05
Epoch 237/512
512/512 - 0s - loss: 2.5877e-07 - val_loss: 1.9706e-05
Epoch 238/512
512/512 - 0s - loss: 2.5743e-07 - val_loss: 1.9500e-05
Epoch 239/512
512/512 - 0s - loss: 2.5524e-07 - val_loss: 1.9831e-05
Epoch 240/512
512/512 - 0s - loss: 2.5337e-07 - val_loss: 1.9557e-05
Epoch 241/512
512/512 - 0s - loss: 2.5193e-07 - val_loss: 2.0117e-05
Epoch 242/512
512/512 - 0s - loss: 2.5004e-07 - val_loss: 1.9378e-05
Epoch 243/512
512/512 - 0s - loss: 2.4832e-07 - val_loss: 1.8950e-05
Epoch 244/512
512/512 - 0s - loss: 2.4683e-07 - val_loss: 1.8844e-05
Epoch 245/512
512/512 - 0s - loss: 2.4508e-07 - val_loss: 1.9100e-05
Epoch 246/512
512/512 - 0s - loss: 2.4349e-07 - val_loss: 1.8652e-05
Epoch 247/512
512/512 - 0s - loss: 2.4263e-07 - val_loss: 1.8317e-05
Epoch 248/512
512/512 - 0s - loss: 2.4092e-07 - val_loss: 1.8209e-05
Epoch 249/512
512/512 - 0s - loss: 2.3907e-07 - val_loss: 1.8527e-05
Epoch 250/512
512/512 - 0s - loss: 2.3730e-07 - val_loss: 1.8485e-05
Epoch 251/512
512/512 - 0s - loss: 2.3584e-07 - val_loss: 1.8504e-05
Epoch 252/512
512/512 - 0s - loss: 2.3441e-07 - val_loss: 1.8808e-05
Epoch 253/512
512/512 - 0s - loss: 2.3292e-07 - val_loss: 1.8258e-05
Epoch 254/512
512/512 - 0s - loss: 2.3180e-07 - val_loss: 1.8693e-05
Epoch 255/512
512/512 - 0s - loss: 2.3071e-07 - val_loss: 1.8234e-05
Epoch 256/512
512/512 - 0s - loss: 2.2870e-07 - val_loss: 1.7933e-05
Epoch 257/512
512/512 - 0s - loss: 2.2745e-07 - val_loss: 1.7654e-05
Epoch 258/512
512/512 - 0s - loss: 2.2611e-07 - val_loss: 1.8095e-05
Epoch 259/512
512/512 - 0s - loss: 2.2483e-07 - val_loss: 1.7951e-05
Epoch 260/512
512/512 - 0s - loss: 2.2379e-07 - val_loss: 1.7702e-05
Epoch 261/512
512/512 - 0s - loss: 2.2230e-07 - val_loss: 1.7535e-05
Epoch 262/512
512/512 - 0s - loss: 2.2135e-07 - val_loss: 1.7137e-05
Epoch 263/512
512/512 - 0s - loss: 2.1966e-07 - val_loss: 1.7652e-05
Epoch 264/512
512/512 - 0s - loss: 2.1840e-07 - val_loss: 1.7166e-05
Epoch 265/512
512/512 - 0s - loss: 2.1713e-07 - val_loss: 1.7396e-05
Epoch 266/512
512/512 - 0s - loss: 2.1597e-07 - val_loss: 1.7124e-05
Epoch 267/512
512/512 - 0s - loss: 2.1473e-07 - val_loss: 1.7045e-05
Epoch 268/512
512/512 - 0s - loss: 2.1361e-07 - val_loss: 1.7162e-05
Epoch 269/512
512/512 - 0s - loss: 2.1256e-07 - val_loss: 1.7308e-05
Epoch 270/512
512/512 - 0s - loss: 2.1135e-07 - val_loss: 1.6838e-05
Epoch 271/512
512/512 - 0s - loss: 2.1012e-07 - val_loss: 1.6763e-05
Epoch 272/512
512/512 - 0s - loss: 2.0918e-07 - val_loss: 1.6649e-05
Epoch 273/512
512/512 - 0s - loss: 2.0793e-07 - val_loss: 1.6441e-05
Epoch 274/512
512/512 - 0s - loss: 2.0689e-07 - val_loss: 1.6251e-05
Epoch 275/512
512/512 - 0s - loss: 2.0584e-07 - val_loss: 1.6448e-05
Epoch 276/512
512/512 - 0s - loss: 2.0473e-07 - val_loss: 1.6387e-05
Epoch 277/512
512/512 - 0s - loss: 2.0381e-07 - val_loss: 1.6473e-05
Epoch 278/512
512/512 - 0s - loss: 2.0261e-07 - val_loss: 1.6049e-05
Epoch 279/512
512/512 - 0s - loss: 2.0167e-07 - val_loss: 1.6086e-05
Epoch 280/512
512/512 - 0s - loss: 2.0060e-07 - val_loss: 1.6059e-05
Epoch 281/512
512/512 - 0s - loss: 1.9956e-07 - val_loss: 1.5812e-05
Epoch 282/512
512/512 - 0s - loss: 1.9857e-07 - val_loss: 1.5735e-05
Epoch 283/512
512/512 - 0s - loss: 1.9759e-07 - val_loss: 1.6004e-05
Epoch 284/512
512/512 - 0s - loss: 1.9681e-07 - val_loss: 1.5881e-05
Epoch 285/512
512/512 - 0s - loss: 1.9574e-07 - val_loss: 1.5603e-05
Epoch 286/512
512/512 - 0s - loss: 1.9476e-07 - val_loss: 1.5755e-05
Epoch 287/512
512/512 - 0s - loss: 1.9381e-07 - val_loss: 1.5044e-05
Epoch 288/512
512/512 - 0s - loss: 1.9296e-07 - val_loss: 1.5559e-05
Epoch 289/512
512/512 - 0s - loss: 1.9212e-07 - val_loss: 1.5750e-05
Epoch 290/512
512/512 - 0s - loss: 1.9111e-07 - val_loss: 1.5180e-05
Epoch 291/512
512/512 - 0s - loss: 1.9018e-07 - val_loss: 1.5442e-05
Epoch 292/512
512/512 - 0s - loss: 1.8936e-07 - val_loss: 1.5303e-05
Epoch 293/512
512/512 - 0s - loss: 1.8838e-07 - val_loss: 1.4943e-05
Epoch 294/512
512/512 - 0s - loss: 1.8754e-07 - val_loss: 1.5262e-05
Epoch 295/512
512/512 - 0s - loss: 1.8673e-07 - val_loss: 1.4801e-05
Epoch 296/512
512/512 - 0s - loss: 1.8592e-07 - val_loss: 1.5200e-05
Epoch 297/512
512/512 - 0s - loss: 1.8519e-07 - val_loss: 1.5248e-05
Epoch 298/512
512/512 - 0s - loss: 1.8420e-07 - val_loss: 1.4922e-05
Epoch 299/512
512/512 - 0s - loss: 1.8336e-07 - val_loss: 1.4713e-05
Epoch 300/512
512/512 - 0s - loss: 1.8254e-07 - val_loss: 1.4719e-05
Epoch 301/512
512/512 - 0s - loss: 1.8173e-07 - val_loss: 1.4654e-05
Epoch 302/512
512/512 - 0s - loss: 1.8103e-07 - val_loss: 1.4362e-05
Epoch 303/512
512/512 - 0s - loss: 1.8018e-07 - val_loss: 1.4407e-05
Epoch 304/512
512/512 - 0s - loss: 1.7941e-07 - val_loss: 1.4608e-05
Epoch 305/512
512/512 - 0s - loss: 1.7862e-07 - val_loss: 1.4460e-05
Epoch 306/512
512/512 - 0s - loss: 1.7795e-07 - val_loss: 1.4195e-05
Epoch 307/512
512/512 - 0s - loss: 1.7710e-07 - val_loss: 1.4314e-05
Epoch 308/512
512/512 - 0s - loss: 1.7635e-07 - val_loss: 1.4101e-05
Epoch 309/512
512/512 - 0s - loss: 1.7590e-07 - val_loss: 1.3999e-05
Epoch 310/512
512/512 - 0s - loss: 1.7510e-07 - val_loss: 1.4322e-05
Epoch 311/512
512/512 - 0s - loss: 1.7415e-07 - val_loss: 1.4138e-05
Epoch 312/512
512/512 - 0s - loss: 1.7351e-07 - val_loss: 1.3903e-05
Epoch 313/512
512/512 - 0s - loss: 1.7271e-07 - val_loss: 1.4005e-05
Epoch 314/512
512/512 - 0s - loss: 1.7203e-07 - val_loss: 1.3955e-05
Epoch 315/512
512/512 - 0s - loss: 1.7135e-07 - val_loss: 1.3880e-05
Epoch 316/512
512/512 - 0s - loss: 1.7062e-07 - val_loss: 1.3984e-05
Epoch 317/512
512/512 - 0s - loss: 1.7004e-07 - val_loss: 1.3804e-05
Epoch 318/512
512/512 - 0s - loss: 1.6944e-07 - val_loss: 1.3667e-05
Epoch 319/512
512/512 - 0s - loss: 1.6863e-07 - val_loss: 1.3792e-05
Epoch 320/512
512/512 - 0s - loss: 1.6807e-07 - val_loss: 1.3472e-05
Epoch 321/512
512/512 - 0s - loss: 1.6737e-07 - val_loss: 1.3541e-05
Epoch 322/512
512/512 - 0s - loss: 1.6665e-07 - val_loss: 1.3501e-05
Epoch 323/512
512/512 - 0s - loss: 1.6597e-07 - val_loss: 1.3455e-05
Epoch 324/512
512/512 - 0s - loss: 1.6535e-07 - val_loss: 1.3682e-05
Epoch 325/512
512/512 - 0s - loss: 1.6493e-07 - val_loss: 1.3244e-05
Epoch 326/512
512/512 - 0s - loss: 1.6421e-07 - val_loss: 1.3151e-05
Epoch 327/512
512/512 - 0s - loss: 1.6356e-07 - val_loss: 1.3201e-05
Epoch 328/512
512/512 - 0s - loss: 1.6290e-07 - val_loss: 1.3264e-05
Epoch 329/512
512/512 - 0s - loss: 1.6222e-07 - val_loss: 1.3069e-05
Epoch 330/512
512/512 - 0s - loss: 1.6175e-07 - val_loss: 1.3142e-05
Epoch 331/512
512/512 - 0s - loss: 1.6102e-07 - val_loss: 1.3062e-05
Epoch 332/512
512/512 - 0s - loss: 1.6044e-07 - val_loss: 1.2986e-05
Epoch 333/512
512/512 - 0s - loss: 1.6000e-07 - val_loss: 1.2758e-05
Epoch 334/512
512/512 - 0s - loss: 1.5922e-07 - val_loss: 1.3088e-05
Epoch 335/512
512/512 - 0s - loss: 1.5877e-07 - val_loss: 1.2715e-05
Epoch 336/512
512/512 - 0s - loss: 1.5809e-07 - val_loss: 1.2977e-05
Epoch 337/512
512/512 - 0s - loss: 1.5760e-07 - val_loss: 1.2562e-05
Epoch 338/512
512/512 - 0s - loss: 1.5708e-07 - val_loss: 1.2765e-05
Epoch 339/512
512/512 - 0s - loss: 1.5637e-07 - val_loss: 1.2815e-05
Epoch 340/512
512/512 - 0s - loss: 1.5583e-07 - val_loss: 1.2757e-05
Epoch 341/512
512/512 - 0s - loss: 1.5528e-07 - val_loss: 1.2506e-05
Epoch 342/512
512/512 - 0s - loss: 1.5475e-07 - val_loss: 1.2755e-05
Epoch 343/512
512/512 - 0s - loss: 1.5424e-07 - val_loss: 1.2419e-05
Epoch 344/512
512/512 - 0s - loss: 1.5366e-07 - val_loss: 1.2581e-05
Epoch 345/512
512/512 - 0s - loss: 1.5313e-07 - val_loss: 1.2677e-05
Epoch 346/512
512/512 - 0s - loss: 1.5274e-07 - val_loss: 1.2239e-05
Epoch 347/512
512/512 - 0s - loss: 1.5210e-07 - val_loss: 1.2457e-05
Epoch 348/512
512/512 - 0s - loss: 1.5168e-07 - val_loss: 1.2438e-05
Epoch 349/512
512/512 - 0s - loss: 1.5106e-07 - val_loss: 1.2223e-05
Epoch 350/512
512/512 - 0s - loss: 1.5051e-07 - val_loss: 1.2482e-05
Epoch 351/512
512/512 - 0s - loss: 1.5003e-07 - val_loss: 1.2207e-05
Epoch 352/512
512/512 - 0s - loss: 1.4966e-07 - val_loss: 1.2423e-05
Epoch 353/512
512/512 - 0s - loss: 1.4917e-07 - val_loss: 1.2066e-05
Epoch 354/512
512/512 - 0s - loss: 1.4876e-07 - val_loss: 1.1917e-05
Epoch 355/512
512/512 - 0s - loss: 1.4805e-07 - val_loss: 1.2017e-05
Epoch 356/512
512/512 - 0s - loss: 1.4756e-07 - val_loss: 1.2042e-05
Epoch 357/512
512/512 - 0s - loss: 1.4705e-07 - val_loss: 1.2065e-05
Epoch 358/512
512/512 - 0s - loss: 1.4655e-07 - val_loss: 1.2022e-05
Epoch 359/512
512/512 - 0s - loss: 1.4608e-07 - val_loss: 1.1965e-05
Epoch 360/512
512/512 - 0s - loss: 1.4562e-07 - val_loss: 1.1884e-05
Epoch 361/512
512/512 - 0s - loss: 1.4518e-07 - val_loss: 1.2080e-05
Epoch 362/512
512/512 - 0s - loss: 1.4476e-07 - val_loss: 1.2070e-05
Epoch 363/512
512/512 - 0s - loss: 1.4421e-07 - val_loss: 1.1901e-05
Epoch 364/512
512/512 - 0s - loss: 1.4377e-07 - val_loss: 1.1932e-05
Epoch 365/512
512/512 - 0s - loss: 1.4331e-07 - val_loss: 1.1833e-05
Epoch 366/512
512/512 - 0s - loss: 1.4294e-07 - val_loss: 1.1618e-05
Epoch 367/512
512/512 - 0s - loss: 1.4241e-07 - val_loss: 1.1749e-05
Epoch 368/512
512/512 - 0s - loss: 1.4200e-07 - val_loss: 1.1810e-05
Epoch 369/512
512/512 - 0s - loss: 1.4154e-07 - val_loss: 1.1542e-05
Epoch 370/512
512/512 - 0s - loss: 1.4109e-07 - val_loss: 1.1661e-05
Epoch 371/512
512/512 - 0s - loss: 1.4067e-07 - val_loss: 1.1401e-05
Epoch 372/512
512/512 - 0s - loss: 1.4024e-07 - val_loss: 1.1548e-05
Epoch 373/512
512/512 - 0s - loss: 1.3979e-07 - val_loss: 1.1447e-05
Epoch 374/512
512/512 - 0s - loss: 1.3939e-07 - val_loss: 1.1322e-05
Epoch 375/512
512/512 - 0s - loss: 1.3893e-07 - val_loss: 1.1572e-05
Epoch 376/512
512/512 - 0s - loss: 1.3855e-07 - val_loss: 1.1265e-05
Epoch 377/512
512/512 - 0s - loss: 1.3812e-07 - val_loss: 1.1367e-05
Epoch 378/512
512/512 - 0s - loss: 1.3768e-07 - val_loss: 1.1195e-05
Epoch 379/512
512/512 - 0s - loss: 1.3732e-07 - val_loss: 1.1283e-05
Epoch 380/512
512/512 - 0s - loss: 1.3695e-07 - val_loss: 1.1223e-05
Epoch 381/512
512/512 - 0s - loss: 1.3651e-07 - val_loss: 1.1428e-05
Epoch 382/512
512/512 - 0s - loss: 1.3606e-07 - val_loss: 1.1100e-05
Epoch 383/512
512/512 - 0s - loss: 1.3565e-07 - val_loss: 1.1343e-05
Epoch 384/512
512/512 - 0s - loss: 1.3531e-07 - val_loss: 1.1097e-05
Epoch 385/512
512/512 - 0s - loss: 1.3491e-07 - val_loss: 1.1227e-05
Epoch 386/512
512/512 - 0s - loss: 1.3450e-07 - val_loss: 1.0960e-05
Epoch 387/512
512/512 - 0s - loss: 1.3416e-07 - val_loss: 1.1062e-05
Epoch 388/512
512/512 - 0s - loss: 1.3374e-07 - val_loss: 1.1137e-05
Epoch 389/512
512/512 - 0s - loss: 1.3335e-07 - val_loss: 1.1000e-05
Epoch 390/512
512/512 - 0s - loss: 1.3300e-07 - val_loss: 1.1105e-05
Epoch 391/512
512/512 - 0s - loss: 1.3258e-07 - val_loss: 1.1027e-05
Epoch 392/512
512/512 - 0s - loss: 1.3224e-07 - val_loss: 1.0920e-05
Epoch 393/512
512/512 - 0s - loss: 1.3194e-07 - val_loss: 1.1079e-05
Epoch 394/512
512/512 - 0s - loss: 1.3158e-07 - val_loss: 1.0955e-05
Epoch 395/512
512/512 - 0s - loss: 1.3118e-07 - val_loss: 1.0820e-05
Epoch 396/512
512/512 - 0s - loss: 1.3074e-07 - val_loss: 1.0759e-05
Epoch 397/512
512/512 - 0s - loss: 1.3035e-07 - val_loss: 1.0698e-05
Epoch 398/512
512/512 - 0s - loss: 1.3006e-07 - val_loss: 1.0708e-05
Epoch 399/512
512/512 - 0s - loss: 1.2965e-07 - val_loss: 1.0570e-05
Epoch 400/512
512/512 - 0s - loss: 1.2929e-07 - val_loss: 1.0701e-05
Epoch 401/512
512/512 - 0s - loss: 1.2900e-07 - val_loss: 1.0671e-05
Epoch 402/512
512/512 - 0s - loss: 1.2859e-07 - val_loss: 1.0510e-05
Epoch 403/512
512/512 - 0s - loss: 1.2825e-07 - val_loss: 1.0653e-05
Epoch 404/512
512/512 - 0s - loss: 1.2792e-07 - val_loss: 1.0603e-05
Epoch 405/512
512/512 - 0s - loss: 1.2757e-07 - val_loss: 1.0492e-05
Epoch 406/512
512/512 - 0s - loss: 1.2721e-07 - val_loss: 1.0616e-05
Epoch 407/512
512/512 - 0s - loss: 1.2687e-07 - val_loss: 1.0439e-05
Epoch 408/512
512/512 - 0s - loss: 1.2652e-07 - val_loss: 1.0601e-05
Epoch 409/512
512/512 - 0s - loss: 1.2619e-07 - val_loss: 1.0529e-05
Epoch 410/512
512/512 - 0s - loss: 1.2584e-07 - val_loss: 1.0366e-05
Epoch 411/512
512/512 - 0s - loss: 1.2552e-07 - val_loss: 1.0453e-05
Epoch 412/512
512/512 - 0s - loss: 1.2523e-07 - val_loss: 1.0450e-05
Epoch 413/512
512/512 - 0s - loss: 1.2502e-07 - val_loss: 1.0244e-05
Epoch 414/512
512/512 - 0s - loss: 1.2457e-07 - val_loss: 1.0239e-05
Epoch 415/512
512/512 - 0s - loss: 1.2422e-07 - val_loss: 1.0264e-05
Epoch 416/512
512/512 - 0s - loss: 1.2392e-07 - val_loss: 1.0093e-05
Epoch 417/512
512/512 - 0s - loss: 1.2369e-07 - val_loss: 1.0143e-05
Epoch 418/512
512/512 - 0s - loss: 1.2326e-07 - val_loss: 1.0407e-05
Epoch 419/512
512/512 - 0s - loss: 1.2301e-07 - val_loss: 1.0256e-05
Epoch 420/512
512/512 - 0s - loss: 1.2264e-07 - val_loss: 1.0240e-05
Epoch 421/512
512/512 - 0s - loss: 1.2235e-07 - val_loss: 1.0206e-05
Epoch 422/512
512/512 - 0s - loss: 1.2203e-07 - val_loss: 1.0037e-05
Epoch 423/512
512/512 - 0s - loss: 1.2171e-07 - val_loss: 1.0051e-05
Epoch 424/512
512/512 - 0s - loss: 1.2141e-07 - val_loss: 9.9957e-06
Epoch 425/512
512/512 - 0s - loss: 1.2109e-07 - val_loss: 1.0137e-05
Epoch 426/512
512/512 - 0s - loss: 1.2087e-07 - val_loss: 1.0093e-05
Epoch 427/512
512/512 - 0s - loss: 1.2053e-07 - val_loss: 1.0135e-05
Epoch 428/512
512/512 - 0s - loss: 1.2020e-07 - val_loss: 1.0033e-05
Epoch 429/512
512/512 - 0s - loss: 1.1989e-07 - val_loss: 9.8872e-06
Epoch 430/512
512/512 - 0s - loss: 1.1961e-07 - val_loss: 9.8624e-06
Epoch 431/512
512/512 - 0s - loss: 1.1938e-07 - val_loss: 9.6958e-06
Epoch 432/512
512/512 - 0s - loss: 1.1903e-07 - val_loss: 9.9047e-06
Epoch 433/512
512/512 - 0s - loss: 1.1873e-07 - val_loss: 9.9120e-06
Epoch 434/512
512/512 - 0s - loss: 1.1850e-07 - val_loss: 9.8367e-06
Epoch 435/512
512/512 - 0s - loss: 1.1815e-07 - val_loss: 9.8696e-06
Epoch 436/512
512/512 - 0s - loss: 1.1787e-07 - val_loss: 9.7853e-06
Epoch 437/512
512/512 - 0s - loss: 1.1761e-07 - val_loss: 9.8151e-06
Epoch 438/512
512/512 - 0s - loss: 1.1732e-07 - val_loss: 9.5343e-06
Epoch 439/512
512/512 - 0s - loss: 1.1704e-07 - val_loss: 9.8113e-06
Epoch 440/512
512/512 - 0s - loss: 1.1675e-07 - val_loss: 9.7831e-06
Epoch 441/512
512/512 - 0s - loss: 1.1648e-07 - val_loss: 9.6604e-06
Epoch 442/512
512/512 - 0s - loss: 1.1625e-07 - val_loss: 9.7221e-06
Epoch 443/512
512/512 - 0s - loss: 1.1596e-07 - val_loss: 9.5102e-06
Epoch 444/512
512/512 - 0s - loss: 1.1578e-07 - val_loss: 9.8547e-06
Epoch 445/512
512/512 - 0s - loss: 1.1540e-07 - val_loss: 9.7158e-06
Epoch 446/512
512/512 - 0s - loss: 1.1519e-07 - val_loss: 9.5744e-06
Epoch 447/512
512/512 - 0s - loss: 1.1486e-07 - val_loss: 9.5226e-06
Epoch 448/512
512/512 - 0s - loss: 1.1459e-07 - val_loss: 9.5665e-06
Epoch 449/512
512/512 - 0s - loss: 1.1436e-07 - val_loss: 9.6536e-06
Epoch 450/512
512/512 - 0s - loss: 1.1417e-07 - val_loss: 9.6761e-06
Epoch 451/512
512/512 - 0s - loss: 1.1392e-07 - val_loss: 9.6249e-06
Epoch 452/512
512/512 - 0s - loss: 1.1356e-07 - val_loss: 9.5605e-06
Epoch 453/512
512/512 - 0s - loss: 1.1328e-07 - val_loss: 9.4739e-06
Epoch 454/512
512/512 - 0s - loss: 1.1303e-07 - val_loss: 9.4022e-06
Epoch 455/512
512/512 - 0s - loss: 1.1289e-07 - val_loss: 9.3395e-06
Epoch 456/512
512/512 - 0s - loss: 1.1255e-07 - val_loss: 9.3174e-06
Epoch 457/512
512/512 - 0s - loss: 1.1233e-07 - val_loss: 9.2598e-06
Epoch 458/512
512/512 - 0s - loss: 1.1200e-07 - val_loss: 9.4371e-06
Epoch 459/512
512/512 - 0s - loss: 1.1183e-07 - val_loss: 9.2902e-06
Epoch 460/512
512/512 - 0s - loss: 1.1154e-07 - val_loss: 9.3576e-06
Epoch 461/512
512/512 - 0s - loss: 1.1132e-07 - val_loss: 9.1943e-06
Epoch 462/512
512/512 - 0s - loss: 1.1102e-07 - val_loss: 9.2087e-06
Epoch 463/512
512/512 - 0s - loss: 1.1077e-07 - val_loss: 9.4113e-06
Epoch 464/512
512/512 - 0s - loss: 1.1059e-07 - val_loss: 9.0394e-06
Epoch 465/512
512/512 - 0s - loss: 1.1030e-07 - val_loss: 9.3636e-06
Epoch 466/512
512/512 - 0s - loss: 1.1009e-07 - val_loss: 9.1638e-06
Epoch 467/512
512/512 - 0s - loss: 1.0986e-07 - val_loss: 9.0268e-06
Epoch 468/512
512/512 - 0s - loss: 1.0963e-07 - val_loss: 9.2698e-06
Epoch 469/512
512/512 - 0s - loss: 1.0935e-07 - val_loss: 9.2069e-06
Epoch 470/512
512/512 - 0s - loss: 1.0911e-07 - val_loss: 9.1551e-06
Epoch 471/512
512/512 - 0s - loss: 1.0888e-07 - val_loss: 9.0769e-06
Epoch 472/512
512/512 - 0s - loss: 1.0862e-07 - val_loss: 9.1133e-06
Epoch 473/512
512/512 - 0s - loss: 1.0843e-07 - val_loss: 9.1012e-06
Epoch 474/512
512/512 - 0s - loss: 1.0818e-07 - val_loss: 8.9607e-06
Epoch 475/512
512/512 - 0s - loss: 1.0794e-07 - val_loss: 9.0444e-06
Epoch 476/512
512/512 - 0s - loss: 1.0770e-07 - val_loss: 9.0476e-06
Epoch 477/512
512/512 - 0s - loss: 1.0747e-07 - val_loss: 8.9613e-06
Epoch 478/512
512/512 - 0s - loss: 1.0725e-07 - val_loss: 8.9480e-06
Epoch 479/512
512/512 - 0s - loss: 1.0704e-07 - val_loss: 8.8504e-06
Epoch 480/512
512/512 - 0s - loss: 1.0688e-07 - val_loss: 8.9836e-06
Epoch 481/512
512/512 - 0s - loss: 1.0662e-07 - val_loss: 8.9641e-06
Epoch 482/512
512/512 - 0s - loss: 1.0636e-07 - val_loss: 8.8352e-06
Epoch 483/512
512/512 - 0s - loss: 1.0613e-07 - val_loss: 8.8483e-06
Epoch 484/512
512/512 - 0s - loss: 1.0593e-07 - val_loss: 8.9273e-06
Epoch 485/512
512/512 - 0s - loss: 1.0573e-07 - val_loss: 8.8985e-06
Epoch 486/512
512/512 - 0s - loss: 1.0550e-07 - val_loss: 8.8989e-06
Epoch 487/512
512/512 - 0s - loss: 1.0529e-07 - val_loss: 8.8851e-06
Epoch 488/512
512/512 - 0s - loss: 1.0508e-07 - val_loss: 8.8215e-06
Epoch 489/512
512/512 - 0s - loss: 1.0483e-07 - val_loss: 8.7462e-06
Epoch 490/512
512/512 - 0s - loss: 1.0465e-07 - val_loss: 8.8124e-06
Epoch 491/512
512/512 - 0s - loss: 1.0444e-07 - val_loss: 8.7961e-06
Epoch 492/512
512/512 - 0s - loss: 1.0423e-07 - val_loss: 8.7947e-06
Epoch 493/512
512/512 - 0s - loss: 1.0408e-07 - val_loss: 8.7942e-06
Epoch 494/512
512/512 - 0s - loss: 1.0381e-07 - val_loss: 8.7116e-06
Epoch 495/512
512/512 - 0s - loss: 1.0366e-07 - val_loss: 8.5825e-06
Epoch 496/512
512/512 - 0s - loss: 1.0337e-07 - val_loss: 8.6523e-06
Epoch 497/512
512/512 - 0s - loss: 1.0316e-07 - val_loss: 8.7255e-06
Epoch 498/512
512/512 - 0s - loss: 1.0295e-07 - val_loss: 8.5672e-06
Epoch 499/512
512/512 - 0s - loss: 1.0277e-07 - val_loss: 8.5344e-06
Epoch 500/512
512/512 - 0s - loss: 1.0257e-07 - val_loss: 8.6437e-06
Epoch 501/512
512/512 - 0s - loss: 1.0234e-07 - val_loss: 8.5820e-06
Epoch 502/512
512/512 - 0s - loss: 1.0218e-07 - val_loss: 8.5996e-06
Epoch 503/512
512/512 - 0s - loss: 1.0199e-07 - val_loss: 8.3767e-06
Epoch 504/512
512/512 - 0s - loss: 1.0178e-07 - val_loss: 8.5067e-06
Epoch 505/512
512/512 - 0s - loss: 1.0157e-07 - val_loss: 8.5072e-06
Epoch 506/512
512/512 - 0s - loss: 1.0135e-07 - val_loss: 8.4786e-06
Epoch 507/512
512/512 - 0s - loss: 1.0115e-07 - val_loss: 8.3767e-06
Epoch 508/512
512/512 - 0s - loss: 1.0097e-07 - val_loss: 8.5693e-06
Epoch 509/512
512/512 - 0s - loss: 1.0077e-07 - val_loss: 8.4031e-06
Epoch 510/512
512/512 - 0s - loss: 1.0057e-07 - val_loss: 8.4527e-06
Epoch 511/512
512/512 - 0s - loss: 1.0040e-07 - val_loss: 8.4920e-06
Epoch 512/512
512/512 - 0s - loss: 1.0020e-07 - val_loss: 8.5426e-06
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7583e-09 - val_loss: 1.9003e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7543e-09 - val_loss: 1.6057e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5514e-09 - val_loss: 1.4642e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4199e-09 - val_loss: 1.3481e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3110e-09 - val_loss: 1.2502e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2188e-09 - val_loss: 1.1669e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1399e-09 - val_loss: 1.0948e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0715e-09 - val_loss: 1.0323e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0118e-09 - val_loss: 9.7717e-10
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.5904e-10 - val_loss: 9.2831e-10
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.1209e-10 - val_loss: 8.8447e-10
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.7013e-10 - val_loss: 8.4540e-10
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.3231e-10 - val_loss: 8.0954e-10
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.9795e-10 - val_loss: 7.7746e-10
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.6659e-10 - val_loss: 7.4770e-10
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.3795e-10 - val_loss: 7.2057e-10
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1153e-10 - val_loss: 6.9544e-10
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.8732e-10 - val_loss: 6.7219e-10
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6471e-10 - val_loss: 6.5113e-10
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.4379e-10 - val_loss: 6.3079e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2427e-10 - val_loss: 6.1231e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0606e-10 - val_loss: 5.9461e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8895e-10 - val_loss: 5.7834e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7284e-10 - val_loss: 5.6287e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5769e-10 - val_loss: 5.4821e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.4341e-10 - val_loss: 5.3458e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.3002e-10 - val_loss: 5.2146e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1722e-10 - val_loss: 5.0915e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.0506e-10 - val_loss: 4.9747e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.9358e-10 - val_loss: 4.8613e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8268e-10 - val_loss: 4.7564e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7217e-10 - val_loss: 4.6540e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6221e-10 - val_loss: 4.5583e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5272e-10 - val_loss: 4.4677e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4373e-10 - val_loss: 4.3795e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3498e-10 - val_loss: 4.2930e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2660e-10 - val_loss: 4.2133e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1863e-10 - val_loss: 4.1356e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1092e-10 - val_loss: 4.0586e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0351e-10 - val_loss: 3.9889e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9649e-10 - val_loss: 3.9184e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8958e-10 - val_loss: 3.8518e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8298e-10 - val_loss: 3.7879e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7668e-10 - val_loss: 3.7250e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7050e-10 - val_loss: 3.6651e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6461e-10 - val_loss: 3.6070e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5885e-10 - val_loss: 3.5505e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5325e-10 - val_loss: 3.4968e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4795e-10 - val_loss: 3.4438e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4267e-10 - val_loss: 3.3929e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3770e-10 - val_loss: 3.3438e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3280e-10 - val_loss: 3.2957e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2812e-10 - val_loss: 3.2484e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2345e-10 - val_loss: 3.2040e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1903e-10 - val_loss: 3.1608e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1473e-10 - val_loss: 3.1176e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1048e-10 - val_loss: 3.0776e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0645e-10 - val_loss: 3.0366e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0243e-10 - val_loss: 2.9983e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9859e-10 - val_loss: 2.9601e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9484e-10 - val_loss: 2.9230e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9115e-10 - val_loss: 2.8867e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8755e-10 - val_loss: 2.8512e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8409e-10 - val_loss: 2.8174e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8066e-10 - val_loss: 2.7832e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7733e-10 - val_loss: 2.7509e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7406e-10 - val_loss: 2.7193e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7093e-10 - val_loss: 2.6874e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6785e-10 - val_loss: 2.6579e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6482e-10 - val_loss: 2.6289e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6191e-10 - val_loss: 2.5990e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5903e-10 - val_loss: 2.5704e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5620e-10 - val_loss: 2.5425e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5346e-10 - val_loss: 2.5165e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5080e-10 - val_loss: 2.4890e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4812e-10 - val_loss: 2.4641e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4563e-10 - val_loss: 2.4369e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4303e-10 - val_loss: 2.4129e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4061e-10 - val_loss: 2.3887e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3817e-10 - val_loss: 2.3651e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3579e-10 - val_loss: 2.3418e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3350e-10 - val_loss: 2.3186e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3118e-10 - val_loss: 2.2970e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2904e-10 - val_loss: 2.2749e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2683e-10 - val_loss: 2.2531e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2467e-10 - val_loss: 2.2312e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2255e-10 - val_loss: 2.2115e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2049e-10 - val_loss: 2.1903e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1851e-10 - val_loss: 2.1706e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1649e-10 - val_loss: 2.1512e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1456e-10 - val_loss: 2.1310e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1262e-10 - val_loss: 2.1139e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1078e-10 - val_loss: 2.0940e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0892e-10 - val_loss: 2.0765e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0711e-10 - val_loss: 2.0581e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0533e-10 - val_loss: 2.0413e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0358e-10 - val_loss: 2.0228e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0184e-10 - val_loss: 2.0065e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0016e-10 - val_loss: 1.9897e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9857e-10 - val_loss: 1.9732e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9691e-10 - val_loss: 1.9577e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9535e-10 - val_loss: 1.9409e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9369e-10 - val_loss: 1.9252e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9215e-10 - val_loss: 1.9103e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9059e-10 - val_loss: 1.8955e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8913e-10 - val_loss: 1.8799e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8761e-10 - val_loss: 1.8656e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8620e-10 - val_loss: 1.8515e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8475e-10 - val_loss: 1.8371e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8336e-10 - val_loss: 1.8234e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8199e-10 - val_loss: 1.8094e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8061e-10 - val_loss: 1.7967e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7928e-10 - val_loss: 1.7828e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7793e-10 - val_loss: 1.7701e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7662e-10 - val_loss: 1.7570e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7535e-10 - val_loss: 1.7447e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7408e-10 - val_loss: 1.7308e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7281e-10 - val_loss: 1.7185e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7168e-10 - val_loss: 1.7085e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7062e-10 - val_loss: 1.6979e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6943e-10 - val_loss: 1.6858e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6821e-10 - val_loss: 1.6731e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6710e-10 - val_loss: 1.6634e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6609e-10 - val_loss: 1.6525e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6509e-10 - val_loss: 1.6441e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6410e-10 - val_loss: 1.6319e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6289e-10 - val_loss: 1.6198e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6175e-10 - val_loss: 1.6108e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6084e-10 - val_loss: 1.6003e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6007e-10 - val_loss: 1.5948e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5934e-10 - val_loss: 1.5872e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5907e-10 - val_loss: 1.5914e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5935e-10 - val_loss: 1.5936e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5985e-10 - val_loss: 1.6002e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6053e-10 - val_loss: 1.6072e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6123e-10 - val_loss: 1.6138e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6195e-10 - val_loss: 1.6236e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6274e-10 - val_loss: 1.6267e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6265e-10 - val_loss: 1.6220e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6215e-10 - val_loss: 1.6164e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6183e-10 - val_loss: 1.6241e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6325e-10 - val_loss: 1.6338e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6383e-10 - val_loss: 1.6329e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6330e-10 - val_loss: 1.6204e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6254e-10 - val_loss: 1.6246e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6281e-10 - val_loss: 1.6357e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6367e-10 - val_loss: 1.6329e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6376e-10 - val_loss: 1.6368e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6406e-10 - val_loss: 1.6402e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6542e-10 - val_loss: 1.6536e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6563e-10 - val_loss: 1.6569e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6609e-10 - val_loss: 1.6583e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6578e-10 - val_loss: 1.6590e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6545e-10 - val_loss: 1.6493e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6537e-10 - val_loss: 1.6521e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6552e-10 - val_loss: 1.6646e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6581e-10 - val_loss: 1.6524e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6541e-10 - val_loss: 1.6529e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6557e-10 - val_loss: 1.6446e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6585e-10 - val_loss: 1.6624e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6640e-10 - val_loss: 1.6546e-10
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6604e-10 - val_loss: 1.6588e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6624e-10 - val_loss: 1.6640e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6740e-10 - val_loss: 1.6766e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6766e-10 - val_loss: 1.6731e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6742e-10 - val_loss: 1.6680e-10
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6763e-10 - val_loss: 1.6769e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6699e-10 - val_loss: 1.6686e-10
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6712e-10 - val_loss: 1.6637e-10
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6583e-10 - val_loss: 1.6570e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6650e-10 - val_loss: 1.6683e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6791e-10 - val_loss: 1.6874e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6990e-10 - val_loss: 1.7015e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7119e-10 - val_loss: 1.7018e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7117e-10 - val_loss: 1.7102e-10
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7021e-10 - val_loss: 1.6975e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7022e-10 - val_loss: 1.6791e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6811e-10 - val_loss: 1.6633e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6662e-10 - val_loss: 1.6637e-10
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6654e-10 - val_loss: 1.6577e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6697e-10 - val_loss: 1.6773e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6827e-10 - val_loss: 1.6837e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7047e-10 - val_loss: 1.7127e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7227e-10 - val_loss: 1.7148e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7094e-10 - val_loss: 1.6942e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6969e-10 - val_loss: 1.6966e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6939e-10 - val_loss: 1.6705e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6673e-10 - val_loss: 1.6554e-10
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6607e-10 - val_loss: 1.6559e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6574e-10 - val_loss: 1.6522e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6559e-10 - val_loss: 1.6573e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6764e-10 - val_loss: 1.6925e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6967e-10 - val_loss: 1.6980e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6946e-10 - val_loss: 1.6941e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6920e-10 - val_loss: 1.6798e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6929e-10 - val_loss: 1.6929e-10
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6977e-10 - val_loss: 1.6796e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6848e-10 - val_loss: 1.6631e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6555e-10 - val_loss: 1.6516e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6437e-10 - val_loss: 1.6421e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6640e-10 - val_loss: 1.6778e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6913e-10 - val_loss: 1.6902e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6921e-10 - val_loss: 1.6823e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6899e-10 - val_loss: 1.6869e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6737e-10 - val_loss: 1.6613e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6709e-10 - val_loss: 1.6700e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6669e-10 - val_loss: 1.6513e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6563e-10 - val_loss: 1.6548e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6585e-10 - val_loss: 1.6502e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6533e-10 - val_loss: 1.6486e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6578e-10 - val_loss: 1.6579e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6714e-10 - val_loss: 1.6673e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6666e-10 - val_loss: 1.6545e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6598e-10 - val_loss: 1.6497e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6515e-10 - val_loss: 1.6503e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6477e-10 - val_loss: 1.6335e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6308e-10 - val_loss: 1.6161e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6258e-10 - val_loss: 1.6268e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6251e-10 - val_loss: 1.6126e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6228e-10 - val_loss: 1.6223e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6250e-10 - val_loss: 1.6296e-10
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6347e-10 - val_loss: 1.6260e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6323e-10 - val_loss: 1.6345e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6341e-10 - val_loss: 1.6206e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6192e-10 - val_loss: 1.6249e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6364e-10 - val_loss: 1.6348e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6348e-10 - val_loss: 1.6405e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6545e-10 - val_loss: 1.6441e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6452e-10 - val_loss: 1.6286e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6208e-10 - val_loss: 1.6027e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5986e-10 - val_loss: 1.5942e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6001e-10 - val_loss: 1.5978e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6159e-10 - val_loss: 1.6142e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6110e-10 - val_loss: 1.5941e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5890e-10 - val_loss: 1.5847e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5850e-10 - val_loss: 1.5734e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5813e-10 - val_loss: 1.5822e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5809e-10 - val_loss: 1.5799e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5821e-10 - val_loss: 1.5813e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5814e-10 - val_loss: 1.5770e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5848e-10 - val_loss: 1.5876e-10
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5962e-10 - val_loss: 1.5935e-10
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5945e-10 - val_loss: 1.5864e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5859e-10 - val_loss: 1.5711e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5589e-10 - val_loss: 1.5403e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5518e-10 - val_loss: 1.5521e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5497e-10 - val_loss: 1.5372e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5456e-10 - val_loss: 1.5519e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5568e-10 - val_loss: 1.5460e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5459e-10 - val_loss: 1.5464e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5383e-10 - val_loss: 1.5249e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5301e-10 - val_loss: 1.5286e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5380e-10 - val_loss: 1.5428e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5480e-10 - val_loss: 1.5462e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5582e-10 - val_loss: 1.5440e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5413e-10 - val_loss: 1.5185e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5132e-10 - val_loss: 1.4939e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4976e-10 - val_loss: 1.4853e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4892e-10 - val_loss: 1.4883e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4988e-10 - val_loss: 1.4974e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5065e-10 - val_loss: 1.5072e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5148e-10 - val_loss: 1.5116e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5109e-10 - val_loss: 1.5016e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5077e-10 - val_loss: 1.4956e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4977e-10 - val_loss: 1.4916e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4955e-10 - val_loss: 1.4825e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4767e-10 - val_loss: 1.4686e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4704e-10 - val_loss: 1.4630e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4736e-10 - val_loss: 1.4739e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4821e-10 - val_loss: 1.4867e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4944e-10 - val_loss: 1.4824e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4831e-10 - val_loss: 1.4748e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4740e-10 - val_loss: 1.4677e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4717e-10 - val_loss: 1.4626e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4634e-10 - val_loss: 1.4429e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4589e-10 - val_loss: 1.4547e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4489e-10 - val_loss: 1.4381e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4366e-10 - val_loss: 1.4332e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4327e-10 - val_loss: 1.4340e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4439e-10 - val_loss: 1.4502e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4597e-10 - val_loss: 1.4557e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4587e-10 - val_loss: 1.4554e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4483e-10 - val_loss: 1.4381e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4407e-10 - val_loss: 1.4215e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4244e-10 - val_loss: 1.4209e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4215e-10 - val_loss: 1.4127e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4115e-10 - val_loss: 1.4013e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3990e-10 - val_loss: 1.3899e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3913e-10 - val_loss: 1.3879e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3918e-10 - val_loss: 1.3901e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4075e-10 - val_loss: 1.4306e-10
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4322e-10 - val_loss: 1.4234e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4263e-10 - val_loss: 1.4172e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4211e-10 - val_loss: 1.4131e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4094e-10 - val_loss: 1.3829e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3894e-10 - val_loss: 1.3877e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3899e-10 - val_loss: 1.3773e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3818e-10 - val_loss: 1.3914e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4033e-10 - val_loss: 1.4009e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4103e-10 - val_loss: 1.4067e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4044e-10 - val_loss: 1.3954e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3941e-10 - val_loss: 1.3698e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3700e-10 - val_loss: 1.3650e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3620e-10 - val_loss: 1.3589e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3558e-10 - val_loss: 1.3490e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3515e-10 - val_loss: 1.3510e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3566e-10 - val_loss: 1.3542e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3556e-10 - val_loss: 1.3443e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3448e-10 - val_loss: 1.3452e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3563e-10 - val_loss: 1.3568e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3712e-10 - val_loss: 1.3744e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3699e-10 - val_loss: 1.3504e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3511e-10 - val_loss: 1.3385e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3438e-10 - val_loss: 1.3377e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3468e-10 - val_loss: 1.3426e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3501e-10 - val_loss: 1.3470e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3474e-10 - val_loss: 1.3296e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3285e-10 - val_loss: 1.3194e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3178e-10 - val_loss: 1.3077e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3041e-10 - val_loss: 1.2924e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2987e-10 - val_loss: 1.2951e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3054e-10 - val_loss: 1.3016e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3056e-10 - val_loss: 1.3008e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3080e-10 - val_loss: 1.3132e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3230e-10 - val_loss: 1.3204e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3237e-10 - val_loss: 1.3225e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3123e-10 - val_loss: 1.2882e-10
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2954e-10 - val_loss: 1.2875e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2809e-10 - val_loss: 1.2717e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2769e-10 - val_loss: 1.2802e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2837e-10 - val_loss: 1.2762e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2806e-10 - val_loss: 1.2720e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2767e-10 - val_loss: 1.2764e-10
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2929e-10 - val_loss: 1.2969e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2954e-10 - val_loss: 1.2843e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2791e-10 - val_loss: 1.2729e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2657e-10 - val_loss: 1.2623e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2727e-10 - val_loss: 1.2687e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2765e-10 - val_loss: 1.2672e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2647e-10 - val_loss: 1.2529e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2594e-10 - val_loss: 1.2594e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2575e-10 - val_loss: 1.2553e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2668e-10 - val_loss: 1.2632e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2628e-10 - val_loss: 1.2574e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2612e-10 - val_loss: 1.2531e-10
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2491e-10 - val_loss: 1.2409e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2365e-10 - val_loss: 1.2257e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2290e-10 - val_loss: 1.2307e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2405e-10 - val_loss: 1.2574e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2629e-10 - val_loss: 1.2631e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2661e-10 - val_loss: 1.2595e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2615e-10 - val_loss: 1.2385e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2406e-10 - val_loss: 1.2324e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2381e-10 - val_loss: 1.2295e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2394e-10 - val_loss: 1.2395e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2372e-10 - val_loss: 1.2233e-10
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2169e-10 - val_loss: 1.2015e-10
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1961e-10 - val_loss: 1.1867e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1942e-10 - val_loss: 1.1997e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2024e-10 - val_loss: 1.2044e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2208e-10 - val_loss: 1.2170e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2183e-10 - val_loss: 1.2072e-10
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1919e-10 - val_loss: 1.1718e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1800e-10 - val_loss: 1.1810e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1911e-10 - val_loss: 1.1930e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1947e-10 - val_loss: 1.1978e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2074e-10 - val_loss: 1.2062e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2096e-10 - val_loss: 1.2083e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2027e-10 - val_loss: 1.1950e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1974e-10 - val_loss: 1.1898e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1875e-10 - val_loss: 1.1847e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1896e-10 - val_loss: 1.1938e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1969e-10 - val_loss: 1.1848e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1830e-10 - val_loss: 1.1753e-10
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1744e-10 - val_loss: 1.1711e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1834e-10 - val_loss: 1.1859e-10
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1866e-10 - val_loss: 1.1882e-10
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1917e-10 - val_loss: 1.1846e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1879e-10 - val_loss: 1.1894e-10
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1862e-10 - val_loss: 1.1716e-10
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1655e-10 - val_loss: 1.1602e-10
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1574e-10 - val_loss: 1.1516e-10
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1496e-10 - val_loss: 1.1470e-10
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1559e-10 - val_loss: 1.1516e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1516e-10 - val_loss: 1.1368e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1380e-10 - val_loss: 1.1374e-10
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1365e-10 - val_loss: 1.1360e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1458e-10 - val_loss: 1.1459e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1492e-10 - val_loss: 1.1458e-10
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1470e-10 - val_loss: 1.1372e-10
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1359e-10 - val_loss: 1.1210e-10
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1232e-10 - val_loss: 1.1247e-10
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1259e-10 - val_loss: 1.1144e-10
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1163e-10 - val_loss: 1.1177e-10
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1191e-10 - val_loss: 1.1176e-10
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1261e-10 - val_loss: 1.1265e-10
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1296e-10 - val_loss: 1.1278e-10
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1372e-10 - val_loss: 1.1395e-10
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1376e-10 - val_loss: 1.1306e-10
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1401e-10 - val_loss: 1.1281e-10
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1333e-10 - val_loss: 1.1392e-10
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1347e-10 - val_loss: 1.1144e-10
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1178e-10 - val_loss: 1.1069e-10
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1054e-10 - val_loss: 1.0893e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0899e-10 - val_loss: 1.0915e-10
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0884e-10 - val_loss: 1.0878e-10
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0884e-10 - val_loss: 1.0935e-10
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1153e-10 - val_loss: 1.1374e-10
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1442e-10 - val_loss: 1.1282e-10
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1217e-10 - val_loss: 1.1102e-10
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1073e-10 - val_loss: 1.0908e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0894e-10 - val_loss: 1.0793e-10
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0771e-10 - val_loss: 1.0679e-10
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0617e-10 - val_loss: 1.0580e-10
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0620e-10 - val_loss: 1.0685e-10
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0821e-10 - val_loss: 1.0979e-10
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1054e-10 - val_loss: 1.1063e-10
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1164e-10 - val_loss: 1.1166e-10
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1113e-10 - val_loss: 1.0873e-10
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0886e-10 - val_loss: 1.0790e-10
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0749e-10 - val_loss: 1.0601e-10
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0630e-10 - val_loss: 1.0602e-10
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0599e-10 - val_loss: 1.0583e-10
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0570e-10 - val_loss: 1.0516e-10
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0570e-10 - val_loss: 1.0585e-10
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0596e-10 - val_loss: 1.0535e-10
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0602e-10 - val_loss: 1.0610e-10
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0736e-10 - val_loss: 1.0721e-10
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0777e-10 - val_loss: 1.0758e-10
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0699e-10 - val_loss: 1.0575e-10
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0616e-10 - val_loss: 1.0617e-10
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0575e-10 - val_loss: 1.0432e-10
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0385e-10 - val_loss: 1.0349e-10
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0439e-10 - val_loss: 1.0500e-10
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0531e-10 - val_loss: 1.0481e-10
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0529e-10 - val_loss: 1.0555e-10
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0612e-10 - val_loss: 1.0579e-10
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0484e-10 - val_loss: 1.0385e-10
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0393e-10 - val_loss: 1.0331e-10
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0379e-10 - val_loss: 1.0408e-10
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0527e-10 - val_loss: 1.0535e-10
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0542e-10 - val_loss: 1.0495e-10
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0515e-10 - val_loss: 1.0531e-10
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0533e-10 - val_loss: 1.0394e-10
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0364e-10 - val_loss: 1.0317e-10
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0335e-10 - val_loss: 1.0357e-10
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0385e-10 - val_loss: 1.0338e-10
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0356e-10 - val_loss: 1.0280e-10
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0254e-10 - val_loss: 1.0167e-10
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0170e-10 - val_loss: 1.0149e-10
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0137e-10 - val_loss: 1.0087e-10
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0166e-10 - val_loss: 1.0176e-10
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0176e-10 - val_loss: 1.0194e-10
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0284e-10 - val_loss: 1.0278e-10
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0341e-10 - val_loss: 1.0215e-10
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0188e-10 - val_loss: 1.0121e-10
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0118e-10 - val_loss: 1.0064e-10
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0043e-10 - val_loss: 9.9486e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0086e-10 - val_loss: 1.0079e-10
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0088e-10 - val_loss: 1.0021e-10
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.9627e-11 - val_loss: 9.8234e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.8096e-11 - val_loss: 9.8017e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.7653e-11 - val_loss: 9.7333e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8431e-11 - val_loss: 9.8934e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0003e-10 - val_loss: 1.0056e-10
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0108e-10 - val_loss: 1.0141e-10
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0154e-10 - val_loss: 1.0043e-10
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0096e-10 - val_loss: 1.0125e-10
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0070e-10 - val_loss: 9.9742e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0078e-10 - val_loss: 1.0022e-10
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9407e-11 - val_loss: 9.9047e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8425e-11 - val_loss: 9.7461e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.7463e-11 - val_loss: 9.7161e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.7587e-11 - val_loss: 9.6440e-11
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.6468e-11 - val_loss: 9.6171e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7066e-11 - val_loss: 9.6372e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6969e-11 - val_loss: 9.7573e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7687e-11 - val_loss: 9.7881e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7870e-11 - val_loss: 9.7236e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7480e-11 - val_loss: 9.7265e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7454e-11 - val_loss: 9.7497e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7190e-11 - val_loss: 9.6539e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7152e-11 - val_loss: 9.6821e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7140e-11 - val_loss: 9.6780e-11
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.6573e-11 - val_loss: 9.5791e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.5352e-11 - val_loss: 9.3955e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3901e-11 - val_loss: 9.4098e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5531e-11 - val_loss: 9.6778e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8132e-11 - val_loss: 9.8416e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8576e-11 - val_loss: 9.8291e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8139e-11 - val_loss: 9.6807e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6644e-11 - val_loss: 9.5276e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.4529e-11 - val_loss: 9.3573e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.3617e-11 - val_loss: 9.2567e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.2365e-11 - val_loss: 9.1821e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2196e-11 - val_loss: 9.2533e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3418e-11 - val_loss: 9.4139e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4763e-11 - val_loss: 9.4920e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5628e-11 - val_loss: 9.4489e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5031e-11 - val_loss: 9.5148e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5193e-11 - val_loss: 9.4582e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4460e-11 - val_loss: 9.4238e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5045e-11 - val_loss: 9.4982e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4553e-11 - val_loss: 9.4128e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4215e-11 - val_loss: 9.3470e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3537e-11 - val_loss: 9.2663e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2883e-11 - val_loss: 9.2638e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2969e-11 - val_loss: 9.2992e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3823e-11 - val_loss: 9.3576e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2781e-11 - val_loss: 9.1873e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2119e-11 - val_loss: 9.2017e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00003-RMS-17/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.2304e-11 - val_loss: 9.1705e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 10.596 | eve: 10.759 | bob: 10.252Epoch   0:   0% | abe: 10.464 | eve: 10.701 | bob: 10.165Epoch   0:   1% | abe: 10.365 | eve: 10.659 | bob: 10.095Epoch   0:   2% | abe: 10.289 | eve: 10.640 | bob: 10.040Epoch   0:   3% | abe: 10.199 | eve: 10.618 | bob: 9.971Epoch   0:   3% | abe: 10.137 | eve: 10.611 | bob: 9.924Epoch   0:   4% | abe: 10.065 | eve: 10.606 | bob: 9.868Epoch   0:   5% | abe: 10.012 | eve: 10.595 | bob: 9.829Epoch   0:   6% | abe: 9.965 | eve: 10.598 | bob: 9.795Epoch   0:   7% | abe: 9.915 | eve: 10.599 | bob: 9.758Epoch   0:   7% | abe: 9.875 | eve: 10.597 | bob: 9.729Epoch   0:   8% | abe: 9.834 | eve: 10.593 | bob: 9.698Epoch   0:   9% | abe: 9.791 | eve: 10.600 | bob: 9.665Epoch   0:  10% | abe: 9.755 | eve: 10.599 | bob: 9.638Epoch   0:  10% | abe: 9.721 | eve: 10.603 | bob: 9.614Epoch   0:  11% | abe: 9.690 | eve: 10.595 | bob: 9.590Epoch   0:  12% | abe: 9.659 | eve: 10.592 | bob: 9.567Epoch   0:  13% | abe: 9.635 | eve: 10.590 | bob: 9.550Epoch   0:  14% | abe: 9.614 | eve: 10.586 | bob: 9.535Epoch   0:  14% | abe: 9.590 | eve: 10.583 | bob: 9.517Epoch   0:  15% | abe: 9.571 | eve: 10.582 | bob: 9.504Epoch   0:  16% | abe: 9.550 | eve: 10.582 | bob: 9.487Epoch   0:  17% | abe: 9.530 | eve: 10.578 | bob: 9.472Epoch   0:  17% | abe: 9.512 | eve: 10.577 | bob: 9.458Epoch   0:  18% | abe: 9.498 | eve: 10.576 | bob: 9.447Epoch   0:  19% | abe: 9.483 | eve: 10.578 | bob: 9.436Epoch   0:  20% | abe: 9.471 | eve: 10.578 | bob: 9.427Epoch   0:  21% | abe: 9.458 | eve: 10.580 | bob: 9.417Epoch   0:  21% | abe: 9.446 | eve: 10.578 | bob: 9.408Epoch   0:  22% | abe: 9.435 | eve: 10.577 | bob: 9.399Epoch   0:  23% | abe: 9.423 | eve: 10.582 | bob: 9.390Epoch   0:  24% | abe: 9.412 | eve: 10.581 | bob: 9.381Epoch   0:  25% | abe: 9.404 | eve: 10.583 | bob: 9.375Epoch   0:  25% | abe: 9.395 | eve: 10.585 | bob: 9.368Epoch   0:  26% | abe: 9.385 | eve: 10.587 | bob: 9.359Epoch   0:  27% | abe: 9.376 | eve: 10.588 | bob: 9.352Epoch   0:  28% | abe: 9.367 | eve: 10.588 | bob: 9.345Epoch   0:  28% | abe: 9.361 | eve: 10.590 | bob: 9.340Epoch   0:  29% | abe: 9.353 | eve: 10.593 | bob: 9.334Epoch   0:  30% | abe: 9.345 | eve: 10.594 | bob: 9.327Epoch   0:  31% | abe: 9.338 | eve: 10.597 | bob: 9.322Epoch   0:  32% | abe: 9.332 | eve: 10.597 | bob: 9.316Epoch   0:  32% | abe: 9.327 | eve: 10.601 | bob: 9.312Epoch   0:  33% | abe: 9.321 | eve: 10.603 | bob: 9.308Epoch   0:  34% | abe: 9.315 | eve: 10.606 | bob: 9.302Epoch   0:  35% | abe: 9.309 | eve: 10.608 | bob: 9.297Epoch   0:  35% | abe: 9.304 | eve: 10.612 | bob: 9.293Epoch   0:  36% | abe: 9.298 | eve: 10.614 | bob: 9.288Epoch   0:  37% | abe: 9.293 | eve: 10.619 | bob: 9.284Epoch   0:  38% | abe: 9.289 | eve: 10.619 | bob: 9.280Epoch   0:  39% | abe: 9.284 | eve: 10.622 | bob: 9.276Epoch   0:  39% | abe: 9.280 | eve: 10.625 | bob: 9.273Epoch   0:  40% | abe: 9.275 | eve: 10.629 | bob: 9.269Epoch   0:  41% | abe: 9.272 | eve: 10.631 | bob: 9.266Epoch   0:  42% | abe: 9.269 | eve: 10.635 | bob: 9.263Epoch   0:  42% | abe: 9.265 | eve: 10.639 | bob: 9.260Epoch   0:  43% | abe: 9.262 | eve: 10.641 | bob: 9.257Epoch   0:  44% | abe: 9.258 | eve: 10.645 | bob: 9.254Epoch   0:  45% | abe: 9.254 | eve: 10.648 | bob: 9.251Epoch   0:  46% | abe: 9.251 | eve: 10.653 | bob: 9.248Epoch   0:  46% | abe: 9.248 | eve: 10.656 | bob: 9.245Epoch   0:  47% | abe: 9.245 | eve: 10.660 | bob: 9.242Epoch   0:  48% | abe: 9.242 | eve: 10.664 | bob: 9.240Epoch   0:  49% | abe: 9.239 | eve: 10.667 | bob: 9.238Epoch   0:  50% | abe: 9.236 | eve: 10.672 | bob: 9.235Epoch   0:  50% | abe: 9.233 | eve: 10.676 | bob: 9.232Epoch   0:  51% | abe: 9.230 | eve: 10.680 | bob: 9.230Epoch   0:  52% | abe: 9.227 | eve: 10.684 | bob: 9.227Epoch   0:  53% | abe: 9.225 | eve: 10.687 | bob: 9.225Epoch   0:  53% | abe: 9.222 | eve: 10.692 | bob: 9.223Epoch   0:  54% | abe: 9.221 | eve: 10.696 | bob: 9.221Epoch   0:  55% | abe: 9.218 | eve: 10.702 | bob: 9.219Epoch   0:  56% | abe: 9.215 | eve: 10.706 | bob: 9.216Epoch   0:  57% | abe: 9.212 | eve: 10.712 | bob: 9.214Epoch   0:  57% | abe: 9.210 | eve: 10.716 | bob: 9.212Epoch   0:  58% | abe: 9.208 | eve: 10.721 | bob: 9.210Epoch   0:  59% | abe: 9.206 | eve: 10.726 | bob: 9.208Epoch   0:  60% | abe: 9.204 | eve: 10.731 | bob: 9.206Epoch   0:  60% | abe: 9.202 | eve: 10.736 | bob: 9.205Epoch   0:  61% | abe: 9.200 | eve: 10.742 | bob: 9.203Epoch   0:  62% | abe: 9.198 | eve: 10.746 | bob: 9.202Epoch   0:  63% | abe: 9.196 | eve: 10.751 | bob: 9.200Epoch   0:  64% | abe: 9.194 | eve: 10.755 | bob: 9.198Epoch   0:  64% | abe: 9.192 | eve: 10.760 | bob: 9.196Epoch   0:  65% | abe: 9.191 | eve: 10.765 | bob: 9.195Epoch   0:  66% | abe: 9.189 | eve: 10.770 | bob: 9.193Epoch   0:  67% | abe: 9.187 | eve: 10.775 | bob: 9.192Epoch   0:  67% | abe: 9.185 | eve: 10.781 | bob: 9.190Epoch   0:  68% | abe: 9.184 | eve: 10.786 | bob: 9.189Epoch   0:  69% | abe: 9.183 | eve: 10.791 | bob: 9.188Epoch   0:  70% | abe: 9.181 | eve: 10.797 | bob: 9.186Epoch   0:  71% | abe: 9.180 | eve: 10.802 | bob: 9.185Epoch   0:  71% | abe: 9.178 | eve: 10.808 | bob: 9.184Epoch   0:  72% | abe: 9.177 | eve: 10.814 | bob: 9.182Epoch   0:  73% | abe: 9.175 | eve: 10.820 | bob: 9.181Epoch   0:  74% | abe: 9.173 | eve: 10.825 | bob: 9.179Epoch   0:  75% | abe: 9.172 | eve: 10.829 | bob: 9.178Epoch   0:  75% | abe: 9.171 | eve: 10.835 | bob: 9.176Epoch   0:  76% | abe: 9.169 | eve: 10.840 | bob: 9.175Epoch   0:  77% | abe: 9.168 | eve: 10.845 | bob: 9.174Epoch   0:  78% | abe: 9.167 | eve: 10.851 | bob: 9.173Epoch   0:  78% | abe: 9.165 | eve: 10.856 | bob: 9.172Epoch   0:  79% | abe: 9.164 | eve: 10.862 | bob: 9.170Epoch   0:  80% | abe: 9.162 | eve: 10.866 | bob: 9.169Epoch   0:  81% | abe: 9.161 | eve: 10.870 | bob: 9.168Epoch   0:  82% | abe: 9.160 | eve: 10.876 | bob: 9.167Epoch   0:  82% | abe: 9.159 | eve: 10.881 | bob: 9.166Epoch   0:  83% | abe: 9.158 | eve: 10.886 | bob: 9.165Epoch   0:  84% | abe: 9.157 | eve: 10.891 | bob: 9.164Epoch   0:  85% | abe: 9.156 | eve: 10.896 | bob: 9.163Epoch   0:  85% | abe: 9.155 | eve: 10.901 | bob: 9.162Epoch   0:  86% | abe: 9.154 | eve: 10.907 | bob: 9.161Epoch   0:  87% | abe: 9.153 | eve: 10.913 | bob: 9.160Epoch   0:  88% | abe: 9.152 | eve: 10.918 | bob: 9.159Epoch   0:  89% | abe: 9.151 | eve: 10.923 | bob: 9.158Epoch   0:  89% | abe: 9.150 | eve: 10.928 | bob: 9.158Epoch   0:  90% | abe: 9.149 | eve: 10.933 | bob: 9.157Epoch   0:  91% | abe: 9.148 | eve: 10.938 | bob: 9.156Epoch   0:  92% | abe: 9.147 | eve: 10.943 | bob: 9.155Epoch   0:  92% | abe: 9.146 | eve: 10.948 | bob: 9.154Epoch   0:  93% | abe: 9.145 | eve: 10.953 | bob: 9.153Epoch   0:  94% | abe: 9.144 | eve: 10.958 | bob: 9.152Epoch   0:  95% | abe: 9.144 | eve: 10.963 | bob: 9.152Epoch   0:  96% | abe: 9.143 | eve: 10.967 | bob: 9.151Epoch   0:  96% | abe: 9.142 | eve: 10.971 | bob: 9.150Epoch   0:  97% | abe: 9.141 | eve: 10.977 | bob: 9.149Epoch   0:  98% | abe: 9.140 | eve: 10.982 | bob: 9.148Epoch   0:  99% | abe: 9.139 | eve: 10.986 | bob: 9.147
New best Bob loss 9.147279595461669 at epoch 0
Epoch   1:   0% | abe: 9.032 | eve: 11.653 | bob: 9.046Epoch   1:   0% | abe: 9.036 | eve: 11.617 | bob: 9.051Epoch   1:   1% | abe: 9.032 | eve: 11.634 | bob: 9.048