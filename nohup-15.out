WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-09 13:01:06.496489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-09 13:01:06.717765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-09 13:01:06.718331: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 13:01:06.721538: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-09 13:01:06.723505: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-09 13:01:06.724120: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-09 13:01:06.727748: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-09 13:01:06.729833: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-09 13:01:06.739080: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-09 13:01:06.767986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-09 13:01:06.768435: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-09 13:01:06.781688: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-09 13:01:06.784176: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50538a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-09 13:01:06.784226: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-09 13:01:07.163246: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2add440 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-09 13:01:07.163339: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-09 13:01:07.169727: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-09 13:01:07.169874: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 13:01:07.169921: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-09 13:01:07.169958: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-09 13:01:07.170007: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-09 13:01:07.170039: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-09 13:01:07.170074: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-09 13:01:07.170107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-09 13:01:07.183398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-09 13:01:07.183512: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-09 13:01:07.190934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-09 13:01:07.190979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-09 13:01:07.191004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-09 13:01:07.204044: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-09 13:01:11.261388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.4980 - val_loss: 0.0020
Epoch 2/512
512/512 - 0s - loss: 0.1110 - val_loss: 2.6388e-04
Epoch 3/512
512/512 - 0s - loss: 0.0134 - val_loss: 2.2041e-05
Epoch 4/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.5596e-06
Epoch 5/512
512/512 - 0s - loss: 1.9264e-04 - val_loss: 1.3226e-06
Epoch 6/512
512/512 - 0s - loss: 1.1272e-04 - val_loss: 8.3157e-07
Epoch 7/512
512/512 - 0s - loss: 6.8624e-05 - val_loss: 4.7028e-07
Epoch 8/512
512/512 - 0s - loss: 3.7160e-05 - val_loss: 2.3126e-07
Epoch 9/512
512/512 - 0s - loss: 1.7337e-05 - val_loss: 9.5388e-08
Epoch 10/512
512/512 - 0s - loss: 6.7115e-06 - val_loss: 3.1484e-08
Epoch 11/512
512/512 - 0s - loss: 2.0536e-06 - val_loss: 7.8241e-09
Epoch 12/512
512/512 - 0s - loss: 4.6662e-07 - val_loss: 1.3520e-09
Epoch 13/512
512/512 - 0s - loss: 7.2941e-08 - val_loss: 1.6766e-10
Epoch 14/512
512/512 - 0s - loss: 3.9341e-08 - val_loss: 4.7985e-09
Epoch 15/512
512/512 - 0s - loss: 3.8639e-05 - val_loss: 1.0557e-05
Epoch 16/512
512/512 - 0s - loss: 0.0040 - val_loss: 8.9243e-06
Epoch 17/512
512/512 - 0s - loss: 3.5760e-04 - val_loss: 5.0810e-07
Epoch 18/512
512/512 - 0s - loss: 4.8666e-05 - val_loss: 8.3044e-07
Epoch 19/512
512/512 - 0s - loss: 3.0783e-04 - val_loss: 1.5064e-05
Epoch 20/512
512/512 - 0s - loss: 0.0024 - val_loss: 1.2375e-05
Epoch 21/512
512/512 - 0s - loss: 6.8340e-04 - val_loss: 2.5342e-06
Epoch 22/512
512/512 - 0s - loss: 2.8509e-04 - val_loss: 5.0472e-06
Epoch 23/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.7081e-05
Epoch 24/512
512/512 - 0s - loss: 0.0014 - val_loss: 6.6922e-06
Epoch 25/512
512/512 - 0s - loss: 5.3940e-04 - val_loss: 4.7572e-06
Epoch 26/512
512/512 - 0s - loss: 6.4951e-04 - val_loss: 1.0187e-05
Epoch 27/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.8811e-06
Epoch 28/512
512/512 - 0s - loss: 8.0929e-04 - val_loss: 5.8851e-06
Epoch 29/512
512/512 - 0s - loss: 6.2108e-04 - val_loss: 7.2479e-06
Epoch 30/512
512/512 - 0s - loss: 8.6233e-04 - val_loss: 9.4344e-06
Epoch 31/512
512/512 - 0s - loss: 8.8233e-04 - val_loss: 7.0867e-06
Epoch 32/512
512/512 - 0s - loss: 6.7599e-04 - val_loss: 6.5010e-06
Epoch 33/512
512/512 - 0s - loss: 7.1931e-04 - val_loss: 7.8945e-06
Epoch 34/512
512/512 - 0s - loss: 8.1196e-04 - val_loss: 7.4069e-06
Epoch 35/512
512/512 - 0s - loss: 7.1215e-04 - val_loss: 6.4277e-06
Epoch 36/512
512/512 - 0s - loss: 6.6809e-04 - val_loss: 6.8649e-06
Epoch 37/512
512/512 - 0s - loss: 7.2422e-04 - val_loss: 7.1225e-06
Epoch 38/512
512/512 - 0s - loss: 6.9975e-04 - val_loss: 6.6174e-06
Epoch 39/512
512/512 - 0s - loss: 6.6835e-04 - val_loss: 6.2739e-06
Epoch 40/512
512/512 - 0s - loss: 6.4440e-04 - val_loss: 6.5443e-06
Epoch 41/512
512/512 - 0s - loss: 6.7248e-04 - val_loss: 6.4699e-06
Epoch 42/512
512/512 - 0s - loss: 6.4121e-04 - val_loss: 6.1462e-06
Epoch 43/512
512/512 - 0s - loss: 6.2229e-04 - val_loss: 6.0987e-06
Epoch 44/512
512/512 - 0s - loss: 6.2066e-04 - val_loss: 6.1035e-06
Epoch 45/512
512/512 - 0s - loss: 6.1372e-04 - val_loss: 5.9230e-06
Epoch 46/512
512/512 - 0s - loss: 5.9905e-04 - val_loss: 5.7857e-06
Epoch 47/512
512/512 - 0s - loss: 5.8484e-04 - val_loss: 5.7843e-06
Epoch 48/512
512/512 - 0s - loss: 5.8634e-04 - val_loss: 5.7120e-06
Epoch 49/512
512/512 - 0s - loss: 5.7430e-04 - val_loss: 5.5101e-06
Epoch 50/512
512/512 - 0s - loss: 5.5276e-04 - val_loss: 5.4657e-06
Epoch 51/512
512/512 - 0s - loss: 5.5384e-04 - val_loss: 5.5781e-06
Epoch 52/512
512/512 - 0s - loss: 5.5673e-04 - val_loss: 5.3377e-06
Epoch 53/512
512/512 - 0s - loss: 5.3145e-04 - val_loss: 5.1788e-06
Epoch 54/512
512/512 - 0s - loss: 5.2530e-04 - val_loss: 5.1861e-06
Epoch 55/512
512/512 - 0s - loss: 5.1974e-04 - val_loss: 5.2250e-06
Epoch 56/512
512/512 - 0s - loss: 5.1940e-04 - val_loss: 5.0983e-06
Epoch 57/512
512/512 - 0s - loss: 5.0707e-04 - val_loss: 4.8891e-06
Epoch 58/512
512/512 - 0s - loss: 4.9106e-04 - val_loss: 4.9251e-06
Epoch 59/512
512/512 - 0s - loss: 4.9892e-04 - val_loss: 4.8591e-06
Epoch 60/512
512/512 - 0s - loss: 4.8279e-04 - val_loss: 4.7413e-06
Epoch 61/512
512/512 - 0s - loss: 4.7589e-04 - val_loss: 4.7138e-06
Epoch 62/512
512/512 - 0s - loss: 4.7339e-04 - val_loss: 4.6284e-06
Epoch 63/512
512/512 - 0s - loss: 4.6516e-04 - val_loss: 4.5579e-06
Epoch 64/512
512/512 - 0s - loss: 4.5904e-04 - val_loss: 4.4877e-06
Epoch 65/512
512/512 - 0s - loss: 4.5085e-04 - val_loss: 4.4680e-06
Epoch 66/512
512/512 - 0s - loss: 4.4694e-04 - val_loss: 4.4406e-06
Epoch 67/512
512/512 - 0s - loss: 4.4375e-04 - val_loss: 4.3484e-06
Epoch 68/512
512/512 - 0s - loss: 4.3501e-04 - val_loss: 4.2694e-06
Epoch 69/512
512/512 - 0s - loss: 4.2887e-04 - val_loss: 4.2448e-06
Epoch 70/512
512/512 - 0s - loss: 4.2579e-04 - val_loss: 4.2113e-06
Epoch 71/512
512/512 - 0s - loss: 4.2306e-04 - val_loss: 4.0747e-06
Epoch 72/512
512/512 - 0s - loss: 4.0842e-04 - val_loss: 4.0810e-06
Epoch 73/512
512/512 - 0s - loss: 4.0813e-04 - val_loss: 4.1554e-06
Epoch 74/512
512/512 - 0s - loss: 4.1685e-04 - val_loss: 3.9068e-06
Epoch 75/512
512/512 - 0s - loss: 3.8941e-04 - val_loss: 3.8547e-06
Epoch 76/512
512/512 - 0s - loss: 3.9238e-04 - val_loss: 3.9962e-06
Epoch 77/512
512/512 - 0s - loss: 4.0024e-04 - val_loss: 3.9034e-06
Epoch 78/512
512/512 - 0s - loss: 3.8826e-04 - val_loss: 3.6879e-06
Epoch 79/512
512/512 - 0s - loss: 3.7324e-04 - val_loss: 3.7236e-06
Epoch 80/512
512/512 - 0s - loss: 3.7684e-04 - val_loss: 3.9084e-06
Epoch 81/512
512/512 - 0s - loss: 3.8604e-04 - val_loss: 3.7419e-06
Epoch 82/512
512/512 - 0s - loss: 3.6607e-04 - val_loss: 3.5657e-06
Epoch 83/512
512/512 - 0s - loss: 3.6124e-04 - val_loss: 3.6263e-06
Epoch 84/512
512/512 - 0s - loss: 3.6406e-04 - val_loss: 3.6871e-06
Epoch 85/512
512/512 - 0s - loss: 3.6504e-04 - val_loss: 3.5699e-06
Epoch 86/512
512/512 - 0s - loss: 3.5384e-04 - val_loss: 3.4472e-06
Epoch 87/512
512/512 - 0s - loss: 3.4827e-04 - val_loss: 3.4895e-06
Epoch 88/512
512/512 - 0s - loss: 3.5219e-04 - val_loss: 3.4821e-06
Epoch 89/512
512/512 - 0s - loss: 3.4359e-04 - val_loss: 3.4509e-06
Epoch 90/512
512/512 - 0s - loss: 3.4221e-04 - val_loss: 3.4506e-06
Epoch 91/512
512/512 - 0s - loss: 3.4175e-04 - val_loss: 3.3827e-06
Epoch 92/512
512/512 - 0s - loss: 3.3695e-04 - val_loss: 3.2844e-06
Epoch 93/512
512/512 - 0s - loss: 3.3136e-04 - val_loss: 3.2587e-06
Epoch 94/512
512/512 - 0s - loss: 3.2690e-04 - val_loss: 3.3089e-06
Epoch 95/512
512/512 - 0s - loss: 3.3257e-04 - val_loss: 3.2474e-06
Epoch 96/512
512/512 - 0s - loss: 3.2386e-04 - val_loss: 3.1426e-06
Epoch 97/512
512/512 - 0s - loss: 3.1437e-04 - val_loss: 3.2543e-06
Epoch 98/512
512/512 - 0s - loss: 3.2641e-04 - val_loss: 3.2370e-06
Epoch 99/512
512/512 - 0s - loss: 3.1867e-04 - val_loss: 3.0781e-06
Epoch 100/512
512/512 - 0s - loss: 3.0770e-04 - val_loss: 3.0905e-06
Epoch 101/512
512/512 - 0s - loss: 3.1408e-04 - val_loss: 3.1080e-06
Epoch 102/512
512/512 - 0s - loss: 3.0760e-04 - val_loss: 3.0890e-06
Epoch 103/512
512/512 - 0s - loss: 3.0658e-04 - val_loss: 3.0849e-06
Epoch 104/512
512/512 - 0s - loss: 3.0447e-04 - val_loss: 3.0644e-06
Epoch 105/512
512/512 - 0s - loss: 3.0581e-04 - val_loss: 2.9893e-06
Epoch 106/512
512/512 - 0s - loss: 2.9535e-04 - val_loss: 2.9903e-06
Epoch 107/512
512/512 - 0s - loss: 2.9910e-04 - val_loss: 2.9839e-06
Epoch 108/512
512/512 - 0s - loss: 2.9981e-04 - val_loss: 2.8616e-06
Epoch 109/512
512/512 - 0s - loss: 2.8519e-04 - val_loss: 2.9321e-06
Epoch 110/512
512/512 - 0s - loss: 2.9622e-04 - val_loss: 2.9729e-06
Epoch 111/512
512/512 - 0s - loss: 2.9412e-04 - val_loss: 2.8168e-06
Epoch 112/512
512/512 - 0s - loss: 2.8278e-04 - val_loss: 2.7541e-06
Epoch 113/512
512/512 - 0s - loss: 2.8236e-04 - val_loss: 2.8556e-06
Epoch 114/512
512/512 - 0s - loss: 2.9033e-04 - val_loss: 2.8119e-06
Epoch 115/512
512/512 - 0s - loss: 2.7949e-04 - val_loss: 2.7371e-06
Epoch 116/512
512/512 - 0s - loss: 2.7653e-04 - val_loss: 2.8029e-06
Epoch 117/512
512/512 - 0s - loss: 2.8111e-04 - val_loss: 2.8197e-06
Epoch 118/512
512/512 - 0s - loss: 2.8111e-04 - val_loss: 2.7159e-06
Epoch 119/512
512/512 - 0s - loss: 2.6949e-04 - val_loss: 2.6994e-06
Epoch 120/512
512/512 - 0s - loss: 2.7320e-04 - val_loss: 2.7633e-06
Epoch 121/512
512/512 - 0s - loss: 2.7629e-04 - val_loss: 2.6913e-06
Epoch 122/512
512/512 - 0s - loss: 2.6872e-04 - val_loss: 2.6341e-06
Epoch 123/512
512/512 - 0s - loss: 2.6533e-04 - val_loss: 2.6913e-06
Epoch 124/512
512/512 - 0s - loss: 2.6966e-04 - val_loss: 2.7037e-06
Epoch 125/512
512/512 - 0s - loss: 2.7047e-04 - val_loss: 2.5603e-06
Epoch 126/512
512/512 - 0s - loss: 2.5608e-04 - val_loss: 2.6057e-06
Epoch 127/512
512/512 - 0s - loss: 2.6416e-04 - val_loss: 2.7062e-06
Epoch 128/512
512/512 - 0s - loss: 2.6924e-04 - val_loss: 2.5908e-06
Epoch 129/512
512/512 - 0s - loss: 2.5592e-04 - val_loss: 2.4998e-06
Epoch 130/512
512/512 - 0s - loss: 2.5456e-04 - val_loss: 2.5982e-06
Epoch 131/512
512/512 - 0s - loss: 2.6342e-04 - val_loss: 2.5581e-06
Epoch 132/512
512/512 - 0s - loss: 2.5345e-04 - val_loss: 2.5161e-06
Epoch 133/512
512/512 - 0s - loss: 2.5485e-04 - val_loss: 2.5542e-06
Epoch 134/512
512/512 - 0s - loss: 2.5521e-04 - val_loss: 2.5399e-06
Epoch 135/512
512/512 - 0s - loss: 2.5302e-04 - val_loss: 2.4993e-06
Epoch 136/512
512/512 - 0s - loss: 2.5043e-04 - val_loss: 2.4732e-06
Epoch 137/512
512/512 - 0s - loss: 2.5039e-04 - val_loss: 2.4673e-06
Epoch 138/512
512/512 - 0s - loss: 2.4926e-04 - val_loss: 2.4620e-06
Epoch 139/512
512/512 - 0s - loss: 2.4778e-04 - val_loss: 2.4686e-06
Epoch 140/512
512/512 - 0s - loss: 2.4584e-04 - val_loss: 2.4811e-06
Epoch 141/512
512/512 - 0s - loss: 2.4669e-04 - val_loss: 2.4705e-06
Epoch 142/512
512/512 - 0s - loss: 2.4680e-04 - val_loss: 2.4075e-06
Epoch 143/512
512/512 - 0s - loss: 2.4047e-04 - val_loss: 2.4181e-06
Epoch 144/512
512/512 - 0s - loss: 2.4536e-04 - val_loss: 2.4166e-06
Epoch 145/512
512/512 - 0s - loss: 2.4014e-04 - val_loss: 2.3981e-06
Epoch 146/512
512/512 - 0s - loss: 2.4184e-04 - val_loss: 2.3765e-06
Epoch 147/512
512/512 - 0s - loss: 2.3746e-04 - val_loss: 2.4056e-06
Epoch 148/512
512/512 - 0s - loss: 2.4006e-04 - val_loss: 2.4364e-06
Epoch 149/512
512/512 - 0s - loss: 2.4353e-04 - val_loss: 2.3100e-06
Epoch 150/512
512/512 - 0s - loss: 2.2905e-04 - val_loss: 2.3291e-06
Epoch 151/512
512/512 - 0s - loss: 2.3836e-04 - val_loss: 2.3986e-06
Epoch 152/512
512/512 - 0s - loss: 2.3858e-04 - val_loss: 2.3380e-06
Epoch 153/512
512/512 - 0s - loss: 2.3238e-04 - val_loss: 2.2998e-06
Epoch 154/512
512/512 - 0s - loss: 2.3259e-04 - val_loss: 2.3162e-06
Epoch 155/512
512/512 - 0s - loss: 2.3245e-04 - val_loss: 2.3542e-06
Epoch 156/512
512/512 - 0s - loss: 2.3521e-04 - val_loss: 2.2963e-06
Epoch 157/512
512/512 - 0s - loss: 2.2892e-04 - val_loss: 2.2496e-06
Epoch 158/512
512/512 - 0s - loss: 2.2787e-04 - val_loss: 2.3128e-06
Epoch 159/512
512/512 - 0s - loss: 2.3234e-04 - val_loss: 2.3247e-06
Epoch 160/512
512/512 - 0s - loss: 2.3139e-04 - val_loss: 2.2164e-06
Epoch 161/512
512/512 - 0s - loss: 2.2317e-04 - val_loss: 2.2311e-06
Epoch 162/512
512/512 - 0s - loss: 2.2907e-04 - val_loss: 2.2571e-06
Epoch 163/512
512/512 - 0s - loss: 2.2610e-04 - val_loss: 2.2522e-06
Epoch 164/512
512/512 - 0s - loss: 2.2480e-04 - val_loss: 2.2820e-06
Epoch 165/512
512/512 - 0s - loss: 2.2737e-04 - val_loss: 2.2447e-06
Epoch 166/512
512/512 - 0s - loss: 2.2431e-04 - val_loss: 2.1935e-06
Epoch 167/512
512/512 - 0s - loss: 2.2114e-04 - val_loss: 2.2241e-06
Epoch 168/512
512/512 - 0s - loss: 2.2401e-04 - val_loss: 2.2555e-06
Epoch 169/512
512/512 - 0s - loss: 2.2233e-04 - val_loss: 2.2444e-06
Epoch 170/512
512/512 - 0s - loss: 2.2330e-04 - val_loss: 2.2065e-06
Epoch 171/512
512/512 - 0s - loss: 2.2074e-04 - val_loss: 2.1647e-06
Epoch 172/512
512/512 - 0s - loss: 2.1877e-04 - val_loss: 2.1707e-06
Epoch 173/512
512/512 - 0s - loss: 2.1917e-04 - val_loss: 2.2047e-06
Epoch 174/512
512/512 - 0s - loss: 2.2101e-04 - val_loss: 2.1915e-06
Epoch 175/512
512/512 - 0s - loss: 2.1966e-04 - val_loss: 2.1288e-06
Epoch 176/512
512/512 - 0s - loss: 2.1445e-04 - val_loss: 2.1567e-06
Epoch 177/512
512/512 - 0s - loss: 2.1653e-04 - val_loss: 2.2147e-06
Epoch 178/512
512/512 - 0s - loss: 2.2253e-04 - val_loss: 2.1186e-06
Epoch 179/512
512/512 - 0s - loss: 2.1183e-04 - val_loss: 2.0800e-06
Epoch 180/512
512/512 - 0s - loss: 2.1246e-04 - val_loss: 2.1765e-06
Epoch 181/512
512/512 - 0s - loss: 2.1864e-04 - val_loss: 2.1822e-06
Epoch 182/512
512/512 - 0s - loss: 2.1429e-04 - val_loss: 2.1352e-06
Epoch 183/512
512/512 - 0s - loss: 2.1208e-04 - val_loss: 2.1487e-06
Epoch 184/512
512/512 - 0s - loss: 2.1616e-04 - val_loss: 2.1116e-06
Epoch 185/512
512/512 - 0s - loss: 2.1051e-04 - val_loss: 2.1013e-06
Epoch 186/512
512/512 - 0s - loss: 2.1101e-04 - val_loss: 2.1372e-06
Epoch 187/512
512/512 - 0s - loss: 2.1533e-04 - val_loss: 2.0920e-06
Epoch 188/512
512/512 - 0s - loss: 2.0807e-04 - val_loss: 2.0642e-06
Epoch 189/512
512/512 - 0s - loss: 2.0938e-04 - val_loss: 2.1170e-06
Epoch 190/512
512/512 - 0s - loss: 2.1304e-04 - val_loss: 2.0842e-06
Epoch 191/512
512/512 - 0s - loss: 2.0864e-04 - val_loss: 2.0516e-06
Epoch 192/512
512/512 - 0s - loss: 2.0465e-04 - val_loss: 2.1257e-06
Epoch 193/512
512/512 - 0s - loss: 2.1355e-04 - val_loss: 2.1018e-06
Epoch 194/512
512/512 - 0s - loss: 2.0827e-04 - val_loss: 2.0078e-06
Epoch 195/512
512/512 - 0s - loss: 2.0362e-04 - val_loss: 2.0298e-06
Epoch 196/512
512/512 - 0s - loss: 2.0770e-04 - val_loss: 2.0823e-06
Epoch 197/512
512/512 - 0s - loss: 2.0666e-04 - val_loss: 2.0770e-06
Epoch 198/512
512/512 - 0s - loss: 2.0674e-04 - val_loss: 2.0564e-06
Epoch 199/512
512/512 - 0s - loss: 2.0761e-04 - val_loss: 1.9811e-06
Epoch 200/512
512/512 - 0s - loss: 2.0119e-04 - val_loss: 1.9757e-06
Epoch 201/512
512/512 - 0s - loss: 2.0151e-04 - val_loss: 2.0882e-06
Epoch 202/512
512/512 - 0s - loss: 2.0942e-04 - val_loss: 2.0636e-06
Epoch 203/512
512/512 - 0s - loss: 2.0391e-04 - val_loss: 1.9689e-06
Epoch 204/512
512/512 - 0s - loss: 1.9849e-04 - val_loss: 2.0085e-06
Epoch 205/512
512/512 - 0s - loss: 2.0449e-04 - val_loss: 2.0388e-06
Epoch 206/512
512/512 - 0s - loss: 2.0224e-04 - val_loss: 2.0077e-06
Epoch 207/512
512/512 - 0s - loss: 2.0065e-04 - val_loss: 2.0091e-06
Epoch 208/512
512/512 - 0s - loss: 2.0177e-04 - val_loss: 1.9929e-06
Epoch 209/512
512/512 - 0s - loss: 1.9866e-04 - val_loss: 2.0223e-06
Epoch 210/512
512/512 - 0s - loss: 2.0269e-04 - val_loss: 1.9934e-06
Epoch 211/512
512/512 - 0s - loss: 1.9910e-04 - val_loss: 1.9514e-06
Epoch 212/512
512/512 - 0s - loss: 1.9660e-04 - val_loss: 1.9858e-06
Epoch 213/512
512/512 - 0s - loss: 2.0070e-04 - val_loss: 1.9975e-06
Epoch 214/512
512/512 - 0s - loss: 1.9984e-04 - val_loss: 1.9332e-06
Epoch 215/512
512/512 - 0s - loss: 1.9448e-04 - val_loss: 1.9504e-06
Epoch 216/512
512/512 - 0s - loss: 1.9812e-04 - val_loss: 1.9892e-06
Epoch 217/512
512/512 - 0s - loss: 1.9746e-04 - val_loss: 1.9681e-06
Epoch 218/512
512/512 - 0s - loss: 1.9679e-04 - val_loss: 1.9317e-06
Epoch 219/512
512/512 - 0s - loss: 1.9490e-04 - val_loss: 1.9058e-06
Epoch 220/512
512/512 - 0s - loss: 1.9175e-04 - val_loss: 1.9961e-06
Epoch 221/512
512/512 - 0s - loss: 1.9771e-04 - val_loss: 2.0229e-06
Epoch 222/512
512/512 - 0s - loss: 1.9766e-04 - val_loss: 1.9156e-06
Epoch 223/512
512/512 - 0s - loss: 1.8945e-04 - val_loss: 1.9194e-06
Epoch 224/512
512/512 - 0s - loss: 1.9416e-04 - val_loss: 1.9640e-06
Epoch 225/512
512/512 - 0s - loss: 1.9757e-04 - val_loss: 1.8681e-06
Epoch 226/512
512/512 - 0s - loss: 1.8679e-04 - val_loss: 1.8835e-06
Epoch 227/512
512/512 - 0s - loss: 1.9195e-04 - val_loss: 1.9472e-06
Epoch 228/512
512/512 - 0s - loss: 1.9479e-04 - val_loss: 1.9076e-06
Epoch 229/512
512/512 - 0s - loss: 1.8991e-04 - val_loss: 1.8835e-06
Epoch 230/512
512/512 - 0s - loss: 1.8898e-04 - val_loss: 1.9085e-06
Epoch 231/512
512/512 - 0s - loss: 1.9263e-04 - val_loss: 1.8784e-06
Epoch 232/512
512/512 - 0s - loss: 1.8737e-04 - val_loss: 1.8732e-06
Epoch 233/512
512/512 - 0s - loss: 1.9117e-04 - val_loss: 1.8504e-06
Epoch 234/512
512/512 - 0s - loss: 1.8557e-04 - val_loss: 1.8688e-06
Epoch 235/512
512/512 - 0s - loss: 1.8770e-04 - val_loss: 1.9226e-06
Epoch 236/512
512/512 - 0s - loss: 1.9198e-04 - val_loss: 1.8450e-06
Epoch 237/512
512/512 - 0s - loss: 1.8348e-04 - val_loss: 1.8363e-06
Epoch 238/512
512/512 - 0s - loss: 1.8541e-04 - val_loss: 1.8938e-06
Epoch 239/512
512/512 - 0s - loss: 1.9004e-04 - val_loss: 1.8537e-06
Epoch 240/512
512/512 - 0s - loss: 1.8390e-04 - val_loss: 1.8031e-06
Epoch 241/512
512/512 - 0s - loss: 1.8239e-04 - val_loss: 1.8398e-06
Epoch 242/512
512/512 - 0s - loss: 1.8555e-04 - val_loss: 1.8635e-06
Epoch 243/512
512/512 - 0s - loss: 1.8519e-04 - val_loss: 1.8340e-06
Epoch 244/512
512/512 - 0s - loss: 1.8278e-04 - val_loss: 1.8102e-06
Epoch 245/512
512/512 - 0s - loss: 1.8199e-04 - val_loss: 1.8177e-06
Epoch 246/512
512/512 - 0s - loss: 1.8171e-04 - val_loss: 1.8345e-06
Epoch 247/512
512/512 - 0s - loss: 1.8329e-04 - val_loss: 1.8177e-06
Epoch 248/512
512/512 - 0s - loss: 1.7903e-04 - val_loss: 1.8151e-06
Epoch 249/512
512/512 - 0s - loss: 1.8185e-04 - val_loss: 1.8107e-06
Epoch 250/512
512/512 - 0s - loss: 1.8053e-04 - val_loss: 1.7904e-06
Epoch 251/512
512/512 - 0s - loss: 1.7948e-04 - val_loss: 1.7607e-06
Epoch 252/512
512/512 - 0s - loss: 1.7628e-04 - val_loss: 1.7890e-06
Epoch 253/512
512/512 - 0s - loss: 1.8145e-04 - val_loss: 1.7714e-06
Epoch 254/512
512/512 - 0s - loss: 1.7463e-04 - val_loss: 1.7692e-06
Epoch 255/512
512/512 - 0s - loss: 1.7813e-04 - val_loss: 1.7939e-06
Epoch 256/512
512/512 - 0s - loss: 1.7829e-04 - val_loss: 1.7432e-06
Epoch 257/512
512/512 - 0s - loss: 1.7503e-04 - val_loss: 1.7030e-06
Epoch 258/512
512/512 - 0s - loss: 1.7023e-04 - val_loss: 1.7930e-06
Epoch 259/512
512/512 - 0s - loss: 1.8093e-04 - val_loss: 1.7744e-06
Epoch 260/512
512/512 - 0s - loss: 1.7283e-04 - val_loss: 1.7042e-06
Epoch 261/512
512/512 - 0s - loss: 1.7071e-04 - val_loss: 1.7255e-06
Epoch 262/512
512/512 - 0s - loss: 1.7537e-04 - val_loss: 1.7234e-06
Epoch 263/512
512/512 - 0s - loss: 1.7073e-04 - val_loss: 1.7034e-06
Epoch 264/512
512/512 - 0s - loss: 1.7239e-04 - val_loss: 1.6706e-06
Epoch 265/512
512/512 - 0s - loss: 1.6787e-04 - val_loss: 1.7097e-06
Epoch 266/512
512/512 - 0s - loss: 1.7096e-04 - val_loss: 1.7515e-06
Epoch 267/512
512/512 - 0s - loss: 1.7337e-04 - val_loss: 1.6546e-06
Epoch 268/512
512/512 - 0s - loss: 1.6397e-04 - val_loss: 1.6405e-06
Epoch 269/512
512/512 - 0s - loss: 1.6598e-04 - val_loss: 1.7533e-06
Epoch 270/512
512/512 - 0s - loss: 1.7377e-04 - val_loss: 1.6793e-06
Epoch 271/512
512/512 - 0s - loss: 1.6356e-04 - val_loss: 1.6212e-06
Epoch 272/512
512/512 - 0s - loss: 1.6402e-04 - val_loss: 1.6667e-06
Epoch 273/512
512/512 - 0s - loss: 1.6701e-04 - val_loss: 1.6662e-06
Epoch 274/512
512/512 - 0s - loss: 1.6530e-04 - val_loss: 1.6189e-06
Epoch 275/512
512/512 - 0s - loss: 1.6157e-04 - val_loss: 1.6051e-06
Epoch 276/512
512/512 - 0s - loss: 1.6069e-04 - val_loss: 1.6764e-06
Epoch 277/512
512/512 - 0s - loss: 1.6694e-04 - val_loss: 1.6294e-06
Epoch 278/512
512/512 - 0s - loss: 1.5882e-04 - val_loss: 1.5924e-06
Epoch 279/512
512/512 - 0s - loss: 1.6136e-04 - val_loss: 1.5905e-06
Epoch 280/512
512/512 - 0s - loss: 1.5874e-04 - val_loss: 1.5932e-06
Epoch 281/512
512/512 - 0s - loss: 1.5816e-04 - val_loss: 1.6179e-06
Epoch 282/512
512/512 - 0s - loss: 1.6035e-04 - val_loss: 1.5768e-06
Epoch 283/512
512/512 - 0s - loss: 1.5668e-04 - val_loss: 1.5437e-06
Epoch 284/512
512/512 - 0s - loss: 1.5374e-04 - val_loss: 1.5829e-06
Epoch 285/512
512/512 - 0s - loss: 1.5791e-04 - val_loss: 1.5805e-06
Epoch 286/512
512/512 - 0s - loss: 1.5544e-04 - val_loss: 1.5403e-06
Epoch 287/512
512/512 - 0s - loss: 1.5159e-04 - val_loss: 1.5484e-06
Epoch 288/512
512/512 - 0s - loss: 1.5452e-04 - val_loss: 1.5293e-06
Epoch 289/512
512/512 - 0s - loss: 1.5286e-04 - val_loss: 1.4861e-06
Epoch 290/512
512/512 - 0s - loss: 1.4877e-04 - val_loss: 1.5058e-06
Epoch 291/512
512/512 - 0s - loss: 1.5219e-04 - val_loss: 1.5026e-06
Epoch 292/512
512/512 - 0s - loss: 1.4907e-04 - val_loss: 1.4809e-06
Epoch 293/512
512/512 - 0s - loss: 1.4765e-04 - val_loss: 1.4814e-06
Epoch 294/512
512/512 - 0s - loss: 1.4879e-04 - val_loss: 1.4667e-06
Epoch 295/512
512/512 - 0s - loss: 1.4619e-04 - val_loss: 1.4444e-06
Epoch 296/512
512/512 - 0s - loss: 1.4456e-04 - val_loss: 1.4556e-06
Epoch 297/512
512/512 - 0s - loss: 1.4503e-04 - val_loss: 1.4636e-06
Epoch 298/512
512/512 - 0s - loss: 1.4456e-04 - val_loss: 1.4375e-06
Epoch 299/512
512/512 - 0s - loss: 1.4134e-04 - val_loss: 1.4342e-06
Epoch 300/512
512/512 - 0s - loss: 1.4239e-04 - val_loss: 1.4394e-06
Epoch 301/512
512/512 - 0s - loss: 1.4172e-04 - val_loss: 1.4036e-06
Epoch 302/512
512/512 - 0s - loss: 1.3900e-04 - val_loss: 1.3895e-06
Epoch 303/512
512/512 - 0s - loss: 1.3766e-04 - val_loss: 1.3995e-06
Epoch 304/512
512/512 - 0s - loss: 1.3841e-04 - val_loss: 1.4065e-06
Epoch 305/512
512/512 - 0s - loss: 1.3832e-04 - val_loss: 1.3450e-06
Epoch 306/512
512/512 - 0s - loss: 1.3249e-04 - val_loss: 1.3645e-06
Epoch 307/512
512/512 - 0s - loss: 1.3672e-04 - val_loss: 1.3628e-06
Epoch 308/512
512/512 - 0s - loss: 1.3427e-04 - val_loss: 1.3032e-06
Epoch 309/512
512/512 - 0s - loss: 1.2911e-04 - val_loss: 1.3310e-06
Epoch 310/512
512/512 - 0s - loss: 1.3342e-04 - val_loss: 1.3322e-06
Epoch 311/512
512/512 - 0s - loss: 1.3207e-04 - val_loss: 1.2501e-06
Epoch 312/512
512/512 - 0s - loss: 1.2492e-04 - val_loss: 1.2788e-06
Epoch 313/512
512/512 - 0s - loss: 1.2885e-04 - val_loss: 1.3217e-06
Epoch 314/512
512/512 - 0s - loss: 1.3156e-04 - val_loss: 1.2175e-06
Epoch 315/512
512/512 - 0s - loss: 1.1966e-04 - val_loss: 1.2297e-06
Epoch 316/512
512/512 - 0s - loss: 1.2493e-04 - val_loss: 1.2976e-06
Epoch 317/512
512/512 - 0s - loss: 1.2801e-04 - val_loss: 1.2300e-06
Epoch 318/512
512/512 - 0s - loss: 1.1960e-04 - val_loss: 1.1769e-06
Epoch 319/512
512/512 - 0s - loss: 1.1827e-04 - val_loss: 1.2459e-06
Epoch 320/512
512/512 - 0s - loss: 1.2455e-04 - val_loss: 1.2083e-06
Epoch 321/512
512/512 - 0s - loss: 1.1677e-04 - val_loss: 1.1634e-06
Epoch 322/512
512/512 - 0s - loss: 1.1571e-04 - val_loss: 1.2030e-06
Epoch 323/512
512/512 - 0s - loss: 1.1841e-04 - val_loss: 1.1891e-06
Epoch 324/512
512/512 - 0s - loss: 1.1628e-04 - val_loss: 1.1238e-06
Epoch 325/512
512/512 - 0s - loss: 1.1151e-04 - val_loss: 1.1175e-06
Epoch 326/512
512/512 - 0s - loss: 1.1360e-04 - val_loss: 1.1292e-06
Epoch 327/512
512/512 - 0s - loss: 1.1084e-04 - val_loss: 1.1269e-06
Epoch 328/512
512/512 - 0s - loss: 1.1073e-04 - val_loss: 1.1135e-06
Epoch 329/512
512/512 - 0s - loss: 1.0987e-04 - val_loss: 1.0812e-06
Epoch 330/512
512/512 - 0s - loss: 1.0688e-04 - val_loss: 1.0592e-06
Epoch 331/512
512/512 - 0s - loss: 1.0582e-04 - val_loss: 1.0708e-06
Epoch 332/512
512/512 - 0s - loss: 1.0554e-04 - val_loss: 1.0767e-06
Epoch 333/512
512/512 - 0s - loss: 1.0550e-04 - val_loss: 1.0297e-06
Epoch 334/512
512/512 - 0s - loss: 1.0151e-04 - val_loss: 1.0077e-06
Epoch 335/512
512/512 - 0s - loss: 1.0112e-04 - val_loss: 1.0170e-06
Epoch 336/512
512/512 - 0s - loss: 1.0055e-04 - val_loss: 1.0109e-06
Epoch 337/512
512/512 - 0s - loss: 9.9870e-05 - val_loss: 9.7890e-07
Epoch 338/512
512/512 - 0s - loss: 9.5764e-05 - val_loss: 9.7832e-07
Epoch 339/512
512/512 - 0s - loss: 9.7381e-05 - val_loss: 9.7843e-07
Epoch 340/512
512/512 - 0s - loss: 9.6286e-05 - val_loss: 9.3664e-07
Epoch 341/512
512/512 - 0s - loss: 9.2275e-05 - val_loss: 9.2343e-07
Epoch 342/512
512/512 - 0s - loss: 9.1798e-05 - val_loss: 9.4816e-07
Epoch 343/512
512/512 - 0s - loss: 9.2866e-05 - val_loss: 9.2804e-07
Epoch 344/512
512/512 - 0s - loss: 8.9886e-05 - val_loss: 8.9246e-07
Epoch 345/512
512/512 - 0s - loss: 8.7746e-05 - val_loss: 8.8973e-07
Epoch 346/512
512/512 - 0s - loss: 8.7746e-05 - val_loss: 8.8528e-07
Epoch 347/512
512/512 - 0s - loss: 8.6942e-05 - val_loss: 8.4889e-07
Epoch 348/512
512/512 - 0s - loss: 8.3468e-05 - val_loss: 8.4533e-07
Epoch 349/512
512/512 - 0s - loss: 8.4164e-05 - val_loss: 8.4244e-07
Epoch 350/512
512/512 - 0s - loss: 8.2156e-05 - val_loss: 8.2797e-07
Epoch 351/512
512/512 - 0s - loss: 8.0695e-05 - val_loss: 8.2013e-07
Epoch 352/512
512/512 - 0s - loss: 8.0439e-05 - val_loss: 8.0032e-07
Epoch 353/512
512/512 - 0s - loss: 7.7949e-05 - val_loss: 7.8582e-07
Epoch 354/512
512/512 - 0s - loss: 7.7200e-05 - val_loss: 7.8130e-07
Epoch 355/512
512/512 - 0s - loss: 7.6478e-05 - val_loss: 7.5164e-07
Epoch 356/512
512/512 - 0s - loss: 7.3864e-05 - val_loss: 7.4326e-07
Epoch 357/512
512/512 - 0s - loss: 7.3347e-05 - val_loss: 7.4589e-07
Epoch 358/512
512/512 - 0s - loss: 7.2925e-05 - val_loss: 7.2185e-07
Epoch 359/512
512/512 - 0s - loss: 7.0239e-05 - val_loss: 7.0618e-07
Epoch 360/512
512/512 - 0s - loss: 6.9202e-05 - val_loss: 7.0839e-07
Epoch 361/512
512/512 - 0s - loss: 6.9847e-05 - val_loss: 6.7809e-07
Epoch 362/512
512/512 - 0s - loss: 6.5875e-05 - val_loss: 6.6266e-07
Epoch 363/512
512/512 - 0s - loss: 6.5150e-05 - val_loss: 6.8558e-07
Epoch 364/512
512/512 - 0s - loss: 6.6801e-05 - val_loss: 6.5836e-07
Epoch 365/512
512/512 - 0s - loss: 6.3027e-05 - val_loss: 6.1842e-07
Epoch 366/512
512/512 - 0s - loss: 6.1799e-05 - val_loss: 6.1103e-07
Epoch 367/512
512/512 - 0s - loss: 6.0729e-05 - val_loss: 6.1535e-07
Epoch 368/512
512/512 - 0s - loss: 6.0142e-05 - val_loss: 6.1585e-07
Epoch 369/512
512/512 - 0s - loss: 5.9339e-05 - val_loss: 5.9441e-07
Epoch 370/512
512/512 - 0s - loss: 5.7485e-05 - val_loss: 5.7138e-07
Epoch 371/512
512/512 - 0s - loss: 5.6565e-05 - val_loss: 5.5218e-07
Epoch 372/512
512/512 - 0s - loss: 5.3958e-05 - val_loss: 5.6225e-07
Epoch 373/512
512/512 - 0s - loss: 5.4930e-05 - val_loss: 5.6545e-07
Epoch 374/512
512/512 - 0s - loss: 5.4187e-05 - val_loss: 5.2845e-07
Epoch 375/512
512/512 - 0s - loss: 5.0922e-05 - val_loss: 5.0857e-07
Epoch 376/512
512/512 - 0s - loss: 5.0359e-05 - val_loss: 5.1564e-07
Epoch 377/512
512/512 - 0s - loss: 5.0501e-05 - val_loss: 5.0714e-07
Epoch 378/512
512/512 - 0s - loss: 4.8469e-05 - val_loss: 4.8976e-07
Epoch 379/512
512/512 - 0s - loss: 4.7707e-05 - val_loss: 4.7480e-07
Epoch 380/512
512/512 - 0s - loss: 4.6380e-05 - val_loss: 4.6384e-07
Epoch 381/512
512/512 - 0s - loss: 4.5349e-05 - val_loss: 4.5530e-07
Epoch 382/512
512/512 - 0s - loss: 4.4568e-05 - val_loss: 4.4468e-07
Epoch 383/512
512/512 - 0s - loss: 4.3416e-05 - val_loss: 4.3459e-07
Epoch 384/512
512/512 - 0s - loss: 4.2416e-05 - val_loss: 4.2607e-07
Epoch 385/512
512/512 - 0s - loss: 4.1364e-05 - val_loss: 4.1668e-07
Epoch 386/512
512/512 - 0s - loss: 4.0551e-05 - val_loss: 4.0643e-07
Epoch 387/512
512/512 - 0s - loss: 3.9289e-05 - val_loss: 3.9894e-07
Epoch 388/512
512/512 - 0s - loss: 3.8730e-05 - val_loss: 3.8969e-07
Epoch 389/512
512/512 - 0s - loss: 3.7848e-05 - val_loss: 3.7251e-07
Epoch 390/512
512/512 - 0s - loss: 3.6250e-05 - val_loss: 3.6459e-07
Epoch 391/512
512/512 - 0s - loss: 3.5925e-05 - val_loss: 3.6014e-07
Epoch 392/512
512/512 - 0s - loss: 3.4811e-05 - val_loss: 3.5433e-07
Epoch 393/512
512/512 - 0s - loss: 3.4148e-05 - val_loss: 3.4019e-07
Epoch 394/512
512/512 - 0s - loss: 3.2838e-05 - val_loss: 3.3186e-07
Epoch 395/512
512/512 - 0s - loss: 3.2301e-05 - val_loss: 3.2643e-07
Epoch 396/512
512/512 - 0s - loss: 3.1637e-05 - val_loss: 3.1592e-07
Epoch 397/512
512/512 - 0s - loss: 3.0301e-05 - val_loss: 3.0864e-07
Epoch 398/512
512/512 - 0s - loss: 2.9966e-05 - val_loss: 3.0178e-07
Epoch 399/512
512/512 - 0s - loss: 2.8961e-05 - val_loss: 2.9139e-07
Epoch 400/512
512/512 - 0s - loss: 2.8223e-05 - val_loss: 2.8570e-07
Epoch 401/512
512/512 - 0s - loss: 2.7558e-05 - val_loss: 2.7458e-07
Epoch 402/512
512/512 - 0s - loss: 2.6593e-05 - val_loss: 2.6677e-07
Epoch 403/512
512/512 - 0s - loss: 2.5784e-05 - val_loss: 2.6348e-07
Epoch 404/512
512/512 - 0s - loss: 2.5416e-05 - val_loss: 2.5707e-07
Epoch 405/512
512/512 - 0s - loss: 2.4715e-05 - val_loss: 2.4267e-07
Epoch 406/512
512/512 - 0s - loss: 2.3412e-05 - val_loss: 2.3796e-07
Epoch 407/512
512/512 - 0s - loss: 2.3106e-05 - val_loss: 2.3911e-07
Epoch 408/512
512/512 - 0s - loss: 2.2757e-05 - val_loss: 2.3289e-07
Epoch 409/512
512/512 - 0s - loss: 2.2064e-05 - val_loss: 2.1590e-07
Epoch 410/512
512/512 - 0s - loss: 2.0627e-05 - val_loss: 2.1154e-07
Epoch 411/512
512/512 - 0s - loss: 2.0585e-05 - val_loss: 2.1484e-07
Epoch 412/512
512/512 - 0s - loss: 2.0513e-05 - val_loss: 2.0197e-07
Epoch 413/512
512/512 - 0s - loss: 1.9049e-05 - val_loss: 1.9006e-07
Epoch 414/512
512/512 - 0s - loss: 1.8336e-05 - val_loss: 1.9437e-07
Epoch 415/512
512/512 - 0s - loss: 1.9116e-05 - val_loss: 1.8264e-07
Epoch 416/512
512/512 - 0s - loss: 1.7185e-05 - val_loss: 1.6932e-07
Epoch 417/512
512/512 - 0s - loss: 1.6451e-05 - val_loss: 1.8080e-07
Epoch 418/512
512/512 - 0s - loss: 1.7474e-05 - val_loss: 1.7340e-07
Epoch 419/512
512/512 - 0s - loss: 1.6054e-05 - val_loss: 1.5442e-07
Epoch 420/512
512/512 - 0s - loss: 1.4929e-05 - val_loss: 1.5579e-07
Epoch 421/512
512/512 - 0s - loss: 1.5183e-05 - val_loss: 1.6168e-07
Epoch 422/512
512/512 - 0s - loss: 1.5116e-05 - val_loss: 1.4703e-07
Epoch 423/512
512/512 - 0s - loss: 1.3657e-05 - val_loss: 1.3894e-07
Epoch 424/512
512/512 - 0s - loss: 1.3453e-05 - val_loss: 1.4500e-07
Epoch 425/512
512/512 - 0s - loss: 1.3784e-05 - val_loss: 1.3627e-07
Epoch 426/512
512/512 - 0s - loss: 1.2727e-05 - val_loss: 1.2288e-07
Epoch 427/512
512/512 - 0s - loss: 1.1956e-05 - val_loss: 1.2280e-07
Epoch 428/512
512/512 - 0s - loss: 1.1883e-05 - val_loss: 1.2797e-07
Epoch 429/512
512/512 - 0s - loss: 1.2229e-05 - val_loss: 1.1574e-07
Epoch 430/512
512/512 - 0s - loss: 1.0772e-05 - val_loss: 1.0555e-07
Epoch 431/512
512/512 - 0s - loss: 1.0395e-05 - val_loss: 1.1257e-07
Epoch 432/512
512/512 - 0s - loss: 1.0913e-05 - val_loss: 1.1061e-07
Epoch 433/512
512/512 - 0s - loss: 1.0200e-05 - val_loss: 9.7065e-08
Epoch 434/512
512/512 - 0s - loss: 9.2831e-06 - val_loss: 9.5298e-08
Epoch 435/512
512/512 - 0s - loss: 9.3487e-06 - val_loss: 9.9724e-08
Epoch 436/512
512/512 - 0s - loss: 9.4648e-06 - val_loss: 9.2615e-08
Epoch 437/512
512/512 - 0s - loss: 8.5262e-06 - val_loss: 8.5876e-08
Epoch 438/512
512/512 - 0s - loss: 8.1839e-06 - val_loss: 8.9631e-08
Epoch 439/512
512/512 - 0s - loss: 8.4962e-06 - val_loss: 8.5976e-08
Epoch 440/512
512/512 - 0s - loss: 7.9259e-06 - val_loss: 7.5557e-08
Epoch 441/512
512/512 - 0s - loss: 7.2566e-06 - val_loss: 7.4515e-08
Epoch 442/512
512/512 - 0s - loss: 7.2680e-06 - val_loss: 7.8683e-08
Epoch 443/512
512/512 - 0s - loss: 7.4336e-06 - val_loss: 7.0699e-08
Epoch 444/512
512/512 - 0s - loss: 6.5434e-06 - val_loss: 6.4672e-08
Epoch 445/512
512/512 - 0s - loss: 6.3660e-06 - val_loss: 6.7434e-08
Epoch 446/512
512/512 - 0s - loss: 6.5687e-06 - val_loss: 6.4307e-08
Epoch 447/512
512/512 - 0s - loss: 5.9316e-06 - val_loss: 5.9890e-08
Epoch 448/512
512/512 - 0s - loss: 5.7714e-06 - val_loss: 6.0284e-08
Epoch 449/512
512/512 - 0s - loss: 5.7312e-06 - val_loss: 5.7699e-08
Epoch 450/512
512/512 - 0s - loss: 5.4182e-06 - val_loss: 5.3596e-08
Epoch 451/512
512/512 - 0s - loss: 5.1342e-06 - val_loss: 5.1840e-08
Epoch 452/512
512/512 - 0s - loss: 4.9813e-06 - val_loss: 5.1535e-08
Epoch 453/512
512/512 - 0s - loss: 4.9154e-06 - val_loss: 4.9065e-08
Epoch 454/512
512/512 - 0s - loss: 4.5918e-06 - val_loss: 4.6786e-08
Epoch 455/512
512/512 - 0s - loss: 4.4605e-06 - val_loss: 4.5365e-08
Epoch 456/512
512/512 - 0s - loss: 4.2927e-06 - val_loss: 4.3642e-08
Epoch 457/512
512/512 - 0s - loss: 4.1665e-06 - val_loss: 4.1105e-08
Epoch 458/512
512/512 - 0s - loss: 3.9292e-06 - val_loss: 3.9215e-08
Epoch 459/512
512/512 - 0s - loss: 3.7535e-06 - val_loss: 3.8824e-08
Epoch 460/512
512/512 - 0s - loss: 3.7167e-06 - val_loss: 3.7609e-08
Epoch 461/512
512/512 - 0s - loss: 3.4987e-06 - val_loss: 3.5923e-08
Epoch 462/512
512/512 - 0s - loss: 3.3905e-06 - val_loss: 3.4849e-08
Epoch 463/512
512/512 - 0s - loss: 3.2288e-06 - val_loss: 3.3764e-08
Epoch 464/512
512/512 - 0s - loss: 3.1855e-06 - val_loss: 3.1559e-08
Epoch 465/512
512/512 - 0s - loss: 2.9369e-06 - val_loss: 2.9964e-08
Epoch 466/512
512/512 - 0s - loss: 2.9021e-06 - val_loss: 2.8700e-08
Epoch 467/512
512/512 - 0s - loss: 2.7283e-06 - val_loss: 2.7935e-08
Epoch 468/512
512/512 - 0s - loss: 2.6573e-06 - val_loss: 2.7043e-08
Epoch 469/512
512/512 - 0s - loss: 2.5243e-06 - val_loss: 2.6319e-08
Epoch 470/512
512/512 - 0s - loss: 2.4780e-06 - val_loss: 2.4700e-08
Epoch 471/512
512/512 - 0s - loss: 2.3221e-06 - val_loss: 2.3258e-08
Epoch 472/512
512/512 - 0s - loss: 2.1854e-06 - val_loss: 2.3544e-08
Epoch 473/512
512/512 - 0s - loss: 2.2221e-06 - val_loss: 2.2511e-08
Epoch 474/512
512/512 - 0s - loss: 2.0439e-06 - val_loss: 2.0779e-08
Epoch 475/512
512/512 - 0s - loss: 1.9633e-06 - val_loss: 2.0163e-08
Epoch 476/512
512/512 - 0s - loss: 1.8922e-06 - val_loss: 1.9710e-08
Epoch 477/512
512/512 - 0s - loss: 1.8394e-06 - val_loss: 1.8705e-08
Epoch 478/512
512/512 - 0s - loss: 1.7419e-06 - val_loss: 1.7445e-08
Epoch 479/512
512/512 - 0s - loss: 1.6333e-06 - val_loss: 1.7083e-08
Epoch 480/512
512/512 - 0s - loss: 1.6213e-06 - val_loss: 1.6628e-08
Epoch 481/512
512/512 - 0s - loss: 1.5422e-06 - val_loss: 1.5402e-08
Epoch 482/512
512/512 - 0s - loss: 1.4363e-06 - val_loss: 1.4852e-08
Epoch 483/512
512/512 - 0s - loss: 1.3851e-06 - val_loss: 1.5132e-08
Epoch 484/512
512/512 - 0s - loss: 1.3979e-06 - val_loss: 1.4080e-08
Epoch 485/512
512/512 - 0s - loss: 1.2743e-06 - val_loss: 1.2779e-08
Epoch 486/512
512/512 - 0s - loss: 1.1929e-06 - val_loss: 1.2822e-08
Epoch 487/512
512/512 - 0s - loss: 1.2146e-06 - val_loss: 1.2389e-08
Epoch 488/512
512/512 - 0s - loss: 1.1495e-06 - val_loss: 1.0944e-08
Epoch 489/512
512/512 - 0s - loss: 1.0208e-06 - val_loss: 1.0809e-08
Epoch 490/512
512/512 - 0s - loss: 1.0526e-06 - val_loss: 1.1026e-08
Epoch 491/512
512/512 - 0s - loss: 1.0192e-06 - val_loss: 9.8154e-09
Epoch 492/512
512/512 - 0s - loss: 9.1860e-07 - val_loss: 9.1196e-09
Epoch 493/512
512/512 - 0s - loss: 8.7726e-07 - val_loss: 9.4979e-09
Epoch 494/512
512/512 - 0s - loss: 9.0268e-07 - val_loss: 9.1073e-09
Epoch 495/512
512/512 - 0s - loss: 8.1894e-07 - val_loss: 8.2049e-09
Epoch 496/512
512/512 - 0s - loss: 7.6690e-07 - val_loss: 8.1850e-09
Epoch 497/512
512/512 - 0s - loss: 7.7843e-07 - val_loss: 7.8877e-09
Epoch 498/512
512/512 - 0s - loss: 7.1561e-07 - val_loss: 7.2363e-09
Epoch 499/512
512/512 - 0s - loss: 6.8095e-07 - val_loss: 7.0978e-09
Epoch 500/512
512/512 - 0s - loss: 6.6629e-07 - val_loss: 6.7777e-09
Epoch 501/512
512/512 - 0s - loss: 6.2843e-07 - val_loss: 6.3293e-09
Epoch 502/512
512/512 - 0s - loss: 5.9768e-07 - val_loss: 6.0977e-09
Epoch 503/512
512/512 - 0s - loss: 5.7356e-07 - val_loss: 5.9228e-09
Epoch 504/512
512/512 - 0s - loss: 5.5411e-07 - val_loss: 5.5804e-09
Epoch 505/512
512/512 - 0s - loss: 5.1830e-07 - val_loss: 5.2756e-09
Epoch 506/512
512/512 - 0s - loss: 4.9119e-07 - val_loss: 5.2681e-09
Epoch 507/512
512/512 - 0s - loss: 4.9539e-07 - val_loss: 4.9432e-09
Epoch 508/512
512/512 - 0s - loss: 4.4928e-07 - val_loss: 4.5287e-09
Epoch 509/512
512/512 - 0s - loss: 4.2612e-07 - val_loss: 4.5178e-09
Epoch 510/512
512/512 - 0s - loss: 4.1981e-07 - val_loss: 4.4136e-09
Epoch 511/512
512/512 - 0s - loss: 4.0370e-07 - val_loss: 4.0176e-09
Epoch 512/512
512/512 - 0s - loss: 3.7373e-07 - val_loss: 3.7261e-09
2024-04-09 13:01:29.834023: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0861e-07 - val_loss: 4.6068e-07
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8851e-07 - val_loss: 3.2949e-07
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9129e-07 - val_loss: 3.0134e-07
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0858e-07 - val_loss: 3.6834e-07
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4300e-07 - val_loss: 3.2861e-07
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8243e-07 - val_loss: 2.6504e-07
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5370e-07 - val_loss: 2.8497e-07
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7911e-07 - val_loss: 2.9699e-07
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6316e-07 - val_loss: 2.4810e-07
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2561e-07 - val_loss: 2.3284e-07
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2566e-07 - val_loss: 2.4924e-07
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3128e-07 - val_loss: 2.2899e-07
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0443e-07 - val_loss: 2.0209e-07
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9059e-07 - val_loss: 2.0493e-07
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9422e-07 - val_loss: 2.0247e-07
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8369e-07 - val_loss: 1.8090e-07
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6565e-07 - val_loss: 1.7252e-07
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6370e-07 - val_loss: 1.7223e-07
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5878e-07 - val_loss: 1.6075e-07
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4723e-07 - val_loss: 1.4868e-07
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3860e-07 - val_loss: 1.4559e-07
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3607e-07 - val_loss: 1.4108e-07
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2939e-07 - val_loss: 1.3028e-07
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2020e-07 - val_loss: 1.2348e-07
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1526e-07 - val_loss: 1.2102e-07
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1215e-07 - val_loss: 1.1468e-07
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0502e-07 - val_loss: 1.0680e-07
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8896e-08 - val_loss: 1.0271e-07
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5925e-08 - val_loss: 9.8833e-08
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1049e-08 - val_loss: 9.2869e-08
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5822e-08 - val_loss: 8.8375e-08
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2015e-08 - val_loss: 8.5143e-08
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8783e-08 - val_loss: 8.0318e-08
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3963e-08 - val_loss: 7.6237e-08
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0612e-08 - val_loss: 7.3284e-08
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7751e-08 - val_loss: 6.9384e-08
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3752e-08 - val_loss: 6.5826e-08
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0975e-08 - val_loss: 6.2773e-08
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8204e-08 - val_loss: 5.9284e-08
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4871e-08 - val_loss: 5.6641e-08
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2342e-08 - val_loss: 5.4362e-08
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0246e-08 - val_loss: 5.1465e-08
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7366e-08 - val_loss: 4.8453e-08
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4971e-08 - val_loss: 4.6284e-08
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3012e-08 - val_loss: 4.4369e-08
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1014e-08 - val_loss: 4.1735e-08
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8617e-08 - val_loss: 3.9785e-08
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6935e-08 - val_loss: 3.8063e-08
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5311e-08 - val_loss: 3.6008e-08
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3260e-08 - val_loss: 3.4222e-08
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1786e-08 - val_loss: 3.2666e-08
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0236e-08 - val_loss: 3.1051e-08
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8790e-08 - val_loss: 2.9389e-08
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7258e-08 - val_loss: 2.7947e-08
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5965e-08 - val_loss: 2.6721e-08
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4852e-08 - val_loss: 2.5345e-08
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3474e-08 - val_loss: 2.4027e-08
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2366e-08 - val_loss: 2.2852e-08
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1203e-08 - val_loss: 2.1808e-08
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0236e-08 - val_loss: 2.0897e-08
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9362e-08 - val_loss: 1.9668e-08
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8195e-08 - val_loss: 1.8703e-08
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7416e-08 - val_loss: 1.7975e-08
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6700e-08 - val_loss: 1.6926e-08
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5767e-08 - val_loss: 1.6023e-08
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4865e-08 - val_loss: 1.5276e-08
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4281e-08 - val_loss: 1.4745e-08
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3707e-08 - val_loss: 1.4037e-08
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2969e-08 - val_loss: 1.3157e-08
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2203e-08 - val_loss: 1.2520e-08
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1749e-08 - val_loss: 1.2029e-08
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1202e-08 - val_loss: 1.1395e-08
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0607e-08 - val_loss: 1.0872e-08
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0179e-08 - val_loss: 1.0326e-08
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6120e-09 - val_loss: 9.7820e-09
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1628e-09 - val_loss: 9.3839e-09
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8070e-09 - val_loss: 9.0008e-09
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3979e-09 - val_loss: 8.4665e-09
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9229e-09 - val_loss: 8.0507e-09
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5736e-09 - val_loss: 7.7904e-09
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3012e-09 - val_loss: 7.4099e-09
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9053e-09 - val_loss: 6.9773e-09
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5455e-09 - val_loss: 6.6016e-09
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2084e-09 - val_loss: 6.4421e-09
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0904e-09 - val_loss: 6.2861e-09
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8474e-09 - val_loss: 5.7958e-09
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3886e-09 - val_loss: 5.4177e-09
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1209e-09 - val_loss: 5.3060e-09
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0410e-09 - val_loss: 5.2367e-09
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9279e-09 - val_loss: 4.9371e-09
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5752e-09 - val_loss: 4.5409e-09
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2680e-09 - val_loss: 4.3228e-09
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1307e-09 - val_loss: 4.2609e-09
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0610e-09 - val_loss: 4.1526e-09
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9226e-09 - val_loss: 3.9099e-09
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6575e-09 - val_loss: 3.6712e-09
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4696e-09 - val_loss: 3.5289e-09
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3601e-09 - val_loss: 3.4437e-09
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2613e-09 - val_loss: 3.2985e-09
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1151e-09 - val_loss: 3.1359e-09
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9720e-09 - val_loss: 2.9816e-09
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8324e-09 - val_loss: 2.8527e-09
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7095e-09 - val_loss: 2.7580e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6378e-09 - val_loss: 2.6979e-09
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5692e-09 - val_loss: 2.5819e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4417e-09 - val_loss: 2.4362e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3039e-09 - val_loss: 2.3314e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2226e-09 - val_loss: 2.2538e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1469e-09 - val_loss: 2.1766e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0804e-09 - val_loss: 2.1330e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0363e-09 - val_loss: 2.0685e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9594e-09 - val_loss: 1.9467e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8489e-09 - val_loss: 1.8453e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7696e-09 - val_loss: 1.8002e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7298e-09 - val_loss: 1.7477e-09
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6662e-09 - val_loss: 1.6661e-09
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5878e-09 - val_loss: 1.5970e-09
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5323e-09 - val_loss: 1.5387e-09
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4764e-09 - val_loss: 1.5074e-09
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4686e-09 - val_loss: 1.5190e-09
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4497e-09 - val_loss: 1.4474e-09
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3707e-09 - val_loss: 1.3550e-09
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2925e-09 - val_loss: 1.2853e-09
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2351e-09 - val_loss: 1.2486e-09
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2075e-09 - val_loss: 1.2246e-09
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1791e-09 - val_loss: 1.1955e-09
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1465e-09 - val_loss: 1.1518e-09
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1037e-09 - val_loss: 1.1152e-09
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0782e-09 - val_loss: 1.0971e-09
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0583e-09 - val_loss: 1.0704e-09
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0358e-09 - val_loss: 1.0428e-09
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0002e-09 - val_loss: 9.9750e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5571e-10 - val_loss: 9.4242e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0892e-10 - val_loss: 9.1265e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8044e-10 - val_loss: 8.8368e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6044e-10 - val_loss: 8.6372e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3632e-10 - val_loss: 8.4027e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1488e-10 - val_loss: 8.2035e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9856e-10 - val_loss: 8.0728e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8084e-10 - val_loss: 7.8288e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6272e-10 - val_loss: 7.6994e-10
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4961e-10 - val_loss: 7.6267e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3546e-10 - val_loss: 7.3473e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0653e-10 - val_loss: 7.0432e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7884e-10 - val_loss: 6.6385e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3948e-10 - val_loss: 6.3811e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2021e-10 - val_loss: 6.3089e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1293e-10 - val_loss: 6.2277e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0966e-10 - val_loss: 6.2032e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0274e-10 - val_loss: 6.0524e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8742e-10 - val_loss: 5.9186e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7395e-10 - val_loss: 5.8720e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7675e-10 - val_loss: 5.8244e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6852e-10 - val_loss: 5.6656e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4265e-10 - val_loss: 5.3436e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1757e-10 - val_loss: 5.1600e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0118e-10 - val_loss: 4.9336e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7673e-10 - val_loss: 4.7919e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6676e-10 - val_loss: 4.7241e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6520e-10 - val_loss: 4.6815e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5673e-10 - val_loss: 4.5776e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4781e-10 - val_loss: 4.4717e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3854e-10 - val_loss: 4.4125e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2996e-10 - val_loss: 4.3317e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2896e-10 - val_loss: 4.4053e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3225e-10 - val_loss: 4.3664e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2573e-10 - val_loss: 4.2811e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1414e-10 - val_loss: 4.1451e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0170e-10 - val_loss: 3.9092e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7298e-10 - val_loss: 3.6258e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5301e-10 - val_loss: 3.5236e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4655e-10 - val_loss: 3.5489e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5236e-10 - val_loss: 3.5857e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5405e-10 - val_loss: 3.6114e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5300e-10 - val_loss: 3.5228e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4381e-10 - val_loss: 3.4237e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3454e-10 - val_loss: 3.3702e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3186e-10 - val_loss: 3.3903e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3571e-10 - val_loss: 3.3896e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3460e-10 - val_loss: 3.3497e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2561e-10 - val_loss: 3.2299e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1219e-10 - val_loss: 3.0324e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9232e-10 - val_loss: 2.8725e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8023e-10 - val_loss: 2.8008e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7628e-10 - val_loss: 2.8285e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8086e-10 - val_loss: 2.8898e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8670e-10 - val_loss: 2.8866e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8249e-10 - val_loss: 2.8232e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7498e-10 - val_loss: 2.7516e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6789e-10 - val_loss: 2.6131e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5747e-10 - val_loss: 2.5983e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5800e-10 - val_loss: 2.6726e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6589e-10 - val_loss: 2.7539e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7240e-10 - val_loss: 2.7678e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6918e-10 - val_loss: 2.6349e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5154e-10 - val_loss: 2.3798e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3012e-10 - val_loss: 2.2503e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1964e-10 - val_loss: 2.1878e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1812e-10 - val_loss: 2.2448e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2425e-10 - val_loss: 2.2686e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2361e-10 - val_loss: 2.2684e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2545e-10 - val_loss: 2.2649e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2215e-10 - val_loss: 2.2089e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1538e-10 - val_loss: 2.1518e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1412e-10 - val_loss: 2.1574e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1160e-10 - val_loss: 2.1189e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1246e-10 - val_loss: 2.1965e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1832e-10 - val_loss: 2.2363e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2215e-10 - val_loss: 2.2483e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1847e-10 - val_loss: 2.0988e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0008e-10 - val_loss: 1.9393e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8906e-10 - val_loss: 1.8651e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8159e-10 - val_loss: 1.8001e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7772e-10 - val_loss: 1.8052e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7783e-10 - val_loss: 1.7921e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8074e-10 - val_loss: 1.8749e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8707e-10 - val_loss: 1.9078e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8896e-10 - val_loss: 1.8997e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8718e-10 - val_loss: 1.8403e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7987e-10 - val_loss: 1.7859e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7564e-10 - val_loss: 1.7482e-10
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7418e-10 - val_loss: 1.7853e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7740e-10 - val_loss: 1.7975e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7854e-10 - val_loss: 1.8137e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7847e-10 - val_loss: 1.7482e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6851e-10 - val_loss: 1.6413e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5975e-10 - val_loss: 1.5686e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5563e-10 - val_loss: 1.5613e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5386e-10 - val_loss: 1.5525e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5434e-10 - val_loss: 1.5734e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5599e-10 - val_loss: 1.6019e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5860e-10 - val_loss: 1.5841e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5750e-10 - val_loss: 1.5642e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5458e-10 - val_loss: 1.5558e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5219e-10 - val_loss: 1.4920e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4738e-10 - val_loss: 1.4698e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4966e-10 - val_loss: 1.5835e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6002e-10 - val_loss: 1.6500e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6127e-10 - val_loss: 1.5606e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4918e-10 - val_loss: 1.4358e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4014e-10 - val_loss: 1.3852e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3670e-10 - val_loss: 1.3613e-10
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3653e-10 - val_loss: 1.3769e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3785e-10 - val_loss: 1.3991e-10
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3804e-10 - val_loss: 1.3822e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3736e-10 - val_loss: 1.3675e-10
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3567e-10 - val_loss: 1.3651e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3537e-10 - val_loss: 1.3476e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3263e-10 - val_loss: 1.3154e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3080e-10 - val_loss: 1.3231e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3065e-10 - val_loss: 1.3055e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2983e-10 - val_loss: 1.3477e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3744e-10 - val_loss: 1.4206e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4184e-10 - val_loss: 1.4195e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3560e-10 - val_loss: 1.2801e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2604e-10 - val_loss: 1.2533e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2190e-10 - val_loss: 1.1983e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1787e-10 - val_loss: 1.1646e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1655e-10 - val_loss: 1.1872e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1878e-10 - val_loss: 1.2096e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2016e-10 - val_loss: 1.1906e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1886e-10 - val_loss: 1.2037e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1908e-10 - val_loss: 1.1921e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1746e-10 - val_loss: 1.1793e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1802e-10 - val_loss: 1.1910e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1867e-10 - val_loss: 1.1984e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1978e-10 - val_loss: 1.2225e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2448e-10 - val_loss: 1.2897e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2828e-10 - val_loss: 1.2726e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2405e-10 - val_loss: 1.1798e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1508e-10 - val_loss: 1.1225e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0916e-10 - val_loss: 1.0579e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0473e-10 - val_loss: 1.0427e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0346e-10 - val_loss: 1.0315e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0348e-10 - val_loss: 1.0582e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0623e-10 - val_loss: 1.0756e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0656e-10 - val_loss: 1.0718e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0777e-10 - val_loss: 1.0672e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0533e-10 - val_loss: 1.0553e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0543e-10 - val_loss: 1.0728e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0636e-10 - val_loss: 1.0726e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0630e-10 - val_loss: 1.0654e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0553e-10 - val_loss: 1.0563e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0706e-10 - val_loss: 1.1066e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1077e-10 - val_loss: 1.1285e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1295e-10 - val_loss: 1.1027e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0778e-10 - val_loss: 1.0736e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0519e-10 - val_loss: 1.0407e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0253e-10 - val_loss: 1.0110e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.9721e-11 - val_loss: 9.7624e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6618e-11 - val_loss: 9.7007e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6156e-11 - val_loss: 9.6480e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5513e-11 - val_loss: 9.6188e-11
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5534e-11 - val_loss: 9.5812e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4906e-11 - val_loss: 9.6540e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5788e-11 - val_loss: 9.4456e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3285e-11 - val_loss: 9.2933e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4070e-11 - val_loss: 9.5353e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3953e-11 - val_loss: 9.3518e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4156e-11 - val_loss: 9.8326e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9243e-11 - val_loss: 1.0452e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0507e-10 - val_loss: 1.0372e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0067e-10 - val_loss: 9.7859e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5593e-11 - val_loss: 9.3896e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1572e-11 - val_loss: 9.0072e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9607e-11 - val_loss: 9.0082e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8510e-11 - val_loss: 8.7127e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6485e-11 - val_loss: 8.6460e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5713e-11 - val_loss: 8.5659e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4742e-11 - val_loss: 8.5691e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6355e-11 - val_loss: 8.8775e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9271e-11 - val_loss: 9.0481e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0026e-11 - val_loss: 8.9917e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8228e-11 - val_loss: 8.6767e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6388e-11 - val_loss: 8.8084e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7166e-11 - val_loss: 8.6822e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8137e-11 - val_loss: 9.1668e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2969e-11 - val_loss: 9.4890e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3140e-11 - val_loss: 9.1241e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9618e-11 - val_loss: 8.7643e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6598e-11 - val_loss: 8.6364e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5613e-11 - val_loss: 8.4964e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4401e-11 - val_loss: 8.3460e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.2742e-11 - val_loss: 8.2423e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1759e-11 - val_loss: 8.1233e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0716e-11 - val_loss: 8.1543e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1619e-11 - val_loss: 8.2008e-11
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1223e-11 - val_loss: 8.0036e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0804e-11 - val_loss: 8.2532e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2063e-11 - val_loss: 8.3247e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3949e-11 - val_loss: 8.4326e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4468e-11 - val_loss: 8.3739e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3489e-11 - val_loss: 8.4495e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5106e-11 - val_loss: 8.7438e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5919e-11 - val_loss: 8.2688e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1692e-11 - val_loss: 7.9259e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7731e-11 - val_loss: 7.7195e-11
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6868e-11 - val_loss: 7.6541e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5849e-11 - val_loss: 7.5700e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5103e-11 - val_loss: 7.5276e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4713e-11 - val_loss: 7.5070e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5354e-11 - val_loss: 7.6723e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6483e-11 - val_loss: 7.6677e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6256e-11 - val_loss: 7.6868e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6327e-11 - val_loss: 7.6095e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6333e-11 - val_loss: 7.7229e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6765e-11 - val_loss: 7.6913e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7458e-11 - val_loss: 7.9183e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8583e-11 - val_loss: 7.7906e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7603e-11 - val_loss: 7.9529e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1164e-11 - val_loss: 8.5001e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4619e-11 - val_loss: 8.4252e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2011e-11 - val_loss: 8.0000e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8181e-11 - val_loss: 7.6700e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6517e-11 - val_loss: 7.7313e-11
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6347e-11 - val_loss: 7.4831e-11
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3385e-11 - val_loss: 7.2207e-11
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1104e-11 - val_loss: 7.0412e-11
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0232e-11 - val_loss: 7.0004e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9881e-11 - val_loss: 7.0252e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0068e-11 - val_loss: 7.0435e-11
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0169e-11 - val_loss: 6.9614e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0252e-11 - val_loss: 7.2590e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2969e-11 - val_loss: 7.4926e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4183e-11 - val_loss: 7.4898e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4583e-11 - val_loss: 7.5148e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5374e-11 - val_loss: 7.8031e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8437e-11 - val_loss: 8.1213e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1232e-11 - val_loss: 7.9680e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7504e-11 - val_loss: 7.4447e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3277e-11 - val_loss: 7.1440e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0821e-11 - val_loss: 7.0610e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9652e-11 - val_loss: 6.8539e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7908e-11 - val_loss: 6.7382e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6990e-11 - val_loss: 6.6842e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6681e-11 - val_loss: 6.7659e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8102e-11 - val_loss: 6.8134e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8637e-11 - val_loss: 6.9904e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9054e-11 - val_loss: 6.8021e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7999e-11 - val_loss: 6.9072e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9335e-11 - val_loss: 6.9801e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9155e-11 - val_loss: 6.8808e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8923e-11 - val_loss: 7.1192e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0560e-11 - val_loss: 7.0344e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1666e-11 - val_loss: 7.3924e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3933e-11 - val_loss: 7.3036e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1466e-11 - val_loss: 7.0612e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9657e-11 - val_loss: 6.9172e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9359e-11 - val_loss: 7.0008e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9429e-11 - val_loss: 6.8523e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7816e-11 - val_loss: 6.7912e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7505e-11 - val_loss: 6.7250e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6357e-11 - val_loss: 6.6025e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5565e-11 - val_loss: 6.5839e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5990e-11 - val_loss: 6.6542e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5932e-11 - val_loss: 6.5336e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4500e-11 - val_loss: 6.4517e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3844e-11 - val_loss: 6.4708e-11
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4357e-11 - val_loss: 6.3579e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3991e-11 - val_loss: 6.4866e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5059e-11 - val_loss: 6.6132e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6423e-11 - val_loss: 7.0406e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1913e-11 - val_loss: 7.3398e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2342e-11 - val_loss: 7.0789e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9232e-11 - val_loss: 6.7639e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6678e-11 - val_loss: 6.5980e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5182e-11 - val_loss: 6.5007e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5166e-11 - val_loss: 6.4361e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2838e-11 - val_loss: 6.1610e-11
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1069e-11 - val_loss: 6.1395e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0789e-11 - val_loss: 6.0308e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0281e-11 - val_loss: 6.0663e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1058e-11 - val_loss: 6.1818e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2052e-11 - val_loss: 6.2352e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1979e-11 - val_loss: 6.2809e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2933e-11 - val_loss: 6.3713e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3934e-11 - val_loss: 6.4646e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4555e-11 - val_loss: 6.5639e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4722e-11 - val_loss: 6.3113e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2814e-11 - val_loss: 6.4922e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5616e-11 - val_loss: 6.6746e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6237e-11 - val_loss: 6.6813e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6087e-11 - val_loss: 6.5441e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5055e-11 - val_loss: 6.5268e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4901e-11 - val_loss: 6.4795e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4102e-11 - val_loss: 6.3913e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3437e-11 - val_loss: 6.3217e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2037e-11 - val_loss: 6.1530e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1186e-11 - val_loss: 6.1253e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0636e-11 - val_loss: 5.9736e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9281e-11 - val_loss: 5.8916e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8805e-11 - val_loss: 5.9540e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9544e-11 - val_loss: 5.9443e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8680e-11 - val_loss: 5.8539e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8401e-11 - val_loss: 5.8390e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8616e-11 - val_loss: 6.0311e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1083e-11 - val_loss: 6.2001e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2054e-11 - val_loss: 6.3449e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4897e-11 - val_loss: 6.6584e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5567e-11 - val_loss: 6.5080e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5010e-11 - val_loss: 6.4570e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2735e-11 - val_loss: 6.1685e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1085e-11 - val_loss: 5.9473e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8779e-11 - val_loss: 5.8033e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8029e-11 - val_loss: 5.8449e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8039e-11 - val_loss: 5.8036e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7663e-11 - val_loss: 5.7709e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7607e-11 - val_loss: 5.7319e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7049e-11 - val_loss: 5.6950e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6781e-11 - val_loss: 5.6723e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6488e-11 - val_loss: 5.6864e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7223e-11 - val_loss: 5.7759e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7942e-11 - val_loss: 5.8572e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8678e-11 - val_loss: 5.9266e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8957e-11 - val_loss: 5.9174e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8743e-11 - val_loss: 5.8513e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9513e-11 - val_loss: 6.0751e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0617e-11 - val_loss: 6.0568e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0520e-11 - val_loss: 6.0975e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0852e-11 - val_loss: 6.0783e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0565e-11 - val_loss: 6.0289e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9652e-11 - val_loss: 5.9241e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8867e-11 - val_loss: 5.8498e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8148e-11 - val_loss: 5.7984e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7605e-11 - val_loss: 5.7526e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8375e-11 - val_loss: 5.9154e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8685e-11 - val_loss: 5.8512e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8255e-11 - val_loss: 5.8559e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7797e-11 - val_loss: 5.6623e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6099e-11 - val_loss: 5.6207e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5989e-11 - val_loss: 5.5569e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5430e-11 - val_loss: 5.5775e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5983e-11 - val_loss: 5.5890e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6125e-11 - val_loss: 5.7104e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8612e-11 - val_loss: 6.2490e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2795e-11 - val_loss: 6.2248e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1143e-11 - val_loss: 6.0953e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0317e-11 - val_loss: 6.0157e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9815e-11 - val_loss: 5.8935e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8616e-11 - val_loss: 5.7805e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7347e-11 - val_loss: 5.7331e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7034e-11 - val_loss: 5.6344e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6044e-11 - val_loss: 5.5372e-11
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5256e-11 - val_loss: 5.5184e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5149e-11 - val_loss: 5.5451e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5230e-11 - val_loss: 5.5542e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5627e-11 - val_loss: 5.5934e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5710e-11 - val_loss: 5.6210e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5947e-11 - val_loss: 5.5964e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5639e-11 - val_loss: 5.5731e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5500e-11 - val_loss: 5.5647e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5785e-11 - val_loss: 5.5380e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5128e-11 - val_loss: 5.5129e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6380e-11 - val_loss: 5.8059e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8018e-11 - val_loss: 5.7836e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7032e-11 - val_loss: 5.6450e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6106e-11 - val_loss: 5.5962e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6024e-11 - val_loss: 5.6732e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6447e-11 - val_loss: 5.6247e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5903e-11 - val_loss: 5.5463e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5362e-11 - val_loss: 5.5225e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4585e-11 - val_loss: 5.4160e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3933e-11 - val_loss: 5.3788e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3581e-11 - val_loss: 5.3586e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3358e-11 - val_loss: 5.2620e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2910e-11 - val_loss: 5.2963e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2413e-11 - val_loss: 5.2231e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2078e-11 - val_loss: 5.2216e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1921e-11 - val_loss: 5.1511e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1806e-11 - val_loss: 5.2704e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2714e-11 - val_loss: 5.2580e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2592e-11 - val_loss: 5.3015e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.0530 - val_loss: 0.0111
Epoch 2/512
512/512 - 0s - loss: 0.0238 - val_loss: 0.0069
Epoch 3/512
512/512 - 0s - loss: 0.0146 - val_loss: 0.0020
Epoch 4/512
512/512 - 0s - loss: 0.0088 - val_loss: 0.0056
Epoch 5/512
512/512 - 0s - loss: 0.0075 - val_loss: 0.0067
Epoch 6/512
512/512 - 0s - loss: 0.0061 - val_loss: 0.0064
Epoch 7/512
512/512 - 0s - loss: 0.0049 - val_loss: 0.0070
Epoch 8/512
512/512 - 0s - loss: 0.0046 - val_loss: 0.0070
Epoch 9/512
512/512 - 0s - loss: 0.0041 - val_loss: 0.0067
Epoch 10/512
512/512 - 0s - loss: 0.0035 - val_loss: 0.0068
Epoch 11/512
512/512 - 0s - loss: 0.0034 - val_loss: 0.0066
Epoch 12/512
512/512 - 0s - loss: 0.0030 - val_loss: 0.0063
Epoch 13/512
512/512 - 0s - loss: 0.0027 - val_loss: 0.0064
Epoch 14/512
512/512 - 0s - loss: 0.0026 - val_loss: 0.0061
Epoch 15/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0059
Epoch 16/512
512/512 - 0s - loss: 0.0022 - val_loss: 0.0058
Epoch 17/512
512/512 - 0s - loss: 0.0021 - val_loss: 0.0057
Epoch 18/512
512/512 - 0s - loss: 0.0020 - val_loss: 0.0054
Epoch 19/512
512/512 - 0s - loss: 0.0019 - val_loss: 0.0053
Epoch 20/512
512/512 - 0s - loss: 0.0018 - val_loss: 0.0052
Epoch 21/512
512/512 - 0s - loss: 0.0017 - val_loss: 0.0050
Epoch 22/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0049
Epoch 23/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0048
Epoch 24/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0046
Epoch 25/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0045
Epoch 26/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0043
Epoch 27/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0042
Epoch 28/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0040
Epoch 29/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0039
Epoch 30/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0038
Epoch 31/512
512/512 - 0s - loss: 9.8657e-04 - val_loss: 0.0036
Epoch 32/512
512/512 - 0s - loss: 8.9853e-04 - val_loss: 0.0034
Epoch 33/512
512/512 - 0s - loss: 8.3338e-04 - val_loss: 0.0033
Epoch 34/512
512/512 - 0s - loss: 7.9107e-04 - val_loss: 0.0032
Epoch 35/512
512/512 - 0s - loss: 7.6171e-04 - val_loss: 0.0030
Epoch 36/512
512/512 - 0s - loss: 6.9652e-04 - val_loss: 0.0029
Epoch 37/512
512/512 - 0s - loss: 6.6343e-04 - val_loss: 0.0028
Epoch 38/512
512/512 - 0s - loss: 6.5080e-04 - val_loss: 0.0027
Epoch 39/512
512/512 - 0s - loss: 6.2287e-04 - val_loss: 0.0025
Epoch 40/512
512/512 - 0s - loss: 5.6014e-04 - val_loss: 0.0024
Epoch 41/512
512/512 - 0s - loss: 5.3177e-04 - val_loss: 0.0023
Epoch 42/512
512/512 - 0s - loss: 5.1793e-04 - val_loss: 0.0022
Epoch 43/512
512/512 - 0s - loss: 4.9035e-04 - val_loss: 0.0021
Epoch 44/512
512/512 - 0s - loss: 4.4979e-04 - val_loss: 0.0020
Epoch 45/512
512/512 - 0s - loss: 4.3512e-04 - val_loss: 0.0019
Epoch 46/512
512/512 - 0s - loss: 4.2057e-04 - val_loss: 0.0018
Epoch 47/512
512/512 - 0s - loss: 3.9606e-04 - val_loss: 0.0017
Epoch 48/512
512/512 - 0s - loss: 3.7922e-04 - val_loss: 0.0016
Epoch 49/512
512/512 - 0s - loss: 3.6165e-04 - val_loss: 0.0016
Epoch 50/512
512/512 - 0s - loss: 3.4842e-04 - val_loss: 0.0015
Epoch 51/512
512/512 - 0s - loss: 3.3601e-04 - val_loss: 0.0014
Epoch 52/512
512/512 - 0s - loss: 3.2280e-04 - val_loss: 0.0014
Epoch 53/512
512/512 - 0s - loss: 3.0979e-04 - val_loss: 0.0013
Epoch 54/512
512/512 - 0s - loss: 3.0166e-04 - val_loss: 0.0012
Epoch 55/512
512/512 - 0s - loss: 2.9113e-04 - val_loss: 0.0012
Epoch 56/512
512/512 - 0s - loss: 2.8138e-04 - val_loss: 0.0011
Epoch 57/512
512/512 - 0s - loss: 2.7495e-04 - val_loss: 0.0011
Epoch 58/512
512/512 - 0s - loss: 2.6738e-04 - val_loss: 0.0010
Epoch 59/512
512/512 - 0s - loss: 2.5930e-04 - val_loss: 9.9181e-04
Epoch 60/512
512/512 - 0s - loss: 2.5006e-04 - val_loss: 9.5438e-04
Epoch 61/512
512/512 - 0s - loss: 2.4936e-04 - val_loss: 9.1444e-04
Epoch 62/512
512/512 - 0s - loss: 2.4142e-04 - val_loss: 8.7178e-04
Epoch 63/512
512/512 - 0s - loss: 2.3544e-04 - val_loss: 8.3994e-04
Epoch 64/512
512/512 - 0s - loss: 2.3401e-04 - val_loss: 8.0740e-04
Epoch 65/512
512/512 - 0s - loss: 2.2876e-04 - val_loss: 7.7148e-04
Epoch 66/512
512/512 - 0s - loss: 2.1880e-04 - val_loss: 7.4283e-04
Epoch 67/512
512/512 - 0s - loss: 2.1949e-04 - val_loss: 7.1882e-04
Epoch 68/512
512/512 - 0s - loss: 2.1616e-04 - val_loss: 6.8792e-04
Epoch 69/512
512/512 - 0s - loss: 2.0761e-04 - val_loss: 6.6418e-04
Epoch 70/512
512/512 - 0s - loss: 2.0856e-04 - val_loss: 6.4396e-04
Epoch 71/512
512/512 - 0s - loss: 2.0731e-04 - val_loss: 6.1997e-04
Epoch 72/512
512/512 - 0s - loss: 1.9973e-04 - val_loss: 5.9462e-04
Epoch 73/512
512/512 - 0s - loss: 1.9546e-04 - val_loss: 5.7706e-04
Epoch 74/512
512/512 - 0s - loss: 1.9474e-04 - val_loss: 5.5824e-04
Epoch 75/512
512/512 - 0s - loss: 1.9315e-04 - val_loss: 5.4069e-04
Epoch 76/512
512/512 - 0s - loss: 1.8980e-04 - val_loss: 5.2164e-04
Epoch 77/512
512/512 - 0s - loss: 1.8409e-04 - val_loss: 5.0763e-04
Epoch 78/512
512/512 - 0s - loss: 1.8451e-04 - val_loss: 4.9304e-04
Epoch 79/512
512/512 - 0s - loss: 1.8021e-04 - val_loss: 4.7740e-04
Epoch 80/512
512/512 - 0s - loss: 1.7729e-04 - val_loss: 4.6300e-04
Epoch 81/512
512/512 - 0s - loss: 1.7481e-04 - val_loss: 4.5047e-04
Epoch 82/512
512/512 - 0s - loss: 1.7365e-04 - val_loss: 4.3963e-04
Epoch 83/512
512/512 - 0s - loss: 1.7110e-04 - val_loss: 4.2813e-04
Epoch 84/512
512/512 - 0s - loss: 1.6814e-04 - val_loss: 4.1628e-04
Epoch 85/512
512/512 - 0s - loss: 1.6589e-04 - val_loss: 4.0478e-04
Epoch 86/512
512/512 - 0s - loss: 1.6275e-04 - val_loss: 3.9766e-04
Epoch 87/512
512/512 - 0s - loss: 1.6376e-04 - val_loss: 3.8559e-04
Epoch 88/512
512/512 - 0s - loss: 1.5722e-04 - val_loss: 3.7758e-04
Epoch 89/512
512/512 - 0s - loss: 1.5835e-04 - val_loss: 3.7051e-04
Epoch 90/512
512/512 - 0s - loss: 1.5563e-04 - val_loss: 3.5991e-04
Epoch 91/512
512/512 - 0s - loss: 1.5059e-04 - val_loss: 3.5289e-04
Epoch 92/512
512/512 - 0s - loss: 1.5134e-04 - val_loss: 3.4814e-04
Epoch 93/512
512/512 - 0s - loss: 1.5208e-04 - val_loss: 3.3975e-04
Epoch 94/512
512/512 - 0s - loss: 1.4758e-04 - val_loss: 3.3133e-04
Epoch 95/512
512/512 - 0s - loss: 1.4438e-04 - val_loss: 3.2791e-04
Epoch 96/512
512/512 - 0s - loss: 1.4517e-04 - val_loss: 3.2063e-04
Epoch 97/512
512/512 - 0s - loss: 1.4118e-04 - val_loss: 3.1372e-04
Epoch 98/512
512/512 - 0s - loss: 1.4014e-04 - val_loss: 3.1125e-04
Epoch 99/512
512/512 - 0s - loss: 1.3892e-04 - val_loss: 3.0480e-04
Epoch 100/512
512/512 - 0s - loss: 1.3775e-04 - val_loss: 3.0218e-04
Epoch 101/512
512/512 - 0s - loss: 1.3841e-04 - val_loss: 2.9384e-04
Epoch 102/512
512/512 - 0s - loss: 1.3295e-04 - val_loss: 2.9026e-04
Epoch 103/512
512/512 - 0s - loss: 1.3224e-04 - val_loss: 2.8758e-04
Epoch 104/512
512/512 - 0s - loss: 1.3259e-04 - val_loss: 2.8288e-04
Epoch 105/512
512/512 - 0s - loss: 1.3218e-04 - val_loss: 2.7676e-04
Epoch 106/512
512/512 - 0s - loss: 1.2716e-04 - val_loss: 2.7335e-04
Epoch 107/512
512/512 - 0s - loss: 1.2826e-04 - val_loss: 2.7178e-04
Epoch 108/512
512/512 - 0s - loss: 1.2771e-04 - val_loss: 2.6721e-04
Epoch 109/512
512/512 - 0s - loss: 1.2429e-04 - val_loss: 2.6217e-04
Epoch 110/512
512/512 - 0s - loss: 1.2331e-04 - val_loss: 2.5980e-04
Epoch 111/512
512/512 - 0s - loss: 1.2374e-04 - val_loss: 2.5653e-04
Epoch 112/512
512/512 - 0s - loss: 1.2182e-04 - val_loss: 2.5332e-04
Epoch 113/512
512/512 - 0s - loss: 1.2237e-04 - val_loss: 2.5120e-04
Epoch 114/512
512/512 - 0s - loss: 1.1980e-04 - val_loss: 2.4652e-04
Epoch 115/512
512/512 - 0s - loss: 1.1786e-04 - val_loss: 2.4426e-04
Epoch 116/512
512/512 - 0s - loss: 1.1980e-04 - val_loss: 2.4325e-04
Epoch 117/512
512/512 - 0s - loss: 1.1890e-04 - val_loss: 2.3700e-04
Epoch 118/512
512/512 - 0s - loss: 1.1323e-04 - val_loss: 2.3423e-04
Epoch 119/512
512/512 - 0s - loss: 1.1570e-04 - val_loss: 2.3471e-04
Epoch 120/512
512/512 - 0s - loss: 1.1924e-04 - val_loss: 2.2978e-04
Epoch 121/512
512/512 - 0s - loss: 1.1080e-04 - val_loss: 2.2390e-04
Epoch 122/512
512/512 - 0s - loss: 1.1100e-04 - val_loss: 2.2727e-04
Epoch 123/512
512/512 - 0s - loss: 1.1718e-04 - val_loss: 2.2300e-04
Epoch 124/512
512/512 - 0s - loss: 1.0985e-04 - val_loss: 2.1412e-04
Epoch 125/512
512/512 - 0s - loss: 1.0818e-04 - val_loss: 2.1829e-04
Epoch 126/512
512/512 - 0s - loss: 1.1293e-04 - val_loss: 2.1389e-04
Epoch 127/512
512/512 - 0s - loss: 1.1005e-04 - val_loss: 2.1056e-04
Epoch 128/512
512/512 - 0s - loss: 1.0744e-04 - val_loss: 2.0704e-04
Epoch 129/512
512/512 - 0s - loss: 1.0845e-04 - val_loss: 2.0633e-04
Epoch 130/512
512/512 - 0s - loss: 1.0867e-04 - val_loss: 2.0145e-04
Epoch 131/512
512/512 - 0s - loss: 1.0569e-04 - val_loss: 1.9762e-04
Epoch 132/512
512/512 - 0s - loss: 1.0563e-04 - val_loss: 1.9787e-04
Epoch 133/512
512/512 - 0s - loss: 1.0767e-04 - val_loss: 1.9344e-04
Epoch 134/512
512/512 - 0s - loss: 1.0456e-04 - val_loss: 1.8915e-04
Epoch 135/512
512/512 - 0s - loss: 1.0322e-04 - val_loss: 1.8963e-04
Epoch 136/512
512/512 - 0s - loss: 1.0613e-04 - val_loss: 1.8433e-04
Epoch 137/512
512/512 - 0s - loss: 1.0230e-04 - val_loss: 1.8102e-04
Epoch 138/512
512/512 - 0s - loss: 1.0252e-04 - val_loss: 1.7906e-04
Epoch 139/512
512/512 - 0s - loss: 1.0188e-04 - val_loss: 1.7656e-04
Epoch 140/512
512/512 - 0s - loss: 1.0174e-04 - val_loss: 1.7416e-04
Epoch 141/512
512/512 - 0s - loss: 1.0214e-04 - val_loss: 1.7021e-04
Epoch 142/512
512/512 - 0s - loss: 1.0022e-04 - val_loss: 1.6815e-04
Epoch 143/512
512/512 - 0s - loss: 1.0039e-04 - val_loss: 1.6397e-04
Epoch 144/512
512/512 - 0s - loss: 9.8814e-05 - val_loss: 1.6248e-04
Epoch 145/512
512/512 - 0s - loss: 1.0030e-04 - val_loss: 1.5871e-04
Epoch 146/512
512/512 - 0s - loss: 9.7755e-05 - val_loss: 1.5521e-04
Epoch 147/512
512/512 - 0s - loss: 9.8095e-05 - val_loss: 1.5181e-04
Epoch 148/512
512/512 - 0s - loss: 9.8012e-05 - val_loss: 1.4963e-04
Epoch 149/512
512/512 - 0s - loss: 9.6810e-05 - val_loss: 1.4520e-04
Epoch 150/512
512/512 - 0s - loss: 9.5995e-05 - val_loss: 1.4521e-04
Epoch 151/512
512/512 - 0s - loss: 9.6569e-05 - val_loss: 1.4051e-04
Epoch 152/512
512/512 - 0s - loss: 9.5043e-05 - val_loss: 1.3615e-04
Epoch 153/512
512/512 - 0s - loss: 9.5459e-05 - val_loss: 1.3582e-04
Epoch 154/512
512/512 - 0s - loss: 9.5900e-05 - val_loss: 1.3015e-04
Epoch 155/512
512/512 - 0s - loss: 9.2250e-05 - val_loss: 1.2811e-04
Epoch 156/512
512/512 - 0s - loss: 9.3219e-05 - val_loss: 1.2664e-04
Epoch 157/512
512/512 - 0s - loss: 9.4983e-05 - val_loss: 1.2214e-04
Epoch 158/512
512/512 - 0s - loss: 9.2376e-05 - val_loss: 1.1889e-04
Epoch 159/512
512/512 - 0s - loss: 9.0772e-05 - val_loss: 1.1565e-04
Epoch 160/512
512/512 - 0s - loss: 9.1498e-05 - val_loss: 1.1351e-04
Epoch 161/512
512/512 - 0s - loss: 9.1831e-05 - val_loss: 1.0984e-04
Epoch 162/512
512/512 - 0s - loss: 8.9180e-05 - val_loss: 1.0653e-04
Epoch 163/512
512/512 - 0s - loss: 9.0340e-05 - val_loss: 1.0422e-04
Epoch 164/512
512/512 - 0s - loss: 9.0527e-05 - val_loss: 1.0096e-04
Epoch 165/512
512/512 - 0s - loss: 8.8114e-05 - val_loss: 9.7380e-05
Epoch 166/512
512/512 - 0s - loss: 8.8511e-05 - val_loss: 9.5945e-05
Epoch 167/512
512/512 - 0s - loss: 8.8139e-05 - val_loss: 9.1706e-05
Epoch 168/512
512/512 - 0s - loss: 8.6688e-05 - val_loss: 8.9390e-05
Epoch 169/512
512/512 - 0s - loss: 8.7902e-05 - val_loss: 8.6573e-05
Epoch 170/512
512/512 - 0s - loss: 8.6680e-05 - val_loss: 8.2682e-05
Epoch 171/512
512/512 - 0s - loss: 8.5057e-05 - val_loss: 7.9766e-05
Epoch 172/512
512/512 - 0s - loss: 8.5355e-05 - val_loss: 7.6994e-05
Epoch 173/512
512/512 - 0s - loss: 8.4645e-05 - val_loss: 7.3609e-05
Epoch 174/512
512/512 - 0s - loss: 8.4506e-05 - val_loss: 7.0951e-05
Epoch 175/512
512/512 - 0s - loss: 8.3656e-05 - val_loss: 6.8002e-05
Epoch 176/512
512/512 - 0s - loss: 8.2403e-05 - val_loss: 6.5882e-05
Epoch 177/512
512/512 - 0s - loss: 8.3649e-05 - val_loss: 6.3271e-05
Epoch 178/512
512/512 - 0s - loss: 8.2248e-05 - val_loss: 5.9019e-05
Epoch 179/512
512/512 - 0s - loss: 8.1101e-05 - val_loss: 5.7321e-05
Epoch 180/512
512/512 - 0s - loss: 8.1275e-05 - val_loss: 5.4728e-05
Epoch 181/512
512/512 - 0s - loss: 8.0137e-05 - val_loss: 5.1676e-05
Epoch 182/512
512/512 - 0s - loss: 7.9359e-05 - val_loss: 4.9341e-05
Epoch 183/512
512/512 - 0s - loss: 7.9230e-05 - val_loss: 4.7222e-05
Epoch 184/512
512/512 - 0s - loss: 8.0142e-05 - val_loss: 4.4401e-05
Epoch 185/512
512/512 - 0s - loss: 7.6990e-05 - val_loss: 4.1704e-05
Epoch 186/512
512/512 - 0s - loss: 7.6842e-05 - val_loss: 4.0003e-05
Epoch 187/512
512/512 - 0s - loss: 7.7355e-05 - val_loss: 3.7754e-05
Epoch 188/512
512/512 - 0s - loss: 7.5975e-05 - val_loss: 3.5056e-05
Epoch 189/512
512/512 - 0s - loss: 7.5070e-05 - val_loss: 3.3546e-05
Epoch 190/512
512/512 - 0s - loss: 7.5863e-05 - val_loss: 3.1407e-05
Epoch 191/512
512/512 - 0s - loss: 7.3941e-05 - val_loss: 2.9247e-05
Epoch 192/512
512/512 - 0s - loss: 7.3454e-05 - val_loss: 2.7681e-05
Epoch 193/512
512/512 - 0s - loss: 7.2731e-05 - val_loss: 2.5980e-05
Epoch 194/512
512/512 - 0s - loss: 7.2369e-05 - val_loss: 2.4042e-05
Epoch 195/512
512/512 - 0s - loss: 7.0930e-05 - val_loss: 2.3002e-05
Epoch 196/512
512/512 - 0s - loss: 7.1318e-05 - val_loss: 2.1723e-05
Epoch 197/512
512/512 - 0s - loss: 7.0747e-05 - val_loss: 2.0380e-05
Epoch 198/512
512/512 - 0s - loss: 6.9117e-05 - val_loss: 1.9275e-05
Epoch 199/512
512/512 - 0s - loss: 6.8338e-05 - val_loss: 1.8481e-05
Epoch 200/512
512/512 - 0s - loss: 6.7657e-05 - val_loss: 1.7740e-05
Epoch 201/512
512/512 - 0s - loss: 6.7997e-05 - val_loss: 1.7171e-05
Epoch 202/512
512/512 - 0s - loss: 6.6166e-05 - val_loss: 1.6770e-05
Epoch 203/512
512/512 - 0s - loss: 6.5083e-05 - val_loss: 1.6575e-05
Epoch 204/512
512/512 - 0s - loss: 6.4939e-05 - val_loss: 1.6552e-05
Epoch 205/512
512/512 - 0s - loss: 6.4634e-05 - val_loss: 1.6706e-05
Epoch 206/512
512/512 - 0s - loss: 6.3016e-05 - val_loss: 1.7325e-05
Epoch 207/512
512/512 - 0s - loss: 6.1719e-05 - val_loss: 1.7606e-05
Epoch 208/512
512/512 - 0s - loss: 6.3073e-05 - val_loss: 1.8688e-05
Epoch 209/512
512/512 - 0s - loss: 6.0125e-05 - val_loss: 2.0128e-05
Epoch 210/512
512/512 - 0s - loss: 5.9175e-05 - val_loss: 2.0978e-05
Epoch 211/512
512/512 - 0s - loss: 5.9831e-05 - val_loss: 2.2262e-05
Epoch 212/512
512/512 - 0s - loss: 5.8134e-05 - val_loss: 2.5169e-05
Epoch 213/512
512/512 - 0s - loss: 5.6318e-05 - val_loss: 2.6542e-05
Epoch 214/512
512/512 - 0s - loss: 5.6657e-05 - val_loss: 2.8882e-05
Epoch 215/512
512/512 - 0s - loss: 5.5199e-05 - val_loss: 3.2197e-05
Epoch 216/512
512/512 - 0s - loss: 5.4020e-05 - val_loss: 3.5032e-05
Epoch 217/512
512/512 - 0s - loss: 5.3634e-05 - val_loss: 3.7775e-05
Epoch 218/512
512/512 - 0s - loss: 5.2159e-05 - val_loss: 4.2473e-05
Epoch 219/512
512/512 - 0s - loss: 5.0651e-05 - val_loss: 4.5417e-05
Epoch 220/512
512/512 - 0s - loss: 5.0184e-05 - val_loss: 5.0088e-05
Epoch 221/512
512/512 - 0s - loss: 4.8814e-05 - val_loss: 5.4273e-05
Epoch 222/512
512/512 - 0s - loss: 4.7983e-05 - val_loss: 5.9183e-05
Epoch 223/512
512/512 - 0s - loss: 4.6983e-05 - val_loss: 6.4395e-05
Epoch 224/512
512/512 - 0s - loss: 4.5362e-05 - val_loss: 7.0747e-05
Epoch 225/512
512/512 - 0s - loss: 4.3801e-05 - val_loss: 7.5810e-05
Epoch 226/512
512/512 - 0s - loss: 4.3475e-05 - val_loss: 8.2374e-05
Epoch 227/512
512/512 - 0s - loss: 4.1727e-05 - val_loss: 8.9208e-05
Epoch 228/512
512/512 - 0s - loss: 4.0352e-05 - val_loss: 9.5668e-05
Epoch 229/512
512/512 - 0s - loss: 3.9586e-05 - val_loss: 1.0243e-04
Epoch 230/512
512/512 - 0s - loss: 3.8004e-05 - val_loss: 1.1050e-04
Epoch 231/512
512/512 - 0s - loss: 3.6252e-05 - val_loss: 1.1760e-04
Epoch 232/512
512/512 - 0s - loss: 3.4906e-05 - val_loss: 1.2495e-04
Epoch 233/512
512/512 - 0s - loss: 3.3844e-05 - val_loss: 1.3168e-04
Epoch 234/512
512/512 - 0s - loss: 3.2544e-05 - val_loss: 1.4104e-04
Epoch 235/512
512/512 - 0s - loss: 3.0621e-05 - val_loss: 1.4956e-04
Epoch 236/512
512/512 - 0s - loss: 2.8962e-05 - val_loss: 1.5575e-04
Epoch 237/512
512/512 - 0s - loss: 2.8236e-05 - val_loss: 1.6241e-04
Epoch 238/512
512/512 - 0s - loss: 2.6416e-05 - val_loss: 1.7195e-04
Epoch 239/512
512/512 - 0s - loss: 2.4588e-05 - val_loss: 1.7791e-04
Epoch 240/512
512/512 - 0s - loss: 2.3348e-05 - val_loss: 1.8306e-04
Epoch 241/512
512/512 - 0s - loss: 2.1969e-05 - val_loss: 1.8965e-04
Epoch 242/512
512/512 - 0s - loss: 2.0299e-05 - val_loss: 1.9399e-04
Epoch 243/512
512/512 - 0s - loss: 1.9286e-05 - val_loss: 1.9710e-04
Epoch 244/512
512/512 - 0s - loss: 1.7932e-05 - val_loss: 2.0137e-04
Epoch 245/512
512/512 - 0s - loss: 1.6400e-05 - val_loss: 2.0429e-04
Epoch 246/512
512/512 - 0s - loss: 1.5247e-05 - val_loss: 2.0525e-04
Epoch 247/512
512/512 - 0s - loss: 1.4233e-05 - val_loss: 2.0547e-04
Epoch 248/512
512/512 - 0s - loss: 1.3201e-05 - val_loss: 2.0621e-04
Epoch 249/512
512/512 - 0s - loss: 1.2021e-05 - val_loss: 2.0626e-04
Epoch 250/512
512/512 - 0s - loss: 1.1114e-05 - val_loss: 2.0407e-04
Epoch 251/512
512/512 - 0s - loss: 1.0407e-05 - val_loss: 2.0182e-04
Epoch 252/512
512/512 - 0s - loss: 9.6180e-06 - val_loss: 2.0002e-04
Epoch 253/512
512/512 - 0s - loss: 8.8137e-06 - val_loss: 1.9730e-04
Epoch 254/512
512/512 - 0s - loss: 8.1417e-06 - val_loss: 1.9363e-04
Epoch 255/512
512/512 - 0s - loss: 7.6067e-06 - val_loss: 1.8942e-04
Epoch 256/512
512/512 - 0s - loss: 7.0734e-06 - val_loss: 1.8622e-04
Epoch 257/512
512/512 - 0s - loss: 6.4977e-06 - val_loss: 1.8230e-04
Epoch 258/512
512/512 - 0s - loss: 6.0387e-06 - val_loss: 1.7745e-04
Epoch 259/512
512/512 - 0s - loss: 5.6592e-06 - val_loss: 1.7322e-04
Epoch 260/512
512/512 - 0s - loss: 5.2518e-06 - val_loss: 1.6907e-04
Epoch 261/512
512/512 - 0s - loss: 4.8718e-06 - val_loss: 1.6475e-04
Epoch 262/512
512/512 - 0s - loss: 4.5385e-06 - val_loss: 1.6013e-04
Epoch 263/512
512/512 - 0s - loss: 4.2516e-06 - val_loss: 1.5558e-04
Epoch 264/512
512/512 - 0s - loss: 3.9953e-06 - val_loss: 1.5096e-04
Epoch 265/512
512/512 - 0s - loss: 3.7361e-06 - val_loss: 1.4694e-04
Epoch 266/512
512/512 - 0s - loss: 3.4774e-06 - val_loss: 1.4266e-04
Epoch 267/512
512/512 - 0s - loss: 3.2702e-06 - val_loss: 1.3829e-04
Epoch 268/512
512/512 - 0s - loss: 3.0922e-06 - val_loss: 1.3413e-04
Epoch 269/512
512/512 - 0s - loss: 2.9172e-06 - val_loss: 1.3021e-04
Epoch 270/512
512/512 - 0s - loss: 2.7366e-06 - val_loss: 1.2643e-04
Epoch 271/512
512/512 - 0s - loss: 2.5739e-06 - val_loss: 1.2265e-04
Epoch 272/512
512/512 - 0s - loss: 2.4357e-06 - val_loss: 1.1888e-04
Epoch 273/512
512/512 - 0s - loss: 2.3166e-06 - val_loss: 1.1533e-04
Epoch 274/512
512/512 - 0s - loss: 2.1935e-06 - val_loss: 1.1194e-04
Epoch 275/512
512/512 - 0s - loss: 2.0773e-06 - val_loss: 1.0872e-04
Epoch 276/512
512/512 - 0s - loss: 1.9662e-06 - val_loss: 1.0555e-04
Epoch 277/512
512/512 - 0s - loss: 1.8689e-06 - val_loss: 1.0240e-04
Epoch 278/512
512/512 - 0s - loss: 1.7891e-06 - val_loss: 9.9372e-05
Epoch 279/512
512/512 - 0s - loss: 1.7087e-06 - val_loss: 9.6613e-05
Epoch 280/512
512/512 - 0s - loss: 1.6226e-06 - val_loss: 9.4036e-05
Epoch 281/512
512/512 - 0s - loss: 1.5464e-06 - val_loss: 9.1268e-05
Epoch 282/512
512/512 - 0s - loss: 1.4875e-06 - val_loss: 8.8634e-05
Epoch 283/512
512/512 - 0s - loss: 1.4272e-06 - val_loss: 8.6302e-05
Epoch 284/512
512/512 - 0s - loss: 1.3640e-06 - val_loss: 8.4054e-05
Epoch 285/512
512/512 - 0s - loss: 1.3056e-06 - val_loss: 8.1844e-05
Epoch 286/512
512/512 - 0s - loss: 1.2548e-06 - val_loss: 7.9640e-05
Epoch 287/512
512/512 - 0s - loss: 1.2114e-06 - val_loss: 7.7448e-05
Epoch 288/512
512/512 - 0s - loss: 1.1685e-06 - val_loss: 7.5550e-05
Epoch 289/512
512/512 - 0s - loss: 1.1240e-06 - val_loss: 7.3736e-05
Epoch 290/512
512/512 - 0s - loss: 1.0832e-06 - val_loss: 7.1894e-05
Epoch 291/512
512/512 - 0s - loss: 1.0459e-06 - val_loss: 7.0173e-05
Epoch 292/512
512/512 - 0s - loss: 1.0107e-06 - val_loss: 6.8503e-05
Epoch 293/512
512/512 - 0s - loss: 9.7837e-07 - val_loss: 6.6857e-05
Epoch 294/512
512/512 - 0s - loss: 9.4756e-07 - val_loss: 6.5338e-05
Epoch 295/512
512/512 - 0s - loss: 9.1847e-07 - val_loss: 6.3845e-05
Epoch 296/512
512/512 - 0s - loss: 8.9125e-07 - val_loss: 6.2439e-05
Epoch 297/512
512/512 - 0s - loss: 8.6490e-07 - val_loss: 6.1150e-05
Epoch 298/512
512/512 - 0s - loss: 8.3988e-07 - val_loss: 5.9814e-05
Epoch 299/512
512/512 - 0s - loss: 8.1669e-07 - val_loss: 5.8562e-05
Epoch 300/512
512/512 - 0s - loss: 7.9454e-07 - val_loss: 5.7399e-05
Epoch 301/512
512/512 - 0s - loss: 7.7344e-07 - val_loss: 5.6290e-05
Epoch 302/512
512/512 - 0s - loss: 7.5364e-07 - val_loss: 5.5181e-05
Epoch 303/512
512/512 - 0s - loss: 7.3483e-07 - val_loss: 5.4090e-05
Epoch 304/512
512/512 - 0s - loss: 7.1694e-07 - val_loss: 5.3049e-05
Epoch 305/512
512/512 - 0s - loss: 6.9990e-07 - val_loss: 5.2058e-05
Epoch 306/512
512/512 - 0s - loss: 6.8365e-07 - val_loss: 5.1095e-05
Epoch 307/512
512/512 - 0s - loss: 6.6817e-07 - val_loss: 5.0196e-05
Epoch 308/512
512/512 - 0s - loss: 6.5339e-07 - val_loss: 4.9260e-05
Epoch 309/512
512/512 - 0s - loss: 6.3933e-07 - val_loss: 4.8382e-05
Epoch 310/512
512/512 - 0s - loss: 6.2576e-07 - val_loss: 4.7600e-05
Epoch 311/512
512/512 - 0s - loss: 6.1283e-07 - val_loss: 4.6807e-05
Epoch 312/512
512/512 - 0s - loss: 6.0046e-07 - val_loss: 4.6009e-05
Epoch 313/512
512/512 - 0s - loss: 5.8859e-07 - val_loss: 4.5263e-05
Epoch 314/512
512/512 - 0s - loss: 5.7722e-07 - val_loss: 4.4545e-05
Epoch 315/512
512/512 - 0s - loss: 5.6627e-07 - val_loss: 4.3829e-05
Epoch 316/512
512/512 - 0s - loss: 5.5576e-07 - val_loss: 4.3151e-05
Epoch 317/512
512/512 - 0s - loss: 5.4567e-07 - val_loss: 4.2497e-05
Epoch 318/512
512/512 - 0s - loss: 5.3595e-07 - val_loss: 4.1846e-05
Epoch 319/512
512/512 - 0s - loss: 5.2659e-07 - val_loss: 4.1246e-05
Epoch 320/512
512/512 - 0s - loss: 5.1758e-07 - val_loss: 4.0640e-05
Epoch 321/512
512/512 - 0s - loss: 5.0888e-07 - val_loss: 4.0067e-05
Epoch 322/512
512/512 - 0s - loss: 5.0050e-07 - val_loss: 3.9503e-05
Epoch 323/512
512/512 - 0s - loss: 4.9240e-07 - val_loss: 3.8961e-05
Epoch 324/512
512/512 - 0s - loss: 4.8458e-07 - val_loss: 3.8458e-05
Epoch 325/512
512/512 - 0s - loss: 4.7704e-07 - val_loss: 3.7911e-05
Epoch 326/512
512/512 - 0s - loss: 4.6972e-07 - val_loss: 3.7422e-05
Epoch 327/512
512/512 - 0s - loss: 4.6266e-07 - val_loss: 3.6949e-05
Epoch 328/512
512/512 - 0s - loss: 4.5582e-07 - val_loss: 3.6471e-05
Epoch 329/512
512/512 - 0s - loss: 4.4919e-07 - val_loss: 3.6014e-05
Epoch 330/512
512/512 - 0s - loss: 4.4278e-07 - val_loss: 3.5575e-05
Epoch 331/512
512/512 - 0s - loss: 4.3655e-07 - val_loss: 3.5146e-05
Epoch 332/512
512/512 - 0s - loss: 4.3052e-07 - val_loss: 3.4710e-05
Epoch 333/512
512/512 - 0s - loss: 4.2467e-07 - val_loss: 3.4311e-05
Epoch 334/512
512/512 - 0s - loss: 4.1899e-07 - val_loss: 3.3923e-05
Epoch 335/512
512/512 - 0s - loss: 4.1348e-07 - val_loss: 3.3524e-05
Epoch 336/512
512/512 - 0s - loss: 4.0811e-07 - val_loss: 3.3162e-05
Epoch 337/512
512/512 - 0s - loss: 4.0290e-07 - val_loss: 3.2769e-05
Epoch 338/512
512/512 - 0s - loss: 3.9783e-07 - val_loss: 3.2413e-05
Epoch 339/512
512/512 - 0s - loss: 3.9290e-07 - val_loss: 3.2071e-05
Epoch 340/512
512/512 - 0s - loss: 3.8811e-07 - val_loss: 3.1723e-05
Epoch 341/512
512/512 - 0s - loss: 3.8344e-07 - val_loss: 3.1379e-05
Epoch 342/512
512/512 - 0s - loss: 3.7890e-07 - val_loss: 3.1066e-05
Epoch 343/512
512/512 - 0s - loss: 3.7447e-07 - val_loss: 3.0741e-05
Epoch 344/512
512/512 - 0s - loss: 3.7016e-07 - val_loss: 3.0427e-05
Epoch 345/512
512/512 - 0s - loss: 3.6595e-07 - val_loss: 3.0129e-05
Epoch 346/512
512/512 - 0s - loss: 3.6186e-07 - val_loss: 2.9825e-05
Epoch 347/512
512/512 - 0s - loss: 3.5786e-07 - val_loss: 2.9531e-05
Epoch 348/512
512/512 - 0s - loss: 3.5396e-07 - val_loss: 2.9248e-05
Epoch 349/512
512/512 - 0s - loss: 3.5015e-07 - val_loss: 2.8970e-05
Epoch 350/512
512/512 - 0s - loss: 3.4643e-07 - val_loss: 2.8706e-05
Epoch 351/512
512/512 - 0s - loss: 3.4280e-07 - val_loss: 2.8433e-05
Epoch 352/512
512/512 - 0s - loss: 3.3926e-07 - val_loss: 2.8168e-05
Epoch 353/512
512/512 - 0s - loss: 3.3580e-07 - val_loss: 2.7912e-05
Epoch 354/512
512/512 - 0s - loss: 3.3241e-07 - val_loss: 2.7660e-05
Epoch 355/512
512/512 - 0s - loss: 3.2910e-07 - val_loss: 2.7414e-05
Epoch 356/512
512/512 - 0s - loss: 3.2586e-07 - val_loss: 2.7178e-05
Epoch 357/512
512/512 - 0s - loss: 3.2270e-07 - val_loss: 2.6942e-05
Epoch 358/512
512/512 - 0s - loss: 3.1960e-07 - val_loss: 2.6708e-05
Epoch 359/512
512/512 - 0s - loss: 3.1657e-07 - val_loss: 2.6479e-05
Epoch 360/512
512/512 - 0s - loss: 3.1360e-07 - val_loss: 2.6263e-05
Epoch 361/512
512/512 - 0s - loss: 3.1070e-07 - val_loss: 2.6044e-05
Epoch 362/512
512/512 - 0s - loss: 3.0786e-07 - val_loss: 2.5829e-05
Epoch 363/512
512/512 - 0s - loss: 3.0507e-07 - val_loss: 2.5616e-05
Epoch 364/512
512/512 - 0s - loss: 3.0234e-07 - val_loss: 2.5415e-05
Epoch 365/512
512/512 - 0s - loss: 2.9967e-07 - val_loss: 2.5207e-05
Epoch 366/512
512/512 - 0s - loss: 2.9704e-07 - val_loss: 2.5011e-05
Epoch 367/512
512/512 - 0s - loss: 2.9447e-07 - val_loss: 2.4819e-05
Epoch 368/512
512/512 - 0s - loss: 2.9195e-07 - val_loss: 2.4626e-05
Epoch 369/512
512/512 - 0s - loss: 2.8948e-07 - val_loss: 2.4433e-05
Epoch 370/512
512/512 - 0s - loss: 2.8706e-07 - val_loss: 2.4250e-05
Epoch 371/512
512/512 - 0s - loss: 2.8468e-07 - val_loss: 2.4066e-05
Epoch 372/512
512/512 - 0s - loss: 2.8234e-07 - val_loss: 2.3891e-05
Epoch 373/512
512/512 - 0s - loss: 2.8005e-07 - val_loss: 2.3715e-05
Epoch 374/512
512/512 - 0s - loss: 2.7780e-07 - val_loss: 2.3540e-05
Epoch 375/512
512/512 - 0s - loss: 2.7559e-07 - val_loss: 2.3374e-05
Epoch 376/512
512/512 - 0s - loss: 2.7343e-07 - val_loss: 2.3203e-05
Epoch 377/512
512/512 - 0s - loss: 2.7129e-07 - val_loss: 2.3039e-05
Epoch 378/512
512/512 - 0s - loss: 2.6920e-07 - val_loss: 2.2874e-05
Epoch 379/512
512/512 - 0s - loss: 2.6715e-07 - val_loss: 2.2716e-05
Epoch 380/512
512/512 - 0s - loss: 2.6513e-07 - val_loss: 2.2562e-05
Epoch 381/512
512/512 - 0s - loss: 2.6314e-07 - val_loss: 2.2408e-05
Epoch 382/512
512/512 - 0s - loss: 2.6119e-07 - val_loss: 2.2256e-05
Epoch 383/512
512/512 - 0s - loss: 2.5927e-07 - val_loss: 2.2110e-05
Epoch 384/512
512/512 - 0s - loss: 2.5738e-07 - val_loss: 2.1958e-05
Epoch 385/512
512/512 - 0s - loss: 2.5553e-07 - val_loss: 2.1817e-05
Epoch 386/512
512/512 - 0s - loss: 2.5370e-07 - val_loss: 2.1672e-05
Epoch 387/512
512/512 - 0s - loss: 2.5190e-07 - val_loss: 2.1534e-05
Epoch 388/512
512/512 - 0s - loss: 2.5014e-07 - val_loss: 2.1396e-05
Epoch 389/512
512/512 - 0s - loss: 2.4840e-07 - val_loss: 2.1260e-05
Epoch 390/512
512/512 - 0s - loss: 2.4669e-07 - val_loss: 2.1126e-05
Epoch 391/512
512/512 - 0s - loss: 2.4501e-07 - val_loss: 2.0994e-05
Epoch 392/512
512/512 - 0s - loss: 2.4335e-07 - val_loss: 2.0861e-05
Epoch 393/512
512/512 - 0s - loss: 2.4172e-07 - val_loss: 2.0735e-05
Epoch 394/512
512/512 - 0s - loss: 2.4011e-07 - val_loss: 2.0607e-05
Epoch 395/512
512/512 - 0s - loss: 2.3853e-07 - val_loss: 2.0481e-05
Epoch 396/512
512/512 - 0s - loss: 2.3697e-07 - val_loss: 2.0357e-05
Epoch 397/512
512/512 - 0s - loss: 2.3543e-07 - val_loss: 2.0237e-05
Epoch 398/512
512/512 - 0s - loss: 2.3392e-07 - val_loss: 2.0117e-05
Epoch 399/512
512/512 - 0s - loss: 2.3243e-07 - val_loss: 1.9999e-05
Epoch 400/512
512/512 - 0s - loss: 2.3096e-07 - val_loss: 1.9883e-05
Epoch 401/512
512/512 - 0s - loss: 2.2951e-07 - val_loss: 1.9769e-05
Epoch 402/512
512/512 - 0s - loss: 2.2809e-07 - val_loss: 1.9655e-05
Epoch 403/512
512/512 - 0s - loss: 2.2668e-07 - val_loss: 1.9543e-05
Epoch 404/512
512/512 - 0s - loss: 2.2529e-07 - val_loss: 1.9432e-05
Epoch 405/512
512/512 - 0s - loss: 2.2393e-07 - val_loss: 1.9323e-05
Epoch 406/512
512/512 - 0s - loss: 2.2258e-07 - val_loss: 1.9215e-05
Epoch 407/512
512/512 - 0s - loss: 2.2125e-07 - val_loss: 1.9109e-05
Epoch 408/512
512/512 - 0s - loss: 2.1994e-07 - val_loss: 1.9004e-05
Epoch 409/512
512/512 - 0s - loss: 2.1864e-07 - val_loss: 1.8902e-05
Epoch 410/512
512/512 - 0s - loss: 2.1737e-07 - val_loss: 1.8798e-05
Epoch 411/512
512/512 - 0s - loss: 2.1611e-07 - val_loss: 1.8697e-05
Epoch 412/512
512/512 - 0s - loss: 2.1487e-07 - val_loss: 1.8599e-05
Epoch 413/512
512/512 - 0s - loss: 2.1364e-07 - val_loss: 1.8499e-05
Epoch 414/512
512/512 - 0s - loss: 2.1243e-07 - val_loss: 1.8402e-05
Epoch 415/512
512/512 - 0s - loss: 2.1124e-07 - val_loss: 1.8306e-05
Epoch 416/512
512/512 - 0s - loss: 2.1006e-07 - val_loss: 1.8209e-05
Epoch 417/512
512/512 - 0s - loss: 2.0890e-07 - val_loss: 1.8116e-05
Epoch 418/512
512/512 - 0s - loss: 2.0775e-07 - val_loss: 1.8026e-05
Epoch 419/512
512/512 - 0s - loss: 2.0662e-07 - val_loss: 1.7934e-05
Epoch 420/512
512/512 - 0s - loss: 2.0550e-07 - val_loss: 1.7844e-05
Epoch 421/512
512/512 - 0s - loss: 2.0440e-07 - val_loss: 1.7754e-05
Epoch 422/512
512/512 - 0s - loss: 2.0331e-07 - val_loss: 1.7664e-05
Epoch 423/512
512/512 - 0s - loss: 2.0223e-07 - val_loss: 1.7577e-05
Epoch 424/512
512/512 - 0s - loss: 2.0116e-07 - val_loss: 1.7489e-05
Epoch 425/512
512/512 - 0s - loss: 2.0011e-07 - val_loss: 1.7405e-05
Epoch 426/512
512/512 - 0s - loss: 1.9907e-07 - val_loss: 1.7320e-05
Epoch 427/512
512/512 - 0s - loss: 1.9805e-07 - val_loss: 1.7238e-05
Epoch 428/512
512/512 - 0s - loss: 1.9703e-07 - val_loss: 1.7155e-05
Epoch 429/512
512/512 - 0s - loss: 1.9603e-07 - val_loss: 1.7073e-05
Epoch 430/512
512/512 - 0s - loss: 1.9504e-07 - val_loss: 1.6992e-05
Epoch 431/512
512/512 - 0s - loss: 1.9407e-07 - val_loss: 1.6912e-05
Epoch 432/512
512/512 - 0s - loss: 1.9310e-07 - val_loss: 1.6835e-05
Epoch 433/512
512/512 - 0s - loss: 1.9214e-07 - val_loss: 1.6757e-05
Epoch 434/512
512/512 - 0s - loss: 1.9120e-07 - val_loss: 1.6680e-05
Epoch 435/512
512/512 - 0s - loss: 1.9026e-07 - val_loss: 1.6604e-05
Epoch 436/512
512/512 - 0s - loss: 1.8934e-07 - val_loss: 1.6527e-05
Epoch 437/512
512/512 - 0s - loss: 1.8843e-07 - val_loss: 1.6452e-05
Epoch 438/512
512/512 - 0s - loss: 1.8752e-07 - val_loss: 1.6376e-05
Epoch 439/512
512/512 - 0s - loss: 1.8663e-07 - val_loss: 1.6305e-05
Epoch 440/512
512/512 - 0s - loss: 1.8575e-07 - val_loss: 1.6232e-05
Epoch 441/512
512/512 - 0s - loss: 1.8488e-07 - val_loss: 1.6159e-05
Epoch 442/512
512/512 - 0s - loss: 1.8401e-07 - val_loss: 1.6089e-05
Epoch 443/512
512/512 - 0s - loss: 1.8316e-07 - val_loss: 1.6019e-05
Epoch 444/512
512/512 - 0s - loss: 1.8231e-07 - val_loss: 1.5950e-05
Epoch 445/512
512/512 - 0s - loss: 1.8148e-07 - val_loss: 1.5880e-05
Epoch 446/512
512/512 - 0s - loss: 1.8065e-07 - val_loss: 1.5812e-05
Epoch 447/512
512/512 - 0s - loss: 1.7983e-07 - val_loss: 1.5744e-05
Epoch 448/512
512/512 - 0s - loss: 1.7902e-07 - val_loss: 1.5678e-05
Epoch 449/512
512/512 - 0s - loss: 1.7822e-07 - val_loss: 1.5612e-05
Epoch 450/512
512/512 - 0s - loss: 1.7743e-07 - val_loss: 1.5547e-05
Epoch 451/512
512/512 - 0s - loss: 1.7665e-07 - val_loss: 1.5482e-05
Epoch 452/512
512/512 - 0s - loss: 1.7587e-07 - val_loss: 1.5417e-05
Epoch 453/512
512/512 - 0s - loss: 1.7510e-07 - val_loss: 1.5354e-05
Epoch 454/512
512/512 - 0s - loss: 1.7434e-07 - val_loss: 1.5291e-05
Epoch 455/512
512/512 - 0s - loss: 1.7359e-07 - val_loss: 1.5229e-05
Epoch 456/512
512/512 - 0s - loss: 1.7284e-07 - val_loss: 1.5166e-05
Epoch 457/512
512/512 - 0s - loss: 1.7211e-07 - val_loss: 1.5105e-05
Epoch 458/512
512/512 - 0s - loss: 1.7138e-07 - val_loss: 1.5045e-05
Epoch 459/512
512/512 - 0s - loss: 1.7066e-07 - val_loss: 1.4985e-05
Epoch 460/512
512/512 - 0s - loss: 1.6994e-07 - val_loss: 1.4926e-05
Epoch 461/512
512/512 - 0s - loss: 1.6923e-07 - val_loss: 1.4866e-05
Epoch 462/512
512/512 - 0s - loss: 1.6853e-07 - val_loss: 1.4807e-05
Epoch 463/512
512/512 - 0s - loss: 1.6783e-07 - val_loss: 1.4751e-05
Epoch 464/512
512/512 - 0s - loss: 1.6715e-07 - val_loss: 1.4692e-05
Epoch 465/512
512/512 - 0s - loss: 1.6647e-07 - val_loss: 1.4637e-05
Epoch 466/512
512/512 - 0s - loss: 1.6579e-07 - val_loss: 1.4580e-05
Epoch 467/512
512/512 - 0s - loss: 1.6512e-07 - val_loss: 1.4523e-05
Epoch 468/512
512/512 - 0s - loss: 1.6446e-07 - val_loss: 1.4468e-05
Epoch 469/512
512/512 - 0s - loss: 1.6380e-07 - val_loss: 1.4414e-05
Epoch 470/512
512/512 - 0s - loss: 1.6315e-07 - val_loss: 1.4359e-05
Epoch 471/512
512/512 - 0s - loss: 1.6251e-07 - val_loss: 1.4305e-05
Epoch 472/512
512/512 - 0s - loss: 1.6187e-07 - val_loss: 1.4252e-05
Epoch 473/512
512/512 - 0s - loss: 1.6124e-07 - val_loss: 1.4199e-05
Epoch 474/512
512/512 - 0s - loss: 1.6061e-07 - val_loss: 1.4146e-05
Epoch 475/512
512/512 - 0s - loss: 1.5999e-07 - val_loss: 1.4095e-05
Epoch 476/512
512/512 - 0s - loss: 1.5938e-07 - val_loss: 1.4043e-05
Epoch 477/512
512/512 - 0s - loss: 1.5877e-07 - val_loss: 1.3992e-05
Epoch 478/512
512/512 - 0s - loss: 1.5817e-07 - val_loss: 1.3941e-05
Epoch 479/512
512/512 - 0s - loss: 1.5757e-07 - val_loss: 1.3891e-05
Epoch 480/512
512/512 - 0s - loss: 1.5698e-07 - val_loss: 1.3841e-05
Epoch 481/512
512/512 - 0s - loss: 1.5639e-07 - val_loss: 1.3792e-05
Epoch 482/512
512/512 - 0s - loss: 1.5581e-07 - val_loss: 1.3742e-05
Epoch 483/512
512/512 - 0s - loss: 1.5523e-07 - val_loss: 1.3694e-05
Epoch 484/512
512/512 - 0s - loss: 1.5466e-07 - val_loss: 1.3646e-05
Epoch 485/512
512/512 - 0s - loss: 1.5409e-07 - val_loss: 1.3598e-05
Epoch 486/512
512/512 - 0s - loss: 1.5353e-07 - val_loss: 1.3550e-05
Epoch 487/512
512/512 - 0s - loss: 1.5297e-07 - val_loss: 1.3504e-05
Epoch 488/512
512/512 - 0s - loss: 1.5242e-07 - val_loss: 1.3457e-05
Epoch 489/512
512/512 - 0s - loss: 1.5187e-07 - val_loss: 1.3411e-05
Epoch 490/512
512/512 - 0s - loss: 1.5132e-07 - val_loss: 1.3365e-05
Epoch 491/512
512/512 - 0s - loss: 1.5079e-07 - val_loss: 1.3319e-05
Epoch 492/512
512/512 - 0s - loss: 1.5025e-07 - val_loss: 1.3274e-05
Epoch 493/512
512/512 - 0s - loss: 1.4972e-07 - val_loss: 1.3229e-05
Epoch 494/512
512/512 - 0s - loss: 1.4920e-07 - val_loss: 1.3184e-05
Epoch 495/512
512/512 - 0s - loss: 1.4868e-07 - val_loss: 1.3140e-05
Epoch 496/512
512/512 - 0s - loss: 1.4816e-07 - val_loss: 1.3097e-05
Epoch 497/512
512/512 - 0s - loss: 1.4765e-07 - val_loss: 1.3054e-05
Epoch 498/512
512/512 - 0s - loss: 1.4714e-07 - val_loss: 1.3011e-05
Epoch 499/512
512/512 - 0s - loss: 1.4663e-07 - val_loss: 1.2968e-05
Epoch 500/512
512/512 - 0s - loss: 1.4613e-07 - val_loss: 1.2925e-05
Epoch 501/512
512/512 - 0s - loss: 1.4564e-07 - val_loss: 1.2883e-05
Epoch 502/512
512/512 - 0s - loss: 1.4515e-07 - val_loss: 1.2841e-05
Epoch 503/512
512/512 - 0s - loss: 1.4466e-07 - val_loss: 1.2800e-05
Epoch 504/512
512/512 - 0s - loss: 1.4417e-07 - val_loss: 1.2759e-05
Epoch 505/512
512/512 - 0s - loss: 1.4369e-07 - val_loss: 1.2718e-05
Epoch 506/512
512/512 - 0s - loss: 1.4322e-07 - val_loss: 1.2677e-05
Epoch 507/512
512/512 - 0s - loss: 1.4274e-07 - val_loss: 1.2637e-05
Epoch 508/512
512/512 - 0s - loss: 1.4227e-07 - val_loss: 1.2597e-05
Epoch 509/512
512/512 - 0s - loss: 1.4181e-07 - val_loss: 1.2558e-05
Epoch 510/512
512/512 - 0s - loss: 1.4135e-07 - val_loss: 1.2518e-05
Epoch 511/512
512/512 - 0s - loss: 1.4089e-07 - val_loss: 1.2479e-05
Epoch 512/512
512/512 - 0s - loss: 1.4043e-07 - val_loss: 1.2441e-05
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7360e-09 - val_loss: 8.9527e-10
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.6872e-10 - val_loss: 6.5242e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3576e-10 - val_loss: 6.1694e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.1258e-10 - val_loss: 6.0413e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0110e-10 - val_loss: 5.9392e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.9110e-10 - val_loss: 5.8398e-10
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8121e-10 - val_loss: 5.7453e-10
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7184e-10 - val_loss: 5.6514e-10
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6244e-10 - val_loss: 5.5608e-10
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5352e-10 - val_loss: 5.4713e-10
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.4467e-10 - val_loss: 5.3856e-10
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.3614e-10 - val_loss: 5.3004e-10
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.2771e-10 - val_loss: 5.2175e-10
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1959e-10 - val_loss: 5.1388e-10
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.1167e-10 - val_loss: 5.0618e-10
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.0396e-10 - val_loss: 4.9855e-10
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.9649e-10 - val_loss: 4.9118e-10
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8917e-10 - val_loss: 4.8399e-10
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.8199e-10 - val_loss: 4.7707e-10
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.7513e-10 - val_loss: 4.7017e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6840e-10 - val_loss: 4.6361e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.6177e-10 - val_loss: 4.5693e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5524e-10 - val_loss: 4.5070e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4903e-10 - val_loss: 4.4451e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4289e-10 - val_loss: 4.3860e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3695e-10 - val_loss: 4.3263e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3114e-10 - val_loss: 4.2707e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2551e-10 - val_loss: 4.2140e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1995e-10 - val_loss: 4.1591e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1455e-10 - val_loss: 4.1056e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0926e-10 - val_loss: 4.0552e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0412e-10 - val_loss: 4.0054e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9917e-10 - val_loss: 3.9537e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9413e-10 - val_loss: 3.9065e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8939e-10 - val_loss: 3.8589e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8469e-10 - val_loss: 3.8130e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8014e-10 - val_loss: 3.7681e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7569e-10 - val_loss: 3.7255e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7135e-10 - val_loss: 3.6812e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6707e-10 - val_loss: 3.6396e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6289e-10 - val_loss: 3.5987e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5879e-10 - val_loss: 3.5580e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5488e-10 - val_loss: 3.5192e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5094e-10 - val_loss: 3.4807e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4716e-10 - val_loss: 3.4435e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4344e-10 - val_loss: 3.4065e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3977e-10 - val_loss: 3.3707e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3621e-10 - val_loss: 3.3355e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3268e-10 - val_loss: 3.3017e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2933e-10 - val_loss: 3.2680e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2605e-10 - val_loss: 3.2351e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2273e-10 - val_loss: 3.2026e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1951e-10 - val_loss: 3.1717e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1643e-10 - val_loss: 3.1408e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1338e-10 - val_loss: 3.1100e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1033e-10 - val_loss: 3.0817e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0746e-10 - val_loss: 3.0523e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0459e-10 - val_loss: 3.0243e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0178e-10 - val_loss: 2.9958e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9903e-10 - val_loss: 2.9693e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9636e-10 - val_loss: 2.9430e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9372e-10 - val_loss: 2.9172e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9114e-10 - val_loss: 2.8909e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8856e-10 - val_loss: 2.8666e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8614e-10 - val_loss: 2.8418e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8369e-10 - val_loss: 2.8176e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8128e-10 - val_loss: 2.7942e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7895e-10 - val_loss: 2.7713e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7669e-10 - val_loss: 2.7492e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7447e-10 - val_loss: 2.7270e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7225e-10 - val_loss: 2.7050e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7008e-10 - val_loss: 2.6840e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6797e-10 - val_loss: 2.6634e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6595e-10 - val_loss: 2.6436e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6390e-10 - val_loss: 2.6228e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6191e-10 - val_loss: 2.6038e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5996e-10 - val_loss: 2.5839e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5806e-10 - val_loss: 2.5655e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5620e-10 - val_loss: 2.5468e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5433e-10 - val_loss: 2.5283e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5257e-10 - val_loss: 2.5104e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5078e-10 - val_loss: 2.4938e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4906e-10 - val_loss: 2.4764e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4731e-10 - val_loss: 2.4592e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4565e-10 - val_loss: 2.4434e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4404e-10 - val_loss: 2.4271e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4242e-10 - val_loss: 2.4106e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4084e-10 - val_loss: 2.3952e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3924e-10 - val_loss: 2.3797e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3773e-10 - val_loss: 2.3649e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3625e-10 - val_loss: 2.3502e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3481e-10 - val_loss: 2.3359e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3339e-10 - val_loss: 2.3216e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3191e-10 - val_loss: 2.3074e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3053e-10 - val_loss: 2.2939e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2921e-10 - val_loss: 2.2802e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2785e-10 - val_loss: 2.2675e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2657e-10 - val_loss: 2.2542e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2524e-10 - val_loss: 2.2413e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2397e-10 - val_loss: 2.2293e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2273e-10 - val_loss: 2.2173e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2153e-10 - val_loss: 2.2044e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2034e-10 - val_loss: 2.1928e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1915e-10 - val_loss: 2.1808e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1797e-10 - val_loss: 2.1694e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1683e-10 - val_loss: 2.1581e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1571e-10 - val_loss: 2.1471e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1462e-10 - val_loss: 2.1365e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1354e-10 - val_loss: 2.1260e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1249e-10 - val_loss: 2.1149e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1140e-10 - val_loss: 2.1047e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1038e-10 - val_loss: 2.0946e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0937e-10 - val_loss: 2.0843e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0839e-10 - val_loss: 2.0749e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0742e-10 - val_loss: 2.0652e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0645e-10 - val_loss: 2.0552e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0548e-10 - val_loss: 2.0462e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0458e-10 - val_loss: 2.0372e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0367e-10 - val_loss: 2.0285e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0276e-10 - val_loss: 2.0193e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0188e-10 - val_loss: 2.0101e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0101e-10 - val_loss: 2.0021e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0014e-10 - val_loss: 1.9933e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9930e-10 - val_loss: 1.9852e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9847e-10 - val_loss: 1.9769e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9766e-10 - val_loss: 1.9688e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9685e-10 - val_loss: 1.9608e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9606e-10 - val_loss: 1.9528e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9527e-10 - val_loss: 1.9450e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9448e-10 - val_loss: 1.9371e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9375e-10 - val_loss: 1.9299e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9301e-10 - val_loss: 1.9230e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9229e-10 - val_loss: 1.9158e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9157e-10 - val_loss: 1.9087e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9087e-10 - val_loss: 1.9013e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9017e-10 - val_loss: 1.8943e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8945e-10 - val_loss: 1.8875e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8879e-10 - val_loss: 1.8808e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8813e-10 - val_loss: 1.8744e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8747e-10 - val_loss: 1.8681e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8683e-10 - val_loss: 1.8618e-10
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8620e-10 - val_loss: 1.8554e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8556e-10 - val_loss: 1.8492e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8493e-10 - val_loss: 1.8425e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8429e-10 - val_loss: 1.8365e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8369e-10 - val_loss: 1.8306e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8310e-10 - val_loss: 1.8247e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8253e-10 - val_loss: 1.8190e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8196e-10 - val_loss: 1.8136e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8139e-10 - val_loss: 1.8082e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8087e-10 - val_loss: 1.8023e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8027e-10 - val_loss: 1.7968e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7973e-10 - val_loss: 1.7912e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7918e-10 - val_loss: 1.7861e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7866e-10 - val_loss: 1.7811e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7814e-10 - val_loss: 1.7757e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7764e-10 - val_loss: 1.7704e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7713e-10 - val_loss: 1.7658e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7663e-10 - val_loss: 1.7606e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7611e-10 - val_loss: 1.7554e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7562e-10 - val_loss: 1.7506e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7515e-10 - val_loss: 1.7456e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7467e-10 - val_loss: 1.7412e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7422e-10 - val_loss: 1.7369e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7376e-10 - val_loss: 1.7320e-10
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7330e-10 - val_loss: 1.7277e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7284e-10 - val_loss: 1.7231e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7240e-10 - val_loss: 1.7187e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7194e-10 - val_loss: 1.7142e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7150e-10 - val_loss: 1.7101e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7109e-10 - val_loss: 1.7054e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7067e-10 - val_loss: 1.7014e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7025e-10 - val_loss: 1.6977e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6984e-10 - val_loss: 1.6934e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6944e-10 - val_loss: 1.6892e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6904e-10 - val_loss: 1.6856e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6864e-10 - val_loss: 1.6812e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6824e-10 - val_loss: 1.6776e-10
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6785e-10 - val_loss: 1.6732e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6745e-10 - val_loss: 1.6697e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6708e-10 - val_loss: 1.6657e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6670e-10 - val_loss: 1.6623e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6634e-10 - val_loss: 1.6589e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6598e-10 - val_loss: 1.6550e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6561e-10 - val_loss: 1.6511e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6524e-10 - val_loss: 1.6477e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6490e-10 - val_loss: 1.6445e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6455e-10 - val_loss: 1.6407e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6419e-10 - val_loss: 1.6372e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6384e-10 - val_loss: 1.6339e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6351e-10 - val_loss: 1.6308e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6319e-10 - val_loss: 1.6271e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6284e-10 - val_loss: 1.6239e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6252e-10 - val_loss: 1.6208e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6220e-10 - val_loss: 1.6175e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6188e-10 - val_loss: 1.6144e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6156e-10 - val_loss: 1.6111e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6126e-10 - val_loss: 1.6083e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6094e-10 - val_loss: 1.6048e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6062e-10 - val_loss: 1.6021e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6033e-10 - val_loss: 1.5989e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6003e-10 - val_loss: 1.5957e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5972e-10 - val_loss: 1.5930e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5943e-10 - val_loss: 1.5899e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5914e-10 - val_loss: 1.5870e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5884e-10 - val_loss: 1.5842e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5856e-10 - val_loss: 1.5813e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5827e-10 - val_loss: 1.5787e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5801e-10 - val_loss: 1.5760e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5773e-10 - val_loss: 1.5729e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5744e-10 - val_loss: 1.5701e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5716e-10 - val_loss: 1.5674e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5688e-10 - val_loss: 1.5647e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5663e-10 - val_loss: 1.5620e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5635e-10 - val_loss: 1.5593e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5610e-10 - val_loss: 1.5568e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5582e-10 - val_loss: 1.5544e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5558e-10 - val_loss: 1.5520e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5533e-10 - val_loss: 1.5492e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5508e-10 - val_loss: 1.5468e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5483e-10 - val_loss: 1.5444e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5459e-10 - val_loss: 1.5419e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5433e-10 - val_loss: 1.5393e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5407e-10 - val_loss: 1.5369e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5383e-10 - val_loss: 1.5344e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5359e-10 - val_loss: 1.5321e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5336e-10 - val_loss: 1.5296e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5311e-10 - val_loss: 1.5273e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5288e-10 - val_loss: 1.5251e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5265e-10 - val_loss: 1.5226e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5241e-10 - val_loss: 1.5204e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5219e-10 - val_loss: 1.5181e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5197e-10 - val_loss: 1.5159e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5174e-10 - val_loss: 1.5135e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5151e-10 - val_loss: 1.5116e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5130e-10 - val_loss: 1.5090e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5105e-10 - val_loss: 1.5070e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5086e-10 - val_loss: 1.5047e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5063e-10 - val_loss: 1.5025e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5041e-10 - val_loss: 1.5006e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5021e-10 - val_loss: 1.4983e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4999e-10 - val_loss: 1.4964e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4978e-10 - val_loss: 1.4941e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4956e-10 - val_loss: 1.4922e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4937e-10 - val_loss: 1.4901e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4917e-10 - val_loss: 1.4881e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4896e-10 - val_loss: 1.4860e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4876e-10 - val_loss: 1.4839e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4855e-10 - val_loss: 1.4819e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4836e-10 - val_loss: 1.4799e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4814e-10 - val_loss: 1.4778e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4795e-10 - val_loss: 1.4757e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4776e-10 - val_loss: 1.4739e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4755e-10 - val_loss: 1.4721e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4736e-10 - val_loss: 1.4700e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4716e-10 - val_loss: 1.4681e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4699e-10 - val_loss: 1.4665e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4680e-10 - val_loss: 1.4644e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4661e-10 - val_loss: 1.4623e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4641e-10 - val_loss: 1.4607e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4623e-10 - val_loss: 1.4588e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4605e-10 - val_loss: 1.4572e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4586e-10 - val_loss: 1.4553e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4568e-10 - val_loss: 1.4532e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4550e-10 - val_loss: 1.4515e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4531e-10 - val_loss: 1.4498e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4513e-10 - val_loss: 1.4477e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4494e-10 - val_loss: 1.4461e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4477e-10 - val_loss: 1.4442e-10
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4459e-10 - val_loss: 1.4427e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4442e-10 - val_loss: 1.4408e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4425e-10 - val_loss: 1.4390e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4407e-10 - val_loss: 1.4375e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4391e-10 - val_loss: 1.4357e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4374e-10 - val_loss: 1.4340e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4357e-10 - val_loss: 1.4324e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4339e-10 - val_loss: 1.4306e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4323e-10 - val_loss: 1.4288e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4306e-10 - val_loss: 1.4272e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4290e-10 - val_loss: 1.4256e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4273e-10 - val_loss: 1.4240e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4256e-10 - val_loss: 1.4221e-10
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4239e-10 - val_loss: 1.4206e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4222e-10 - val_loss: 1.4189e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4206e-10 - val_loss: 1.4173e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4190e-10 - val_loss: 1.4157e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4173e-10 - val_loss: 1.4142e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4158e-10 - val_loss: 1.4124e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4142e-10 - val_loss: 1.4111e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4127e-10 - val_loss: 1.4094e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4111e-10 - val_loss: 1.4078e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4095e-10 - val_loss: 1.4063e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4079e-10 - val_loss: 1.4047e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4063e-10 - val_loss: 1.4032e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4048e-10 - val_loss: 1.4015e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4033e-10 - val_loss: 1.4001e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4017e-10 - val_loss: 1.3984e-10
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4003e-10 - val_loss: 1.3970e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3987e-10 - val_loss: 1.3956e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3972e-10 - val_loss: 1.3940e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3956e-10 - val_loss: 1.3923e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3941e-10 - val_loss: 1.3910e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3927e-10 - val_loss: 1.3893e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3911e-10 - val_loss: 1.3879e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3896e-10 - val_loss: 1.3865e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3882e-10 - val_loss: 1.3853e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3868e-10 - val_loss: 1.3836e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3852e-10 - val_loss: 1.3820e-10
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3838e-10 - val_loss: 1.3806e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3824e-10 - val_loss: 1.3793e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3809e-10 - val_loss: 1.3777e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3795e-10 - val_loss: 1.3765e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3781e-10 - val_loss: 1.3748e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3767e-10 - val_loss: 1.3738e-10
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3753e-10 - val_loss: 1.3721e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3738e-10 - val_loss: 1.3708e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3725e-10 - val_loss: 1.3693e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3711e-10 - val_loss: 1.3678e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3697e-10 - val_loss: 1.3666e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3682e-10 - val_loss: 1.3650e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3667e-10 - val_loss: 1.3637e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3654e-10 - val_loss: 1.3622e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3640e-10 - val_loss: 1.3609e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3627e-10 - val_loss: 1.3596e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3613e-10 - val_loss: 1.3582e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3600e-10 - val_loss: 1.3570e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3586e-10 - val_loss: 1.3555e-10
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3572e-10 - val_loss: 1.3541e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3559e-10 - val_loss: 1.3527e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3546e-10 - val_loss: 1.3516e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3532e-10 - val_loss: 1.3502e-10
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3519e-10 - val_loss: 1.3488e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3506e-10 - val_loss: 1.3474e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3493e-10 - val_loss: 1.3462e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3480e-10 - val_loss: 1.3449e-10
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3467e-10 - val_loss: 1.3436e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3454e-10 - val_loss: 1.3422e-10
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3440e-10 - val_loss: 1.3410e-10
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3427e-10 - val_loss: 1.3398e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3414e-10 - val_loss: 1.3383e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3401e-10 - val_loss: 1.3372e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3388e-10 - val_loss: 1.3357e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3375e-10 - val_loss: 1.3344e-10
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3362e-10 - val_loss: 1.3333e-10
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3349e-10 - val_loss: 1.3321e-10
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3337e-10 - val_loss: 1.3307e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3324e-10 - val_loss: 1.3294e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3312e-10 - val_loss: 1.3282e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3299e-10 - val_loss: 1.3269e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3287e-10 - val_loss: 1.3258e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3275e-10 - val_loss: 1.3244e-10
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3262e-10 - val_loss: 1.3232e-10
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3249e-10 - val_loss: 1.3219e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3237e-10 - val_loss: 1.3207e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3224e-10 - val_loss: 1.3195e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3212e-10 - val_loss: 1.3180e-10
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3200e-10 - val_loss: 1.3170e-10
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3188e-10 - val_loss: 1.3158e-10
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3175e-10 - val_loss: 1.3145e-10
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3163e-10 - val_loss: 1.3134e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3151e-10 - val_loss: 1.3122e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3139e-10 - val_loss: 1.3109e-10
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3126e-10 - val_loss: 1.3096e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3113e-10 - val_loss: 1.3086e-10
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3103e-10 - val_loss: 1.3074e-10
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3090e-10 - val_loss: 1.3060e-10
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3078e-10 - val_loss: 1.3049e-10
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3067e-10 - val_loss: 1.3037e-10
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3055e-10 - val_loss: 1.3024e-10
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3042e-10 - val_loss: 1.3012e-10
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3031e-10 - val_loss: 1.3002e-10
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3019e-10 - val_loss: 1.2990e-10
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3008e-10 - val_loss: 1.2979e-10
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2996e-10 - val_loss: 1.2967e-10
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2984e-10 - val_loss: 1.2955e-10
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2973e-10 - val_loss: 1.2944e-10
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2961e-10 - val_loss: 1.2932e-10
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2949e-10 - val_loss: 1.2920e-10
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2938e-10 - val_loss: 1.2909e-10
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2927e-10 - val_loss: 1.2898e-10
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2915e-10 - val_loss: 1.2888e-10
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2904e-10 - val_loss: 1.2875e-10
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2893e-10 - val_loss: 1.2864e-10
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2881e-10 - val_loss: 1.2853e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2870e-10 - val_loss: 1.2841e-10
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2858e-10 - val_loss: 1.2829e-10
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2846e-10 - val_loss: 1.2818e-10
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2835e-10 - val_loss: 1.2807e-10
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2823e-10 - val_loss: 1.2794e-10
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2812e-10 - val_loss: 1.2784e-10
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2802e-10 - val_loss: 1.2773e-10
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2790e-10 - val_loss: 1.2760e-10
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2779e-10 - val_loss: 1.2752e-10
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2768e-10 - val_loss: 1.2739e-10
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2757e-10 - val_loss: 1.2727e-10
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2745e-10 - val_loss: 1.2716e-10
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2734e-10 - val_loss: 1.2708e-10
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2724e-10 - val_loss: 1.2697e-10
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2713e-10 - val_loss: 1.2684e-10
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2702e-10 - val_loss: 1.2673e-10
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2691e-10 - val_loss: 1.2662e-10
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2680e-10 - val_loss: 1.2652e-10
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2669e-10 - val_loss: 1.2640e-10
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2658e-10 - val_loss: 1.2630e-10
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2648e-10 - val_loss: 1.2618e-10
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2637e-10 - val_loss: 1.2608e-10
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2626e-10 - val_loss: 1.2599e-10
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2615e-10 - val_loss: 1.2587e-10
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2605e-10 - val_loss: 1.2576e-10
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2594e-10 - val_loss: 1.2566e-10
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2583e-10 - val_loss: 1.2556e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2572e-10 - val_loss: 1.2544e-10
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2561e-10 - val_loss: 1.2534e-10
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2551e-10 - val_loss: 1.2522e-10
Epoch 415/512

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2539e-10 - val_loss: 1.2511e-10
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2529e-10 - val_loss: 1.2502e-10
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2519e-10 - val_loss: 1.2492e-10
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2508e-10 - val_loss: 1.2481e-10
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2497e-10 - val_loss: 1.2469e-10
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2487e-10 - val_loss: 1.2459e-10
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2476e-10 - val_loss: 1.2449e-10
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2466e-10 - val_loss: 1.2438e-10
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2456e-10 - val_loss: 1.2429e-10
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2445e-10 - val_loss: 1.2417e-10
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2435e-10 - val_loss: 1.2407e-10
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2425e-10 - val_loss: 1.2398e-10
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2415e-10 - val_loss: 1.2387e-10
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2405e-10 - val_loss: 1.2376e-10
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2394e-10 - val_loss: 1.2365e-10
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2383e-10 - val_loss: 1.2357e-10
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2374e-10 - val_loss: 1.2347e-10
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2364e-10 - val_loss: 1.2336e-10
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2353e-10 - val_loss: 1.2325e-10
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2342e-10 - val_loss: 1.2317e-10
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2333e-10 - val_loss: 1.2306e-10
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2323e-10 - val_loss: 1.2295e-10
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2313e-10 - val_loss: 1.2285e-10
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2303e-10 - val_loss: 1.2275e-10
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2293e-10 - val_loss: 1.2266e-10
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2282e-10 - val_loss: 1.2255e-10
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2272e-10 - val_loss: 1.2245e-10
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2262e-10 - val_loss: 1.2233e-10
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2252e-10 - val_loss: 1.2225e-10
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2242e-10 - val_loss: 1.2216e-10
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2232e-10 - val_loss: 1.2207e-10
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2222e-10 - val_loss: 1.2195e-10
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2212e-10 - val_loss: 1.2185e-10
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2203e-10 - val_loss: 1.2177e-10
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2192e-10 - val_loss: 1.2165e-10
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2182e-10 - val_loss: 1.2157e-10
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2173e-10 - val_loss: 1.2146e-10
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2163e-10 - val_loss: 1.2137e-10
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2153e-10 - val_loss: 1.2127e-10
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2143e-10 - val_loss: 1.2117e-10
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2134e-10 - val_loss: 1.2108e-10
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2125e-10 - val_loss: 1.2098e-10
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2115e-10 - val_loss: 1.2088e-10
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2105e-10 - val_loss: 1.2077e-10
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2095e-10 - val_loss: 1.2068e-10
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2086e-10 - val_loss: 1.2058e-10
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2076e-10 - val_loss: 1.2049e-10
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2066e-10 - val_loss: 1.2040e-10
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2057e-10 - val_loss: 1.2029e-10
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2048e-10 - val_loss: 1.2022e-10
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2038e-10 - val_loss: 1.2011e-10
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2029e-10 - val_loss: 1.2002e-10
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2019e-10 - val_loss: 1.1990e-10
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2009e-10 - val_loss: 1.1982e-10
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1999e-10 - val_loss: 1.1974e-10
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1990e-10 - val_loss: 1.1962e-10
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1979e-10 - val_loss: 1.1953e-10
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1970e-10 - val_loss: 1.1944e-10
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1961e-10 - val_loss: 1.1932e-10
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1951e-10 - val_loss: 1.1924e-10
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1942e-10 - val_loss: 1.1916e-10
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1933e-10 - val_loss: 1.1906e-10
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1923e-10 - val_loss: 1.1897e-10
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1914e-10 - val_loss: 1.1888e-10
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1905e-10 - val_loss: 1.1880e-10
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1895e-10 - val_loss: 1.1869e-10
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1886e-10 - val_loss: 1.1859e-10
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1877e-10 - val_loss: 1.1852e-10
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1868e-10 - val_loss: 1.1842e-10
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1859e-10 - val_loss: 1.1831e-10
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1849e-10 - val_loss: 1.1822e-10
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1840e-10 - val_loss: 1.1814e-10
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1831e-10 - val_loss: 1.1805e-10
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1822e-10 - val_loss: 1.1796e-10
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1813e-10 - val_loss: 1.1786e-10
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1803e-10 - val_loss: 1.1778e-10
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1796e-10 - val_loss: 1.1769e-10
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1785e-10 - val_loss: 1.1759e-10
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1776e-10 - val_loss: 1.1751e-10
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1767e-10 - val_loss: 1.1742e-10
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1758e-10 - val_loss: 1.1732e-10
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1750e-10 - val_loss: 1.1724e-10
Epoch 497/512

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1740e-10 - val_loss: 1.1715e-10
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1732e-10 - val_loss: 1.1705e-10
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1722e-10 - val_loss: 1.1697e-10
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1713e-10 - val_loss: 1.1686e-10
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1703e-10 - val_loss: 1.1677e-10
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1694e-10 - val_loss: 1.1668e-10
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1686e-10 - val_loss: 1.1660e-10
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1676e-10 - val_loss: 1.1652e-10
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1668e-10 - val_loss: 1.1643e-10
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1659e-10 - val_loss: 1.1634e-10
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1650e-10 - val_loss: 1.1624e-10
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1641e-10 - val_loss: 1.1615e-10
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1632e-10 - val_loss: 1.1607e-10
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1624e-10 - val_loss: 1.1599e-10
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1615e-10 - val_loss: 1.1590e-10
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0-5-15/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1607e-10 - val_loss: 1.1580e-10
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.255 | eve: 9.263 | bob: 9.033Epoch   0:   0% | abe: 9.165 | eve: 9.247 | bob: 9.016Epoch   0:   1% | abe: 9.132 | eve: 9.229 | bob: 9.010Epoch   0:   2% | abe: 9.129 | eve: 9.235 | bob: 9.020Epoch   0:   3% | abe: 9.108 | eve: 9.225 | bob: 9.008Epoch   0:   3% | abe: 9.109 | eve: 9.222 | bob: 9.014Epoch   0:   4% | abe: 9.094 | eve: 9.223 | bob: 9.001Epoch   0:   5% | abe: 9.095 | eve: 9.217 | bob: 9.005Epoch   0:   6% | abe: 9.098 | eve: 9.222 | bob: 9.011Epoch   0:   7% | abe: 9.096 | eve: 9.230 | bob: 9.010Epoch   0:   7% | abe: 9.100 | eve: 9.232 | bob: 9.016Epoch   0:   8% | abe: 9.098 | eve: 9.230 | bob: 9.014Epoch   0:   9% | abe: 9.092 | eve: 9.238 | bob: 9.009Epoch   0:  10% | abe: 9.091 | eve: 9.237 | bob: 9.008Epoch   0:  10% | abe: 9.091 | eve: 9.244 | bob: 9.008Epoch   0:  11% | abe: 9.089 | eve: 9.237 | bob: 9.007Epoch   0:  12% | abe: 9.087 | eve: 9.235 | bob: 9.005Epoch   0:  13% | abe: 9.090 | eve: 9.234 | bob: 9.008Epoch   0:  14% | abe: 9.092 | eve: 9.232 | bob: 9.011Epoch   0:  14% | abe: 9.090 | eve: 9.228 | bob: 9.009Epoch   0:  15% | abe: 9.093 | eve: 9.227 | bob: 9.012Epoch   0:  16% | abe: 9.089 | eve: 9.227 | bob: 9.009Epoch   0:  17% | abe: 9.087 | eve: 9.223 | bob: 9.007Epoch   0:  17% | abe: 9.086 | eve: 9.220 | bob: 9.006Epoch   0:  18% | abe: 9.088 | eve: 9.217 | bob: 9.007Epoch   0:  19% | abe: 9.088 | eve: 9.218 | bob: 9.008Epoch   0:  20% | abe: 9.089 | eve: 9.218 | bob: 9.009Epoch   0:  21% | abe: 9.089 | eve: 9.218 | bob: 9.009Epoch   0:  21% | abe: 9.089 | eve: 9.215 | bob: 9.010Epoch   0:  22% | abe: 9.089 | eve: 9.213 | bob: 9.010Epoch   0:  23% | abe: 9.089 | eve: 9.217 | bob: 9.009Epoch   0:  24% | abe: 9.089 | eve: 9.215 | bob: 9.009Epoch   0:  25% | abe: 9.091 | eve: 9.215 | bob: 9.011Epoch   0:  25% | abe: 9.092 | eve: 9.217 | bob: 9.012Epoch   0:  26% | abe: 9.089 | eve: 9.216 | bob: 9.010Epoch   0:  27% | abe: 9.089 | eve: 9.214 | bob: 9.009Epoch   0:  28% | abe: 9.088 | eve: 9.213 | bob: 9.009Epoch   0:  28% | abe: 9.089 | eve: 9.213 | bob: 9.009Epoch   0:  29% | abe: 9.088 | eve: 9.214 | bob: 9.009Epoch   0:  30% | abe: 9.087 | eve: 9.213 | bob: 9.008Epoch   0:  31% | abe: 9.087 | eve: 9.213 | bob: 9.007Epoch   0:  32% | abe: 9.086 | eve: 9.211 | bob: 9.007Epoch   0:  32% | abe: 9.088 | eve: 9.212 | bob: 9.009Epoch   0:  33% | abe: 9.088 | eve: 9.212 | bob: 9.009Epoch   0:  34% | abe: 9.087 | eve: 9.211 | bob: 9.008Epoch   0:  35% | abe: 9.085 | eve: 9.211 | bob: 9.006Epoch   0:  35% | abe: 9.086 | eve: 9.211 | bob: 9.006Epoch   0:  36% | abe: 9.085 | eve: 9.209 | bob: 9.005Epoch   0:  37% | abe: 9.085 | eve: 9.210 | bob: 9.005Epoch   0:  38% | abe: 9.085 | eve: 9.208 | bob: 9.006Epoch   0:  39% | abe: 9.085 | eve: 9.207 | bob: 9.005Epoch   0:  39% | abe: 9.086 | eve: 9.207 | bob: 9.006Epoch   0:  40% | abe: 9.086 | eve: 9.207 | bob: 9.007Epoch   0:  41% | abe: 9.087 | eve: 9.206 | bob: 9.007Epoch   0:  42% | abe: 9.087 | eve: 9.205 | bob: 9.008Epoch   0:  42% | abe: 9.088 | eve: 9.205 | bob: 9.009Epoch   0:  43% | abe: 9.087 | eve: 9.204 | bob: 9.008Epoch   0:  44% | abe: 9.087 | eve: 9.203 | bob: 9.008Epoch   0:  45% | abe: 9.087 | eve: 9.202 | bob: 9.007Epoch   0:  46% | abe: 9.086 | eve: 9.203 | bob: 9.007Epoch   0:  46% | abe: 9.086 | eve: 9.202 | bob: 9.007Epoch   0:  47% | abe: 9.086 | eve: 9.202 | bob: 9.007Epoch   0:  48% | abe: 9.086 | eve: 9.202 | bob: 9.007Epoch   0:  49% | abe: 9.087 | eve: 9.201 | bob: 9.007Epoch   0:  50% | abe: 9.086 | eve: 9.201 | bob: 9.007Epoch   0:  50% | abe: 9.086 | eve: 9.201 | bob: 9.007Epoch   0:  51% | abe: 9.086 | eve: 9.201 | bob: 9.006Epoch   0:  52% | abe: 9.085 | eve: 9.199 | bob: 9.006Epoch   0:  53% | abe: 9.085 | eve: 9.198 | bob: 9.006Epoch   0:  53% | abe: 9.086 | eve: 9.199 | bob: 9.006Epoch   0:  54% | abe: 9.087 | eve: 9.198 | bob: 9.008Epoch   0:  55% | abe: 9.086 | eve: 9.199 | bob: 9.007Epoch   0:  56% | abe: 9.085 | eve: 9.199 | bob: 9.006Epoch   0:  57% | abe: 9.085 | eve: 9.199 | bob: 9.006Epoch   0:  57% | abe: 9.085 | eve: 9.198 | bob: 9.005Epoch   0:  58% | abe: 9.084 | eve: 9.198 | bob: 9.005Epoch   0:  59% | abe: 9.084 | eve: 9.198 | bob: 9.005Epoch   0:  60% | abe: 9.084 | eve: 9.198 | bob: 9.005Epoch   0:  60% | abe: 9.085 | eve: 9.198 | bob: 9.006Epoch   0:  61% | abe: 9.085 | eve: 9.198 | bob: 9.006Epoch   0:  62% | abe: 9.085 | eve: 9.197 | bob: 9.006Epoch   0:  63% | abe: 9.085 | eve: 9.196 | bob: 9.006Epoch   0:  64% | abe: 9.085 | eve: 9.195 | bob: 9.006Epoch   0:  64% | abe: 9.085 | eve: 9.194 | bob: 9.006Epoch   0:  65% | abe: 9.085 | eve: 9.195 | bob: 9.006Epoch   0:  66% | abe: 9.085 | eve: 9.194 | bob: 9.006Epoch   0:  67% | abe: 9.085 | eve: 9.194 | bob: 9.006Epoch   0:  67% | abe: 9.085 | eve: 9.194 | bob: 9.005Epoch   0:  68% | abe: 9.085 | eve: 9.194 | bob: 9.006Epoch   0:  69% | abe: 9.085 | eve: 9.193 | bob: 9.006Epoch   0:  70% | abe: 9.085 | eve: 9.193 | bob: 9.006Epoch   0:  71% | abe: 9.085 | eve: 9.193 | bob: 9.006Epoch   0:  71% | abe: 9.085 | eve: 9.192 | bob: 9.006Epoch   0:  72% | abe: 9.085 | eve: 9.193 | bob: 9.005Epoch   0:  73% | abe: 9.084 | eve: 9.193 | bob: 9.005Epoch   0:  74% | abe: 9.084 | eve: 9.193 | bob: 9.005Epoch   0:  75% | abe: 9.084 | eve: 9.192 | bob: 9.005Epoch   0:  75% | abe: 9.084 | eve: 9.191 | bob: 9.005Epoch   0:  76% | abe: 9.084 | eve: 9.191 | bob: 9.005Epoch   0:  77% | abe: 9.084 | eve: 9.190 | bob: 9.005Epoch   0:  78% | abe: 9.084 | eve: 9.190 | bob: 9.005Epoch   0:  78% | abe: 9.084 | eve: 9.189 | bob: 9.005Epoch   0:  79% | abe: 9.084 | eve: 9.189 | bob: 9.005Epoch   0:  80% | abe: 9.083 | eve: 9.188 | bob: 9.005Epoch   0:  81% | abe: 9.083 | eve: 9.187 | bob: 9.004Epoch   0:  82% | abe: 9.084 | eve: 9.186 | bob: 9.005Epoch   0:  82% | abe: 9.084 | eve: 9.186 | bob: 9.005Epoch   0:  83% | abe: 9.084 | eve: 9.185 | bob: 9.005Epoch   0:  84% | abe: 9.084 | eve: 9.184 | bob: 9.005Epoch   0:  85% | abe: 9.084 | eve: 9.184 | bob: 9.005Epoch   0:  85% | abe: 9.084 | eve: 9.183 | bob: 9.005Epoch   0:  86% | abe: 9.084 | eve: 9.183 | bob: 9.005Epoch   0:  87% | abe: 9.084 | eve: 9.183 | bob: 9.005Epoch   0:  88% | abe: 9.085 | eve: 9.182 | bob: 9.006Epoch   0:  89% | abe: 9.085 | eve: 9.182 | bob: 9.006Epoch   0:  89% | abe: 9.085 | eve: 9.181 | bob: 9.007Epoch   0:  90% | abe: 9.085 | eve: 9.181 | bob: 9.007Epoch   0:  91% | abe: 9.085 | eve: 9.180 | bob: 9.007Epoch   0:  92% | abe: 9.085 | eve: 9.179 | bob: 9.007Epoch   0:  92% | abe: 9.085 | eve: 9.178 | bob: 9.006Epoch   0:  93% | abe: 9.085 | eve: 9.177 | bob: 9.007Epoch   0:  94% | abe: 9.085 | eve: 9.177 | bob: 9.007Epoch   0:  95% | abe: 9.086 | eve: 9.176 | bob: 9.007Epoch   0:  96% | abe: 9.086 | eve: 9.176 | bob: 9.008Epoch   0:  96% | abe: 9.086 | eve: 9.174 | bob: 9.007Epoch   0:  97% | abe: 9.086 | eve: 9.174 | bob: 9.007Epoch   0:  98% | abe: 9.086 | eve: 9.174 | bob: 9.007Epoch   0:  99% | abe: 9.085 | eve: 9.173 | bob: 9.007
New best Bob loss 9.006882431922577 at epoch 0
Epoch   1:   0% | abe: 9.058 | eve: 9.155 | bob: 8.979Epoch   1:   0% | abe: 9.083 | eve: 9.115 | bob: 9.005Epoch   1:   1% | abe: 9.087 | eve: 9.127 | bob: 9.009Epoch   1:   2% | abe: 9.099 | eve: 9.122 | bob: 9.021Epoch   1:   3% | abe: 9.099 | eve: 9.112 | bob: 9.020Epoch   1:   3% | abe: 9.092 | eve: 9.113 | bob: 9.013Epoch   1:   4% | abe: 9.095 | eve: 9.102 | bob: 9.016Epoch   1:   5% | abe: 9.084 | eve: 9.097 | bob: 9.005Epoch   1:   6% | abe: 9.078 | eve: 9.091 | bob: 8.998Epoch   1:   7% | abe: 9.078 | eve: 9.080 | bob: 8.998Epoch   1:   7% | abe: 9.079 | eve: 9.078 | bob: 8.999Epoch   1:   8% | abe: 9.079 | eve: 9.085 | bob: 8.999Epoch   1:   9% | abe: 9.078 | eve: 9.083 | bob: 8.998Epoch   1:  10% | abe: 9.076 | eve: 9.082 | bob: 8.996Epoch   1:  10% | abe: 9.080 | eve: 9.083 | bob: 9.001Epoch   1:  11% | abe: 9.081 | eve: 9.083 | bob: 9.001Epoch   1:  12% | abe: 9.084 | eve: 9.084 | bob: 9.004Epoch   1:  13% | abe: 9.080 | eve: 9.084 | bob: 9.001Epoch   1:  14% | abe: 9.077 | eve: 9.082 | bob: 8.998Epoch   1:  14% | abe: 9.077 | eve: 9.084 | bob: 8.997Epoch   1:  15% | abe: 9.077 | eve: 9.083 | bob: 8.998Epoch   1:  16% | abe: 9.076 | eve: 9.083 | bob: 8.997Epoch   1:  17% | abe: 9.077 | eve: 9.082 | bob: 8.998Epoch   1:  17% | abe: 9.080 | eve: 9.084 | bob: 9.001Epoch   1:  18% | abe: 9.079 | eve: 9.086 | bob: 9.000Epoch   1:  19% | abe: 9.077 | eve: 9.086 | bob: 8.998Epoch   1:  20% | abe: 9.076 | eve: 9.084 | bob: 8.998Epoch   1:  21% | abe: 9.075 | eve: 9.084 | bob: 8.996Epoch   1:  21% | abe: 9.078 | eve: 9.084 | bob: 8.999Epoch   1:  22% | abe: 9.077 | eve: 9.085 | bob: 8.998Epoch   1:  23% | abe: 9.076 | eve: 9.086 | bob: 8.997Epoch   1:  24% | abe: 9.077 | eve: 9.084 | bob: 8.998Epoch   1:  25% | abe: 9.074 | eve: 9.084 | bob: 8.995Epoch   1:  25% | abe: 9.073 | eve: 9.088 | bob: 8.994Epoch   1:  26% | abe: 9.073 | eve: 9.088 | bob: 8.994Epoch   1:  27% | abe: 9.073 | eve: 9.088 | bob: 8.994Epoch   1:  28% | abe: 9.074 | eve: 9.090 | bob: 8.995Epoch   1:  28% | abe: 9.075 | eve: 9.091 | bob: 8.995Epoch   1:  29% | abe: 9.074 | eve: 9.092 | bob: 8.995Epoch   1:  30% | abe: 9.073 | eve: 9.092 | bob: 8.994Epoch   1:  31% | abe: 9.072 | eve: 9.095 | bob: 8.993Epoch   1:  32% | abe: 9.072 | eve: 9.094 | bob: 8.992Epoch   1:  32% | abe: 9.072 | eve: 9.096 | bob: 8.993Epoch   1:  33% | abe: 9.074 | eve: 9.095 | bob: 8.994Epoch   1:  34% | abe: 9.073 | eve: 9.096 | bob: 8.993Epoch   1:  35% | abe: 9.072 | eve: 9.095 | bob: 8.993Epoch   1:  35% | abe: 9.073 | eve: 9.096 | bob: 8.993Epoch   1:  36% | abe: 9.074 | eve: 9.096 | bob: 8.995Epoch   1:  37% | abe: 9.074 | eve: 9.095 | bob: 8.995Epoch   1:  38% | abe: 9.075 | eve: 9.096 | bob: 8.995Epoch   1:  39% | abe: 9.075 | eve: 9.098 | bob: 8.996Epoch   1:  39% | abe: 9.076 | eve: 9.098 | bob: 8.997Epoch   1:  40% | abe: 9.076 | eve: 9.099 | bob: 8.997Epoch   1:  41% | abe: 9.074 | eve: 9.098 | bob: 8.994Epoch   1:  42% | abe: 9.073 | eve: 9.099 | bob: 8.994Epoch   1:  42% | abe: 9.074 | eve: 9.099 | bob: 8.994Epoch   1:  43% | abe: 9.073 | eve: 9.099 | bob: 8.993Epoch   1:  44% | abe: 9.072 | eve: 9.099 | bob: 8.993Epoch   1:  45% | abe: 9.073 | eve: 9.099 | bob: 8.994Epoch   1:  46% | abe: 9.074 | eve: 9.100 | bob: 8.994Epoch   1:  46% | abe: 9.073 | eve: 9.100 | bob: 8.994Epoch   1:  47% | abe: 9.072 | eve: 9.101 | bob: 8.993Epoch   1:  48% | abe: 9.073 | eve: 9.102 | bob: 8.994Epoch   1:  49% | abe: 9.072 | eve: 9.102 | bob: 8.993Epoch   1:  50% | abe: 9.072 | eve: 9.102 | bob: 8.993Epoch   1:  50% | abe: 9.071 | eve: 9.104 | bob: 8.991Epoch   1:  51% | abe: 9.070 | eve: 9.103 | bob: 8.991Epoch   1:  52% | abe: 9.071 | eve: 9.103 | bob: 8.991Epoch   1:  53% | abe: 9.070 | eve: 9.103 | bob: 8.991Epoch   1:  53% | abe: 9.069 | eve: 9.103 | bob: 8.990Epoch   1:  54% | abe: 9.068 | eve: 9.103 | bob: 8.989Epoch   1:  55% | abe: 9.067 | eve: 9.104 | bob: 8.988Epoch   1:  56% | abe: 9.066 | eve: 9.103 | bob: 8.987Epoch   1:  57% | abe: 9.066 | eve: 9.104 | bob: 8.987Epoch   1:  57% | abe: 9.067 | eve: 9.103 | bob: 8.988Epoch   1:  58% | abe: 9.066 | eve: 9.103 | bob: 8.987Epoch   1:  59% | abe: 9.066 | eve: 9.104 | bob: 8.987Epoch   1:  60% | abe: 9.066 | eve: 9.103 | bob: 8.986Epoch   1:  60% | abe: 9.065 | eve: 9.102 | bob: 8.986Epoch   1:  61% | abe: 9.064 | eve: 9.102 | bob: 8.985Epoch   1:  62% | abe: 9.064 | eve: 9.102 | bob: 8.985Epoch   1:  63% | abe: 9.063 | eve: 9.103 | bob: 8.984Epoch   1:  64% | abe: 9.063 | eve: 9.104 | bob: 8.983Epoch   1:  64% | abe: 9.062 | eve: 9.104 | bob: 8.983Epoch   1:  65% | abe: 9.062 | eve: 9.103 | bob: 8.982Epoch   1:  66% | abe: 9.061 | eve: 9.103 | bob: 8.982Epoch   1:  67% | abe: 9.061 | eve: 9.103 | bob: 8.981Epoch   1:  67% | abe: 9.060 | eve: 9.104 | bob: 8.981Epoch   1:  68% | abe: 9.059 | eve: 9.104 | bob: 8.979Epoch   1:  69% | abe: 9.058 | eve: 9.105 | bob: 8.978Epoch   1:  70% | abe: 9.057 | eve: 9.105 | bob: 8.977Epoch   1:  71% | abe: 9.055 | eve: 9.104 | bob: 8.976Epoch   1:  71% | abe: 9.055 | eve: 9.105 | bob: 8.976Epoch   1:  72% | abe: 9.053 | eve: 9.105 | bob: 8.974Epoch   1:  73% | abe: 9.053 | eve: 9.105 | bob: 8.974Epoch   1:  74% | abe: 9.052 | eve: 9.106 | bob: 8.973Epoch   1:  75% | abe: 9.051 | eve: 9.106 | bob: 8.972Epoch   1:  75% | abe: 9.050 | eve: 9.106 | bob: 8.971Epoch   1:  76% | abe: 9.049 | eve: 9.106 | bob: 8.970Epoch   1:  77% | abe: 9.048 | eve: 9.107 | bob: 8.969Epoch   1:  78% | abe: 9.047 | eve: 9.106 | bob: 8.968Epoch   1:  78% | abe: 9.047 | eve: 9.106 | bob: 8.967Epoch   1:  79% | abe: 9.046 | eve: 9.106 | bob: 8.966Epoch   1:  80% | abe: 9.045 | eve: 9.105 | bob: 8.966Epoch   1:  81% | abe: 9.044 | eve: 9.106 | bob: 8.964Epoch   1:  82% | abe: 9.042 | eve: 9.106 | bob: 8.963Epoch   1:  82% | abe: 9.041 | eve: 9.106 | bob: 8.961Epoch   1:  83% | abe: 9.039 | eve: 9.106 | bob: 8.960Epoch   1:  84% | abe: 9.038 | eve: 9.105 | bob: 8.958Epoch   1:  85% | abe: 9.036 | eve: 9.105 | bob: 8.957Epoch   1:  85% | abe: 9.035 | eve: 9.105 | bob: 8.955Epoch   1:  86% | abe: 9.033 | eve: 9.105 | bob: 8.954Epoch   1:  87% | abe: 9.032 | eve: 9.106 | bob: 8.952Epoch   1:  88% | abe: 9.030 | eve: 9.105 | bob: 8.951Epoch   1:  89% | abe: 9.029 | eve: 9.106 | bob: 8.950Epoch   1:  89% | abe: 9.028 | eve: 9.105 | bob: 8.948Epoch   1:  90% | abe: 9.027 | eve: 9.105 | bob: 8.947Epoch   1:  91% | abe: 9.025 | eve: 9.104 | bob: 8.946Epoch   1:  92% | abe: 9.024 | eve: 9.104 | bob: 8.944Epoch   1:  92% | abe: 9.022 | eve: 9.103 | bob: 8.943Epoch   1:  93% | abe: 9.020 | eve: 9.104 | bob: 8.941Epoch   1:  94% | abe: 9.018 | eve: 9.104 | bob: 8.939Epoch   1:  95% | abe: 9.017 | eve: 9.104 | bob: 8.937Epoch   1:  96% | abe: 9.015 | eve: 9.104 | bob: 8.936Epoch   1:  96% | abe: 9.013 | eve: 9.104 | bob: 8.934Epoch   1:  97% | abe: 9.011 | eve: 9.104 | bob: 8.932Epoch   1:  98% | abe: 9.010 | eve: 9.104 | bob: 8.931Epoch   1:  99% | abe: 9.008 | eve: 9.104 | bob: 8.929
New best Bob loss 8.92880502700882 at epoch 1
Epoch   2:   0% | abe: 8.712 | eve: 9.078 | bob: 8.670Epoch   2:   0% | abe: 8.754 | eve: 9.115 | bob: 8.686Epoch   2:   1% | abe: 8.734 | eve: 9.092 | bob: 8.675Epoch   2:   2% | abe: 8.741 | eve: 9.090 | bob: 8.672Epoch   2:   3% | abe: 8.737 | eve: 9.097 | bob: 8.671Epoch   2:   3% | abe: 8.732 | eve: 9.100 | bob: 8.663Epoch   2:   4% | abe: 8.729 | eve: 9.100 | bob: 8.662Epoch   2:   5% | abe: 8.728 | eve: 9.096 | bob: 8.657Epoch   2:   6% | abe: 8.724 | eve: 9.101 | bob: 8.655Epoch   2:   7% | abe: 8.723 | eve: 9.100 | bob: 8.652Epoch   2:   7% | abe: 8.720 | eve: 9.104 | bob: 8.650Epoch   2:   8% | abe: 8.718 | eve: 9.110 | bob: 8.648Epoch   2:   9% | abe: 8.711 | eve: 9.106 | bob: 8.641Epoch   2:  10% | abe: 8.705 | eve: 9.104 | bob: 8.636Epoch   2:  10% | abe: 8.702 | eve: 9.105 | bob: 8.632Epoch   2:  11% | abe: 8.702 | eve: 9.108 | bob: 8.631Epoch   2:  12% | abe: 8.694 | eve: 9.111 | bob: 8.624Epoch   2:  13% | abe: 8.689 | eve: 9.112 | bob: 8.618Epoch   2:  14% | abe: 8.686 | eve: 9.114 | bob: 8.616Epoch   2:  14% | abe: 8.683 | eve: 9.117 | bob: 8.612Epoch   2:  15% | abe: 8.675 | eve: 9.115 | bob: 8.603Epoch   2:  16% | abe: 8.669 | eve: 9.113 | bob: 8.598Epoch   2:  17% | abe: 8.665 | eve: 9.113 | bob: 8.593Epoch   2:  17% | abe: 8.659 | eve: 9.111 | bob: 8.588Epoch   2:  18% | abe: 8.654 | eve: 9.112 | bob: 8.582Epoch   2:  19% | abe: 8.649 | eve: 9.110 | bob: 8.577Epoch   2:  20% | abe: 8.643 | eve: 9.110 | bob: 8.571Epoch   2:  21% | abe: 8.637 | eve: 9.111 | bob: 8.565Epoch   2:  21% | abe: 8.632 | eve: 9.111 | bob: 8.560Epoch   2:  22% | abe: 8.627 | eve: 9.106 | bob: 8.555Epoch   2:  23% | abe: 8.622 | eve: 9.107 | bob: 8.550Epoch   2:  24% | abe: 8.617 | eve: 9.107 | bob: 8.545Epoch   2:  25% | abe: 8.612 | eve: 9.108 | bob: 8.539Epoch   2:  25% | abe: 8.608 | eve: 9.108 | bob: 8.536Epoch   2:  26% | abe: 8.603 | eve: 9.108 | bob: 8.530Epoch   2:  27% | abe: 8.598 | eve: 9.107 | bob: 8.526Epoch   2:  28% | abe: 8.594 | eve: 9.106 | bob: 8.522Epoch   2:  28% | abe: 8.590 | eve: 9.107 | bob: 8.519Epoch   2:  29% | abe: 8.586 | eve: 9.107 | bob: 8.515Epoch   2:  30% | abe: 8.583 | eve: 9.107 | bob: 8.511Epoch   2:  31% | abe: 8.579 | eve: 9.105 | bob: 8.508Epoch   2:  32% | abe: 8.576 | eve: 9.106 | bob: 8.505Epoch   2:  32% | abe: 8.572 | eve: 9.106 | bob: 8.501Epoch   2:  33% | abe: 8.568 | eve: 9.105 | bob: 8.496Epoch   2:  34% | abe: 8.564 | eve: 9.104 | bob: 8.493Epoch   2:  35% | abe: 8.560 | eve: 9.105 | bob: 8.488Epoch   2:  35% | abe: 8.555 | eve: 9.104 | bob: 8.484Epoch   2:  36% | abe: 8.551 | eve: 9.103 | bob: 8.480Epoch   2:  37% | abe: 8.547 | eve: 9.103 | bob: 8.476Epoch   2:  38% | abe: 8.542 | eve: 9.105 | bob: 8.470Epoch   2:  39% | abe: 8.537 | eve: 9.105 | bob: 8.465Epoch   2:  39% | abe: 8.532 | eve: 9.105 | bob: 8.461Epoch   2:  40% | abe: 8.527 | eve: 9.107 | bob: 8.455Epoch   2:  41% | abe: 8.523 | eve: 9.107 | bob: 8.452Epoch   2:  42% | abe: 8.519 | eve: 9.106 | bob: 8.447Epoch   2:  42% | abe: 8.515 | eve: 9.105 | bob: 8.443Epoch   2:  43% | abe: 8.509 | eve: 9.107 | bob: 8.438Epoch   2:  44% | abe: 8.503 | eve: 9.106 | bob: 8.432Epoch   2:  45% | abe: 8.497 | eve: 9.105 | bob: 8.427Epoch   2:  46% | abe: 8.491 | eve: 9.104 | bob: 8.420Epoch   2:  46% | abe: 8.485 | eve: 9.103 | bob: 8.414Epoch   2:  47% | abe: 8.479 | eve: 9.104 | bob: 8.409Epoch   2:  48% | abe: 8.472 | eve: 9.103 | bob: 8.402Epoch   2:  49% | abe: 8.465 | eve: 9.104 | bob: 8.396Epoch   2:  50% | abe: 8.460 | eve: 9.104 | bob: 8.392Epoch   2:  50% | abe: 8.454 | eve: 9.103 | bob: 8.386Epoch   2:  51% | abe: 8.448 | eve: 9.103 | bob: 8.381Epoch   2:  52% | abe: 8.443 | eve: 9.102 | bob: 8.375Epoch   2:  53% | abe: 8.437 | eve: 9.103 | bob: 8.370Epoch   2:  53% | abe: 8.432 | eve: 9.103 | bob: 8.365Epoch   2:  54% | abe: 8.427 | eve: 9.103 | bob: 8.360Epoch   2:  55% | abe: 8.423 | eve: 9.103 | bob: 8.356Epoch   2:  56% | abe: 8.419 | eve: 9.104 | bob: 8.352Epoch   2:  57% | abe: 8.415 | eve: 9.104 | bob: 8.348Epoch   2:  57% | abe: 8.409 | eve: 9.105 | bob: 8.344Epoch   2:  58% | abe: 8.405 | eve: 9.104 | bob: 8.340Epoch   2:  59% | abe: 8.401 | eve: 9.105 | bob: 8.335Epoch   2:  60% | abe: 8.395 | eve: 9.105 | bob: 8.330Epoch   2:  60% | abe: 8.390 | eve: 9.105 | bob: 8.325Epoch   2:  61% | abe: 8.384 | eve: 9.105 | bob: 8.319Epoch   2:  62% | abe: 8.378 | eve: 9.104 | bob: 8.313Epoch   2:  63% | abe: 8.371 | eve: 9.105 | bob: 8.307Epoch   2:  64% | abe: 8.366 | eve: 9.104 | bob: 8.301Epoch   2:  64% | abe: 8.360 | eve: 9.103 | bob: 8.295Epoch   2:  65% | abe: 8.353 | eve: 9.103 | bob: 8.289Epoch   2:  66% | abe: 8.347 | eve: 9.103 | bob: 8.283Epoch   2:  67% | abe: 8.341 | eve: 9.103 | bob: 8.277Epoch   2:  67% | abe: 8.334 | eve: 9.104 | bob: 8.271Epoch   2:  68% | abe: 8.328 | eve: 9.104 | bob: 8.266Epoch   2:  69% | abe: 8.322 | eve: 9.105 | bob: 8.259Epoch   2:  70% | abe: 8.315 | eve: 9.105 | bob: 8.254Epoch   2:  71% | abe: 8.309 | eve: 9.104 | bob: 8.248Epoch   2:  71% | abe: 8.302 | eve: 9.105 | bob: 8.241Epoch   2:  72% | abe: 8.296 | eve: 9.105 | bob: 8.235Epoch   2:  73% | abe: 8.290 | eve: 9.104 | bob: 8.228Epoch   2:  74% | abe: 8.283 | eve: 9.104 | bob: 8.223Epoch   2:  75% | abe: 8.277 | eve: 9.105 | bob: 8.217Epoch   2:  75% | abe: 8.270 | eve: 9.105 | bob: 8.210Epoch   2:  76% | abe: 8.264 | eve: 9.105 | bob: 8.204Epoch   2:  77% | abe: 8.257 | eve: 9.105 | bob: 8.197Epoch   2:  78% | abe: 8.250 | eve: 9.104 | bob: 8.190Epoch   2:  78% | abe: 8.243 | eve: 9.104 | bob: 8.184Epoch   2:  79% | abe: 8.236 | eve: 9.104 | bob: 8.177Epoch   2:  80% | abe: 8.230 | eve: 9.103 | bob: 8.170Epoch   2:  81% | abe: 8.222 | eve: 9.102 | bob: 8.163Epoch   2:  82% | abe: 8.215 | eve: 9.102 | bob: 8.157Epoch   2:  82% | abe: 8.208 | eve: 9.102 | bob: 8.149Epoch   2:  83% | abe: 8.201 | eve: 9.102 | bob: 8.143Epoch   2:  84% | abe: 8.194 | eve: 9.103 | bob: 8.135Epoch   2:  85% | abe: 8.186 | eve: 9.103 | bob: 8.128Epoch   2:  85% | abe: 8.179 | eve: 9.103 | bob: 8.121Epoch   2:  86% | abe: 8.171 | eve: 9.102 | bob: 8.114Epoch   2:  87% | abe: 8.164 | eve: 9.102 | bob: 8.106Epoch   2:  88% | abe: 8.156 | eve: 9.102 | bob: 8.098Epoch   2:  89% | abe: 8.148 | eve: 9.101 | bob: 8.091Epoch   2:  89% | abe: 8.140 | eve: 9.101 | bob: 8.083Epoch   2:  90% | abe: 8.132 | eve: 9.101 | bob: 8.076Epoch   2:  91% | abe: 8.124 | eve: 9.102 | bob: 8.069Epoch   2:  92% | abe: 8.117 | eve: 9.102 | bob: 8.061Epoch   2:  92% | abe: 8.109 | eve: 9.102 | bob: 8.054Epoch   2:  93% | abe: 8.101 | eve: 9.101 | bob: 8.047Epoch   2:  94% | abe: 8.094 | eve: 9.101 | bob: 8.040Epoch   2:  95% | abe: 8.087 | eve: 9.100 | bob: 8.033Epoch   2:  96% | abe: 8.079 | eve: 9.100 | bob: 8.025Epoch   2:  96% | abe: 8.070 | eve: 9.100 | bob: 8.018Epoch   2:  97% | abe: 8.064 | eve: 9.101 | bob: 8.012Epoch   2:  98% | abe: 8.056 | eve: 9.101 | bob: 8.005Epoch   2:  99% | abe: 8.049 | eve: 9.101 | bob: 7.998
New best Bob loss 7.9976409802206945 at epoch 2
Epoch   3:   0% | abe: 7.030 | eve: 9.083 | bob: 7.100Epoch   3:   0% | abe: 7.036 | eve: 9.098 | bob: 7.068Epoch   3:   1% | abe: 7.033 | eve: 9.115 | bob: 7.048Epoch   3:   2% | abe: 7.023 | eve: 9.123 | bob: 7.030Epoch   3:   3% | abe: 7.007 | eve: 9.110 | bob: 7.022Epoch   3:   3% | abe: 6.989 | eve: 9.112 | bob: 7.000Epoch   3:   4% | abe: 6.974 | eve: 9.108 | bob: 6.985Epoch   3:   5% | abe: 6.966 | eve: 9.113 | bob: 6.982Epoch   3:   6% | abe: 6.956 | eve: 9.111 | bob: 6.974Epoch   3:   7% | abe: 6.945 | eve: 9.111 | bob: 6.962Epoch   3:   7% | abe: 6.937 | eve: 9.113 | bob: 6.954Epoch   3:   8% | abe: 6.927 | eve: 9.123 | bob: 6.947Epoch   3:   9% | abe: 6.918 | eve: 9.120 | bob: 6.939Epoch   3:  10% | abe: 6.908 | eve: 9.115 | bob: 6.930Epoch   3:  10% | abe: 6.900 | eve: 9.110 | bob: 6.924Epoch   3:  11% | abe: 6.891 | eve: 9.107 | bob: 6.916Epoch   3:  12% | abe: 6.881 | eve: 9.107 | bob: 6.908Epoch   3:  13% | abe: 6.869 | eve: 9.105 | bob: 6.898Epoch   3:  14% | abe: 6.858 | eve: 9.102 | bob: 6.889Epoch   3:  14% | abe: 6.848 | eve: 9.102 | bob: 6.880Epoch   3:  15% | abe: 6.838 | eve: 9.099 | bob: 6.870Epoch   3:  16% | abe: 6.831 | eve: 9.097 | bob: 6.865Epoch   3:  17% | abe: 6.820 | eve: 9.100 | bob: 6.855Epoch   3:  17% | abe: 6.812 | eve: 9.100 | bob: 6.849Epoch   3:  18% | abe: 6.802 | eve: 9.098 | bob: 6.841Epoch   3:  19% | abe: 6.796 | eve: 9.098 | bob: 6.834Epoch   3:  20% | abe: 6.787 | eve: 9.097 | bob: 6.827Epoch   3:  21% | abe: 6.778 | eve: 9.096 | bob: 6.821Epoch   3:  21% | abe: 6.771 | eve: 9.099 | bob: 6.816Epoch   3:  22% | abe: 6.763 | eve: 9.099 | bob: 6.807Epoch   3:  23% | abe: 6.754 | eve: 9.102 | bob: 6.799Epoch   3:  24% | abe: 6.748 | eve: 9.098 | bob: 6.795Epoch   3:  25% | abe: 6.742 | eve: 9.100 | bob: 6.788Epoch   3:  25% | abe: 6.734 | eve: 9.101 | bob: 6.783Epoch   3:  26% | abe: 6.728 | eve: 9.098 | bob: 6.778Epoch   3:  27% | abe: 6.722 | eve: 9.097 | bob: 6.771Epoch   3:  28% | abe: 6.713 | eve: 9.098 | bob: 6.766Epoch   3:  28% | abe: 6.707 | eve: 9.099 | bob: 6.761Epoch   3:  29% | abe: 6.701 | eve: 9.100 | bob: 6.755Epoch   3:  30% | abe: 6.694 | eve: 9.099 | bob: 6.750Epoch   3:  31% | abe: 6.687 | eve: 9.099 | bob: 6.744Epoch   3:  32% | abe: 6.679 | eve: 9.100 | bob: 6.737Epoch   3:  32% | abe: 6.671 | eve: 9.100 | bob: 6.731Epoch   3:  33% | abe: 6.664 | eve: 9.099 | bob: 6.725Epoch   3:  34% | abe: 6.656 | eve: 9.100 | bob: 6.718Epoch   3:  35% | abe: 6.649 | eve: 9.099 | bob: 6.712Epoch   3:  35% | abe: 6.643 | eve: 9.099 | bob: 6.707Epoch   3:  36% | abe: 6.634 | eve: 9.099 | bob: 6.700Epoch   3:  37% | abe: 6.624 | eve: 9.100 | bob: 6.694Epoch   3:  38% | abe: 6.618 | eve: 9.100 | bob: 6.687Epoch   3:  39% | abe: 6.610 | eve: 9.100 | bob: 6.680Epoch   3:  39% | abe: 6.602 | eve: 9.098 | bob: 6.674Epoch   3:  40% | abe: 6.593 | eve: 9.099 | bob: 6.667Epoch   3:  41% | abe: 6.585 | eve: 9.098 | bob: 6.660Epoch   3:  42% | abe: 6.576 | eve: 9.097 | bob: 6.654Epoch   3:  42% | abe: 6.569 | eve: 9.095 | bob: 6.648Epoch   3:  43% | abe: 6.562 | eve: 9.095 | bob: 6.642Epoch   3:  44% | abe: 6.555 | eve: 9.096 | bob: 6.636Epoch   3:  45% | abe: 6.546 | eve: 9.096 | bob: 6.630Epoch   3:  46% | abe: 6.539 | eve: 9.097 | bob: 6.624Epoch   3:  46% | abe: 6.531 | eve: 9.098 | bob: 6.618Epoch   3:  47% | abe: 6.523 | eve: 9.098 | bob: 6.610Epoch   3:  48% | abe: 6.514 | eve: 9.098 | bob: 6.603Epoch   3:  49% | abe: 6.506 | eve: 9.100 | bob: 6.596Epoch   3:  50% | abe: 6.497 | eve: 9.101 | bob: 6.589Epoch   3:  50% | abe: 6.490 | eve: 9.102 | bob: 6.585Epoch   3:  51% | abe: 6.484 | eve: 9.101 | bob: 6.578Epoch   3:  52% | abe: 6.477 | eve: 9.100 | bob: 6.573Epoch   3:  53% | abe: 6.470 | eve: 9.100 | bob: 6.567Epoch   3:  53% | abe: 6.462 | eve: 9.100 | bob: 6.561Epoch   3:  54% | abe: 6.455 | eve: 9.100 | bob: 6.556Epoch   3:  55% | abe: 6.450 | eve: 9.099 | bob: 6.553Epoch   3:  56% | abe: 6.444 | eve: 9.099 | bob: 6.549Epoch   3:  57% | abe: 6.439 | eve: 9.099 | bob: 6.544Epoch   3:  57% | abe: 6.433 | eve: 9.098 | bob: 6.539Epoch   3:  58% | abe: 6.426 | eve: 9.099 | bob: 6.533Epoch   3:  59% | abe: 6.420 | eve: 9.099 | bob: 6.528Epoch   3:  60% | abe: 6.414 | eve: 9.099 | bob: 6.522Epoch   3:  60% | abe: 6.407 | eve: 9.100 | bob: 6.516Epoch   3:  61% | abe: 6.400 | eve: 9.100 | bob: 6.510Epoch   3:  62% | abe: 6.393 | eve: 9.100 | bob: 6.503Epoch   3:  63% | abe: 6.386 | eve: 9.101 | bob: 6.498Epoch   3:  64% | abe: 6.380 | eve: 9.101 | bob: 6.492Epoch   3:  64% | abe: 6.373 | eve: 9.101 | bob: 6.486Epoch   3:  65% | abe: 6.366 | eve: 9.101 | bob: 6.479Epoch   3:  66% | abe: 6.358 | eve: 9.101 | bob: 6.473Epoch   3:  67% | abe: 6.351 | eve: 9.101 | bob: 6.466Epoch   3:  67% | abe: 6.344 | eve: 9.100 | bob: 6.460Epoch   3:  68% | abe: 6.339 | eve: 9.100 | bob: 6.455Epoch   3:  69% | abe: 6.332 | eve: 9.101 | bob: 6.448Epoch   3:  70% | abe: 6.324 | eve: 9.100 | bob: 6.441Epoch   3:  71% | abe: 6.317 | eve: 9.101 | bob: 6.435Epoch   3:  71% | abe: 6.311 | eve: 9.101 | bob: 6.429Epoch   3:  72% | abe: 6.304 | eve: 9.101 | bob: 6.422Epoch   3:  73% | abe: 6.297 | eve: 9.101 | bob: 6.415Epoch   3:  74% | abe: 6.290 | eve: 9.101 | bob: 6.409Epoch   3:  75% | abe: 6.282 | eve: 9.101 | bob: 6.402Epoch   3:  75% | abe: 6.275 | eve: 9.102 | bob: 6.395Epoch   3:  76% | abe: 6.268 | eve: 9.102 | bob: 6.389Epoch   3:  77% | abe: 6.261 | eve: 9.101 | bob: 6.382Epoch   3:  78% | abe: 6.254 | eve: 9.102 | bob: 6.376Epoch   3:  78% | abe: 6.246 | eve: 9.101 | bob: 6.369Epoch   3:  79% | abe: 6.239 | eve: 9.102 | bob: 6.363Epoch   3:  80% | abe: 6.232 | eve: 9.102 | bob: 6.356Epoch   3:  81% | abe: 6.225 | eve: 9.102 | bob: 6.351Epoch   3:  82% | abe: 6.218 | eve: 9.103 | bob: 6.345Epoch   3:  82% | abe: 6.211 | eve: 9.103 | bob: 6.338Epoch   3:  83% | abe: 6.204 | eve: 9.104 | bob: 6.331Epoch   3:  84% | abe: 6.196 | eve: 9.104 | bob: 6.325Epoch   3:  85% | abe: 6.190 | eve: 9.104 | bob: 6.318Epoch   3:  85% | abe: 6.183 | eve: 9.105 | bob: 6.313Epoch   3:  86% | abe: 6.176 | eve: 9.106 | bob: 6.307Epoch   3:  87% | abe: 6.169 | eve: 9.106 | bob: 6.300Epoch   3:  88% | abe: 6.161 | eve: 9.105 | bob: 6.294Epoch   3:  89% | abe: 6.155 | eve: 9.105 | bob: 6.289Epoch   3:  89% | abe: 6.149 | eve: 9.105 | bob: 6.284Epoch   3:  90% | abe: 6.143 | eve: 9.105 | bob: 6.280Epoch   3:  91% | abe: 6.139 | eve: 9.105 | bob: 6.275Epoch   3:  92% | abe: 6.133 | eve: 9.104 | bob: 6.270Epoch   3:  92% | abe: 6.128 | eve: 9.105 | bob: 6.265Epoch   3:  93% | abe: 6.121 | eve: 9.106 | bob: 6.259Epoch   3:  94% | abe: 6.114 | eve: 9.106 | bob: 6.253Epoch   3:  95% | abe: 6.108 | eve: 9.106 | bob: 6.248Epoch   3:  96% | abe: 6.102 | eve: 9.106 | bob: 6.243Epoch   3:  96% | abe: 6.097 | eve: 9.106 | bob: 6.238Epoch   3:  97% | abe: 6.091 | eve: 9.106 | bob: 6.232Epoch   3:  98% | abe: 6.084 | eve: 9.107 | bob: 6.227Epoch   3:  99% | abe: 6.079 | eve: 9.107 | bob: 6.222
New best Bob loss 6.221893054512066 at epoch 3
Epoch   4:   0% | abe: 5.305 | eve: 9.183 | bob: 5.414Epoch   4:   0% | abe: 5.256 | eve: 9.196 | bob: 5.430Epoch   4:   1% | abe: 5.264 | eve: 9.177 | bob: 5.488Epoch   4:   2% | abe: 5.241 | eve: 9.148 | bob: 5.468Epoch   4:   3% | abe: 5.251 | eve: 9.139 | bob: 5.472Epoch   4:   3% | abe: 5.266 | eve: 9.143 | bob: 5.478Epoch   4:   4% | abe: 5.270 | eve: 9.143 | bob: 5.472Epoch   4:   5% | abe: 5.240 | eve: 9.133 | bob: 5.450Epoch   4:   6% | abe: 5.239 | eve: 9.137 | bob: 5.455Epoch   4:   7% | abe: 5.236 | eve: 9.141 | bob: 5.451Epoch   4:   7% | abe: 5.232 | eve: 9.130 | bob: 5.440Epoch   4:   8% | abe: 5.228 | eve: 9.132 | bob: 5.437Epoch   4:   9% | abe: 5.229 | eve: 9.125 | bob: 5.437Epoch   4:  10% | abe: 5.221 | eve: 9.124 | bob: 5.435Epoch   4:  10% | abe: 5.218 | eve: 9.124 | bob: 5.430Epoch   4:  11% | abe: 5.216 | eve: 9.123 | bob: 5.425Epoch   4:  12% | abe: 5.206 | eve: 9.120 | bob: 5.419Epoch   4:  13% | abe: 5.202 | eve: 9.124 | bob: 5.414Epoch   4:  14% | abe: 5.197 | eve: 9.128 | bob: 5.406Epoch   4:  14% | abe: 5.191 | eve: 9.127 | bob: 5.402Epoch   4:  15% | abe: 5.190 | eve: 9.127 | bob: 5.399Epoch   4:  16% | abe: 5.185 | eve: 9.130 | bob: 5.393Epoch   4:  17% | abe: 5.180 | eve: 9.129 | bob: 5.390Epoch   4:  17% | abe: 5.177 | eve: 9.131 | bob: 5.387Epoch   4:  18% | abe: 5.178 | eve: 9.132 | bob: 5.388Epoch   4:  19% | abe: 5.180 | eve: 9.131 | bob: 5.387Epoch   4:  20% | abe: 5.176 | eve: 9.132 | bob: 5.384Epoch   4:  21% | abe: 5.169 | eve: 9.130 | bob: 5.376Epoch   4:  21% | abe: 5.168 | eve: 9.130 | bob: 5.372Epoch   4:  22% | abe: 5.165 | eve: 9.132 | bob: 5.369Epoch   4:  23% | abe: 5.164 | eve: 9.131 | bob: 5.364Epoch   4:  24% | abe: 5.160 | eve: 9.135 | bob: 5.362Epoch   4:  25% | abe: 5.159 | eve: 9.136 | bob: 5.358Epoch   4:  25% | abe: 5.156 | eve: 9.135 | bob: 5.354Epoch   4:  26% | abe: 5.153 | eve: 9.134 | bob: 5.352Epoch   4:  27% | abe: 5.151 | eve: 9.134 | bob: 5.350Epoch   4:  28% | abe: 5.152 | eve: 9.136 | bob: 5.348Epoch   4:  28% | abe: 5.149 | eve: 9.136 | bob: 5.343Epoch   4:  29% | abe: 5.144 | eve: 9.134 | bob: 5.338Epoch   4:  30% | abe: 5.142 | eve: 9.134 | bob: 5.334Epoch   4:  31% | abe: 5.140 | eve: 9.134 | bob: 5.331Epoch   4:  32% | abe: 5.139 | eve: 9.134 | bob: 5.329Epoch   4:  32% | abe: 5.139 | eve: 9.134 | bob: 5.329Epoch   4:  33% | abe: 5.138 | eve: 9.134 | bob: 5.327Epoch   4:  34% | abe: 5.136 | eve: 9.134 | bob: 5.324Epoch   4:  35% | abe: 5.134 | eve: 9.136 | bob: 5.322Epoch   4:  35% | abe: 5.132 | eve: 9.136 | bob: 5.319Epoch   4:  36% | abe: 5.131 | eve: 9.135 | bob: 5.317Epoch   4:  37% | abe: 5.128 | eve: 9.135 | bob: 5.313Epoch   4:  38% | abe: 5.127 | eve: 9.135 | bob: 5.311Epoch   4:  39% | abe: 5.125 | eve: 9.136 | bob: 5.309Epoch   4:  39% | abe: 5.125 | eve: 9.136 | bob: 5.308Epoch   4:  40% | abe: 5.124 | eve: 9.135 | bob: 5.306Epoch   4:  41% | abe: 5.121 | eve: 9.133 | bob: 5.302Epoch   4:  42% | abe: 5.117 | eve: 9.134 | bob: 5.298Epoch   4:  42% | abe: 5.114 | eve: 9.132 | bob: 5.295Epoch   4:  43% | abe: 5.112 | eve: 9.132 | bob: 5.293Epoch   4:  44% | abe: 5.110 | eve: 9.132 | bob: 5.290Epoch   4:  45% | abe: 5.108 | eve: 9.132 | bob: 5.288Epoch   4:  46% | abe: 5.108 | eve: 9.132 | bob: 5.287Epoch   4:  46% | abe: 5.107 | eve: 9.132 | bob: 5.286Epoch   4:  47% | abe: 5.106 | eve: 9.132 | bob: 5.285Epoch   4:  48% | abe: 5.105 | eve: 9.131 | bob: 5.284Epoch   4:  49% | abe: 5.104 | eve: 9.130 | bob: 5.282Epoch   4:  50% | abe: 5.101 | eve: 9.129 | bob: 5.279Epoch   4:  50% | abe: 5.099 | eve: 9.129 | bob: 5.276Epoch   4:  51% | abe: 5.098 | eve: 9.130 | bob: 5.275Epoch   4:  52% | abe: 5.096 | eve: 9.130 | bob: 5.273Epoch   4:  53% | abe: 5.094 | eve: 9.128 | bob: 5.271Epoch   4:  53% | abe: 5.093 | eve: 9.128 | bob: 5.269Epoch   4:  54% | abe: 5.092 | eve: 9.127 | bob: 5.268Epoch   4:  55% | abe: 5.092 | eve: 9.127 | bob: 5.267Epoch   4:  56% | abe: 5.090 | eve: 9.128 | bob: 5.264Epoch   4:  57% | abe: 5.089 | eve: 9.126 | bob: 5.263Epoch   4:  57% | abe: 5.088 | eve: 9.126 | bob: 5.262Epoch   4:  58% | abe: 5.086 | eve: 9.126 | bob: 5.259Epoch   4:  59% | abe: 5.085 | eve: 9.124 | bob: 5.258Epoch   4:  60% | abe: 5.084 | eve: 9.124 | bob: 5.256Epoch   4:  60% | abe: 5.083 | eve: 9.123 | bob: 5.255Epoch   4:  61% | abe: 5.081 | eve: 9.122 | bob: 5.252Epoch   4:  62% | abe: 5.079 | eve: 9.122 | bob: 5.250Epoch   4:  63% | abe: 5.078 | eve: 9.123 | bob: 5.248Epoch   4:  64% | abe: 5.077 | eve: 9.123 | bob: 5.247Epoch   4:  64% | abe: 5.077 | eve: 9.122 | bob: 5.246Epoch   4:  65% | abe: 5.075 | eve: 9.122 | bob: 5.244Epoch   4:  66% | abe: 5.074 | eve: 9.121 | bob: 5.242Epoch   4:  67% | abe: 5.072 | eve: 9.122 | bob: 5.241Epoch   4:  67% | abe: 5.071 | eve: 9.121 | bob: 5.239Epoch   4:  68% | abe: 5.069 | eve: 9.121 | bob: 5.236Epoch   4:  69% | abe: 5.068 | eve: 9.120 | bob: 5.235Epoch   4:  70% | abe: 5.068 | eve: 9.121 | bob: 5.235Epoch   4:  71% | abe: 5.067 | eve: 9.121 | bob: 5.233Epoch   4:  71% | abe: 5.066 | eve: 9.120 | bob: 5.232Epoch   4:  72% | abe: 5.065 | eve: 9.121 | bob: 5.231Epoch   4:  73% | abe: 5.064 | eve: 9.121 | bob: 5.229Epoch   4:  74% | abe: 5.063 | eve: 9.121 | bob: 5.228Epoch   4:  75% | abe: 5.063 | eve: 9.122 | bob: 5.227Epoch   4:  75% | abe: 5.062 | eve: 9.122 | bob: 5.226Epoch   4:  76% | abe: 5.061 | eve: 9.122 | bob: 5.225Epoch   4:  77% | abe: 5.060 | eve: 9.121 | bob: 5.225Epoch   4:  78% | abe: 5.059 | eve: 9.122 | bob: 5.223Epoch   4:  78% | abe: 5.058 | eve: 9.122 | bob: 5.222Epoch   4:  79% | abe: 5.057 | eve: 9.122 | bob: 5.220Epoch   4:  80% | abe: 5.056 | eve: 9.122 | bob: 5.219Epoch   4:  81% | abe: 5.055 | eve: 9.122 | bob: 5.217Epoch   4:  82% | abe: 5.054 | eve: 9.122 | bob: 5.216Epoch   4:  82% | abe: 5.053 | eve: 9.122 | bob: 5.215Epoch   4:  83% | abe: 5.052 | eve: 9.122 | bob: 5.214Epoch   4:  84% | abe: 5.052 | eve: 9.121 | bob: 5.213Epoch   4:  85% | abe: 5.051 | eve: 9.121 | bob: 5.212Epoch   4:  85% | abe: 5.050 | eve: 9.121 | bob: 5.210Epoch   4:  86% | abe: 5.049 | eve: 9.120 | bob: 5.209Epoch   4:  87% | abe: 5.048 | eve: 9.120 | bob: 5.208Epoch   4:  88% | abe: 5.047 | eve: 9.119 | bob: 5.207Epoch   4:  89% | abe: 5.046 | eve: 9.119 | bob: 5.205Epoch   4:  89% | abe: 5.045 | eve: 9.119 | bob: 5.204Epoch   4:  90% | abe: 5.045 | eve: 9.119 | bob: 5.203Epoch   4:  91% | abe: 5.044 | eve: 9.119 | bob: 5.203Epoch   4:  92% | abe: 5.043 | eve: 9.120 | bob: 5.202Epoch   4:  92% | abe: 5.043 | eve: 9.120 | bob: 5.201Epoch   4:  93% | abe: 5.042 | eve: 9.121 | bob: 5.200Epoch   4:  94% | abe: 5.042 | eve: 9.121 | bob: 5.199Epoch   4:  95% | abe: 5.041 | eve: 9.121 | bob: 5.199Epoch   4:  96% | abe: 5.041 | eve: 9.121 | bob: 5.198Epoch   4:  96% | abe: 5.040 | eve: 9.121 | bob: 5.197Epoch   4:  97% | abe: 5.039 | eve: 9.121 | bob: 5.196Epoch   4:  98% | abe: 5.038 | eve: 9.121 | bob: 5.194Epoch   4:  99% | abe: 5.038 | eve: 9.122 | bob: 5.194
New best Bob loss 5.193705078803077 at epoch 4
Epoch   5:   0% | abe: 4.993 | eve: 9.179 | bob: 5.125Epoch   5:   0% | abe: 4.970 | eve: 9.165 | bob: 5.103Epoch   5:   1% | abe: 4.960 | eve: 9.161 | bob: 5.088Epoch   5:   2% | abe: 4.966 | eve: 9.150 | bob: 5.088Epoch   5:   3% | abe: 4.970 | eve: 9.154 | bob: 5.091Epoch   5:   3% | abe: 4.974 | eve: 9.163 | bob: 5.089Epoch   5:   4% | abe: 4.976 | eve: 9.153 | bob: 5.090Epoch   5:   5% | abe: 4.977 | eve: 9.155 | bob: 5.091Epoch   5:   6% | abe: 4.972 | eve: 9.154 | bob: 5.088Epoch   5:   7% | abe: 4.975 | eve: 9.145 | bob: 5.093Epoch   5:   7% | abe: 4.975 | eve: 9.149 | bob: 5.095Epoch   5:   8% | abe: 4.977 | eve: 9.150 | bob: 5.098Epoch   5:   9% | abe: 4.979 | eve: 9.151 | bob: 5.101Epoch   5:  10% | abe: 4.980 | eve: 9.151 | bob: 5.101Epoch   5:  10% | abe: 4.977 | eve: 9.148 | bob: 5.095Epoch   5:  11% | abe: 4.978 | eve: 9.146 | bob: 5.097Epoch   5:  12% | abe: 4.978 | eve: 9.152 | bob: 5.096Epoch   5:  13% | abe: 4.977 | eve: 9.150 | bob: 5.095Epoch   5:  14% | abe: 4.980 | eve: 9.149 | bob: 5.100Epoch   5:  14% | abe: 4.971 | eve: 9.149 | bob: 5.093Epoch   5:  15% | abe: 4.967 | eve: 9.148 | bob: 5.090Epoch   5:  16% | abe: 4.968 | eve: 9.149 | bob: 5.091Epoch   5:  17% | abe: 4.968 | eve: 9.148 | bob: 5.093Epoch   5:  17% | abe: 4.969 | eve: 9.149 | bob: 5.095Epoch   5:  18% | abe: 4.967 | eve: 9.150 | bob: 5.094Epoch   5:  19% | abe: 4.968 | eve: 9.149 | bob: 5.095Epoch   5:  20% | abe: 4.973 | eve: 9.148 | bob: 5.099Epoch   5:  21% | abe: 4.973 | eve: 9.148 | bob: 5.099Epoch   5:  21% | abe: 4.972 | eve: 9.147 | bob: 5.098Epoch   5:  22% | abe: 4.971 | eve: 9.147 | bob: 5.095Epoch   5:  23% | abe: 4.973 | eve: 9.145 | bob: 5.097Epoch   5:  24% | abe: 4.974 | eve: 9.143 | bob: 5.097Epoch   5:  25% | abe: 4.975 | eve: 9.143 | bob: 5.099Epoch   5:  25% | abe: 4.975 | eve: 9.142 | bob: 5.099Epoch   5:  26% | abe: 4.978 | eve: 9.141 | bob: 5.102Epoch   5:  27% | abe: 4.978 | eve: 9.143 | bob: 5.102Epoch   5:  28% | abe: 4.977 | eve: 9.142 | bob: 5.102Epoch   5:  28% | abe: 4.977 | eve: 9.141 | bob: 5.102Epoch   5:  29% | abe: 4.977 | eve: 9.142 | bob: 5.101Epoch   5:  30% | abe: 4.976 | eve: 9.142 | bob: 5.099Epoch   5:  31% | abe: 4.977 | eve: 9.141 | bob: 5.099Epoch   5:  32% | abe: 4.976 | eve: 9.141 | bob: 5.098Epoch   5:  32% | abe: 4.975 | eve: 9.143 | bob: 5.097Epoch   5:  33% | abe: 4.975 | eve: 9.142 | bob: 5.097Epoch   5:  34% | abe: 4.976 | eve: 9.142 | bob: 5.098Epoch   5:  35% | abe: 4.976 | eve: 9.143 | bob: 5.098Epoch   5:  35% | abe: 4.977 | eve: 9.142 | bob: 5.099Epoch   5:  36% | abe: 4.977 | eve: 9.142 | bob: 5.099Epoch   5:  37% | abe: 4.978 | eve: 9.142 | bob: 5.099Epoch   5:  38% | abe: 4.979 | eve: 9.140 | bob: 5.100Epoch   5:  39% | abe: 4.976 | eve: 9.141 | bob: 5.098Epoch   5:  39% | abe: 4.975 | eve: 9.141 | bob: 5.096Epoch   5:  40% | abe: 4.977 | eve: 9.142 | bob: 5.098Epoch   5:  41% | abe: 4.976 | eve: 9.142 | bob: 5.096Epoch   5:  42% | abe: 4.975 | eve: 9.142 | bob: 5.095Epoch   5:  42% | abe: 4.976 | eve: 9.141 | bob: 5.095Epoch   5:  43% | abe: 4.975 | eve: 9.140 | bob: 5.094Epoch   5:  44% | abe: 4.976 | eve: 9.141 | bob: 5.094Epoch   5:  45% | abe: 4.975 | eve: 9.141 | bob: 5.093Epoch   5:  46% | abe: 4.974 | eve: 9.142 | bob: 5.093Epoch   5:  46% | abe: 4.974 | eve: 9.142 | bob: 5.092Epoch   5:  47% | abe: 4.973 | eve: 9.141 | bob: 5.091Epoch   5:  48% | abe: 4.973 | eve: 9.142 | bob: 5.091Epoch   5:  49% | abe: 4.973 | eve: 9.143 | bob: 5.091Epoch   5:  50% | abe: 4.972 | eve: 9.142 | bob: 5.090Epoch   5:  50% | abe: 4.971 | eve: 9.141 | bob: 5.088Epoch   5:  51% | abe: 4.971 | eve: 9.140 | bob: 5.088Epoch   5:  52% | abe: 4.971 | eve: 9.140 | bob: 5.087Epoch   5:  53% | abe: 4.970 | eve: 9.140 | bob: 5.086Epoch   5:  53% | abe: 4.970 | eve: 9.140 | bob: 5.086Epoch   5:  54% | abe: 4.972 | eve: 9.140 | bob: 5.087Epoch   5:  55% | abe: 4.972 | eve: 9.142 | bob: 5.087Epoch   5:  56% | abe: 4.972 | eve: 9.141 | bob: 5.087Epoch   5:  57% | abe: 4.972 | eve: 9.142 | bob: 5.087Epoch   5:  57% | abe: 4.972 | eve: 9.142 | bob: 5.087Epoch   5:  58% | abe: 4.973 | eve: 9.142 | bob: 5.088Epoch   5:  59% | abe: 4.973 | eve: 9.142 | bob: 5.088Epoch   5:  60% | abe: 4.973 | eve: 9.141 | bob: 5.088Epoch   5:  60% | abe: 4.974 | eve: 9.141 | bob: 5.088Epoch   5:  61% | abe: 4.974 | eve: 9.141 | bob: 5.087Epoch   5:  62% | abe: 4.973 | eve: 9.142 | bob: 5.087Epoch   5:  63% | abe: 4.974 | eve: 9.142 | bob: 5.087Epoch   5:  64% | abe: 4.972 | eve: 9.141 | bob: 5.085Epoch   5:  64% | abe: 4.972 | eve: 9.141 | bob: 5.084Epoch   5:  65% | abe: 4.972 | eve: 9.142 | bob: 5.084Epoch   5:  66% | abe: 4.972 | eve: 9.142 | bob: 5.084Epoch   5:  67% | abe: 4.972 | eve: 9.142 | bob: 5.084Epoch   5:  67% | abe: 4.972 | eve: 9.141 | bob: 5.083Epoch   5:  68% | abe: 4.973 | eve: 9.141 | bob: 5.084Epoch   5:  69% | abe: 4.973 | eve: 9.141 | bob: 5.084Epoch   5:  70% | abe: 4.974 | eve: 9.141 | bob: 5.085Epoch   5:  71% | abe: 4.973 | eve: 9.141 | bob: 5.084Epoch   5:  71% | abe: 4.973 | eve: 9.141 | bob: 5.084Epoch   5:  72% | abe: 4.973 | eve: 9.140 | bob: 5.084Epoch   5:  73% | abe: 4.974 | eve: 9.141 | bob: 5.084Epoch   5:  74% | abe: 4.974 | eve: 9.141 | bob: 5.084Epoch   5:  75% | abe: 4.974 | eve: 9.141 | bob: 5.084Epoch   5:  75% | abe: 4.975 | eve: 9.141 | bob: 5.085Epoch   5:  76% | abe: 4.974 | eve: 9.142 | bob: 5.084Epoch   5:  77% | abe: 4.974 | eve: 9.141 | bob: 5.084Epoch   5:  78% | abe: 4.974 | eve: 9.142 | bob: 5.084Epoch   5:  78% | abe: 4.974 | eve: 9.142 | bob: 5.083Epoch   5:  79% | abe: 4.974 | eve: 9.142 | bob: 5.083Epoch   5:  80% | abe: 4.974 | eve: 9.141 | bob: 5.083Epoch   5:  81% | abe: 4.973 | eve: 9.142 | bob: 5.083Epoch   5:  82% | abe: 4.973 | eve: 9.142 | bob: 5.083Epoch   5:  82% | abe: 4.972 | eve: 9.142 | bob: 5.082Epoch   5:  83% | abe: 4.973 | eve: 9.142 | bob: 5.082Epoch   5:  84% | abe: 4.972 | eve: 9.142 | bob: 5.081Epoch   5:  85% | abe: 4.972 | eve: 9.142 | bob: 5.081Epoch   5:  85% | abe: 4.972 | eve: 9.142 | bob: 5.081Epoch   5:  86% | abe: 4.972 | eve: 9.142 | bob: 5.081Epoch   5:  87% | abe: 4.973 | eve: 9.142 | bob: 5.081Epoch   5:  88% | abe: 4.973 | eve: 9.142 | bob: 5.082Epoch   5:  89% | abe: 4.973 | eve: 9.143 | bob: 5.081Epoch   5:  89% | abe: 4.972 | eve: 9.143 | bob: 5.080Epoch   5:  90% | abe: 4.972 | eve: 9.143 | bob: 5.079Epoch   5:  91% | abe: 4.972 | eve: 9.143 | bob: 5.080Epoch   5:  92% | abe: 4.972 | eve: 9.143 | bob: 5.080Epoch   5:  92% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  93% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  94% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  95% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  96% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  96% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  97% | abe: 4.973 | eve: 9.143 | bob: 5.080Epoch   5:  98% | abe: 4.973 | eve: 9.143 | bob: 5.079Epoch   5:  99% | abe: 4.973 | eve: 9.144 | bob: 5.079
New best Bob loss 5.079024867953649 at epoch 5
Epoch   6:   0% | abe: 4.978 | eve: 9.181 | bob: 5.063Epoch   6:   0% | abe: 4.969 | eve: 9.137 | bob: 5.060Epoch   6:   1% | abe: 4.975 | eve: 9.167 | bob: 5.070Epoch   6:   2% | abe: 4.982 | eve: 9.146 | bob: 5.085Epoch   6:   3% | abe: 4.986 | eve: 9.141 | bob: 5.093Epoch   6:   3% | abe: 4.987 | eve: 9.140 | bob: 5.092Epoch   6:   4% | abe: 4.978 | eve: 9.154 | bob: 5.082Epoch   6:   5% | abe: 4.973 | eve: 9.149 | bob: 5.075Epoch   6:   6% | abe: 4.970 | eve: 9.152 | bob: 5.068Epoch   6:   7% | abe: 4.968 | eve: 9.153 | bob: 5.064Epoch   6:   7% | abe: 4.969 | eve: 9.159 | bob: 5.064Epoch   6:   8% | abe: 4.965 | eve: 9.156 | bob: 5.060Epoch   6:   9% | abe: 4.966 | eve: 9.158 | bob: 5.061Epoch   6:  10% | abe: 4.962 | eve: 9.155 | bob: 5.058Epoch   6:  10% | abe: 4.962 | eve: 9.154 | bob: 5.059Epoch   6:  11% | abe: 4.961 | eve: 9.153 | bob: 5.058Epoch   6:  12% | abe: 4.960 | eve: 9.152 | bob: 5.059Epoch   6:  13% | abe: 4.962 | eve: 9.153 | bob: 5.061Epoch   6:  14% | abe: 4.959 | eve: 9.145 | bob: 5.059Epoch   6:  14% | abe: 4.960 | eve: 9.148 | bob: 5.060Epoch   6:  15% | abe: 4.959 | eve: 9.150 | bob: 5.057Epoch   6:  16% | abe: 4.957 | eve: 9.153 | bob: 5.055Epoch   6:  17% | abe: 4.953 | eve: 9.153 | bob: 5.049Epoch   6:  17% | abe: 4.955 | eve: 9.147 | bob: 5.051Epoch   6:  18% | abe: 4.953 | eve: 9.147 | bob: 5.049Epoch   6:  19% | abe: 4.952 | eve: 9.152 | bob: 5.049Epoch   6:  20% | abe: 4.950 | eve: 9.156 | bob: 5.048Epoch   6:  21% | abe: 4.952 | eve: 9.157 | bob: 5.049Epoch   6:  21% | abe: 4.952 | eve: 9.155 | bob: 5.050Epoch   6:  22% | abe: 4.954 | eve: 9.154 | bob: 5.051Epoch   6:  23% | abe: 4.955 | eve: 9.156 | bob: 5.052Epoch   6:  24% | abe: 4.954 | eve: 9.155 | bob: 5.050Epoch   6:  25% | abe: 4.954 | eve: 9.157 | bob: 5.050Epoch   6:  25% | abe: 4.957 | eve: 9.157 | bob: 5.053Epoch   6:  26% | abe: 4.957 | eve: 9.157 | bob: 5.053Epoch   6:  27% | abe: 4.959 | eve: 9.156 | bob: 5.055Epoch   6:  28% | abe: 4.959 | eve: 9.157 | bob: 5.055Epoch   6:  28% | abe: 4.956 | eve: 9.156 | bob: 5.052Epoch   6:  29% | abe: 4.956 | eve: 9.155 | bob: 5.052Epoch   6:  30% | abe: 4.956 | eve: 9.155 | bob: 5.051Epoch   6:  31% | abe: 4.957 | eve: 9.156 | bob: 5.052Epoch   6:  32% | abe: 4.958 | eve: 9.156 | bob: 5.053Epoch   6:  32% | abe: 4.958 | eve: 9.158 | bob: 5.052Epoch   6:  33% | abe: 4.959 | eve: 9.157 | bob: 5.053Epoch   6:  34% | abe: 4.961 | eve: 9.157 | bob: 5.056Epoch   6:  35% | abe: 4.962 | eve: 9.156 | bob: 5.056Epoch   6:  35% | abe: 4.961 | eve: 9.156 | bob: 5.055Epoch   6:  36% | abe: 4.961 | eve: 9.156 | bob: 5.056Epoch   6:  37% | abe: 4.959 | eve: 9.156 | bob: 5.054Epoch   6:  38% | abe: 4.958 | eve: 9.157 | bob: 5.053Epoch   6:  39% | abe: 4.957 | eve: 9.157 | bob: 5.051Epoch   6:  39% | abe: 4.957 | eve: 9.157 | bob: 5.051Epoch   6:  40% | abe: 4.957 | eve: 9.156 | bob: 5.051Epoch   6:  41% | abe: 4.958 | eve: 9.157 | bob: 5.052Epoch   6:  42% | abe: 4.959 | eve: 9.157 | bob: 5.052Epoch   6:  42% | abe: 4.959 | eve: 9.157 | bob: 5.053Epoch   6:  43% | abe: 4.958 | eve: 9.157 | bob: 5.052Epoch   6:  44% | abe: 4.958 | eve: 9.157 | bob: 5.052Epoch   6:  45% | abe: 4.958 | eve: 9.157 | bob: 5.052Epoch   6:  46% | abe: 4.959 | eve: 9.156 | bob: 5.053Epoch   6:  46% | abe: 4.958 | eve: 9.157 | bob: 5.052Epoch   6:  47% | abe: 4.958 | eve: 9.156 | bob: 5.052Epoch   6:  48% | abe: 4.958 | eve: 9.156 | bob: 5.051Epoch   6:  49% | abe: 4.958 | eve: 9.156 | bob: 5.052Epoch   6:  50% | abe: 4.958 | eve: 9.156 | bob: 5.052Epoch   6:  50% | abe: 4.958 | eve: 9.156 | bob: 5.052Epoch   6:  51% | abe: 4.959 | eve: 9.157 | bob: 5.052Epoch   6:  52% | abe: 4.958 | eve: 9.156 | bob: 5.051Epoch   6:  53% | abe: 4.959 | eve: 9.157 | bob: 5.052Epoch   6:  53% | abe: 4.957 | eve: 9.157 | bob: 5.050Epoch   6:  54% | abe: 4.957 | eve: 9.156 | bob: 5.050Epoch   6:  55% | abe: 4.956 | eve: 9.155 | bob: 5.050Epoch   6:  56% | abe: 4.957 | eve: 9.154 | bob: 5.050Epoch   6:  57% | abe: 4.957 | eve: 9.154 | bob: 5.050Epoch   6:  57% | abe: 4.957 | eve: 9.153 | bob: 5.050Epoch   6:  58% | abe: 4.957 | eve: 9.151 | bob: 5.049Epoch   6:  59% | abe: 4.956 | eve: 9.152 | bob: 5.048Epoch   6:  60% | abe: 4.956 | eve: 9.150 | bob: 5.049Epoch   6:  60% | abe: 4.956 | eve: 9.150 | bob: 5.049Epoch   6:  61% | abe: 4.956 | eve: 9.150 | bob: 5.049Epoch   6:  62% | abe: 4.956 | eve: 9.149 | bob: 5.049Epoch   6:  63% | abe: 4.956 | eve: 9.150 | bob: 5.049Epoch   6:  64% | abe: 4.956 | eve: 9.150 | bob: 5.049Epoch   6:  64% | abe: 4.956 | eve: 9.150 | bob: 5.048Epoch   6:  65% | abe: 4.956 | eve: 9.150 | bob: 5.048Epoch   6:  66% | abe: 4.956 | eve: 9.151 | bob: 5.048Epoch   6:  67% | abe: 4.955 | eve: 9.150 | bob: 5.047Epoch   6:  67% | abe: 4.955 | eve: 9.150 | bob: 5.047Epoch   6:  68% | abe: 4.954 | eve: 9.150 | bob: 5.047Epoch   6:  69% | abe: 4.954 | eve: 9.149 | bob: 5.047Epoch   6:  70% | abe: 4.954 | eve: 9.148 | bob: 5.047Epoch   6:  71% | abe: 4.954 | eve: 9.148 | bob: 5.047Epoch   6:  71% | abe: 4.955 | eve: 9.148 | bob: 5.048Epoch   6:  72% | abe: 4.954 | eve: 9.148 | bob: 5.047Epoch   6:  73% | abe: 4.954 | eve: 9.147 | bob: 5.046Epoch   6:  74% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  75% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  75% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  76% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  77% | abe: 4.956 | eve: 9.148 | bob: 5.048Epoch   6:  78% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  78% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  79% | abe: 4.954 | eve: 9.149 | bob: 5.046Epoch   6:  80% | abe: 4.954 | eve: 9.148 | bob: 5.046Epoch   6:  81% | abe: 4.955 | eve: 9.148 | bob: 5.047Epoch   6:  82% | abe: 4.956 | eve: 9.148 | bob: 5.048Epoch   6:  82% | abe: 4.957 | eve: 9.148 | bob: 5.048Epoch   6:  83% | abe: 4.957 | eve: 9.148 | bob: 5.049Epoch   6:  84% | abe: 4.957 | eve: 9.148 | bob: 5.048Epoch   6:  85% | abe: 4.956 | eve: 9.148 | bob: 5.047Epoch   6:  85% | abe: 4.956 | eve: 9.148 | bob: 5.047Epoch   6:  86% | abe: 4.956 | eve: 9.147 | bob: 5.047Epoch   6:  87% | abe: 4.955 | eve: 9.147 | bob: 5.046Epoch   6:  88% | abe: 4.956 | eve: 9.147 | bob: 5.047Epoch   6:  89% | abe: 4.957 | eve: 9.146 | bob: 5.047Epoch   6:  89% | abe: 4.957 | eve: 9.146 | bob: 5.047Epoch   6:  90% | abe: 4.957 | eve: 9.146 | bob: 5.048Epoch   6:  91% | abe: 4.958 | eve: 9.146 | bob: 5.048Epoch   6:  92% | abe: 4.957 | eve: 9.146 | bob: 5.048Epoch   6:  92% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  93% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  94% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  95% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  96% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  96% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  97% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  98% | abe: 4.958 | eve: 9.145 | bob: 5.048Epoch   6:  99% | abe: 4.958 | eve: 9.145 | bob: 5.048
New best Bob loss 5.047511893987803 at epoch 6
Epoch   7:   0% | abe: 4.951 | eve: 9.149 | bob: 5.041Epoch   7:   0% | abe: 4.966 | eve: 9.153 | bob: 5.054Epoch   7:   1% | abe: 4.965 | eve: 9.150 | bob: 5.053Epoch   7:   2% | abe: 4.931 | eve: 9.132 | bob: 5.018Epoch   7:   3% | abe: 4.940 | eve: 9.109 | bob: 5.025Epoch   7:   3% | abe: 4.938 | eve: 9.110 | bob: 5.023Epoch   7:   4% | abe: 4.936 | eve: 9.111 | bob: 5.018Epoch   7:   5% | abe: 4.942 | eve: 9.106 | bob: 5.021Epoch   7:   6% | abe: 4.939 | eve: 9.116 | bob: 5.016Epoch   7:   7% | abe: 4.933 | eve: 9.123 | bob: 5.009Epoch   7:   7% | abe: 4.934 | eve: 9.125 | bob: 5.010Epoch   7:   8% | abe: 4.936 | eve: 9.121 | bob: 5.012Epoch   7:   9% | abe: 4.940 | eve: 9.120 | bob: 5.015Epoch   7:  10% | abe: 4.943 | eve: 9.122 | bob: 5.019Epoch   7:  10% | abe: 4.943 | eve: 9.127 | bob: 5.019Epoch   7:  11% | abe: 4.947 | eve: 9.128 | bob: 5.023Epoch   7:  12% | abe: 4.944 | eve: 9.131 | bob: 5.021Epoch   7:  13% | abe: 4.939 | eve: 9.131 | bob: 5.018Epoch   7:  14% | abe: 4.938 | eve: 9.133 | bob: 5.017Epoch   7:  14% | abe: 4.940 | eve: 9.135 | bob: 5.019Epoch   7:  15% | abe: 4.944 | eve: 9.135 | bob: 5.023Epoch   7:  16% | abe: 4.947 | eve: 9.133 | bob: 5.026Epoch   7:  17% | abe: 4.946 | eve: 9.135 | bob: 5.025Epoch   7:  17% | abe: 4.944 | eve: 9.138 | bob: 5.022Epoch   7:  18% | abe: 4.947 | eve: 9.141 | bob: 5.025Epoch   7:  19% | abe: 4.950 | eve: 9.145 | bob: 5.029Epoch   7:  20% | abe: 4.950 | eve: 9.146 | bob: 5.028Epoch   7:  21% | abe: 4.951 | eve: 9.146 | bob: 5.029Epoch   7:  21% | abe: 4.954 | eve: 9.148 | bob: 5.032Epoch   7:  22% | abe: 4.952 | eve: 9.147 | bob: 5.030Epoch   7:  23% | abe: 4.951 | eve: 9.148 | bob: 5.029Epoch   7:  24% | abe: 4.951 | eve: 9.147 | bob: 5.029Epoch   7:  25% | abe: 4.949 | eve: 9.147 | bob: 5.028Epoch   7:  25% | abe: 4.949 | eve: 9.149 | bob: 5.028Epoch   7:  26% | abe: 4.952 | eve: 9.151 | bob: 5.030Epoch   7:  27% | abe: 4.951 | eve: 9.150 | bob: 5.029Epoch   7:  28% | abe: 4.952 | eve: 9.149 | bob: 5.030Epoch   7:  28% | abe: 4.952 | eve: 9.149 | bob: 5.030Epoch   7:  29% | abe: 4.953 | eve: 9.151 | bob: 5.031Epoch   7:  30% | abe: 4.954 | eve: 9.150 | bob: 5.032Epoch   7:  31% | abe: 4.955 | eve: 9.150 | bob: 5.033Epoch   7:  32% | abe: 4.955 | eve: 9.149 | bob: 5.034Epoch   7:  32% | abe: 4.954 | eve: 9.150 | bob: 5.034Epoch   7:  33% | abe: 4.955 | eve: 9.149 | bob: 5.034Epoch   7:  34% | abe: 4.955 | eve: 9.148 | bob: 5.034Epoch   7:  35% | abe: 4.954 | eve: 9.147 | bob: 5.034Epoch   7:  35% | abe: 4.953 | eve: 9.148 | bob: 5.033Epoch   7:  36% | abe: 4.952 | eve: 9.148 | bob: 5.031Epoch   7:  37% | abe: 4.953 | eve: 9.150 | bob: 5.032Epoch   7:  38% | abe: 4.952 | eve: 9.148 | bob: 5.032Epoch   7:  39% | abe: 4.953 | eve: 9.147 | bob: 5.033Epoch   7:  39% | abe: 4.953 | eve: 9.146 | bob: 5.033Epoch   7:  40% | abe: 4.951 | eve: 9.145 | bob: 5.031Epoch   7:  41% | abe: 4.952 | eve: 9.146 | bob: 5.032Epoch   7:  42% | abe: 4.953 | eve: 9.146 | bob: 5.032Epoch   7:  42% | abe: 4.953 | eve: 9.146 | bob: 5.032Epoch   7:  43% | abe: 4.952 | eve: 9.146 | bob: 5.031Epoch   7:  44% | abe: 4.952 | eve: 9.146 | bob: 5.031Epoch   7:  45% | abe: 4.953 | eve: 9.148 | bob: 5.032Epoch   7:  46% | abe: 4.953 | eve: 9.149 | bob: 5.033Epoch   7:  46% | abe: 4.953 | eve: 9.150 | bob: 5.033Epoch   7:  47% | abe: 4.954 | eve: 9.149 | bob: 5.034Epoch   7:  48% | abe: 4.954 | eve: 9.149 | bob: 5.035Epoch   7:  49% | abe: 4.954 | eve: 9.148 | bob: 5.035Epoch   7:  50% | abe: 4.955 | eve: 9.150 | bob: 5.036Epoch   7:  50% | abe: 4.954 | eve: 9.151 | bob: 5.036Epoch   7:  51% | abe: 4.953 | eve: 9.151 | bob: 5.035Epoch   7:  52% | abe: 4.953 | eve: 9.150 | bob: 5.035Epoch   7:  53% | abe: 4.952 | eve: 9.150 | bob: 5.035Epoch   7:  53% | abe: 4.953 | eve: 9.151 | bob: 5.035Epoch   7:  54% | abe: 4.953 | eve: 9.151 | bob: 5.035Epoch   7:  55% | abe: 4.951 | eve: 9.152 | bob: 5.033Epoch   7:  56% | abe: 4.951 | eve: 9.153 | bob: 5.033Epoch   7:  57% | abe: 4.951 | eve: 9.153 | bob: 5.033Epoch   7:  57% | abe: 4.950 | eve: 9.152 | bob: 5.033Epoch   7:  58% | abe: 4.950 | eve: 9.153 | bob: 5.032Epoch   7:  59% | abe: 4.951 | eve: 9.154 | bob: 5.033Epoch   7:  60% | abe: 4.950 | eve: 9.155 | bob: 5.032Epoch   7:  60% | abe: 4.950 | eve: 9.155 | bob: 5.033Epoch   7:  61% | abe: 4.952 | eve: 9.155 | bob: 5.034Epoch   7:  62% | abe: 4.952 | eve: 9.156 | bob: 5.034Epoch   7:  63% | abe: 4.951 | eve: 9.156 | bob: 5.034Epoch   7:  64% | abe: 4.951 | eve: 9.156 | bob: 5.034Epoch   7:  64% | abe: 4.951 | eve: 9.155 | bob: 5.034Epoch   7:  65% | abe: 4.951 | eve: 9.156 | bob: 5.034Epoch   7:  66% | abe: 4.953 | eve: 9.156 | bob: 5.036Epoch   7:  67% | abe: 4.954 | eve: 9.156 | bob: 5.037Epoch   7:  67% | abe: 4.955 | eve: 9.156 | bob: 5.037Epoch   7:  68% | abe: 4.955 | eve: 9.156 | bob: 5.037Epoch   7:  69% | abe: 4.955 | eve: 9.156 | bob: 5.037Epoch   7:  70% | abe: 4.956 | eve: 9.157 | bob: 5.038Epoch   7:  71% | abe: 4.956 | eve: 9.157 | bob: 5.039Epoch   7:  71% | abe: 4.956 | eve: 9.157 | bob: 5.039Epoch   7:  72% | abe: 4.956 | eve: 9.157 | bob: 5.039Epoch   7:  73% | abe: 4.956 | eve: 9.157 | bob: 5.040Epoch   7:  74% | abe: 4.956 | eve: 9.156 | bob: 5.039Epoch   7:  75% | abe: 4.956 | eve: 9.156 | bob: 5.039Epoch   7:  75% | abe: 4.957 | eve: 9.157 | bob: 5.040Epoch   7:  76% | abe: 4.957 | eve: 9.156 | bob: 5.040Epoch   7:  77% | abe: 4.958 | eve: 9.157 | bob: 5.041Epoch   7:  78% | abe: 4.957 | eve: 9.157 | bob: 5.040Epoch   7:  78% | abe: 4.957 | eve: 9.158 | bob: 5.040Epoch   7:  79% | abe: 4.957 | eve: 9.158 | bob: 5.040Epoch   7:  80% | abe: 4.957 | eve: 9.159 | bob: 5.041Epoch   7:  81% | abe: 4.958 | eve: 9.159 | bob: 5.042Epoch   7:  82% | abe: 4.957 | eve: 9.159 | bob: 5.042Epoch   7:  82% | abe: 4.957 | eve: 9.159 | bob: 5.042Epoch   7:  83% | abe: 4.957 | eve: 9.158 | bob: 5.041Epoch   7:  84% | abe: 4.958 | eve: 9.158 | bob: 5.042Epoch   7:  85% | abe: 4.957 | eve: 9.158 | bob: 5.041Epoch   7:  85% | abe: 4.957 | eve: 9.158 | bob: 5.040Epoch   7:  86% | abe: 4.957 | eve: 9.159 | bob: 5.040Epoch   7:  87% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  88% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  89% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  89% | abe: 4.958 | eve: 9.160 | bob: 5.040Epoch   7:  90% | abe: 4.958 | eve: 9.161 | bob: 5.040Epoch   7:  91% | abe: 4.958 | eve: 9.161 | bob: 5.040Epoch   7:  92% | abe: 4.958 | eve: 9.161 | bob: 5.039Epoch   7:  92% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  93% | abe: 4.957 | eve: 9.161 | bob: 5.039Epoch   7:  94% | abe: 4.957 | eve: 9.161 | bob: 5.039Epoch   7:  95% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  96% | abe: 4.957 | eve: 9.161 | bob: 5.039Epoch   7:  96% | abe: 4.957 | eve: 9.160 | bob: 5.039Epoch   7:  97% | abe: 4.957 | eve: 9.161 | bob: 5.038Epoch   7:  98% | abe: 4.957 | eve: 9.161 | bob: 5.039Epoch   7:  99% | abe: 4.957 | eve: 9.161 | bob: 5.039
New best Bob loss 5.038969693620629 at epoch 7
Epoch   8:   0% | abe: 4.847 | eve: 9.163 | bob: 4.908Epoch   8:   0% | abe: 4.929 | eve: 9.167 | bob: 5.003Epoch   8:   1% | abe: 4.933 | eve: 9.157 | bob: 5.008Epoch   8:   2% | abe: 4.956 | eve: 9.143 | bob: 5.029Epoch   8:   3% | abe: 4.955 | eve: 9.158 | bob: 5.030Epoch   8:   3% | abe: 4.950 | eve: 9.167 | bob: 5.025Epoch   8:   4% | abe: 4.954 | eve: 9.177 | bob: 5.029Epoch   8:   5% | abe: 4.941 | eve: 9.186 | bob: 5.018Epoch   8:   6% | abe: 4.952 | eve: 9.189 | bob: 5.028Epoch   8:   7% | abe: 4.953 | eve: 9.197 | bob: 5.030Epoch   8:   7% | abe: 4.955 | eve: 9.195 | bob: 5.032Epoch   8:   8% | abe: 4.954 | eve: 9.187 | bob: 5.029Epoch   8:   9% | abe: 4.960 | eve: 9.190 | bob: 5.034Epoch   8:  10% | abe: 4.961 | eve: 9.187 | bob: 5.033Epoch   8:  10% | abe: 4.960 | eve: 9.182 | bob: 5.031Epoch   8:  11% | abe: 4.959 | eve: 9.189 | bob: 5.030Epoch   8:  12% | abe: 4.959 | eve: 9.192 | bob: 5.031Epoch   8:  13% | abe: 4.958 | eve: 9.191 | bob: 5.030Epoch   8:  14% | abe: 4.958 | eve: 9.190 | bob: 5.031Epoch   8:  14% | abe: 4.961 | eve: 9.190 | bob: 5.035Epoch   8:  15% | abe: 4.960 | eve: 9.184 | bob: 5.034Epoch   8:  16% | abe: 4.960 | eve: 9.184 | bob: 5.034Epoch   8:  17% | abe: 4.961 | eve: 9.185 | bob: 5.035Epoch   8:  17% | abe: 4.964 | eve: 9.183 | bob: 5.038Epoch   8:  18% | abe: 4.966 | eve: 9.182 | bob: 5.040Epoch   8:  19% | abe: 4.967 | eve: 9.180 | bob: 5.042Epoch   8:  20% | abe: 4.969 | eve: 9.177 | bob: 5.045Epoch   8:  21% | abe: 4.971 | eve: 9.178 | bob: 5.049Epoch   8:  21% | abe: 4.968 | eve: 9.177 | bob: 5.046Epoch   8:  22% | abe: 4.967 | eve: 9.177 | bob: 5.046Epoch   8:  23% | abe: 4.967 | eve: 9.177 | bob: 5.047Epoch   8:  24% | abe: 4.969 | eve: 9.178 | bob: 5.049Epoch   8:  25% | abe: 4.969 | eve: 9.180 | bob: 5.049Epoch   8:  25% | abe: 4.971 | eve: 9.181 | bob: 5.050Epoch   8:  26% | abe: 4.970 | eve: 9.179 | bob: 5.049Epoch   8:  27% | abe: 4.970 | eve: 9.178 | bob: 5.048Epoch   8:  28% | abe: 4.969 | eve: 9.178 | bob: 5.048Epoch   8:  28% | abe: 4.968 | eve: 9.180 | bob: 5.046Epoch   8:  29% | abe: 4.969 | eve: 9.180 | bob: 5.048Epoch   8:  30% | abe: 4.970 | eve: 9.179 | bob: 5.050Epoch   8:  31% | abe: 4.969 | eve: 9.178 | bob: 5.048Epoch   8:  32% | abe: 4.970 | eve: 9.178 | bob: 5.049Epoch   8:  32% | abe: 4.969 | eve: 9.177 | bob: 5.049Epoch   8:  33% | abe: 4.970 | eve: 9.176 | bob: 5.049Epoch   8:  34% | abe: 4.969 | eve: 9.174 | bob: 5.048Epoch   8:  35% | abe: 4.968 | eve: 9.173 | bob: 5.047Epoch   8:  35% | abe: 4.968 | eve: 9.172 | bob: 5.047Epoch   8:  36% | abe: 4.970 | eve: 9.174 | bob: 5.049Epoch   8:  37% | abe: 4.969 | eve: 9.174 | bob: 5.048Epoch   8:  38% | abe: 4.968 | eve: 9.175 | bob: 5.049Epoch   8:  39% | abe: 4.967 | eve: 9.174 | bob: 5.047Epoch   8:  39% | abe: 4.966 | eve: 9.175 | bob: 5.047Epoch   8:  40% | abe: 4.966 | eve: 9.175 | bob: 5.047Epoch   8:  41% | abe: 4.964 | eve: 9.175 | bob: 5.045Epoch   8:  42% | abe: 4.963 | eve: 9.174 | bob: 5.044Epoch   8:  42% | abe: 4.963 | eve: 9.175 | bob: 5.044Epoch   8:  43% | abe: 4.963 | eve: 9.173 | bob: 5.044Epoch   8:  44% | abe: 4.964 | eve: 9.174 | bob: 5.044Epoch   8:  45% | abe: 4.965 | eve: 9.174 | bob: 5.045Epoch   8:  46% | abe: 4.964 | eve: 9.175 | bob: 5.044Epoch   8:  46% | abe: 4.965 | eve: 9.174 | bob: 5.044Epoch   8:  47% | abe: 4.964 | eve: 9.173 | bob: 5.043Epoch   8:  48% | abe: 4.964 | eve: 9.173 | bob: 5.042Epoch   8:  49% | abe: 4.964 | eve: 9.173 | bob: 5.042Epoch   8:  50% | abe: 4.964 | eve: 9.173 | bob: 5.042Epoch   8:  50% | abe: 4.964 | eve: 9.173 | bob: 5.041Epoch   8:  51% | abe: 4.964 | eve: 9.174 | bob: 5.042Epoch   8:  52% | abe: 4.964 | eve: 9.174 | bob: 5.042Epoch   8:  53% | abe: 4.965 | eve: 9.174 | bob: 5.043Epoch   8:  53% | abe: 4.964 | eve: 9.174 | bob: 5.042Epoch   8:  54% | abe: 4.964 | eve: 9.173 | bob: 5.042Epoch   8:  55% | abe: 4.964 | eve: 9.173 | bob: 5.042Epoch   8:  56% | abe: 4.964 | eve: 9.174 | bob: 5.041Epoch   8:  57% | abe: 4.964 | eve: 9.173 | bob: 5.041Epoch   8:  57% | abe: 4.965 | eve: 9.174 | bob: 5.042Epoch   8:  58% | abe: 4.966 | eve: 9.173 | bob: 5.042Epoch   8:  59% | abe: 4.965 | eve: 9.173 | bob: 5.041Epoch   8:  60% | abe: 4.965 | eve: 9.173 | bob: 5.041Epoch   8:  60% | abe: 4.965 | eve: 9.173 | bob: 5.041Epoch   8:  61% | abe: 4.965 | eve: 9.173 | bob: 5.041Epoch   8:  62% | abe: 4.964 | eve: 9.172 | bob: 5.040Epoch   8:  63% | abe: 4.964 | eve: 9.173 | bob: 5.040Epoch   8:  64% | abe: 4.965 | eve: 9.172 | bob: 5.040Epoch   8:  64% | abe: 4.963 | eve: 9.172 | bob: 5.038Epoch   8:  65% | abe: 4.963 | eve: 9.173 | bob: 5.038Epoch   8:  66% | abe: 4.964 | eve: 9.173 | bob: 5.039Epoch   8:  67% | abe: 4.964 | eve: 9.172 | bob: 5.039Epoch   8:  67% | abe: 4.963 | eve: 9.173 | bob: 5.038Epoch   8:  68% | abe: 4.964 | eve: 9.173 | bob: 5.039Epoch   8:  69% | abe: 4.965 | eve: 9.173 | bob: 5.040Epoch   8:  70% | abe: 4.965 | eve: 9.173 | bob: 5.040Epoch   8:  71% | abe: 4.965 | eve: 9.173 | bob: 5.040Epoch   8:  71% | abe: 4.966 | eve: 9.173 | bob: 5.041Epoch   8:  72% | abe: 4.965 | eve: 9.174 | bob: 5.040Epoch   8:  73% | abe: 4.965 | eve: 9.174 | bob: 5.040Epoch   8:  74% | abe: 4.965 | eve: 9.175 | bob: 5.040Epoch   8:  75% | abe: 4.965 | eve: 9.174 | bob: 5.039Epoch   8:  75% | abe: 4.964 | eve: 9.173 | bob: 5.039Epoch   8:  76% | abe: 4.965 | eve: 9.172 | bob: 5.040Epoch   8:  77% | abe: 4.963 | eve: 9.172 | bob: 5.039Epoch   8:  78% | abe: 4.963 | eve: 9.172 | bob: 5.039Epoch   8:  78% | abe: 4.963 | eve: 9.172 | bob: 5.039Epoch   8:  79% | abe: 4.964 | eve: 9.172 | bob: 5.040Epoch   8:  80% | abe: 4.964 | eve: 9.172 | bob: 5.040Epoch   8:  81% | abe: 4.965 | eve: 9.172 | bob: 5.041Epoch   8:  82% | abe: 4.965 | eve: 9.173 | bob: 5.041Epoch   8:  82% | abe: 4.964 | eve: 9.173 | bob: 5.040Epoch   8:  83% | abe: 4.964 | eve: 9.172 | bob: 5.039Epoch   8:  84% | abe: 4.963 | eve: 9.173 | bob: 5.039Epoch   8:  85% | abe: 4.963 | eve: 9.172 | bob: 5.039Epoch   8:  85% | abe: 4.963 | eve: 9.172 | bob: 5.038Epoch   8:  86% | abe: 4.962 | eve: 9.173 | bob: 5.037Epoch   8:  87% | abe: 4.963 | eve: 9.173 | bob: 5.038Epoch   8:  88% | abe: 4.963 | eve: 9.173 | bob: 5.037Epoch   8:  89% | abe: 4.963 | eve: 9.173 | bob: 5.037Epoch   8:  89% | abe: 4.963 | eve: 9.172 | bob: 5.038Epoch   8:  90% | abe: 4.963 | eve: 9.172 | bob: 5.037Epoch   8:  91% | abe: 4.962 | eve: 9.172 | bob: 5.036Epoch   8:  92% | abe: 4.962 | eve: 9.171 | bob: 5.037Epoch   8:  92% | abe: 4.962 | eve: 9.171 | bob: 5.036Epoch   8:  93% | abe: 4.961 | eve: 9.170 | bob: 5.036Epoch   8:  94% | abe: 4.962 | eve: 9.171 | bob: 5.036Epoch   8:  95% | abe: 4.962 | eve: 9.170 | bob: 5.036Epoch   8:  96% | abe: 4.963 | eve: 9.170 | bob: 5.037Epoch   8:  96% | abe: 4.963 | eve: 9.170 | bob: 5.037Epoch   8:  97% | abe: 4.963 | eve: 9.171 | bob: 5.037Epoch   8:  98% | abe: 4.963 | eve: 9.171 | bob: 5.037Epoch   8:  99% | abe: 4.963 | eve: 9.170 | bob: 5.037
New best Bob loss 5.0366188577054345 at epoch 8
Epoch   9:   0% | abe: 4.932 | eve: 9.138 | bob: 4.992Epoch   9:   0% | abe: 4.956 | eve: 9.139 | bob: 5.017Epoch   9:   1% | abe: 4.949 | eve: 9.143 | bob: 5.006Epoch   9:   2% | abe: 4.966 | eve: 9.163 | bob: 5.019Epoch   9:   3% | abe: 4.960 | eve: 9.176 | bob: 5.016Epoch   9:   3% | abe: 4.957 | eve: 9.185 | bob: 5.016Epoch   9:   4% | abe: 4.949 | eve: 9.178 | bob: 5.011Epoch   9:   5% | abe: 4.947 | eve: 9.168 | bob: 5.013Epoch   9:   6% | abe: 4.952 | eve: 9.166 | bob: 5.022Epoch   9:   7% | abe: 4.950 | eve: 9.161 | bob: 5.022Epoch   9:   7% | abe: 4.955 | eve: 9.162 | bob: 5.026Epoch   9:   8% | abe: 4.960 | eve: 9.169 | bob: 5.029Epoch   9:   9% | abe: 4.962 | eve: 9.170 | bob: 5.030Epoch   9:  10% | abe: 4.963 | eve: 9.171 | bob: 5.029Epoch   9:  10% | abe: 4.966 | eve: 9.170 | bob: 5.031Epoch   9:  11% | abe: 4.959 | eve: 9.172 | bob: 5.023Epoch   9:  12% | abe: 4.959 | eve: 9.177 | bob: 5.022Epoch   9:  13% | abe: 4.964 | eve: 9.174 | bob: 5.028Epoch   9:  14% | abe: 4.966 | eve: 9.173 | bob: 5.030Epoch   9:  14% | abe: 4.966 | eve: 9.172 | bob: 5.030Epoch   9:  15% | abe: 4.967 | eve: 9.174 | bob: 5.032Epoch   9:  16% | abe: 4.968 | eve: 9.169 | bob: 5.035Epoch   9:  17% | abe: 4.968 | eve: 9.170 | bob: 5.036Epoch   9:  17% | abe: 4.971 | eve: 9.170 | bob: 5.040Epoch   9:  18% | abe: 4.968 | eve: 9.167 | bob: 5.038Epoch   9:  19% | abe: 4.969 | eve: 9.167 | bob: 5.039Epoch   9:  20% | abe: 4.968 | eve: 9.167 | bob: 5.038Epoch   9:  21% | abe: 4.969 | eve: 9.163 | bob: 5.040Epoch   9:  21% | abe: 4.967 | eve: 9.163 | bob: 5.037Epoch   9:  22% | abe: 4.967 | eve: 9.164 | bob: 5.037Epoch   9:  23% | abe: 4.967 | eve: 9.164 | bob: 5.037Epoch   9:  24% | abe: 4.970 | eve: 9.164 | bob: 5.040Epoch   9:  25% | abe: 4.969 | eve: 9.165 | bob: 5.038Epoch   9:  25% | abe: 4.970 | eve: 9.164 | bob: 5.039Epoch   9:  26% | abe: 4.968 | eve: 9.164 | bob: 5.037Epoch   9:  27% | abe: 4.967 | eve: 9.165 | bob: 5.036Epoch   9:  28% | abe: 4.966 | eve: 9.163 | bob: 5.035Epoch   9:  28% | abe: 4.966 | eve: 9.162 | bob: 5.036Epoch   9:  29% | abe: 4.968 | eve: 9.163 | bob: 5.038Epoch   9:  30% | abe: 4.966 | eve: 9.162 | bob: 5.036Epoch   9:  31% | abe: 4.967 | eve: 9.163 | bob: 5.036Epoch   9:  32% | abe: 4.966 | eve: 9.165 | bob: 5.035Epoch   9:  32% | abe: 4.966 | eve: 9.163 | bob: 5.035Epoch   9:  33% | abe: 4.967 | eve: 9.163 | bob: 5.035Epoch   9:  34% | abe: 4.967 | eve: 9.164 | bob: 5.036Epoch   9:  35% | abe: 4.967 | eve: 9.165 | bob: 5.035Epoch   9:  35% | abe: 4.967 | eve: 9.166 | bob: 5.035Epoch   9:  36% | abe: 4.965 | eve: 9.166 | bob: 5.033Epoch   9:  37% | abe: 4.965 | eve: 9.168 | bob: 5.033Epoch   9:  38% | abe: 4.964 | eve: 9.167 | bob: 5.032Epoch   9:  39% | abe: 4.965 | eve: 9.167 | bob: 5.033Epoch   9:  39% | abe: 4.965 | eve: 9.168 | bob: 5.034Epoch   9:  40% | abe: 4.966 | eve: 9.169 | bob: 5.035Epoch   9:  41% | abe: 4.965 | eve: 9.169 | bob: 5.034Epoch   9:  42% | abe: 4.965 | eve: 9.167 | bob: 5.034Epoch   9:  42% | abe: 4.964 | eve: 9.168 | bob: 5.033Epoch   9:  43% | abe: 4.964 | eve: 9.168 | bob: 5.032Epoch   9:  44% | abe: 4.966 | eve: 9.168 | bob: 5.033Epoch   9:  45% | abe: 4.966 | eve: 9.166 | bob: 5.034Epoch   9:  46% | abe: 4.967 | eve: 9.166 | bob: 5.034Epoch   9:  46% | abe: 4.966 | eve: 9.167 | bob: 5.034Epoch   9:  47% | abe: 4.967 | eve: 9.168 | bob: 5.034Epoch   9:  48% | abe: 4.967 | eve: 9.168 | bob: 5.034Epoch   9:  49% | abe: 4.966 | eve: 9.168 | bob: 5.034Epoch   9:  50% | abe: 4.966 | eve: 9.168 | bob: 5.033Epoch   9:  50% | abe: 4.966 | eve: 9.168 | bob: 5.034Epoch   9:  51% | abe: 4.965 | eve: 9.167 | bob: 5.033Epoch   9:  52% | abe: 4.964 | eve: 9.167 | bob: 5.032Epoch   9:  53% | abe: 4.964 | eve: 9.167 | bob: 5.032Epoch   9:  53% | abe: 4.964 | eve: 9.167 | bob: 5.033Epoch   9:  54% | abe: 4.964 | eve: 9.168 | bob: 5.033Epoch   9:  55% | abe: 4.964 | eve: 9.168 | bob: 5.033Epoch   9:  56% | abe: 4.964 | eve: 9.167 | bob: 5.033Epoch   9:  57% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  57% | abe: 4.964 | eve: 9.168 | bob: 5.034Epoch   9:  58% | abe: 4.963 | eve: 9.167 | bob: 5.034Epoch   9:  59% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  60% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  60% | abe: 4.964 | eve: 9.167 | bob: 5.035Epoch   9:  61% | abe: 4.964 | eve: 9.167 | bob: 5.035Epoch   9:  62% | abe: 4.965 | eve: 9.168 | bob: 5.035Epoch   9:  63% | abe: 4.965 | eve: 9.167 | bob: 5.035Epoch   9:  64% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  64% | abe: 4.963 | eve: 9.168 | bob: 5.033Epoch   9:  65% | abe: 4.963 | eve: 9.167 | bob: 5.033Epoch   9:  66% | abe: 4.963 | eve: 9.167 | bob: 5.033Epoch   9:  67% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  67% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  68% | abe: 4.965 | eve: 9.167 | bob: 5.034Epoch   9:  69% | abe: 4.965 | eve: 9.167 | bob: 5.034Epoch   9:  70% | abe: 4.965 | eve: 9.166 | bob: 5.034Epoch   9:  71% | abe: 4.965 | eve: 9.167 | bob: 5.035Epoch   9:  71% | abe: 4.964 | eve: 9.167 | bob: 5.034Epoch   9:  72% | abe: 4.965 | eve: 9.167 | bob: 5.034Epoch   9:  73% | abe: 4.965 | eve: 9.168 | bob: 5.034Epoch   9:  74% | abe: 4.964 | eve: 9.168 | bob: 5.034Epoch   9:  75% | abe: 4.964 | eve: 9.167 | bob: 5.033Epoch   9:  75% | abe: 4.964 | eve: 9.168 | bob: 5.033Epoch   9:  76% | abe: 4.963 | eve: 9.168 | bob: 5.033Epoch   9:  77% | abe: 4.963 | eve: 9.167 | bob: 5.032Epoch   9:  78% | abe: 4.963 | eve: 9.167 | bob: 5.032Epoch   9:  78% | abe: 4.964 | eve: 9.167 | bob: 5.033Epoch   9:  79% | abe: 4.964 | eve: 9.166 | bob: 5.032Epoch   9:  80% | abe: 4.964 | eve: 9.166 | bob: 5.032Epoch   9:  81% | abe: 4.963 | eve: 9.167 | bob: 5.031Epoch   9:  82% | abe: 4.964 | eve: 9.168 | bob: 5.032Epoch   9:  82% | abe: 4.963 | eve: 9.168 | bob: 5.031Epoch   9:  83% | abe: 4.963 | eve: 9.168 | bob: 5.031Epoch   9:  84% | abe: 4.963 | eve: 9.168 | bob: 5.032Epoch   9:  85% | abe: 4.963 | eve: 9.168 | bob: 5.032Epoch   9:  85% | abe: 4.963 | eve: 9.169 | bob: 5.032Epoch   9:  86% | abe: 4.963 | eve: 9.169 | bob: 5.032Epoch   9:  87% | abe: 4.963 | eve: 9.170 | bob: 5.032Epoch   9:  88% | abe: 4.963 | eve: 9.169 | bob: 5.032Epoch   9:  89% | abe: 4.963 | eve: 9.169 | bob: 5.031Epoch   9:  89% | abe: 4.963 | eve: 9.170 | bob: 5.031Epoch   9:  90% | abe: 4.963 | eve: 9.170 | bob: 5.031Epoch   9:  91% | abe: 4.963 | eve: 9.169 | bob: 5.030Epoch   9:  92% | abe: 4.963 | eve: 9.170 | bob: 5.031Epoch   9:  92% | abe: 4.963 | eve: 9.170 | bob: 5.031Epoch   9:  93% | abe: 4.962 | eve: 9.171 | bob: 5.030Epoch   9:  94% | abe: 4.962 | eve: 9.171 | bob: 5.030Epoch   9:  95% | abe: 4.963 | eve: 9.172 | bob: 5.031Epoch   9:  96% | abe: 4.963 | eve: 9.172 | bob: 5.031Epoch   9:  96% | abe: 4.964 | eve: 9.172 | bob: 5.031Epoch   9:  97% | abe: 4.963 | eve: 9.172 | bob: 5.030Epoch   9:  98% | abe: 4.964 | eve: 9.172 | bob: 5.030Epoch   9:  99% | abe: 4.964 | eve: 9.172 | bob: 5.031
New best Bob loss 5.030748206531598 at epoch 9
Epoch  10:   0% | abe: 4.977 | eve: 9.110 | bob: 5.035Epoch  10:   0% | abe: 4.993 | eve: 9.139 | bob: 5.056Epoch  10:   1% | abe: 4.985 | eve: 9.157 | bob: 5.052Epoch  10:   2% | abe: 4.985 | eve: 9.171 | bob: 5.052Epoch  10:   3% | abe: 4.991 | eve: 9.192 | bob: 5.062Epoch  10:   3% | abe: 5.007 | eve: 9.200 | bob: 5.079Epoch  10:   4% | abe: 5.001 | eve: 9.199 | bob: 5.074Epoch  10:   5% | abe: 4.998 | eve: 9.191 | bob: 5.072Epoch  10:   6% | abe: 4.992 | eve: 9.186 | bob: 5.068Epoch  10:   7% | abe: 4.989 | eve: 9.181 | bob: 5.065Epoch  10:   7% | abe: 4.988 | eve: 9.181 | bob: 5.064Epoch  10:   8% | abe: 4.983 | eve: 9.177 | bob: 5.058Epoch  10:   9% | abe: 4.979 | eve: 9.176 | bob: 5.053Epoch  10:  10% | abe: 4.980 | eve: 9.179 | bob: 5.052Epoch  10:  10% | abe: 4.980 | eve: 9.179 | bob: 5.051Epoch  10:  11% | abe: 4.977 | eve: 9.182 | bob: 5.047Epoch  10:  12% | abe: 4.975 | eve: 9.182 | bob: 5.045Epoch  10:  13% | abe: 4.973 | eve: 9.179 | bob: 5.042Epoch  10:  14% | abe: 4.975 | eve: 9.184 | bob: 5.045Epoch  10:  14% | abe: 4.971 | eve: 9.185 | bob: 5.041Epoch  10:  15% | abe: 4.968 | eve: 9.183 | bob: 5.039Epoch  10:  16% | abe: 4.966 | eve: 9.183 | bob: 5.037Epoch  10:  17% | abe: 4.963 | eve: 9.186 | bob: 5.034Epoch  10:  17% | abe: 4.961 | eve: 9.187 | bob: 5.032Epoch  10:  18% | abe: 4.961 | eve: 9.184 | bob: 5.033Epoch  10:  19% | abe: 4.962 | eve: 9.185 | bob: 5.033Epoch  10:  20% | abe: 4.963 | eve: 9.187 | bob: 5.034Epoch  10:  21% | abe: 4.961 | eve: 9.187 | bob: 5.031Epoch  10:  21% | abe: 4.962 | eve: 9.188 | bob: 5.032Epoch  10:  22% | abe: 4.960 | eve: 9.189 | bob: 5.029Epoch  10:  23% | abe: 4.959 | eve: 9.189 | bob: 5.027Epoch  10:  24% | abe: 4.959 | eve: 9.187 | bob: 5.027Epoch  10:  25% | abe: 4.959 | eve: 9.188 | bob: 5.028Epoch  10:  25% | abe: 4.956 | eve: 9.188 | bob: 5.025Epoch  10:  26% | abe: 4.958 | eve: 9.187 | bob: 5.027Epoch  10:  27% | abe: 4.959 | eve: 9.185 | bob: 5.028Epoch  10:  28% | abe: 4.959 | eve: 9.183 | bob: 5.029Epoch  10:  28% | abe: 4.960 | eve: 9.182 | bob: 5.030Epoch  10:  29% | abe: 4.961 | eve: 9.179 | bob: 5.031Epoch  10:  30% | abe: 4.960 | eve: 9.179 | bob: 5.030Epoch  10:  31% | abe: 4.961 | eve: 9.179 | bob: 5.031Epoch  10:  32% | abe: 4.961 | eve: 9.178 | bob: 5.031Epoch  10:  32% | abe: 4.960 | eve: 9.178 | bob: 5.029Epoch  10:  33% | abe: 4.959 | eve: 9.179 | bob: 5.028Epoch  10:  34% | abe: 4.959 | eve: 9.178 | bob: 5.026Epoch  10:  35% | abe: 4.958 | eve: 9.180 | bob: 5.026Epoch  10:  35% | abe: 4.959 | eve: 9.180 | bob: 5.026Epoch  10:  36% | abe: 4.959 | eve: 9.179 | bob: 5.026Epoch  10:  37% | abe: 4.958 | eve: 9.177 | bob: 5.024Epoch  10:  38% | abe: 4.959 | eve: 9.179 | bob: 5.025Epoch  10:  39% | abe: 4.959 | eve: 9.179 | bob: 5.025Epoch  10:  39% | abe: 4.959 | eve: 9.179 | bob: 5.025Epoch  10:  40% | abe: 4.959 | eve: 9.180 | bob: 5.025Epoch  10:  41% | abe: 4.959 | eve: 9.179 | bob: 5.025Epoch  10:  42% | abe: 4.959 | eve: 9.180 | bob: 5.025Epoch  10:  42% | abe: 4.958 | eve: 9.180 | bob: 5.025Epoch  10:  43% | abe: 4.959 | eve: 9.180 | bob: 5.026Epoch  10:  44% | abe: 4.958 | eve: 9.180 | bob: 5.025Epoch  10:  45% | abe: 4.958 | eve: 9.180 | bob: 5.026Epoch  10:  46% | abe: 4.958 | eve: 9.180 | bob: 5.026Epoch  10:  46% | abe: 4.958 | eve: 9.179 | bob: 5.026Epoch  10:  47% | abe: 4.957 | eve: 9.179 | bob: 5.025Epoch  10:  48% | abe: 4.957 | eve: 9.177 | bob: 5.024Epoch  10:  49% | abe: 4.956 | eve: 9.176 | bob: 5.023Epoch  10:  50% | abe: 4.957 | eve: 9.179 | bob: 5.024Epoch  10:  50% | abe: 4.957 | eve: 9.178 | bob: 5.024Epoch  10:  51% | abe: 4.958 | eve: 9.178 | bob: 5.024Epoch  10:  52% | abe: 4.957 | eve: 9.178 | bob: 5.024Epoch  10:  53% | abe: 4.957 | eve: 9.178 | bob: 5.025Epoch  10:  53% | abe: 4.957 | eve: 9.178 | bob: 5.025Epoch  10:  54% | abe: 4.957 | eve: 9.178 | bob: 5.025Epoch  10:  55% | abe: 4.957 | eve: 9.179 | bob: 5.025Epoch  10:  56% | abe: 4.958 | eve: 9.181 | bob: 5.027Epoch  10:  57% | abe: 4.958 | eve: 9.181 | bob: 5.026Epoch  10:  57% | abe: 4.957 | eve: 9.182 | bob: 5.026Epoch  10:  58% | abe: 4.957 | eve: 9.183 | bob: 5.025Epoch  10:  59% | abe: 4.956 | eve: 9.183 | bob: 5.025Epoch  10:  60% | abe: 4.957 | eve: 9.184 | bob: 5.025Epoch  10:  60% | abe: 4.956 | eve: 9.184 | bob: 5.024Epoch  10:  61% | abe: 4.956 | eve: 9.184 | bob: 5.024Epoch  10:  62% | abe: 4.955 | eve: 9.185 | bob: 5.023Epoch  10:  63% | abe: 4.956 | eve: 9.184 | bob: 5.024Epoch  10:  64% | abe: 4.955 | eve: 9.185 | bob: 5.023Epoch  10:  64% | abe: 4.955 | eve: 9.185 | bob: 5.023Epoch  10:  65% | abe: 4.955 | eve: 9.186 | bob: 5.023Epoch  10:  66% | abe: 4.956 | eve: 9.185 | bob: 5.023Epoch  10:  67% | abe: 4.955 | eve: 9.185 | bob: 5.023Epoch  10:  67% | abe: 4.955 | eve: 9.186 | bob: 5.024Epoch  10:  68% | abe: 4.956 | eve: 9.185 | bob: 5.024Epoch  10:  69% | abe: 4.956 | eve: 9.186 | bob: 5.025Epoch  10:  70% | abe: 4.956 | eve: 9.185 | bob: 5.025Epoch  10:  71% | abe: 4.956 | eve: 9.185 | bob: 5.025Epoch  10:  71% | abe: 4.956 | eve: 9.184 | bob: 5.024Epoch  10:  72% | abe: 4.956 | eve: 9.185 | bob: 5.024Epoch  10:  73% | abe: 4.956 | eve: 9.185 | bob: 5.024Epoch  10:  74% | abe: 4.957 | eve: 9.185 | bob: 5.024Epoch  10:  75% | abe: 4.957 | eve: 9.185 | bob: 5.025Epoch  10:  75% | abe: 4.956 | eve: 9.185 | bob: 5.024Epoch  10:  76% | abe: 4.956 | eve: 9.185 | bob: 5.024Epoch  10:  77% | abe: 4.957 | eve: 9.185 | bob: 5.025Epoch  10:  78% | abe: 4.957 | eve: 9.185 | bob: 5.025Epoch  10:  78% | abe: 4.957 | eve: 9.185 | bob: 5.026Epoch  10:  79% | abe: 4.958 | eve: 9.185 | bob: 5.026Epoch  10:  80% | abe: 4.958 | eve: 9.185 | bob: 5.026Epoch  10:  81% | abe: 4.957 | eve: 9.186 | bob: 5.026Epoch  10:  82% | abe: 4.958 | eve: 9.186 | bob: 5.027Epoch  10:  82% | abe: 4.958 | eve: 9.186 | bob: 5.027Epoch  10:  83% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  84% | abe: 4.959 | eve: 9.185 | bob: 5.027Epoch  10:  85% | abe: 4.959 | eve: 9.186 | bob: 5.027Epoch  10:  85% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  86% | abe: 4.959 | eve: 9.185 | bob: 5.027Epoch  10:  87% | abe: 4.959 | eve: 9.185 | bob: 5.027Epoch  10:  88% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  89% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  89% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  90% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  91% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  92% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  92% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  93% | abe: 4.957 | eve: 9.187 | bob: 5.026Epoch  10:  94% | abe: 4.958 | eve: 9.187 | bob: 5.026Epoch  10:  95% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  96% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  96% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  97% | abe: 4.958 | eve: 9.186 | bob: 5.026Epoch  10:  98% | abe: 4.959 | eve: 9.186 | bob: 5.027Epoch  10:  99% | abe: 4.959 | eve: 9.187 | bob: 5.027
New best Bob loss 5.026707122775406 at epoch 10
Epoch  11:   0% | abe: 4.946 | eve: 9.153 | bob: 5.016Epoch  11:   0% | abe: 4.972 | eve: 9.208 | bob: 5.041Epoch  11:   1% | abe: 4.958 | eve: 9.211 | bob: 5.034Epoch  11:   2% | abe: 4.948 | eve: 9.196 | bob: 5.022Epoch  11:   3% | abe: 4.945 | eve: 9.191 | bob: 5.018Epoch  11:   3% | abe: 4.942 | eve: 9.173 | bob: 5.013Epoch  11:   4% | abe: 4.951 | eve: 9.179 | bob: 5.021Epoch  11:   5% | abe: 4.953 | eve: 9.176 | bob: 5.021Epoch  11:   6% | abe: 4.953 | eve: 9.174 | bob: 5.020Epoch  11:   7% | abe: 4.958 | eve: 9.177 | bob: 5.025Epoch  11:   7% | abe: 4.957 | eve: 9.177 | bob: 5.025Epoch  11:   8% | abe: 4.954 | eve: 9.180 | bob: 5.025Epoch  11:   9% | abe: 4.946 | eve: 9.179 | bob: 5.018Epoch  11:  10% | abe: 4.952 | eve: 9.178 | bob: 5.027Epoch  11:  10% | abe: 4.953 | eve: 9.181 | bob: 5.028Epoch  11:  11% | abe: 4.951 | eve: 9.186 | bob: 5.027Epoch  11:  12% | abe: 4.954 | eve: 9.186 | bob: 5.031Epoch  11:  13% | abe: 4.953 | eve: 9.187 | bob: 5.029Epoch  11:  14% | abe: 4.956 | eve: 9.181 | bob: 5.031Epoch  11:  14% | abe: 4.954 | eve: 9.181 | bob: 5.029Epoch  11:  15% | abe: 4.955 | eve: 9.183 | bob: 5.030Epoch  11:  16% | abe: 4.955 | eve: 9.182 | bob: 5.030Epoch  11:  17% | abe: 4.957 | eve: 9.182 | bob: 5.031Epoch  11:  17% | abe: 4.957 | eve: 9.178 | bob: 5.031Epoch  11:  18% | abe: 4.956 | eve: 9.177 | bob: 5.030Epoch  11:  19% | abe: 4.954 | eve: 9.180 | bob: 5.028Epoch  11:  20% | abe: 4.956 | eve: 9.182 | bob: 5.030Epoch  11:  21% | abe: 4.957 | eve: 9.183 | bob: 5.031Epoch  11:  21% | abe: 4.957 | eve: 9.184 | bob: 5.031Epoch  11:  22% | abe: 4.957 | eve: 9.183 | bob: 5.032Epoch  11:  23% | abe: 4.961 | eve: 9.182 | bob: 5.035Epoch  11:  24% | abe: 4.960 | eve: 9.182 | bob: 5.034Epoch  11:  25% | abe: 4.961 | eve: 9.181 | bob: 5.034Epoch  11:  25% | abe: 4.961 | eve: 9.183 | bob: 5.033Epoch  11:  26% | abe: 4.961 | eve: 9.183 | bob: 5.032Epoch  11:  27% | abe: 4.960 | eve: 9.183 | bob: 5.030Epoch  11:  28% | abe: 4.960 | eve: 9.184 | bob: 5.030Epoch  11:  28% | abe: 4.960 | eve: 9.185 | bob: 5.029Epoch  11:  29% | abe: 4.959 | eve: 9.184 | bob: 5.029Epoch  11:  30% | abe: 4.961 | eve: 9.184 | bob: 5.032Epoch  11:  31% | abe: 4.961 | eve: 9.184 | bob: 5.033Epoch  11:  32% | abe: 4.963 | eve: 9.185 | bob: 5.035Epoch  11:  32% | abe: 4.964 | eve: 9.186 | bob: 5.037Epoch  11:  33% | abe: 4.964 | eve: 9.188 | bob: 5.039Epoch  11:  34% | abe: 4.964 | eve: 9.187 | bob: 5.038Epoch  11:  35% | abe: 4.962 | eve: 9.188 | bob: 5.037Epoch  11:  35% | abe: 4.962 | eve: 9.187 | bob: 5.036Epoch  11:  36% | abe: 4.960 | eve: 9.188 | bob: 5.035Epoch  11:  37% | abe: 4.960 | eve: 9.189 | bob: 5.034Epoch  11:  38% | abe: 4.960 | eve: 9.188 | bob: 5.034Epoch  11:  39% | abe: 4.959 | eve: 9.186 | bob: 5.033Epoch  11:  39% | abe: 4.959 | eve: 9.188 | bob: 5.032Epoch  11:  40% | abe: 4.960 | eve: 9.188 | bob: 5.033Epoch  11:  41% | abe: 4.958 | eve: 9.187 | bob: 5.032Epoch  11:  42% | abe: 4.957 | eve: 9.188 | bob: 5.030Epoch  11:  42% | abe: 4.957 | eve: 9.186 | bob: 5.031Epoch  11:  43% | abe: 4.956 | eve: 9.186 | bob: 5.031Epoch  11:  44% | abe: 4.956 | eve: 9.187 | bob: 5.031Epoch  11:  45% | abe: 4.956 | eve: 9.188 | bob: 5.031Epoch  11:  46% | abe: 4.957 | eve: 9.189 | bob: 5.031Epoch  11:  46% | abe: 4.957 | eve: 9.190 | bob: 5.031Epoch  11:  47% | abe: 4.956 | eve: 9.190 | bob: 5.029Epoch  11:  48% | abe: 4.956 | eve: 9.190 | bob: 5.029Epoch  11:  49% | abe: 4.956 | eve: 9.189 | bob: 5.029Epoch  11:  50% | abe: 4.956 | eve: 9.187 | bob: 5.028Epoch  11:  50% | abe: 4.956 | eve: 9.188 | bob: 5.027Epoch  11:  51% | abe: 4.956 | eve: 9.187 | bob: 5.027Epoch  11:  52% | abe: 4.957 | eve: 9.187 | bob: 5.028Epoch  11:  53% | abe: 4.958 | eve: 9.187 | bob: 5.029Epoch  11:  53% | abe: 4.958 | eve: 9.188 | bob: 5.030Epoch  11:  54% | abe: 4.957 | eve: 9.188 | bob: 5.030Epoch  11:  55% | abe: 4.957 | eve: 9.189 | bob: 5.030Epoch  11:  56% | abe: 4.958 | eve: 9.189 | bob: 5.031Epoch  11:  57% | abe: 4.958 | eve: 9.189 | bob: 5.031Epoch  11:  57% | abe: 4.958 | eve: 9.189 | bob: 5.031Epoch  11:  58% | abe: 4.959 | eve: 9.190 | bob: 5.031Epoch  11:  59% | abe: 4.959 | eve: 9.191 | bob: 5.031Epoch  11:  60% | abe: 4.959 | eve: 9.190 | bob: 5.031Epoch  11:  60% | abe: 4.959 | eve: 9.190 | bob: 5.030Epoch  11:  61% | abe: 4.960 | eve: 9.190 | bob: 5.031Epoch  11:  62% | abe: 4.961 | eve: 9.190 | bob: 5.032Epoch  11:  63% | abe: 4.961 | eve: 9.190 | bob: 5.033Epoch  11:  64% | abe: 4.960 | eve: 9.190 | bob: 5.032Epoch  11:  64% | abe: 4.961 | eve: 9.191 | bob: 5.033Epoch  11:  65% | abe: 4.962 | eve: 9.190 | bob: 5.034Epoch  11:  66% | abe: 4.962 | eve: 9.190 | bob: 5.033Epoch  11:  67% | abe: 4.962 | eve: 9.191 | bob: 5.033Epoch  11:  67% | abe: 4.962 | eve: 9.190 | bob: 5.034Epoch  11:  68% | abe: 4.962 | eve: 9.189 | bob: 5.032Epoch  11:  69% | abe: 4.962 | eve: 9.189 | bob: 5.032Epoch  11:  70% | abe: 4.962 | eve: 9.189 | bob: 5.032Epoch  11:  71% | abe: 4.961 | eve: 9.189 | bob: 5.032Epoch  11:  71% | abe: 4.961 | eve: 9.189 | bob: 5.031Epoch  11:  72% | abe: 4.961 | eve: 9.189 | bob: 5.031Epoch  11:  73% | abe: 4.960 | eve: 9.189 | bob: 5.030Epoch  11:  74% | abe: 4.960 | eve: 9.189 | bob: 5.030Epoch  11:  75% | abe: 4.960 | eve: 9.189 | bob: 5.030Epoch  11:  75% | abe: 4.959 | eve: 9.189 | bob: 5.030Epoch  11:  76% | abe: 4.958 | eve: 9.189 | bob: 5.029Epoch  11:  77% | abe: 4.958 | eve: 9.189 | bob: 5.029Epoch  11:  78% | abe: 4.957 | eve: 9.190 | bob: 5.028Epoch  11:  78% | abe: 4.958 | eve: 9.190 | bob: 5.029Epoch  11:  79% | abe: 4.958 | eve: 9.190 | bob: 5.028Epoch  11:  80% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  81% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  82% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  82% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  83% | abe: 4.957 | eve: 9.189 | bob: 5.027Epoch  11:  84% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  85% | abe: 4.957 | eve: 9.190 | bob: 5.027Epoch  11:  85% | abe: 4.957 | eve: 9.190 | bob: 5.028Epoch  11:  86% | abe: 4.957 | eve: 9.189 | bob: 5.027Epoch  11:  87% | abe: 4.957 | eve: 9.189 | bob: 5.027Epoch  11:  88% | abe: 4.957 | eve: 9.189 | bob: 5.027Epoch  11:  89% | abe: 4.957 | eve: 9.189 | bob: 5.026Epoch  11:  89% | abe: 4.957 | eve: 9.190 | bob: 5.026Epoch  11:  90% | abe: 4.957 | eve: 9.190 | bob: 5.026Epoch  11:  91% | abe: 4.956 | eve: 9.190 | bob: 5.025Epoch  11:  92% | abe: 4.957 | eve: 9.190 | bob: 5.025Epoch  11:  92% | abe: 4.957 | eve: 9.190 | bob: 5.025Epoch  11:  93% | abe: 4.957 | eve: 9.191 | bob: 5.025Epoch  11:  94% | abe: 4.957 | eve: 9.191 | bob: 5.025Epoch  11:  95% | abe: 4.957 | eve: 9.191 | bob: 5.025Epoch  11:  96% | abe: 4.957 | eve: 9.191 | bob: 5.025Epoch  11:  96% | abe: 4.957 | eve: 9.191 | bob: 5.025Epoch  11:  97% | abe: 4.957 | eve: 9.190 | bob: 5.026Epoch  11:  98% | abe: 4.957 | eve: 9.190 | bob: 5.026Epoch  11:  99% | abe: 4.958 | eve: 9.190 | bob: 5.026
New best Bob loss 5.02609230338021 at epoch 11
Epoch  12:   0% | abe: 5.001 | eve: 9.153 | bob: 5.076Epoch  12:   0% | abe: 4.989 | eve: 9.170 | bob: 5.057Epoch  12:   1% | abe: 5.013 | eve: 9.168 | bob: 5.072Epoch  12:   2% | abe: 4.996 | eve: 9.184 | bob: 5.055Epoch  12:   3% | abe: 4.985 | eve: 9.185 | bob: 5.038Epoch  12:   3% | abe: 4.979 | eve: 9.180 | bob: 5.030Epoch  12:   4% | abe: 4.970 | eve: 9.179 | bob: 5.020Epoch  12:   5% | abe: 4.972 | eve: 9.182 | bob: 5.022Epoch  12:   6% | abe: 4.972 | eve: 9.174 | bob: 5.023Epoch  12:   7% | abe: 4.971 | eve: 9.172 | bob: 5.023Epoch  12:   7% | abe: 4.977 | eve: 9.173 | bob: 5.032Epoch  12:   8% | abe: 4.971 | eve: 9.166 | bob: 5.029Epoch  12:   9% | abe: 4.972 | eve: 9.173 | bob: 5.032Epoch  12:  10% | abe: 4.967 | eve: 9.171 | bob: 5.028Epoch  12:  10% | abe: 4.969 | eve: 9.179 | bob: 5.030Epoch  12:  11% | abe: 4.966 | eve: 9.177 | bob: 5.027Epoch  12:  12% | abe: 4.972 | eve: 9.177 | bob: 5.031Epoch  12:  13% | abe: 4.965 | eve: 9.175 | bob: 5.022Epoch  12:  14% | abe: 4.964 | eve: 9.175 | bob: 5.020Epoch  12:  14% | abe: 4.964 | eve: 9.179 | bob: 5.019Epoch  12:  15% | abe: 4.963 | eve: 9.180 | bob: 5.017Epoch  12:  16% | abe: 4.965 | eve: 9.180 | bob: 5.019Epoch  12:  17% | abe: 4.966 | eve: 9.176 | bob: 5.021Epoch  12:  17% | abe: 4.965 | eve: 9.177 | bob: 5.020Epoch  12:  18% | abe: 4.967 | eve: 9.175 | bob: 5.023Epoch  12:  19% | abe: 4.967 | eve: 9.175 | bob: 5.024Epoch  12:  20% | abe: 4.968 | eve: 9.178 | bob: 5.026Epoch  12:  21% | abe: 4.968 | eve: 9.178 | bob: 5.027Epoch  12:  21% | abe: 4.970 | eve: 9.178 | bob: 5.029Epoch  12:  22% | abe: 4.969 | eve: 9.179 | bob: 5.028Epoch  12:  23% | abe: 4.971 | eve: 9.180 | bob: 5.031Epoch  12:  24% | abe: 4.974 | eve: 9.178 | bob: 5.033Epoch  12:  25% | abe: 4.976 | eve: 9.182 | bob: 5.035Epoch  12:  25% | abe: 4.977 | eve: 9.183 | bob: 5.036Epoch  12:  26% | abe: 4.976 | eve: 9.182 | bob: 5.036Epoch  12:  27% | abe: 4.974 | eve: 9.183 | bob: 5.034Epoch  12:  28% | abe: 4.974 | eve: 9.183 | bob: 5.035Epoch  12:  28% | abe: 4.973 | eve: 9.182 | bob: 5.034Epoch  12:  29% | abe: 4.973 | eve: 9.183 | bob: 5.035Epoch  12:  30% | abe: 4.972 | eve: 9.185 | bob: 5.035Epoch  12:  31% | abe: 4.971 | eve: 9.185 | bob: 5.034Epoch  12:  32% | abe: 4.969 | eve: 9.185 | bob: 5.032Epoch  12:  32% | abe: 4.969 | eve: 9.185 | bob: 5.032Epoch  12:  33% | abe: 4.969 | eve: 9.186 | bob: 5.032Epoch  12:  34% | abe: 4.970 | eve: 9.187 | bob: 5.033Epoch  12:  35% | abe: 4.968 | eve: 9.187 | bob: 5.031Epoch  12:  35% | abe: 4.967 | eve: 9.188 | bob: 5.029Epoch  12:  36% | abe: 4.968 | eve: 9.188 | bob: 5.030Epoch  12:  37% | abe: 4.967 | eve: 9.188 | bob: 5.028Epoch  12:  38% | abe: 4.967 | eve: 9.188 | bob: 5.027Epoch  12:  39% | abe: 4.966 | eve: 9.188 | bob: 5.026Epoch  12:  39% | abe: 4.964 | eve: 9.187 | bob: 5.024Epoch  12:  40% | abe: 4.964 | eve: 9.189 | bob: 5.024Epoch  12:  41% | abe: 4.963 | eve: 9.188 | bob: 5.023Epoch  12:  42% | abe: 4.963 | eve: 9.188 | bob: 5.023Epoch  12:  42% | abe: 4.962 | eve: 9.189 | bob: 5.022Epoch  12:  43% | abe: 4.964 | eve: 9.190 | bob: 5.024Epoch  12:  44% | abe: 4.963 | eve: 9.191 | bob: 5.024Epoch  12:  45% | abe: 4.964 | eve: 9.190 | bob: 5.024Epoch  12:  46% | abe: 4.963 | eve: 9.190 | bob: 5.023Epoch  12:  46% | abe: 4.964 | eve: 9.192 | bob: 5.024Epoch  12:  47% | abe: 4.964 | eve: 9.190 | bob: 5.024Epoch  12:  48% | abe: 4.965 | eve: 9.189 | bob: 5.024Epoch  12:  49% | abe: 4.965 | eve: 9.188 | bob: 5.024Epoch  12:  50% | abe: 4.963 | eve: 9.188 | bob: 5.023Epoch  12:  50% | abe: 4.964 | eve: 9.189 | bob: 5.024Epoch  12:  51% | abe: 4.964 | eve: 9.190 | bob: 5.024Epoch  12:  52% | abe: 4.965 | eve: 9.190 | bob: 5.025Epoch  12:  53% | abe: 4.965 | eve: 9.190 | bob: 5.025Epoch  12:  53% | abe: 4.964 | eve: 9.190 | bob: 5.024Epoch  12:  54% | abe: 4.965 | eve: 9.190 | bob: 5.024Epoch  12:  55% | abe: 4.964 | eve: 9.190 | bob: 5.023Epoch  12:  56% | abe: 4.964 | eve: 9.191 | bob: 5.022Epoch  12:  57% | abe: 4.964 | eve: 9.191 | bob: 5.023Epoch  12:  57% | abe: 4.964 | eve: 9.192 | bob: 5.023Epoch  12:  58% | abe: 4.965 | eve: 9.192 | bob: 5.024Epoch  12:  59% | abe: 4.965 | eve: 9.192 | bob: 5.025Epoch  12:  60% | abe: 4.966 | eve: 9.192 | bob: 5.026Epoch  12:  60% | abe: 4.965 | eve: 9.192 | bob: 5.026Epoch  12:  61% | abe: 4.966 | eve: 9.193 | bob: 5.026Epoch  12:  62% | abe: 4.964 | eve: 9.193 | bob: 5.025Epoch  12:  63% | abe: 4.965 | eve: 9.193 | bob: 5.026Epoch  12:  64% | abe: 4.965 | eve: 9.193 | bob: 5.025Epoch  12:  64% | abe: 4.964 | eve: 9.193 | bob: 5.025Epoch  12:  65% | abe: 4.965 | eve: 9.193 | bob: 5.025Epoch  12:  66% | abe: 4.965 | eve: 9.192 | bob: 5.026Epoch  12:  67% | abe: 4.965 | eve: 9.192 | bob: 5.026Epoch  12:  67% | abe: 4.966 | eve: 9.192 | bob: 5.026Epoch  12:  68% | abe: 4.966 | eve: 9.191 | bob: 5.027Epoch  12:  69% | abe: 4.965 | eve: 9.190 | bob: 5.026Epoch  12:  70% | abe: 4.966 | eve: 9.191 | bob: 5.027Epoch  12:  71% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  71% | abe: 4.966 | eve: 9.192 | bob: 5.027Epoch  12:  72% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  73% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  74% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  75% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  75% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  76% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  77% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  78% | abe: 4.966 | eve: 9.191 | bob: 5.028Epoch  12:  78% | abe: 4.966 | eve: 9.192 | bob: 5.028Epoch  12:  79% | abe: 4.966 | eve: 9.192 | bob: 5.028Epoch  12:  80% | abe: 4.966 | eve: 9.193 | bob: 5.028Epoch  12:  81% | abe: 4.966 | eve: 9.192 | bob: 5.029Epoch  12:  82% | abe: 4.966 | eve: 9.193 | bob: 5.029Epoch  12:  82% | abe: 4.965 | eve: 9.193 | bob: 5.028Epoch  12:  83% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  84% | abe: 4.964 | eve: 9.192 | bob: 5.027Epoch  12:  85% | abe: 4.964 | eve: 9.192 | bob: 5.026Epoch  12:  85% | abe: 4.965 | eve: 9.192 | bob: 5.027Epoch  12:  86% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  87% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  88% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  89% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  89% | abe: 4.965 | eve: 9.190 | bob: 5.027Epoch  12:  90% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  91% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  92% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  92% | abe: 4.965 | eve: 9.190 | bob: 5.027Epoch  12:  93% | abe: 4.965 | eve: 9.191 | bob: 5.027Epoch  12:  94% | abe: 4.964 | eve: 9.191 | bob: 5.027Epoch  12:  95% | abe: 4.964 | eve: 9.191 | bob: 5.027Epoch  12:  96% | abe: 4.963 | eve: 9.191 | bob: 5.027Epoch  12:  96% | abe: 4.963 | eve: 9.191 | bob: 5.027Epoch  12:  97% | abe: 4.964 | eve: 9.191 | bob: 5.027Epoch  12:  98% | abe: 4.964 | eve: 9.192 | bob: 5.027Epoch  12:  99% | abe: 4.963 | eve: 9.192 | bob: 5.026Epoch  13:   0% | abe: 4.924 | eve: 9.284 | bob: 4.973Epoch  13:   0% | abe: 4.959 | eve: 9.256 | bob: 5.011Epoch  13:   1% | abe: 4.955 | eve: 9.257 | bob: 5.012Epoch  13:   2% | abe: 4.933 | eve: 9.262 | bob: 4.994Epoch  13:   3% | abe: 4.925 | eve: 9.262 | bob: 4.991Epoch  13:   3% | abe: 4.926 | eve: 9.244 | bob: 4.996Epoch  13:   4% | abe: 4.935 | eve: 9.231 | bob: 5.006Epoch  13:   5% | abe: 4.936 | eve: 9.214 | bob: 5.008Epoch  13:   6% | abe: 4.942 | eve: 9.217 | bob: 5.014Epoch  13:   7% | abe: 4.938 | eve: 9.216 | bob: 5.008Epoch  13:   7% | abe: 4.950 | eve: 9.214 | bob: 5.017Epoch  13:   8% | abe: 4.956 | eve: 9.210 | bob: 5.022Epoch  13:   9% | abe: 4.956 | eve: 9.206 | bob: 5.020Epoch  13:  10% | abe: 4.953 | eve: 9.207 | bob: 5.017Epoch  13:  10% | abe: 4.952 | eve: 9.210 | bob: 5.017Epoch  13:  11% | abe: 4.957 | eve: 9.209 | bob: 5.024Epoch  13:  12% | abe: 4.957 | eve: 9.208 | bob: 5.025Epoch  13:  13% | abe: 4.954 | eve: 9.205 | bob: 5.024Epoch  13:  14% | abe: 4.952 | eve: 9.206 | bob: 5.024Epoch  13:  14% | abe: 4.950 | eve: 9.205 | bob: 5.024Epoch  13:  15% | abe: 4.952 | eve: 9.203 | bob: 5.025Epoch  13:  16% | abe: 4.951 | eve: 9.201 | bob: 5.024Epoch  13:  17% | abe: 4.951 | eve: 9.198 | bob: 5.023Epoch  13:  17% | abe: 4.951 | eve: 9.196 | bob: 5.022Epoch  13:  18% | abe: 4.953 | eve: 9.196 | bob: 5.023Epoch  13:  19% | abe: 4.955 | eve: 9.196 | bob: 5.025Epoch  13:  20% | abe: 4.956 | eve: 9.194 | bob: 5.025Epoch  13:  21% | abe: 4.955 | eve: 9.193 | bob: 5.024Epoch  13:  21% | abe: 4.953 | eve: 9.194 | bob: 5.022Epoch  13:  22% | abe: 4.951 | eve: 9.194 | bob: 5.019Epoch  13:  23% | abe: 4.950 | eve: 9.195 | bob: 5.019Epoch  13:  24% | abe: 4.948 | eve: 9.195 | bob: 5.017Epoch  13:  25% | abe: 4.949 | eve: 9.198 | bob: 5.019Epoch  13:  25% | abe: 4.951 | eve: 9.200 | bob: 5.020Epoch  13:  26% | abe: 4.950 | eve: 9.201 | bob: 5.020Epoch  13:  27% | abe: 4.951 | eve: 9.202 | bob: 5.021Epoch  13:  28% | abe: 4.952 | eve: 9.203 | bob: 5.023Epoch  13:  28% | abe: 4.953 | eve: 9.201 | bob: 5.023Epoch  13:  29% | abe: 4.950 | eve: 9.204 | bob: 5.020Epoch  13:  30% | abe: 4.951 | eve: 9.202 | bob: 5.020Epoch  13:  31% | abe: 4.951 | eve: 9.203 | bob: 5.020Epoch  13:  32% | abe: 4.954 | eve: 9.204 | bob: 5.022Epoch  13:  32% | abe: 4.953 | eve: 9.203 | bob: 5.020Epoch  13:  33% | abe: 4.952 | eve: 9.203 | bob: 5.018Epoch  13:  34% | abe: 4.951 | eve: 9.205 | bob: 5.017Epoch  13:  35% | abe: 4.951 | eve: 9.204 | bob: 5.017Epoch  13:  35% | abe: 4.952 | eve: 9.205 | bob: 5.019Epoch  13:  36% | abe: 4.951 | eve: 9.204 | bob: 5.018Epoch  13:  37% | abe: 4.950 | eve: 9.203 | bob: 5.018Epoch  13:  38% | abe: 4.951 | eve: 9.202 | bob: 5.020Epoch  13:  39% | abe: 4.950 | eve: 9.202 | bob: 5.019Epoch  13:  39% | abe: 4.950 | eve: 9.201 | bob: 5.018Epoch  13:  40% | abe: 4.951 | eve: 9.202 | bob: 5.019Epoch  13:  41% | abe: 4.951 | eve: 9.202 | bob: 5.018Epoch  13:  42% | abe: 4.950 | eve: 9.201 | bob: 5.017Epoch  13:  42% | abe: 4.950 | eve: 9.201 | bob: 5.017Epoch  13:  43% | abe: 4.951 | eve: 9.199 | bob: 5.017Epoch  13:  44% | abe: 4.952 | eve: 9.199 | bob: 5.018Epoch  13:  45% | abe: 4.953 | eve: 9.200 | bob: 5.019Epoch  13:  46% | abe: 4.953 | eve: 9.199 | bob: 5.019Epoch  13:  46% | abe: 4.954 | eve: 9.198 | bob: 5.020Epoch  13:  47% | abe: 4.954 | eve: 9.200 | bob: 5.021Epoch  13:  48% | abe: 4.955 | eve: 9.200 | bob: 5.021Epoch  13:  49% | abe: 4.954 | eve: 9.199 | bob: 5.021Epoch  13:  50% | abe: 4.953 | eve: 9.198 | bob: 5.020Epoch  13:  50% | abe: 4.953 | eve: 9.199 | bob: 5.020Epoch  13:  51% | abe: 4.953 | eve: 9.199 | bob: 5.019Epoch  13:  52% | abe: 4.954 | eve: 9.200 | bob: 5.020Epoch  13:  53% | abe: 4.955 | eve: 9.199 | bob: 5.020Epoch  13:  53% | abe: 4.955 | eve: 9.200 | bob: 5.020Epoch  13:  54% | abe: 4.954 | eve: 9.200 | bob: 5.019Epoch  13:  55% | abe: 4.954 | eve: 9.199 | bob: 5.019Epoch  13:  56% | abe: 4.954 | eve: 9.199 | bob: 5.019Epoch  13:  57% | abe: 4.954 | eve: 9.200 | bob: 5.019Epoch  13:  57% | abe: 4.954 | eve: 9.201 | bob: 5.020Epoch  13:  58% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  59% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  60% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  60% | abe: 4.955 | eve: 9.201 | bob: 5.022Epoch  13:  61% | abe: 4.956 | eve: 9.201 | bob: 5.022Epoch  13:  62% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  63% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  64% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  64% | abe: 4.955 | eve: 9.200 | bob: 5.021Epoch  13:  65% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  66% | abe: 4.954 | eve: 9.202 | bob: 5.019Epoch  13:  67% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  67% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  68% | abe: 4.955 | eve: 9.201 | bob: 5.019Epoch  13:  69% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  70% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  71% | abe: 4.955 | eve: 9.202 | bob: 5.020Epoch  13:  71% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  72% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  73% | abe: 4.954 | eve: 9.202 | bob: 5.020Epoch  13:  74% | abe: 4.955 | eve: 9.203 | bob: 5.020Epoch  13:  75% | abe: 4.954 | eve: 9.203 | bob: 5.020Epoch  13:  75% | abe: 4.954 | eve: 9.203 | bob: 5.019Epoch  13:  76% | abe: 4.954 | eve: 9.203 | bob: 5.020Epoch  13:  77% | abe: 4.954 | eve: 9.202 | bob: 5.019Epoch  13:  78% | abe: 4.953 | eve: 9.201 | bob: 5.018Epoch  13:  78% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  79% | abe: 4.953 | eve: 9.201 | bob: 5.018Epoch  13:  80% | abe: 4.954 | eve: 9.202 | bob: 5.019Epoch  13:  81% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  82% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  82% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  83% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  84% | abe: 4.955 | eve: 9.200 | bob: 5.021Epoch  13:  85% | abe: 4.955 | eve: 9.200 | bob: 5.021Epoch  13:  85% | abe: 4.955 | eve: 9.201 | bob: 5.021Epoch  13:  86% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  87% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  88% | abe: 4.955 | eve: 9.200 | bob: 5.019Epoch  13:  89% | abe: 4.955 | eve: 9.200 | bob: 5.019Epoch  13:  89% | abe: 4.955 | eve: 9.201 | bob: 5.019Epoch  13:  90% | abe: 4.954 | eve: 9.201 | bob: 5.019Epoch  13:  91% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  92% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  92% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  93% | abe: 4.955 | eve: 9.202 | bob: 5.020Epoch  13:  94% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  95% | abe: 4.954 | eve: 9.202 | bob: 5.020Epoch  13:  96% | abe: 4.954 | eve: 9.202 | bob: 5.019Epoch  13:  96% | abe: 4.955 | eve: 9.202 | bob: 5.020Epoch  13:  97% | abe: 4.955 | eve: 9.201 | bob: 5.020Epoch  13:  98% | abe: 4.955 | eve: 9.202 | bob: 5.019Epoch  13:  99% | abe: 4.955 | eve: 9.201 | bob: 5.019
New best Bob loss 5.0189711224365965 at epoch 13
Epoch  14:   0% | abe: 4.939 | eve: 9.249 | bob: 4.986Epoch  14:   0% | abe: 4.914 | eve: 9.202 | bob: 4.968Epoch  14:   1% | abe: 4.939 | eve: 9.207 | bob: 5.005Epoch  14:   2% | abe: 4.936 | eve: 9.204 | bob: 5.012Epoch  14:   3% | abe: 4.943 | eve: 9.220 | bob: 5.027Epoch  14:   3% | abe: 4.956 | eve: 9.204 | bob: 5.048Epoch  14:   4% | abe: 4.954 | eve: 9.211 | bob: 5.051Epoch  14:   5% | abe: 4.957 | eve: 9.210 | bob: 5.058Epoch  14:   6% | abe: 4.951 | eve: 9.211 | bob: 5.054Epoch  14:   7% | abe: 4.947 | eve: 9.203 | bob: 5.048Epoch  14:   7% | abe: 4.942 | eve: 9.212 | bob: 5.042Epoch  14:   8% | abe: 4.945 | eve: 9.210 | bob: 5.043Epoch  14:   9% | abe: 4.941 | eve: 9.204 | bob: 5.034Epoch  14:  10% | abe: 4.942 | eve: 9.211 | bob: 5.030Epoch  14:  10% | abe: 4.945 | eve: 9.214 | bob: 5.031Epoch  14:  11% | abe: 4.946 | eve: 9.215 | bob: 5.031Epoch  14:  12% | abe: 4.945 | eve: 9.214 | bob: 5.031Epoch  14:  13% | abe: 4.946 | eve: 9.209 | bob: 5.032Epoch  14:  14% | abe: 4.949 | eve: 9.210 | bob: 5.037Epoch  14:  14% | abe: 4.950 | eve: 9.215 | bob: 5.040Epoch  14:  15% | abe: 4.948 | eve: 9.214 | bob: 5.040Epoch  14:  16% | abe: 4.948 | eve: 9.213 | bob: 5.040Epoch  14:  17% | abe: 4.951 | eve: 9.214 | bob: 5.043Epoch  14:  17% | abe: 4.950 | eve: 9.211 | bob: 5.041Epoch  14:  18% | abe: 4.951 | eve: 9.214 | bob: 5.043Epoch  14:  19% | abe: 4.954 | eve: 9.216 | bob: 5.046Epoch  14:  20% | abe: 4.954 | eve: 9.214 | bob: 5.045Epoch  14:  21% | abe: 4.952 | eve: 9.215 | bob: 5.042Epoch  14:  21% | abe: 4.951 | eve: 9.216 | bob: 5.039Epoch  14:  22% | abe: 4.953 | eve: 9.217 | bob: 5.041Epoch  14:  23% | abe: 4.954 | eve: 9.219 | bob: 5.043Epoch  14:  24% | abe: 4.952 | eve: 9.220 | bob: 5.040Epoch  14:  25% | abe: 4.953 | eve: 9.222 | bob: 5.040Epoch  14:  25% | abe: 4.954 | eve: 9.219 | bob: 5.042Epoch  14:  26% | abe: 4.955 | eve: 9.217 | bob: 5.043Epoch  14:  27% | abe: 4.954 | eve: 9.215 | bob: 5.043Epoch  14:  28% | abe: 4.957 | eve: 9.216 | bob: 5.046Epoch  14:  28% | abe: 4.957 | eve: 9.216 | bob: 5.045Epoch  14:  29% | abe: 4.957 | eve: 9.215 | bob: 5.046Epoch  14:  30% | abe: 4.957 | eve: 9.215 | bob: 5.046Epoch  14:  31% | abe: 4.957 | eve: 9.214 | bob: 5.046Epoch  14:  32% | abe: 4.960 | eve: 9.213 | bob: 5.049Epoch  14:  32% | abe: 4.959 | eve: 9.213 | bob: 5.047Epoch  14:  33% | abe: 4.959 | eve: 9.212 | bob: 5.047Epoch  14:  34% | abe: 4.959 | eve: 9.213 | bob: 5.047Epoch  14:  35% | abe: 4.959 | eve: 9.211 | bob: 5.048Epoch  14:  35% | abe: 4.959 | eve: 9.212 | bob: 5.047Epoch  14:  36% | abe: 4.959 | eve: 9.212 | bob: 5.047Epoch  14:  37% | abe: 4.958 | eve: 9.212 | bob: 5.046Epoch  14:  38% | abe: 4.959 | eve: 9.211 | bob: 5.047Epoch  14:  39% | abe: 4.957 | eve: 9.212 | bob: 5.045Epoch  14:  39% | abe: 4.956 | eve: 9.212 | bob: 5.044Epoch  14:  40% | abe: 4.956 | eve: 9.213 | bob: 5.044Epoch  14:  41% | abe: 4.956 | eve: 9.213 | bob: 5.044Epoch  14:  42% | abe: 4.956 | eve: 9.212 | bob: 5.044Epoch  14:  42% | abe: 4.956 | eve: 9.213 | bob: 5.044Epoch  14:  43% | abe: 4.957 | eve: 9.212 | bob: 5.044Epoch  14:  44% | abe: 4.956 | eve: 9.212 | bob: 5.044Epoch  14:  45% | abe: 4.957 | eve: 9.213 | bob: 5.045Epoch  14:  46% | abe: 4.957 | eve: 9.212 | bob: 5.045Epoch  14:  46% | abe: 4.956 | eve: 9.212 | bob: 5.045Epoch  14:  47% | abe: 4.958 | eve: 9.212 | bob: 5.047Epoch  14:  48% | abe: 4.957 | eve: 9.212 | bob: 5.047Epoch  14:  49% | abe: 4.957 | eve: 9.212 | bob: 5.047Epoch  14:  50% | abe: 4.957 | eve: 9.213 | bob: 5.047Epoch  14:  50% | abe: 4.957 | eve: 9.213 | bob: 5.048Epoch  14:  51% | abe: 4.957 | eve: 9.214 | bob: 5.047Epoch  14:  52% | abe: 4.956 | eve: 9.213 | bob: 5.046Epoch  14:  53% | abe: 4.957 | eve: 9.211 | bob: 5.047Epoch  14:  53% | abe: 4.957 | eve: 9.212 | bob: 5.047Epoch  14:  54% | abe: 4.957 | eve: 9.212 | bob: 5.047Epoch  14:  55% | abe: 4.958 | eve: 9.212 | bob: 5.047Epoch  14:  56% | abe: 4.957 | eve: 9.212 | bob: 5.047Epoch  14:  57% | abe: 4.958 | eve: 9.211 | bob: 5.047Epoch  14:  57% | abe: 4.957 | eve: 9.211 | bob: 5.047Epoch  14:  58% | abe: 4.956 | eve: 9.211 | bob: 5.046Epoch  14:  59% | abe: 4.955 | eve: 9.211 | bob: 5.045Epoch  14:  60% | abe: 4.955 | eve: 9.212 | bob: 5.044Epoch  14:  60% | abe: 4.955 | eve: 9.211 | bob: 5.044Epoch  14:  61% | abe: 4.956 | eve: 9.211 | bob: 5.045Epoch  14:  62% | abe: 4.955 | eve: 9.211 | bob: 5.044Epoch  14:  63% | abe: 4.955 | eve: 9.210 | bob: 5.044Epoch  14:  64% | abe: 4.955 | eve: 9.211 | bob: 5.044Epoch  14:  64% | abe: 4.956 | eve: 9.209 | bob: 5.045Epoch  14:  65% | abe: 4.956 | eve: 9.210 | bob: 5.045Epoch  14:  66% | abe: 4.956 | eve: 9.210 | bob: 5.045Epoch  14:  67% | abe: 4.956 | eve: 9.210 | bob: 5.045Epoch  14:  67% | abe: 4.956 | eve: 9.210 | bob: 5.045Epoch  14:  68% | abe: 4.957 | eve: 9.210 | bob: 5.046Epoch  14:  69% | abe: 4.957 | eve: 9.209 | bob: 5.045Epoch  14:  70% | abe: 4.957 | eve: 9.210 | bob: 5.045Epoch  14:  71% | abe: 4.956 | eve: 9.210 | bob: 5.044Epoch  14:  71% | abe: 4.956 | eve: 9.210 | bob: 5.043Epoch  14:  72% | abe: 4.956 | eve: 9.210 | bob: 5.043Epoch  14:  73% | abe: 4.956 | eve: 9.209 | bob: 5.043Epoch  14:  74% | abe: 4.957 | eve: 9.210 | bob: 5.043Epoch  14:  75% | abe: 4.956 | eve: 9.210 | bob: 5.041Epoch  14:  75% | abe: 4.956 | eve: 9.210 | bob: 5.042Epoch  14:  76% | abe: 4.956 | eve: 9.210 | bob: 5.042Epoch  14:  77% | abe: 4.956 | eve: 9.210 | bob: 5.042Epoch  14:  78% | abe: 4.955 | eve: 9.210 | bob: 5.041Epoch  14:  78% | abe: 4.955 | eve: 9.210 | bob: 5.040Epoch  14:  79% | abe: 4.955 | eve: 9.209 | bob: 5.040Epoch  14:  80% | abe: 4.955 | eve: 9.209 | bob: 5.040Epoch  14:  81% | abe: 4.955 | eve: 9.210 | bob: 5.040Epoch  14:  82% | abe: 4.955 | eve: 9.210 | bob: 5.040Epoch  14:  82% | abe: 4.955 | eve: 9.210 | bob: 5.040Epoch  14:  83% | abe: 4.955 | eve: 9.209 | bob: 5.040Epoch  14:  84% | abe: 4.955 | eve: 9.209 | bob: 5.040Epoch  14:  85% | abe: 4.955 | eve: 9.210 | bob: 5.039Epoch  14:  85% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  86% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  87% | abe: 4.955 | eve: 9.209 | bob: 5.039Epoch  14:  88% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  89% | abe: 4.956 | eve: 9.209 | bob: 5.039Epoch  14:  89% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  90% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  91% | abe: 4.955 | eve: 9.209 | bob: 5.038Epoch  14:  92% | abe: 4.955 | eve: 9.209 | bob: 5.037Epoch  14:  92% | abe: 4.954 | eve: 9.209 | bob: 5.037Epoch  14:  93% | abe: 4.954 | eve: 9.208 | bob: 5.037Epoch  14:  94% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  14:  95% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  14:  96% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  14:  96% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  14:  97% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  14:  98% | abe: 4.956 | eve: 9.208 | bob: 5.038Epoch  14:  99% | abe: 4.955 | eve: 9.208 | bob: 5.038Epoch  15:   0% | abe: 5.001 | eve: 9.182 | bob: 5.088Epoch  15:   0% | abe: 4.976 | eve: 9.189 | bob: 5.062Epoch  15:   1% | abe: 4.977 | eve: 9.199 | bob: 5.061Epoch  15:   2% | abe: 4.970 | eve: 9.196 | bob: 5.053Epoch  15:   3% | abe: 4.965 | eve: 9.206 | bob: 5.048Epoch  15:   3% | abe: 4.947 | eve: 9.207 | bob: 5.027Epoch  15:   4% | abe: 4.951 | eve: 9.215 | bob: 5.030Epoch  15:   5% | abe: 4.960 | eve: 9.212 | bob: 5.038Epoch  15:   6% | abe: 4.950 | eve: 9.221 | bob: 5.026Epoch  15:   7% | abe: 4.951 | eve: 9.225 | bob: 5.027Epoch  15:   7% | abe: 4.943 | eve: 9.216 | bob: 5.019Epoch  15:   8% | abe: 4.945 | eve: 9.218 | bob: 5.022Epoch  15:   9% | abe: 4.947 | eve: 9.215 | bob: 5.023Epoch  15:  10% | abe: 4.950 | eve: 9.213 | bob: 5.025Epoch  15:  10% | abe: 4.949 | eve: 9.211 | bob: 5.023Epoch  15:  11% | abe: 4.953 | eve: 9.213 | bob: 5.027Epoch  15:  12% | abe: 4.956 | eve: 9.212 | bob: 5.030Epoch  15:  13% | abe: 4.952 | eve: 9.213 | bob: 5.027Epoch  15:  14% | abe: 4.950 | eve: 9.215 | bob: 5.026Epoch  15:  14% | abe: 4.951 | eve: 9.215 | bob: 5.027Epoch  15:  15% | abe: 4.954 | eve: 9.217 | bob: 5.030Epoch  15:  16% | abe: 4.954 | eve: 9.217 | bob: 5.030Epoch  15:  17% | abe: 4.952 | eve: 9.211 | bob: 5.029Epoch  15:  17% | abe: 4.952 | eve: 9.210 | bob: 5.028Epoch  15:  18% | abe: 4.953 | eve: 9.207 | bob: 5.030Epoch  15:  19% | abe: 4.955 | eve: 9.205 | bob: 5.031Epoch  15:  20% | abe: 4.957 | eve: 9.207 | bob: 5.033Epoch  15:  21% | abe: 4.957 | eve: 9.208 | bob: 5.031Epoch  15:  21% | abe: 4.956 | eve: 9.210 | bob: 5.030Epoch  15:  22% | abe: 4.955 | eve: 9.208 | bob: 5.029Epoch  15:  23% | abe: 4.953 | eve: 9.207 | bob: 5.028Epoch  15:  24% | abe: 4.954 | eve: 9.207 | bob: 5.028Epoch  15:  25% | abe: 4.954 | eve: 9.208 | bob: 5.029Epoch  15:  25% | abe: 4.953 | eve: 9.211 | bob: 5.028Epoch  15:  26% | abe: 4.951 | eve: 9.211 | bob: 5.027Epoch  15:  27% | abe: 4.950 | eve: 9.209 | bob: 5.027Epoch  15:  28% | abe: 4.949 | eve: 9.209 | bob: 5.027Epoch  15:  28% | abe: 4.948 | eve: 9.208 | bob: 5.026Epoch  15:  29% | abe: 4.950 | eve: 9.207 | bob: 5.027Epoch  15:  30% | abe: 4.951 | eve: 9.208 | bob: 5.028Epoch  15:  31% | abe: 4.949 | eve: 9.207 | bob: 5.026Epoch  15:  32% | abe: 4.948 | eve: 9.207 | bob: 5.025Epoch  15:  32% | abe: 4.948 | eve: 9.208 | bob: 5.025Epoch  15:  33% | abe: 4.948 | eve: 9.209 | bob: 5.025Epoch  15:  34% | abe: 4.948 | eve: 9.208 | bob: 5.026Epoch  15:  35% | abe: 4.947 | eve: 9.209 | bob: 5.024Epoch  15:  35% | abe: 4.946 | eve: 9.207 | bob: 5.024Epoch  15:  36% | abe: 4.946 | eve: 9.207 | bob: 5.023Epoch  15:  37% | abe: 4.948 | eve: 9.207 | bob: 5.026Epoch  15:  38% | abe: 4.948 | eve: 9.208 | bob: 5.026Epoch  15:  39% | abe: 4.948 | eve: 9.208 | bob: 5.026Epoch  15:  39% | abe: 4.948 | eve: 9.208 | bob: 5.026Epoch  15:  40% | abe: 4.947 | eve: 9.208 | bob: 5.025Epoch  15:  41% | abe: 4.947 | eve: 9.208 | bob: 5.025Epoch  15:  42% | abe: 4.948 | eve: 9.209 | bob: 5.026Epoch  15:  42% | abe: 4.946 | eve: 9.208 | bob: 5.024Epoch  15:  43% | abe: 4.947 | eve: 9.207 | bob: 5.026Epoch  15:  44% | abe: 4.949 | eve: 9.208 | bob: 5.028Epoch  15:  45% | abe: 4.950 | eve: 9.208 | bob: 5.029Epoch  15:  46% | abe: 4.949 | eve: 9.208 | bob: 5.028Epoch  15:  46% | abe: 4.951 | eve: 9.207 | bob: 5.029Epoch  15:  47% | abe: 4.950 | eve: 9.209 | bob: 5.028Epoch  15:  48% | abe: 4.950 | eve: 9.208 | bob: 5.028Epoch  15:  49% | abe: 4.949 | eve: 9.209 | bob: 5.027Epoch  15:  50% | abe: 4.950 | eve: 9.209 | bob: 5.028Epoch  15:  50% | abe: 4.950 | eve: 9.210 | bob: 5.029Epoch  15:  51% | abe: 4.950 | eve: 9.211 | bob: 5.030Epoch  15:  52% | abe: 4.950 | eve: 9.210 | bob: 5.030Epoch  15:  53% | abe: 4.950 | eve: 9.211 | bob: 5.030Epoch  15:  53% | abe: 4.950 | eve: 9.211 | bob: 5.030Epoch  15:  54% | abe: 4.949 | eve: 9.209 | bob: 5.029Epoch  15:  55% | abe: 4.950 | eve: 9.208 | bob: 5.031Epoch  15:  56% | abe: 4.951 | eve: 9.209 | bob: 5.031Epoch  15:  57% | abe: 4.951 | eve: 9.209 | bob: 5.031Epoch  15:  57% | abe: 4.952 | eve: 9.208 | bob: 5.032Epoch  15:  58% | abe: 4.952 | eve: 9.208 | bob: 5.032Epoch  15:  59% | abe: 4.951 | eve: 9.208 | bob: 5.031Epoch  15:  60% | abe: 4.951 | eve: 9.208 | bob: 5.031Epoch  15:  60% | abe: 4.951 | eve: 9.207 | bob: 5.031Epoch  15:  61% | abe: 4.952 | eve: 9.207 | bob: 5.031Epoch  15:  62% | abe: 4.952 | eve: 9.207 | bob: 5.032Epoch  15:  63% | abe: 4.952 | eve: 9.208 | bob: 5.032Epoch  15:  64% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  64% | abe: 4.954 | eve: 9.207 | bob: 5.035Epoch  15:  65% | abe: 4.954 | eve: 9.207 | bob: 5.035Epoch  15:  66% | abe: 4.955 | eve: 9.206 | bob: 5.036Epoch  15:  67% | abe: 4.955 | eve: 9.207 | bob: 5.036Epoch  15:  67% | abe: 4.954 | eve: 9.206 | bob: 5.036Epoch  15:  68% | abe: 4.954 | eve: 9.206 | bob: 5.036Epoch  15:  69% | abe: 4.953 | eve: 9.206 | bob: 5.035Epoch  15:  70% | abe: 4.954 | eve: 9.206 | bob: 5.035Epoch  15:  71% | abe: 4.953 | eve: 9.206 | bob: 5.035Epoch  15:  71% | abe: 4.953 | eve: 9.206 | bob: 5.035Epoch  15:  72% | abe: 4.953 | eve: 9.205 | bob: 5.034Epoch  15:  73% | abe: 4.953 | eve: 9.206 | bob: 5.035Epoch  15:  74% | abe: 4.953 | eve: 9.206 | bob: 5.034Epoch  15:  75% | abe: 4.952 | eve: 9.206 | bob: 5.034Epoch  15:  75% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  76% | abe: 4.952 | eve: 9.208 | bob: 5.033Epoch  15:  77% | abe: 4.952 | eve: 9.208 | bob: 5.033Epoch  15:  78% | abe: 4.952 | eve: 9.208 | bob: 5.033Epoch  15:  78% | abe: 4.952 | eve: 9.208 | bob: 5.033Epoch  15:  79% | abe: 4.953 | eve: 9.208 | bob: 5.033Epoch  15:  80% | abe: 4.952 | eve: 9.209 | bob: 5.033Epoch  15:  81% | abe: 4.952 | eve: 9.209 | bob: 5.033Epoch  15:  82% | abe: 4.953 | eve: 9.210 | bob: 5.033Epoch  15:  82% | abe: 4.953 | eve: 9.210 | bob: 5.033Epoch  15:  83% | abe: 4.953 | eve: 9.210 | bob: 5.033Epoch  15:  84% | abe: 4.953 | eve: 9.210 | bob: 5.033Epoch  15:  85% | abe: 4.953 | eve: 9.209 | bob: 5.033Epoch  15:  85% | abe: 4.953 | eve: 9.210 | bob: 5.034Epoch  15:  86% | abe: 4.953 | eve: 9.209 | bob: 5.033Epoch  15:  87% | abe: 4.953 | eve: 9.210 | bob: 5.034Epoch  15:  88% | abe: 4.953 | eve: 9.209 | bob: 5.034Epoch  15:  89% | abe: 4.954 | eve: 9.209 | bob: 5.034Epoch  15:  89% | abe: 4.953 | eve: 9.209 | bob: 5.034Epoch  15:  90% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  91% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  92% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  92% | abe: 4.953 | eve: 9.207 | bob: 5.034Epoch  15:  93% | abe: 4.954 | eve: 9.208 | bob: 5.034Epoch  15:  94% | abe: 4.954 | eve: 9.208 | bob: 5.034Epoch  15:  95% | abe: 4.954 | eve: 9.208 | bob: 5.035Epoch  15:  96% | abe: 4.954 | eve: 9.208 | bob: 5.035Epoch  15:  96% | abe: 4.954 | eve: 9.208 | bob: 5.035Epoch  15:  97% | abe: 4.954 | eve: 9.207 | bob: 5.035Epoch  15:  98% | abe: 4.954 | eve: 9.207 | bob: 5.035Epoch  15:  99% | abe: 4.954 | eve: 9.207 | bob: 5.035Epoch  16:   0% | abe: 4.899 | eve: 9.211 | bob: 4.976Epoch  16:   0% | abe: 4.920 | eve: 9.215 | bob: 4.993Epoch  16:   1% | abe: 4.957 | eve: 9.217 | bob: 5.039Epoch  16:   2% | abe: 4.949 | eve: 9.213 | bob: 5.035Epoch  16:   3% | abe: 4.951 | eve: 9.220 | bob: 5.042Epoch  16:   3% | abe: 4.953 | eve: 9.213 | bob: 5.045Epoch  16:   4% | abe: 4.943 | eve: 9.226 | bob: 5.037Epoch  16:   5% | abe: 4.946 | eve: 9.222 | bob: 5.041Epoch  16:   6% | abe: 4.946 | eve: 9.225 | bob: 5.041Epoch  16:   7% | abe: 4.949 | eve: 9.223 | bob: 5.044Epoch  16:   7% | abe: 4.957 | eve: 9.219 | bob: 5.049Epoch  16:   8% | abe: 4.954 | eve: 9.217 | bob: 5.045Epoch  16:   9% | abe: 4.957 | eve: 9.215 | bob: 5.046Epoch  16:  10% | abe: 4.956 | eve: 9.215 | bob: 5.045Epoch  16:  10% | abe: 4.961 | eve: 9.219 | bob: 5.048Epoch  16:  11% | abe: 4.960 | eve: 9.224 | bob: 5.048Epoch  16:  12% | abe: 4.963 | eve: 9.223 | bob: 5.051Epoch  16:  13% | abe: 4.960 | eve: 9.223 | bob: 5.049Epoch  16:  14% | abe: 4.962 | eve: 9.224 | bob: 5.052Epoch  16:  14% | abe: 4.965 | eve: 9.221 | bob: 5.056Epoch  16:  15% | abe: 4.968 | eve: 9.218 | bob: 5.061Epoch  16:  16% | abe: 4.964 | eve: 9.218 | bob: 5.059Epoch  16:  17% | abe: 4.964 | eve: 9.215 | bob: 5.059Epoch  16:  17% | abe: 4.960 | eve: 9.217 | bob: 5.055Epoch  16:  18% | abe: 4.960 | eve: 9.217 | bob: 5.055Epoch  16:  19% | abe: 4.961 | eve: 9.219 | bob: 5.054Epoch  16:  20% | abe: 4.961 | eve: 9.218 | bob: 5.053Epoch  16:  21% | abe: 4.960 | eve: 9.215 | bob: 5.050Epoch  16:  21% | abe: 4.961 | eve: 9.217 | bob: 5.051Epoch  16:  22% | abe: 4.962 | eve: 9.217 | bob: 5.052Epoch  16:  23% | abe: 4.963 | eve: 9.218 | bob: 5.053Epoch  16:  24% | abe: 4.965 | eve: 9.217 | bob: 5.055Epoch  16:  25% | abe: 4.965 | eve: 9.219 | bob: 5.056Epoch  16:  25% | abe: 4.966 | eve: 9.219 | bob: 5.058Epoch  16:  26% | abe: 4.968 | eve: 9.218 | bob: 5.061Epoch  16:  27% | abe: 4.969 | eve: 9.218 | bob: 5.062Epoch  16:  28% | abe: 4.968 | eve: 9.218 | bob: 5.062Epoch  16:  28% | abe: 4.969 | eve: 9.218 | bob: 5.062Epoch  16:  29% | abe: 4.969 | eve: 9.216 | bob: 5.062Epoch  16:  30% | abe: 4.971 | eve: 9.217 | bob: 5.062Epoch  16:  31% | abe: 4.970 | eve: 9.215 | bob: 5.062Epoch  16:  32% | abe: 4.970 | eve: 9.216 | bob: 5.061Epoch  16:  32% | abe: 4.971 | eve: 9.215 | bob: 5.062Epoch  16:  33% | abe: 4.972 | eve: 9.214 | bob: 5.064Epoch  16:  34% | abe: 4.972 | eve: 9.215 | bob: 5.065Epoch  16:  35% | abe: 4.971 | eve: 9.213 | bob: 5.065Epoch  16:  35% | abe: 4.972 | eve: 9.212 | bob: 5.066Epoch  16:  36% | abe: 4.970 | eve: 9.212 | bob: 5.064Epoch  16:  37% | abe: 4.969 | eve: 9.212 | bob: 5.064Epoch  16:  38% | abe: 4.968 | eve: 9.210 | bob: 5.062Epoch  16:  39% | abe: 4.968 | eve: 9.211 | bob: 5.062Epoch  16:  39% | abe: 4.968 | eve: 9.212 | bob: 5.061Epoch  16:  40% | abe: 4.968 | eve: 9.213 | bob: 5.060Epoch  16:  41% | abe: 4.968 | eve: 9.213 | bob: 5.060Epoch  16:  42% | abe: 4.967 | eve: 9.213 | bob: 5.059Epoch  16:  42% | abe: 4.967 | eve: 9.213 | bob: 5.059Epoch  16:  43% | abe: 4.968 | eve: 9.212 | bob: 5.060Epoch  16:  44% | abe: 4.967 | eve: 9.213 | bob: 5.059Epoch  16:  45% | abe: 4.966 | eve: 9.213 | bob: 5.059Epoch  16:  46% | abe: 4.966 | eve: 9.212 | bob: 5.059Epoch  16:  46% | abe: 4.966 | eve: 9.213 | bob: 5.060Epoch  16:  47% | abe: 4.968 | eve: 9.212 | bob: 5.061Epoch  16:  48% | abe: 4.968 | eve: 9.212 | bob: 5.061Epoch  16:  49% | abe: 4.967 | eve: 9.211 | bob: 5.060Epoch  16:  50% | abe: 4.968 | eve: 9.211 | bob: 5.060Epoch  16:  50% | abe: 4.967 | eve: 9.210 | bob: 5.059Epoch  16:  51% | abe: 4.966 | eve: 9.208 | bob: 5.058Epoch  16:  52% | abe: 4.965 | eve: 9.209 | bob: 5.057Epoch  16:  53% | abe: 4.964 | eve: 9.209 | bob: 5.057Epoch  16:  53% | abe: 4.964 | eve: 9.209 | bob: 5.056Epoch  16:  54% | abe: 4.964 | eve: 9.209 | bob: 5.057Epoch  16:  55% | abe: 4.964 | eve: 9.209 | bob: 5.057Epoch  16:  56% | abe: 4.964 | eve: 9.210 | bob: 5.057Epoch  16:  57% | abe: 4.962 | eve: 9.210 | bob: 5.056Epoch  16:  57% | abe: 4.963 | eve: 9.210 | bob: 5.056Epoch  16:  58% | abe: 4.963 | eve: 9.211 | bob: 5.056Epoch  16:  59% | abe: 4.963 | eve: 9.210 | bob: 5.056Epoch  16:  60% | abe: 4.963 | eve: 9.211 | bob: 5.056Epoch  16:  60% | abe: 4.963 | eve: 9.211 | bob: 5.055Epoch  16:  61% | abe: 4.963 | eve: 9.212 | bob: 5.055Epoch  16:  62% | abe: 4.963 | eve: 9.212 | bob: 5.055Epoch  16:  63% | abe: 4.963 | eve: 9.212 | bob: 5.055Epoch  16:  64% | abe: 4.963 | eve: 9.213 | bob: 5.055Epoch  16:  64% | abe: 4.963 | eve: 9.212 | bob: 5.055Epoch  16:  65% | abe: 4.963 | eve: 9.212 | bob: 5.055Epoch  16:  66% | abe: 4.962 | eve: 9.212 | bob: 5.054Epoch  16:  67% | abe: 4.962 | eve: 9.212 | bob: 5.054Epoch  16:  67% | abe: 4.961 | eve: 9.211 | bob: 5.053Epoch  16:  68% | abe: 4.962 | eve: 9.211 | bob: 5.054Epoch  16:  69% | abe: 4.961 | eve: 9.211 | bob: 5.053Epoch  16:  70% | abe: 4.961 | eve: 9.212 | bob: 5.052Epoch  16:  71% | abe: 4.961 | eve: 9.212 | bob: 5.052Epoch  16:  71% | abe: 4.961 | eve: 9.212 | bob: 5.052Epoch  16:  72% | abe: 4.960 | eve: 9.213 | bob: 5.051Epoch  16:  73% | abe: 4.961 | eve: 9.213 | bob: 5.051Epoch  16:  74% | abe: 4.961 | eve: 9.213 | bob: 5.052Epoch  16:  75% | abe: 4.961 | eve: 9.214 | bob: 5.052Epoch  16:  75% | abe: 4.961 | eve: 9.214 | bob: 5.053Epoch  16:  76% | abe: 4.961 | eve: 9.215 | bob: 5.053Epoch  16:  77% | abe: 4.960 | eve: 9.215 | bob: 5.053Epoch  16:  78% | abe: 4.960 | eve: 9.216 | bob: 5.053Epoch  16:  78% | abe: 4.961 | eve: 9.216 | bob: 5.053Epoch  16:  79% | abe: 4.961 | eve: 9.216 | bob: 5.053Epoch  16:  80% | abe: 4.961 | eve: 9.215 | bob: 5.053Epoch  16:  81% | abe: 4.961 | eve: 9.216 | bob: 5.053Epoch  16:  82% | abe: 4.961 | eve: 9.216 | bob: 5.054Epoch  16:  82% | abe: 4.961 | eve: 9.216 | bob: 5.054Epoch  16:  83% | abe: 4.962 | eve: 9.215 | bob: 5.054Epoch  16:  84% | abe: 4.962 | eve: 9.215 | bob: 5.054Epoch  16:  85% | abe: 4.962 | eve: 9.215 | bob: 5.055Epoch  16:  85% | abe: 4.962 | eve: 9.215 | bob: 5.055Epoch  16:  86% | abe: 4.961 | eve: 9.215 | bob: 5.054Epoch  16:  87% | abe: 4.961 | eve: 9.215 | bob: 5.055Epoch  16:  88% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  89% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  89% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  90% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  91% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  92% | abe: 4.962 | eve: 9.216 | bob: 5.055Epoch  16:  92% | abe: 4.961 | eve: 9.217 | bob: 5.055Epoch  16:  93% | abe: 4.961 | eve: 9.217 | bob: 5.055Epoch  16:  94% | abe: 4.961 | eve: 9.217 | bob: 5.055Epoch  16:  95% | abe: 4.961 | eve: 9.217 | bob: 5.055Epoch  16:  96% | abe: 4.961 | eve: 9.216 | bob: 5.055Epoch  16:  96% | abe: 4.961 | eve: 9.216 | bob: 5.055Epoch  16:  97% | abe: 4.960 | eve: 9.217 | bob: 5.055Epoch  16:  98% | abe: 4.961 | eve: 9.217 | bob: 5.056Epoch  16:  99% | abe: 4.961 | eve: 9.218 | bob: 5.056Epoch  17:   0% | abe: 4.902 | eve: 9.193 | bob: 5.005Epoch  17:   0% | abe: 4.933 | eve: 9.208 | bob: 5.028Epoch  17:   1% | abe: 4.951 | eve: 9.195 | bob: 5.042Epoch  17:   2% | abe: 4.960 | eve: 9.212 | bob: 5.047Epoch  17:   3% | abe: 4.952 | eve: 9.220 | bob: 5.036Epoch  17:   3% | abe: 4.952 | eve: 9.215 | bob: 5.033Epoch  17:   4% | abe: 4.940 | eve: 9.213 | bob: 5.019Epoch  17:   5% | abe: 4.950 | eve: 9.217 | bob: 5.030Epoch  17:   6% | abe: 4.948 | eve: 9.222 | bob: 5.031Epoch  17:   7% | abe: 4.945 | eve: 9.223 | bob: 5.031Epoch  17:   7% | abe: 4.943 | eve: 9.221 | bob: 5.031Epoch  17:   8% | abe: 4.942 | eve: 9.217 | bob: 5.031Epoch  17:   9% | abe: 4.942 | eve: 9.225 | bob: 5.032Epoch  17:  10% | abe: 4.942 | eve: 9.221 | bob: 5.033Epoch  17:  10% | abe: 4.938 | eve: 9.224 | bob: 5.029Epoch  17:  11% | abe: 4.941 | eve: 9.221 | bob: 5.032Epoch  17:  12% | abe: 4.944 | eve: 9.223 | bob: 5.034Epoch  17:  13% | abe: 4.947 | eve: 9.221 | bob: 5.036Epoch  17:  14% | abe: 4.951 | eve: 9.226 | bob: 5.039Epoch  17:  14% | abe: 4.953 | eve: 9.224 | bob: 5.039Epoch  17:  15% | abe: 4.950 | eve: 9.223 | bob: 5.036Epoch  17:  16% | abe: 4.949 | eve: 9.223 | bob: 5.036Epoch  17:  17% | abe: 4.948 | eve: 9.223 | bob: 5.035Epoch  17:  17% | abe: 4.950 | eve: 9.223 | bob: 5.038Epoch  17:  18% | abe: 4.950 | eve: 9.221 | bob: 5.038Epoch  17:  19% | abe: 4.948 | eve: 9.221 | bob: 5.038Epoch  17:  20% | abe: 4.951 | eve: 9.222 | bob: 5.042Epoch  17:  21% | abe: 4.951 | eve: 9.223 | bob: 5.042Epoch  17:  21% | abe: 4.949 | eve: 9.222 | bob: 5.041Epoch  17:  22% | abe: 4.949 | eve: 9.221 | bob: 5.041Epoch  17:  23% | abe: 4.948 | eve: 9.224 | bob: 5.040Epoch  17:  24% | abe: 4.950 | eve: 9.225 | bob: 5.042Epoch  17:  25% | abe: 4.950 | eve: 9.227 | bob: 5.042Epoch  17:  25% | abe: 4.950 | eve: 9.228 | bob: 5.043Epoch  17:  26% | abe: 4.951 | eve: 9.227 | bob: 5.045Epoch  17:  27% | abe: 4.952 | eve: 9.228 | bob: 5.047Epoch  17:  28% | abe: 4.954 | eve: 9.228 | bob: 5.049Epoch  17:  28% | abe: 4.953 | eve: 9.228 | bob: 5.049Epoch  17:  29% | abe: 4.953 | eve: 9.226 | bob: 5.049Epoch  17:  30% | abe: 4.951 | eve: 9.225 | bob: 5.047Epoch  17:  31% | abe: 4.951 | eve: 9.225 | bob: 5.047Epoch  17:  32% | abe: 4.951 | eve: 9.226 | bob: 5.047Epoch  17:  32% | abe: 4.951 | eve: 9.225 | bob: 5.047Epoch  17:  33% | abe: 4.950 | eve: 9.226 | bob: 5.047Epoch  17:  34% | abe: 4.951 | eve: 9.226 | bob: 5.048Epoch  17:  35% | abe: 4.951 | eve: 9.227 | bob: 5.049Epoch  17:  35% | abe: 4.951 | eve: 9.227 | bob: 5.049Epoch  17:  36% | abe: 4.949 | eve: 9.226 | bob: 5.047Epoch  17:  37% | abe: 4.950 | eve: 9.225 | bob: 5.048Epoch  17:  38% | abe: 4.948 | eve: 9.225 | bob: 5.046Epoch  17:  39% | abe: 4.947 | eve: 9.226 | bob: 5.045Epoch  17:  39% | abe: 4.947 | eve: 9.224 | bob: 5.044Epoch  17:  40% | abe: 4.948 | eve: 9.225 | bob: 5.045Epoch  17:  41% | abe: 4.949 | eve: 9.226 | bob: 5.045Epoch  17:  42% | abe: 4.949 | eve: 9.226 | bob: 5.046Epoch  17:  42% | abe: 4.950 | eve: 9.227 | bob: 5.047Epoch  17:  43% | abe: 4.950 | eve: 9.227 | bob: 5.047Epoch  17:  44% | abe: 4.950 | eve: 9.226 | bob: 5.047Epoch  17:  45% | abe: 4.949 | eve: 9.225 | bob: 5.047Epoch  17:  46% | abe: 4.949 | eve: 9.226 | bob: 5.047Epoch  17:  46% | abe: 4.949 | eve: 9.226 | bob: 5.047Epoch  17:  47% | abe: 4.949 | eve: 9.226 | bob: 5.048Epoch  17:  48% | abe: 4.949 | eve: 9.226 | bob: 5.047Epoch  17:  49% | abe: 4.949 | eve: 9.227 | bob: 5.048Epoch  17:  50% | abe: 4.949 | eve: 9.227 | bob: 5.047Epoch  17:  50% | abe: 4.949 | eve: 9.226 | bob: 5.048Epoch  17:  51% | abe: 4.950 | eve: 9.226 | bob: 5.048Epoch  17:  52% | abe: 4.950 | eve: 9.225 | bob: 5.048Epoch  17:  53% | abe: 4.951 | eve: 9.225 | bob: 5.048Epoch  17:  53% | abe: 4.950 | eve: 9.224 | bob: 5.047Epoch  17:  54% | abe: 4.950 | eve: 9.224 | bob: 5.048Epoch  17:  55% | abe: 4.950 | eve: 9.224 | bob: 5.048Epoch  17:  56% | abe: 4.951 | eve: 9.223 | bob: 5.049Epoch  17:  57% | abe: 4.950 | eve: 9.223 | bob: 5.048Epoch  17:  57% | abe: 4.950 | eve: 9.224 | bob: 5.049Epoch  17:  58% | abe: 4.950 | eve: 9.224 | bob: 5.049Epoch  17:  59% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  60% | abe: 4.950 | eve: 9.224 | bob: 5.049Epoch  17:  60% | abe: 4.950 | eve: 9.223 | bob: 5.050Epoch  17:  61% | abe: 4.950 | eve: 9.224 | bob: 5.050Epoch  17:  62% | abe: 4.949 | eve: 9.224 | bob: 5.048Epoch  17:  63% | abe: 4.949 | eve: 9.225 | bob: 5.049Epoch  17:  64% | abe: 4.950 | eve: 9.225 | bob: 5.049Epoch  17:  64% | abe: 4.949 | eve: 9.225 | bob: 5.048Epoch  17:  65% | abe: 4.950 | eve: 9.225 | bob: 5.049Epoch  17:  66% | abe: 4.950 | eve: 9.224 | bob: 5.049Epoch  17:  67% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  67% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  68% | abe: 4.951 | eve: 9.223 | bob: 5.049Epoch  17:  69% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  70% | abe: 4.950 | eve: 9.223 | bob: 5.048Epoch  17:  71% | abe: 4.950 | eve: 9.224 | bob: 5.048Epoch  17:  71% | abe: 4.950 | eve: 9.223 | bob: 5.048Epoch  17:  72% | abe: 4.949 | eve: 9.223 | bob: 5.048Epoch  17:  73% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  74% | abe: 4.950 | eve: 9.222 | bob: 5.049Epoch  17:  75% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  75% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  76% | abe: 4.950 | eve: 9.223 | bob: 5.050Epoch  17:  77% | abe: 4.950 | eve: 9.223 | bob: 5.049Epoch  17:  78% | abe: 4.950 | eve: 9.222 | bob: 5.050Epoch  17:  78% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  79% | abe: 4.951 | eve: 9.222 | bob: 5.049Epoch  17:  80% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  81% | abe: 4.951 | eve: 9.222 | bob: 5.049Epoch  17:  82% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  82% | abe: 4.951 | eve: 9.223 | bob: 5.049Epoch  17:  83% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  84% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  85% | abe: 4.951 | eve: 9.221 | bob: 5.050Epoch  17:  85% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  86% | abe: 4.951 | eve: 9.222 | bob: 5.050Epoch  17:  87% | abe: 4.952 | eve: 9.222 | bob: 5.051Epoch  17:  88% | abe: 4.952 | eve: 9.222 | bob: 5.052Epoch  17:  89% | abe: 4.952 | eve: 9.222 | bob: 5.052Epoch  17:  89% | abe: 4.952 | eve: 9.223 | bob: 5.052Epoch  17:  90% | abe: 4.952 | eve: 9.224 | bob: 5.052Epoch  17:  91% | abe: 4.952 | eve: 9.224 | bob: 5.052Epoch  17:  92% | abe: 4.951 | eve: 9.224 | bob: 5.051Epoch  17:  92% | abe: 4.951 | eve: 9.225 | bob: 5.051Epoch  17:  93% | abe: 4.952 | eve: 9.224 | bob: 5.052Epoch  17:  94% | abe: 4.952 | eve: 9.225 | bob: 5.052Epoch  17:  95% | abe: 4.951 | eve: 9.225 | bob: 5.052Epoch  17:  96% | abe: 4.951 | eve: 9.225 | bob: 5.051Epoch  17:  96% | abe: 4.951 | eve: 9.225 | bob: 5.051Epoch  17:  97% | abe: 4.950 | eve: 9.225 | bob: 5.051Epoch  17:  98% | abe: 4.950 | eve: 9.224 | bob: 5.051Epoch  17:  99% | abe: 4.949 | eve: 9.224 | bob: 5.050Epoch  18:   0% | abe: 4.881 | eve: 9.265 | bob: 5.024Epoch  18:   0% | abe: 4.862 | eve: 9.192 | bob: 5.002Epoch  18:   1% | abe: 4.896 | eve: 9.209 | bob: 5.026Epoch  18:   2% | abe: 4.921 | eve: 9.224 | bob: 5.046Epoch  18:   3% | abe: 4.927 | eve: 9.216 | bob: 5.047Epoch  18:   3% | abe: 4.925 | eve: 9.223 | bob: 5.042Epoch  18:   4% | abe: 4.925 | eve: 9.219 | bob: 5.037Epoch  18:   5% | abe: 4.929 | eve: 9.231 | bob: 5.040Epoch  18:   6% | abe: 4.922 | eve: 9.226 | bob: 5.033Epoch  18:   7% | abe: 4.924 | eve: 9.219 | bob: 5.034Epoch  18:   7% | abe: 4.925 | eve: 9.216 | bob: 5.033Epoch  18:   8% | abe: 4.930 | eve: 9.220 | bob: 5.040Epoch  18:   9% | abe: 4.935 | eve: 9.221 | bob: 5.044Epoch  18:  10% | abe: 4.937 | eve: 9.223 | bob: 5.045Epoch  18:  10% | abe: 4.934 | eve: 9.224 | bob: 5.041Epoch  18:  11% | abe: 4.932 | eve: 9.227 | bob: 5.039Epoch  18:  12% | abe: 4.936 | eve: 9.226 | bob: 5.042Epoch  18:  13% | abe: 4.938 | eve: 9.226 | bob: 5.045Epoch  18:  14% | abe: 4.940 | eve: 9.223 | bob: 5.046Epoch  18:  14% | abe: 4.946 | eve: 9.222 | bob: 5.052Epoch  18:  15% | abe: 4.944 | eve: 9.221 | bob: 5.049Epoch  18:  16% | abe: 4.943 | eve: 9.225 | bob: 5.048Epoch  18:  17% | abe: 4.943 | eve: 9.224 | bob: 5.048Epoch  18:  17% | abe: 4.943 | eve: 9.219 | bob: 5.047Epoch  18:  18% | abe: 4.943 | eve: 9.219 | bob: 5.048Epoch  18:  19% | abe: 4.942 | eve: 9.219 | bob: 5.046Epoch  18:  20% | abe: 4.941 | eve: 9.217 | bob: 5.046Epoch  18:  21% | abe: 4.943 | eve: 9.218 | bob: 5.048Epoch  18:  21% | abe: 4.945 | eve: 9.217 | bob: 5.050Epoch  18:  22% | abe: 4.946 | eve: 9.217 | bob: 5.051Epoch  18:  23% | abe: 4.947 | eve: 9.215 | bob: 5.051Epoch  18:  24% | abe: 4.947 | eve: 9.217 | bob: 5.050Epoch  18:  25% | abe: 4.948 | eve: 9.215 | bob: 5.050Epoch  18:  25% | abe: 4.947 | eve: 9.212 | bob: 5.049Epoch  18:  26% | abe: 4.947 | eve: 9.213 | bob: 5.049Epoch  18:  27% | abe: 4.947 | eve: 9.214 | bob: 5.049Epoch  18:  28% | abe: 4.949 | eve: 9.213 | bob: 5.050Epoch  18:  28% | abe: 4.948 | eve: 9.213 | bob: 5.050Epoch  18:  29% | abe: 4.948 | eve: 9.213 | bob: 5.050Epoch  18:  30% | abe: 4.951 | eve: 9.212 | bob: 5.053Epoch  18:  31% | abe: 4.952 | eve: 9.211 | bob: 5.055Epoch  18:  32% | abe: 4.951 | eve: 9.210 | bob: 5.054Epoch  18:  32% | abe: 4.950 | eve: 9.211 | bob: 5.052Epoch  18:  33% | abe: 4.949 | eve: 9.210 | bob: 5.052Epoch  18:  34% | abe: 4.949 | eve: 9.210 | bob: 5.052Epoch  18:  35% | abe: 4.949 | eve: 9.210 | bob: 5.051Epoch  18:  35% | abe: 4.949 | eve: 9.211 | bob: 5.051Epoch  18:  36% | abe: 4.950 | eve: 9.210 | bob: 5.052Epoch  18:  37% | abe: 4.950 | eve: 9.211 | bob: 5.053Epoch  18:  38% | abe: 4.951 | eve: 9.212 | bob: 5.054Epoch  18:  39% | abe: 4.952 | eve: 9.212 | bob: 5.055Epoch  18:  39% | abe: 4.952 | eve: 9.212 | bob: 5.055Epoch  18:  40% | abe: 4.952 | eve: 9.212 | bob: 5.055Epoch  18:  41% | abe: 4.952 | eve: 9.211 | bob: 5.056Epoch  18:  42% | abe: 4.952 | eve: 9.212 | bob: 5.056Epoch  18:  42% | abe: 4.951 | eve: 9.211 | bob: 5.055Epoch  18:  43% | abe: 4.952 | eve: 9.212 | bob: 5.056Epoch  18:  44% | abe: 4.950 | eve: 9.213 | bob: 5.054Epoch  18:  45% | abe: 4.952 | eve: 9.214 | bob: 5.055Epoch  18:  46% | abe: 4.952 | eve: 9.215 | bob: 5.055Epoch  18:  46% | abe: 4.953 | eve: 9.215 | bob: 5.056Epoch  18:  47% | abe: 4.953 | eve: 9.215 | bob: 5.055Epoch  18:  48% | abe: 4.952 | eve: 9.215 | bob: 5.054Epoch  18:  49% | abe: 4.951 | eve: 9.215 | bob: 5.053Epoch  18:  50% | abe: 4.952 | eve: 9.215 | bob: 5.054Epoch  18:  50% | abe: 4.952 | eve: 9.215 | bob: 5.054Epoch  18:  51% | abe: 4.952 | eve: 9.216 | bob: 5.054Epoch  18:  52% | abe: 4.953 | eve: 9.218 | bob: 5.055Epoch  18:  53% | abe: 4.952 | eve: 9.217 | bob: 5.054Epoch  18:  53% | abe: 4.952 | eve: 9.217 | bob: 5.054Epoch  18:  54% | abe: 4.951 | eve: 9.218 | bob: 5.053Epoch  18:  55% | abe: 4.950 | eve: 9.219 | bob: 5.051Epoch  18:  56% | abe: 4.951 | eve: 9.219 | bob: 5.052Epoch  18:  57% | abe: 4.951 | eve: 9.218 | bob: 5.052Epoch  18:  57% | abe: 4.950 | eve: 9.218 | bob: 5.051Epoch  18:  58% | abe: 4.949 | eve: 9.217 | bob: 5.050Epoch  18:  59% | abe: 4.950 | eve: 9.217 | bob: 5.051Epoch  18:  60% | abe: 4.950 | eve: 9.217 | bob: 5.052Epoch  18:  60% | abe: 4.949 | eve: 9.217 | bob: 5.051Epoch  18:  61% | abe: 4.950 | eve: 9.218 | bob: 5.052Epoch  18:  62% | abe: 4.949 | eve: 9.219 | bob: 5.052Epoch  18:  63% | abe: 4.950 | eve: 9.219 | bob: 5.052Epoch  18:  64% | abe: 4.950 | eve: 9.218 | bob: 5.053Epoch  18:  64% | abe: 4.949 | eve: 9.219 | bob: 5.052Epoch  18:  65% | abe: 4.949 | eve: 9.220 | bob: 5.052Epoch  18:  66% | abe: 4.950 | eve: 9.220 | bob: 5.052Epoch  18:  67% | abe: 4.949 | eve: 9.219 | bob: 5.052Epoch  18:  67% | abe: 4.950 | eve: 9.219 | bob: 5.052Epoch  18:  68% | abe: 4.950 | eve: 9.220 | bob: 5.052Epoch  18:  69% | abe: 4.951 | eve: 9.220 | bob: 5.053Epoch  18:  70% | abe: 4.949 | eve: 9.221 | bob: 5.052Epoch  18:  71% | abe: 4.950 | eve: 9.221 | bob: 5.053Epoch  18:  71% | abe: 4.950 | eve: 9.221 | bob: 5.053Epoch  18:  72% | abe: 4.950 | eve: 9.221 | bob: 5.054Epoch  18:  73% | abe: 4.950 | eve: 9.221 | bob: 5.054Epoch  18:  74% | abe: 4.949 | eve: 9.221 | bob: 5.054Epoch  18:  75% | abe: 4.949 | eve: 9.221 | bob: 5.054Epoch  18:  75% | abe: 4.949 | eve: 9.220 | bob: 5.054Epoch  18:  76% | abe: 4.950 | eve: 9.220 | bob: 5.055Epoch  18:  77% | abe: 4.949 | eve: 9.221 | bob: 5.055Epoch  18:  78% | abe: 4.950 | eve: 9.221 | bob: 5.056Epoch  18:  78% | abe: 4.949 | eve: 9.221 | bob: 5.055Epoch  18:  79% | abe: 4.949 | eve: 9.221 | bob: 5.056Epoch  18:  80% | abe: 4.949 | eve: 9.221 | bob: 5.055Epoch  18:  81% | abe: 4.950 | eve: 9.221 | bob: 5.056Epoch  18:  82% | abe: 4.950 | eve: 9.221 | bob: 5.057Epoch  18:  82% | abe: 4.950 | eve: 9.222 | bob: 5.056Epoch  18:  83% | abe: 4.950 | eve: 9.221 | bob: 5.056Epoch  18:  84% | abe: 4.950 | eve: 9.221 | bob: 5.056Epoch  18:  85% | abe: 4.950 | eve: 9.222 | bob: 5.056Epoch  18:  85% | abe: 4.951 | eve: 9.223 | bob: 5.057Epoch  18:  86% | abe: 4.951 | eve: 9.223 | bob: 5.057Epoch  18:  87% | abe: 4.951 | eve: 9.223 | bob: 5.058Epoch  18:  88% | abe: 4.951 | eve: 9.223 | bob: 5.058Epoch  18:  89% | abe: 4.951 | eve: 9.222 | bob: 5.058Epoch  18:  89% | abe: 4.951 | eve: 9.222 | bob: 5.059Epoch  18:  90% | abe: 4.951 | eve: 9.222 | bob: 5.059Epoch  18:  91% | abe: 4.950 | eve: 9.222 | bob: 5.058Epoch  18:  92% | abe: 4.950 | eve: 9.222 | bob: 5.058Epoch  18:  92% | abe: 4.950 | eve: 9.222 | bob: 5.058Epoch  18:  93% | abe: 4.950 | eve: 9.223 | bob: 5.058Epoch  18:  94% | abe: 4.950 | eve: 9.223 | bob: 5.058Epoch  18:  95% | abe: 4.950 | eve: 9.223 | bob: 5.058Epoch  18:  96% | abe: 4.949 | eve: 9.223 | bob: 5.058Epoch  18:  96% | abe: 4.949 | eve: 9.223 | bob: 5.058Epoch  18:  97% | abe: 4.948 | eve: 9.223 | bob: 5.058Epoch  18:  98% | abe: 4.949 | eve: 9.224 | bob: 5.058Epoch  18:  99% | abe: 4.949 | eve: 9.224 | bob: 5.058Epoch  19:   0% | abe: 4.932 | eve: 9.200 | bob: 5.016Epoch  19:   0% | abe: 4.933 | eve: 9.233 | bob: 5.026Epoch  19:   1% | abe: 4.904 | eve: 9.238 | bob: 5.015Epoch  19:   2% | abe: 4.918 | eve: 9.246 | bob: 5.036Epoch  19:   3% | abe: 4.936 | eve: 9.240 | bob: 5.056Epoch  19:   3% | abe: 4.946 | eve: 9.250 | bob: 5.067Epoch  19:   4% | abe: 4.949 | eve: 9.240 | bob: 5.073Epoch  19:   5% | abe: 4.952 | eve: 9.247 | bob: 5.078Epoch  19:   6% | abe: 4.960 | eve: 9.242 | bob: 5.091Epoch  19:   7% | abe: 4.958 | eve: 9.244 | bob: 5.088Epoch  19:   7% | abe: 4.964 | eve: 9.238 | bob: 5.093Epoch  19:   8% | abe: 4.965 | eve: 9.242 | bob: 5.094Epoch  19:   9% | abe: 4.959 | eve: 9.242 | bob: 5.088Epoch  19:  10% | abe: 4.958 | eve: 9.238 | bob: 5.087Epoch  19:  10% | abe: 4.959 | eve: 9.235 | bob: 5.086Epoch  19:  11% | abe: 4.959 | eve: 9.235 | bob: 5.084Epoch  19:  12% | abe: 4.958 | eve: 9.234 | bob: 5.082Epoch  19:  13% | abe: 4.951 | eve: 9.236 | bob: 5.074Epoch  19:  14% | abe: 4.947 | eve: 9.237 | bob: 5.069Epoch  19:  14% | abe: 4.949 | eve: 9.232 | bob: 5.070Epoch  19:  15% | abe: 4.951 | eve: 9.230 | bob: 5.073Epoch  19:  16% | abe: 4.950 | eve: 9.235 | bob: 5.072Epoch  19:  17% | abe: 4.950 | eve: 9.234 | bob: 5.072Epoch  19:  17% | abe: 4.950 | eve: 9.234 | bob: 5.073Epoch  19:  18% | abe: 4.949 | eve: 9.237 | bob: 5.072Epoch  19:  19% | abe: 4.949 | eve: 9.235 | bob: 5.073Epoch  19:  20% | abe: 4.948 | eve: 9.234 | bob: 5.073Epoch  19:  21% | abe: 4.950 | eve: 9.233 | bob: 5.076Epoch  19:  21% | abe: 4.953 | eve: 9.234 | bob: 5.077Epoch  19:  22% | abe: 4.953 | eve: 9.235 | bob: 5.077Epoch  19:  23% | abe: 4.953 | eve: 9.237 | bob: 5.078Epoch  19:  24% | abe: 4.954 | eve: 9.236 | bob: 5.079Epoch  19:  25% | abe: 4.954 | eve: 9.236 | bob: 5.077Epoch  19:  25% | abe: 4.954 | eve: 9.237 | bob: 5.077Epoch  19:  26% | abe: 4.957 | eve: 9.237 | bob: 5.078Epoch  19:  27% | abe: 4.958 | eve: 9.238 | bob: 5.079Epoch  19:  28% | abe: 4.957 | eve: 9.237 | bob: 5.077Epoch  19:  28% | abe: 4.955 | eve: 9.238 | bob: 5.074Epoch  19:  29% | abe: 4.954 | eve: 9.239 | bob: 5.072Epoch  19:  30% | abe: 4.955 | eve: 9.237 | bob: 5.073Epoch  19:  31% | abe: 4.953 | eve: 9.235 | bob: 5.070Epoch  19:  32% | abe: 4.951 | eve: 9.235 | bob: 5.069Epoch  19:  32% | abe: 4.951 | eve: 9.236 | bob: 5.069Epoch  19:  33% | abe: 4.950 | eve: 9.234 | bob: 5.068Epoch  19:  34% | abe: 4.952 | eve: 9.234 | bob: 5.070Epoch  19:  35% | abe: 4.953 | eve: 9.233 | bob: 5.072Epoch  19:  35% | abe: 4.952 | eve: 9.235 | bob: 5.072Epoch  19:  36% | abe: 4.952 | eve: 9.232 | bob: 5.073Epoch  19:  37% | abe: 4.952 | eve: 9.231 | bob: 5.073Epoch  19:  38% | abe: 4.954 | eve: 9.232 | bob: 5.074Epoch  19:  39% | abe: 4.953 | eve: 9.232 | bob: 5.074Epoch  19:  39% | abe: 4.952 | eve: 9.232 | bob: 5.073Epoch  19:  40% | abe: 4.954 | eve: 9.233 | bob: 5.075Epoch  19:  41% | abe: 4.956 | eve: 9.235 | bob: 5.077Epoch  19:  42% | abe: 4.956 | eve: 9.233 | bob: 5.077Epoch  19:  42% | abe: 4.956 | eve: 9.234 | bob: 5.077Epoch  19:  43% | abe: 4.957 | eve: 9.234 | bob: 5.078Epoch  19:  44% | abe: 4.956 | eve: 9.233 | bob: 5.078Epoch  19:  45% | abe: 4.958 | eve: 9.232 | bob: 5.079Epoch  19:  46% | abe: 4.957 | eve: 9.232 | bob: 5.079Epoch  19:  46% | abe: 4.956 | eve: 9.231 | bob: 5.078Epoch  19:  47% | abe: 4.956 | eve: 9.230 | bob: 5.079Epoch  19:  48% | abe: 4.957 | eve: 9.229 | bob: 5.079Epoch  19:  49% | abe: 4.956 | eve: 9.228 | bob: 5.079Epoch  19:  50% | abe: 4.957 | eve: 9.228 | bob: 5.080Epoch  19:  50% | abe: 4.957 | eve: 9.227 | bob: 5.079Epoch  19:  51% | abe: 4.956 | eve: 9.227 | bob: 5.078Epoch  19:  52% | abe: 4.955 | eve: 9.228 | bob: 5.077Epoch  19:  53% | abe: 4.955 | eve: 9.229 | bob: 5.078Epoch  19:  53% | abe: 4.956 | eve: 9.230 | bob: 5.080Epoch  19:  54% | abe: 4.956 | eve: 9.230 | bob: 5.079Epoch  19:  55% | abe: 4.957 | eve: 9.230 | bob: 5.081Epoch  19:  56% | abe: 4.958 | eve: 9.229 | bob: 5.081Epoch  19:  57% | abe: 4.958 | eve: 9.231 | bob: 5.081Epoch  19:  57% | abe: 4.959 | eve: 9.231 | bob: 5.082Epoch  19:  58% | abe: 4.959 | eve: 9.232 | bob: 5.082Epoch  19:  59% | abe: 4.959 | eve: 9.232 | bob: 5.082Epoch  19:  60% | abe: 4.958 | eve: 9.232 | bob: 5.080Epoch  19:  60% | abe: 4.958 | eve: 9.232 | bob: 5.081Epoch  19:  61% | abe: 4.959 | eve: 9.232 | bob: 5.081Epoch  19:  62% | abe: 4.958 | eve: 9.232 | bob: 5.079Epoch  19:  63% | abe: 4.957 | eve: 9.233 | bob: 5.079Epoch  19:  64% | abe: 4.958 | eve: 9.233 | bob: 5.079Epoch  19:  64% | abe: 4.958 | eve: 9.233 | bob: 5.080Epoch  19:  65% | abe: 4.958 | eve: 9.234 | bob: 5.080Epoch  19:  66% | abe: 4.958 | eve: 9.234 | bob: 5.080Epoch  19:  67% | abe: 4.959 | eve: 9.234 | bob: 5.081Epoch  19:  67% | abe: 4.958 | eve: 9.234 | bob: 5.080Epoch  19:  68% | abe: 4.957 | eve: 9.235 | bob: 5.079Epoch  19:  69% | abe: 4.958 | eve: 9.235 | bob: 5.080Epoch  19:  70% | abe: 4.958 | eve: 9.235 | bob: 5.080Epoch  19:  71% | abe: 4.958 | eve: 9.235 | bob: 5.079Epoch  19:  71% | abe: 4.957 | eve: 9.235 | bob: 5.079Epoch  19:  72% | abe: 4.959 | eve: 9.234 | bob: 5.080Epoch  19:  73% | abe: 4.958 | eve: 9.235 | bob: 5.079Epoch  19:  74% | abe: 4.958 | eve: 9.236 | bob: 5.079Epoch  19:  75% | abe: 4.958 | eve: 9.236 | bob: 5.080Epoch  19:  75% | abe: 4.958 | eve: 9.235 | bob: 5.080Epoch  19:  76% | abe: 4.958 | eve: 9.236 | bob: 5.079Epoch  19:  77% | abe: 4.958 | eve: 9.235 | bob: 5.080Epoch  19:  78% | abe: 4.957 | eve: 9.235 | bob: 5.079Epoch  19:  78% | abe: 4.957 | eve: 9.236 | bob: 5.079Epoch  19:  79% | abe: 4.957 | eve: 9.235 | bob: 5.079Epoch  19:  80% | abe: 4.957 | eve: 9.235 | bob: 5.079Epoch  19:  81% | abe: 4.957 | eve: 9.236 | bob: 5.079Epoch  19:  82% | abe: 4.957 | eve: 9.236 | bob: 5.078Epoch  19:  82% | abe: 4.957 | eve: 9.236 | bob: 5.078Epoch  19:  83% | abe: 4.957 | eve: 9.236 | bob: 5.078Epoch  19:  84% | abe: 4.957 | eve: 9.237 | bob: 5.078Epoch  19:  85% | abe: 4.956 | eve: 9.236 | bob: 5.077Epoch  19:  85% | abe: 4.956 | eve: 9.236 | bob: 5.077Epoch  19:  86% | abe: 4.956 | eve: 9.237 | bob: 5.077Epoch  19:  87% | abe: 4.956 | eve: 9.236 | bob: 5.077Epoch  19:  88% | abe: 4.956 | eve: 9.236 | bob: 5.077Epoch  19:  89% | abe: 4.956 | eve: 9.236 | bob: 5.076Epoch  19:  89% | abe: 4.955 | eve: 9.235 | bob: 5.076Epoch  19:  90% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  91% | abe: 4.955 | eve: 9.235 | bob: 5.076Epoch  19:  92% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  92% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  93% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  94% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  95% | abe: 4.955 | eve: 9.235 | bob: 5.075Epoch  19:  96% | abe: 4.955 | eve: 9.235 | bob: 5.074Epoch  19:  96% | abe: 4.955 | eve: 9.234 | bob: 5.075Epoch  19:  97% | abe: 4.955 | eve: 9.234 | bob: 5.075Epoch  19:  98% | abe: 4.954 | eve: 9.234 | bob: 5.074Epoch  19:  99% | abe: 4.954 | eve: 9.235 | bob: 5.074
Early stopping: No improvement after 5 epochs since epoch 13. Best Bob loss: 5.0189711224365965
Training complete.
cipher1 + cipher2
[[0.3681     0.42673486 0.         ... 2.         1.1714689  1.9303277 ]
 [1.3418124  1.5195765  0.         ... 2.         1.3681777  2.        ]
 [1.         0.12877813 1.         ... 2.         1.7533048  1.5712272 ]
 ...
 [0.         0.7630031  0.47416762 ... 1.         1.         1.424884  ]
 [1.         1.006611   1.3835719  ... 1.6177616  1.4463459  1.6327965 ]
 [1.2812698  0.6274051  0.21563587 ... 1.7033323  1.2048141  1.8878417 ]]
HO addition:
[[0.36808586 0.4267185  0.         ... 2.0018997  1.1717628  1.9320924 ]
 [1.3424364  1.5208796  0.         ... 2.0018997  1.368853   2.0018997 ]
 [1.0019382  0.12902772 1.0019382  ... 2.0018997  1.7547266  1.572296  ]
 ...
 [0.         0.7641871  0.47414944 ... 0.9999616  0.9999616  1.4256691 ]
 [1.0019382  1.0085489  1.3854954  ... 1.6189207  1.4471726  1.6339848 ]
 [1.2831972  0.6281113  0.21562761 ... 1.7046572  1.2051727  1.8897458 ]]
cipher1 * cipher2
[[0.         0.         0.         ... 1.         0.17146888 0.9303277 ]
 [0.3418123  0.57222813 0.         ... 1.         0.3681777  1.        ]
 [0.         0.         0.         ... 1.         0.75330484 0.5712272 ]
 ...
 [0.         0.09158566 0.         ... 0.         0.         0.42488396]
 [0.         0.00661096 0.3835719  ... 0.6177617  0.44634593 0.6327966 ]
 [0.28126973 0.0952978  0.         ... 0.7033323  0.20481408 0.88784176]]
HO multiplication
[[-2.1712678e-04 -2.1885389e-04 -6.7494817e-05 ...  9.9966872e-01
   1.7150071e-01  9.3006396e-01]
 [ 3.4188136e-01  5.7219648e-01 -6.7494817e-05 ...  9.9966872e-01
   3.6824456e-01  9.9966872e-01]
 [-2.1699800e-04  1.1270475e-03 -2.1699800e-04 ...  9.9966872e-01
   7.5319099e-01  5.7122833e-01]
 ...
 [-6.7494817e-05  9.1498554e-02 -2.1953019e-04 ... -2.2308783e-04
  -2.2308783e-04  4.2494053e-01]
 [-2.1699800e-04  6.4030453e-03  3.8352454e-01 ...  6.1773771e-01
   4.4639680e-01  6.3276398e-01]
 [ 2.8122777e-01  9.5228814e-02 -8.6730113e-05 ...  7.0325434e-01
   2.0486185e-01  8.8758504e-01]]
HO model Accuracy Percentage Addition: 50.81%
HO model Accuracy Percentage Multiplication: 94.28%
Bob decrypted addition: [[0.         0.         0.         ... 0.         0.80623364 0.        ]
 [0.02282637 0.29986805 0.         ... 1.0002935  1.0001003  0.        ]
 [0.         0.         0.         ... 0.16345197 0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.89714444 0.        ]
 [0.         0.         0.         ... 1.0001047  0.         0.        ]
 [0.         0.         0.         ... 0.         1.0001004  0.        ]]
Bob decrypted bits addition: [[0 0 0 ... 0 1 0]
 [0 0 0 ... 1 1 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 1 0]
 [0 0 0 ... 1 0 0]
 [0 0 0 ... 0 1 0]]
Number of correctly decrypted bits addition: 4098
Total number of bits addition: 8192
Decryption accuracy addition: 50.0244140625%
Bob decrypted multiplication: [[0.         0.         0.         ... 0.         0.7598322  0.        ]
 [0.03767985 0.7833935  0.         ... 0.99994516 1.0004776  0.        ]
 [0.         0.         0.         ... 0.8724823  0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.94532794 0.        ]
 [0.         0.         0.         ... 0.9999966  0.         0.        ]
 [0.         0.         0.         ... 0.         0.9994683  0.        ]]
Bob decrypted bits multiplication: [[0 0 0 ... 0 1 0]
 [0 1 0 ... 1 1 0]
 [0 0 0 ... 1 0 0]
 ...
 [0 0 0 ... 0 1 0]
 [0 0 0 ... 1 0 0]
 [0 0 0 ... 0 1 0]]
Number of correctly decrypted bits multiplication: 4734
Total number of bits multiplication: 8192
Decryption accuracy multiplication: 57.7880859375%
Eve decrypted addition: [[0.9406085  0.9827363  0.9592657  ... 0.9624766  0.9714464  1.0871857 ]
 [0.94065166 0.9821225  0.95893985 ... 0.9624161  0.97074026 1.0870211 ]
 [0.9405111  0.98204106 0.9586215  ... 0.96221465 0.97006273 1.0870061 ]
 ...
 [0.94066447 0.9815329  0.9585689  ... 0.9623113  0.9699959  1.0868546 ]
 [0.9406133  0.9822788  0.95896715 ... 0.96238637 0.9707583  1.0870616 ]
 [0.9406039  0.9821098  0.95883787 ... 0.9623566  0.97057724 1.0870215 ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 4029
Total number of bits addition: 8192
Decryption accuracy by Eve addition: 49.18212890625%
Eve decrypted mulitplication: [[0.94055814 0.98303384 0.95903563 ... 0.96267295 0.9714012  1.087236  ]
 [0.94075614 0.9817233  0.9587195  ... 0.96269894 0.9704216  1.0869445 ]
 [0.94033754 0.982958   0.95871586 ... 0.9625122  0.9699775  1.0872688 ]
 ...
 [0.9409628  0.9823496  0.9588175  ... 0.962865   0.9705227  1.0870285 ]
 [0.94056684 0.9826581  0.9587204  ... 0.9624778  0.9707223  1.0871167 ]
 [0.94047326 0.98254365 0.95900756 ... 0.9625313  0.97024715 1.0871539 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 2110
Total number of bits mulitplication: 8192
Decryption accuracy by Eve mulitplication: 25.7568359375%
Bob decrypted P1: [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00
  9.9970704e-01 0.0000000e+00]
 [3.0839443e-04 0.0000000e+00 0.0000000e+00 ... 9.9903762e-01
  1.0000445e+00 0.0000000e+00]
 [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 9.9957228e-01
  0.0000000e+00 0.0000000e+00]
 ...
 [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00
  0.0000000e+00 0.0000000e+00]
 [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 9.9997014e-01
  0.0000000e+00 0.0000000e+00]
 [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00
  9.9973106e-01 0.0000000e+00]]
Bob decrypted bits P1: [[0 0 0 ... 0 1 0]
 [0 0 0 ... 1 1 0]
 [0 0 0 ... 1 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 1 0 0]
 [0 0 0 ... 0 1 0]]
Number of correctly decrypted bits P1: 6868
Total number of bits P1: 8192
Decryption accuracy P1: 83.837890625%
Bob decrypted P2: [[0.         0.         0.         ... 0.         0.         0.        ]
 [0.04529476 0.99846214 0.         ... 0.99984723 1.0000402  0.        ]
 [0.         0.         0.         ... 0.         0.         0.        ]
 ...
 [0.         0.         0.         ... 0.         0.99990755 0.        ]
 [0.         0.         0.         ... 1.000137   0.         0.        ]
 [0.         0.         0.         ... 0.         0.99974006 0.        ]]
Bob decrypted bits P2: [[0 0 0 ... 0 0 0]
 [0 1 0 ... 1 1 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 1 0]
 [0 0 0 ... 1 0 0]
 [0 0 0 ... 0 1 0]]
Number of correctly decrypted bits P2: 6881
Total number of bits P2: 8192
Decryption accuracy P2: 83.99658203125%
