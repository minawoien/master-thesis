WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-08 16:06:54.136585: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-08 16:06:54.303938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-08 16:06:54.306352: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 16:06:54.311314: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-08 16:06:54.314419: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-08 16:06:54.315790: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-08 16:06:54.335898: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-08 16:06:54.339189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-08 16:06:54.348063: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-08 16:06:54.363929: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-08 16:06:54.365624: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-08 16:06:54.389936: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-08 16:06:54.393200: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5ace690 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-08 16:06:54.393505: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-08 16:06:54.854024: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4dc9da0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-08 16:06:54.854119: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-08 16:06:54.858922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-08 16:06:54.859069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 16:06:54.859099: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-08 16:06:54.859127: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-08 16:06:54.859154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-08 16:06:54.859181: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-08 16:06:54.859208: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-08 16:06:54.859236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-08 16:06:54.869790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-08 16:06:54.870556: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-08 16:06:54.880833: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-08 16:06:54.881136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-08 16:06:54.881169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-08 16:06:54.890719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-08 16:07:01.005922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 384 samples, validate on 384 samples
Epoch 1/512
384/384 - 1s - loss: 0.5756 - val_loss: 0.0030
Epoch 2/512
384/384 - 0s - loss: 0.2393 - val_loss: 0.0015
Epoch 3/512
384/384 - 0s - loss: 0.1232 - val_loss: 6.7024e-04
Epoch 4/512
384/384 - 0s - loss: 0.0435 - val_loss: 1.0361e-04
Epoch 5/512
384/384 - 0s - loss: 0.0058 - val_loss: 1.5740e-05
Epoch 6/512
384/384 - 0s - loss: 0.0013 - val_loss: 1.0288e-05
Epoch 7/512
384/384 - 0s - loss: 9.3361e-04 - val_loss: 7.5422e-06
Epoch 8/512
384/384 - 0s - loss: 6.7518e-04 - val_loss: 5.2778e-06
Epoch 9/512
384/384 - 0s - loss: 4.6475e-04 - val_loss: 3.4916e-06
Epoch 10/512
384/384 - 0s - loss: 3.0157e-04 - val_loss: 2.1599e-06
Epoch 11/512
384/384 - 0s - loss: 1.8231e-04 - val_loss: 1.2318e-06
Epoch 12/512
384/384 - 0s - loss: 1.0116e-04 - val_loss: 6.3632e-07
Epoch 13/512
384/384 - 0s - loss: 5.0589e-05 - val_loss: 2.9137e-07
Epoch 14/512
384/384 - 0s - loss: 2.2313e-05 - val_loss: 1.1631e-07
Epoch 15/512
384/384 - 0s - loss: 8.9030e-06 - val_loss: 7.8881e-08
Epoch 16/512
384/384 - 0s - loss: 4.5008e-05 - val_loss: 6.0710e-06
Epoch 17/512
384/384 - 0s - loss: 0.0029 - val_loss: 3.5161e-05
Epoch 18/512
384/384 - 0s - loss: 0.0020 - val_loss: 3.4662e-06
Epoch 19/512
384/384 - 0s - loss: 2.3379e-04 - val_loss: 1.3164e-06
Epoch 20/512
384/384 - 0s - loss: 1.4966e-04 - val_loss: 2.6105e-06
Epoch 21/512
384/384 - 0s - loss: 4.9927e-04 - val_loss: 1.4551e-05
Epoch 22/512
384/384 - 0s - loss: 0.0021 - val_loss: 2.3469e-05
Epoch 23/512
384/384 - 0s - loss: 0.0017 - val_loss: 7.7448e-06
Epoch 24/512
384/384 - 0s - loss: 6.0941e-04 - val_loss: 4.6212e-06
Epoch 25/512
384/384 - 0s - loss: 5.2093e-04 - val_loss: 7.8235e-06
Epoch 26/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.6448e-05
Epoch 27/512
384/384 - 0s - loss: 0.0017 - val_loss: 1.4180e-05
Epoch 28/512
384/384 - 0s - loss: 0.0011 - val_loss: 7.5225e-06
Epoch 29/512
384/384 - 0s - loss: 7.0969e-04 - val_loss: 6.9486e-06
Epoch 30/512
384/384 - 0s - loss: 7.7800e-04 - val_loss: 1.0229e-05
Epoch 31/512
384/384 - 0s - loss: 0.0012 - val_loss: 1.3442e-05
Epoch 32/512
384/384 - 0s - loss: 0.0012 - val_loss: 9.8091e-06
Epoch 33/512
384/384 - 0s - loss: 8.8032e-04 - val_loss: 7.6125e-06
Epoch 34/512
384/384 - 0s - loss: 7.5834e-04 - val_loss: 8.1164e-06
Epoch 35/512
384/384 - 0s - loss: 8.9627e-04 - val_loss: 1.0362e-05
Epoch 36/512
384/384 - 0s - loss: 0.0010 - val_loss: 1.0008e-05
Epoch 37/512
384/384 - 0s - loss: 9.4986e-04 - val_loss: 8.3232e-06
Epoch 38/512
384/384 - 0s - loss: 7.9344e-04 - val_loss: 7.4964e-06
Epoch 39/512
384/384 - 0s - loss: 7.7227e-04 - val_loss: 8.4343e-06
Epoch 40/512
384/384 - 0s - loss: 8.8074e-04 - val_loss: 8.8689e-06
Epoch 41/512
384/384 - 0s - loss: 8.7342e-04 - val_loss: 8.3359e-06
Epoch 42/512
384/384 - 0s - loss: 8.0043e-04 - val_loss: 7.4239e-06
Epoch 43/512
384/384 - 0s - loss: 7.3576e-04 - val_loss: 7.4952e-06
Epoch 44/512
384/384 - 0s - loss: 7.5948e-04 - val_loss: 7.8350e-06
Epoch 45/512
384/384 - 0s - loss: 7.8711e-04 - val_loss: 7.7951e-06
Epoch 46/512
384/384 - 0s - loss: 7.6057e-04 - val_loss: 7.1524e-06
Epoch 47/512
384/384 - 0s - loss: 7.0000e-04 - val_loss: 6.8636e-06
Epoch 48/512
384/384 - 0s - loss: 6.8742e-04 - val_loss: 6.9568e-06
Epoch 49/512
384/384 - 0s - loss: 7.0311e-04 - val_loss: 7.0279e-06
Epoch 50/512
384/384 - 0s - loss: 6.9481e-04 - val_loss: 6.7607e-06
Epoch 51/512
384/384 - 0s - loss: 6.6362e-04 - val_loss: 6.4270e-06
Epoch 52/512
384/384 - 0s - loss: 6.4028e-04 - val_loss: 6.2786e-06
Epoch 53/512
384/384 - 0s - loss: 6.2882e-04 - val_loss: 6.2318e-06
Epoch 54/512
384/384 - 0s - loss: 6.2174e-04 - val_loss: 6.2516e-06
Epoch 55/512
384/384 - 0s - loss: 6.2333e-04 - val_loss: 5.9985e-06
Epoch 56/512
384/384 - 0s - loss: 5.9148e-04 - val_loss: 5.7858e-06
Epoch 57/512
384/384 - 0s - loss: 5.7073e-04 - val_loss: 5.6756e-06
Epoch 58/512
384/384 - 0s - loss: 5.6902e-04 - val_loss: 5.7226e-06
Epoch 59/512
384/384 - 0s - loss: 5.6879e-04 - val_loss: 5.4895e-06
Epoch 60/512
384/384 - 0s - loss: 5.4233e-04 - val_loss: 5.3316e-06
Epoch 61/512
384/384 - 0s - loss: 5.2236e-04 - val_loss: 5.1963e-06
Epoch 62/512
384/384 - 0s - loss: 5.2110e-04 - val_loss: 5.2429e-06
Epoch 63/512
384/384 - 0s - loss: 5.1465e-04 - val_loss: 5.0222e-06
Epoch 64/512
384/384 - 0s - loss: 4.9962e-04 - val_loss: 4.9013e-06
Epoch 65/512
384/384 - 0s - loss: 4.7841e-04 - val_loss: 4.7096e-06
Epoch 66/512
384/384 - 0s - loss: 4.7257e-04 - val_loss: 4.7526e-06
Epoch 67/512
384/384 - 0s - loss: 4.6988e-04 - val_loss: 4.5572e-06
Epoch 68/512
384/384 - 0s - loss: 4.5208e-04 - val_loss: 4.5098e-06
Epoch 69/512
384/384 - 0s - loss: 4.4537e-04 - val_loss: 4.2556e-06
Epoch 70/512
384/384 - 0s - loss: 4.2287e-04 - val_loss: 4.3025e-06
Epoch 71/512
384/384 - 0s - loss: 4.2562e-04 - val_loss: 4.1513e-06
Epoch 72/512
384/384 - 0s - loss: 4.1359e-04 - val_loss: 4.1384e-06
Epoch 73/512
384/384 - 0s - loss: 4.0971e-04 - val_loss: 3.8231e-06
Epoch 74/512
384/384 - 0s - loss: 3.8057e-04 - val_loss: 3.8393e-06
Epoch 75/512
384/384 - 0s - loss: 3.8207e-04 - val_loss: 3.7444e-06
Epoch 76/512
384/384 - 0s - loss: 3.7819e-04 - val_loss: 3.7966e-06
Epoch 77/512
384/384 - 0s - loss: 3.6978e-04 - val_loss: 3.5385e-06
Epoch 78/512
384/384 - 0s - loss: 3.5420e-04 - val_loss: 3.5075e-06
Epoch 79/512
384/384 - 0s - loss: 3.4501e-04 - val_loss: 3.3228e-06
Epoch 80/512
384/384 - 0s - loss: 3.3399e-04 - val_loss: 3.4185e-06
Epoch 81/512
384/384 - 0s - loss: 3.3735e-04 - val_loss: 3.2303e-06
Epoch 82/512
384/384 - 0s - loss: 3.2425e-04 - val_loss: 3.2008e-06
Epoch 83/512
384/384 - 0s - loss: 3.1212e-04 - val_loss: 2.9872e-06
Epoch 84/512
384/384 - 0s - loss: 3.0177e-04 - val_loss: 3.0856e-06
Epoch 85/512
384/384 - 0s - loss: 3.0483e-04 - val_loss: 2.9076e-06
Epoch 86/512
384/384 - 0s - loss: 2.9171e-04 - val_loss: 2.9109e-06
Epoch 87/512
384/384 - 0s - loss: 2.8323e-04 - val_loss: 2.7174e-06
Epoch 88/512
384/384 - 0s - loss: 2.7497e-04 - val_loss: 2.7756e-06
Epoch 89/512
384/384 - 0s - loss: 2.7343e-04 - val_loss: 2.5899e-06
Epoch 90/512
384/384 - 0s - loss: 2.5890e-04 - val_loss: 2.6478e-06
Epoch 91/512
384/384 - 0s - loss: 2.5930e-04 - val_loss: 2.4827e-06
Epoch 92/512
384/384 - 0s - loss: 2.4911e-04 - val_loss: 2.5255e-06
Epoch 93/512
384/384 - 0s - loss: 2.4627e-04 - val_loss: 2.3254e-06
Epoch 94/512
384/384 - 0s - loss: 2.3173e-04 - val_loss: 2.3878e-06
Epoch 95/512
384/384 - 0s - loss: 2.3365e-04 - val_loss: 2.2391e-06
Epoch 96/512
384/384 - 0s - loss: 2.2436e-04 - val_loss: 2.2882e-06
Epoch 97/512
384/384 - 0s - loss: 2.2281e-04 - val_loss: 2.0842e-06
Epoch 98/512
384/384 - 0s - loss: 2.0692e-04 - val_loss: 2.1446e-06
Epoch 99/512
384/384 - 0s - loss: 2.0942e-04 - val_loss: 2.0228e-06
Epoch 100/512
384/384 - 0s - loss: 2.0325e-04 - val_loss: 2.0653e-06
Epoch 101/512
384/384 - 0s - loss: 2.0022e-04 - val_loss: 1.8604e-06
Epoch 102/512
384/384 - 0s - loss: 1.8769e-04 - val_loss: 1.8869e-06
Epoch 103/512
384/384 - 0s - loss: 1.8346e-04 - val_loss: 1.7891e-06
Epoch 104/512
384/384 - 0s - loss: 1.8226e-04 - val_loss: 1.8753e-06
Epoch 105/512
384/384 - 0s - loss: 1.8318e-04 - val_loss: 1.6701e-06
Epoch 106/512
384/384 - 0s - loss: 1.6623e-04 - val_loss: 1.6933e-06
Epoch 107/512
384/384 - 0s - loss: 1.6621e-04 - val_loss: 1.5835e-06
Epoch 108/512
384/384 - 0s - loss: 1.6018e-04 - val_loss: 1.6775e-06
Epoch 109/512
384/384 - 0s - loss: 1.6411e-04 - val_loss: 1.5139e-06
Epoch 110/512
384/384 - 0s - loss: 1.5093e-04 - val_loss: 1.5241e-06
Epoch 111/512
384/384 - 0s - loss: 1.4942e-04 - val_loss: 1.3922e-06
Epoch 112/512
384/384 - 0s - loss: 1.4199e-04 - val_loss: 1.4568e-06
Epoch 113/512
384/384 - 0s - loss: 1.4277e-04 - val_loss: 1.3609e-06
Epoch 114/512
384/384 - 0s - loss: 1.3745e-04 - val_loss: 1.4093e-06
Epoch 115/512
384/384 - 0s - loss: 1.3637e-04 - val_loss: 1.2543e-06
Epoch 116/512
384/384 - 0s - loss: 1.2607e-04 - val_loss: 1.2847e-06
Epoch 117/512
384/384 - 0s - loss: 1.2492e-04 - val_loss: 1.2040e-06
Epoch 118/512
384/384 - 0s - loss: 1.2294e-04 - val_loss: 1.2638e-06
Epoch 119/512
384/384 - 0s - loss: 1.2170e-04 - val_loss: 1.1296e-06
Epoch 120/512
384/384 - 0s - loss: 1.1320e-04 - val_loss: 1.1579e-06
Epoch 121/512
384/384 - 0s - loss: 1.1221e-04 - val_loss: 1.0618e-06
Epoch 122/512
384/384 - 0s - loss: 1.0753e-04 - val_loss: 1.1194e-06
Epoch 123/512
384/384 - 0s - loss: 1.0853e-04 - val_loss: 1.0112e-06
Epoch 124/512
384/384 - 0s - loss: 1.0201e-04 - val_loss: 1.0313e-06
Epoch 125/512
384/384 - 0s - loss: 9.9357e-05 - val_loss: 9.3562e-07
Epoch 126/512
384/384 - 0s - loss: 9.5834e-05 - val_loss: 9.8184e-07
Epoch 127/512
384/384 - 0s - loss: 9.5558e-05 - val_loss: 8.9156e-07
Epoch 128/512
384/384 - 0s - loss: 9.0188e-05 - val_loss: 9.2643e-07
Epoch 129/512
384/384 - 0s - loss: 8.9676e-05 - val_loss: 8.4073e-07
Epoch 130/512
384/384 - 0s - loss: 8.5572e-05 - val_loss: 8.7120e-07
Epoch 131/512
384/384 - 0s - loss: 8.3852e-05 - val_loss: 7.8590e-07
Epoch 132/512
384/384 - 0s - loss: 8.0033e-05 - val_loss: 8.2368e-07
Epoch 133/512
384/384 - 0s - loss: 7.9554e-05 - val_loss: 7.4558e-07
Epoch 134/512
384/384 - 0s - loss: 7.6109e-05 - val_loss: 7.7071e-07
Epoch 135/512
384/384 - 0s - loss: 7.4254e-05 - val_loss: 6.9069e-07
Epoch 136/512
384/384 - 0s - loss: 7.0268e-05 - val_loss: 7.2834e-07
Epoch 137/512
384/384 - 0s - loss: 7.0979e-05 - val_loss: 6.6033e-07
Epoch 138/512
384/384 - 0s - loss: 6.6914e-05 - val_loss: 6.8931e-07
Epoch 139/512
384/384 - 0s - loss: 6.6718e-05 - val_loss: 6.1244e-07
Epoch 140/512
384/384 - 0s - loss: 6.2485e-05 - val_loss: 6.3437e-07
Epoch 141/512
384/384 - 0s - loss: 6.1187e-05 - val_loss: 5.8082e-07
Epoch 142/512
384/384 - 0s - loss: 5.9533e-05 - val_loss: 6.1741e-07
Epoch 143/512
384/384 - 0s - loss: 5.9366e-05 - val_loss: 5.4774e-07
Epoch 144/512
384/384 - 0s - loss: 5.5457e-05 - val_loss: 5.6584e-07
Epoch 145/512
384/384 - 0s - loss: 5.4647e-05 - val_loss: 5.0462e-07
Epoch 146/512
384/384 - 0s - loss: 5.1546e-05 - val_loss: 5.3650e-07
Epoch 147/512
384/384 - 0s - loss: 5.1797e-05 - val_loss: 4.8819e-07
Epoch 148/512
384/384 - 0s - loss: 4.9773e-05 - val_loss: 5.0990e-07
Epoch 149/512
384/384 - 0s - loss: 4.9039e-05 - val_loss: 4.4354e-07
Epoch 150/512
384/384 - 0s - loss: 4.4881e-05 - val_loss: 4.6572e-07
Epoch 151/512
384/384 - 0s - loss: 4.5028e-05 - val_loss: 4.3066e-07
Epoch 152/512
384/384 - 0s - loss: 4.4218e-05 - val_loss: 4.5475e-07
Epoch 153/512
384/384 - 0s - loss: 4.3013e-05 - val_loss: 3.9775e-07
Epoch 154/512
384/384 - 0s - loss: 4.0316e-05 - val_loss: 4.1434e-07
Epoch 155/512
384/384 - 0s - loss: 4.0068e-05 - val_loss: 3.6899e-07
Epoch 156/512
384/384 - 0s - loss: 3.7693e-05 - val_loss: 3.9262e-07
Epoch 157/512
384/384 - 0s - loss: 3.7829e-05 - val_loss: 3.5307e-07
Epoch 158/512
384/384 - 0s - loss: 3.6125e-05 - val_loss: 3.6815e-07
Epoch 159/512
384/384 - 0s - loss: 3.5186e-05 - val_loss: 3.2319e-07
Epoch 160/512
384/384 - 0s - loss: 3.2934e-05 - val_loss: 3.4427e-07
Epoch 161/512
384/384 - 0s - loss: 3.3142e-05 - val_loss: 3.1183e-07
Epoch 162/512
384/384 - 0s - loss: 3.1718e-05 - val_loss: 3.2823e-07
Epoch 163/512
384/384 - 0s - loss: 3.1141e-05 - val_loss: 2.8806e-07
Epoch 164/512
384/384 - 0s - loss: 2.9259e-05 - val_loss: 3.0287e-07
Epoch 165/512
384/384 - 0s - loss: 2.9027e-05 - val_loss: 2.6915e-07
Epoch 166/512
384/384 - 0s - loss: 2.7353e-05 - val_loss: 2.8695e-07
Epoch 167/512
384/384 - 0s - loss: 2.7479e-05 - val_loss: 2.5491e-07
Epoch 168/512
384/384 - 0s - loss: 2.5926e-05 - val_loss: 2.6632e-07
Epoch 169/512
384/384 - 0s - loss: 2.5322e-05 - val_loss: 2.3592e-07
Epoch 170/512
384/384 - 0s - loss: 2.4020e-05 - val_loss: 2.5262e-07
Epoch 171/512
384/384 - 0s - loss: 2.4297e-05 - val_loss: 2.2194e-07
Epoch 172/512
384/384 - 0s - loss: 2.2484e-05 - val_loss: 2.3157e-07
Epoch 173/512
384/384 - 0s - loss: 2.2146e-05 - val_loss: 2.0685e-07
Epoch 174/512
384/384 - 0s - loss: 2.1445e-05 - val_loss: 2.2004e-07
Epoch 175/512
384/384 - 0s - loss: 2.0966e-05 - val_loss: 1.9232e-07
Epoch 176/512
384/384 - 0s - loss: 1.9534e-05 - val_loss: 2.0413e-07
Epoch 177/512
384/384 - 0s - loss: 1.9611e-05 - val_loss: 1.8304e-07
Epoch 178/512
384/384 - 0s - loss: 1.8689e-05 - val_loss: 1.9454e-07
Epoch 179/512
384/384 - 0s - loss: 1.8518e-05 - val_loss: 1.6853e-07
Epoch 180/512
384/384 - 0s - loss: 1.7022e-05 - val_loss: 1.7865e-07
Epoch 181/512
384/384 - 0s - loss: 1.7131e-05 - val_loss: 1.5960e-07
Epoch 182/512
384/384 - 0s - loss: 1.6365e-05 - val_loss: 1.7001e-07
Epoch 183/512
384/384 - 0s - loss: 1.6179e-05 - val_loss: 1.4694e-07
Epoch 184/512
384/384 - 0s - loss: 1.4942e-05 - val_loss: 1.5530e-07
Epoch 185/512
384/384 - 0s - loss: 1.5089e-05 - val_loss: 1.3693e-07
Epoch 186/512
384/384 - 0s - loss: 1.4124e-05 - val_loss: 1.4532e-07
Epoch 187/512
384/384 - 0s - loss: 1.3874e-05 - val_loss: 1.2961e-07
Epoch 188/512
384/384 - 0s - loss: 1.3300e-05 - val_loss: 1.3984e-07
Epoch 189/512
384/384 - 0s - loss: 1.3205e-05 - val_loss: 1.2206e-07
Epoch 190/512
384/384 - 0s - loss: 1.2423e-05 - val_loss: 1.2856e-07
Epoch 191/512
384/384 - 0s - loss: 1.2271e-05 - val_loss: 1.1089e-07
Epoch 192/512
384/384 - 0s - loss: 1.1380e-05 - val_loss: 1.1837e-07
Epoch 193/512
384/384 - 0s - loss: 1.1359e-05 - val_loss: 1.0591e-07
Epoch 194/512
384/384 - 0s - loss: 1.0800e-05 - val_loss: 1.1471e-07
Epoch 195/512
384/384 - 0s - loss: 1.0998e-05 - val_loss: 9.8159e-08
Epoch 196/512
384/384 - 0s - loss: 9.9675e-06 - val_loss: 1.0230e-07
Epoch 197/512
384/384 - 0s - loss: 9.8060e-06 - val_loss: 9.0226e-08
Epoch 198/512
384/384 - 0s - loss: 9.3527e-06 - val_loss: 9.8698e-08
Epoch 199/512
384/384 - 0s - loss: 9.4948e-06 - val_loss: 8.5877e-08
Epoch 200/512
384/384 - 0s - loss: 8.7620e-06 - val_loss: 9.0128e-08
Epoch 201/512
384/384 - 0s - loss: 8.6065e-06 - val_loss: 7.8849e-08
Epoch 202/512
384/384 - 0s - loss: 8.0811e-06 - val_loss: 8.6264e-08
Epoch 203/512
384/384 - 0s - loss: 8.2852e-06 - val_loss: 7.5241e-08
Epoch 204/512
384/384 - 0s - loss: 7.6239e-06 - val_loss: 7.9295e-08
Epoch 205/512
384/384 - 0s - loss: 7.5420e-06 - val_loss: 6.8682e-08
Epoch 206/512
384/384 - 0s - loss: 7.0259e-06 - val_loss: 7.4508e-08
Epoch 207/512
384/384 - 0s - loss: 7.1896e-06 - val_loss: 6.4862e-08
Epoch 208/512
384/384 - 0s - loss: 6.6237e-06 - val_loss: 6.8787e-08
Epoch 209/512
384/384 - 0s - loss: 6.5069e-06 - val_loss: 6.0254e-08
Epoch 210/512
384/384 - 0s - loss: 6.1869e-06 - val_loss: 6.5692e-08
Epoch 211/512
384/384 - 0s - loss: 6.2802e-06 - val_loss: 5.6070e-08
Epoch 212/512
384/384 - 0s - loss: 5.7054e-06 - val_loss: 5.8941e-08
Epoch 213/512
384/384 - 0s - loss: 5.6608e-06 - val_loss: 5.1889e-08
Epoch 214/512
384/384 - 0s - loss: 5.3420e-06 - val_loss: 5.7071e-08
Epoch 215/512
384/384 - 0s - loss: 5.4840e-06 - val_loss: 4.9095e-08
Epoch 216/512
384/384 - 0s - loss: 4.9989e-06 - val_loss: 5.1483e-08
Epoch 217/512
384/384 - 0s - loss: 4.9568e-06 - val_loss: 4.4287e-08
Epoch 218/512
384/384 - 0s - loss: 4.5731e-06 - val_loss: 4.8497e-08
Epoch 219/512
384/384 - 0s - loss: 4.6808e-06 - val_loss: 4.2823e-08
Epoch 220/512
384/384 - 0s - loss: 4.4113e-06 - val_loss: 4.5460e-08
Epoch 221/512
384/384 - 0s - loss: 4.3171e-06 - val_loss: 3.8726e-08
Epoch 222/512
384/384 - 0s - loss: 3.9755e-06 - val_loss: 4.2035e-08
Epoch 223/512
384/384 - 0s - loss: 4.0150e-06 - val_loss: 3.7265e-08
Epoch 224/512
384/384 - 0s - loss: 3.8504e-06 - val_loss: 4.0061e-08
Epoch 225/512
384/384 - 0s - loss: 3.7975e-06 - val_loss: 3.3457e-08
Epoch 226/512
384/384 - 0s - loss: 3.4353e-06 - val_loss: 3.5823e-08
Epoch 227/512
384/384 - 0s - loss: 3.4258e-06 - val_loss: 3.1897e-08
Epoch 228/512
384/384 - 0s - loss: 3.3106e-06 - val_loss: 3.5196e-08
Epoch 229/512
384/384 - 0s - loss: 3.3193e-06 - val_loss: 2.9833e-08
Epoch 230/512
384/384 - 0s - loss: 3.0257e-06 - val_loss: 3.1694e-08
Epoch 231/512
384/384 - 0s - loss: 2.9925e-06 - val_loss: 2.7331e-08
Epoch 232/512
384/384 - 0s - loss: 2.8463e-06 - val_loss: 2.9558e-08
Epoch 233/512
384/384 - 0s - loss: 2.8144e-06 - val_loss: 2.5488e-08
Epoch 234/512
384/384 - 0s - loss: 2.6238e-06 - val_loss: 2.7631e-08
Epoch 235/512
384/384 - 0s - loss: 2.6581e-06 - val_loss: 2.3582e-08
Epoch 236/512
384/384 - 0s - loss: 2.4192e-06 - val_loss: 2.5499e-08
Epoch 237/512
384/384 - 0s - loss: 2.4557e-06 - val_loss: 2.1982e-08
Epoch 238/512
384/384 - 0s - loss: 2.2526e-06 - val_loss: 2.4029e-08
Epoch 239/512
384/384 - 0s - loss: 2.2956e-06 - val_loss: 2.0683e-08
Epoch 240/512
384/384 - 0s - loss: 2.1099e-06 - val_loss: 2.2360e-08
Epoch 241/512
384/384 - 0s - loss: 2.1241e-06 - val_loss: 1.9107e-08
Epoch 242/512
384/384 - 0s - loss: 1.9657e-06 - val_loss: 2.0545e-08
Epoch 243/512
384/384 - 0s - loss: 1.9609e-06 - val_loss: 1.7696e-08
Epoch 244/512
384/384 - 0s - loss: 1.8222e-06 - val_loss: 1.9401e-08
Epoch 245/512
384/384 - 0s - loss: 1.8450e-06 - val_loss: 1.6609e-08
Epoch 246/512
384/384 - 0s - loss: 1.7025e-06 - val_loss: 1.7880e-08
Epoch 247/512
384/384 - 0s - loss: 1.6907e-06 - val_loss: 1.5311e-08
Epoch 248/512
384/384 - 0s - loss: 1.5834e-06 - val_loss: 1.6743e-08
Epoch 249/512
384/384 - 0s - loss: 1.5873e-06 - val_loss: 1.4267e-08
Epoch 250/512
384/384 - 0s - loss: 1.4637e-06 - val_loss: 1.5488e-08
Epoch 251/512
384/384 - 0s - loss: 1.4665e-06 - val_loss: 1.3274e-08
Epoch 252/512
384/384 - 0s - loss: 1.3758e-06 - val_loss: 1.4377e-08
Epoch 253/512
384/384 - 0s - loss: 1.3628e-06 - val_loss: 1.2228e-08
Epoch 254/512
384/384 - 0s - loss: 1.2539e-06 - val_loss: 1.3423e-08
Epoch 255/512
384/384 - 0s - loss: 1.2785e-06 - val_loss: 1.1558e-08
Epoch 256/512
384/384 - 0s - loss: 1.1918e-06 - val_loss: 1.2440e-08
Epoch 257/512
384/384 - 0s - loss: 1.1770e-06 - val_loss: 1.0450e-08
Epoch 258/512
384/384 - 0s - loss: 1.0755e-06 - val_loss: 1.1440e-08
Epoch 259/512
384/384 - 0s - loss: 1.0968e-06 - val_loss: 9.9634e-09
Epoch 260/512
384/384 - 0s - loss: 1.0239e-06 - val_loss: 1.0915e-08
Epoch 261/512
384/384 - 0s - loss: 1.0229e-06 - val_loss: 9.2190e-09
Epoch 262/512
384/384 - 0s - loss: 9.5210e-07 - val_loss: 9.8171e-09
Epoch 263/512
384/384 - 0s - loss: 9.3266e-07 - val_loss: 8.3203e-09
Epoch 264/512
384/384 - 0s - loss: 8.6032e-07 - val_loss: 9.3755e-09
Epoch 265/512
384/384 - 0s - loss: 8.9270e-07 - val_loss: 8.0709e-09
Epoch 266/512
384/384 - 0s - loss: 8.3020e-07 - val_loss: 8.5965e-09
Epoch 267/512
384/384 - 0s - loss: 8.0931e-07 - val_loss: 7.0979e-09
Epoch 268/512
384/384 - 0s - loss: 7.3478e-07 - val_loss: 7.8498e-09
Epoch 269/512
384/384 - 0s - loss: 7.5293e-07 - val_loss: 6.9122e-09
Epoch 270/512
384/384 - 0s - loss: 7.2046e-07 - val_loss: 7.5099e-09
Epoch 271/512
384/384 - 0s - loss: 7.0534e-07 - val_loss: 6.2073e-09
Epoch 272/512
384/384 - 0s - loss: 6.3811e-07 - val_loss: 6.7629e-09
Epoch 273/512
384/384 - 0s - loss: 6.4990e-07 - val_loss: 5.8300e-09
Epoch 274/512
384/384 - 0s - loss: 6.1143e-07 - val_loss: 6.3837e-09
Epoch 275/512
384/384 - 0s - loss: 5.9990e-07 - val_loss: 5.3913e-09
Epoch 276/512
384/384 - 0s - loss: 5.5755e-07 - val_loss: 5.9853e-09
Epoch 277/512
384/384 - 0s - loss: 5.7259e-07 - val_loss: 4.9844e-09
Epoch 278/512
384/384 - 0s - loss: 5.1169e-07 - val_loss: 5.3832e-09
Epoch 279/512
384/384 - 0s - loss: 5.1048e-07 - val_loss: 4.6666e-09
Epoch 280/512
384/384 - 0s - loss: 4.8738e-07 - val_loss: 5.2063e-09
Epoch 281/512
384/384 - 0s - loss: 4.8878e-07 - val_loss: 4.3554e-09
Epoch 282/512
384/384 - 0s - loss: 4.4597e-07 - val_loss: 4.6826e-09
Epoch 283/512
384/384 - 0s - loss: 4.4464e-07 - val_loss: 3.9631e-09
Epoch 284/512
384/384 - 0s - loss: 4.1387e-07 - val_loss: 4.3773e-09
Epoch 285/512
384/384 - 0s - loss: 4.1550e-07 - val_loss: 3.7098e-09
Epoch 286/512
384/384 - 0s - loss: 3.8191e-07 - val_loss: 4.1091e-09
Epoch 287/512
384/384 - 0s - loss: 3.9117e-07 - val_loss: 3.4386e-09
Epoch 288/512
384/384 - 0s - loss: 3.5423e-07 - val_loss: 3.7367e-09
Epoch 289/512
384/384 - 0s - loss: 3.5319e-07 - val_loss: 3.1799e-09
Epoch 290/512
384/384 - 0s - loss: 3.2756e-07 - val_loss: 3.5489e-09
Epoch 291/512
384/384 - 0s - loss: 3.3558e-07 - val_loss: 3.0092e-09
Epoch 292/512
384/384 - 0s - loss: 3.1081e-07 - val_loss: 3.2427e-09
Epoch 293/512
384/384 - 0s - loss: 3.0476e-07 - val_loss: 2.6752e-09
Epoch 294/512
384/384 - 0s - loss: 2.7726e-07 - val_loss: 2.9871e-09
Epoch 295/512
384/384 - 0s - loss: 2.8952e-07 - val_loss: 2.5429e-09
Epoch 296/512
384/384 - 0s - loss: 2.6224e-07 - val_loss: 2.7754e-09
Epoch 297/512
384/384 - 0s - loss: 2.6355e-07 - val_loss: 2.3372e-09
Epoch 298/512
384/384 - 0s - loss: 2.4043e-07 - val_loss: 2.6159e-09
Epoch 299/512
384/384 - 0s - loss: 2.4968e-07 - val_loss: 2.1850e-09
Epoch 300/512
384/384 - 0s - loss: 2.2508e-07 - val_loss: 2.3632e-09
Epoch 301/512
384/384 - 0s - loss: 2.2391e-07 - val_loss: 1.9973e-09
Epoch 302/512
384/384 - 0s - loss: 2.0885e-07 - val_loss: 2.2319e-09
Epoch 303/512
384/384 - 0s - loss: 2.1170e-07 - val_loss: 1.8761e-09
Epoch 304/512
384/384 - 0s - loss: 1.9291e-07 - val_loss: 2.0643e-09
Epoch 305/512
384/384 - 0s - loss: 1.9502e-07 - val_loss: 1.7337e-09
Epoch 306/512
384/384 - 0s - loss: 1.7974e-07 - val_loss: 1.9108e-09
Epoch 307/512
384/384 - 0s - loss: 1.8230e-07 - val_loss: 1.5787e-09
Epoch 308/512
384/384 - 0s - loss: 1.6315e-07 - val_loss: 1.7388e-09
Epoch 309/512
384/384 - 0s - loss: 1.6668e-07 - val_loss: 1.4880e-09
Epoch 310/512
384/384 - 0s - loss: 1.5330e-07 - val_loss: 1.6801e-09
Epoch 311/512
384/384 - 0s - loss: 1.5882e-07 - val_loss: 1.3919e-09
Epoch 312/512
384/384 - 0s - loss: 1.4161e-07 - val_loss: 1.5123e-09
Epoch 313/512
384/384 - 0s - loss: 1.4354e-07 - val_loss: 1.2556e-09
Epoch 314/512
384/384 - 0s - loss: 1.3074e-07 - val_loss: 1.3932e-09
Epoch 315/512
384/384 - 0s - loss: 1.3319e-07 - val_loss: 1.1782e-09
Epoch 316/512
384/384 - 0s - loss: 1.2206e-07 - val_loss: 1.3198e-09
Epoch 317/512
384/384 - 0s - loss: 1.2624e-07 - val_loss: 1.0843e-09
Epoch 318/512
384/384 - 0s - loss: 1.1177e-07 - val_loss: 1.1755e-09
Epoch 319/512
384/384 - 0s - loss: 1.1280e-07 - val_loss: 9.9464e-10
Epoch 320/512
384/384 - 0s - loss: 1.0501e-07 - val_loss: 1.1182e-09
Epoch 321/512
384/384 - 0s - loss: 1.0613e-07 - val_loss: 9.4454e-10
Epoch 322/512
384/384 - 0s - loss: 9.7914e-08 - val_loss: 1.0463e-09
Epoch 323/512
384/384 - 0s - loss: 9.8205e-08 - val_loss: 8.6672e-10
Epoch 324/512
384/384 - 0s - loss: 8.9764e-08 - val_loss: 9.6451e-10
Epoch 325/512
384/384 - 0s - loss: 9.1582e-08 - val_loss: 8.0257e-10
Epoch 326/512
384/384 - 0s - loss: 8.2174e-08 - val_loss: 8.9514e-10
Epoch 327/512
384/384 - 0s - loss: 8.5324e-08 - val_loss: 7.5070e-10
Epoch 328/512
384/384 - 0s - loss: 7.7467e-08 - val_loss: 8.2294e-10
Epoch 329/512
384/384 - 0s - loss: 7.7181e-08 - val_loss: 6.9079e-10
Epoch 330/512
384/384 - 0s - loss: 7.1312e-08 - val_loss: 7.7560e-10
Epoch 331/512
384/384 - 0s - loss: 7.3058e-08 - val_loss: 6.5184e-10
Epoch 332/512
384/384 - 0s - loss: 6.7092e-08 - val_loss: 7.1360e-10
Epoch 333/512
384/384 - 0s - loss: 6.7532e-08 - val_loss: 5.8273e-10
Epoch 334/512
384/384 - 0s - loss: 6.0543e-08 - val_loss: 6.4393e-10
Epoch 335/512
384/384 - 0s - loss: 6.1691e-08 - val_loss: 5.5099e-10
Epoch 336/512
384/384 - 0s - loss: 5.7478e-08 - val_loss: 6.1748e-10
Epoch 337/512
384/384 - 0s - loss: 5.8329e-08 - val_loss: 5.1424e-10
Epoch 338/512
384/384 - 0s - loss: 5.3181e-08 - val_loss: 5.6362e-10
Epoch 339/512
384/384 - 0s - loss: 5.3118e-08 - val_loss: 4.6774e-10
Epoch 340/512
384/384 - 0s - loss: 4.8508e-08 - val_loss: 5.2463e-10
Epoch 341/512
384/384 - 0s - loss: 4.9851e-08 - val_loss: 4.4345e-10
Epoch 342/512
384/384 - 0s - loss: 4.6195e-08 - val_loss: 4.8792e-10
Epoch 343/512
384/384 - 0s - loss: 4.5851e-08 - val_loss: 4.0263e-10
Epoch 344/512
384/384 - 0s - loss: 4.1851e-08 - val_loss: 4.5009e-10
Epoch 345/512
384/384 - 0s - loss: 4.2908e-08 - val_loss: 3.7787e-10
Epoch 346/512
384/384 - 0s - loss: 3.9035e-08 - val_loss: 4.1917e-10
Epoch 347/512
384/384 - 0s - loss: 3.9425e-08 - val_loss: 3.5300e-10
Epoch 348/512
384/384 - 0s - loss: 3.6600e-08 - val_loss: 3.9402e-10
Epoch 349/512
384/384 - 0s - loss: 3.7295e-08 - val_loss: 3.2493e-10
Epoch 350/512
384/384 - 0s - loss: 3.3594e-08 - val_loss: 3.5707e-10
Epoch 351/512
384/384 - 0s - loss: 3.3903e-08 - val_loss: 2.9898e-10
Epoch 352/512
384/384 - 0s - loss: 3.0960e-08 - val_loss: 3.3490e-10
Epoch 353/512
384/384 - 0s - loss: 3.1878e-08 - val_loss: 2.8568e-10
Epoch 354/512
384/384 - 0s - loss: 2.9572e-08 - val_loss: 3.1628e-10
Epoch 355/512
384/384 - 0s - loss: 2.9497e-08 - val_loss: 2.6191e-10
Epoch 356/512
384/384 - 0s - loss: 2.7116e-08 - val_loss: 2.8848e-10
Epoch 357/512
384/384 - 0s - loss: 2.7136e-08 - val_loss: 2.4350e-10
Epoch 358/512
384/384 - 0s - loss: 2.5115e-08 - val_loss: 2.7156e-10
Epoch 359/512
384/384 - 0s - loss: 2.5711e-08 - val_loss: 2.2789e-10
Epoch 360/512
384/384 - 0s - loss: 2.3619e-08 - val_loss: 2.4942e-10
Epoch 361/512
384/384 - 0s - loss: 2.3458e-08 - val_loss: 2.0941e-10
Epoch 362/512
384/384 - 0s - loss: 2.1790e-08 - val_loss: 2.3406e-10
Epoch 363/512
384/384 - 0s - loss: 2.2380e-08 - val_loss: 1.9545e-10
Epoch 364/512
384/384 - 0s - loss: 2.0240e-08 - val_loss: 2.1383e-10
Epoch 365/512
384/384 - 0s - loss: 2.0284e-08 - val_loss: 1.8145e-10
Epoch 366/512
384/384 - 0s - loss: 1.8916e-08 - val_loss: 2.0360e-10
Epoch 367/512
384/384 - 0s - loss: 1.9308e-08 - val_loss: 1.7151e-10
Epoch 368/512
384/384 - 0s - loss: 1.7746e-08 - val_loss: 1.8826e-10
Epoch 369/512
384/384 - 0s - loss: 1.7733e-08 - val_loss: 1.5745e-10
Epoch 370/512
384/384 - 0s - loss: 1.6215e-08 - val_loss: 1.7551e-10
Epoch 371/512
384/384 - 0s - loss: 1.6744e-08 - val_loss: 1.4889e-10
Epoch 372/512
384/384 - 0s - loss: 1.5372e-08 - val_loss: 1.6431e-10
Epoch 373/512
384/384 - 0s - loss: 1.5482e-08 - val_loss: 1.3900e-10
Epoch 374/512
384/384 - 0s - loss: 1.4368e-08 - val_loss: 1.5397e-10
Epoch 375/512
384/384 - 0s - loss: 1.4648e-08 - val_loss: 1.2879e-10
Epoch 376/512
384/384 - 0s - loss: 1.3265e-08 - val_loss: 1.4120e-10
Epoch 377/512
384/384 - 0s - loss: 1.3355e-08 - val_loss: 1.2080e-10
Epoch 378/512
384/384 - 0s - loss: 1.2565e-08 - val_loss: 1.3374e-10
Epoch 379/512
384/384 - 0s - loss: 1.2658e-08 - val_loss: 1.1381e-10
Epoch 380/512
384/384 - 0s - loss: 1.1774e-08 - val_loss: 1.2500e-10
Epoch 381/512
384/384 - 0s - loss: 1.1889e-08 - val_loss: 1.0563e-10
Epoch 382/512
384/384 - 0s - loss: 1.0908e-08 - val_loss: 1.1518e-10
Epoch 383/512
384/384 - 0s - loss: 1.0970e-08 - val_loss: 9.8468e-11
Epoch 384/512
384/384 - 0s - loss: 1.0271e-08 - val_loss: 1.0901e-10
Epoch 385/512
384/384 - 0s - loss: 1.0500e-08 - val_loss: 9.2761e-11
Epoch 386/512
384/384 - 0s - loss: 9.5549e-09 - val_loss: 1.0108e-10
Epoch 387/512
384/384 - 0s - loss: 9.6026e-09 - val_loss: 8.6738e-11
Epoch 388/512
384/384 - 0s - loss: 8.9830e-09 - val_loss: 9.6651e-11
Epoch 389/512
384/384 - 0s - loss: 9.2865e-09 - val_loss: 8.2365e-11
Epoch 390/512
384/384 - 0s - loss: 8.4315e-09 - val_loss: 8.9181e-11
Epoch 391/512
384/384 - 0s - loss: 8.4749e-09 - val_loss: 7.6253e-11
Epoch 392/512
384/384 - 0s - loss: 7.8892e-09 - val_loss: 8.4949e-11
Epoch 393/512
384/384 - 0s - loss: 8.0632e-09 - val_loss: 7.3291e-11
Epoch 394/512
384/384 - 0s - loss: 7.4946e-09 - val_loss: 8.0442e-11
Epoch 395/512
384/384 - 0s - loss: 7.6489e-09 - val_loss: 6.7849e-11
Epoch 396/512
384/384 - 0s - loss: 6.9187e-09 - val_loss: 7.4374e-11
Epoch 397/512
384/384 - 0s - loss: 7.0801e-09 - val_loss: 6.3778e-11
Epoch 398/512
384/384 - 0s - loss: 6.5713e-09 - val_loss: 6.9925e-11
Epoch 399/512
384/384 - 0s - loss: 6.5994e-09 - val_loss: 6.0825e-11
Epoch 400/512
384/384 - 0s - loss: 6.2980e-09 - val_loss: 6.6707e-11
Epoch 401/512
384/384 - 0s - loss: 6.3981e-09 - val_loss: 5.6780e-11
Epoch 402/512
384/384 - 0s - loss: 5.8135e-09 - val_loss: 6.1435e-11
Epoch 403/512
384/384 - 0s - loss: 5.8358e-09 - val_loss: 5.3443e-11
Epoch 404/512
384/384 - 0s - loss: 5.5130e-09 - val_loss: 5.8962e-11
Epoch 405/512
384/384 - 0s - loss: 5.6175e-09 - val_loss: 5.1292e-11
Epoch 406/512
384/384 - 0s - loss: 5.3241e-09 - val_loss: 5.5712e-11
Epoch 407/512
384/384 - 0s - loss: 5.2549e-09 - val_loss: 4.8030e-11
Epoch 408/512
384/384 - 0s - loss: 4.9601e-09 - val_loss: 5.2240e-11
Epoch 409/512
384/384 - 0s - loss: 4.9928e-09 - val_loss: 4.5132e-11
Epoch 410/512
384/384 - 0s - loss: 4.6831e-09 - val_loss: 4.9284e-11
Epoch 411/512
384/384 - 0s - loss: 4.7574e-09 - val_loss: 4.2763e-11
Epoch 412/512
384/384 - 0s - loss: 4.4150e-09 - val_loss: 4.5974e-11
Epoch 413/512
384/384 - 0s - loss: 4.4064e-09 - val_loss: 3.9907e-11
Epoch 414/512
384/384 - 0s - loss: 4.0951e-09 - val_loss: 4.4121e-11
Epoch 415/512
384/384 - 0s - loss: 4.2705e-09 - val_loss: 3.9180e-11
Epoch 416/512
384/384 - 0s - loss: 4.0523e-09 - val_loss: 4.2536e-11
Epoch 417/512
384/384 - 0s - loss: 4.0072e-09 - val_loss: 3.6778e-11
Epoch 418/512
384/384 - 0s - loss: 3.7762e-09 - val_loss: 3.9311e-11
Epoch 419/512
384/384 - 0s - loss: 3.7696e-09 - val_loss: 3.4274e-11
Epoch 420/512
384/384 - 0s - loss: 3.5507e-09 - val_loss: 3.7203e-11
Epoch 421/512
384/384 - 0s - loss: 3.5726e-09 - val_loss: 3.2959e-11
Epoch 422/512
384/384 - 0s - loss: 3.4128e-09 - val_loss: 3.5784e-11
Epoch 423/512
384/384 - 0s - loss: 3.3795e-09 - val_loss: 3.1379e-11
Epoch 424/512
384/384 - 0s - loss: 3.2593e-09 - val_loss: 3.3862e-11
Epoch 425/512
384/384 - 0s - loss: 3.2423e-09 - val_loss: 2.9537e-11
Epoch 426/512
384/384 - 0s - loss: 3.0351e-09 - val_loss: 3.2053e-11
Epoch 427/512
384/384 - 0s - loss: 3.0713e-09 - val_loss: 2.8577e-11
Epoch 428/512
384/384 - 0s - loss: 2.9538e-09 - val_loss: 3.0918e-11
Epoch 429/512
384/384 - 0s - loss: 2.9428e-09 - val_loss: 2.7005e-11
Epoch 430/512
384/384 - 0s - loss: 2.7574e-09 - val_loss: 2.9429e-11
Epoch 431/512
384/384 - 0s - loss: 2.8426e-09 - val_loss: 2.5917e-11
Epoch 432/512
384/384 - 0s - loss: 2.6552e-09 - val_loss: 2.7557e-11
Epoch 433/512
384/384 - 0s - loss: 2.6470e-09 - val_loss: 2.4254e-11
Epoch 434/512
384/384 - 0s - loss: 2.5135e-09 - val_loss: 2.6084e-11
Epoch 435/512
384/384 - 0s - loss: 2.5054e-09 - val_loss: 2.3258e-11
Epoch 436/512
384/384 - 0s - loss: 2.3728e-09 - val_loss: 2.5281e-11
Epoch 437/512
384/384 - 0s - loss: 2.4461e-09 - val_loss: 2.2734e-11
Epoch 438/512
384/384 - 0s - loss: 2.3344e-09 - val_loss: 2.4401e-11
Epoch 439/512
384/384 - 0s - loss: 2.3512e-09 - val_loss: 2.1565e-11
Epoch 440/512
384/384 - 0s - loss: 2.2014e-09 - val_loss: 2.2907e-11
Epoch 441/512
384/384 - 0s - loss: 2.2061e-09 - val_loss: 2.0334e-11
Epoch 442/512
384/384 - 0s - loss: 2.0736e-09 - val_loss: 2.1774e-11
Epoch 443/512
384/384 - 0s - loss: 2.1149e-09 - val_loss: 1.9745e-11
Epoch 444/512
384/384 - 0s - loss: 2.0456e-09 - val_loss: 2.1593e-11
Epoch 445/512
384/384 - 0s - loss: 2.0866e-09 - val_loss: 1.9333e-11
Epoch 446/512
384/384 - 0s - loss: 1.9751e-09 - val_loss: 2.0370e-11
Epoch 447/512
384/384 - 0s - loss: 1.9512e-09 - val_loss: 1.7894e-11
Epoch 448/512
384/384 - 0s - loss: 1.8335e-09 - val_loss: 1.9178e-11
Epoch 449/512
384/384 - 0s - loss: 1.8342e-09 - val_loss: 1.7315e-11
Epoch 450/512
384/384 - 0s - loss: 1.7842e-09 - val_loss: 1.8445e-11
Epoch 451/512
384/384 - 0s - loss: 1.8070e-09 - val_loss: 1.6576e-11
Epoch 452/512
384/384 - 0s - loss: 1.7063e-09 - val_loss: 1.7642e-11
Epoch 453/512
384/384 - 0s - loss: 1.7069e-09 - val_loss: 1.5724e-11
Epoch 454/512
384/384 - 0s - loss: 1.6162e-09 - val_loss: 1.6743e-11
Epoch 455/512
384/384 - 0s - loss: 1.6136e-09 - val_loss: 1.5155e-11
Epoch 456/512
384/384 - 0s - loss: 1.5554e-09 - val_loss: 1.6446e-11
Epoch 457/512
384/384 - 0s - loss: 1.5906e-09 - val_loss: 1.5168e-11
Epoch 458/512
384/384 - 0s - loss: 1.5426e-09 - val_loss: 1.6054e-11
Epoch 459/512
384/384 - 0s - loss: 1.5464e-09 - val_loss: 1.4334e-11
Epoch 460/512
384/384 - 0s - loss: 1.4642e-09 - val_loss: 1.5030e-11
Epoch 461/512
384/384 - 0s - loss: 1.4710e-09 - val_loss: 1.3393e-11
Epoch 462/512
384/384 - 0s - loss: 1.3597e-09 - val_loss: 1.4178e-11
Epoch 463/512
384/384 - 0s - loss: 1.3648e-09 - val_loss: 1.2873e-11
Epoch 464/512
384/384 - 0s - loss: 1.3322e-09 - val_loss: 1.4095e-11
Epoch 465/512
384/384 - 0s - loss: 1.3608e-09 - val_loss: 1.2922e-11
Epoch 466/512
384/384 - 0s - loss: 1.3070e-09 - val_loss: 1.3754e-11
Epoch 467/512
384/384 - 0s - loss: 1.3243e-09 - val_loss: 1.2295e-11
Epoch 468/512
384/384 - 0s - loss: 1.2437e-09 - val_loss: 1.2873e-11
Epoch 469/512
384/384 - 0s - loss: 1.2386e-09 - val_loss: 1.1608e-11
Epoch 470/512
384/384 - 0s - loss: 1.1919e-09 - val_loss: 1.2442e-11
Epoch 471/512
384/384 - 0s - loss: 1.2000e-09 - val_loss: 1.1297e-11
Epoch 472/512
384/384 - 0s - loss: 1.1514e-09 - val_loss: 1.1948e-11
Epoch 473/512
384/384 - 0s - loss: 1.1675e-09 - val_loss: 1.1025e-11
Epoch 474/512
384/384 - 0s - loss: 1.1391e-09 - val_loss: 1.1746e-11
Epoch 475/512
384/384 - 0s - loss: 1.1452e-09 - val_loss: 1.0491e-11
Epoch 476/512
384/384 - 0s - loss: 1.0607e-09 - val_loss: 1.0934e-11
Epoch 477/512
384/384 - 0s - loss: 1.0632e-09 - val_loss: 1.0165e-11
Epoch 478/512
384/384 - 0s - loss: 1.0482e-09 - val_loss: 1.0843e-11
Epoch 479/512
384/384 - 0s - loss: 1.0632e-09 - val_loss: 9.9239e-12
Epoch 480/512
384/384 - 0s - loss: 1.0041e-09 - val_loss: 1.0348e-11
Epoch 481/512
384/384 - 0s - loss: 1.0054e-09 - val_loss: 9.3775e-12
Epoch 482/512
384/384 - 0s - loss: 9.4954e-10 - val_loss: 9.9019e-12
Epoch 483/512
384/384 - 0s - loss: 9.5430e-10 - val_loss: 9.1737e-12
Epoch 484/512
384/384 - 0s - loss: 9.4485e-10 - val_loss: 9.7897e-12
Epoch 485/512
384/384 - 0s - loss: 9.5283e-10 - val_loss: 8.9346e-12
Epoch 486/512
384/384 - 0s - loss: 9.0857e-10 - val_loss: 9.4518e-12
Epoch 487/512
384/384 - 0s - loss: 9.0918e-10 - val_loss: 8.7516e-12
Epoch 488/512
384/384 - 0s - loss: 8.9043e-10 - val_loss: 9.2501e-12
Epoch 489/512
384/384 - 0s - loss: 9.0104e-10 - val_loss: 8.4686e-12
Epoch 490/512
384/384 - 0s - loss: 8.6820e-10 - val_loss: 8.9137e-12
Epoch 491/512
384/384 - 0s - loss: 8.6749e-10 - val_loss: 8.1105e-12
Epoch 492/512
384/384 - 0s - loss: 8.1990e-10 - val_loss: 8.4822e-12
Epoch 493/512
384/384 - 0s - loss: 8.2328e-10 - val_loss: 7.6813e-12
Epoch 494/512
384/384 - 0s - loss: 7.8035e-10 - val_loss: 8.1985e-12
Epoch 495/512
384/384 - 0s - loss: 8.0334e-10 - val_loss: 7.6747e-12
Epoch 496/512
384/384 - 0s - loss: 7.7872e-10 - val_loss: 8.0755e-12
Epoch 497/512
384/384 - 0s - loss: 7.9018e-10 - val_loss: 7.4869e-12
Epoch 498/512
384/384 - 0s - loss: 7.6484e-10 - val_loss: 7.8693e-12
Epoch 499/512
384/384 - 0s - loss: 7.6667e-10 - val_loss: 7.1927e-12
Epoch 500/512
384/384 - 0s - loss: 7.3428e-10 - val_loss: 7.5551e-12
Epoch 501/512
384/384 - 0s - loss: 7.3486e-10 - val_loss: 6.9305e-12
Epoch 502/512
384/384 - 0s - loss: 6.9788e-10 - val_loss: 7.1349e-12
Epoch 503/512
384/384 - 0s - loss: 6.9202e-10 - val_loss: 6.7526e-12
Epoch 504/512
384/384 - 0s - loss: 6.9220e-10 - val_loss: 7.0786e-12
Epoch 505/512
384/384 - 0s - loss: 6.8724e-10 - val_loss: 6.6994e-12
Epoch 506/512
384/384 - 0s - loss: 6.8363e-10 - val_loss: 7.0254e-12
Epoch 507/512
384/384 - 0s - loss: 6.9552e-10 - val_loss: 6.4833e-12
Epoch 508/512
384/384 - 0s - loss: 6.5736e-10 - val_loss: 6.6589e-12
Epoch 509/512
384/384 - 0s - loss: 6.4356e-10 - val_loss: 5.9923e-12
Epoch 510/512
384/384 - 0s - loss: 6.0607e-10 - val_loss: 6.1422e-12
Epoch 511/512
384/384 - 0s - loss: 6.0853e-10 - val_loss: 5.9514e-12
Epoch 512/512
384/384 - 0s - loss: 6.0739e-10 - val_loss: 6.3643e-12
2024-04-08 16:07:18.358894: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 384 samples, validate on 384 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 7.9890e-10 - val_loss: 1.0406e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1860e-09 - val_loss: 1.4231e-09
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3680e-09 - val_loss: 1.1857e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.1205e-09 - val_loss: 9.6027e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.4544e-10 - val_loss: 6.6241e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 6.3138e-10 - val_loss: 5.8759e-10
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.6662e-10 - val_loss: 5.4088e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6554e-10 - val_loss: 6.2935e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4691e-10 - val_loss: 6.7768e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.1997e-10 - val_loss: 8.0649e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0790e-10 - val_loss: 7.9859e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.2154e-10 - val_loss: 8.5123e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.1296e-10 - val_loss: 7.3496e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.2960e-10 - val_loss: 7.1236e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7531e-10 - val_loss: 6.2046e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2329e-10 - val_loss: 6.3897e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2769e-10 - val_loss: 6.0887e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2322e-10 - val_loss: 6.5327e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4470e-10 - val_loss: 6.2426e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3948e-10 - val_loss: 6.6951e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5793e-10 - val_loss: 6.3816e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5067e-10 - val_loss: 6.7414e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4845e-10 - val_loss: 6.0359e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1056e-10 - val_loss: 6.2028e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0467e-10 - val_loss: 5.6744e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7556e-10 - val_loss: 5.9816e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.8740e-10 - val_loss: 5.6960e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7959e-10 - val_loss: 5.9803e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7794e-10 - val_loss: 5.4835e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5665e-10 - val_loss: 5.6881e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.5812e-10 - val_loss: 5.3072e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3928e-10 - val_loss: 5.5968e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.4537e-10 - val_loss: 5.2152e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2998e-10 - val_loss: 5.4446e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.2788e-10 - val_loss: 5.0248e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0887e-10 - val_loss: 5.1838e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.0148e-10 - val_loss: 4.8201e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8922e-10 - val_loss: 5.0490e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.9848e-10 - val_loss: 4.8132e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8681e-10 - val_loss: 4.9693e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.8933e-10 - val_loss: 4.6778e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7580e-10 - val_loss: 4.8348e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.6767e-10 - val_loss: 4.4654e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.5575e-10 - val_loss: 4.7244e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.6228e-10 - val_loss: 4.5212e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.5945e-10 - val_loss: 4.7069e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.5999e-10 - val_loss: 4.3887e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3933e-10 - val_loss: 4.4672e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.3824e-10 - val_loss: 4.1938e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2566e-10 - val_loss: 4.4302e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.3088e-10 - val_loss: 4.0475e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0932e-10 - val_loss: 4.2006e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.0974e-10 - val_loss: 3.8774e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9409e-10 - val_loss: 4.0684e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0402e-10 - val_loss: 3.9512e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0376e-10 - val_loss: 4.1948e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.0884e-10 - val_loss: 3.8561e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9028e-10 - val_loss: 4.0067e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.8680e-10 - val_loss: 3.6101e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6370e-10 - val_loss: 3.7387e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.6810e-10 - val_loss: 3.5645e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6271e-10 - val_loss: 3.7737e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7221e-10 - val_loss: 3.6104e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6883e-10 - val_loss: 3.8663e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8203e-10 - val_loss: 3.7173e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7828e-10 - val_loss: 3.8990e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.7702e-10 - val_loss: 3.5203e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5462e-10 - val_loss: 3.6219e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.5466e-10 - val_loss: 3.3957e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4412e-10 - val_loss: 3.4801e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.3574e-10 - val_loss: 3.1625e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1905e-10 - val_loss: 3.2471e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.1825e-10 - val_loss: 3.0595e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1233e-10 - val_loss: 3.2737e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2328e-10 - val_loss: 3.1461e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2297e-10 - val_loss: 3.4076e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3736e-10 - val_loss: 3.2668e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3359e-10 - val_loss: 3.4588e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3779e-10 - val_loss: 3.2176e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2192e-10 - val_loss: 3.1347e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.0238e-10 - val_loss: 2.8655e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8809e-10 - val_loss: 2.8907e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.8322e-10 - val_loss: 2.7433e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8268e-10 - val_loss: 2.9663e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9489e-10 - val_loss: 2.9036e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9657e-10 - val_loss: 3.1327e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1042e-10 - val_loss: 3.0360e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0808e-10 - val_loss: 3.1372e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0422e-10 - val_loss: 2.8687e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8986e-10 - val_loss: 2.9396e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.8366e-10 - val_loss: 2.6678e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6925e-10 - val_loss: 2.7230e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.6344e-10 - val_loss: 2.5249e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5642e-10 - val_loss: 2.6238e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.5380e-10 - val_loss: 2.4269e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4932e-10 - val_loss: 2.6077e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5820e-10 - val_loss: 2.5034e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5441e-10 - val_loss: 2.6215e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5771e-10 - val_loss: 2.5046e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5327e-10 - val_loss: 2.5710e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5177e-10 - val_loss: 2.4686e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5055e-10 - val_loss: 2.6085e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.5400e-10 - val_loss: 2.4093e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4392e-10 - val_loss: 2.4830e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.4108e-10 - val_loss: 2.3159e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3296e-10 - val_loss: 2.3273e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2981e-10 - val_loss: 2.2767e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2966e-10 - val_loss: 2.3119e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2766e-10 - val_loss: 2.2263e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2600e-10 - val_loss: 2.2763e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2438e-10 - val_loss: 2.2073e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2679e-10 - val_loss: 2.3219e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2769e-10 - val_loss: 2.2028e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2440e-10 - val_loss: 2.2958e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2639e-10 - val_loss: 2.1976e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2408e-10 - val_loss: 2.2836e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.2391e-10 - val_loss: 2.0968e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0924e-10 - val_loss: 2.1250e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.0733e-10 - val_loss: 2.0095e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0305e-10 - val_loss: 2.0975e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0739e-10 - val_loss: 2.0219e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0875e-10 - val_loss: 2.1878e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1361e-10 - val_loss: 2.0329e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0454e-10 - val_loss: 2.0781e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.0487e-10 - val_loss: 1.9483e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9679e-10 - val_loss: 2.0175e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0171e-10 - val_loss: 1.9914e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0095e-10 - val_loss: 2.0373e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.9817e-10 - val_loss: 1.9084e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9338e-10 - val_loss: 1.9444e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.9125e-10 - val_loss: 1.8642e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8785e-10 - val_loss: 1.9428e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9377e-10 - val_loss: 1.8842e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8926e-10 - val_loss: 1.9480e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.9117e-10 - val_loss: 1.8184e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.8064e-10 - val_loss: 1.8074e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.7908e-10 - val_loss: 1.7354e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7612e-10 - val_loss: 1.8221e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7981e-10 - val_loss: 1.7759e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7933e-10 - val_loss: 1.8273e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.8056e-10 - val_loss: 1.7323e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7345e-10 - val_loss: 1.7853e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.7686e-10 - val_loss: 1.7262e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.7306e-10 - val_loss: 1.7198e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.7124e-10 - val_loss: 1.6917e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7098e-10 - val_loss: 1.7782e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.7477e-10 - val_loss: 1.6891e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7222e-10 - val_loss: 1.7245e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.6689e-10 - val_loss: 1.5759e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6004e-10 - val_loss: 1.6180e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.5864e-10 - val_loss: 1.5323e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5539e-10 - val_loss: 1.6197e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5974e-10 - val_loss: 1.5492e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5676e-10 - val_loss: 1.5853e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5751e-10 - val_loss: 1.5606e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5701e-10 - val_loss: 1.6064e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.5710e-10 - val_loss: 1.5065e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5210e-10 - val_loss: 1.5328e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.4982e-10 - val_loss: 1.5041e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5496e-10 - val_loss: 1.5859e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5667e-10 - val_loss: 1.5401e-10
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5391e-10 - val_loss: 1.5679e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.5429e-10 - val_loss: 1.5013e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5256e-10 - val_loss: 1.5237e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.4771e-10 - val_loss: 1.4482e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4715e-10 - val_loss: 1.5004e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.4802e-10 - val_loss: 1.4190e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4295e-10 - val_loss: 1.4298e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.4185e-10 - val_loss: 1.3927e-10
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4095e-10 - val_loss: 1.4672e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4514e-10 - val_loss: 1.4571e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4779e-10 - val_loss: 1.5018e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4859e-10 - val_loss: 1.4238e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4373e-10 - val_loss: 1.4337e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.4011e-10 - val_loss: 1.3505e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.3521e-10 - val_loss: 1.3457e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.3233e-10 - val_loss: 1.2767e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2943e-10 - val_loss: 1.3134e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3058e-10 - val_loss: 1.2921e-10
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3058e-10 - val_loss: 1.3514e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3283e-10 - val_loss: 1.2797e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2763e-10 - val_loss: 1.3014e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.2894e-10 - val_loss: 1.2605e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2594e-10 - val_loss: 1.2813e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.2647e-10 - val_loss: 1.2242e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2246e-10 - val_loss: 1.2548e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2481e-10 - val_loss: 1.2398e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2667e-10 - val_loss: 1.3080e-10
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2872e-10 - val_loss: 1.2483e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2734e-10 - val_loss: 1.2881e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.2422e-10 - val_loss: 1.1945e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2037e-10 - val_loss: 1.2121e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.1899e-10 - val_loss: 1.1741e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1839e-10 - val_loss: 1.2122e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.1804e-10 - val_loss: 1.1212e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1354e-10 - val_loss: 1.1770e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.1548e-10 - val_loss: 1.1178e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1316e-10 - val_loss: 1.1780e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1638e-10 - val_loss: 1.1336e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1519e-10 - val_loss: 1.1979e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1970e-10 - val_loss: 1.1444e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1523e-10 - val_loss: 1.1715e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1754e-10 - val_loss: 1.1471e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1549e-10 - val_loss: 1.1382e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.0929e-10 - val_loss: 1.0481e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0798e-10 - val_loss: 1.1212e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0914e-10 - val_loss: 1.0627e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0991e-10 - val_loss: 1.1275e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0999e-10 - val_loss: 1.0801e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0958e-10 - val_loss: 1.1138e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0914e-10 - val_loss: 1.0636e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0857e-10 - val_loss: 1.1082e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0929e-10 - val_loss: 1.0656e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0819e-10 - val_loss: 1.0924e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.0640e-10 - val_loss: 1.0247e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0170e-10 - val_loss: 1.0289e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 1.0134e-10 - val_loss: 9.9268e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 9.8443e-11 - val_loss: 9.7179e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 9.6863e-11 - val_loss: 9.5510e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.5373e-11 - val_loss: 9.5658e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.6379e-11 - val_loss: 9.7959e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0069e-10 - val_loss: 1.0282e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0157e-10 - val_loss: 1.0012e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0085e-10 - val_loss: 1.0007e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.7725e-11 - val_loss: 9.6681e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.9079e-11 - val_loss: 1.0343e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0429e-10 - val_loss: 1.0246e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0336e-10 - val_loss: 1.0516e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0462e-10 - val_loss: 1.0023e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0004e-10 - val_loss: 1.0139e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 9.8406e-11 - val_loss: 9.1845e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.2406e-11 - val_loss: 9.2029e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 9.0429e-11 - val_loss: 8.9043e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.9837e-11 - val_loss: 8.8394e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.5418e-11 - val_loss: 8.3754e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.6226e-11 - val_loss: 9.1503e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.0930e-11 - val_loss: 8.9691e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.1272e-11 - val_loss: 9.2911e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.1189e-11 - val_loss: 9.0645e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.0725e-11 - val_loss: 9.2254e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.1216e-11 - val_loss: 8.8354e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.7122e-11 - val_loss: 8.4894e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.5097e-11 - val_loss: 8.5156e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.6350e-11 - val_loss: 8.6732e-11
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.4051e-11 - val_loss: 8.2666e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.5091e-11 - val_loss: 8.7767e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.6751e-11 - val_loss: 8.2313e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3080e-11 - val_loss: 8.6612e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.5869e-11 - val_loss: 8.2357e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.2371e-11 - val_loss: 8.1523e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.1782e-11 - val_loss: 8.3008e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.2832e-11 - val_loss: 8.2790e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.1284e-11 - val_loss: 8.0606e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.4003e-11 - val_loss: 8.7109e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.6306e-11 - val_loss: 8.5662e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.6107e-11 - val_loss: 8.6644e-11
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.4725e-11 - val_loss: 7.9999e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0026e-11 - val_loss: 8.0632e-11
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.0511e-11 - val_loss: 7.8501e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9442e-11 - val_loss: 8.1802e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0755e-11 - val_loss: 8.0144e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0785e-11 - val_loss: 8.1088e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 8.0145e-11 - val_loss: 7.7916e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9967e-11 - val_loss: 8.5415e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3688e-11 - val_loss: 7.8657e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.8694e-11 - val_loss: 7.9678e-11
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 7.9527e-11 - val_loss: 7.7392e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.7986e-11 - val_loss: 7.8302e-11
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 7.5513e-11 - val_loss: 6.8817e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 6.8020e-11 - val_loss: 6.8350e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 6.6896e-11 - val_loss: 6.2067e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3026e-11 - val_loss: 6.7446e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7986e-11 - val_loss: 6.8995e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9944e-11 - val_loss: 7.5107e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.5898e-11 - val_loss: 7.8357e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0443e-11 - val_loss: 8.2547e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9618e-11 - val_loss: 7.6176e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6698e-11 - val_loss: 7.4926e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.2097e-11 - val_loss: 6.8599e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9223e-11 - val_loss: 7.2849e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.2601e-11 - val_loss: 6.9673e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.0102e-11 - val_loss: 7.0495e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.8996e-11 - val_loss: 6.9370e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.0573e-11 - val_loss: 7.3367e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.1865e-11 - val_loss: 6.9756e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.0882e-11 - val_loss: 7.4177e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.3377e-11 - val_loss: 6.9164e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9196e-11 - val_loss: 7.0282e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9294e-11 - val_loss: 6.7826e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.8144e-11 - val_loss: 6.9024e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.8032e-11 - val_loss: 6.6415e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7023e-11 - val_loss: 6.5869e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 6.3102e-11 - val_loss: 6.1715e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3397e-11 - val_loss: 6.6030e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5664e-11 - val_loss: 6.5288e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4727e-11 - val_loss: 6.3663e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4219e-11 - val_loss: 6.4376e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5333e-11 - val_loss: 6.7793e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7661e-11 - val_loss: 6.8479e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9156e-11 - val_loss: 6.9029e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7235e-11 - val_loss: 6.7354e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7318e-11 - val_loss: 6.5894e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3393e-11 - val_loss: 6.2599e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3393e-11 - val_loss: 6.4553e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4340e-11 - val_loss: 6.2753e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 6.1867e-11 - val_loss: 6.0795e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.9305e-11 - val_loss: 5.8692e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0551e-11 - val_loss: 6.4723e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4720e-11 - val_loss: 6.2546e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2680e-11 - val_loss: 6.4538e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4563e-11 - val_loss: 6.3980e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5190e-11 - val_loss: 6.5068e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3138e-11 - val_loss: 5.9242e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.8812e-11 - val_loss: 5.8212e-11
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.7895e-11 - val_loss: 5.6445e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6858e-11 - val_loss: 5.7768e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7587e-11 - val_loss: 5.6592e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.6720e-11 - val_loss: 5.5814e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.5608e-11 - val_loss: 5.5778e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6454e-11 - val_loss: 5.6968e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.6436e-11 - val_loss: 5.5117e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6593e-11 - val_loss: 6.1712e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1634e-11 - val_loss: 5.9429e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9669e-11 - val_loss: 6.0802e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9863e-11 - val_loss: 6.0644e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2660e-11 - val_loss: 6.6057e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5043e-11 - val_loss: 6.1171e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0356e-11 - val_loss: 6.0554e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0536e-11 - val_loss: 6.0984e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1478e-11 - val_loss: 6.0617e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.8445e-11 - val_loss: 5.4112e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.3551e-11 - val_loss: 5.1912e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.1052e-11 - val_loss: 4.8681e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.8471e-11 - val_loss: 4.8373e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8442e-11 - val_loss: 4.9962e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1982e-11 - val_loss: 5.6385e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5004e-11 - val_loss: 5.0991e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0079e-11 - val_loss: 5.0013e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1718e-11 - val_loss: 5.4237e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5690e-11 - val_loss: 5.7762e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6376e-11 - val_loss: 5.2879e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2211e-11 - val_loss: 5.1898e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3064e-11 - val_loss: 5.5785e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7432e-11 - val_loss: 5.9375e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.8544e-11 - val_loss: 5.7771e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7523e-11 - val_loss: 5.8413e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7193e-11 - val_loss: 5.3867e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3338e-11 - val_loss: 5.1752e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 5.0113e-11 - val_loss: 4.7307e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7281e-11 - val_loss: 4.7615e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.6261e-11 - val_loss: 4.2923e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.2834e-11 - val_loss: 4.2763e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3786e-11 - val_loss: 4.5271e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.6229e-11 - val_loss: 4.7483e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7281e-11 - val_loss: 4.7172e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8268e-11 - val_loss: 5.2201e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2650e-11 - val_loss: 5.3602e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3339e-11 - val_loss: 5.2786e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1578e-11 - val_loss: 4.9357e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9735e-11 - val_loss: 5.2717e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2726e-11 - val_loss: 5.2651e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1414e-11 - val_loss: 4.9109e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8559e-11 - val_loss: 4.7637e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8027e-11 - val_loss: 4.8783e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9920e-11 - val_loss: 5.0302e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9683e-11 - val_loss: 4.8771e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7791e-11 - val_loss: 4.7195e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7767e-11 - val_loss: 4.9570e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0836e-11 - val_loss: 5.1577e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2810e-11 - val_loss: 5.2924e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1417e-11 - val_loss: 4.7837e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7326e-11 - val_loss: 4.6769e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.5672e-11 - val_loss: 4.2544e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.1824e-11 - val_loss: 4.0944e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0723e-11 - val_loss: 4.1200e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2365e-11 - val_loss: 4.4820e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.5926e-11 - val_loss: 4.7275e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8792e-11 - val_loss: 5.2356e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0969e-11 - val_loss: 4.7550e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7984e-11 - val_loss: 4.8501e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8512e-11 - val_loss: 4.8603e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9092e-11 - val_loss: 4.9920e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9037e-11 - val_loss: 4.5250e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.5011e-11 - val_loss: 4.3897e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3033e-11 - val_loss: 4.2847e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2668e-11 - val_loss: 4.2398e-11
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 4.1296e-11 - val_loss: 3.8523e-11
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.8162e-11 - val_loss: 3.7372e-11
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.6428e-11 - val_loss: 3.4980e-11
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.4045e-11 - val_loss: 3.2886e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.2335e-11 - val_loss: 3.1936e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2117e-11 - val_loss: 3.2189e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1901e-11 - val_loss: 3.2221e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3111e-11 - val_loss: 3.5200e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5735e-11 - val_loss: 3.6824e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7157e-11 - val_loss: 3.6038e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4804e-11 - val_loss: 3.3831e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3446e-11 - val_loss: 3.1955e-11
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 3.1019e-11 - val_loss: 2.9545e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.8255e-11 - val_loss: 2.6242e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.5745e-11 - val_loss: 2.5631e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.5187e-11 - val_loss: 2.4830e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4513e-11 - val_loss: 2.5614e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6914e-11 - val_loss: 2.8741e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0118e-11 - val_loss: 3.2861e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3918e-11 - val_loss: 3.6597e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7289e-11 - val_loss: 3.7389e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8219e-11 - val_loss: 4.0011e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0787e-11 - val_loss: 4.2835e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3431e-11 - val_loss: 4.4697e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.4466e-11 - val_loss: 4.3395e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2259e-11 - val_loss: 4.0395e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9121e-11 - val_loss: 3.6452e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5820e-11 - val_loss: 3.4194e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2699e-11 - val_loss: 3.0274e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0363e-11 - val_loss: 3.0689e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0299e-11 - val_loss: 3.0157e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0674e-11 - val_loss: 3.1190e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0882e-11 - val_loss: 3.2078e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3567e-11 - val_loss: 3.5914e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6546e-11 - val_loss: 3.7321e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7395e-11 - val_loss: 3.6490e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5747e-11 - val_loss: 3.4295e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3325e-11 - val_loss: 3.1726e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1045e-11 - val_loss: 3.2118e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3409e-11 - val_loss: 3.5958e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5988e-11 - val_loss: 3.5872e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5028e-11 - val_loss: 3.3502e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2574e-11 - val_loss: 3.1030e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0266e-11 - val_loss: 2.9539e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9287e-11 - val_loss: 2.9532e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7984e-11 - val_loss: 2.6033e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.5591e-11 - val_loss: 2.4156e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4000e-11 - val_loss: 2.4809e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4822e-11 - val_loss: 2.6059e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7150e-11 - val_loss: 2.8119e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8518e-11 - val_loss: 2.9750e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0144e-11 - val_loss: 3.0812e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1126e-11 - val_loss: 3.3006e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4530e-11 - val_loss: 3.6960e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7176e-11 - val_loss: 3.7825e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8755e-11 - val_loss: 3.8959e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7796e-11 - val_loss: 3.5614e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4954e-11 - val_loss: 3.4380e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5444e-11 - val_loss: 3.6480e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7117e-11 - val_loss: 3.8030e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7823e-11 - val_loss: 3.7359e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7879e-11 - val_loss: 3.6794e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4962e-11 - val_loss: 3.2498e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2152e-11 - val_loss: 3.0665e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0093e-11 - val_loss: 2.9062e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8587e-11 - val_loss: 2.7574e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6819e-11 - val_loss: 2.5341e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5546e-11 - val_loss: 2.5934e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6758e-11 - val_loss: 2.9073e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9877e-11 - val_loss: 3.1278e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0975e-11 - val_loss: 3.0185e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0728e-11 - val_loss: 3.3050e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4477e-11 - val_loss: 3.5594e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6460e-11 - val_loss: 3.7183e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6604e-11 - val_loss: 3.6717e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7121e-11 - val_loss: 3.6281e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5185e-11 - val_loss: 3.3165e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3414e-11 - val_loss: 3.2992e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2337e-11 - val_loss: 3.2767e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3327e-11 - val_loss: 3.4248e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3876e-11 - val_loss: 3.2758e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2322e-11 - val_loss: 3.1707e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1573e-11 - val_loss: 3.1292e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1118e-11 - val_loss: 3.1358e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1142e-11 - val_loss: 3.1062e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1628e-11 - val_loss: 3.4027e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4775e-11 - val_loss: 3.5439e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6366e-11 - val_loss: 3.7609e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6644e-11 - val_loss: 3.3771e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2942e-11 - val_loss: 3.1485e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0879e-11 - val_loss: 3.0274e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0387e-11 - val_loss: 2.9362e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8212e-11 - val_loss: 2.5997e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5635e-11 - val_loss: 2.5386e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5181e-11 - val_loss: 2.6069e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6748e-11 - val_loss: 2.8280e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8340e-11 - val_loss: 2.8866e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9721e-11 - val_loss: 3.0563e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0832e-11 - val_loss: 3.1261e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2259e-11 - val_loss: 3.5130e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5901e-11 - val_loss: 3.6855e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7087e-11 - val_loss: 3.7554e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6632e-11 - val_loss: 3.4077e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3165e-11 - val_loss: 3.2177e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1239e-11 - val_loss: 2.9776e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0234e-11 - val_loss: 2.9339e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8372e-11 - val_loss: 2.6025e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5730e-11 - val_loss: 2.4970e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_addition_weights.h5
384/384 - 0s - loss: 2.4391e-11 - val_loss: 2.3988e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5112e-11 - val_loss: 2.8022e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8875e-11 - val_loss: 3.0245e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0231e-11 - val_loss: 3.0281e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0534e-11 - val_loss: 3.0988e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1135e-11 - val_loss: 2.9815e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8505e-11 - val_loss: 2.5795e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6118e-11 - val_loss: 2.6619e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7276e-11 - val_loss: 2.8287e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9158e-11 - val_loss: 3.0428e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0485e-11 - val_loss: 3.0387e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0453e-11 - val_loss: 3.0608e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0604e-11 - val_loss: 3.0747e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1418e-11 - val_loss: 3.4203e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4798e-11 - val_loss: 3.5237e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4094e-11 - val_loss: 3.2457e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1368e-11 - val_loss: 3.0338e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0406e-11 - val_loss: 3.0662e-11
Train on 384 samples, validate on 384 samples
Epoch 1/512
384/384 - 1s - loss: 0.4534 - val_loss: 0.3140
Epoch 2/512
384/384 - 0s - loss: 0.1558 - val_loss: 0.1737
Epoch 3/512
384/384 - 0s - loss: 0.0614 - val_loss: 0.0321
Epoch 4/512
384/384 - 0s - loss: 0.0190 - val_loss: 0.0461
Epoch 5/512
384/384 - 0s - loss: 0.0083 - val_loss: 0.0506
Epoch 6/512
384/384 - 0s - loss: 0.0068 - val_loss: 0.0559
Epoch 7/512
384/384 - 0s - loss: 0.0063 - val_loss: 0.0596
Epoch 8/512
384/384 - 0s - loss: 0.0061 - val_loss: 0.0604
Epoch 9/512
384/384 - 0s - loss: 0.0060 - val_loss: 0.0585
Epoch 10/512
384/384 - 0s - loss: 0.0059 - val_loss: 0.0699
Epoch 11/512
384/384 - 0s - loss: 0.0092 - val_loss: 0.1551
Epoch 12/512
384/384 - 0s - loss: 0.0105 - val_loss: 0.0448
Epoch 13/512
384/384 - 0s - loss: 0.0056 - val_loss: 0.0487
Epoch 14/512
384/384 - 0s - loss: 0.0060 - val_loss: 0.0794
Epoch 15/512
384/384 - 0s - loss: 0.0088 - val_loss: 0.0652
Epoch 16/512
384/384 - 0s - loss: 0.0066 - val_loss: 0.0640
Epoch 17/512
384/384 - 0s - loss: 0.0066 - val_loss: 0.0642
Epoch 18/512
384/384 - 0s - loss: 0.0070 - val_loss: 0.0616
Epoch 19/512
384/384 - 0s - loss: 0.0066 - val_loss: 0.0581
Epoch 20/512
384/384 - 0s - loss: 0.0065 - val_loss: 0.0588
Epoch 21/512
384/384 - 0s - loss: 0.0062 - val_loss: 0.0662
Epoch 22/512
384/384 - 0s - loss: 0.0061 - val_loss: 0.0711
Epoch 23/512
384/384 - 0s - loss: 0.0066 - val_loss: 0.0402
Epoch 24/512
384/384 - 0s - loss: 0.0049 - val_loss: 0.0676
Epoch 25/512
384/384 - 0s - loss: 0.0068 - val_loss: 0.0395
Epoch 26/512
384/384 - 0s - loss: 0.0045 - val_loss: 0.0609
Epoch 27/512
384/384 - 0s - loss: 0.0064 - val_loss: 0.0604
Epoch 28/512
384/384 - 0s - loss: 0.0064 - val_loss: 0.0363
Epoch 29/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0590
Epoch 30/512
384/384 - 0s - loss: 0.0068 - val_loss: 0.0632
Epoch 31/512
384/384 - 0s - loss: 0.0053 - val_loss: 0.0257
Epoch 32/512
384/384 - 0s - loss: 0.0042 - val_loss: 0.0391
Epoch 33/512
384/384 - 0s - loss: 0.0043 - val_loss: 0.0310
Epoch 34/512
384/384 - 0s - loss: 0.0074 - val_loss: 0.0151
Epoch 35/512
384/384 - 0s - loss: 0.0048 - val_loss: 0.0306
Epoch 36/512
384/384 - 0s - loss: 0.0046 - val_loss: 0.0340
Epoch 37/512
384/384 - 0s - loss: 0.0036 - val_loss: 0.0274
Epoch 38/512
384/384 - 0s - loss: 0.0063 - val_loss: 0.0539
Epoch 39/512
384/384 - 0s - loss: 0.0061 - val_loss: 0.0229
Epoch 40/512
384/384 - 0s - loss: 0.0057 - val_loss: 0.0234
Epoch 41/512
384/384 - 0s - loss: 0.0031 - val_loss: 0.0275
Epoch 42/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0309
Epoch 43/512
384/384 - 0s - loss: 0.0040 - val_loss: 0.0243
Epoch 44/512
384/384 - 0s - loss: 0.0046 - val_loss: 0.0370
Epoch 45/512
384/384 - 0s - loss: 0.0046 - val_loss: 0.0252
Epoch 46/512
384/384 - 0s - loss: 0.0106 - val_loss: 0.0160
Epoch 47/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0248
Epoch 48/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0321
Epoch 49/512
384/384 - 0s - loss: 0.0092 - val_loss: 0.0299
Epoch 50/512
384/384 - 0s - loss: 0.0034 - val_loss: 0.0169
Epoch 51/512
384/384 - 0s - loss: 0.0047 - val_loss: 0.0224
Epoch 52/512
384/384 - 0s - loss: 0.0035 - val_loss: 0.0132
Epoch 53/512
384/384 - 0s - loss: 0.0049 - val_loss: 0.0346
Epoch 54/512
384/384 - 0s - loss: 0.0056 - val_loss: 0.0180
Epoch 55/512
384/384 - 0s - loss: 0.0052 - val_loss: 0.0275
Epoch 56/512
384/384 - 0s - loss: 0.0051 - val_loss: 0.0121
Epoch 57/512
384/384 - 0s - loss: 0.0054 - val_loss: 0.0194
Epoch 58/512
384/384 - 0s - loss: 0.0041 - val_loss: 0.0307
Epoch 59/512
384/384 - 0s - loss: 0.0038 - val_loss: 0.0187
Epoch 60/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0224
Epoch 61/512
384/384 - 0s - loss: 0.0057 - val_loss: 0.0245
Epoch 62/512
384/384 - 0s - loss: 0.0078 - val_loss: 0.0107
Epoch 63/512
384/384 - 0s - loss: 0.0065 - val_loss: 0.0221
Epoch 64/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0189
Epoch 65/512
384/384 - 0s - loss: 0.0032 - val_loss: 0.0291
Epoch 66/512
384/384 - 0s - loss: 0.0080 - val_loss: 0.0158
Epoch 67/512
384/384 - 0s - loss: 0.0030 - val_loss: 0.0334
Epoch 68/512
384/384 - 0s - loss: 0.0051 - val_loss: 0.0083
Epoch 69/512
384/384 - 0s - loss: 0.0058 - val_loss: 0.0214
Epoch 70/512
384/384 - 0s - loss: 0.0023 - val_loss: 0.0188
Epoch 71/512
384/384 - 0s - loss: 0.0033 - val_loss: 0.0138
Epoch 72/512
384/384 - 0s - loss: 0.0072 - val_loss: 0.0243
Epoch 73/512
384/384 - 0s - loss: 0.0045 - val_loss: 0.0140
Epoch 74/512
384/384 - 0s - loss: 0.0058 - val_loss: 0.0177
Epoch 75/512
384/384 - 0s - loss: 0.0023 - val_loss: 0.0166
Epoch 76/512
384/384 - 0s - loss: 0.0021 - val_loss: 0.0169
Epoch 77/512
384/384 - 0s - loss: 0.0043 - val_loss: 0.0242
Epoch 78/512
384/384 - 0s - loss: 0.0064 - val_loss: 0.0109
Epoch 79/512
384/384 - 0s - loss: 0.0046 - val_loss: 0.0235
Epoch 80/512
384/384 - 0s - loss: 0.0030 - val_loss: 0.0067
Epoch 81/512
384/384 - 0s - loss: 0.0050 - val_loss: 0.0183
Epoch 82/512
384/384 - 0s - loss: 0.0023 - val_loss: 0.0176
Epoch 83/512
384/384 - 0s - loss: 0.0034 - val_loss: 0.0109
Epoch 84/512
384/384 - 0s - loss: 0.0073 - val_loss: 0.0252
Epoch 85/512
384/384 - 0s - loss: 0.0035 - val_loss: 0.0094
Epoch 86/512
384/384 - 0s - loss: 0.0035 - val_loss: 0.0199
Epoch 87/512
384/384 - 0s - loss: 0.0040 - val_loss: 0.0128
Epoch 88/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0241
Epoch 89/512
384/384 - 0s - loss: 0.0049 - val_loss: 0.0045
Epoch 90/512
384/384 - 0s - loss: 0.0064 - val_loss: 0.0154
Epoch 91/512
384/384 - 0s - loss: 0.0013 - val_loss: 0.0133
Epoch 92/512
384/384 - 0s - loss: 0.0015 - val_loss: 0.0220
Epoch 93/512
384/384 - 0s - loss: 0.0061 - val_loss: 0.0035
Epoch 94/512
384/384 - 0s - loss: 0.0070 - val_loss: 0.0132
Epoch 95/512
384/384 - 0s - loss: 0.0014 - val_loss: 0.0108
Epoch 96/512
384/384 - 0s - loss: 0.0041 - val_loss: 0.0196
Epoch 97/512
384/384 - 0s - loss: 0.0034 - val_loss: 0.0080
Epoch 98/512
384/384 - 0s - loss: 0.0049 - val_loss: 0.0155
Epoch 99/512
384/384 - 0s - loss: 0.0020 - val_loss: 0.0074
Epoch 100/512
384/384 - 0s - loss: 0.0032 - val_loss: 0.0163
Epoch 101/512
384/384 - 0s - loss: 0.0041 - val_loss: 0.0088
Epoch 102/512
384/384 - 0s - loss: 0.0021 - val_loss: 0.0189
Epoch 103/512
384/384 - 0s - loss: 0.0044 - val_loss: 0.0055
Epoch 104/512
384/384 - 0s - loss: 0.0035 - val_loss: 0.0138
Epoch 105/512
384/384 - 0s - loss: 0.0015 - val_loss: 0.0103
Epoch 106/512
384/384 - 0s - loss: 0.0034 - val_loss: 0.0078
Epoch 107/512
384/384 - 0s - loss: 0.0052 - val_loss: 0.0176
Epoch 108/512
384/384 - 0s - loss: 0.0026 - val_loss: 0.0071
Epoch 109/512
384/384 - 0s - loss: 0.0047 - val_loss: 0.0131
Epoch 110/512
384/384 - 0s - loss: 0.0014 - val_loss: 0.0077
Epoch 111/512
384/384 - 0s - loss: 0.0048 - val_loss: 0.0130
Epoch 112/512
384/384 - 0s - loss: 0.0016 - val_loss: 0.0084
Epoch 113/512
384/384 - 0s - loss: 0.0012 - val_loss: 0.0065
Epoch 114/512
384/384 - 0s - loss: 0.0047 - val_loss: 0.0157
Epoch 115/512
384/384 - 0s - loss: 0.0045 - val_loss: 0.0063
Epoch 116/512
384/384 - 0s - loss: 0.0037 - val_loss: 0.0103
Epoch 117/512
384/384 - 0s - loss: 9.5270e-04 - val_loss: 0.0066
Epoch 118/512
384/384 - 0s - loss: 0.0027 - val_loss: 0.0112
Epoch 119/512
384/384 - 0s - loss: 0.0034 - val_loss: 0.0037
Epoch 120/512
384/384 - 0s - loss: 0.0031 - val_loss: 0.0097
Epoch 121/512
384/384 - 0s - loss: 0.0027 - val_loss: 0.0043
Epoch 122/512
384/384 - 0s - loss: 0.0038 - val_loss: 0.0090
Epoch 123/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0034
Epoch 124/512
384/384 - 0s - loss: 0.0030 - val_loss: 0.0086
Epoch 125/512
384/384 - 0s - loss: 0.0029 - val_loss: 0.0041
Epoch 126/512
384/384 - 0s - loss: 0.0039 - val_loss: 0.0080
Epoch 127/512
384/384 - 0s - loss: 9.7869e-04 - val_loss: 0.0047
Epoch 128/512
384/384 - 0s - loss: 0.0022 - val_loss: 0.0077
Epoch 129/512
384/384 - 0s - loss: 0.0036 - val_loss: 0.0041
Epoch 130/512
384/384 - 0s - loss: 0.0035 - val_loss: 0.0064
Epoch 131/512
384/384 - 0s - loss: 9.3469e-04 - val_loss: 0.0039
Epoch 132/512
384/384 - 0s - loss: 0.0036 - val_loss: 0.0068
Epoch 133/512
384/384 - 0s - loss: 0.0013 - val_loss: 0.0035
Epoch 134/512
384/384 - 0s - loss: 0.0046 - val_loss: 0.0067
Epoch 135/512
384/384 - 0s - loss: 0.0011 - val_loss: 0.0040
Epoch 136/512
384/384 - 0s - loss: 0.0012 - val_loss: 0.0055
Epoch 137/512
384/384 - 0s - loss: 0.0053 - val_loss: 0.0031
Epoch 138/512
384/384 - 0s - loss: 6.9605e-04 - val_loss: 0.0045
Epoch 139/512
384/384 - 0s - loss: 0.0027 - val_loss: 0.0019
Epoch 140/512
384/384 - 0s - loss: 0.0032 - val_loss: 0.0045
Epoch 141/512
384/384 - 0s - loss: 0.0024 - val_loss: 0.0024
Epoch 142/512
384/384 - 0s - loss: 0.0025 - val_loss: 0.0038
Epoch 143/512
384/384 - 0s - loss: 0.0016 - val_loss: 0.0019
Epoch 144/512
384/384 - 0s - loss: 0.0024 - val_loss: 0.0038
Epoch 145/512
384/384 - 0s - loss: 0.0026 - val_loss: 0.0019
Epoch 146/512
384/384 - 0s - loss: 0.0024 - val_loss: 0.0033
Epoch 147/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0016
Epoch 148/512
384/384 - 0s - loss: 0.0022 - val_loss: 0.0031
Epoch 149/512
384/384 - 0s - loss: 0.0022 - val_loss: 0.0015
Epoch 150/512
384/384 - 0s - loss: 0.0024 - val_loss: 0.0026
Epoch 151/512
384/384 - 0s - loss: 0.0020 - val_loss: 0.0014
Epoch 152/512
384/384 - 0s - loss: 0.0022 - val_loss: 0.0022
Epoch 153/512
384/384 - 0s - loss: 0.0018 - val_loss: 0.0013
Epoch 154/512
384/384 - 0s - loss: 0.0021 - val_loss: 0.0023
Epoch 155/512
384/384 - 0s - loss: 0.0022 - val_loss: 9.6144e-04
Epoch 156/512
384/384 - 0s - loss: 0.0023 - val_loss: 0.0022
Epoch 157/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0012
Epoch 158/512
384/384 - 0s - loss: 0.0018 - val_loss: 0.0013
Epoch 159/512
384/384 - 0s - loss: 0.0014 - val_loss: 0.0012
Epoch 160/512
384/384 - 0s - loss: 0.0020 - val_loss: 0.0017
Epoch 161/512
384/384 - 0s - loss: 0.0024 - val_loss: 7.5508e-04
Epoch 162/512
384/384 - 0s - loss: 0.0022 - val_loss: 0.0015
Epoch 163/512
384/384 - 0s - loss: 0.0015 - val_loss: 9.7737e-04
Epoch 164/512
384/384 - 0s - loss: 0.0018 - val_loss: 0.0010
Epoch 165/512
384/384 - 0s - loss: 0.0014 - val_loss: 9.9675e-04
Epoch 166/512
384/384 - 0s - loss: 0.0019 - val_loss: 0.0012
Epoch 167/512
384/384 - 0s - loss: 0.0022 - val_loss: 6.1059e-04
Epoch 168/512
384/384 - 0s - loss: 0.0021 - val_loss: 0.0011
Epoch 169/512
384/384 - 0s - loss: 0.0015 - val_loss: 8.0147e-04
Epoch 170/512
384/384 - 0s - loss: 0.0017 - val_loss: 8.0637e-04
Epoch 171/512
384/384 - 0s - loss: 0.0014 - val_loss: 7.7795e-04
Epoch 172/512
384/384 - 0s - loss: 0.0017 - val_loss: 9.1664e-04
Epoch 173/512
384/384 - 0s - loss: 0.0020 - val_loss: 5.4207e-04
Epoch 174/512
384/384 - 0s - loss: 0.0020 - val_loss: 8.4446e-04
Epoch 175/512
384/384 - 0s - loss: 0.0015 - val_loss: 6.7160e-04
Epoch 176/512
384/384 - 0s - loss: 0.0015 - val_loss: 6.8030e-04
Epoch 177/512
384/384 - 0s - loss: 0.0014 - val_loss: 5.8410e-04
Epoch 178/512
384/384 - 0s - loss: 0.0018 - val_loss: 7.9514e-04
Epoch 179/512
384/384 - 0s - loss: 0.0018 - val_loss: 5.4391e-04
Epoch 180/512
384/384 - 0s - loss: 0.0016 - val_loss: 6.1157e-04
Epoch 181/512
384/384 - 0s - loss: 0.0013 - val_loss: 5.2919e-04
Epoch 182/512
384/384 - 0s - loss: 0.0015 - val_loss: 6.1110e-04
Epoch 183/512
384/384 - 0s - loss: 0.0016 - val_loss: 4.7827e-04
Epoch 184/512
384/384 - 0s - loss: 0.0015 - val_loss: 5.5799e-04
Epoch 185/512
384/384 - 0s - loss: 0.0015 - val_loss: 4.7128e-04
Epoch 186/512
384/384 - 0s - loss: 0.0014 - val_loss: 5.1883e-04
Epoch 187/512
384/384 - 0s - loss: 0.0014 - val_loss: 4.4741e-04
Epoch 188/512
384/384 - 0s - loss: 0.0015 - val_loss: 4.9220e-04
Epoch 189/512
384/384 - 0s - loss: 0.0014 - val_loss: 4.3326e-04
Epoch 190/512
384/384 - 0s - loss: 0.0014 - val_loss: 4.5202e-04
Epoch 191/512
384/384 - 0s - loss: 0.0013 - val_loss: 4.0632e-04
Epoch 192/512
384/384 - 0s - loss: 0.0014 - val_loss: 4.3628e-04
Epoch 193/512
384/384 - 0s - loss: 0.0014 - val_loss: 3.9636e-04
Epoch 194/512
384/384 - 0s - loss: 0.0014 - val_loss: 4.1350e-04
Epoch 195/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.8872e-04
Epoch 196/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.9631e-04
Epoch 197/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.7450e-04
Epoch 198/512
384/384 - 0s - loss: 0.0014 - val_loss: 3.7889e-04
Epoch 199/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5857e-04
Epoch 200/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.7461e-04
Epoch 201/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5696e-04
Epoch 202/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5754e-04
Epoch 203/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.4661e-04
Epoch 204/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.7828e-04
Epoch 205/512
384/384 - 0s - loss: 0.0014 - val_loss: 3.3769e-04
Epoch 206/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.3534e-04
Epoch 207/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.3346e-04
Epoch 208/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.6253e-04
Epoch 209/512
384/384 - 0s - loss: 0.0014 - val_loss: 3.1503e-04
Epoch 210/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5516e-04
Epoch 211/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.1280e-04
Epoch 212/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5654e-04
Epoch 213/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.2467e-04
Epoch 214/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5722e-04
Epoch 215/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.1125e-04
Epoch 216/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.5825e-04
Epoch 217/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.1539e-04
Epoch 218/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5093e-04
Epoch 219/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.1358e-04
Epoch 220/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5444e-04
Epoch 221/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.0087e-04
Epoch 222/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.4856e-04
Epoch 223/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.0774e-04
Epoch 224/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.4981e-04
Epoch 225/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.1283e-04
Epoch 226/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.5080e-04
Epoch 227/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.0765e-04
Epoch 228/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.4713e-04
Epoch 229/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.0260e-04
Epoch 230/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.4864e-04
Epoch 231/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.1299e-04
Epoch 232/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.5077e-04
Epoch 233/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.0730e-04
Epoch 234/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.4111e-04
Epoch 235/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.9999e-04
Epoch 236/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.4155e-04
Epoch 237/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.9795e-04
Epoch 238/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.3403e-04
Epoch 239/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.9223e-04
Epoch 240/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.3304e-04
Epoch 241/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.9357e-04
Epoch 242/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.3136e-04
Epoch 243/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.8705e-04
Epoch 244/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.2719e-04
Epoch 245/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.8225e-04
Epoch 246/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.2218e-04
Epoch 247/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.7997e-04
Epoch 248/512
384/384 - 0s - loss: 0.0013 - val_loss: 3.2216e-04
Epoch 249/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.7615e-04
Epoch 250/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.1143e-04
Epoch 251/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.7057e-04
Epoch 252/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.0656e-04
Epoch 253/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.6641e-04
Epoch 254/512
384/384 - 0s - loss: 0.0012 - val_loss: 3.0143e-04
Epoch 255/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.5967e-04
Epoch 256/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.9521e-04
Epoch 257/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.5327e-04
Epoch 258/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.9218e-04
Epoch 259/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.4791e-04
Epoch 260/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.8291e-04
Epoch 261/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.3965e-04
Epoch 262/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.7423e-04
Epoch 263/512
384/384 - 0s - loss: 0.0013 - val_loss: 2.3376e-04
Epoch 264/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.6752e-04
Epoch 265/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.2591e-04
Epoch 266/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.6146e-04
Epoch 267/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.1887e-04
Epoch 268/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.5000e-04
Epoch 269/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.0975e-04
Epoch 270/512
384/384 - 0s - loss: 0.0011 - val_loss: 2.4190e-04
Epoch 271/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.0173e-04
Epoch 272/512
384/384 - 0s - loss: 0.0012 - val_loss: 2.3262e-04
Epoch 273/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.9209e-04
Epoch 274/512
384/384 - 0s - loss: 0.0011 - val_loss: 2.2422e-04
Epoch 275/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.8240e-04
Epoch 276/512
384/384 - 0s - loss: 0.0011 - val_loss: 2.1149e-04
Epoch 277/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.7271e-04
Epoch 278/512
384/384 - 0s - loss: 0.0011 - val_loss: 2.0064e-04
Epoch 279/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.6223e-04
Epoch 280/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.8849e-04
Epoch 281/512
384/384 - 0s - loss: 0.0011 - val_loss: 1.5086e-04
Epoch 282/512
384/384 - 0s - loss: 0.0010 - val_loss: 1.7667e-04
Epoch 283/512
384/384 - 0s - loss: 0.0010 - val_loss: 1.4041e-04
Epoch 284/512
384/384 - 0s - loss: 9.8626e-04 - val_loss: 1.6598e-04
Epoch 285/512
384/384 - 0s - loss: 9.8190e-04 - val_loss: 1.2913e-04
Epoch 286/512
384/384 - 0s - loss: 9.4909e-04 - val_loss: 1.5363e-04
Epoch 287/512
384/384 - 0s - loss: 9.4038e-04 - val_loss: 1.1750e-04
Epoch 288/512
384/384 - 0s - loss: 8.9889e-04 - val_loss: 1.4031e-04
Epoch 289/512
384/384 - 0s - loss: 8.9174e-04 - val_loss: 1.0460e-04
Epoch 290/512
384/384 - 0s - loss: 8.5037e-04 - val_loss: 1.2794e-04
Epoch 291/512
384/384 - 0s - loss: 8.2759e-04 - val_loss: 9.2757e-05
Epoch 292/512
384/384 - 0s - loss: 7.6635e-04 - val_loss: 1.1690e-04
Epoch 293/512
384/384 - 0s - loss: 7.8299e-04 - val_loss: 8.0578e-05
Epoch 294/512
384/384 - 0s - loss: 7.6198e-04 - val_loss: 1.0832e-04
Epoch 295/512
384/384 - 0s - loss: 7.6882e-04 - val_loss: 7.0435e-05
Epoch 296/512
384/384 - 0s - loss: 7.5027e-04 - val_loss: 1.0292e-04
Epoch 297/512
384/384 - 0s - loss: 8.1893e-04 - val_loss: 6.3281e-05
Epoch 298/512
384/384 - 0s - loss: 8.1172e-04 - val_loss: 9.8206e-05
Epoch 299/512
384/384 - 0s - loss: 7.7962e-04 - val_loss: 6.1048e-05
Epoch 300/512
384/384 - 0s - loss: 7.4069e-04 - val_loss: 9.3551e-05
Epoch 301/512
384/384 - 0s - loss: 7.8637e-04 - val_loss: 5.9138e-05
Epoch 302/512
384/384 - 0s - loss: 7.5877e-04 - val_loss: 9.0841e-05
Epoch 303/512
384/384 - 0s - loss: 7.4568e-04 - val_loss: 5.8608e-05
Epoch 304/512
384/384 - 0s - loss: 7.1316e-04 - val_loss: 8.8405e-05
Epoch 305/512
384/384 - 0s - loss: 7.2804e-04 - val_loss: 5.8530e-05
Epoch 306/512
384/384 - 0s - loss: 7.0350e-04 - val_loss: 8.7356e-05
Epoch 307/512
384/384 - 0s - loss: 6.8859e-04 - val_loss: 5.9138e-05
Epoch 308/512
384/384 - 0s - loss: 6.7103e-04 - val_loss: 8.7126e-05
Epoch 309/512
384/384 - 0s - loss: 6.7245e-04 - val_loss: 6.0264e-05
Epoch 310/512
384/384 - 0s - loss: 6.4345e-04 - val_loss: 8.7395e-05
Epoch 311/512
384/384 - 0s - loss: 6.4013e-04 - val_loss: 6.1705e-05
Epoch 312/512
384/384 - 0s - loss: 6.2087e-04 - val_loss: 8.8689e-05
Epoch 313/512
384/384 - 0s - loss: 6.0823e-04 - val_loss: 6.3955e-05
Epoch 314/512
384/384 - 0s - loss: 5.7457e-04 - val_loss: 9.0369e-05
Epoch 315/512
384/384 - 0s - loss: 5.7866e-04 - val_loss: 6.6121e-05
Epoch 316/512
384/384 - 0s - loss: 5.6930e-04 - val_loss: 9.2750e-05
Epoch 317/512
384/384 - 0s - loss: 5.6175e-04 - val_loss: 6.8695e-05
Epoch 318/512
384/384 - 0s - loss: 5.2875e-04 - val_loss: 9.4556e-05
Epoch 319/512
384/384 - 0s - loss: 5.1794e-04 - val_loss: 7.1114e-05
Epoch 320/512
384/384 - 0s - loss: 5.1052e-04 - val_loss: 9.6788e-05
Epoch 321/512
384/384 - 0s - loss: 4.9830e-04 - val_loss: 7.3706e-05
Epoch 322/512
384/384 - 0s - loss: 4.7837e-04 - val_loss: 9.8258e-05
Epoch 323/512
384/384 - 0s - loss: 4.7131e-04 - val_loss: 7.5875e-05
Epoch 324/512
384/384 - 0s - loss: 4.4684e-04 - val_loss: 9.9190e-05
Epoch 325/512
384/384 - 0s - loss: 4.4135e-04 - val_loss: 7.7221e-05
Epoch 326/512
384/384 - 0s - loss: 4.3087e-04 - val_loss: 9.9670e-05
Epoch 327/512
384/384 - 0s - loss: 4.2304e-04 - val_loss: 7.8190e-05
Epoch 328/512
384/384 - 0s - loss: 4.0062e-04 - val_loss: 9.8408e-05
Epoch 329/512
384/384 - 0s - loss: 3.8643e-04 - val_loss: 7.7668e-05
Epoch 330/512
384/384 - 0s - loss: 3.7898e-04 - val_loss: 9.6210e-05
Epoch 331/512
384/384 - 0s - loss: 3.7144e-04 - val_loss: 7.6012e-05
Epoch 332/512
384/384 - 0s - loss: 3.5748e-04 - val_loss: 9.2311e-05
Epoch 333/512
384/384 - 0s - loss: 3.4030e-04 - val_loss: 7.3125e-05
Epoch 334/512
384/384 - 0s - loss: 3.3147e-04 - val_loss: 8.7204e-05
Epoch 335/512
384/384 - 0s - loss: 3.2983e-04 - val_loss: 6.8778e-05
Epoch 336/512
384/384 - 0s - loss: 3.1182e-04 - val_loss: 8.0340e-05
Epoch 337/512
384/384 - 0s - loss: 3.0745e-04 - val_loss: 6.3678e-05
Epoch 338/512
384/384 - 0s - loss: 2.8820e-04 - val_loss: 7.3277e-05
Epoch 339/512
384/384 - 0s - loss: 2.8630e-04 - val_loss: 5.7467e-05
Epoch 340/512
384/384 - 0s - loss: 2.7413e-04 - val_loss: 6.4467e-05
Epoch 341/512
384/384 - 0s - loss: 2.6698e-04 - val_loss: 5.0471e-05
Epoch 342/512
384/384 - 0s - loss: 2.5369e-04 - val_loss: 5.5574e-05
Epoch 343/512
384/384 - 0s - loss: 2.5291e-04 - val_loss: 4.3590e-05
Epoch 344/512
384/384 - 0s - loss: 2.4048e-04 - val_loss: 4.7197e-05
Epoch 345/512
384/384 - 0s - loss: 2.3516e-04 - val_loss: 3.7633e-05
Epoch 346/512
384/384 - 0s - loss: 2.2574e-04 - val_loss: 3.9307e-05
Epoch 347/512
384/384 - 0s - loss: 2.2089e-04 - val_loss: 3.1848e-05
Epoch 348/512
384/384 - 0s - loss: 2.1129e-04 - val_loss: 3.1913e-05
Epoch 349/512
384/384 - 0s - loss: 2.0578e-04 - val_loss: 2.7641e-05
Epoch 350/512
384/384 - 0s - loss: 1.9758e-04 - val_loss: 2.6081e-05
Epoch 351/512
384/384 - 0s - loss: 1.9588e-04 - val_loss: 2.5115e-05
Epoch 352/512
384/384 - 0s - loss: 1.8970e-04 - val_loss: 2.1931e-05
Epoch 353/512
384/384 - 0s - loss: 1.8559e-04 - val_loss: 2.4071e-05
Epoch 354/512
384/384 - 0s - loss: 1.7085e-04 - val_loss: 1.8435e-05
Epoch 355/512
384/384 - 0s - loss: 1.6783e-04 - val_loss: 2.5134e-05
Epoch 356/512
384/384 - 0s - loss: 1.7113e-04 - val_loss: 1.6828e-05
Epoch 357/512
384/384 - 0s - loss: 1.7141e-04 - val_loss: 2.7134e-05
Epoch 358/512
384/384 - 0s - loss: 1.5264e-04 - val_loss: 1.6746e-05
Epoch 359/512
384/384 - 0s - loss: 1.5233e-04 - val_loss: 3.1192e-05
Epoch 360/512
384/384 - 0s - loss: 1.5340e-04 - val_loss: 1.7294e-05
Epoch 361/512
384/384 - 0s - loss: 1.5056e-04 - val_loss: 3.5286e-05
Epoch 362/512
384/384 - 0s - loss: 1.4024e-04 - val_loss: 1.8857e-05
Epoch 363/512
384/384 - 0s - loss: 1.3939e-04 - val_loss: 4.1117e-05
Epoch 364/512
384/384 - 0s - loss: 1.3850e-04 - val_loss: 2.0661e-05
Epoch 365/512
384/384 - 0s - loss: 1.3978e-04 - val_loss: 4.5905e-05
Epoch 366/512
384/384 - 0s - loss: 1.3056e-04 - val_loss: 2.2890e-05
Epoch 367/512
384/384 - 0s - loss: 1.2739e-04 - val_loss: 5.1024e-05
Epoch 368/512
384/384 - 0s - loss: 1.2331e-04 - val_loss: 2.4660e-05
Epoch 369/512
384/384 - 0s - loss: 1.2912e-04 - val_loss: 5.5872e-05
Epoch 370/512
384/384 - 0s - loss: 1.2194e-04 - val_loss: 2.7150e-05
Epoch 371/512
384/384 - 0s - loss: 1.2004e-04 - val_loss: 5.9328e-05
Epoch 372/512
384/384 - 0s - loss: 1.1406e-04 - val_loss: 2.8876e-05
Epoch 373/512
384/384 - 0s - loss: 1.1614e-04 - val_loss: 6.2811e-05
Epoch 374/512
384/384 - 0s - loss: 1.1241e-04 - val_loss: 2.9776e-05
Epoch 375/512
384/384 - 0s - loss: 1.1420e-04 - val_loss: 6.3409e-05
Epoch 376/512
384/384 - 0s - loss: 1.0795e-04 - val_loss: 3.0428e-05
Epoch 377/512
384/384 - 0s - loss: 1.0725e-04 - val_loss: 6.5270e-05
Epoch 378/512
384/384 - 0s - loss: 1.0424e-04 - val_loss: 3.1149e-05
Epoch 379/512
384/384 - 0s - loss: 1.0664e-04 - val_loss: 6.3909e-05
Epoch 380/512
384/384 - 0s - loss: 1.0090e-04 - val_loss: 3.0708e-05
Epoch 381/512
384/384 - 0s - loss: 9.9221e-05 - val_loss: 6.2513e-05
Epoch 382/512
384/384 - 0s - loss: 9.5915e-05 - val_loss: 3.0785e-05
Epoch 383/512
384/384 - 0s - loss: 9.9225e-05 - val_loss: 6.1243e-05
Epoch 384/512
384/384 - 0s - loss: 9.4921e-05 - val_loss: 3.0663e-05
Epoch 385/512
384/384 - 0s - loss: 9.3736e-05 - val_loss: 5.8222e-05
Epoch 386/512
384/384 - 0s - loss: 8.9267e-05 - val_loss: 3.0168e-05
Epoch 387/512
384/384 - 0s - loss: 9.0536e-05 - val_loss: 5.4816e-05
Epoch 388/512
384/384 - 0s - loss: 8.8421e-05 - val_loss: 2.8863e-05
Epoch 389/512
384/384 - 0s - loss: 8.8923e-05 - val_loss: 5.0784e-05
Epoch 390/512
384/384 - 0s - loss: 8.2890e-05 - val_loss: 2.8858e-05
Epoch 391/512
384/384 - 0s - loss: 8.3884e-05 - val_loss: 4.7454e-05
Epoch 392/512
384/384 - 0s - loss: 8.2973e-05 - val_loss: 2.8123e-05
Epoch 393/512
384/384 - 0s - loss: 8.3251e-05 - val_loss: 4.2595e-05
Epoch 394/512
384/384 - 0s - loss: 7.6410e-05 - val_loss: 2.8228e-05
Epoch 395/512
384/384 - 0s - loss: 7.6388e-05 - val_loss: 4.0039e-05
Epoch 396/512
384/384 - 0s - loss: 7.5614e-05 - val_loss: 2.8608e-05
Epoch 397/512
384/384 - 0s - loss: 7.6558e-05 - val_loss: 3.6328e-05
Epoch 398/512
384/384 - 0s - loss: 7.2156e-05 - val_loss: 2.9259e-05
Epoch 399/512
384/384 - 0s - loss: 7.1052e-05 - val_loss: 3.3534e-05
Epoch 400/512
384/384 - 0s - loss: 6.7527e-05 - val_loss: 3.0499e-05
Epoch 401/512
384/384 - 0s - loss: 6.8786e-05 - val_loss: 3.1709e-05
Epoch 402/512
384/384 - 0s - loss: 6.7362e-05 - val_loss: 3.2445e-05
Epoch 403/512
384/384 - 0s - loss: 6.5779e-05 - val_loss: 2.9370e-05
Epoch 404/512
384/384 - 0s - loss: 6.1463e-05 - val_loss: 3.4276e-05
Epoch 405/512
384/384 - 0s - loss: 6.3078e-05 - val_loss: 2.8324e-05
Epoch 406/512
384/384 - 0s - loss: 6.1529e-05 - val_loss: 3.7461e-05
Epoch 407/512
384/384 - 0s - loss: 6.0139e-05 - val_loss: 2.7767e-05
Epoch 408/512
384/384 - 0s - loss: 5.6145e-05 - val_loss: 3.9840e-05
Epoch 409/512
384/384 - 0s - loss: 5.6509e-05 - val_loss: 2.7816e-05
Epoch 410/512
384/384 - 0s - loss: 5.5087e-05 - val_loss: 4.3931e-05
Epoch 411/512
384/384 - 0s - loss: 5.5156e-05 - val_loss: 2.8517e-05
Epoch 412/512
384/384 - 0s - loss: 5.1889e-05 - val_loss: 4.7333e-05
Epoch 413/512
384/384 - 0s - loss: 5.1221e-05 - val_loss: 2.9689e-05
Epoch 414/512
384/384 - 0s - loss: 4.8878e-05 - val_loss: 5.1498e-05
Epoch 415/512
384/384 - 0s - loss: 4.9954e-05 - val_loss: 3.1245e-05
Epoch 416/512
384/384 - 0s - loss: 4.7633e-05 - val_loss: 5.6015e-05
Epoch 417/512
384/384 - 0s - loss: 4.6124e-05 - val_loss: 3.3614e-05
Epoch 418/512
384/384 - 0s - loss: 4.4606e-05 - val_loss: 6.0680e-05
Epoch 419/512
384/384 - 0s - loss: 4.4617e-05 - val_loss: 3.6075e-05
Epoch 420/512
384/384 - 0s - loss: 4.1995e-05 - val_loss: 6.4877e-05
Epoch 421/512
384/384 - 0s - loss: 4.1999e-05 - val_loss: 3.8866e-05
Epoch 422/512
384/384 - 0s - loss: 4.1191e-05 - val_loss: 7.0742e-05
Epoch 423/512
384/384 - 0s - loss: 4.0442e-05 - val_loss: 4.2721e-05
Epoch 424/512
384/384 - 0s - loss: 3.7906e-05 - val_loss: 7.4661e-05
Epoch 425/512
384/384 - 0s - loss: 3.7719e-05 - val_loss: 4.6049e-05
Epoch 426/512
384/384 - 0s - loss: 3.6320e-05 - val_loss: 8.0645e-05
Epoch 427/512
384/384 - 0s - loss: 3.6258e-05 - val_loss: 4.9819e-05
Epoch 428/512
384/384 - 0s - loss: 3.4757e-05 - val_loss: 8.4844e-05
Epoch 429/512
384/384 - 0s - loss: 3.3566e-05 - val_loss: 5.3947e-05
Epoch 430/512
384/384 - 0s - loss: 3.2853e-05 - val_loss: 9.0675e-05
Epoch 431/512
384/384 - 0s - loss: 3.3117e-05 - val_loss: 5.8305e-05
Epoch 432/512
384/384 - 0s - loss: 3.1526e-05 - val_loss: 9.5957e-05
Epoch 433/512
384/384 - 0s - loss: 3.0127e-05 - val_loss: 6.4348e-05
Epoch 434/512
384/384 - 0s - loss: 2.8586e-05 - val_loss: 9.9782e-05
Epoch 435/512
384/384 - 0s - loss: 2.9201e-05 - val_loss: 6.7463e-05
Epoch 436/512
384/384 - 0s - loss: 2.8505e-05 - val_loss: 1.0711e-04
Epoch 437/512
384/384 - 0s - loss: 2.7474e-05 - val_loss: 7.4120e-05
Epoch 438/512
384/384 - 0s - loss: 2.5718e-05 - val_loss: 1.0970e-04
Epoch 439/512
384/384 - 0s - loss: 2.5104e-05 - val_loss: 7.8735e-05
Epoch 440/512
384/384 - 0s - loss: 2.4473e-05 - val_loss: 1.1573e-04
Epoch 441/512
384/384 - 0s - loss: 2.4431e-05 - val_loss: 8.3515e-05
Epoch 442/512
384/384 - 0s - loss: 2.3319e-05 - val_loss: 1.2002e-04
Epoch 443/512
384/384 - 0s - loss: 2.2001e-05 - val_loss: 8.9639e-05
Epoch 444/512
384/384 - 0s - loss: 2.1182e-05 - val_loss: 1.2392e-04
Epoch 445/512
384/384 - 0s - loss: 2.0975e-05 - val_loss: 9.4451e-05
Epoch 446/512
384/384 - 0s - loss: 1.9915e-05 - val_loss: 1.2894e-04
Epoch 447/512
384/384 - 0s - loss: 1.9042e-05 - val_loss: 1.0034e-04
Epoch 448/512
384/384 - 0s - loss: 1.8053e-05 - val_loss: 1.3062e-04
Epoch 449/512
384/384 - 0s - loss: 1.7537e-05 - val_loss: 1.0378e-04
Epoch 450/512
384/384 - 0s - loss: 1.6673e-05 - val_loss: 1.3396e-04
Epoch 451/512
384/384 - 0s - loss: 1.5882e-05 - val_loss: 1.0835e-04
Epoch 452/512
384/384 - 0s - loss: 1.4886e-05 - val_loss: 1.3597e-04
Epoch 453/512
384/384 - 0s - loss: 1.4382e-05 - val_loss: 1.1175e-04
Epoch 454/512
384/384 - 0s - loss: 1.3644e-05 - val_loss: 1.3858e-04
Epoch 455/512
384/384 - 0s - loss: 1.2925e-05 - val_loss: 1.1564e-04
Epoch 456/512
384/384 - 0s - loss: 1.2053e-05 - val_loss: 1.3959e-04
Epoch 457/512
384/384 - 0s - loss: 1.1409e-05 - val_loss: 1.1857e-04
Epoch 458/512
384/384 - 0s - loss: 1.0571e-05 - val_loss: 1.3962e-04
Epoch 459/512
384/384 - 0s - loss: 9.8931e-06 - val_loss: 1.2200e-04
Epoch 460/512
384/384 - 0s - loss: 9.3404e-06 - val_loss: 1.4251e-04
Epoch 461/512
384/384 - 0s - loss: 9.0347e-06 - val_loss: 1.2440e-04
Epoch 462/512
384/384 - 0s - loss: 8.2164e-06 - val_loss: 1.4048e-04
Epoch 463/512
384/384 - 0s - loss: 7.6224e-06 - val_loss: 1.2489e-04
Epoch 464/512
384/384 - 0s - loss: 7.1685e-06 - val_loss: 1.3795e-04
Epoch 465/512
384/384 - 0s - loss: 6.8808e-06 - val_loss: 1.2304e-04
Epoch 466/512
384/384 - 0s - loss: 6.4701e-06 - val_loss: 1.3391e-04
Epoch 467/512
384/384 - 0s - loss: 6.0715e-06 - val_loss: 1.2014e-04
Epoch 468/512
384/384 - 0s - loss: 5.6404e-06 - val_loss: 1.2907e-04
Epoch 469/512
384/384 - 0s - loss: 5.4096e-06 - val_loss: 1.1631e-04
Epoch 470/512
384/384 - 0s - loss: 5.1412e-06 - val_loss: 1.2423e-04
Epoch 471/512
384/384 - 0s - loss: 4.8762e-06 - val_loss: 1.1188e-04
Epoch 472/512
384/384 - 0s - loss: 4.5859e-06 - val_loss: 1.1949e-04
Epoch 473/512
384/384 - 0s - loss: 4.3951e-06 - val_loss: 1.0794e-04
Epoch 474/512
384/384 - 0s - loss: 4.1666e-06 - val_loss: 1.1462e-04
Epoch 475/512
384/384 - 0s - loss: 3.9950e-06 - val_loss: 1.0385e-04
Epoch 476/512
384/384 - 0s - loss: 3.7376e-06 - val_loss: 1.0967e-04
Epoch 477/512
384/384 - 0s - loss: 3.6109e-06 - val_loss: 9.9666e-05
Epoch 478/512
384/384 - 0s - loss: 3.4439e-06 - val_loss: 1.0536e-04
Epoch 479/512
384/384 - 0s - loss: 3.3182e-06 - val_loss: 9.5477e-05
Epoch 480/512
384/384 - 0s - loss: 3.0998e-06 - val_loss: 1.0019e-04
Epoch 481/512
384/384 - 0s - loss: 2.9901e-06 - val_loss: 9.1250e-05
Epoch 482/512
384/384 - 0s - loss: 2.8808e-06 - val_loss: 9.6357e-05
Epoch 483/512
384/384 - 0s - loss: 2.8300e-06 - val_loss: 8.7814e-05
Epoch 484/512
384/384 - 0s - loss: 2.6463e-06 - val_loss: 9.2028e-05
Epoch 485/512
384/384 - 0s - loss: 2.5117e-06 - val_loss: 8.3930e-05
Epoch 486/512
384/384 - 0s - loss: 2.4119e-06 - val_loss: 8.7876e-05
Epoch 487/512
384/384 - 0s - loss: 2.3769e-06 - val_loss: 8.0394e-05
Epoch 488/512
384/384 - 0s - loss: 2.2671e-06 - val_loss: 8.4314e-05
Epoch 489/512
384/384 - 0s - loss: 2.1778e-06 - val_loss: 7.7120e-05
Epoch 490/512
384/384 - 0s - loss: 2.0714e-06 - val_loss: 8.0603e-05
Epoch 491/512
384/384 - 0s - loss: 2.0125e-06 - val_loss: 7.3749e-05
Epoch 492/512
384/384 - 0s - loss: 1.9342e-06 - val_loss: 7.7212e-05
Epoch 493/512
384/384 - 0s - loss: 1.8931e-06 - val_loss: 7.0829e-05
Epoch 494/512
384/384 - 0s - loss: 1.8130e-06 - val_loss: 7.4115e-05
Epoch 495/512
384/384 - 0s - loss: 1.7498e-06 - val_loss: 6.7989e-05
Epoch 496/512
384/384 - 0s - loss: 1.6677e-06 - val_loss: 7.0948e-05
Epoch 497/512
384/384 - 0s - loss: 1.6340e-06 - val_loss: 6.5136e-05
Epoch 498/512
384/384 - 0s - loss: 1.5845e-06 - val_loss: 6.8283e-05
Epoch 499/512
384/384 - 0s - loss: 1.5428e-06 - val_loss: 6.2682e-05
Epoch 500/512
384/384 - 0s - loss: 1.4764e-06 - val_loss: 6.5562e-05
Epoch 501/512
384/384 - 0s - loss: 1.4288e-06 - val_loss: 6.0191e-05
Epoch 502/512
384/384 - 0s - loss: 1.3822e-06 - val_loss: 6.3001e-05
Epoch 503/512
384/384 - 0s - loss: 1.3661e-06 - val_loss: 5.7934e-05
Epoch 504/512
384/384 - 0s - loss: 1.3040e-06 - val_loss: 6.0520e-05
Epoch 505/512
384/384 - 0s - loss: 1.2620e-06 - val_loss: 5.5665e-05
Epoch 506/512
384/384 - 0s - loss: 1.2218e-06 - val_loss: 5.8309e-05
Epoch 507/512
384/384 - 0s - loss: 1.2074e-06 - val_loss: 5.3578e-05
Epoch 508/512
384/384 - 0s - loss: 1.1662e-06 - val_loss: 5.6232e-05
Epoch 509/512
384/384 - 0s - loss: 1.1368e-06 - val_loss: 5.1731e-05
Epoch 510/512
384/384 - 0s - loss: 1.0928e-06 - val_loss: 5.4203e-05
Epoch 511/512
384/384 - 0s - loss: 1.0769e-06 - val_loss: 4.9856e-05
Epoch 512/512
384/384 - 0s - loss: 1.0402e-06 - val_loss: 5.2333e-05
Train on 384 samples, validate on 384 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.2502e-06 - val_loss: 2.5338e-07
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0754e-07 - val_loss: 4.5943e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.9713e-09 - val_loss: 1.5830e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.5047e-09 - val_loss: 1.4889e-09
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7016e-09 - val_loss: 2.8963e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.1942e-09 - val_loss: 4.0941e-08
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4000e-07 - val_loss: 1.3057e-06
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2050e-06 - val_loss: 2.4519e-07
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1877e-07 - val_loss: 1.9578e-08
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5159e-08 - val_loss: 1.2147e-08
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5876e-08 - val_loss: 3.4323e-08
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.1937e-08 - val_loss: 3.3633e-07
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0427e-07 - val_loss: 5.2547e-07
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6305e-07 - val_loss: 1.0282e-07
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7994e-08 - val_loss: 3.5887e-08
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9117e-08 - val_loss: 6.1468e-08
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0145e-07 - val_loss: 2.3445e-07
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4674e-07 - val_loss: 4.6325e-07
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1031e-07 - val_loss: 1.1246e-07
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.8627e-08 - val_loss: 6.1301e-08
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2497e-08 - val_loss: 8.0413e-08
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1658e-07 - val_loss: 2.3118e-07
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6372e-07 - val_loss: 2.2828e-07
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9432e-07 - val_loss: 1.1857e-07
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.2828e-08 - val_loss: 6.6459e-08
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.3039e-08 - val_loss: 1.0165e-07
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2459e-07 - val_loss: 1.6427e-07
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8160e-07 - val_loss: 1.8285e-07
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4423e-07 - val_loss: 8.7141e-08
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.1097e-08 - val_loss: 7.6921e-08
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9236e-08 - val_loss: 9.0565e-08
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0849e-07 - val_loss: 1.4635e-07
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3893e-07 - val_loss: 1.0923e-07
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0160e-07 - val_loss: 8.5186e-08
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.5935e-08 - val_loss: 6.6040e-08
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.2510e-08 - val_loss: 9.1410e-08
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.5594e-08 - val_loss: 9.6019e-08
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.8630e-08 - val_loss: 9.6274e-08
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3557e-08 - val_loss: 6.3819e-08
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4041e-08 - val_loss: 6.7564e-08
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7544e-08 - val_loss: 6.8252e-08
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.4448e-08 - val_loss: 8.5128e-08
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9126e-08 - val_loss: 6.5173e-08
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4234e-08 - val_loss: 6.2132e-08
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7917e-08 - val_loss: 5.2157e-08
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5613e-08 - val_loss: 6.3841e-08
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2568e-08 - val_loss: 5.7846e-08
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9286e-08 - val_loss: 6.0280e-08
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5101e-08 - val_loss: 4.6273e-08
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7310e-08 - val_loss: 5.0241e-08
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8712e-08 - val_loss: 4.5749e-08
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8223e-08 - val_loss: 5.2572e-08
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9446e-08 - val_loss: 4.2807e-08
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3347e-08 - val_loss: 4.4235e-08
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.1582e-08 - val_loss: 3.7233e-08
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8878e-08 - val_loss: 4.2441e-08
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.0800e-08 - val_loss: 3.7084e-08
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8185e-08 - val_loss: 3.9849e-08
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7245e-08 - val_loss: 3.2515e-08
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3351e-08 - val_loss: 3.5166e-08
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3599e-08 - val_loss: 3.0631e-08
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1855e-08 - val_loss: 3.4133e-08
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.2355e-08 - val_loss: 2.8779e-08
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9473e-08 - val_loss: 3.0675e-08
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8989e-08 - val_loss: 2.5860e-08
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6697e-08 - val_loss: 2.8419e-08
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7145e-08 - val_loss: 2.4594e-08
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5374e-08 - val_loss: 2.6761e-08
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5345e-08 - val_loss: 2.2559e-08
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3144e-08 - val_loss: 2.4259e-08
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3066e-08 - val_loss: 2.0800e-08
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1468e-08 - val_loss: 2.2756e-08
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1691e-08 - val_loss: 1.9550e-08
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0087e-08 - val_loss: 2.1072e-08
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0008e-08 - val_loss: 1.7963e-08
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8456e-08 - val_loss: 1.9389e-08
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8481e-08 - val_loss: 1.6730e-08
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7221e-08 - val_loss: 1.8144e-08
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7264e-08 - val_loss: 1.5577e-08
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5992e-08 - val_loss: 1.6773e-08
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5976e-08 - val_loss: 1.4439e-08
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4832e-08 - val_loss: 1.5582e-08
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4863e-08 - val_loss: 1.3462e-08
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3817e-08 - val_loss: 1.4489e-08
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3828e-08 - val_loss: 1.2514e-08
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2830e-08 - val_loss: 1.3439e-08
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2828e-08 - val_loss: 1.1643e-08
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1955e-08 - val_loss: 1.2546e-08
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1981e-08 - val_loss: 1.0884e-08
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1164e-08 - val_loss: 1.1678e-08
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1152e-08 - val_loss: 1.0135e-08
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0377e-08 - val_loss: 1.0829e-08
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0348e-08 - val_loss: 9.4273e-09
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.6711e-09 - val_loss: 1.0137e-08
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.7103e-09 - val_loss: 8.8673e-09
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.0825e-09 - val_loss: 9.4769e-09
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.0582e-09 - val_loss: 8.2543e-09
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.4433e-09 - val_loss: 8.8007e-09
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.4306e-09 - val_loss: 7.7151e-09
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9016e-09 - val_loss: 8.2552e-09
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9172e-09 - val_loss: 7.2514e-09
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.4174e-09 - val_loss: 7.7257e-09
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.4104e-09 - val_loss: 6.7786e-09
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9238e-09 - val_loss: 7.2049e-09
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9126e-09 - val_loss: 6.3426e-09
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.4863e-09 - val_loss: 6.7677e-09
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5007e-09 - val_loss: 5.9787e-09
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1133e-09 - val_loss: 6.3590e-09
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1051e-09 - val_loss: 5.6091e-09
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7253e-09 - val_loss: 5.9458e-09
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7183e-09 - val_loss: 5.2628e-09
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3715e-09 - val_loss: 5.5854e-09
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3731e-09 - val_loss: 4.9596e-09
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0672e-09 - val_loss: 5.2723e-09
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0708e-09 - val_loss: 4.6771e-09
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7683e-09 - val_loss: 4.9444e-09
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.7595e-09 - val_loss: 4.4035e-09
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.4936e-09 - val_loss: 4.6582e-09
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.4813e-09 - val_loss: 4.1482e-09
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2287e-09 - val_loss: 4.3787e-09
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2171e-09 - val_loss: 3.9101e-09
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9848e-09 - val_loss: 4.1325e-09
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9856e-09 - val_loss: 3.7017e-09
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7712e-09 - val_loss: 3.9039e-09
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.7646e-09 - val_loss: 3.5001e-09
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5633e-09 - val_loss: 3.6874e-09
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5590e-09 - val_loss: 3.3120e-09
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3737e-09 - val_loss: 3.4899e-09
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3710e-09 - val_loss: 3.1392e-09
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1909e-09 - val_loss: 3.2919e-09
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1801e-09 - val_loss: 2.9647e-09
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0202e-09 - val_loss: 3.1219e-09
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0195e-09 - val_loss: 2.8228e-09
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8756e-09 - val_loss: 2.9704e-09
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8698e-09 - val_loss: 2.6758e-09
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7205e-09 - val_loss: 2.8049e-09
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7144e-09 - val_loss: 2.5361e-09
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5789e-09 - val_loss: 2.6622e-09
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5796e-09 - val_loss: 2.4179e-09
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4593e-09 - val_loss: 2.5380e-09
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4569e-09 - val_loss: 2.3012e-09
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3392e-09 - val_loss: 2.4114e-09
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3352e-09 - val_loss: 2.1856e-09
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2196e-09 - val_loss: 2.2851e-09
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2137e-09 - val_loss: 2.0783e-09
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1103e-09 - val_loss: 2.1772e-09
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1130e-09 - val_loss: 1.9901e-09
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0245e-09 - val_loss: 2.0837e-09
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0208e-09 - val_loss: 1.8979e-09
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9270e-09 - val_loss: 1.9848e-09
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9270e-09 - val_loss: 1.8136e-09
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8376e-09 - val_loss: 1.8849e-09
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8308e-09 - val_loss: 1.7248e-09
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7518e-09 - val_loss: 1.8018e-09
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7533e-09 - val_loss: 1.6525e-09
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6767e-09 - val_loss: 1.7267e-09
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6789e-09 - val_loss: 1.5870e-09
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6102e-09 - val_loss: 1.6545e-09
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6081e-09 - val_loss: 1.5169e-09
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5378e-09 - val_loss: 1.5764e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.5323e-09 - val_loss: 1.4466e-09
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4650e-09 - val_loss: 1.5054e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4677e-09 - val_loss: 1.3930e-09
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4124e-09 - val_loss: 1.4517e-09
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4116e-09 - val_loss: 1.3387e-09
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3570e-09 - val_loss: 1.3885e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.3502e-09 - val_loss: 1.2767e-09
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2928e-09 - val_loss: 1.3235e-09
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2895e-09 - val_loss: 1.2258e-09
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2432e-09 - val_loss: 1.2763e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2448e-09 - val_loss: 1.1815e-09
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1967e-09 - val_loss: 1.2267e-09
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1961e-09 - val_loss: 1.1393e-09
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1553e-09 - val_loss: 1.1828e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1512e-09 - val_loss: 1.0937e-09
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1068e-09 - val_loss: 1.1319e-09
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1016e-09 - val_loss: 1.0452e-09
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0574e-09 - val_loss: 1.0843e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0592e-09 - val_loss: 1.0127e-09
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0267e-09 - val_loss: 1.0526e-09
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0265e-09 - val_loss: 9.7819e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.9028e-10 - val_loss: 1.0132e-09
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.8875e-10 - val_loss: 9.4406e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.5716e-10 - val_loss: 9.7890e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.5341e-10 - val_loss: 9.0614e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.1610e-10 - val_loss: 9.3514e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.1272e-10 - val_loss: 8.7099e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.8135e-10 - val_loss: 9.0077e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.8137e-10 - val_loss: 8.4310e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.5312e-10 - val_loss: 8.7134e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.5333e-10 - val_loss: 8.1811e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.2832e-10 - val_loss: 8.4638e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.2707e-10 - val_loss: 7.8814e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.9537e-10 - val_loss: 8.1153e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.9533e-10 - val_loss: 7.6180e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6979e-10 - val_loss: 7.8600e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.6892e-10 - val_loss: 7.3546e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.4317e-10 - val_loss: 7.5878e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.4292e-10 - val_loss: 7.1119e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.1779e-10 - val_loss: 7.3146e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.1610e-10 - val_loss: 6.8833e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9600e-10 - val_loss: 7.1200e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.9829e-10 - val_loss: 6.6974e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7636e-10 - val_loss: 6.8837e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.7315e-10 - val_loss: 6.4509e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5201e-10 - val_loss: 6.6425e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.5030e-10 - val_loss: 6.2450e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3163e-10 - val_loss: 6.4522e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.3153e-10 - val_loss: 6.0692e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1469e-10 - val_loss: 6.2766e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.1612e-10 - val_loss: 5.9232e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9752e-10 - val_loss: 6.0844e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.9643e-10 - val_loss: 5.7184e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7700e-10 - val_loss: 5.8557e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.7338e-10 - val_loss: 5.5156e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5637e-10 - val_loss: 5.6668e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.5628e-10 - val_loss: 5.3606e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.4191e-10 - val_loss: 5.5310e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.4325e-10 - val_loss: 5.2504e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3052e-10 - val_loss: 5.4016e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.3074e-10 - val_loss: 5.1158e-10
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.1612e-10 - val_loss: 5.2480e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.1463e-10 - val_loss: 4.9340e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9758e-10 - val_loss: 5.0637e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.9789e-10 - val_loss: 4.8069e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8466e-10 - val_loss: 4.9143e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.8255e-10 - val_loss: 4.6482e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.6884e-10 - val_loss: 4.7742e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.6947e-10 - val_loss: 4.5518e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.5808e-10 - val_loss: 4.6443e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.5625e-10 - val_loss: 4.3982e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.4441e-10 - val_loss: 4.5283e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.4578e-10 - val_loss: 4.3127e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.3435e-10 - val_loss: 4.4048e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.3304e-10 - val_loss: 4.1830e-10
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.2216e-10 - val_loss: 4.2944e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.2179e-10 - val_loss: 4.0675e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.1073e-10 - val_loss: 4.1677e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.0883e-10 - val_loss: 3.9507e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9905e-10 - val_loss: 4.0668e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.0032e-10 - val_loss: 3.8833e-10
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.9165e-10 - val_loss: 3.9936e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.9353e-10 - val_loss: 3.8031e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.8328e-10 - val_loss: 3.8853e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.8115e-10 - val_loss: 3.6779e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.6932e-10 - val_loss: 3.7438e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.6925e-10 - val_loss: 3.5638e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5905e-10 - val_loss: 3.6456e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.5926e-10 - val_loss: 3.4837e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.5105e-10 - val_loss: 3.5710e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.5113e-10 - val_loss: 3.4081e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.4394e-10 - val_loss: 3.5006e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.4483e-10 - val_loss: 3.3502e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3742e-10 - val_loss: 3.4226e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.3720e-10 - val_loss: 3.2789e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.3038e-10 - val_loss: 3.3468e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.2821e-10 - val_loss: 3.1700e-10
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1945e-10 - val_loss: 3.2335e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.1802e-10 - val_loss: 3.0907e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.1184e-10 - val_loss: 3.1654e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.1168e-10 - val_loss: 3.0242e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
384/384 - 0s - loss: 3.0518e-10 - val_loss: 3.0937e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 3.0511e-10 - val_loss: 2.9663e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9845e-10 - val_loss: 3.0297e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.9896e-10 - val_loss: 2.9161e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.9392e-10 - val_loss: 2.9764e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.9301e-10 - val_loss: 2.8398e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.8670e-10 - val_loss: 2.9063e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.8597e-10 - val_loss: 2.7763e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7942e-10 - val_loss: 2.8237e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.7770e-10 - val_loss: 2.6982e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.7255e-10 - val_loss: 2.7649e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.7303e-10 - val_loss: 2.6513e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6636e-10 - val_loss: 2.7048e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.6724e-10 - val_loss: 2.5949e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.6130e-10 - val_loss: 2.6553e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.6210e-10 - val_loss: 2.5369e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5506e-10 - val_loss: 2.5941e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.5647e-10 - val_loss: 2.4930e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.5114e-10 - val_loss: 2.5428e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.5077e-10 - val_loss: 2.4321e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.4475e-10 - val_loss: 2.4789e-10
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.4468e-10 - val_loss: 2.3861e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3981e-10 - val_loss: 2.4236e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.3909e-10 - val_loss: 2.3372e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3524e-10 - val_loss: 2.3830e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.3486e-10 - val_loss: 2.2851e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.3002e-10 - val_loss: 2.3329e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.2988e-10 - val_loss: 2.2345e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2512e-10 - val_loss: 2.2802e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.2507e-10 - val_loss: 2.1978e-10
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.2141e-10 - val_loss: 2.2415e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.2124e-10 - val_loss: 2.1521e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1754e-10 - val_loss: 2.2177e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.1925e-10 - val_loss: 2.1324e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.1381e-10 - val_loss: 2.1590e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.1274e-10 - val_loss: 2.0641e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0756e-10 - val_loss: 2.1067e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.0749e-10 - val_loss: 2.0125e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
384/384 - 0s - loss: 2.0307e-10 - val_loss: 2.0590e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 2.0341e-10 - val_loss: 1.9806e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9882e-10 - val_loss: 2.0158e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.9902e-10 - val_loss: 1.9426e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9555e-10 - val_loss: 1.9821e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.9567e-10 - val_loss: 1.9094e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.9228e-10 - val_loss: 1.9485e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.9221e-10 - val_loss: 1.8788e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8889e-10 - val_loss: 1.9194e-10
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.8961e-10 - val_loss: 1.8523e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8630e-10 - val_loss: 1.8826e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.8588e-10 - val_loss: 1.8165e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.8262e-10 - val_loss: 1.8489e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.8245e-10 - val_loss: 1.7738e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7821e-10 - val_loss: 1.7934e-10
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.7732e-10 - val_loss: 1.7344e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7423e-10 - val_loss: 1.7666e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.7510e-10 - val_loss: 1.7266e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.7399e-10 - val_loss: 1.7608e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.7397e-10 - val_loss: 1.6915e-10
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6990e-10 - val_loss: 1.7121e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.6933e-10 - val_loss: 1.6499e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6497e-10 - val_loss: 1.6630e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.6490e-10 - val_loss: 1.6149e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6266e-10 - val_loss: 1.6502e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.6325e-10 - val_loss: 1.5996e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.6119e-10 - val_loss: 1.6298e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.6086e-10 - val_loss: 1.5642e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5697e-10 - val_loss: 1.5896e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.5799e-10 - val_loss: 1.5561e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5666e-10 - val_loss: 1.5858e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.5717e-10 - val_loss: 1.5347e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5434e-10 - val_loss: 1.5654e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.5455e-10 - val_loss: 1.4985e-10
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.5035e-10 - val_loss: 1.5101e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4928e-10 - val_loss: 1.4629e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4686e-10 - val_loss: 1.4780e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4622e-10 - val_loss: 1.4343e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4424e-10 - val_loss: 1.4655e-10
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4492e-10 - val_loss: 1.4194e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4301e-10 - val_loss: 1.4508e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4337e-10 - val_loss: 1.3966e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.4028e-10 - val_loss: 1.4193e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.4083e-10 - val_loss: 1.3751e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3797e-10 - val_loss: 1.3946e-10
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.3818e-10 - val_loss: 1.3504e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3527e-10 - val_loss: 1.3618e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.3492e-10 - val_loss: 1.3233e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3306e-10 - val_loss: 1.3459e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.3288e-10 - val_loss: 1.3040e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.3151e-10 - val_loss: 1.3277e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.3101e-10 - val_loss: 1.2794e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2897e-10 - val_loss: 1.3084e-10
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2902e-10 - val_loss: 1.2611e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2703e-10 - val_loss: 1.2852e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2743e-10 - val_loss: 1.2514e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2564e-10 - val_loss: 1.2599e-10
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2464e-10 - val_loss: 1.2273e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2341e-10 - val_loss: 1.2506e-10
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2406e-10 - val_loss: 1.2132e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2205e-10 - val_loss: 1.2323e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.2186e-10 - val_loss: 1.1925e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1972e-10 - val_loss: 1.2116e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.2073e-10 - val_loss: 1.1942e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1975e-10 - val_loss: 1.2070e-10
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1925e-10 - val_loss: 1.1653e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1690e-10 - val_loss: 1.1766e-10
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1651e-10 - val_loss: 1.1428e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1521e-10 - val_loss: 1.1591e-10
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1473e-10 - val_loss: 1.1247e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1279e-10 - val_loss: 1.1358e-10
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1265e-10 - val_loss: 1.1051e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.1061e-10 - val_loss: 1.1151e-10
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.1019e-10 - val_loss: 1.0753e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0820e-10 - val_loss: 1.0945e-10
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0895e-10 - val_loss: 1.0737e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0813e-10 - val_loss: 1.0909e-10
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0772e-10 - val_loss: 1.0512e-10
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0556e-10 - val_loss: 1.0598e-10
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0484e-10 - val_loss: 1.0232e-10
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0317e-10 - val_loss: 1.0484e-10
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0404e-10 - val_loss: 1.0178e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0249e-10 - val_loss: 1.0377e-10
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0314e-10 - val_loss: 1.0131e-10
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0152e-10 - val_loss: 1.0215e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0157e-10 - val_loss: 1.0011e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0110e-10 - val_loss: 1.0281e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0188e-10 - val_loss: 1.0021e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0086e-10 - val_loss: 1.0183e-10
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 1.0119e-10 - val_loss: 9.9682e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
384/384 - 0s - loss: 1.0008e-10 - val_loss: 1.0010e-10
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.8699e-11 - val_loss: 9.6645e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.6865e-11 - val_loss: 9.6951e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.5968e-11 - val_loss: 9.4648e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.5092e-11 - val_loss: 9.5932e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.4731e-11 - val_loss: 9.2199e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.2510e-11 - val_loss: 9.3406e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.2510e-11 - val_loss: 9.1184e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.1529e-11 - val_loss: 9.2706e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.2502e-11 - val_loss: 9.1548e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.2349e-11 - val_loss: 9.3538e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.2248e-11 - val_loss: 9.0197e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
384/384 - 0s - loss: 9.0498e-11 - val_loss: 9.0948e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 9.0721e-11 - val_loss: 8.9389e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.9571e-11 - val_loss: 9.0000e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.9584e-11 - val_loss: 8.8977e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.9453e-11 - val_loss: 8.9169e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.7647e-11 - val_loss: 8.5555e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.5676e-11 - val_loss: 8.6092e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.5079e-11 - val_loss: 8.3491e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3670e-11 - val_loss: 8.4133e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.3737e-11 - val_loss: 8.3231e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3598e-11 - val_loss: 8.4739e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.4326e-11 - val_loss: 8.3012e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3438e-11 - val_loss: 8.4076e-11
Epoch 415/512

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.3505e-11 - val_loss: 8.2726e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.3444e-11 - val_loss: 8.4978e-11
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.4106e-11 - val_loss: 8.2429e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.2745e-11 - val_loss: 8.3613e-11
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.2453e-11 - val_loss: 8.0193e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
384/384 - 0s - loss: 8.0724e-11 - val_loss: 8.1227e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 8.0402e-11 - val_loss: 7.8526e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.8744e-11 - val_loss: 7.8680e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.8259e-11 - val_loss: 7.7187e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.7620e-11 - val_loss: 7.7988e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.7459e-11 - val_loss: 7.6190e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6407e-11 - val_loss: 7.6590e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.6080e-11 - val_loss: 7.5416e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.5923e-11 - val_loss: 7.6778e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6790e-11 - val_loss: 7.6207e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6532e-11 - val_loss: 7.6923e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6659e-11 - val_loss: 7.5747e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.6005e-11 - val_loss: 7.5871e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.5099e-11 - val_loss: 7.3684e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.3835e-11 - val_loss: 7.4209e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.3353e-11 - val_loss: 7.1843e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.2279e-11 - val_loss: 7.2870e-11
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.2032e-11 - val_loss: 7.0629e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.1056e-11 - val_loss: 7.1551e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 7.0843e-11 - val_loss: 6.9492e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9775e-11 - val_loss: 7.0708e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
384/384 - 0s - loss: 7.0702e-11 - val_loss: 6.9557e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9685e-11 - val_loss: 7.0034e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.9089e-11 - val_loss: 6.7739e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7918e-11 - val_loss: 6.8274e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.7878e-11 - val_loss: 6.7381e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7778e-11 - val_loss: 6.8689e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.8545e-11 - val_loss: 6.7915e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.8698e-11 - val_loss: 6.9885e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.9240e-11 - val_loss: 6.7864e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7872e-11 - val_loss: 6.8431e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.8073e-11 - val_loss: 6.6897e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.7380e-11 - val_loss: 6.7981e-11
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.7279e-11 - val_loss: 6.5668e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.5813e-11 - val_loss: 6.6060e-11
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.5995e-11 - val_loss: 6.5539e-11
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.5637e-11 - val_loss: 6.5419e-11
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.4606e-11 - val_loss: 6.3399e-11
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.3195e-11 - val_loss: 6.3233e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.2809e-11 - val_loss: 6.1743e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2366e-11 - val_loss: 6.3196e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.3004e-11 - val_loss: 6.2178e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.2789e-11 - val_loss: 6.3315e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.2573e-11 - val_loss: 6.1520e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.1825e-11 - val_loss: 6.2472e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 6.1693e-11 - val_loss: 6.0341e-11
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.9893e-11 - val_loss: 5.9702e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.9375e-11 - val_loss: 5.8631e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9029e-11 - val_loss: 6.0252e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0136e-11 - val_loss: 5.9827e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0462e-11 - val_loss: 6.1287e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
384/384 - 0s - loss: 6.0535e-11 - val_loss: 5.9299e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.9457e-11 - val_loss: 6.0049e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.9595e-11 - val_loss: 5.8594e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.8484e-11 - val_loss: 5.8813e-11
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.8331e-11 - val_loss: 5.7007e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.7211e-11 - val_loss: 5.7943e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.7716e-11 - val_loss: 5.6925e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6955e-11 - val_loss: 5.6932e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.6420e-11 - val_loss: 5.5762e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6095e-11 - val_loss: 5.6489e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.6299e-11 - val_loss: 5.5635e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.6028e-11 - val_loss: 5.6157e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.5865e-11 - val_loss: 5.5170e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.5764e-11 - val_loss: 5.6536e-11
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.5996e-11 - val_loss: 5.4864e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.4859e-11 - val_loss: 5.4989e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.4911e-11 - val_loss: 5.4127e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.4243e-11 - val_loss: 5.4379e-11
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.4232e-11 - val_loss: 5.4124e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.4448e-11 - val_loss: 5.4796e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.4262e-11 - val_loss: 5.3457e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3608e-11 - val_loss: 5.4095e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.3659e-11 - val_loss: 5.2782e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3010e-11 - val_loss: 5.3851e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3814e-11 - val_loss: 5.3642e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.3768e-11 - val_loss: 5.3834e-11
Epoch 497/512

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.3654e-11 - val_loss: 5.2665e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2482e-11 - val_loss: 5.2839e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.2450e-11 - val_loss: 5.2113e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.2338e-11 - val_loss: 5.2682e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.2176e-11 - val_loss: 5.1029e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0942e-11 - val_loss: 5.1083e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.0604e-11 - val_loss: 5.0211e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0611e-11 - val_loss: 5.1092e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.0697e-11 - val_loss: 4.9867e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
384/384 - 0s - loss: 5.0129e-11 - val_loss: 5.0574e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 5.0174e-11 - val_loss: 4.9140e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.9415e-11 - val_loss: 4.9643e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.9311e-11 - val_loss: 4.8697e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8669e-11 - val_loss: 4.8798e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr-00005-384b-6/multiplication_multiplication_weights.h5
384/384 - 0s - loss: 4.8525e-11 - val_loss: 4.8123e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
384/384 - 0s - loss: 4.8651e-11 - val_loss: 4.9104e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.189 | eve: 9.376 | bob: 9.113Epoch   0:   0% | abe: 9.180 | eve: 9.374 | bob: 9.108Epoch   0:   1% | abe: 9.158 | eve: 9.373 | bob: 9.089Epoch   0:   1% | abe: 9.148 | eve: 9.396 | bob: 9.082Epoch   0:   2% | abe: 9.130 | eve: 9.392 | bob: 9.067Epoch   0:   2% | abe: 9.123 | eve: 9.399 | bob: 9.063Epoch   0:   3% | abe: 9.125 | eve: 9.393 | bob: 9.067Epoch   0:   4% | abe: 9.116 | eve: 9.391 | bob: 9.059Epoch   0:   4% | abe: 9.114 | eve: 9.392 | bob: 9.059Epoch   0:   5% | abe: 9.120 | eve: 9.394 | bob: 9.066Epoch   0:   5% | abe: 9.115 | eve: 9.403 | bob: 9.063Epoch   0:   6% | abe: 9.117 | eve: 9.398 | bob: 9.065Epoch   0:   7% | abe: 9.111 | eve: 9.396 | bob: 9.061Epoch   0:   7% | abe: 9.105 | eve: 9.395 | bob: 9.055Epoch   0:   8% | abe: 9.107 | eve: 9.392 | bob: 9.057Epoch   0:   8% | abe: 9.100 | eve: 9.394 | bob: 9.050Epoch   0:   9% | abe: 9.101 | eve: 9.394 | bob: 9.052Epoch   0:  10% | abe: 9.097 | eve: 9.395 | bob: 9.048Epoch   0:  10% | abe: 9.097 | eve: 9.390 | bob: 9.048Epoch   0:  11% | abe: 9.096 | eve: 9.392 | bob: 9.047Epoch   0:  11% | abe: 9.093 | eve: 9.392 | bob: 9.045Epoch   0:  12% | abe: 9.091 | eve: 9.389 | bob: 9.043Epoch   0:  12% | abe: 9.087 | eve: 9.391 | bob: 9.039Epoch   0:  13% | abe: 9.089 | eve: 9.395 | bob: 9.040Epoch   0:  14% | abe: 9.089 | eve: 9.398 | bob: 9.040Epoch   0:  14% | abe: 9.088 | eve: 9.398 | bob: 9.040Epoch   0:  15% | abe: 9.085 | eve: 9.397 | bob: 9.037Epoch   0:  15% | abe: 9.083 | eve: 9.395 | bob: 9.035Epoch   0:  16% | abe: 9.083 | eve: 9.394 | bob: 9.035Epoch   0:  17% | abe: 9.083 | eve: 9.392 | bob: 9.035Epoch   0:  17% | abe: 9.082 | eve: 9.391 | bob: 9.034Epoch   0:  18% | abe: 9.082 | eve: 9.389 | bob: 9.034Epoch   0:  18% | abe: 9.083 | eve: 9.390 | bob: 9.035Epoch   0:  19% | abe: 9.084 | eve: 9.392 | bob: 9.036Epoch   0:  20% | abe: 9.083 | eve: 9.393 | bob: 9.034Epoch   0:  20% | abe: 9.081 | eve: 9.393 | bob: 9.032Epoch   0:  21% | abe: 9.080 | eve: 9.394 | bob: 9.031Epoch   0:  21% | abe: 9.082 | eve: 9.394 | bob: 9.034Epoch   0:  22% | abe: 9.082 | eve: 9.394 | bob: 9.033Epoch   0:  22% | abe: 9.083 | eve: 9.395 | bob: 9.034Epoch   0:  23% | abe: 9.083 | eve: 9.393 | bob: 9.034Epoch   0:  24% | abe: 9.083 | eve: 9.395 | bob: 9.034Epoch   0:  24% | abe: 9.083 | eve: 9.394 | bob: 9.034Epoch   0:  25% | abe: 9.083 | eve: 9.392 | bob: 9.034Epoch   0:  25% | abe: 9.083 | eve: 9.393 | bob: 9.034Epoch   0:  26% | abe: 9.082 | eve: 9.394 | bob: 9.033Epoch   0:  27% | abe: 9.082 | eve: 9.395 | bob: 9.034Epoch   0:  27% | abe: 9.080 | eve: 9.396 | bob: 9.031Epoch   0:  28% | abe: 9.079 | eve: 9.396 | bob: 9.030Epoch   0:  28% | abe: 9.078 | eve: 9.395 | bob: 9.029Epoch   0:  29% | abe: 9.078 | eve: 9.392 | bob: 9.029Epoch   0:  30% | abe: 9.077 | eve: 9.391 | bob: 9.028Epoch   0:  30% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  31% | abe: 9.078 | eve: 9.389 | bob: 9.029Epoch   0:  31% | abe: 9.077 | eve: 9.390 | bob: 9.028Epoch   0:  32% | abe: 9.078 | eve: 9.392 | bob: 9.029Epoch   0:  32% | abe: 9.078 | eve: 9.392 | bob: 9.029Epoch   0:  33% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  34% | abe: 9.078 | eve: 9.393 | bob: 9.028Epoch   0:  34% | abe: 9.078 | eve: 9.392 | bob: 9.029Epoch   0:  35% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  35% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  36% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  37% | abe: 9.077 | eve: 9.391 | bob: 9.028Epoch   0:  37% | abe: 9.077 | eve: 9.392 | bob: 9.028Epoch   0:  38% | abe: 9.079 | eve: 9.394 | bob: 9.030Epoch   0:  38% | abe: 9.079 | eve: 9.394 | bob: 9.030Epoch   0:  39% | abe: 9.080 | eve: 9.394 | bob: 9.030Epoch   0:  40% | abe: 9.079 | eve: 9.394 | bob: 9.030Epoch   0:  40% | abe: 9.079 | eve: 9.395 | bob: 9.030Epoch   0:  41% | abe: 9.079 | eve: 9.395 | bob: 9.030Epoch   0:  41% | abe: 9.080 | eve: 9.395 | bob: 9.031Epoch   0:  42% | abe: 9.081 | eve: 9.395 | bob: 9.032Epoch   0:  42% | abe: 9.080 | eve: 9.394 | bob: 9.030Epoch   0:  43% | abe: 9.080 | eve: 9.395 | bob: 9.030Epoch   0:  44% | abe: 9.078 | eve: 9.396 | bob: 9.029Epoch   0:  44% | abe: 9.079 | eve: 9.395 | bob: 9.030Epoch   0:  45% | abe: 9.079 | eve: 9.395 | bob: 9.029Epoch   0:  45% | abe: 9.078 | eve: 9.395 | bob: 9.029Epoch   0:  46% | abe: 9.078 | eve: 9.395 | bob: 9.028Epoch   0:  47% | abe: 9.077 | eve: 9.396 | bob: 9.027Epoch   0:  47% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  48% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  48% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  49% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  50% | abe: 9.077 | eve: 9.396 | bob: 9.027Epoch   0:  50% | abe: 9.077 | eve: 9.397 | bob: 9.027Epoch   0:  51% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  51% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  52% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  52% | abe: 9.075 | eve: 9.396 | bob: 9.025Epoch   0:  53% | abe: 9.075 | eve: 9.396 | bob: 9.025Epoch   0:  54% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  54% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  55% | abe: 9.076 | eve: 9.396 | bob: 9.026Epoch   0:  55% | abe: 9.076 | eve: 9.395 | bob: 9.026Epoch   0:  56% | abe: 9.076 | eve: 9.395 | bob: 9.026Epoch   0:  57% | abe: 9.076 | eve: 9.395 | bob: 9.026Epoch   0:  57% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  58% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  58% | abe: 9.077 | eve: 9.395 | bob: 9.027Epoch   0:  59% | abe: 9.078 | eve: 9.396 | bob: 9.027Epoch   0:  60% | abe: 9.078 | eve: 9.397 | bob: 9.028Epoch   0:  60% | abe: 9.078 | eve: 9.397 | bob: 9.028Epoch   0:  61% | abe: 9.078 | eve: 9.397 | bob: 9.028Epoch   0:  61% | abe: 9.078 | eve: 9.397 | bob: 9.027Epoch   0:  62% | abe: 9.078 | eve: 9.397 | bob: 9.028Epoch   0:  62% | abe: 9.079 | eve: 9.398 | bob: 9.029Epoch   0:  63% | abe: 9.079 | eve: 9.398 | bob: 9.029Epoch   0:  64% | abe: 9.079 | eve: 9.398 | bob: 9.028Epoch   0:  64% | abe: 9.079 | eve: 9.397 | bob: 9.028Epoch   0:  65% | abe: 9.079 | eve: 9.397 | bob: 9.029Epoch   0:  65% | abe: 9.078 | eve: 9.397 | bob: 9.028Epoch   0:  66% | abe: 9.078 | eve: 9.397 | bob: 9.027Epoch   0:  67% | abe: 9.078 | eve: 9.398 | bob: 9.028Epoch   0:  67% | abe: 9.078 | eve: 9.398 | bob: 9.027Epoch   0:  68% | abe: 9.079 | eve: 9.398 | bob: 9.028Epoch   0:  68% | abe: 9.078 | eve: 9.398 | bob: 9.028Epoch   0:  69% | abe: 9.078 | eve: 9.399 | bob: 9.028Epoch   0:  70% | abe: 9.078 | eve: 9.399 | bob: 9.028Epoch   0:  70% | abe: 9.079 | eve: 9.399 | bob: 9.028Epoch   0:  71% | abe: 9.079 | eve: 9.399 | bob: 9.028Epoch   0:  71% | abe: 9.078 | eve: 9.400 | bob: 9.028Epoch   0:  72% | abe: 9.078 | eve: 9.401 | bob: 9.028Epoch   0:  72% | abe: 9.078 | eve: 9.402 | bob: 9.028Epoch   0:  73% | abe: 9.078 | eve: 9.402 | bob: 9.028Epoch   0:  74% | abe: 9.078 | eve: 9.401 | bob: 9.028Epoch   0:  74% | abe: 9.078 | eve: 9.402 | bob: 9.028Epoch   0:  75% | abe: 9.078 | eve: 9.402 | bob: 9.028Epoch   0:  75% | abe: 9.079 | eve: 9.402 | bob: 9.028Epoch   0:  76% | abe: 9.079 | eve: 9.402 | bob: 9.028Epoch   0:  77% | abe: 9.078 | eve: 9.403 | bob: 9.028Epoch   0:  77% | abe: 9.078 | eve: 9.403 | bob: 9.028Epoch   0:  78% | abe: 9.078 | eve: 9.403 | bob: 9.027Epoch   0:  78% | abe: 9.078 | eve: 9.404 | bob: 9.028Epoch   0:  79% | abe: 9.078 | eve: 9.404 | bob: 9.027Epoch   0:  80% | abe: 9.077 | eve: 9.404 | bob: 9.027Epoch   0:  80% | abe: 9.077 | eve: 9.404 | bob: 9.027Epoch   0:  81% | abe: 9.077 | eve: 9.404 | bob: 9.026Epoch   0:  81% | abe: 9.076 | eve: 9.404 | bob: 9.026Epoch   0:  82% | abe: 9.076 | eve: 9.404 | bob: 9.026Epoch   0:  82% | abe: 9.077 | eve: 9.405 | bob: 9.027Epoch   0:  83% | abe: 9.077 | eve: 9.405 | bob: 9.027Epoch   0:  84% | abe: 9.077 | eve: 9.405 | bob: 9.027Epoch   0:  84% | abe: 9.077 | eve: 9.405 | bob: 9.027Epoch   0:  85% | abe: 9.077 | eve: 9.406 | bob: 9.026Epoch   0:  85% | abe: 9.077 | eve: 9.406 | bob: 9.027Epoch   0:  86% | abe: 9.077 | eve: 9.406 | bob: 9.027Epoch   0:  87% | abe: 9.077 | eve: 9.407 | bob: 9.027Epoch   0:  87% | abe: 9.078 | eve: 9.407 | bob: 9.027Epoch   0:  88% | abe: 9.078 | eve: 9.407 | bob: 9.027Epoch   0:  88% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  89% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  90% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  90% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  91% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  91% | abe: 9.077 | eve: 9.408 | bob: 9.027Epoch   0:  92% | abe: 9.077 | eve: 9.409 | bob: 9.027Epoch   0:  92% | abe: 9.078 | eve: 9.409 | bob: 9.027Epoch   0:  93% | abe: 9.078 | eve: 9.409 | bob: 9.027Epoch   0:  94% | abe: 9.077 | eve: 9.409 | bob: 9.027Epoch   0:  94% | abe: 9.077 | eve: 9.409 | bob: 9.027Epoch   0:  95% | abe: 9.077 | eve: 9.410 | bob: 9.027Epoch   0:  95% | abe: 9.077 | eve: 9.410 | bob: 9.027Epoch   0:  96% | abe: 9.077 | eve: 9.410 | bob: 9.027Epoch   0:  97% | abe: 9.078 | eve: 9.410 | bob: 9.027Epoch   0:  97% | abe: 9.078 | eve: 9.410 | bob: 9.027Epoch   0:  98% | abe: 9.078 | eve: 9.410 | bob: 9.027Epoch   0:  98% | abe: 9.078 | eve: 9.411 | bob: 9.027Epoch   0:  99% | abe: 9.078 | eve: 9.411 | bob: 9.028
New best Bob loss 9.027593807599452 at epoch 0
Epoch   1:   0% | abe: 9.050 | eve: 9.472 | bob: 8.999Epoch   1:   0% | abe: 9.051 | eve: 9.504 | bob: 9.000Epoch   1:   1% | abe: 9.061 | eve: 9.453 | bob: 9.011Epoch   1:   1% | abe: 9.076 | eve: 9.479 | bob: 9.025Epoch   1:   2% | abe: 9.067 | eve: 9.453 | bob: 9.017Epoch   1:   2% | abe: 9.070 | eve: 9.457 | bob: 9.020Epoch   1:   3% | abe: 9.070 | eve: 9.463 | bob: 9.020Epoch   1:   4% | abe: 9.081 | eve: 9.459 | bob: 9.031Epoch   1:   4% | abe: 9.080 | eve: 9.463 | bob: 9.029Epoch   1:   5% | abe: 9.086 | eve: 9.456 | bob: 9.036Epoch   1:   5% | abe: 9.087 | eve: 9.450 | bob: 9.037Epoch   1:   6% | abe: 9.090 | eve: 9.445 | bob: 9.040Epoch   1:   7% | abe: 9.092 | eve: 9.451 | bob: 9.042Epoch   1:   7% | abe: 9.089 | eve: 9.452 | bob: 9.038Epoch   1:   8% | abe: 9.085 | eve: 9.454 | bob: 9.034Epoch   1:   8% | abe: 9.082 | eve: 9.458 | bob: 9.032Epoch   1:   9% | abe: 9.079 | eve: 9.457 | bob: 9.028Epoch   1:  10% | abe: 9.081 | eve: 9.456 | bob: 9.030Epoch   1:  10% | abe: 9.082 | eve: 9.460 | bob: 9.032Epoch   1:  11% | abe: 9.080 | eve: 9.458 | bob: 9.029Epoch   1:  11% | abe: 9.079 | eve: 9.459 | bob: 9.029Epoch   1:  12% | abe: 9.079 | eve: 9.459 | bob: 9.029Epoch   1:  12% | abe: 9.081 | eve: 9.462 | bob: 9.031Epoch   1:  13% | abe: 9.082 | eve: 9.462 | bob: 9.032Epoch   1:  14% | abe: 9.081 | eve: 9.460 | bob: 9.031Epoch   1:  14% | abe: 9.081 | eve: 9.460 | bob: 9.031Epoch   1:  15% | abe: 9.079 | eve: 9.457 | bob: 9.029Epoch   1:  15% | abe: 9.080 | eve: 9.460 | bob: 9.030Epoch   1:  16% | abe: 9.079 | eve: 9.461 | bob: 9.029Epoch   1:  17% | abe: 9.076 | eve: 9.460 | bob: 9.026Epoch   1:  17% | abe: 9.076 | eve: 9.459 | bob: 9.025Epoch   1:  18% | abe: 9.078 | eve: 9.461 | bob: 9.028Epoch   1:  18% | abe: 9.078 | eve: 9.462 | bob: 9.028Epoch   1:  19% | abe: 9.077 | eve: 9.462 | bob: 9.027Epoch   1:  20% | abe: 9.078 | eve: 9.462 | bob: 9.027Epoch   1:  20% | abe: 9.078 | eve: 9.460 | bob: 9.028Epoch   1:  21% | abe: 9.078 | eve: 9.458 | bob: 9.028Epoch   1:  21% | abe: 9.077 | eve: 9.459 | bob: 9.026Epoch   1:  22% | abe: 9.075 | eve: 9.457 | bob: 9.024Epoch   1:  22% | abe: 9.075 | eve: 9.459 | bob: 9.025Epoch   1:  23% | abe: 9.075 | eve: 9.460 | bob: 9.024Epoch   1:  24% | abe: 9.074 | eve: 9.459 | bob: 9.023Epoch   1:  24% | abe: 9.073 | eve: 9.459 | bob: 9.022Epoch   1:  25% | abe: 9.072 | eve: 9.460 | bob: 9.022Epoch   1:  25% | abe: 9.073 | eve: 9.458 | bob: 9.023Epoch   1:  26% | abe: 9.075 | eve: 9.456 | bob: 9.024Epoch   1:  27% | abe: 9.075 | eve: 9.456 | bob: 9.024Epoch   1:  27% | abe: 9.075 | eve: 9.456 | bob: 9.025Epoch   1:  28% | abe: 9.075 | eve: 9.455 | bob: 9.025Epoch   1:  28% | abe: 9.075 | eve: 9.455 | bob: 9.025Epoch   1:  29% | abe: 9.077 | eve: 9.454 | bob: 9.027Epoch   1:  30% | abe: 9.077 | eve: 9.454 | bob: 9.026Epoch   1:  30% | abe: 9.078 | eve: 9.453 | bob: 9.028Epoch   1:  31% | abe: 9.079 | eve: 9.453 | bob: 9.030Epoch   1:  31% | abe: 9.077 | eve: 9.452 | bob: 9.027Epoch   1:  32% | abe: 9.078 | eve: 9.452 | bob: 9.028Epoch   1:  32% | abe: 9.077 | eve: 9.453 | bob: 9.027Epoch   1:  33% | abe: 9.077 | eve: 9.453 | bob: 9.027Epoch   1:  34% | abe: 9.078 | eve: 9.453 | bob: 9.028Epoch   1:  34% | abe: 9.078 | eve: 9.453 | bob: 9.028Epoch   1:  35% | abe: 9.077 | eve: 9.453 | bob: 9.027Epoch   1:  35% | abe: 9.077 | eve: 9.453 | bob: 9.027Epoch   1:  36% | abe: 9.076 | eve: 9.454 | bob: 9.026Epoch   1:  37% | abe: 9.075 | eve: 9.454 | bob: 9.025Epoch   1:  37% | abe: 9.076 | eve: 9.454 | bob: 9.026Epoch   1:  38% | abe: 9.075 | eve: 9.454 | bob: 9.025Epoch   1:  38% | abe: 9.073 | eve: 9.454 | bob: 9.023Epoch   1:  39% | abe: 9.074 | eve: 9.454 | bob: 9.024Epoch   1:  40% | abe: 9.075 | eve: 9.453 | bob: 9.024Epoch   1:  40% | abe: 9.073 | eve: 9.454 | bob: 9.023Epoch   1:  41% | abe: 9.074 | eve: 9.452 | bob: 9.024Epoch   1:  41% | abe: 9.074 | eve: 9.452 | bob: 9.023Epoch   1:  42% | abe: 9.075 | eve: 9.452 | bob: 9.025Epoch   1:  42% | abe: 9.077 | eve: 9.450 | bob: 9.027Epoch   1:  43% | abe: 9.076 | eve: 9.449 | bob: 9.026Epoch   1:  44% | abe: 9.076 | eve: 9.450 | bob: 9.026Epoch   1:  44% | abe: 9.075 | eve: 9.450 | bob: 9.025Epoch   1:  45% | abe: 9.076 | eve: 9.450 | bob: 9.025Epoch   1:  45% | abe: 9.076 | eve: 9.450 | bob: 9.025Epoch   1:  46% | abe: 9.076 | eve: 9.452 | bob: 9.026Epoch   1:  47% | abe: 9.076 | eve: 9.453 | bob: 9.026Epoch   1:  47% | abe: 9.076 | eve: 9.453 | bob: 9.026Epoch   1:  48% | abe: 9.077 | eve: 9.453 | bob: 9.026Epoch   1:  48% | abe: 9.076 | eve: 9.452 | bob: 9.026Epoch   1:  49% | abe: 9.076 | eve: 9.452 | bob: 9.026Epoch   1:  50% | abe: 9.076 | eve: 9.452 | bob: 9.026Epoch   1:  50% | abe: 9.076 | eve: 9.452 | bob: 9.025Epoch   1:  51% | abe: 9.075 | eve: 9.452 | bob: 9.024Epoch   1:  51% | abe: 9.076 | eve: 9.452 | bob: 9.025Epoch   1:  52% | abe: 9.076 | eve: 9.453 | bob: 9.026Epoch   1:  52% | abe: 9.076 | eve: 9.453 | bob: 9.026Epoch   1:  53% | abe: 9.076 | eve: 9.454 | bob: 9.026Epoch   1:  54% | abe: 9.076 | eve: 9.455 | bob: 9.026Epoch   1:  54% | abe: 9.075 | eve: 9.455 | bob: 9.024Epoch   1:  55% | abe: 9.076 | eve: 9.456 | bob: 9.025Epoch   1:  55% | abe: 9.075 | eve: 9.455 | bob: 9.025Epoch   1:  56% | abe: 9.075 | eve: 9.455 | bob: 9.025Epoch   1:  57% | abe: 9.075 | eve: 9.455 | bob: 9.024Epoch   1:  57% | abe: 9.075 | eve: 9.456 | bob: 9.025Epoch   1:  58% | abe: 9.076 | eve: 9.456 | bob: 9.025Epoch   1:  58% | abe: 9.076 | eve: 9.455 | bob: 9.025Epoch   1:  59% | abe: 9.076 | eve: 9.455 | bob: 9.026Epoch   1:  60% | abe: 9.076 | eve: 9.455 | bob: 9.025Epoch   1:  60% | abe: 9.076 | eve: 9.456 | bob: 9.026Epoch   1:  61% | abe: 9.077 | eve: 9.455 | bob: 9.026Epoch   1:  61% | abe: 9.076 | eve: 9.456 | bob: 9.025Epoch   1:  62% | abe: 9.076 | eve: 9.456 | bob: 9.026Epoch   1:  62% | abe: 9.076 | eve: 9.455 | bob: 9.026Epoch   1:  63% | abe: 9.076 | eve: 9.455 | bob: 9.025Epoch   1:  64% | abe: 9.076 | eve: 9.455 | bob: 9.025Epoch   1:  64% | abe: 9.076 | eve: 9.455 | bob: 9.026Epoch   1:  65% | abe: 9.076 | eve: 9.454 | bob: 9.025Epoch   1:  65% | abe: 9.075 | eve: 9.454 | bob: 9.025Epoch   1:  66% | abe: 9.075 | eve: 9.454 | bob: 9.025Epoch   1:  67% | abe: 9.075 | eve: 9.454 | bob: 9.024Epoch   1:  67% | abe: 9.074 | eve: 9.454 | bob: 9.023Epoch   1:  68% | abe: 9.073 | eve: 9.454 | bob: 9.023Epoch   1:  68% | abe: 9.073 | eve: 9.454 | bob: 9.023Epoch   1:  69% | abe: 9.073 | eve: 9.454 | bob: 9.023Epoch   1:  70% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  70% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  71% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  71% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  72% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  72% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  73% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  74% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  74% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  75% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  75% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  76% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  77% | abe: 9.074 | eve: 9.455 | bob: 9.023Epoch   1:  77% | abe: 9.073 | eve: 9.455 | bob: 9.023Epoch   1:  78% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  78% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  79% | abe: 9.073 | eve: 9.455 | bob: 9.022Epoch   1:  80% | abe: 9.072 | eve: 9.456 | bob: 9.022Epoch   1:  80% | abe: 9.073 | eve: 9.456 | bob: 9.022Epoch   1:  81% | abe: 9.073 | eve: 9.456 | bob: 9.022Epoch   1:  81% | abe: 9.073 | eve: 9.456 | bob: 9.022Epoch   1:  82% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  82% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  83% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  84% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  84% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  85% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  85% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  86% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  87% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  87% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  88% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  88% | abe: 9.072 | eve: 9.457 | bob: 9.022Epoch   1:  89% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  90% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  90% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  91% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  91% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  92% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  92% | abe: 9.073 | eve: 9.457 | bob: 9.023Epoch   1:  93% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  94% | abe: 9.072 | eve: 9.457 | bob: 9.022Epoch   1:  94% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  95% | abe: 9.073 | eve: 9.456 | bob: 9.022Epoch   1:  95% | abe: 9.073 | eve: 9.456 | bob: 9.023Epoch   1:  96% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  97% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  97% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  98% | abe: 9.072 | eve: 9.457 | bob: 9.022Epoch   1:  98% | abe: 9.073 | eve: 9.457 | bob: 9.022Epoch   1:  99% | abe: 9.073 | eve: 9.457 | bob: 9.022
New best Bob loss 9.022081272198141 at epoch 1
Epoch   2:   0% | abe: 9.180 | eve: 9.444 | bob: 9.129Epoch   2:   0% | abe: 9.163 | eve: 9.447 | bob: 9.113Epoch   2:   1% | abe: 9.095 | eve: 9.452 | bob: 9.043Epoch   2:   1% | abe: 9.103 | eve: 9.491 | bob: 9.051Epoch   2:   2% | abe: 9.110 | eve: 9.475 | bob: 9.059Epoch   2:   2% | abe: 9.107 | eve: 9.474 | bob: 9.054Epoch   2:   3% | abe: 9.099 | eve: 9.467 | bob: 9.047Epoch   2:   4% | abe: 9.104 | eve: 9.474 | bob: 9.051Epoch   2:   4% | abe: 9.092 | eve: 9.477 | bob: 9.039Epoch   2:   5% | abe: 9.098 | eve: 9.480 | bob: 9.045Epoch   2:   5% | abe: 9.089 | eve: 9.476 | bob: 9.036Epoch   2:   6% | abe: 9.088 | eve: 9.481 | bob: 9.035Epoch   2:   7% | abe: 9.082 | eve: 9.477 | bob: 9.029Epoch   2:   7% | abe: 9.086 | eve: 9.479 | bob: 9.032Epoch   2:   8% | abe: 9.081 | eve: 9.478 | bob: 9.027Epoch   2:   8% | abe: 9.078 | eve: 9.478 | bob: 9.024Epoch   2:   9% | abe: 9.075 | eve: 9.478 | bob: 9.022Epoch   2:  10% | abe: 9.074 | eve: 9.479 | bob: 9.020Epoch   2:  10% | abe: 9.075 | eve: 9.478 | bob: 9.022Epoch   2:  11% | abe: 9.070 | eve: 9.470 | bob: 9.017Epoch   2:  11% | abe: 9.070 | eve: 9.469 | bob: 9.016Epoch   2:  12% | abe: 9.068 | eve: 9.469 | bob: 9.014Epoch   2:  12% | abe: 9.068 | eve: 9.467 | bob: 9.014Epoch   2:  13% | abe: 9.068 | eve: 9.470 | bob: 9.014Epoch   2:  14% | abe: 9.065 | eve: 9.472 | bob: 9.011Epoch   2:  14% | abe: 9.062 | eve: 9.472 | bob: 9.008Epoch   2:  15% | abe: 9.063 | eve: 9.472 | bob: 9.009Epoch   2:  15% | abe: 9.063 | eve: 9.469 | bob: 9.009Epoch   2:  16% | abe: 9.064 | eve: 9.467 | bob: 9.010Epoch   2:  17% | abe: 9.064 | eve: 9.468 | bob: 9.010Epoch   2:  17% | abe: 9.061 | eve: 9.467 | bob: 9.007Epoch   2:  18% | abe: 9.060 | eve: 9.467 | bob: 9.006Epoch   2:  18% | abe: 9.059 | eve: 9.468 | bob: 9.005Epoch   2:  19% | abe: 9.060 | eve: 9.469 | bob: 9.006Epoch   2:  20% | abe: 9.060 | eve: 9.468 | bob: 9.006Epoch   2:  20% | abe: 9.062 | eve: 9.468 | bob: 9.008Epoch   2:  21% | abe: 9.062 | eve: 9.468 | bob: 9.008Epoch   2:  21% | abe: 9.061 | eve: 9.468 | bob: 9.006Epoch   2:  22% | abe: 9.061 | eve: 9.467 | bob: 9.006Epoch   2:  22% | abe: 9.060 | eve: 9.468 | bob: 9.005Epoch   2:  23% | abe: 9.059 | eve: 9.468 | bob: 9.004Epoch   2:  24% | abe: 9.059 | eve: 9.469 | bob: 9.004Epoch   2:  24% | abe: 9.057 | eve: 9.471 | bob: 9.002Epoch   2:  25% | abe: 9.057 | eve: 9.473 | bob: 9.002Epoch   2:  25% | abe: 9.058 | eve: 9.474 | bob: 9.003Epoch   2:  26% | abe: 9.058 | eve: 9.475 | bob: 9.002Epoch   2:  27% | abe: 9.058 | eve: 9.475 | bob: 9.003Epoch   2:  27% | abe: 9.060 | eve: 9.475 | bob: 9.005Epoch   2:  28% | abe: 9.059 | eve: 9.476 | bob: 9.004Epoch   2:  28% | abe: 9.059 | eve: 9.475 | bob: 9.003Epoch   2:  29% | abe: 9.059 | eve: 9.475 | bob: 9.003Epoch   2:  30% | abe: 9.057 | eve: 9.473 | bob: 9.002Epoch   2:  30% | abe: 9.056 | eve: 9.476 | bob: 9.001Epoch   2:  31% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  31% | abe: 9.057 | eve: 9.478 | bob: 9.001Epoch   2:  32% | abe: 9.056 | eve: 9.476 | bob: 9.000Epoch   2:  32% | abe: 9.055 | eve: 9.475 | bob: 8.999Epoch   2:  33% | abe: 9.054 | eve: 9.476 | bob: 8.998Epoch   2:  34% | abe: 9.053 | eve: 9.476 | bob: 8.997Epoch   2:  34% | abe: 9.054 | eve: 9.478 | bob: 8.998Epoch   2:  35% | abe: 9.054 | eve: 9.478 | bob: 8.998Epoch   2:  35% | abe: 9.055 | eve: 9.476 | bob: 8.999Epoch   2:  36% | abe: 9.055 | eve: 9.474 | bob: 8.999Epoch   2:  37% | abe: 9.054 | eve: 9.476 | bob: 8.997Epoch   2:  37% | abe: 9.054 | eve: 9.478 | bob: 8.998Epoch   2:  38% | abe: 9.053 | eve: 9.478 | bob: 8.997Epoch   2:  38% | abe: 9.054 | eve: 9.478 | bob: 8.997Epoch   2:  39% | abe: 9.054 | eve: 9.478 | bob: 8.998Epoch   2:  40% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  40% | abe: 9.054 | eve: 9.476 | bob: 8.998Epoch   2:  41% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  41% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  42% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  42% | abe: 9.054 | eve: 9.478 | bob: 8.998Epoch   2:  43% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  44% | abe: 9.057 | eve: 9.479 | bob: 9.000Epoch   2:  44% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  45% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  45% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  46% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  47% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  47% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  48% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  48% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  49% | abe: 9.056 | eve: 9.478 | bob: 8.999Epoch   2:  50% | abe: 9.056 | eve: 9.478 | bob: 9.000Epoch   2:  50% | abe: 9.056 | eve: 9.478 | bob: 9.000Epoch   2:  51% | abe: 9.057 | eve: 9.478 | bob: 9.001Epoch   2:  51% | abe: 9.057 | eve: 9.478 | bob: 9.001Epoch   2:  52% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  52% | abe: 9.057 | eve: 9.478 | bob: 9.001Epoch   2:  53% | abe: 9.058 | eve: 9.477 | bob: 9.002Epoch   2:  54% | abe: 9.058 | eve: 9.478 | bob: 9.002Epoch   2:  54% | abe: 9.059 | eve: 9.478 | bob: 9.003Epoch   2:  55% | abe: 9.059 | eve: 9.478 | bob: 9.003Epoch   2:  55% | abe: 9.059 | eve: 9.477 | bob: 9.003Epoch   2:  56% | abe: 9.058 | eve: 9.478 | bob: 9.002Epoch   2:  57% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  57% | abe: 9.057 | eve: 9.476 | bob: 9.001Epoch   2:  58% | abe: 9.057 | eve: 9.476 | bob: 9.001Epoch   2:  58% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  59% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  60% | abe: 9.057 | eve: 9.477 | bob: 9.001Epoch   2:  60% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  61% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  61% | abe: 9.056 | eve: 9.478 | bob: 9.000Epoch   2:  62% | abe: 9.056 | eve: 9.478 | bob: 9.000Epoch   2:  62% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  63% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  64% | abe: 9.055 | eve: 9.478 | bob: 8.999Epoch   2:  64% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  65% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  65% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  66% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  67% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  67% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  68% | abe: 9.054 | eve: 9.476 | bob: 8.997Epoch   2:  68% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  69% | abe: 9.054 | eve: 9.478 | bob: 8.997Epoch   2:  70% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  70% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  71% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  71% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  72% | abe: 9.055 | eve: 9.476 | bob: 8.998Epoch   2:  72% | abe: 9.054 | eve: 9.476 | bob: 8.998Epoch   2:  73% | abe: 9.054 | eve: 9.476 | bob: 8.998Epoch   2:  74% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  74% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  75% | abe: 9.056 | eve: 9.476 | bob: 8.999Epoch   2:  75% | abe: 9.056 | eve: 9.477 | bob: 8.999Epoch   2:  76% | abe: 9.056 | eve: 9.476 | bob: 9.000Epoch   2:  77% | abe: 9.056 | eve: 9.476 | bob: 8.999Epoch   2:  77% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  78% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  78% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  79% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  80% | abe: 9.056 | eve: 9.477 | bob: 9.000Epoch   2:  80% | abe: 9.056 | eve: 9.476 | bob: 9.000Epoch   2:  81% | abe: 9.056 | eve: 9.476 | bob: 9.000Epoch   2:  81% | abe: 9.056 | eve: 9.476 | bob: 8.999Epoch   2:  82% | abe: 9.055 | eve: 9.476 | bob: 8.999Epoch   2:  82% | abe: 9.055 | eve: 9.475 | bob: 8.999Epoch   2:  83% | abe: 9.056 | eve: 9.475 | bob: 8.999Epoch   2:  84% | abe: 9.055 | eve: 9.475 | bob: 8.999Epoch   2:  84% | abe: 9.055 | eve: 9.475 | bob: 8.999Epoch   2:  85% | abe: 9.055 | eve: 9.474 | bob: 8.999Epoch   2:  85% | abe: 9.054 | eve: 9.475 | bob: 8.998Epoch   2:  86% | abe: 9.055 | eve: 9.475 | bob: 8.998Epoch   2:  87% | abe: 9.055 | eve: 9.476 | bob: 8.998Epoch   2:  87% | abe: 9.054 | eve: 9.476 | bob: 8.998Epoch   2:  88% | abe: 9.054 | eve: 9.475 | bob: 8.998Epoch   2:  88% | abe: 9.055 | eve: 9.476 | bob: 8.998Epoch   2:  89% | abe: 9.055 | eve: 9.476 | bob: 8.998Epoch   2:  90% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  90% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  91% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  91% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  92% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  92% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  93% | abe: 9.056 | eve: 9.477 | bob: 8.999Epoch   2:  94% | abe: 9.056 | eve: 9.477 | bob: 8.999Epoch   2:  94% | abe: 9.055 | eve: 9.477 | bob: 8.999Epoch   2:  95% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  95% | abe: 9.055 | eve: 9.477 | bob: 8.998Epoch   2:  96% | abe: 9.054 | eve: 9.477 | bob: 8.998Epoch   2:  97% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  97% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  98% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  98% | abe: 9.054 | eve: 9.477 | bob: 8.997Epoch   2:  99% | abe: 9.054 | eve: 9.477 | bob: 8.997
New best Bob loss 8.997418232617544 at epoch 2
Epoch   3:   0% | abe: 9.048 | eve: 9.465 | bob: 8.988Epoch   3:   0% | abe: 9.099 | eve: 9.480 | bob: 9.041Epoch   3:   1% | abe: 9.089 | eve: 9.511 | bob: 9.031Epoch   3:   1% | abe: 9.072 | eve: 9.494 | bob: 9.013Epoch   3:   2% | abe: 9.051 | eve: 9.492 | bob: 8.992Epoch   3:   2% | abe: 9.054 | eve: 9.492 | bob: 8.995Epoch   3:   3% | abe: 9.047 | eve: 9.505 | bob: 8.987Epoch   3:   4% | abe: 9.055 | eve: 9.503 | bob: 8.996Epoch   3:   4% | abe: 9.061 | eve: 9.498 | bob: 9.002Epoch   3:   5% | abe: 9.049 | eve: 9.497 | bob: 8.990Epoch   3:   5% | abe: 9.048 | eve: 9.491 | bob: 8.989Epoch   3:   6% | abe: 9.052 | eve: 9.485 | bob: 8.994Epoch   3:   7% | abe: 9.050 | eve: 9.491 | bob: 8.991Epoch   3:   7% | abe: 9.047 | eve: 9.493 | bob: 8.989Epoch   3:   8% | abe: 9.051 | eve: 9.491 | bob: 8.993Epoch   3:   8% | abe: 9.052 | eve: 9.487 | bob: 8.994Epoch   3:   9% | abe: 9.053 | eve: 9.489 | bob: 8.995Epoch   3:  10% | abe: 9.051 | eve: 9.485 | bob: 8.993Epoch   3:  10% | abe: 9.050 | eve: 9.487 | bob: 8.992Epoch   3:  11% | abe: 9.049 | eve: 9.489 | bob: 8.991Epoch   3:  11% | abe: 9.050 | eve: 9.488 | bob: 8.993Epoch   3:  12% | abe: 9.048 | eve: 9.482 | bob: 8.990Epoch   3:  12% | abe: 9.048 | eve: 9.484 | bob: 8.990Epoch   3:  13% | abe: 9.047 | eve: 9.482 | bob: 8.989Epoch   3:  14% | abe: 9.050 | eve: 9.479 | bob: 8.992Epoch   3:  14% | abe: 9.049 | eve: 9.478 | bob: 8.991Epoch   3:  15% | abe: 9.050 | eve: 9.477 | bob: 8.992Epoch   3:  15% | abe: 9.051 | eve: 9.476 | bob: 8.993Epoch   3:  16% | abe: 9.051 | eve: 9.473 | bob: 8.993Epoch   3:  17% | abe: 9.054 | eve: 9.473 | bob: 8.996Epoch   3:  17% | abe: 9.054 | eve: 9.475 | bob: 8.996Epoch   3:  18% | abe: 9.052 | eve: 9.475 | bob: 8.995Epoch   3:  18% | abe: 9.053 | eve: 9.474 | bob: 8.996Epoch   3:  19% | abe: 9.053 | eve: 9.474 | bob: 8.995Epoch   3:  20% | abe: 9.054 | eve: 9.474 | bob: 8.996Epoch   3:  20% | abe: 9.054 | eve: 9.474 | bob: 8.996Epoch   3:  21% | abe: 9.053 | eve: 9.473 | bob: 8.996Epoch   3:  21% | abe: 9.053 | eve: 9.472 | bob: 8.995Epoch   3:  22% | abe: 9.052 | eve: 9.468 | bob: 8.995Epoch   3:  22% | abe: 9.051 | eve: 9.466 | bob: 8.993Epoch   3:  23% | abe: 9.052 | eve: 9.466 | bob: 8.994Epoch   3:  24% | abe: 9.050 | eve: 9.468 | bob: 8.992Epoch   3:  24% | abe: 9.050 | eve: 9.468 | bob: 8.993Epoch   3:  25% | abe: 9.051 | eve: 9.468 | bob: 8.993Epoch   3:  25% | abe: 9.051 | eve: 9.468 | bob: 8.993Epoch   3:  26% | abe: 9.051 | eve: 9.468 | bob: 8.993Epoch   3:  27% | abe: 9.051 | eve: 9.465 | bob: 8.993Epoch   3:  27% | abe: 9.052 | eve: 9.463 | bob: 8.994Epoch   3:  28% | abe: 9.052 | eve: 9.467 | bob: 8.993Epoch   3:  28% | abe: 9.053 | eve: 9.467 | bob: 8.994Epoch   3:  29% | abe: 9.052 | eve: 9.467 | bob: 8.994Epoch   3:  30% | abe: 9.052 | eve: 9.468 | bob: 8.993Epoch   3:  30% | abe: 9.052 | eve: 9.467 | bob: 8.994Epoch   3:  31% | abe: 9.052 | eve: 9.468 | bob: 8.994Epoch   3:  31% | abe: 9.051 | eve: 9.467 | bob: 8.993Epoch   3:  32% | abe: 9.051 | eve: 9.468 | bob: 8.992Epoch   3:  32% | abe: 9.049 | eve: 9.469 | bob: 8.991Epoch   3:  33% | abe: 9.049 | eve: 9.468 | bob: 8.990Epoch   3:  34% | abe: 9.050 | eve: 9.471 | bob: 8.991Epoch   3:  34% | abe: 9.050 | eve: 9.470 | bob: 8.992Epoch   3:  35% | abe: 9.050 | eve: 9.472 | bob: 8.991Epoch   3:  35% | abe: 9.048 | eve: 9.472 | bob: 8.990Epoch   3:  36% | abe: 9.049 | eve: 9.473 | bob: 8.990Epoch   3:  37% | abe: 9.049 | eve: 9.471 | bob: 8.990Epoch   3:  37% | abe: 9.050 | eve: 9.472 | bob: 8.991Epoch   3:  38% | abe: 9.048 | eve: 9.472 | bob: 8.990Epoch   3:  38% | abe: 9.048 | eve: 9.471 | bob: 8.990Epoch   3:  39% | abe: 9.048 | eve: 9.472 | bob: 8.989Epoch   3:  40% | abe: 9.049 | eve: 9.472 | bob: 8.990Epoch   3:  40% | abe: 9.048 | eve: 9.471 | bob: 8.989Epoch   3:  41% | abe: 9.047 | eve: 9.471 | bob: 8.988Epoch   3:  41% | abe: 9.046 | eve: 9.473 | bob: 8.987Epoch   3:  42% | abe: 9.047 | eve: 9.473 | bob: 8.988Epoch   3:  42% | abe: 9.046 | eve: 9.473 | bob: 8.986Epoch   3:  43% | abe: 9.045 | eve: 9.473 | bob: 8.986Epoch   3:  44% | abe: 9.046 | eve: 9.473 | bob: 8.987Epoch   3:  44% | abe: 9.046 | eve: 9.473 | bob: 8.986Epoch   3:  45% | abe: 9.045 | eve: 9.472 | bob: 8.986Epoch   3:  45% | abe: 9.045 | eve: 9.473 | bob: 8.986Epoch   3:  46% | abe: 9.045 | eve: 9.472 | bob: 8.985Epoch   3:  47% | abe: 9.044 | eve: 9.473 | bob: 8.985Epoch   3:  47% | abe: 9.045 | eve: 9.473 | bob: 8.986Epoch   3:  48% | abe: 9.044 | eve: 9.472 | bob: 8.984Epoch   3:  48% | abe: 9.044 | eve: 9.472 | bob: 8.984Epoch   3:  49% | abe: 9.044 | eve: 9.472 | bob: 8.985Epoch   3:  50% | abe: 9.044 | eve: 9.471 | bob: 8.985Epoch   3:  50% | abe: 9.044 | eve: 9.471 | bob: 8.985Epoch   3:  51% | abe: 9.043 | eve: 9.470 | bob: 8.984Epoch   3:  51% | abe: 9.043 | eve: 9.470 | bob: 8.983Epoch   3:  52% | abe: 9.043 | eve: 9.470 | bob: 8.984Epoch   3:  52% | abe: 9.043 | eve: 9.470 | bob: 8.983Epoch   3:  53% | abe: 9.043 | eve: 9.470 | bob: 8.984Epoch   3:  54% | abe: 9.043 | eve: 9.470 | bob: 8.983Epoch   3:  54% | abe: 9.043 | eve: 9.469 | bob: 8.983Epoch   3:  55% | abe: 9.042 | eve: 9.469 | bob: 8.983Epoch   3:  55% | abe: 9.043 | eve: 9.469 | bob: 8.983Epoch   3:  56% | abe: 9.043 | eve: 9.469 | bob: 8.984Epoch   3:  57% | abe: 9.043 | eve: 9.469 | bob: 8.983Epoch   3:  57% | abe: 9.043 | eve: 9.470 | bob: 8.983Epoch   3:  58% | abe: 9.043 | eve: 9.470 | bob: 8.984Epoch   3:  58% | abe: 9.043 | eve: 9.470 | bob: 8.983Epoch   3:  59% | abe: 9.043 | eve: 9.469 | bob: 8.983Epoch   3:  60% | abe: 9.042 | eve: 9.469 | bob: 8.983Epoch   3:  60% | abe: 9.042 | eve: 9.470 | bob: 8.982Epoch   3:  61% | abe: 9.041 | eve: 9.471 | bob: 8.981Epoch   3:  61% | abe: 9.041 | eve: 9.471 | bob: 8.981Epoch   3:  62% | abe: 9.040 | eve: 9.471 | bob: 8.980Epoch   3:  62% | abe: 9.040 | eve: 9.472 | bob: 8.980Epoch   3:  63% | abe: 9.039 | eve: 9.471 | bob: 8.980Epoch   3:  64% | abe: 9.039 | eve: 9.471 | bob: 8.979Epoch   3:  64% | abe: 9.038 | eve: 9.471 | bob: 8.978Epoch   3:  65% | abe: 9.038 | eve: 9.471 | bob: 8.978Epoch   3:  65% | abe: 9.038 | eve: 9.471 | bob: 8.978Epoch   3:  66% | abe: 9.037 | eve: 9.470 | bob: 8.977Epoch   3:  67% | abe: 9.038 | eve: 9.470 | bob: 8.978Epoch   3:  67% | abe: 9.038 | eve: 9.470 | bob: 8.978Epoch   3:  68% | abe: 9.038 | eve: 9.470 | bob: 8.978Epoch   3:  68% | abe: 9.037 | eve: 9.470 | bob: 8.977Epoch   3:  69% | abe: 9.037 | eve: 9.470 | bob: 8.977Epoch   3:  70% | abe: 9.036 | eve: 9.470 | bob: 8.976Epoch   3:  70% | abe: 9.036 | eve: 9.471 | bob: 8.976Epoch   3:  71% | abe: 9.036 | eve: 9.470 | bob: 8.976Epoch   3:  71% | abe: 9.036 | eve: 9.470 | bob: 8.976Epoch   3:  72% | abe: 9.036 | eve: 9.470 | bob: 8.976Epoch   3:  72% | abe: 9.036 | eve: 9.469 | bob: 8.976Epoch   3:  73% | abe: 9.035 | eve: 9.469 | bob: 8.975Epoch   3:  74% | abe: 9.034 | eve: 9.469 | bob: 8.974Epoch   3:  74% | abe: 9.034 | eve: 9.470 | bob: 8.974Epoch   3:  75% | abe: 9.034 | eve: 9.470 | bob: 8.973Epoch   3:  75% | abe: 9.034 | eve: 9.469 | bob: 8.974Epoch   3:  76% | abe: 9.034 | eve: 9.469 | bob: 8.974Epoch   3:  77% | abe: 9.034 | eve: 9.469 | bob: 8.973Epoch   3:  77% | abe: 9.034 | eve: 9.467 | bob: 8.974Epoch   3:  78% | abe: 9.034 | eve: 9.468 | bob: 8.973Epoch   3:  78% | abe: 9.034 | eve: 9.468 | bob: 8.973Epoch   3:  79% | abe: 9.033 | eve: 9.468 | bob: 8.973Epoch   3:  80% | abe: 9.033 | eve: 9.468 | bob: 8.973Epoch   3:  80% | abe: 9.033 | eve: 9.468 | bob: 8.973Epoch   3:  81% | abe: 9.033 | eve: 9.469 | bob: 8.972Epoch   3:  81% | abe: 9.033 | eve: 9.469 | bob: 8.973Epoch   3:  82% | abe: 9.033 | eve: 9.469 | bob: 8.972Epoch   3:  82% | abe: 9.033 | eve: 9.468 | bob: 8.972Epoch   3:  83% | abe: 9.032 | eve: 9.468 | bob: 8.972Epoch   3:  84% | abe: 9.032 | eve: 9.468 | bob: 8.972Epoch   3:  84% | abe: 9.032 | eve: 9.468 | bob: 8.972Epoch   3:  85% | abe: 9.032 | eve: 9.469 | bob: 8.972Epoch   3:  85% | abe: 9.032 | eve: 9.469 | bob: 8.971Epoch   3:  86% | abe: 9.032 | eve: 9.470 | bob: 8.971Epoch   3:  87% | abe: 9.032 | eve: 9.470 | bob: 8.971Epoch   3:  87% | abe: 9.031 | eve: 9.470 | bob: 8.971Epoch   3:  88% | abe: 9.031 | eve: 9.470 | bob: 8.970Epoch   3:  88% | abe: 9.030 | eve: 9.470 | bob: 8.970Epoch   3:  89% | abe: 9.030 | eve: 9.471 | bob: 8.970Epoch   3:  90% | abe: 9.030 | eve: 9.471 | bob: 8.969Epoch   3:  90% | abe: 9.031 | eve: 9.471 | bob: 8.970Epoch   3:  91% | abe: 9.031 | eve: 9.471 | bob: 8.970Epoch   3:  91% | abe: 9.031 | eve: 9.471 | bob: 8.970Epoch   3:  92% | abe: 9.031 | eve: 9.472 | bob: 8.970Epoch   3:  92% | abe: 9.031 | eve: 9.472 | bob: 8.970Epoch   3:  93% | abe: 9.031 | eve: 9.472 | bob: 8.970Epoch   3:  94% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  94% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  95% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  95% | abe: 9.030 | eve: 9.474 | bob: 8.969Epoch   3:  96% | abe: 9.030 | eve: 9.474 | bob: 8.969Epoch   3:  97% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  97% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  98% | abe: 9.030 | eve: 9.473 | bob: 8.969Epoch   3:  98% | abe: 9.029 | eve: 9.473 | bob: 8.969Epoch   3:  99% | abe: 9.029 | eve: 9.473 | bob: 8.968
New best Bob loss 8.96830096857499 at epoch 3
Epoch   4:   0% | abe: 8.974 | eve: 9.495 | bob: 8.908Epoch   4:   0% | abe: 8.993 | eve: 9.523 | bob: 8.928Epoch   4:   1% | abe: 8.983 | eve: 9.509 | bob: 8.918Epoch   4:   1% | abe: 8.987 | eve: 9.507 | bob: 8.922Epoch   4:   2% | abe: 9.001 | eve: 9.502 | bob: 8.936Epoch   4:   2% | abe: 9.001 | eve: 9.492 | bob: 8.936Epoch   4:   3% | abe: 9.009 | eve: 9.501 | bob: 8.945Epoch   4:   4% | abe: 9.013 | eve: 9.491 | bob: 8.949Epoch   4:   4% | abe: 9.020 | eve: 9.495 | bob: 8.956Epoch   4:   5% | abe: 9.011 | eve: 9.489 | bob: 8.947Epoch   4:   5% | abe: 9.007 | eve: 9.486 | bob: 8.943Epoch   4:   6% | abe: 9.009 | eve: 9.485 | bob: 8.946Epoch   4:   7% | abe: 9.013 | eve: 9.481 | bob: 8.950Epoch   4:   7% | abe: 9.015 | eve: 9.479 | bob: 8.953Epoch   4:   8% | abe: 9.018 | eve: 9.485 | bob: 8.956Epoch   4:   8% | abe: 9.019 | eve: 9.489 | bob: 8.957Epoch   4:   9% | abe: 9.022 | eve: 9.490 | bob: 8.960Epoch   4:  10% | abe: 9.018 | eve: 9.491 | bob: 8.956Epoch   4:  10% | abe: 9.018 | eve: 9.486 | bob: 8.956Epoch   4:  11% | abe: 9.012 | eve: 9.488 | bob: 8.950Epoch   4:  11% | abe: 9.006 | eve: 9.484 | bob: 8.943Epoch   4:  12% | abe: 9.004 | eve: 9.487 | bob: 8.941Epoch   4:  12% | abe: 9.002 | eve: 9.488 | bob: 8.938Epoch   4:  13% | abe: 9.007 | eve: 9.486 | bob: 8.943Epoch   4:  14% | abe: 9.004 | eve: 9.487 | bob: 8.940Epoch   4:  14% | abe: 9.003 | eve: 9.489 | bob: 8.939Epoch   4:  15% | abe: 9.001 | eve: 9.489 | bob: 8.937Epoch   4:  15% | abe: 9.003 | eve: 9.486 | bob: 8.939Epoch   4:  16% | abe: 9.005 | eve: 9.486 | bob: 8.940Epoch   4:  17% | abe: 9.005 | eve: 9.486 | bob: 8.941Epoch   4:  17% | abe: 9.005 | eve: 9.483 | bob: 8.941Epoch   4:  18% | abe: 9.005 | eve: 9.484 | bob: 8.941Epoch   4:  18% | abe: 9.002 | eve: 9.484 | bob: 8.939Epoch   4:  19% | abe: 9.000 | eve: 9.482 | bob: 8.936Epoch   4:  20% | abe: 8.999 | eve: 9.483 | bob: 8.935Epoch   4:  20% | abe: 8.998 | eve: 9.483 | bob: 8.934Epoch   4:  21% | abe: 8.998 | eve: 9.483 | bob: 8.934Epoch   4:  21% | abe: 8.998 | eve: 9.482 | bob: 8.934Epoch   4:  22% | abe: 8.999 | eve: 9.481 | bob: 8.935Epoch   4:  22% | abe: 8.997 | eve: 9.481 | bob: 8.933Epoch   4:  23% | abe: 8.998 | eve: 9.484 | bob: 8.934Epoch   4:  24% | abe: 8.998 | eve: 9.483 | bob: 8.934Epoch   4:  24% | abe: 8.998 | eve: 9.485 | bob: 8.934Epoch   4:  25% | abe: 8.997 | eve: 9.486 | bob: 8.933Epoch   4:  25% | abe: 8.997 | eve: 9.484 | bob: 8.933Epoch   4:  26% | abe: 8.996 | eve: 9.486 | bob: 8.932Epoch   4:  27% | abe: 8.996 | eve: 9.485 | bob: 8.932Epoch   4:  27% | abe: 8.996 | eve: 9.486 | bob: 8.932Epoch   4:  28% | abe: 8.995 | eve: 9.484 | bob: 8.931Epoch   4:  28% | abe: 8.995 | eve: 9.485 | bob: 8.932Epoch   4:  29% | abe: 8.995 | eve: 9.486 | bob: 8.932Epoch   4:  30% | abe: 8.995 | eve: 9.485 | bob: 8.932Epoch   4:  30% | abe: 8.994 | eve: 9.485 | bob: 8.931Epoch   4:  31% | abe: 8.993 | eve: 9.486 | bob: 8.930Epoch   4:  31% | abe: 8.992 | eve: 9.486 | bob: 8.928Epoch   4:  32% | abe: 8.991 | eve: 9.485 | bob: 8.927Epoch   4:  32% | abe: 8.991 | eve: 9.486 | bob: 8.927Epoch   4:  33% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  34% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  34% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  35% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  35% | abe: 8.990 | eve: 9.487 | bob: 8.927Epoch   4:  36% | abe: 8.991 | eve: 9.486 | bob: 8.927Epoch   4:  37% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  37% | abe: 8.991 | eve: 9.487 | bob: 8.927Epoch   4:  38% | abe: 8.990 | eve: 9.487 | bob: 8.926Epoch   4:  38% | abe: 8.990 | eve: 9.487 | bob: 8.926Epoch   4:  39% | abe: 8.990 | eve: 9.486 | bob: 8.926Epoch   4:  40% | abe: 8.989 | eve: 9.488 | bob: 8.925Epoch   4:  40% | abe: 8.988 | eve: 9.488 | bob: 8.925Epoch   4:  41% | abe: 8.989 | eve: 9.488 | bob: 8.925Epoch   4:  41% | abe: 8.989 | eve: 9.487 | bob: 8.925Epoch   4:  42% | abe: 8.987 | eve: 9.487 | bob: 8.923Epoch   4:  42% | abe: 8.988 | eve: 9.487 | bob: 8.924Epoch   4:  43% | abe: 8.986 | eve: 9.487 | bob: 8.922Epoch   4:  44% | abe: 8.986 | eve: 9.486 | bob: 8.922Epoch   4:  44% | abe: 8.986 | eve: 9.486 | bob: 8.922Epoch   4:  45% | abe: 8.987 | eve: 9.486 | bob: 8.923Epoch   4:  45% | abe: 8.986 | eve: 9.486 | bob: 8.922Epoch   4:  46% | abe: 8.987 | eve: 9.485 | bob: 8.923Epoch   4:  47% | abe: 8.985 | eve: 9.486 | bob: 8.921Epoch   4:  47% | abe: 8.984 | eve: 9.486 | bob: 8.920Epoch   4:  48% | abe: 8.982 | eve: 9.485 | bob: 8.918Epoch   4:  48% | abe: 8.982 | eve: 9.485 | bob: 8.918Epoch   4:  49% | abe: 8.982 | eve: 9.484 | bob: 8.918Epoch   4:  50% | abe: 8.981 | eve: 9.483 | bob: 8.917Epoch   4:  50% | abe: 8.981 | eve: 9.483 | bob: 8.917Epoch   4:  51% | abe: 8.981 | eve: 9.482 | bob: 8.917Epoch   4:  51% | abe: 8.981 | eve: 9.481 | bob: 8.917Epoch   4:  52% | abe: 8.980 | eve: 9.482 | bob: 8.916Epoch   4:  52% | abe: 8.980 | eve: 9.482 | bob: 8.916Epoch   4:  53% | abe: 8.980 | eve: 9.482 | bob: 8.916Epoch   4:  54% | abe: 8.979 | eve: 9.481 | bob: 8.915Epoch   4:  54% | abe: 8.979 | eve: 9.482 | bob: 8.915Epoch   4:  55% | abe: 8.978 | eve: 9.482 | bob: 8.914Epoch   4:  55% | abe: 8.979 | eve: 9.482 | bob: 8.915Epoch   4:  56% | abe: 8.979 | eve: 9.482 | bob: 8.915Epoch   4:  57% | abe: 8.978 | eve: 9.483 | bob: 8.914Epoch   4:  57% | abe: 8.977 | eve: 9.483 | bob: 8.913Epoch   4:  58% | abe: 8.976 | eve: 9.483 | bob: 8.912Epoch   4:  58% | abe: 8.976 | eve: 9.483 | bob: 8.912Epoch   4:  59% | abe: 8.977 | eve: 9.483 | bob: 8.913Epoch   4:  60% | abe: 8.976 | eve: 9.483 | bob: 8.913Epoch   4:  60% | abe: 8.976 | eve: 9.483 | bob: 8.912Epoch   4:  61% | abe: 8.975 | eve: 9.483 | bob: 8.911Epoch   4:  61% | abe: 8.975 | eve: 9.483 | bob: 8.911Epoch   4:  62% | abe: 8.974 | eve: 9.483 | bob: 8.910Epoch   4:  62% | abe: 8.974 | eve: 9.484 | bob: 8.910Epoch   4:  63% | abe: 8.974 | eve: 9.483 | bob: 8.910Epoch   4:  64% | abe: 8.974 | eve: 9.484 | bob: 8.910Epoch   4:  64% | abe: 8.974 | eve: 9.485 | bob: 8.910Epoch   4:  65% | abe: 8.974 | eve: 9.485 | bob: 8.910Epoch   4:  65% | abe: 8.974 | eve: 9.484 | bob: 8.910Epoch   4:  66% | abe: 8.974 | eve: 9.485 | bob: 8.910Epoch   4:  67% | abe: 8.973 | eve: 9.484 | bob: 8.910Epoch   4:  67% | abe: 8.973 | eve: 9.484 | bob: 8.909Epoch   4:  68% | abe: 8.973 | eve: 9.484 | bob: 8.910Epoch   4:  68% | abe: 8.972 | eve: 9.484 | bob: 8.908Epoch   4:  69% | abe: 8.971 | eve: 9.484 | bob: 8.907Epoch   4:  70% | abe: 8.970 | eve: 9.483 | bob: 8.906Epoch   4:  70% | abe: 8.969 | eve: 9.482 | bob: 8.906