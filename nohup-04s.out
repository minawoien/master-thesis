WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-18 19:37:42.481527: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-18 19:37:42.642894: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-18 19:37:42.643500: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:37:42.646781: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-18 19:37:42.649540: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-18 19:37:42.650362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-18 19:37:42.654061: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-18 19:37:42.656855: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-18 19:37:42.664269: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-18 19:37:42.676530: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-18 19:37:42.676995: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-18 19:37:42.697505: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-18 19:37:42.700406: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3f7a360 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-18 19:37:42.700433: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-18 19:37:42.907273: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3943990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-18 19:37:42.907358: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-18 19:37:42.917668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-18 19:37:42.917785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:37:42.917815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-18 19:37:42.917839: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-18 19:37:42.917862: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-18 19:37:42.917885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-18 19:37:42.917908: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-18 19:37:42.917932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-18 19:37:42.924884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-18 19:37:42.924957: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-18 19:37:42.929585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-18 19:37:42.929613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-18 19:37:42.929647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-18 19:37:42.937835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output bob_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_2.
WARNING:tensorflow:Output bob_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_3.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
WARNING:tensorflow:Output eve_2 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_2.
WARNING:tensorflow:Output eve_3 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_3.
2024-04-18 19:37:46.304049: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9205 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4573 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1464 - val_loss: 3.9164e-04
Epoch 4/512
448/448 - 0s - loss: 0.0224 - val_loss: 4.0186e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5937e-06
Epoch 6/512
448/448 - 0s - loss: 7.4149e-04 - val_loss: 5.8664e-06
Epoch 7/512
448/448 - 0s - loss: 5.3884e-04 - val_loss: 4.4158e-06
Epoch 8/512
448/448 - 0s - loss: 3.9852e-04 - val_loss: 3.1151e-06
Epoch 9/512
448/448 - 0s - loss: 2.7451e-04 - val_loss: 2.0187e-06
Epoch 10/512
448/448 - 0s - loss: 1.7277e-04 - val_loss: 1.1766e-06
Epoch 11/512
448/448 - 0s - loss: 9.7130e-05 - val_loss: 5.9894e-07
Epoch 12/512
448/448 - 0s - loss: 4.7332e-05 - val_loss: 2.5765e-07
Epoch 13/512
448/448 - 0s - loss: 1.9300e-05 - val_loss: 8.9160e-08
Epoch 14/512
448/448 - 0s - loss: 6.2632e-06 - val_loss: 2.3404e-08
Epoch 15/512
448/448 - 0s - loss: 1.5244e-06 - val_loss: 4.3537e-09
Epoch 16/512
448/448 - 0s - loss: 3.0794e-07 - val_loss: 1.5300e-08
Epoch 17/512
448/448 - 0s - loss: 1.3937e-04 - val_loss: 5.1577e-05
Epoch 18/512
448/448 - 0s - loss: 0.0056 - val_loss: 3.8935e-06
Epoch 19/512
448/448 - 0s - loss: 1.8238e-04 - val_loss: 3.8274e-07
Epoch 20/512
448/448 - 0s - loss: 4.3274e-05 - val_loss: 1.1203e-06
Epoch 21/512
448/448 - 0s - loss: 4.1707e-04 - val_loss: 2.7183e-05
Epoch 22/512
448/448 - 0s - loss: 0.0031 - val_loss: 1.1233e-05
Epoch 23/512
448/448 - 0s - loss: 6.6229e-04 - val_loss: 2.8858e-06
Epoch 24/512
448/448 - 0s - loss: 3.4782e-04 - val_loss: 7.8904e-06
Epoch 25/512
448/448 - 0s - loss: 0.0014 - val_loss: 2.2150e-05
Epoch 26/512
448/448 - 0s - loss: 0.0016 - val_loss: 7.4885e-06
Epoch 27/512
448/448 - 0s - loss: 6.3092e-04 - val_loss: 6.5664e-06
Epoch 28/512
448/448 - 0s - loss: 8.5386e-04 - val_loss: 1.4334e-05
Epoch 29/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.1748e-05
Epoch 30/512
448/448 - 0s - loss: 9.5380e-04 - val_loss: 7.6756e-06
Epoch 31/512
448/448 - 0s - loss: 7.9405e-04 - val_loss: 1.0461e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1915e-05
Epoch 33/512
448/448 - 0s - loss: 0.0011 - val_loss: 8.9522e-06
Epoch 34/512
448/448 - 0s - loss: 8.6093e-04 - val_loss: 8.9512e-06
Epoch 35/512
448/448 - 0s - loss: 9.4089e-04 - val_loss: 1.0596e-05
Epoch 36/512
448/448 - 0s - loss: 0.0010 - val_loss: 9.6537e-06
Epoch 37/512
448/448 - 0s - loss: 9.0717e-04 - val_loss: 8.8013e-06
Epoch 38/512
448/448 - 0s - loss: 8.8518e-04 - val_loss: 9.3626e-06
Epoch 39/512
448/448 - 0s - loss: 9.3284e-04 - val_loss: 9.6163e-06
Epoch 40/512
448/448 - 0s - loss: 9.1583e-04 - val_loss: 9.0738e-06
Epoch 41/512
448/448 - 0s - loss: 8.7210e-04 - val_loss: 8.9042e-06
Epoch 42/512
448/448 - 0s - loss: 8.7415e-04 - val_loss: 8.9644e-06
Epoch 43/512
448/448 - 0s - loss: 8.7404e-04 - val_loss: 8.8179e-06
Epoch 44/512
448/448 - 0s - loss: 8.5644e-04 - val_loss: 8.5402e-06
Epoch 45/512
448/448 - 0s - loss: 8.3937e-04 - val_loss: 8.4010e-06
Epoch 46/512
448/448 - 0s - loss: 8.3178e-04 - val_loss: 8.4226e-06
Epoch 47/512
448/448 - 0s - loss: 8.2880e-04 - val_loss: 8.3174e-06
Epoch 48/512
448/448 - 0s - loss: 8.1964e-04 - val_loss: 8.0355e-06
Epoch 49/512
448/448 - 0s - loss: 7.9105e-04 - val_loss: 8.1995e-06
Epoch 50/512
448/448 - 0s - loss: 8.0787e-04 - val_loss: 8.1736e-06
Epoch 51/512
448/448 - 0s - loss: 7.9028e-04 - val_loss: 7.8750e-06
Epoch 52/512
448/448 - 0s - loss: 7.6556e-04 - val_loss: 7.9531e-06
Epoch 53/512
448/448 - 0s - loss: 7.7912e-04 - val_loss: 7.8580e-06
Epoch 54/512
448/448 - 0s - loss: 7.6154e-04 - val_loss: 7.6360e-06
Epoch 55/512
448/448 - 0s - loss: 7.4464e-04 - val_loss: 7.6836e-06
Epoch 56/512
448/448 - 0s - loss: 7.5490e-04 - val_loss: 7.4465e-06
Epoch 57/512
448/448 - 0s - loss: 7.2990e-04 - val_loss: 7.3245e-06
Epoch 58/512
448/448 - 0s - loss: 7.2402e-04 - val_loss: 7.4050e-06
Epoch 59/512
448/448 - 0s - loss: 7.3157e-04 - val_loss: 7.1263e-06
Epoch 60/512
448/448 - 0s - loss: 6.9818e-04 - val_loss: 7.1603e-06
Epoch 61/512
448/448 - 0s - loss: 7.0868e-04 - val_loss: 7.1861e-06
Epoch 62/512
448/448 - 0s - loss: 7.0357e-04 - val_loss: 6.9827e-06
Epoch 63/512
448/448 - 0s - loss: 6.8565e-04 - val_loss: 6.8226e-06
Epoch 64/512
448/448 - 0s - loss: 6.7526e-04 - val_loss: 6.9483e-06
Epoch 65/512
448/448 - 0s - loss: 6.8633e-04 - val_loss: 6.7907e-06
Epoch 66/512
448/448 - 0s - loss: 6.6311e-04 - val_loss: 6.7083e-06
Epoch 67/512
448/448 - 0s - loss: 6.6285e-04 - val_loss: 6.6226e-06
Epoch 68/512
448/448 - 0s - loss: 6.5246e-04 - val_loss: 6.6045e-06
Epoch 69/512
448/448 - 0s - loss: 6.5213e-04 - val_loss: 6.4943e-06
Epoch 70/512
448/448 - 0s - loss: 6.3883e-04 - val_loss: 6.4552e-06
Epoch 71/512
448/448 - 0s - loss: 6.3260e-04 - val_loss: 6.5628e-06
Epoch 72/512
448/448 - 0s - loss: 6.4135e-04 - val_loss: 6.3312e-06
Epoch 73/512
448/448 - 0s - loss: 6.1855e-04 - val_loss: 6.1164e-06
Epoch 74/512
448/448 - 0s - loss: 6.0752e-04 - val_loss: 6.2718e-06
Epoch 75/512
448/448 - 0s - loss: 6.2291e-04 - val_loss: 6.1668e-06
Epoch 76/512
448/448 - 0s - loss: 6.0055e-04 - val_loss: 6.1157e-06
Epoch 77/512
448/448 - 0s - loss: 6.0583e-04 - val_loss: 5.9271e-06
Epoch 78/512
448/448 - 0s - loss: 5.8509e-04 - val_loss: 5.9590e-06
Epoch 79/512
448/448 - 0s - loss: 5.9202e-04 - val_loss: 5.9418e-06
Epoch 80/512
448/448 - 0s - loss: 5.8268e-04 - val_loss: 5.9115e-06
Epoch 81/512
448/448 - 0s - loss: 5.7975e-04 - val_loss: 5.7949e-06
Epoch 82/512
448/448 - 0s - loss: 5.7153e-04 - val_loss: 5.7059e-06
Epoch 83/512
448/448 - 0s - loss: 5.6682e-04 - val_loss: 5.6003e-06
Epoch 84/512
448/448 - 0s - loss: 5.5395e-04 - val_loss: 5.6804e-06
Epoch 85/512
448/448 - 0s - loss: 5.6068e-04 - val_loss: 5.7013e-06
Epoch 86/512
448/448 - 0s - loss: 5.5963e-04 - val_loss: 5.4214e-06
Epoch 87/512
448/448 - 0s - loss: 5.3751e-04 - val_loss: 5.3231e-06
Epoch 88/512
448/448 - 0s - loss: 5.3457e-04 - val_loss: 5.4969e-06
Epoch 89/512
448/448 - 0s - loss: 5.4670e-04 - val_loss: 5.4183e-06
Epoch 90/512
448/448 - 0s - loss: 5.3012e-04 - val_loss: 5.2139e-06
Epoch 91/512
448/448 - 0s - loss: 5.1820e-04 - val_loss: 5.2662e-06
Epoch 92/512
448/448 - 0s - loss: 5.2612e-04 - val_loss: 5.2694e-06
Epoch 93/512
448/448 - 0s - loss: 5.1944e-04 - val_loss: 5.1641e-06
Epoch 94/512
448/448 - 0s - loss: 5.1299e-04 - val_loss: 4.9870e-06
Epoch 95/512
448/448 - 0s - loss: 4.9841e-04 - val_loss: 5.1101e-06
Epoch 96/512
448/448 - 0s - loss: 5.1302e-04 - val_loss: 5.0365e-06
Epoch 97/512
448/448 - 0s - loss: 4.9697e-04 - val_loss: 4.9251e-06
Epoch 98/512
448/448 - 0s - loss: 4.8881e-04 - val_loss: 5.0209e-06
Epoch 99/512
448/448 - 0s - loss: 4.9803e-04 - val_loss: 4.9260e-06
Epoch 100/512
448/448 - 0s - loss: 4.8662e-04 - val_loss: 4.7086e-06
Epoch 101/512
448/448 - 0s - loss: 4.6905e-04 - val_loss: 4.8549e-06
Epoch 102/512
448/448 - 0s - loss: 4.8620e-04 - val_loss: 4.8137e-06
Epoch 103/512
448/448 - 0s - loss: 4.7330e-04 - val_loss: 4.6832e-06
Epoch 104/512
448/448 - 0s - loss: 4.6591e-04 - val_loss: 4.6024e-06
Epoch 105/512
448/448 - 0s - loss: 4.6128e-04 - val_loss: 4.6072e-06
Epoch 106/512
448/448 - 0s - loss: 4.6194e-04 - val_loss: 4.5477e-06
Epoch 107/512
448/448 - 0s - loss: 4.5182e-04 - val_loss: 4.5698e-06
Epoch 108/512
448/448 - 0s - loss: 4.5243e-04 - val_loss: 4.6049e-06
Epoch 109/512
448/448 - 0s - loss: 4.5882e-04 - val_loss: 4.2796e-06
Epoch 110/512
448/448 - 0s - loss: 4.2723e-04 - val_loss: 4.3552e-06
Epoch 111/512
448/448 - 0s - loss: 4.4239e-04 - val_loss: 4.5162e-06
Epoch 112/512
448/448 - 0s - loss: 4.4657e-04 - val_loss: 4.2980e-06
Epoch 113/512
448/448 - 0s - loss: 4.2452e-04 - val_loss: 4.2090e-06
Epoch 114/512
448/448 - 0s - loss: 4.2488e-04 - val_loss: 4.3156e-06
Epoch 115/512
448/448 - 0s - loss: 4.3064e-04 - val_loss: 4.2500e-06
Epoch 116/512
448/448 - 0s - loss: 4.2197e-04 - val_loss: 4.0882e-06
Epoch 117/512
448/448 - 0s - loss: 4.0705e-04 - val_loss: 4.2171e-06
Epoch 118/512
448/448 - 0s - loss: 4.2241e-04 - val_loss: 4.1417e-06
Epoch 119/512
448/448 - 0s - loss: 4.0858e-04 - val_loss: 3.9964e-06
Epoch 120/512
448/448 - 0s - loss: 3.9819e-04 - val_loss: 4.0639e-06
Epoch 121/512
448/448 - 0s - loss: 4.0732e-04 - val_loss: 4.0217e-06
Epoch 122/512
448/448 - 0s - loss: 3.9591e-04 - val_loss: 3.9890e-06
Epoch 123/512
448/448 - 0s - loss: 3.9592e-04 - val_loss: 3.9524e-06
Epoch 124/512
448/448 - 0s - loss: 3.9140e-04 - val_loss: 3.9148e-06
Epoch 125/512
448/448 - 0s - loss: 3.8929e-04 - val_loss: 3.8523e-06
Epoch 126/512
448/448 - 0s - loss: 3.8048e-04 - val_loss: 3.8897e-06
Epoch 127/512
448/448 - 0s - loss: 3.8614e-04 - val_loss: 3.7738e-06
Epoch 128/512
448/448 - 0s - loss: 3.7376e-04 - val_loss: 3.6806e-06
Epoch 129/512
448/448 - 0s - loss: 3.6854e-04 - val_loss: 3.7969e-06
Epoch 130/512
448/448 - 0s - loss: 3.7864e-04 - val_loss: 3.6704e-06
Epoch 131/512
448/448 - 0s - loss: 3.6477e-04 - val_loss: 3.5323e-06
Epoch 132/512
448/448 - 0s - loss: 3.5390e-04 - val_loss: 3.7019e-06
Epoch 133/512
448/448 - 0s - loss: 3.6988e-04 - val_loss: 3.6868e-06
Epoch 134/512
448/448 - 0s - loss: 3.5953e-04 - val_loss: 3.4469e-06
Epoch 135/512
448/448 - 0s - loss: 3.4288e-04 - val_loss: 3.5211e-06
Epoch 136/512
448/448 - 0s - loss: 3.5594e-04 - val_loss: 3.5004e-06
Epoch 137/512
448/448 - 0s - loss: 3.4650e-04 - val_loss: 3.4597e-06
Epoch 138/512
448/448 - 0s - loss: 3.4207e-04 - val_loss: 3.4575e-06
Epoch 139/512
448/448 - 0s - loss: 3.4161e-04 - val_loss: 3.4189e-06
Epoch 140/512
448/448 - 0s - loss: 3.3649e-04 - val_loss: 3.3632e-06
Epoch 141/512
448/448 - 0s - loss: 3.3470e-04 - val_loss: 3.3095e-06
Epoch 142/512
448/448 - 0s - loss: 3.2983e-04 - val_loss: 3.2845e-06
Epoch 143/512
448/448 - 0s - loss: 3.2785e-04 - val_loss: 3.2608e-06
Epoch 144/512
448/448 - 0s - loss: 3.2615e-04 - val_loss: 3.1669e-06
Epoch 145/512
448/448 - 0s - loss: 3.1764e-04 - val_loss: 3.1640e-06
Epoch 146/512
448/448 - 0s - loss: 3.2004e-04 - val_loss: 3.1515e-06
Epoch 147/512
448/448 - 0s - loss: 3.1528e-04 - val_loss: 3.0987e-06
Epoch 148/512
448/448 - 0s - loss: 3.1075e-04 - val_loss: 3.1059e-06
Epoch 149/512
448/448 - 0s - loss: 3.1133e-04 - val_loss: 3.0708e-06
Epoch 150/512
448/448 - 0s - loss: 3.0616e-04 - val_loss: 3.0250e-06
Epoch 151/512
448/448 - 0s - loss: 3.0116e-04 - val_loss: 3.0561e-06
Epoch 152/512
448/448 - 0s - loss: 3.0490e-04 - val_loss: 2.9616e-06
Epoch 153/512
448/448 - 0s - loss: 2.9247e-04 - val_loss: 2.9573e-06
Epoch 154/512
448/448 - 0s - loss: 2.9609e-04 - val_loss: 2.9560e-06
Epoch 155/512
448/448 - 0s - loss: 2.9327e-04 - val_loss: 2.8544e-06
Epoch 156/512
448/448 - 0s - loss: 2.8443e-04 - val_loss: 2.8670e-06
Epoch 157/512
448/448 - 0s - loss: 2.8692e-04 - val_loss: 2.8681e-06
Epoch 158/512
448/448 - 0s - loss: 2.8599e-04 - val_loss: 2.7608e-06
Epoch 159/512
448/448 - 0s - loss: 2.7467e-04 - val_loss: 2.7948e-06
Epoch 160/512
448/448 - 0s - loss: 2.8227e-04 - val_loss: 2.7391e-06
Epoch 161/512
448/448 - 0s - loss: 2.7322e-04 - val_loss: 2.6490e-06
Epoch 162/512
448/448 - 0s - loss: 2.6774e-04 - val_loss: 2.6945e-06
Epoch 163/512
448/448 - 0s - loss: 2.7219e-04 - val_loss: 2.6992e-06
Epoch 164/512
448/448 - 0s - loss: 2.6667e-04 - val_loss: 2.6406e-06
Epoch 165/512
448/448 - 0s - loss: 2.6212e-04 - val_loss: 2.6078e-06
Epoch 166/512
448/448 - 0s - loss: 2.6121e-04 - val_loss: 2.5999e-06
Epoch 167/512
448/448 - 0s - loss: 2.5993e-04 - val_loss: 2.5234e-06
Epoch 168/512
448/448 - 0s - loss: 2.5171e-04 - val_loss: 2.5316e-06
Epoch 169/512
448/448 - 0s - loss: 2.5506e-04 - val_loss: 2.4965e-06
Epoch 170/512
448/448 - 0s - loss: 2.4863e-04 - val_loss: 2.4755e-06
Epoch 171/512
448/448 - 0s - loss: 2.4813e-04 - val_loss: 2.4188e-06
Epoch 172/512
448/448 - 0s - loss: 2.4324e-04 - val_loss: 2.4140e-06
Epoch 173/512
448/448 - 0s - loss: 2.4262e-04 - val_loss: 2.3935e-06
Epoch 174/512
448/448 - 0s - loss: 2.4024e-04 - val_loss: 2.3461e-06
Epoch 175/512
448/448 - 0s - loss: 2.3568e-04 - val_loss: 2.3318e-06
Epoch 176/512
448/448 - 0s - loss: 2.3547e-04 - val_loss: 2.3016e-06
Epoch 177/512
448/448 - 0s - loss: 2.3047e-04 - val_loss: 2.3009e-06
Epoch 178/512
448/448 - 0s - loss: 2.2909e-04 - val_loss: 2.3179e-06
Epoch 179/512
448/448 - 0s - loss: 2.3051e-04 - val_loss: 2.2086e-06
Epoch 180/512
448/448 - 0s - loss: 2.1855e-04 - val_loss: 2.2334e-06
Epoch 181/512
448/448 - 0s - loss: 2.2507e-04 - val_loss: 2.2133e-06
Epoch 182/512
448/448 - 0s - loss: 2.1906e-04 - val_loss: 2.1705e-06
Epoch 183/512
448/448 - 0s - loss: 2.1689e-04 - val_loss: 2.1049e-06
Epoch 184/512
448/448 - 0s - loss: 2.1108e-04 - val_loss: 2.1309e-06
Epoch 185/512
448/448 - 0s - loss: 2.1345e-04 - val_loss: 2.1430e-06
Epoch 186/512
448/448 - 0s - loss: 2.1213e-04 - val_loss: 2.0662e-06
Epoch 187/512
448/448 - 0s - loss: 2.0599e-04 - val_loss: 1.9983e-06
Epoch 188/512
448/448 - 0s - loss: 2.0099e-04 - val_loss: 2.0700e-06
Epoch 189/512
448/448 - 0s - loss: 2.0657e-04 - val_loss: 2.0270e-06
Epoch 190/512
448/448 - 0s - loss: 1.9958e-04 - val_loss: 1.9375e-06
Epoch 191/512
448/448 - 0s - loss: 1.9382e-04 - val_loss: 1.9582e-06
Epoch 192/512
448/448 - 0s - loss: 1.9667e-04 - val_loss: 1.9633e-06
Epoch 193/512
448/448 - 0s - loss: 1.9490e-04 - val_loss: 1.8821e-06
Epoch 194/512
448/448 - 0s - loss: 1.8790e-04 - val_loss: 1.8450e-06
Epoch 195/512
448/448 - 0s - loss: 1.8661e-04 - val_loss: 1.8671e-06
Epoch 196/512
448/448 - 0s - loss: 1.8908e-04 - val_loss: 1.8104e-06
Epoch 197/512
448/448 - 0s - loss: 1.8146e-04 - val_loss: 1.7701e-06
Epoch 198/512
448/448 - 0s - loss: 1.7954e-04 - val_loss: 1.8017e-06
Epoch 199/512
448/448 - 0s - loss: 1.8127e-04 - val_loss: 1.7840e-06
Epoch 200/512
448/448 - 0s - loss: 1.7738e-04 - val_loss: 1.7193e-06
Epoch 201/512
448/448 - 0s - loss: 1.7260e-04 - val_loss: 1.7046e-06
Epoch 202/512
448/448 - 0s - loss: 1.7160e-04 - val_loss: 1.7205e-06
Epoch 203/512
448/448 - 0s - loss: 1.7300e-04 - val_loss: 1.6404e-06
Epoch 204/512
448/448 - 0s - loss: 1.6391e-04 - val_loss: 1.6364e-06
Epoch 205/512
448/448 - 0s - loss: 1.6619e-04 - val_loss: 1.6464e-06
Epoch 206/512
448/448 - 0s - loss: 1.6461e-04 - val_loss: 1.6186e-06
Epoch 207/512
448/448 - 0s - loss: 1.6119e-04 - val_loss: 1.5675e-06
Epoch 208/512
448/448 - 0s - loss: 1.5737e-04 - val_loss: 1.5700e-06
Epoch 209/512
448/448 - 0s - loss: 1.5773e-04 - val_loss: 1.5612e-06
Epoch 210/512
448/448 - 0s - loss: 1.5589e-04 - val_loss: 1.4928e-06
Epoch 211/512
448/448 - 0s - loss: 1.4798e-04 - val_loss: 1.5561e-06
Epoch 212/512
448/448 - 0s - loss: 1.5693e-04 - val_loss: 1.4537e-06
Epoch 213/512
448/448 - 0s - loss: 1.4355e-04 - val_loss: 1.4195e-06
Epoch 214/512
448/448 - 0s - loss: 1.4513e-04 - val_loss: 1.4656e-06
Epoch 215/512
448/448 - 0s - loss: 1.4777e-04 - val_loss: 1.3945e-06
Epoch 216/512
448/448 - 0s - loss: 1.3878e-04 - val_loss: 1.3461e-06
Epoch 217/512
448/448 - 0s - loss: 1.3750e-04 - val_loss: 1.4053e-06
Epoch 218/512
448/448 - 0s - loss: 1.4102e-04 - val_loss: 1.3777e-06
Epoch 219/512
448/448 - 0s - loss: 1.3701e-04 - val_loss: 1.2653e-06
Epoch 220/512
448/448 - 0s - loss: 1.2848e-04 - val_loss: 1.2814e-06
Epoch 221/512
448/448 - 0s - loss: 1.3149e-04 - val_loss: 1.3677e-06
Epoch 222/512
448/448 - 0s - loss: 1.3601e-04 - val_loss: 1.2383e-06
Epoch 223/512
448/448 - 0s - loss: 1.2264e-04 - val_loss: 1.1818e-06
Epoch 224/512
448/448 - 0s - loss: 1.2214e-04 - val_loss: 1.2752e-06
Epoch 225/512
448/448 - 0s - loss: 1.2803e-04 - val_loss: 1.2642e-06
Epoch 226/512
448/448 - 0s - loss: 1.2296e-04 - val_loss: 1.1649e-06
Epoch 227/512
448/448 - 0s - loss: 1.1581e-04 - val_loss: 1.1615e-06
Epoch 228/512
448/448 - 0s - loss: 1.1867e-04 - val_loss: 1.1644e-06
Epoch 229/512
448/448 - 0s - loss: 1.1630e-04 - val_loss: 1.1357e-06
Epoch 230/512
448/448 - 0s - loss: 1.1400e-04 - val_loss: 1.0887e-06
Epoch 231/512
448/448 - 0s - loss: 1.0949e-04 - val_loss: 1.1124e-06
Epoch 232/512
448/448 - 0s - loss: 1.1195e-04 - val_loss: 1.0942e-06
Epoch 233/512
448/448 - 0s - loss: 1.0853e-04 - val_loss: 1.0428e-06
Epoch 234/512
448/448 - 0s - loss: 1.0427e-04 - val_loss: 1.0343e-06
Epoch 235/512
448/448 - 0s - loss: 1.0482e-04 - val_loss: 1.0239e-06
Epoch 236/512
448/448 - 0s - loss: 1.0274e-04 - val_loss: 9.9530e-07
Epoch 237/512
448/448 - 0s - loss: 1.0018e-04 - val_loss: 9.7101e-07
Epoch 238/512
448/448 - 0s - loss: 9.8583e-05 - val_loss: 9.5572e-07
Epoch 239/512
448/448 - 0s - loss: 9.6451e-05 - val_loss: 9.6194e-07
Epoch 240/512
448/448 - 0s - loss: 9.6102e-05 - val_loss: 9.4492e-07
Epoch 241/512
448/448 - 0s - loss: 9.4178e-05 - val_loss: 8.9800e-07
Epoch 242/512
448/448 - 0s - loss: 9.0129e-05 - val_loss: 8.8975e-07
Epoch 243/512
448/448 - 0s - loss: 9.0032e-05 - val_loss: 8.9470e-07
Epoch 244/512
448/448 - 0s - loss: 8.9571e-05 - val_loss: 8.5753e-07
Epoch 245/512
448/448 - 0s - loss: 8.6119e-05 - val_loss: 8.1366e-07
Epoch 246/512
448/448 - 0s - loss: 8.2602e-05 - val_loss: 8.4848e-07
Epoch 247/512
448/448 - 0s - loss: 8.6004e-05 - val_loss: 8.2273e-07
Epoch 248/512
448/448 - 0s - loss: 8.1858e-05 - val_loss: 7.5679e-07
Epoch 249/512
448/448 - 0s - loss: 7.6701e-05 - val_loss: 7.8008e-07
Epoch 250/512
448/448 - 0s - loss: 8.0431e-05 - val_loss: 7.6777e-07
Epoch 251/512
448/448 - 0s - loss: 7.6413e-05 - val_loss: 7.4819e-07
Epoch 252/512
448/448 - 0s - loss: 7.5163e-05 - val_loss: 7.2398e-07
Epoch 253/512
448/448 - 0s - loss: 7.2897e-05 - val_loss: 7.1926e-07
Epoch 254/512
448/448 - 0s - loss: 7.2361e-05 - val_loss: 7.1259e-07
Epoch 255/512
448/448 - 0s - loss: 7.1169e-05 - val_loss: 6.8368e-07
Epoch 256/512
448/448 - 0s - loss: 6.8432e-05 - val_loss: 6.6401e-07
Epoch 257/512
448/448 - 0s - loss: 6.7431e-05 - val_loss: 6.5028e-07
Epoch 258/512
448/448 - 0s - loss: 6.5964e-05 - val_loss: 6.4133e-07
Epoch 259/512
448/448 - 0s - loss: 6.5172e-05 - val_loss: 6.0917e-07
Epoch 260/512
448/448 - 0s - loss: 6.2134e-05 - val_loss: 6.0102e-07
Epoch 261/512
448/448 - 0s - loss: 6.1266e-05 - val_loss: 6.1855e-07
Epoch 262/512
448/448 - 0s - loss: 6.2100e-05 - val_loss: 5.8776e-07
Epoch 263/512
448/448 - 0s - loss: 5.8170e-05 - val_loss: 5.5504e-07
Epoch 264/512
448/448 - 0s - loss: 5.6608e-05 - val_loss: 5.5138e-07
Epoch 265/512
448/448 - 0s - loss: 5.6190e-05 - val_loss: 5.5920e-07
Epoch 266/512
448/448 - 0s - loss: 5.5902e-05 - val_loss: 5.3469e-07
Epoch 267/512
448/448 - 0s - loss: 5.3382e-05 - val_loss: 5.0040e-07
Epoch 268/512
448/448 - 0s - loss: 5.1110e-05 - val_loss: 5.0039e-07
Epoch 269/512
448/448 - 0s - loss: 5.1583e-05 - val_loss: 4.9782e-07
Epoch 270/512
448/448 - 0s - loss: 5.0041e-05 - val_loss: 4.8035e-07
Epoch 271/512
448/448 - 0s - loss: 4.8481e-05 - val_loss: 4.6150e-07
Epoch 272/512
448/448 - 0s - loss: 4.6814e-05 - val_loss: 4.5717e-07
Epoch 273/512
448/448 - 0s - loss: 4.6539e-05 - val_loss: 4.4608e-07
Epoch 274/512
448/448 - 0s - loss: 4.4870e-05 - val_loss: 4.3601e-07
Epoch 275/512
448/448 - 0s - loss: 4.4076e-05 - val_loss: 4.1945e-07
Epoch 276/512
448/448 - 0s - loss: 4.2415e-05 - val_loss: 4.1400e-07
Epoch 277/512
448/448 - 0s - loss: 4.1979e-05 - val_loss: 3.9579e-07
Epoch 278/512
448/448 - 0s - loss: 4.0030e-05 - val_loss: 3.8567e-07
Epoch 279/512
448/448 - 0s - loss: 3.9230e-05 - val_loss: 3.8729e-07
Epoch 280/512
448/448 - 0s - loss: 3.9273e-05 - val_loss: 3.6385e-07
Epoch 281/512
448/448 - 0s - loss: 3.6628e-05 - val_loss: 3.5337e-07
Epoch 282/512
448/448 - 0s - loss: 3.6149e-05 - val_loss: 3.5878e-07
Epoch 283/512
448/448 - 0s - loss: 3.5872e-05 - val_loss: 3.5413e-07
Epoch 284/512
448/448 - 0s - loss: 3.5176e-05 - val_loss: 3.1738e-07
Epoch 285/512
448/448 - 0s - loss: 3.2056e-05 - val_loss: 3.1673e-07
Epoch 286/512
448/448 - 0s - loss: 3.2764e-05 - val_loss: 3.2437e-07
Epoch 287/512
448/448 - 0s - loss: 3.2584e-05 - val_loss: 3.0121e-07
Epoch 288/512
448/448 - 0s - loss: 3.0095e-05 - val_loss: 2.8720e-07
Epoch 289/512
448/448 - 0s - loss: 2.9476e-05 - val_loss: 2.8925e-07
Epoch 290/512
448/448 - 0s - loss: 2.9192e-05 - val_loss: 2.9008e-07
Epoch 291/512
448/448 - 0s - loss: 2.8885e-05 - val_loss: 2.6557e-07
Epoch 292/512
448/448 - 0s - loss: 2.6640e-05 - val_loss: 2.5372e-07
Epoch 293/512
448/448 - 0s - loss: 2.6118e-05 - val_loss: 2.5897e-07
Epoch 294/512
448/448 - 0s - loss: 2.6435e-05 - val_loss: 2.5144e-07
Epoch 295/512
448/448 - 0s - loss: 2.5189e-05 - val_loss: 2.2818e-07
Epoch 296/512
448/448 - 0s - loss: 2.3214e-05 - val_loss: 2.2976e-07
Epoch 297/512
448/448 - 0s - loss: 2.3850e-05 - val_loss: 2.3276e-07
Epoch 298/512
448/448 - 0s - loss: 2.3296e-05 - val_loss: 2.1695e-07
Epoch 299/512
448/448 - 0s - loss: 2.1809e-05 - val_loss: 2.0582e-07
Epoch 300/512
448/448 - 0s - loss: 2.1178e-05 - val_loss: 2.0273e-07
Epoch 301/512
448/448 - 0s - loss: 2.0633e-05 - val_loss: 2.0416e-07
Epoch 302/512
448/448 - 0s - loss: 2.0748e-05 - val_loss: 1.8801e-07
Epoch 303/512
448/448 - 0s - loss: 1.8939e-05 - val_loss: 1.8171e-07
Epoch 304/512
448/448 - 0s - loss: 1.8775e-05 - val_loss: 1.8139e-07
Epoch 305/512
448/448 - 0s - loss: 1.8478e-05 - val_loss: 1.7513e-07
Epoch 306/512
448/448 - 0s - loss: 1.7699e-05 - val_loss: 1.6499e-07
Epoch 307/512
448/448 - 0s - loss: 1.6938e-05 - val_loss: 1.5907e-07
Epoch 308/512
448/448 - 0s - loss: 1.6274e-05 - val_loss: 1.6100e-07
Epoch 309/512
448/448 - 0s - loss: 1.6427e-05 - val_loss: 1.5324e-07
Epoch 310/512
448/448 - 0s - loss: 1.5395e-05 - val_loss: 1.4373e-07
Epoch 311/512
448/448 - 0s - loss: 1.4649e-05 - val_loss: 1.4425e-07
Epoch 312/512
448/448 - 0s - loss: 1.4748e-05 - val_loss: 1.3996e-07
Epoch 313/512
448/448 - 0s - loss: 1.4052e-05 - val_loss: 1.3080e-07
Epoch 314/512
448/448 - 0s - loss: 1.3235e-05 - val_loss: 1.2841e-07
Epoch 315/512
448/448 - 0s - loss: 1.3209e-05 - val_loss: 1.2423e-07
Epoch 316/512
448/448 - 0s - loss: 1.2560e-05 - val_loss: 1.1997e-07
Epoch 317/512
448/448 - 0s - loss: 1.2233e-05 - val_loss: 1.1499e-07
Epoch 318/512
448/448 - 0s - loss: 1.1641e-05 - val_loss: 1.1326e-07
Epoch 319/512
448/448 - 0s - loss: 1.1568e-05 - val_loss: 1.0646e-07
Epoch 320/512
448/448 - 0s - loss: 1.0743e-05 - val_loss: 1.0425e-07
Epoch 321/512
448/448 - 0s - loss: 1.0664e-05 - val_loss: 1.0154e-07
Epoch 322/512
448/448 - 0s - loss: 1.0174e-05 - val_loss: 9.8935e-08
Epoch 323/512
448/448 - 0s - loss: 9.9900e-06 - val_loss: 9.1999e-08
Epoch 324/512
448/448 - 0s - loss: 9.3903e-06 - val_loss: 8.6922e-08
Epoch 325/512
448/448 - 0s - loss: 8.9711e-06 - val_loss: 8.6688e-08
Epoch 326/512
448/448 - 0s - loss: 8.9070e-06 - val_loss: 8.6184e-08
Epoch 327/512
448/448 - 0s - loss: 8.7212e-06 - val_loss: 7.8310e-08
Epoch 328/512
448/448 - 0s - loss: 7.9067e-06 - val_loss: 7.6895e-08
Epoch 329/512
448/448 - 0s - loss: 7.9336e-06 - val_loss: 7.8011e-08
Epoch 330/512
448/448 - 0s - loss: 7.8296e-06 - val_loss: 7.1504e-08
Epoch 331/512
448/448 - 0s - loss: 7.1676e-06 - val_loss: 6.7445e-08
Epoch 332/512
448/448 - 0s - loss: 6.9830e-06 - val_loss: 6.6836e-08
Epoch 333/512
448/448 - 0s - loss: 6.8516e-06 - val_loss: 6.5327e-08
Epoch 334/512
448/448 - 0s - loss: 6.6459e-06 - val_loss: 6.0806e-08
Epoch 335/512
448/448 - 0s - loss: 6.1870e-06 - val_loss: 5.8630e-08
Epoch 336/512
448/448 - 0s - loss: 6.0295e-06 - val_loss: 5.7948e-08
Epoch 337/512
448/448 - 0s - loss: 5.9114e-06 - val_loss: 5.5181e-08
Epoch 338/512
448/448 - 0s - loss: 5.6377e-06 - val_loss: 5.0864e-08
Epoch 339/512
448/448 - 0s - loss: 5.2229e-06 - val_loss: 5.1525e-08
Epoch 340/512
448/448 - 0s - loss: 5.3533e-06 - val_loss: 4.9180e-08
Epoch 341/512
448/448 - 0s - loss: 4.9491e-06 - val_loss: 4.6178e-08
Epoch 342/512
448/448 - 0s - loss: 4.7296e-06 - val_loss: 4.5671e-08
Epoch 343/512
448/448 - 0s - loss: 4.6759e-06 - val_loss: 4.4359e-08
Epoch 344/512
448/448 - 0s - loss: 4.4855e-06 - val_loss: 4.0801e-08
Epoch 345/512
448/448 - 0s - loss: 4.1580e-06 - val_loss: 3.9344e-08
Epoch 346/512
448/448 - 0s - loss: 4.0937e-06 - val_loss: 3.8702e-08
Epoch 347/512
448/448 - 0s - loss: 3.9617e-06 - val_loss: 3.7366e-08
Epoch 348/512
448/448 - 0s - loss: 3.8000e-06 - val_loss: 3.4904e-08
Epoch 349/512
448/448 - 0s - loss: 3.5757e-06 - val_loss: 3.3844e-08
Epoch 350/512
448/448 - 0s - loss: 3.4833e-06 - val_loss: 3.3076e-08
Epoch 351/512
448/448 - 0s - loss: 3.3720e-06 - val_loss: 3.1537e-08
Epoch 352/512
448/448 - 0s - loss: 3.2098e-06 - val_loss: 2.9446e-08
Epoch 353/512
448/448 - 0s - loss: 3.0116e-06 - val_loss: 2.9310e-08
Epoch 354/512
448/448 - 0s - loss: 3.0162e-06 - val_loss: 2.8122e-08
Epoch 355/512
448/448 - 0s - loss: 2.8442e-06 - val_loss: 2.5529e-08
Epoch 356/512
448/448 - 0s - loss: 2.6308e-06 - val_loss: 2.4889e-08
Epoch 357/512
448/448 - 0s - loss: 2.6211e-06 - val_loss: 2.4771e-08
Epoch 358/512
448/448 - 0s - loss: 2.5525e-06 - val_loss: 2.3039e-08
Epoch 359/512
448/448 - 0s - loss: 2.3547e-06 - val_loss: 2.1657e-08
Epoch 360/512
448/448 - 0s - loss: 2.2340e-06 - val_loss: 2.2509e-08
Epoch 361/512
448/448 - 0s - loss: 2.3321e-06 - val_loss: 2.0359e-08
Epoch 362/512
448/448 - 0s - loss: 2.0490e-06 - val_loss: 1.8559e-08
Epoch 363/512
448/448 - 0s - loss: 1.9381e-06 - val_loss: 1.9463e-08
Epoch 364/512
448/448 - 0s - loss: 2.0295e-06 - val_loss: 1.8692e-08
Epoch 365/512
448/448 - 0s - loss: 1.8737e-06 - val_loss: 1.6426e-08
Epoch 366/512
448/448 - 0s - loss: 1.6996e-06 - val_loss: 1.6206e-08
Epoch 367/512
448/448 - 0s - loss: 1.7137e-06 - val_loss: 1.6393e-08
Epoch 368/512
448/448 - 0s - loss: 1.6757e-06 - val_loss: 1.5123e-08
Epoch 369/512
448/448 - 0s - loss: 1.5282e-06 - val_loss: 1.4118e-08
Epoch 370/512
448/448 - 0s - loss: 1.4725e-06 - val_loss: 1.4083e-08
Epoch 371/512
448/448 - 0s - loss: 1.4539e-06 - val_loss: 1.3421e-08
Epoch 372/512
448/448 - 0s - loss: 1.3718e-06 - val_loss: 1.2429e-08
Epoch 373/512
448/448 - 0s - loss: 1.2882e-06 - val_loss: 1.1955e-08
Epoch 374/512
448/448 - 0s - loss: 1.2520e-06 - val_loss: 1.1760e-08
Epoch 375/512
448/448 - 0s - loss: 1.2116e-06 - val_loss: 1.1080e-08
Epoch 376/512
448/448 - 0s - loss: 1.1441e-06 - val_loss: 1.0325e-08
Epoch 377/512
448/448 - 0s - loss: 1.0688e-06 - val_loss: 1.0321e-08
Epoch 378/512
448/448 - 0s - loss: 1.0688e-06 - val_loss: 1.0064e-08
Epoch 379/512
448/448 - 0s - loss: 1.0198e-06 - val_loss: 9.0152e-09
Epoch 380/512
448/448 - 0s - loss: 9.3123e-07 - val_loss: 8.5453e-09
Epoch 381/512
448/448 - 0s - loss: 9.0731e-07 - val_loss: 8.5404e-09
Epoch 382/512
448/448 - 0s - loss: 8.8415e-07 - val_loss: 8.3675e-09
Epoch 383/512
448/448 - 0s - loss: 8.5503e-07 - val_loss: 7.5452e-09
Epoch 384/512
448/448 - 0s - loss: 7.7574e-07 - val_loss: 7.2181e-09
Epoch 385/512
448/448 - 0s - loss: 7.5651e-07 - val_loss: 7.3062e-09
Epoch 386/512
448/448 - 0s - loss: 7.5238e-07 - val_loss: 6.8797e-09
Epoch 387/512
448/448 - 0s - loss: 6.9444e-07 - val_loss: 6.2873e-09
Epoch 388/512
448/448 - 0s - loss: 6.5390e-07 - val_loss: 6.1050e-09
Epoch 389/512
448/448 - 0s - loss: 6.3744e-07 - val_loss: 5.9743e-09
Epoch 390/512
448/448 - 0s - loss: 6.1911e-07 - val_loss: 5.5544e-09
Epoch 391/512
448/448 - 0s - loss: 5.7265e-07 - val_loss: 5.1486e-09
Epoch 392/512
448/448 - 0s - loss: 5.4030e-07 - val_loss: 5.1880e-09
Epoch 393/512
448/448 - 0s - loss: 5.4396e-07 - val_loss: 4.9746e-09
Epoch 394/512
448/448 - 0s - loss: 5.0775e-07 - val_loss: 4.5089e-09
Epoch 395/512
448/448 - 0s - loss: 4.7007e-07 - val_loss: 4.2746e-09
Epoch 396/512
448/448 - 0s - loss: 4.5228e-07 - val_loss: 4.3440e-09
Epoch 397/512
448/448 - 0s - loss: 4.5470e-07 - val_loss: 4.0854e-09
Epoch 398/512
448/448 - 0s - loss: 4.1952e-07 - val_loss: 3.6548e-09
Epoch 399/512
448/448 - 0s - loss: 3.7961e-07 - val_loss: 3.7692e-09
Epoch 400/512
448/448 - 0s - loss: 3.9820e-07 - val_loss: 3.6276e-09
Epoch 401/512
448/448 - 0s - loss: 3.6894e-07 - val_loss: 3.1273e-09
Epoch 402/512
448/448 - 0s - loss: 3.2447e-07 - val_loss: 3.1892e-09
Epoch 403/512
448/448 - 0s - loss: 3.4124e-07 - val_loss: 3.1955e-09
Epoch 404/512
448/448 - 0s - loss: 3.2550e-07 - val_loss: 2.8376e-09
Epoch 405/512
448/448 - 0s - loss: 2.9082e-07 - val_loss: 2.6497e-09
Epoch 406/512
448/448 - 0s - loss: 2.8019e-07 - val_loss: 2.7457e-09
Epoch 407/512
448/448 - 0s - loss: 2.8576e-07 - val_loss: 2.5826e-09
Epoch 408/512
448/448 - 0s - loss: 2.6138e-07 - val_loss: 2.2646e-09
Epoch 409/512
448/448 - 0s - loss: 2.3641e-07 - val_loss: 2.2340e-09
Epoch 410/512
448/448 - 0s - loss: 2.3924e-07 - val_loss: 2.2943e-09
Epoch 411/512
448/448 - 0s - loss: 2.3692e-07 - val_loss: 2.0206e-09
Epoch 412/512
448/448 - 0s - loss: 2.0731e-07 - val_loss: 1.8670e-09
Epoch 413/512
448/448 - 0s - loss: 1.9853e-07 - val_loss: 1.9548e-09
Epoch 414/512
448/448 - 0s - loss: 2.0535e-07 - val_loss: 1.8387e-09
Epoch 415/512
448/448 - 0s - loss: 1.8752e-07 - val_loss: 1.5979e-09
Epoch 416/512
448/448 - 0s - loss: 1.6717e-07 - val_loss: 1.6204e-09
Epoch 417/512
448/448 - 0s - loss: 1.7395e-07 - val_loss: 1.6301e-09
Epoch 418/512
448/448 - 0s - loss: 1.6593e-07 - val_loss: 1.4666e-09
Epoch 419/512
448/448 - 0s - loss: 1.4952e-07 - val_loss: 1.3560e-09
Epoch 420/512
448/448 - 0s - loss: 1.4440e-07 - val_loss: 1.3334e-09
Epoch 421/512
448/448 - 0s - loss: 1.4040e-07 - val_loss: 1.2794e-09
Epoch 422/512
448/448 - 0s - loss: 1.3265e-07 - val_loss: 1.2283e-09
Epoch 423/512
448/448 - 0s - loss: 1.2697e-07 - val_loss: 1.1668e-09
Epoch 424/512
448/448 - 0s - loss: 1.2103e-07 - val_loss: 1.0908e-09
Epoch 425/512
448/448 - 0s - loss: 1.1317e-07 - val_loss: 1.0457e-09
Epoch 426/512
448/448 - 0s - loss: 1.0911e-07 - val_loss: 1.0106e-09
Epoch 427/512
448/448 - 0s - loss: 1.0532e-07 - val_loss: 9.4769e-10
Epoch 428/512
448/448 - 0s - loss: 9.7792e-08 - val_loss: 8.9975e-10
Epoch 429/512
448/448 - 0s - loss: 9.3952e-08 - val_loss: 8.7699e-10
Epoch 430/512
448/448 - 0s - loss: 9.0986e-08 - val_loss: 8.1573e-10
Epoch 431/512
448/448 - 0s - loss: 8.3818e-08 - val_loss: 7.9457e-10
Epoch 432/512
448/448 - 0s - loss: 8.2773e-08 - val_loss: 7.4169e-10
Epoch 433/512
448/448 - 0s - loss: 7.6600e-08 - val_loss: 6.9967e-10
Epoch 434/512
448/448 - 0s - loss: 7.3275e-08 - val_loss: 6.7722e-10
Epoch 435/512
448/448 - 0s - loss: 7.0973e-08 - val_loss: 6.3707e-10
Epoch 436/512
448/448 - 0s - loss: 6.6399e-08 - val_loss: 5.9832e-10
Epoch 437/512
448/448 - 0s - loss: 6.2731e-08 - val_loss: 5.8967e-10
Epoch 438/512
448/448 - 0s - loss: 6.1690e-08 - val_loss: 5.6138e-10
Epoch 439/512
448/448 - 0s - loss: 5.7901e-08 - val_loss: 5.1126e-10
Epoch 440/512
448/448 - 0s - loss: 5.3652e-08 - val_loss: 4.9558e-10
Epoch 441/512
448/448 - 0s - loss: 5.1876e-08 - val_loss: 4.9928e-10
Epoch 442/512
448/448 - 0s - loss: 5.1610e-08 - val_loss: 4.5529e-10
Epoch 443/512
448/448 - 0s - loss: 4.6498e-08 - val_loss: 4.2293e-10
Epoch 444/512
448/448 - 0s - loss: 4.4483e-08 - val_loss: 4.0734e-10
Epoch 445/512
448/448 - 0s - loss: 4.2711e-08 - val_loss: 4.0138e-10
Epoch 446/512
448/448 - 0s - loss: 4.1850e-08 - val_loss: 3.7479e-10
Epoch 447/512
448/448 - 0s - loss: 3.8479e-08 - val_loss: 3.4595e-10
Epoch 448/512
448/448 - 0s - loss: 3.5933e-08 - val_loss: 3.4721e-10
Epoch 449/512
448/448 - 0s - loss: 3.6415e-08 - val_loss: 3.3015e-10
Epoch 450/512
448/448 - 0s - loss: 3.3627e-08 - val_loss: 3.0437e-10
Epoch 451/512
448/448 - 0s - loss: 3.1491e-08 - val_loss: 2.8620e-10
Epoch 452/512
448/448 - 0s - loss: 3.0319e-08 - val_loss: 2.7420e-10
Epoch 453/512
448/448 - 0s - loss: 2.8765e-08 - val_loss: 2.6876e-10
Epoch 454/512
448/448 - 0s - loss: 2.7850e-08 - val_loss: 2.6143e-10
Epoch 455/512
448/448 - 0s - loss: 2.6835e-08 - val_loss: 2.3779e-10
Epoch 456/512
448/448 - 0s - loss: 2.4482e-08 - val_loss: 2.2606e-10
Epoch 457/512
448/448 - 0s - loss: 2.3631e-08 - val_loss: 2.1981e-10
Epoch 458/512
448/448 - 0s - loss: 2.2898e-08 - val_loss: 2.1381e-10
Epoch 459/512
448/448 - 0s - loss: 2.1865e-08 - val_loss: 2.0008e-10
Epoch 460/512
448/448 - 0s - loss: 2.0662e-08 - val_loss: 1.8429e-10
Epoch 461/512
448/448 - 0s - loss: 1.9090e-08 - val_loss: 1.7737e-10
Epoch 462/512
448/448 - 0s - loss: 1.8699e-08 - val_loss: 1.7482e-10
Epoch 463/512
448/448 - 0s - loss: 1.8234e-08 - val_loss: 1.6370e-10
Epoch 464/512
448/448 - 0s - loss: 1.6867e-08 - val_loss: 1.5290e-10
Epoch 465/512
448/448 - 0s - loss: 1.5823e-08 - val_loss: 1.5286e-10
Epoch 466/512
448/448 - 0s - loss: 1.5931e-08 - val_loss: 1.4482e-10
Epoch 467/512
448/448 - 0s - loss: 1.4753e-08 - val_loss: 1.3425e-10
Epoch 468/512
448/448 - 0s - loss: 1.3903e-08 - val_loss: 1.2610e-10
Epoch 469/512
448/448 - 0s - loss: 1.3162e-08 - val_loss: 1.2538e-10
Epoch 470/512
448/448 - 0s - loss: 1.3079e-08 - val_loss: 1.1908e-10
Epoch 471/512
448/448 - 0s - loss: 1.2228e-08 - val_loss: 1.1081e-10
Epoch 472/512
448/448 - 0s - loss: 1.1467e-08 - val_loss: 1.0719e-10
Epoch 473/512
448/448 - 0s - loss: 1.1102e-08 - val_loss: 1.0476e-10
Epoch 474/512
448/448 - 0s - loss: 1.0906e-08 - val_loss: 9.6158e-11
Epoch 475/512
448/448 - 0s - loss: 9.9377e-09 - val_loss: 9.1648e-11
Epoch 476/512
448/448 - 0s - loss: 9.5181e-09 - val_loss: 9.0516e-11
Epoch 477/512
448/448 - 0s - loss: 9.4505e-09 - val_loss: 8.4538e-11
Epoch 478/512
448/448 - 0s - loss: 8.7303e-09 - val_loss: 7.9459e-11
Epoch 479/512
448/448 - 0s - loss: 8.2587e-09 - val_loss: 7.8086e-11
Epoch 480/512
448/448 - 0s - loss: 8.1114e-09 - val_loss: 7.6139e-11
Epoch 481/512
448/448 - 0s - loss: 7.8273e-09 - val_loss: 7.2735e-11
Epoch 482/512
448/448 - 0s - loss: 7.4544e-09 - val_loss: 6.6716e-11
Epoch 483/512
448/448 - 0s - loss: 6.8688e-09 - val_loss: 6.3544e-11
Epoch 484/512
448/448 - 0s - loss: 6.6249e-09 - val_loss: 6.1943e-11
Epoch 485/512
448/448 - 0s - loss: 6.4310e-09 - val_loss: 6.1906e-11
Epoch 486/512
448/448 - 0s - loss: 6.4036e-09 - val_loss: 5.8074e-11
Epoch 487/512
448/448 - 0s - loss: 5.9190e-09 - val_loss: 5.2864e-11
Epoch 488/512
448/448 - 0s - loss: 5.4374e-09 - val_loss: 5.1231e-11
Epoch 489/512
448/448 - 0s - loss: 5.3362e-09 - val_loss: 5.1850e-11
Epoch 490/512
448/448 - 0s - loss: 5.3777e-09 - val_loss: 4.9658e-11
Epoch 491/512
448/448 - 0s - loss: 5.0731e-09 - val_loss: 4.5573e-11
Epoch 492/512
448/448 - 0s - loss: 4.6286e-09 - val_loss: 4.3977e-11
Epoch 493/512
448/448 - 0s - loss: 4.5573e-09 - val_loss: 4.3525e-11
Epoch 494/512
448/448 - 0s - loss: 4.5010e-09 - val_loss: 4.1638e-11
Epoch 495/512
448/448 - 0s - loss: 4.2649e-09 - val_loss: 3.7969e-11
Epoch 496/512
448/448 - 0s - loss: 3.8883e-09 - val_loss: 3.6698e-11
Epoch 497/512
448/448 - 0s - loss: 3.8415e-09 - val_loss: 3.7326e-11
Epoch 498/512
448/448 - 0s - loss: 3.8677e-09 - val_loss: 3.5754e-11
Epoch 499/512
448/448 - 0s - loss: 3.6556e-09 - val_loss: 3.3186e-11
Epoch 500/512
448/448 - 0s - loss: 3.3988e-09 - val_loss: 3.1004e-11
Epoch 501/512
448/448 - 0s - loss: 3.1908e-09 - val_loss: 3.0981e-11
Epoch 502/512
448/448 - 0s - loss: 3.2197e-09 - val_loss: 3.0461e-11
Epoch 503/512
448/448 - 0s - loss: 3.1523e-09 - val_loss: 2.8921e-11
Epoch 504/512
448/448 - 0s - loss: 2.9548e-09 - val_loss: 2.7589e-11
Epoch 505/512
448/448 - 0s - loss: 2.8199e-09 - val_loss: 2.6740e-11
Epoch 506/512
448/448 - 0s - loss: 2.7253e-09 - val_loss: 2.6140e-11
Epoch 507/512
448/448 - 0s - loss: 2.6656e-09 - val_loss: 2.5315e-11
Epoch 508/512
448/448 - 0s - loss: 2.5813e-09 - val_loss: 2.3912e-11
Epoch 509/512
448/448 - 0s - loss: 2.4247e-09 - val_loss: 2.2917e-11
Epoch 510/512
448/448 - 0s - loss: 2.3511e-09 - val_loss: 2.2678e-11
Epoch 511/512
448/448 - 0s - loss: 2.3272e-09 - val_loss: 2.1652e-11
Epoch 512/512
448/448 - 0s - loss: 2.2018e-09 - val_loss: 2.0384e-11
2024-04-18 19:38:03.657229: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9347e-09 - val_loss: 1.8684e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9581e-09 - val_loss: 1.9504e-09
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0251e-09 - val_loss: 1.9296e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9483e-09 - val_loss: 1.7730e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7821e-09 - val_loss: 1.6743e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7154e-09 - val_loss: 1.6441e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7015e-09 - val_loss: 1.6219e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6489e-09 - val_loss: 1.5740e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6068e-09 - val_loss: 1.5197e-09
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5452e-09 - val_loss: 1.4510e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4721e-09 - val_loss: 1.4001e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4439e-09 - val_loss: 1.3672e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3988e-09 - val_loss: 1.3340e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3533e-09 - val_loss: 1.2743e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2964e-09 - val_loss: 1.2251e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2510e-09 - val_loss: 1.1921e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2242e-09 - val_loss: 1.1691e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2045e-09 - val_loss: 1.1574e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1723e-09 - val_loss: 1.1011e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1207e-09 - val_loss: 1.0724e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0924e-09 - val_loss: 1.0425e-09
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0573e-09 - val_loss: 1.0028e-09
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0239e-09 - val_loss: 9.8411e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0026e-09 - val_loss: 9.6458e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.8874e-10 - val_loss: 9.3459e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.5621e-10 - val_loss: 9.1447e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.3086e-10 - val_loss: 8.7339e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.8097e-10 - val_loss: 8.4150e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.5924e-10 - val_loss: 8.2359e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.4197e-10 - val_loss: 8.0164e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.2066e-10 - val_loss: 7.9572e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.1248e-10 - val_loss: 7.8232e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.9417e-10 - val_loss: 7.5639e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.6793e-10 - val_loss: 7.3133e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.4670e-10 - val_loss: 7.1493e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.2028e-10 - val_loss: 6.8686e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.9643e-10 - val_loss: 6.7502e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.7858e-10 - val_loss: 6.5554e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6641e-10 - val_loss: 6.4645e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6327e-10 - val_loss: 6.4019e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.4673e-10 - val_loss: 6.1581e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.2502e-10 - val_loss: 6.0377e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.1453e-10 - val_loss: 5.9966e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.0853e-10 - val_loss: 5.8231e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.8145e-10 - val_loss: 5.4998e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.6237e-10 - val_loss: 5.4185e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.4920e-10 - val_loss: 5.2938e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.4217e-10 - val_loss: 5.2634e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3144e-10 - val_loss: 5.0821e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.0991e-10 - val_loss: 4.8700e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9873e-10 - val_loss: 4.8768e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0015e-10 - val_loss: 4.9380e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.9604e-10 - val_loss: 4.7858e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.8660e-10 - val_loss: 4.6438e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.6564e-10 - val_loss: 4.4513e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.4744e-10 - val_loss: 4.3187e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.3419e-10 - val_loss: 4.1516e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.2201e-10 - val_loss: 4.1485e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2493e-10 - val_loss: 4.1853e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2858e-10 - val_loss: 4.1917e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2839e-10 - val_loss: 4.2179e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.2368e-10 - val_loss: 4.0411e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.0644e-10 - val_loss: 3.8613e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.8429e-10 - val_loss: 3.6850e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.7153e-10 - val_loss: 3.5627e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.6272e-10 - val_loss: 3.5393e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.6138e-10 - val_loss: 3.5341e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5586e-10 - val_loss: 3.4625e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.4895e-10 - val_loss: 3.2918e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3380e-10 - val_loss: 3.2980e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3688e-10 - val_loss: 3.3035e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3628e-10 - val_loss: 3.3627e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.4010e-10 - val_loss: 3.2487e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.3052e-10 - val_loss: 3.1741e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.1360e-10 - val_loss: 3.0171e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0633e-10 - val_loss: 2.9457e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9641e-10 - val_loss: 2.8871e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9398e-10 - val_loss: 2.8602e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8913e-10 - val_loss: 2.8302e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9095e-10 - val_loss: 2.8618e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8928e-10 - val_loss: 2.8550e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.9235e-10 - val_loss: 2.8209e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8137e-10 - val_loss: 2.7148e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.7531e-10 - val_loss: 2.6610e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6747e-10 - val_loss: 2.5989e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.6459e-10 - val_loss: 2.5442e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.5448e-10 - val_loss: 2.4201e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.4371e-10 - val_loss: 2.4009e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4596e-10 - val_loss: 2.4280e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4806e-10 - val_loss: 2.4319e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.4438e-10 - val_loss: 2.3356e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.3673e-10 - val_loss: 2.3093e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.3332e-10 - val_loss: 2.2443e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2647e-10 - val_loss: 2.2355e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2667e-10 - val_loss: 2.2058e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2213e-10 - val_loss: 2.1527e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1684e-10 - val_loss: 2.1071e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1296e-10 - val_loss: 2.0918e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.1172e-10 - val_loss: 2.0680e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.0941e-10 - val_loss: 2.0397e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.0628e-10 - val_loss: 1.9428e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9561e-10 - val_loss: 1.9267e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9589e-10 - val_loss: 1.9370e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9334e-10 - val_loss: 1.9109e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9574e-10 - val_loss: 1.9570e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.9815e-10 - val_loss: 1.9049e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.8910e-10 - val_loss: 1.8835e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9201e-10 - val_loss: 1.9032e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.8844e-10 - val_loss: 1.7776e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7796e-10 - val_loss: 1.7405e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7849e-10 - val_loss: 1.7871e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7769e-10 - val_loss: 1.7381e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7605e-10 - val_loss: 1.7405e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7749e-10 - val_loss: 1.7242e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7195e-10 - val_loss: 1.6954e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7213e-10 - val_loss: 1.7146e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7345e-10 - val_loss: 1.6515e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.6305e-10 - val_loss: 1.5684e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6172e-10 - val_loss: 1.6476e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6409e-10 - val_loss: 1.5838e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5843e-10 - val_loss: 1.5393e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5741e-10 - val_loss: 1.5802e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5766e-10 - val_loss: 1.5265e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5166e-10 - val_loss: 1.4818e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5206e-10 - val_loss: 1.5045e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4864e-10 - val_loss: 1.3717e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3833e-10 - val_loss: 1.3613e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3794e-10 - val_loss: 1.4051e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4172e-10 - val_loss: 1.3656e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3604e-10 - val_loss: 1.3086e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3267e-10 - val_loss: 1.3302e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3775e-10 - val_loss: 1.3870e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3864e-10 - val_loss: 1.3306e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.3195e-10 - val_loss: 1.2393e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2662e-10 - val_loss: 1.2394e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2406e-10 - val_loss: 1.2083e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2155e-10 - val_loss: 1.1792e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1896e-10 - val_loss: 1.2024e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2281e-10 - val_loss: 1.2185e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2326e-10 - val_loss: 1.2069e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2145e-10 - val_loss: 1.1732e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1987e-10 - val_loss: 1.2224e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2038e-10 - val_loss: 1.1777e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2007e-10 - val_loss: 1.1577e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1714e-10 - val_loss: 1.1307e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1360e-10 - val_loss: 1.1220e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1201e-10 - val_loss: 1.0912e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0829e-10 - val_loss: 1.0151e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0313e-10 - val_loss: 1.0296e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0643e-10 - val_loss: 1.0781e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0979e-10 - val_loss: 1.0841e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1017e-10 - val_loss: 1.0679e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0666e-10 - val_loss: 1.0620e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0912e-10 - val_loss: 1.0904e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0966e-10 - val_loss: 1.0561e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0457e-10 - val_loss: 9.9653e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.9656e-11 - val_loss: 9.6250e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.5612e-11 - val_loss: 8.7728e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8810e-11 - val_loss: 9.0821e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4184e-11 - val_loss: 9.5982e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9652e-11 - val_loss: 1.0425e-10
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0660e-10 - val_loss: 1.0407e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0104e-10 - val_loss: 9.4216e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4224e-11 - val_loss: 9.1033e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2612e-11 - val_loss: 9.3404e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4062e-11 - val_loss: 9.4757e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5340e-11 - val_loss: 8.8550e-11
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.7660e-11 - val_loss: 8.5015e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7090e-11 - val_loss: 8.9423e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0448e-11 - val_loss: 9.1337e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3164e-11 - val_loss: 9.4438e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5165e-11 - val_loss: 8.9145e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8781e-11 - val_loss: 8.6385e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7375e-11 - val_loss: 8.8068e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0346e-11 - val_loss: 9.0531e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0234e-11 - val_loss: 8.6736e-11
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.7064e-11 - val_loss: 8.2729e-11
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.1408e-11 - val_loss: 7.8556e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0005e-11 - val_loss: 7.9760e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.0458e-11 - val_loss: 7.8066e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8175e-11 - val_loss: 7.8933e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0342e-11 - val_loss: 7.8181e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.8004e-11 - val_loss: 7.4962e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5766e-11 - val_loss: 7.5476e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7470e-11 - val_loss: 7.7057e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8262e-11 - val_loss: 7.8922e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0203e-11 - val_loss: 7.8477e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8339e-11 - val_loss: 7.5583e-11
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.3973e-11 - val_loss: 6.8871e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.7557e-11 - val_loss: 6.6809e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7770e-11 - val_loss: 6.9992e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2588e-11 - val_loss: 7.4955e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6632e-11 - val_loss: 7.9083e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0041e-11 - val_loss: 7.7072e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5563e-11 - val_loss: 7.0433e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.8240e-11 - val_loss: 6.6438e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9302e-11 - val_loss: 7.1510e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3229e-11 - val_loss: 7.3579e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4089e-11 - val_loss: 7.3406e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2734e-11 - val_loss: 6.7557e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6384e-11 - val_loss: 6.1537e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.0103e-11 - val_loss: 5.6210e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6428e-11 - val_loss: 5.7335e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9380e-11 - val_loss: 6.2080e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4704e-11 - val_loss: 6.8757e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1338e-11 - val_loss: 7.1903e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9670e-11 - val_loss: 6.4620e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2736e-11 - val_loss: 5.8620e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.7414e-11 - val_loss: 5.5116e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5012e-11 - val_loss: 5.5651e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8027e-11 - val_loss: 6.1841e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3843e-11 - val_loss: 6.6959e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9405e-11 - val_loss: 7.0191e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7973e-11 - val_loss: 6.0424e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.8886e-11 - val_loss: 5.5066e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.4403e-11 - val_loss: 5.3435e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3190e-11 - val_loss: 5.2072e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3918e-11 - val_loss: 5.8719e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1006e-11 - val_loss: 6.2767e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5267e-11 - val_loss: 6.8948e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9815e-11 - val_loss: 6.8051e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6103e-11 - val_loss: 6.0322e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9582e-11 - val_loss: 5.5878e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.4455e-11 - val_loss: 5.0359e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.9892e-11 - val_loss: 4.7708e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8764e-11 - val_loss: 5.1124e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2664e-11 - val_loss: 5.3272e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5436e-11 - val_loss: 5.8056e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8396e-11 - val_loss: 5.3325e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2101e-11 - val_loss: 4.7753e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.6955e-11 - val_loss: 4.3488e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3501e-11 - val_loss: 4.4128e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4202e-11 - val_loss: 4.3630e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4538e-11 - val_loss: 4.5135e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5306e-11 - val_loss: 4.7106e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8832e-11 - val_loss: 5.1018e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2934e-11 - val_loss: 5.5007e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5142e-11 - val_loss: 5.1336e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0681e-11 - val_loss: 4.7263e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7718e-11 - val_loss: 4.5876e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6377e-11 - val_loss: 4.4807e-11
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.4232e-11 - val_loss: 4.2032e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.2083e-11 - val_loss: 4.0795e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1234e-11 - val_loss: 4.3194e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5262e-11 - val_loss: 4.6809e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8442e-11 - val_loss: 5.0715e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2179e-11 - val_loss: 5.2163e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0121e-11 - val_loss: 4.4487e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.3202e-11 - val_loss: 4.0763e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 4.0080e-11 - val_loss: 3.7019e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.6777e-11 - val_loss: 3.4903e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5126e-11 - val_loss: 3.4682e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4829e-11 - val_loss: 3.5828e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6801e-11 - val_loss: 3.8353e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0251e-11 - val_loss: 4.4940e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6659e-11 - val_loss: 4.8810e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0328e-11 - val_loss: 5.0525e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1209e-11 - val_loss: 4.8344e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7448e-11 - val_loss: 4.4598e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2861e-11 - val_loss: 4.0155e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7879e-11 - val_loss: 3.4734e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.5145e-11 - val_loss: 3.4344e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.3974e-11 - val_loss: 3.2669e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2998e-11 - val_loss: 3.2736e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3541e-11 - val_loss: 3.4821e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5933e-11 - val_loss: 3.7435e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8049e-11 - val_loss: 4.0747e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3002e-11 - val_loss: 4.7242e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8986e-11 - val_loss: 4.9636e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6746e-11 - val_loss: 4.1566e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9420e-11 - val_loss: 3.6022e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4693e-11 - val_loss: 3.2682e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.2476e-11 - val_loss: 3.0544e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0240e-11 - val_loss: 3.0737e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1395e-11 - val_loss: 3.1225e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1237e-11 - val_loss: 3.1882e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2846e-11 - val_loss: 3.4209e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4981e-11 - val_loss: 3.5507e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5283e-11 - val_loss: 3.3974e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5437e-11 - val_loss: 4.0090e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2033e-11 - val_loss: 4.6142e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7961e-11 - val_loss: 4.8965e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7451e-11 - val_loss: 4.2844e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0924e-11 - val_loss: 3.6433e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5965e-11 - val_loss: 3.4256e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4166e-11 - val_loss: 3.3200e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2765e-11 - val_loss: 3.1691e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2550e-11 - val_loss: 3.2972e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3764e-11 - val_loss: 3.3399e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4446e-11 - val_loss: 3.5392e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5178e-11 - val_loss: 3.3543e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3628e-11 - val_loss: 3.3643e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3543e-11 - val_loss: 3.3288e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4122e-11 - val_loss: 3.7923e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9443e-11 - val_loss: 4.1162e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2198e-11 - val_loss: 4.0330e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9096e-11 - val_loss: 3.5275e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4852e-11 - val_loss: 3.3125e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2326e-11 - val_loss: 3.0680e-11
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 3.0310e-11 - val_loss: 2.9104e-11
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8582e-11 - val_loss: 2.6960e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7436e-11 - val_loss: 2.8412e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9114e-11 - val_loss: 2.9255e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0082e-11 - val_loss: 3.0627e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1706e-11 - val_loss: 3.1974e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2420e-11 - val_loss: 3.2660e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2741e-11 - val_loss: 3.2000e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2312e-11 - val_loss: 3.2031e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2038e-11 - val_loss: 3.1549e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1484e-11 - val_loss: 3.2396e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1777e-11 - val_loss: 3.0291e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9469e-11 - val_loss: 2.8118e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8180e-11 - val_loss: 2.7543e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8021e-11 - val_loss: 2.8018e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8451e-11 - val_loss: 2.8096e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8402e-11 - val_loss: 3.0272e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0604e-11 - val_loss: 2.9703e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0158e-11 - val_loss: 3.0409e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0379e-11 - val_loss: 2.8792e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.8243e-11 - val_loss: 2.6274e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.5949e-11 - val_loss: 2.4307e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5148e-11 - val_loss: 2.6566e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7489e-11 - val_loss: 2.7445e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7562e-11 - val_loss: 2.7274e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7329e-11 - val_loss: 2.7012e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7363e-11 - val_loss: 2.7673e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8588e-11 - val_loss: 2.9946e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9869e-11 - val_loss: 2.9551e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9793e-11 - val_loss: 2.9280e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8058e-11 - val_loss: 2.6855e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7044e-11 - val_loss: 2.6603e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6892e-11 - val_loss: 2.5789e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6425e-11 - val_loss: 2.7539e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7733e-11 - val_loss: 2.7729e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7847e-11 - val_loss: 2.8042e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8525e-11 - val_loss: 2.8495e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8057e-11 - val_loss: 2.6508e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6061e-11 - val_loss: 2.4852e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5290e-11 - val_loss: 2.6135e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6450e-11 - val_loss: 2.5554e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.5086e-11 - val_loss: 2.3587e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3269e-11 - val_loss: 2.4001e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.3919e-11 - val_loss: 2.2739e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3157e-11 - val_loss: 2.4187e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5068e-11 - val_loss: 2.5627e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5171e-11 - val_loss: 2.3761e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4120e-11 - val_loss: 2.5151e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5485e-11 - val_loss: 2.4616e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 2.2709e-11 - val_loss: 1.8733e-11
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.7989e-11 - val_loss: 1.6168e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.5544e-11 - val_loss: 1.4422e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4414e-11 - val_loss: 1.3707e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3765e-11 - val_loss: 1.4951e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5843e-11 - val_loss: 1.7355e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8475e-11 - val_loss: 2.0050e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0782e-11 - val_loss: 2.2786e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3578e-11 - val_loss: 2.4484e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5327e-11 - val_loss: 2.6475e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6715e-11 - val_loss: 2.5677e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5130e-11 - val_loss: 2.3547e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3107e-11 - val_loss: 2.3078e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3349e-11 - val_loss: 2.2722e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2255e-11 - val_loss: 2.2049e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2374e-11 - val_loss: 2.1689e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2072e-11 - val_loss: 2.3279e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3966e-11 - val_loss: 2.4380e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3968e-11 - val_loss: 2.2785e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3261e-11 - val_loss: 2.2932e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2530e-11 - val_loss: 2.1513e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1131e-11 - val_loss: 2.0136e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0055e-11 - val_loss: 2.0221e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0907e-11 - val_loss: 2.1437e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1000e-11 - val_loss: 1.9268e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8353e-11 - val_loss: 1.5403e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.4702e-11 - val_loss: 1.3096e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.2572e-11 - val_loss: 1.2079e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1761e-11 - val_loss: 1.1171e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.1355e-11 - val_loss: 1.0932e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0976e-11 - val_loss: 1.0905e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0799e-11 - val_loss: 1.0359e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0493e-11 - val_loss: 1.0999e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1432e-11 - val_loss: 1.3250e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4821e-11 - val_loss: 1.7169e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8411e-11 - val_loss: 2.0329e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1661e-11 - val_loss: 2.3503e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3871e-11 - val_loss: 2.4310e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4914e-11 - val_loss: 2.5393e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4870e-11 - val_loss: 2.3741e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3130e-11 - val_loss: 2.1301e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0954e-11 - val_loss: 2.0374e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0158e-11 - val_loss: 2.0466e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0980e-11 - val_loss: 2.1549e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1146e-11 - val_loss: 1.9665e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9658e-11 - val_loss: 1.9040e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9377e-11 - val_loss: 1.9524e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9335e-11 - val_loss: 1.9500e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0252e-11 - val_loss: 2.0688e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0246e-11 - val_loss: 2.0138e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0169e-11 - val_loss: 1.9312e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9140e-11 - val_loss: 1.9093e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9450e-11 - val_loss: 1.9249e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9352e-11 - val_loss: 1.9586e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0002e-11 - val_loss: 2.0410e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9681e-11 - val_loss: 1.6826e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5777e-11 - val_loss: 1.2974e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2338e-11 - val_loss: 1.0865e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 1.0351e-11 - val_loss: 9.7125e-12
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 9.3420e-12 - val_loss: 8.3040e-12
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.1738e-12 - val_loss: 8.2263e-12
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 8.1674e-12 - val_loss: 7.9833e-12
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.9089e-12 - val_loss: 7.7745e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8243e-12 - val_loss: 7.8112e-12
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.5540e-12 - val_loss: 7.3198e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2350e-12 - val_loss: 7.7593e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0194e-12 - val_loss: 8.1077e-12
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0260e-12 - val_loss: 7.8684e-12
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0743e-12 - val_loss: 9.1356e-12
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0082e-11 - val_loss: 1.1958e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3464e-11 - val_loss: 1.7115e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8535e-11 - val_loss: 1.9472e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0240e-11 - val_loss: 2.1394e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1708e-11 - val_loss: 2.2134e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2707e-11 - val_loss: 2.3111e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2268e-11 - val_loss: 2.1174e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0653e-11 - val_loss: 1.8181e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8209e-11 - val_loss: 1.8934e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8744e-11 - val_loss: 1.7333e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6769e-11 - val_loss: 1.5976e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6083e-11 - val_loss: 1.5447e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5516e-11 - val_loss: 1.5814e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6308e-11 - val_loss: 1.6296e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6699e-11 - val_loss: 1.7661e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8163e-11 - val_loss: 1.8627e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8233e-11 - val_loss: 1.7610e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7854e-11 - val_loss: 1.8154e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8509e-11 - val_loss: 1.8608e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8263e-11 - val_loss: 1.8197e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8767e-11 - val_loss: 1.9207e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9301e-11 - val_loss: 1.7913e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7185e-11 - val_loss: 1.6575e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6826e-11 - val_loss: 1.7826e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8310e-11 - val_loss: 1.8576e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7172e-11 - val_loss: 1.4293e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3455e-11 - val_loss: 1.2345e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1418e-11 - val_loss: 9.8803e-12
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1429e-12 - val_loss: 7.6094e-12
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5586e-12 - val_loss: 7.4963e-12
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 7.1235e-12 - val_loss: 6.5593e-12
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.2968e-12 - val_loss: 6.0323e-12
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.8546e-12 - val_loss: 5.9746e-12
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0707e-12 - val_loss: 6.1219e-12
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2538e-12 - val_loss: 6.1706e-12
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6431e-12 - val_loss: 7.6654e-12
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6671e-12 - val_loss: 7.2183e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3892e-12 - val_loss: 7.7804e-12
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1360e-12 - val_loss: 8.3491e-12
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7393e-12 - val_loss: 9.0976e-12
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0569e-12 - val_loss: 8.9274e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5820e-12 - val_loss: 7.8435e-12
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0730e-12 - val_loss: 8.1121e-12
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0379e-12 - val_loss: 7.6003e-12
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4824e-12 - val_loss: 6.4457e-12
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4138e-12 - val_loss: 7.1717e-12
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0328e-12 - val_loss: 9.7779e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0866e-11 - val_loss: 1.3026e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4353e-11 - val_loss: 1.6407e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7099e-11 - val_loss: 1.8551e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9305e-11 - val_loss: 2.0213e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0639e-11 - val_loss: 1.9715e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9728e-11 - val_loss: 1.8571e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8196e-11 - val_loss: 1.6121e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5691e-11 - val_loss: 1.3925e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3855e-11 - val_loss: 1.3655e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3757e-11 - val_loss: 1.4578e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5330e-11 - val_loss: 1.7149e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7744e-11 - val_loss: 1.7604e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8361e-11 - val_loss: 1.7942e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7777e-11 - val_loss: 1.7262e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7442e-11 - val_loss: 1.6691e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6052e-11 - val_loss: 1.4949e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5112e-11 - val_loss: 1.5120e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5233e-11 - val_loss: 1.5106e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4339e-11 - val_loss: 1.2665e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2137e-11 - val_loss: 1.1597e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1553e-11 - val_loss: 1.2148e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2026e-11 - val_loss: 1.2689e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2929e-11 - val_loss: 1.2504e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3410e-11 - val_loss: 1.5539e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6277e-11 - val_loss: 1.7985e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7874e-11 - val_loss: 1.6473e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5863e-11 - val_loss: 1.5630e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5784e-11 - val_loss: 1.5214e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4159e-11 - val_loss: 1.2294e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1129e-11 - val_loss: 9.1373e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9958e-12 - val_loss: 7.3848e-12
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.6325e-12 - val_loss: 5.7506e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0833e-12 - val_loss: 6.6315e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8210e-12 - val_loss: 6.6615e-12
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7274e-12 - val_loss: 7.2647e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4025e-12 - val_loss: 6.8318e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6317e-12 - val_loss: 5.8253e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8366e-12 - val_loss: 6.7113e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7766e-12 - val_loss: 6.8329e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9910e-12 - val_loss: 7.3650e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0873e-12 - val_loss: 6.6034e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5964e-12 - val_loss: 6.8165e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9615e-12 - val_loss: 7.0472e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7950e-12 - val_loss: 6.5501e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8452e-12 - val_loss: 7.3655e-12
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0916e-12 - val_loss: 6.5551e-12
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 6.1029e-12 - val_loss: 5.4549e-12
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/addition_weights.h5
448/448 - 0s - loss: 5.3725e-12 - val_loss: 5.3548e-12
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0457 - val_loss: 7.5773e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.0528e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.1307e-04
Epoch 4/512
448/448 - 0s - loss: 0.0010 - val_loss: 2.0443e-04
Epoch 5/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.4317e-04
Epoch 6/512
448/448 - 0s - loss: 8.7072e-04 - val_loss: 1.3549e-04
Epoch 7/512
448/448 - 0s - loss: 5.5835e-04 - val_loss: 1.0689e-04
Epoch 8/512
448/448 - 0s - loss: 9.1201e-04 - val_loss: 4.1455e-05
Epoch 9/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2755e-05
Epoch 10/512
448/448 - 0s - loss: 5.9521e-04 - val_loss: 1.9989e-05
Epoch 11/512
448/448 - 0s - loss: 5.7636e-04 - val_loss: 1.0184e-05
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0563e-05
Epoch 13/512
448/448 - 0s - loss: 6.2948e-04 - val_loss: 1.2274e-05
Epoch 14/512
448/448 - 0s - loss: 4.3094e-04 - val_loss: 1.4637e-05
Epoch 15/512
448/448 - 0s - loss: 8.0625e-04 - val_loss: 1.9054e-05
Epoch 16/512
448/448 - 0s - loss: 5.9006e-04 - val_loss: 2.0239e-05
Epoch 17/512
448/448 - 0s - loss: 3.4106e-04 - val_loss: 2.0648e-05
Epoch 18/512
448/448 - 0s - loss: 5.4841e-04 - val_loss: 2.2088e-05
Epoch 19/512
448/448 - 0s - loss: 4.9870e-04 - val_loss: 2.2443e-05
Epoch 20/512
448/448 - 0s - loss: 2.7836e-04 - val_loss: 2.0773e-05
Epoch 21/512
448/448 - 0s - loss: 3.8900e-04 - val_loss: 2.1019e-05
Epoch 22/512
448/448 - 0s - loss: 3.8668e-04 - val_loss: 2.0434e-05
Epoch 23/512
448/448 - 0s - loss: 2.3788e-04 - val_loss: 1.8014e-05
Epoch 24/512
448/448 - 0s - loss: 2.8013e-04 - val_loss: 1.7209e-05
Epoch 25/512
448/448 - 0s - loss: 2.9743e-04 - val_loss: 1.6262e-05
Epoch 26/512
448/448 - 0s - loss: 2.0125e-04 - val_loss: 1.3868e-05
Epoch 27/512
448/448 - 0s - loss: 2.1088e-04 - val_loss: 1.2648e-05
Epoch 28/512
448/448 - 0s - loss: 2.2620e-04 - val_loss: 1.1355e-05
Epoch 29/512
448/448 - 0s - loss: 1.7263e-04 - val_loss: 9.4362e-06
Epoch 30/512
448/448 - 0s - loss: 1.6726e-04 - val_loss: 8.2282e-06
Epoch 31/512
448/448 - 0s - loss: 1.7550e-04 - val_loss: 7.3752e-06
Epoch 32/512
448/448 - 0s - loss: 1.4811e-04 - val_loss: 6.5542e-06
Epoch 33/512
448/448 - 0s - loss: 1.4197e-04 - val_loss: 6.1504e-06
Epoch 34/512
448/448 - 0s - loss: 1.3850e-04 - val_loss: 5.9141e-06
Epoch 35/512
448/448 - 0s - loss: 1.3233e-04 - val_loss: 5.8588e-06
Epoch 36/512
448/448 - 0s - loss: 1.1987e-04 - val_loss: 5.8081e-06
Epoch 37/512
448/448 - 0s - loss: 1.1632e-04 - val_loss: 5.7501e-06
Epoch 38/512
448/448 - 0s - loss: 1.1458e-04 - val_loss: 5.6391e-06
Epoch 39/512
448/448 - 0s - loss: 1.0732e-04 - val_loss: 5.4353e-06
Epoch 40/512
448/448 - 0s - loss: 1.0345e-04 - val_loss: 5.1443e-06
Epoch 41/512
448/448 - 0s - loss: 9.8627e-05 - val_loss: 4.9014e-06
Epoch 42/512
448/448 - 0s - loss: 9.6589e-05 - val_loss: 4.7456e-06
Epoch 43/512
448/448 - 0s - loss: 9.2874e-05 - val_loss: 4.6727e-06
Epoch 44/512
448/448 - 0s - loss: 8.8878e-05 - val_loss: 4.2777e-06
Epoch 45/512
448/448 - 0s - loss: 8.7502e-05 - val_loss: 3.9587e-06
Epoch 46/512
448/448 - 0s - loss: 8.4286e-05 - val_loss: 3.6722e-06
Epoch 47/512
448/448 - 0s - loss: 8.0801e-05 - val_loss: 3.4281e-06
Epoch 48/512
448/448 - 0s - loss: 8.0794e-05 - val_loss: 3.2324e-06
Epoch 49/512
448/448 - 0s - loss: 7.6944e-05 - val_loss: 2.9602e-06
Epoch 50/512
448/448 - 0s - loss: 7.4540e-05 - val_loss: 2.8074e-06
Epoch 51/512
448/448 - 0s - loss: 7.3516e-05 - val_loss: 2.6159e-06
Epoch 52/512
448/448 - 0s - loss: 7.2175e-05 - val_loss: 2.4080e-06
Epoch 53/512
448/448 - 0s - loss: 6.9122e-05 - val_loss: 2.1959e-06
Epoch 54/512
448/448 - 0s - loss: 6.7988e-05 - val_loss: 2.0855e-06
Epoch 55/512
448/448 - 0s - loss: 6.5878e-05 - val_loss: 1.9276e-06
Epoch 56/512
448/448 - 0s - loss: 6.5344e-05 - val_loss: 1.8049e-06
Epoch 57/512
448/448 - 0s - loss: 6.3171e-05 - val_loss: 1.6772e-06
Epoch 58/512
448/448 - 0s - loss: 6.1858e-05 - val_loss: 1.5628e-06
Epoch 59/512
448/448 - 0s - loss: 6.0813e-05 - val_loss: 1.4457e-06
Epoch 60/512
448/448 - 0s - loss: 5.9589e-05 - val_loss: 1.3292e-06
Epoch 61/512
448/448 - 0s - loss: 5.7332e-05 - val_loss: 1.2477e-06
Epoch 62/512
448/448 - 0s - loss: 5.6909e-05 - val_loss: 1.1988e-06
Epoch 63/512
448/448 - 0s - loss: 5.6515e-05 - val_loss: 1.1036e-06
Epoch 64/512
448/448 - 0s - loss: 5.4292e-05 - val_loss: 1.0151e-06
Epoch 65/512
448/448 - 0s - loss: 5.2992e-05 - val_loss: 9.6931e-07
Epoch 66/512
448/448 - 0s - loss: 5.3402e-05 - val_loss: 9.0721e-07
Epoch 67/512
448/448 - 0s - loss: 5.1092e-05 - val_loss: 8.4380e-07
Epoch 68/512
448/448 - 0s - loss: 5.0668e-05 - val_loss: 7.8438e-07
Epoch 69/512
448/448 - 0s - loss: 4.9891e-05 - val_loss: 7.5096e-07
Epoch 70/512
448/448 - 0s - loss: 4.8404e-05 - val_loss: 7.0937e-07
Epoch 71/512
448/448 - 0s - loss: 4.8096e-05 - val_loss: 6.7208e-07
Epoch 72/512
448/448 - 0s - loss: 4.7354e-05 - val_loss: 6.2833e-07
Epoch 73/512
448/448 - 0s - loss: 4.6020e-05 - val_loss: 5.9558e-07
Epoch 74/512
448/448 - 0s - loss: 4.5441e-05 - val_loss: 5.6668e-07
Epoch 75/512
448/448 - 0s - loss: 4.4926e-05 - val_loss: 5.3345e-07
Epoch 76/512
448/448 - 0s - loss: 4.3277e-05 - val_loss: 5.1212e-07
Epoch 77/512
448/448 - 0s - loss: 4.3823e-05 - val_loss: 4.8708e-07
Epoch 78/512
448/448 - 0s - loss: 4.2576e-05 - val_loss: 4.5797e-07
Epoch 79/512
448/448 - 0s - loss: 4.1439e-05 - val_loss: 4.3376e-07
Epoch 80/512
448/448 - 0s - loss: 4.1216e-05 - val_loss: 4.1716e-07
Epoch 81/512
448/448 - 0s - loss: 4.0828e-05 - val_loss: 3.9399e-07
Epoch 82/512
448/448 - 0s - loss: 3.8901e-05 - val_loss: 3.8600e-07
Epoch 83/512
448/448 - 0s - loss: 3.9933e-05 - val_loss: 3.7222e-07
Epoch 84/512
448/448 - 0s - loss: 3.8431e-05 - val_loss: 3.5299e-07
Epoch 85/512
448/448 - 0s - loss: 3.7287e-05 - val_loss: 3.4222e-07
Epoch 86/512
448/448 - 0s - loss: 3.7863e-05 - val_loss: 3.2768e-07
Epoch 87/512
448/448 - 0s - loss: 3.6893e-05 - val_loss: 3.0829e-07
Epoch 88/512
448/448 - 0s - loss: 3.5759e-05 - val_loss: 2.9695e-07
Epoch 89/512
448/448 - 0s - loss: 3.5327e-05 - val_loss: 2.9208e-07
Epoch 90/512
448/448 - 0s - loss: 3.5624e-05 - val_loss: 2.7965e-07
Epoch 91/512
448/448 - 0s - loss: 3.4206e-05 - val_loss: 2.6746e-07
Epoch 92/512
448/448 - 0s - loss: 3.3807e-05 - val_loss: 2.6112e-07
Epoch 93/512
448/448 - 0s - loss: 3.3224e-05 - val_loss: 2.5556e-07
Epoch 94/512
448/448 - 0s - loss: 3.3278e-05 - val_loss: 2.4175e-07
Epoch 95/512
448/448 - 0s - loss: 3.1804e-05 - val_loss: 2.3639e-07
Epoch 96/512
448/448 - 0s - loss: 3.1866e-05 - val_loss: 2.3306e-07
Epoch 97/512
448/448 - 0s - loss: 3.1482e-05 - val_loss: 2.2305e-07
Epoch 98/512
448/448 - 0s - loss: 3.0357e-05 - val_loss: 2.2101e-07
Epoch 99/512
448/448 - 0s - loss: 3.0357e-05 - val_loss: 2.1731e-07
Epoch 100/512
448/448 - 0s - loss: 3.0253e-05 - val_loss: 2.0503e-07
Epoch 101/512
448/448 - 0s - loss: 2.8945e-05 - val_loss: 1.9772e-07
Epoch 102/512
448/448 - 0s - loss: 2.8697e-05 - val_loss: 1.9643e-07
Epoch 103/512
448/448 - 0s - loss: 2.8568e-05 - val_loss: 1.9120e-07
Epoch 104/512
448/448 - 0s - loss: 2.7841e-05 - val_loss: 1.8377e-07
Epoch 105/512
448/448 - 0s - loss: 2.7070e-05 - val_loss: 1.8124e-07
Epoch 106/512
448/448 - 0s - loss: 2.7047e-05 - val_loss: 1.7767e-07
Epoch 107/512
448/448 - 0s - loss: 2.6653e-05 - val_loss: 1.7322e-07
Epoch 108/512
448/448 - 0s - loss: 2.5897e-05 - val_loss: 1.6975e-07
Epoch 109/512
448/448 - 0s - loss: 2.5606e-05 - val_loss: 1.6686e-07
Epoch 110/512
448/448 - 0s - loss: 2.5286e-05 - val_loss: 1.6230e-07
Epoch 111/512
448/448 - 0s - loss: 2.4570e-05 - val_loss: 1.5801e-07
Epoch 112/512
448/448 - 0s - loss: 2.4338e-05 - val_loss: 1.5417e-07
Epoch 113/512
448/448 - 0s - loss: 2.3800e-05 - val_loss: 1.5154e-07
Epoch 114/512
448/448 - 0s - loss: 2.3425e-05 - val_loss: 1.4828e-07
Epoch 115/512
448/448 - 0s - loss: 2.3032e-05 - val_loss: 1.4589e-07
Epoch 116/512
448/448 - 0s - loss: 2.2533e-05 - val_loss: 1.4354e-07
Epoch 117/512
448/448 - 0s - loss: 2.2241e-05 - val_loss: 1.3995e-07
Epoch 118/512
448/448 - 0s - loss: 2.1804e-05 - val_loss: 1.3476e-07
Epoch 119/512
448/448 - 0s - loss: 2.1123e-05 - val_loss: 1.3384e-07
Epoch 120/512
448/448 - 0s - loss: 2.1015e-05 - val_loss: 1.3336e-07
Epoch 121/512
448/448 - 0s - loss: 2.0773e-05 - val_loss: 1.2909e-07
Epoch 122/512
448/448 - 0s - loss: 2.0070e-05 - val_loss: 1.2560e-07
Epoch 123/512
448/448 - 0s - loss: 1.9778e-05 - val_loss: 1.2274e-07
Epoch 124/512
448/448 - 0s - loss: 1.9387e-05 - val_loss: 1.2205e-07
Epoch 125/512
448/448 - 0s - loss: 1.9162e-05 - val_loss: 1.1903e-07
Epoch 126/512
448/448 - 0s - loss: 1.8680e-05 - val_loss: 1.1491e-07
Epoch 127/512
448/448 - 0s - loss: 1.8129e-05 - val_loss: 1.1366e-07
Epoch 128/512
448/448 - 0s - loss: 1.8090e-05 - val_loss: 1.1119e-07
Epoch 129/512
448/448 - 0s - loss: 1.7564e-05 - val_loss: 1.0824e-07
Epoch 130/512
448/448 - 0s - loss: 1.7154e-05 - val_loss: 1.0620e-07
Epoch 131/512
448/448 - 0s - loss: 1.7144e-05 - val_loss: 1.0138e-07
Epoch 132/512
448/448 - 0s - loss: 1.6144e-05 - val_loss: 1.0008e-07
Epoch 133/512
448/448 - 0s - loss: 1.6261e-05 - val_loss: 9.9790e-08
Epoch 134/512
448/448 - 0s - loss: 1.6016e-05 - val_loss: 9.6052e-08
Epoch 135/512
448/448 - 0s - loss: 1.5497e-05 - val_loss: 9.2250e-08
Epoch 136/512
448/448 - 0s - loss: 1.4938e-05 - val_loss: 9.2141e-08
Epoch 137/512
448/448 - 0s - loss: 1.5102e-05 - val_loss: 8.9750e-08
Epoch 138/512
448/448 - 0s - loss: 1.4574e-05 - val_loss: 8.5224e-08
Epoch 139/512
448/448 - 0s - loss: 1.4030e-05 - val_loss: 8.3084e-08
Epoch 140/512
448/448 - 0s - loss: 1.3780e-05 - val_loss: 8.3314e-08
Epoch 141/512
448/448 - 0s - loss: 1.3952e-05 - val_loss: 7.9461e-08
Epoch 142/512
448/448 - 0s - loss: 1.3081e-05 - val_loss: 7.6493e-08
Epoch 143/512
448/448 - 0s - loss: 1.2859e-05 - val_loss: 7.6323e-08
Epoch 144/512
448/448 - 0s - loss: 1.2786e-05 - val_loss: 7.4912e-08
Epoch 145/512
448/448 - 0s - loss: 1.2503e-05 - val_loss: 7.1542e-08
Epoch 146/512
448/448 - 0s - loss: 1.2007e-05 - val_loss: 6.8900e-08
Epoch 147/512
448/448 - 0s - loss: 1.1716e-05 - val_loss: 6.7781e-08
Epoch 148/512
448/448 - 0s - loss: 1.1551e-05 - val_loss: 6.6592e-08
Epoch 149/512
448/448 - 0s - loss: 1.1322e-05 - val_loss: 6.4094e-08
Epoch 150/512
448/448 - 0s - loss: 1.0880e-05 - val_loss: 6.2714e-08
Epoch 151/512
448/448 - 0s - loss: 1.0726e-05 - val_loss: 6.1612e-08
Epoch 152/512
448/448 - 0s - loss: 1.0536e-05 - val_loss: 5.8971e-08
Epoch 153/512
448/448 - 0s - loss: 1.0061e-05 - val_loss: 5.7581e-08
Epoch 154/512
448/448 - 0s - loss: 9.8974e-06 - val_loss: 5.7181e-08
Epoch 155/512
448/448 - 0s - loss: 9.8008e-06 - val_loss: 5.5175e-08
Epoch 156/512
448/448 - 0s - loss: 9.3635e-06 - val_loss: 5.3236e-08
Epoch 157/512
448/448 - 0s - loss: 9.1493e-06 - val_loss: 5.2195e-08
Epoch 158/512
448/448 - 0s - loss: 8.9621e-06 - val_loss: 5.1086e-08
Epoch 159/512
448/448 - 0s - loss: 8.7575e-06 - val_loss: 4.9167e-08
Epoch 160/512
448/448 - 0s - loss: 8.3934e-06 - val_loss: 4.8304e-08
Epoch 161/512
448/448 - 0s - loss: 8.3033e-06 - val_loss: 4.7380e-08
Epoch 162/512
448/448 - 0s - loss: 8.0740e-06 - val_loss: 4.5379e-08
Epoch 163/512
448/448 - 0s - loss: 7.7468e-06 - val_loss: 4.4032e-08
Epoch 164/512
448/448 - 0s - loss: 7.5627e-06 - val_loss: 4.3374e-08
Epoch 165/512
448/448 - 0s - loss: 7.4479e-06 - val_loss: 4.1995e-08
Epoch 166/512
448/448 - 0s - loss: 7.1610e-06 - val_loss: 4.0551e-08
Epoch 167/512
448/448 - 0s - loss: 6.9284e-06 - val_loss: 3.9915e-08
Epoch 168/512
448/448 - 0s - loss: 6.8162e-06 - val_loss: 3.9062e-08
Epoch 169/512
448/448 - 0s - loss: 6.6058e-06 - val_loss: 3.7569e-08
Epoch 170/512
448/448 - 0s - loss: 6.3062e-06 - val_loss: 3.7366e-08
Epoch 171/512
448/448 - 0s - loss: 6.2982e-06 - val_loss: 3.6298e-08
Epoch 172/512
448/448 - 0s - loss: 6.0273e-06 - val_loss: 3.4791e-08
Epoch 173/512
448/448 - 0s - loss: 5.7820e-06 - val_loss: 3.4254e-08
Epoch 174/512
448/448 - 0s - loss: 5.7277e-06 - val_loss: 3.3544e-08
Epoch 175/512
448/448 - 0s - loss: 5.5024e-06 - val_loss: 3.2579e-08
Epoch 176/512
448/448 - 0s - loss: 5.3386e-06 - val_loss: 3.1646e-08
Epoch 177/512
448/448 - 0s - loss: 5.1967e-06 - val_loss: 3.0343e-08
Epoch 178/512
448/448 - 0s - loss: 4.9614e-06 - val_loss: 2.9774e-08
Epoch 179/512
448/448 - 0s - loss: 4.8792e-06 - val_loss: 2.9196e-08
Epoch 180/512
448/448 - 0s - loss: 4.7223e-06 - val_loss: 2.8414e-08
Epoch 181/512
448/448 - 0s - loss: 4.5503e-06 - val_loss: 2.7833e-08
Epoch 182/512
448/448 - 0s - loss: 4.4420e-06 - val_loss: 2.7123e-08
Epoch 183/512
448/448 - 0s - loss: 4.3052e-06 - val_loss: 2.6067e-08
Epoch 184/512
448/448 - 0s - loss: 4.0780e-06 - val_loss: 2.5783e-08
Epoch 185/512
448/448 - 0s - loss: 4.0756e-06 - val_loss: 2.4905e-08
Epoch 186/512
448/448 - 0s - loss: 3.8544e-06 - val_loss: 2.4071e-08
Epoch 187/512
448/448 - 0s - loss: 3.7247e-06 - val_loss: 2.3837e-08
Epoch 188/512
448/448 - 0s - loss: 3.6634e-06 - val_loss: 2.3336e-08
Epoch 189/512
448/448 - 0s - loss: 3.5281e-06 - val_loss: 2.2383e-08
Epoch 190/512
448/448 - 0s - loss: 3.3528e-06 - val_loss: 2.1965e-08
Epoch 191/512
448/448 - 0s - loss: 3.2988e-06 - val_loss: 2.1594e-08
Epoch 192/512
448/448 - 0s - loss: 3.2043e-06 - val_loss: 2.0599e-08
Epoch 193/512
448/448 - 0s - loss: 3.0159e-06 - val_loss: 2.0199e-08
Epoch 194/512
448/448 - 0s - loss: 2.9616e-06 - val_loss: 2.0051e-08
Epoch 195/512
448/448 - 0s - loss: 2.8963e-06 - val_loss: 1.9351e-08
Epoch 196/512
448/448 - 0s - loss: 2.7379e-06 - val_loss: 1.8697e-08
Epoch 197/512
448/448 - 0s - loss: 2.6524e-06 - val_loss: 1.8271e-08
Epoch 198/512
448/448 - 0s - loss: 2.5616e-06 - val_loss: 1.8055e-08
Epoch 199/512
448/448 - 0s - loss: 2.5036e-06 - val_loss: 1.7598e-08
Epoch 200/512
448/448 - 0s - loss: 2.4116e-06 - val_loss: 1.6698e-08
Epoch 201/512
448/448 - 0s - loss: 2.2493e-06 - val_loss: 1.6601e-08
Epoch 202/512
448/448 - 0s - loss: 2.2486e-06 - val_loss: 1.6489e-08
Epoch 203/512
448/448 - 0s - loss: 2.1851e-06 - val_loss: 1.5572e-08
Epoch 204/512
448/448 - 0s - loss: 2.0185e-06 - val_loss: 1.5174e-08
Epoch 205/512
448/448 - 0s - loss: 1.9776e-06 - val_loss: 1.5355e-08
Epoch 206/512
448/448 - 0s - loss: 1.9769e-06 - val_loss: 1.4703e-08
Epoch 207/512
448/448 - 0s - loss: 1.8382e-06 - val_loss: 1.3882e-08
Epoch 208/512
448/448 - 0s - loss: 1.7377e-06 - val_loss: 1.3773e-08
Epoch 209/512
448/448 - 0s - loss: 1.7154e-06 - val_loss: 1.3943e-08
Epoch 210/512
448/448 - 0s - loss: 1.7138e-06 - val_loss: 1.3079e-08
Epoch 211/512
448/448 - 0s - loss: 1.5395e-06 - val_loss: 1.2603e-08
Epoch 212/512
448/448 - 0s - loss: 1.5095e-06 - val_loss: 1.2721e-08
Epoch 213/512
448/448 - 0s - loss: 1.5057e-06 - val_loss: 1.2478e-08
Epoch 214/512
448/448 - 0s - loss: 1.4242e-06 - val_loss: 1.1893e-08
Epoch 215/512
448/448 - 0s - loss: 1.3429e-06 - val_loss: 1.1551e-08
Epoch 216/512
448/448 - 0s - loss: 1.2984e-06 - val_loss: 1.1513e-08
Epoch 217/512
448/448 - 0s - loss: 1.2790e-06 - val_loss: 1.1172e-08
Epoch 218/512
448/448 - 0s - loss: 1.2055e-06 - val_loss: 1.0760e-08
Epoch 219/512
448/448 - 0s - loss: 1.1437e-06 - val_loss: 1.0730e-08
Epoch 220/512
448/448 - 0s - loss: 1.1447e-06 - val_loss: 1.0284e-08
Epoch 221/512
448/448 - 0s - loss: 1.0501e-06 - val_loss: 1.0017e-08
Epoch 222/512
448/448 - 0s - loss: 1.0256e-06 - val_loss: 1.0012e-08
Epoch 223/512
448/448 - 0s - loss: 1.0161e-06 - val_loss: 9.5680e-09
Epoch 224/512
448/448 - 0s - loss: 9.2888e-07 - val_loss: 9.3336e-09
Epoch 225/512
448/448 - 0s - loss: 9.0721e-07 - val_loss: 9.2609e-09
Epoch 226/512
448/448 - 0s - loss: 8.8214e-07 - val_loss: 9.1359e-09
Epoch 227/512
448/448 - 0s - loss: 8.5295e-07 - val_loss: 8.7467e-09
Epoch 228/512
448/448 - 0s - loss: 7.9095e-07 - val_loss: 8.5809e-09
Epoch 229/512
448/448 - 0s - loss: 7.7566e-07 - val_loss: 8.4959e-09
Epoch 230/512
448/448 - 0s - loss: 7.5689e-07 - val_loss: 8.1201e-09
Epoch 231/512
448/448 - 0s - loss: 6.9587e-07 - val_loss: 7.9320e-09
Epoch 232/512
448/448 - 0s - loss: 6.7939e-07 - val_loss: 7.9124e-09
Epoch 233/512
448/448 - 0s - loss: 6.6608e-07 - val_loss: 7.7079e-09
Epoch 234/512
448/448 - 0s - loss: 6.2842e-07 - val_loss: 7.4357e-09
Epoch 235/512
448/448 - 0s - loss: 5.9610e-07 - val_loss: 7.2350e-09
Epoch 236/512
448/448 - 0s - loss: 5.7157e-07 - val_loss: 7.2205e-09
Epoch 237/512
448/448 - 0s - loss: 5.6586e-07 - val_loss: 7.0073e-09
Epoch 238/512
448/448 - 0s - loss: 5.2583e-07 - val_loss: 6.7765e-09
Epoch 239/512
448/448 - 0s - loss: 5.0229e-07 - val_loss: 6.6512e-09
Epoch 240/512
448/448 - 0s - loss: 4.8585e-07 - val_loss: 6.5754e-09
Epoch 241/512
448/448 - 0s - loss: 4.7117e-07 - val_loss: 6.3733e-09
Epoch 242/512
448/448 - 0s - loss: 4.4053e-07 - val_loss: 6.2510e-09
Epoch 243/512
448/448 - 0s - loss: 4.2812e-07 - val_loss: 6.1391e-09
Epoch 244/512
448/448 - 0s - loss: 4.0959e-07 - val_loss: 5.9861e-09
Epoch 245/512
448/448 - 0s - loss: 3.9072e-07 - val_loss: 5.8268e-09
Epoch 246/512
448/448 - 0s - loss: 3.7154e-07 - val_loss: 5.7337e-09
Epoch 247/512
448/448 - 0s - loss: 3.5862e-07 - val_loss: 5.6555e-09
Epoch 248/512
448/448 - 0s - loss: 3.4570e-07 - val_loss: 5.5286e-09
Epoch 249/512
448/448 - 0s - loss: 3.2931e-07 - val_loss: 5.3484e-09
Epoch 250/512
448/448 - 0s - loss: 3.0867e-07 - val_loss: 5.2483e-09
Epoch 251/512
448/448 - 0s - loss: 2.9937e-07 - val_loss: 5.2072e-09
Epoch 252/512
448/448 - 0s - loss: 2.9160e-07 - val_loss: 5.0692e-09
Epoch 253/512
448/448 - 0s - loss: 2.7369e-07 - val_loss: 4.9382e-09
Epoch 254/512
448/448 - 0s - loss: 2.6173e-07 - val_loss: 4.8174e-09
Epoch 255/512
448/448 - 0s - loss: 2.4838e-07 - val_loss: 4.7626e-09
Epoch 256/512
448/448 - 0s - loss: 2.4362e-07 - val_loss: 4.6684e-09
Epoch 257/512
448/448 - 0s - loss: 2.2983e-07 - val_loss: 4.5458e-09
Epoch 258/512
448/448 - 0s - loss: 2.1727e-07 - val_loss: 4.4798e-09
Epoch 259/512
448/448 - 0s - loss: 2.1166e-07 - val_loss: 4.3878e-09
Epoch 260/512
448/448 - 0s - loss: 1.9984e-07 - val_loss: 4.3032e-09
Epoch 261/512
448/448 - 0s - loss: 1.9269e-07 - val_loss: 4.2200e-09
Epoch 262/512
448/448 - 0s - loss: 1.8360e-07 - val_loss: 4.1205e-09
Epoch 263/512
448/448 - 0s - loss: 1.7340e-07 - val_loss: 4.0717e-09
Epoch 264/512
448/448 - 0s - loss: 1.7032e-07 - val_loss: 3.9639e-09
Epoch 265/512
448/448 - 0s - loss: 1.5710e-07 - val_loss: 3.8862e-09
Epoch 266/512
448/448 - 0s - loss: 1.5205e-07 - val_loss: 3.8651e-09
Epoch 267/512
448/448 - 0s - loss: 1.5109e-07 - val_loss: 3.7174e-09
Epoch 268/512
448/448 - 0s - loss: 1.3420e-07 - val_loss: 3.6540e-09
Epoch 269/512
448/448 - 0s - loss: 1.3388e-07 - val_loss: 3.6187e-09
Epoch 270/512
448/448 - 0s - loss: 1.2912e-07 - val_loss: 3.5252e-09
Epoch 271/512
448/448 - 0s - loss: 1.2033e-07 - val_loss: 3.4534e-09
Epoch 272/512
448/448 - 0s - loss: 1.1492e-07 - val_loss: 3.4221e-09
Epoch 273/512
448/448 - 0s - loss: 1.1269e-07 - val_loss: 3.3651e-09
Epoch 274/512
448/448 - 0s - loss: 1.0655e-07 - val_loss: 3.2659e-09
Epoch 275/512
448/448 - 0s - loss: 9.9122e-08 - val_loss: 3.2210e-09
Epoch 276/512
448/448 - 0s - loss: 9.6952e-08 - val_loss: 3.1825e-09
Epoch 277/512
448/448 - 0s - loss: 9.3463e-08 - val_loss: 3.1159e-09
Epoch 278/512
448/448 - 0s - loss: 8.7706e-08 - val_loss: 3.0536e-09
Epoch 279/512
448/448 - 0s - loss: 8.3059e-08 - val_loss: 3.0285e-09
Epoch 280/512
448/448 - 0s - loss: 8.2403e-08 - val_loss: 2.9617e-09
Epoch 281/512
448/448 - 0s - loss: 7.6818e-08 - val_loss: 2.8780e-09
Epoch 282/512
448/448 - 0s - loss: 7.0947e-08 - val_loss: 2.8539e-09
Epoch 283/512
448/448 - 0s - loss: 7.0891e-08 - val_loss: 2.8332e-09
Epoch 284/512
448/448 - 0s - loss: 6.9188e-08 - val_loss: 2.7466e-09
Epoch 285/512
448/448 - 0s - loss: 6.1843e-08 - val_loss: 2.7023e-09
Epoch 286/512
448/448 - 0s - loss: 6.1064e-08 - val_loss: 2.6735e-09
Epoch 287/512
448/448 - 0s - loss: 5.9176e-08 - val_loss: 2.6291e-09
Epoch 288/512
448/448 - 0s - loss: 5.5361e-08 - val_loss: 2.5940e-09
Epoch 289/512
448/448 - 0s - loss: 5.3995e-08 - val_loss: 2.5424e-09
Epoch 290/512
448/448 - 0s - loss: 5.0534e-08 - val_loss: 2.4884e-09
Epoch 291/512
448/448 - 0s - loss: 4.7499e-08 - val_loss: 2.4739e-09
Epoch 292/512
448/448 - 0s - loss: 4.7405e-08 - val_loss: 2.4496e-09
Epoch 293/512
448/448 - 0s - loss: 4.5669e-08 - val_loss: 2.3864e-09
Epoch 294/512
448/448 - 0s - loss: 4.1467e-08 - val_loss: 2.3382e-09
Epoch 295/512
448/448 - 0s - loss: 3.9530e-08 - val_loss: 2.3305e-09
Epoch 296/512
448/448 - 0s - loss: 3.9768e-08 - val_loss: 2.2986e-09
Epoch 297/512
448/448 - 0s - loss: 3.7661e-08 - val_loss: 2.2498e-09
Epoch 298/512
448/448 - 0s - loss: 3.4745e-08 - val_loss: 2.2135e-09
Epoch 299/512
448/448 - 0s - loss: 3.3548e-08 - val_loss: 2.1947e-09
Epoch 300/512
448/448 - 0s - loss: 3.2987e-08 - val_loss: 2.1690e-09
Epoch 301/512
448/448 - 0s - loss: 3.1569e-08 - val_loss: 2.1250e-09
Epoch 302/512
448/448 - 0s - loss: 2.9096e-08 - val_loss: 2.1002e-09
Epoch 303/512
448/448 - 0s - loss: 2.8565e-08 - val_loss: 2.0818e-09
Epoch 304/512
448/448 - 0s - loss: 2.7765e-08 - val_loss: 2.0509e-09
Epoch 305/512
448/448 - 0s - loss: 2.6328e-08 - val_loss: 2.0112e-09
Epoch 306/512
448/448 - 0s - loss: 2.4495e-08 - val_loss: 1.9885e-09
Epoch 307/512
448/448 - 0s - loss: 2.3943e-08 - val_loss: 1.9704e-09
Epoch 308/512
448/448 - 0s - loss: 2.3278e-08 - val_loss: 1.9408e-09
Epoch 309/512
448/448 - 0s - loss: 2.1999e-08 - val_loss: 1.9124e-09
Epoch 310/512
448/448 - 0s - loss: 2.0871e-08 - val_loss: 1.8867e-09
Epoch 311/512
448/448 - 0s - loss: 2.0177e-08 - val_loss: 1.8737e-09
Epoch 312/512
448/448 - 0s - loss: 1.9864e-08 - val_loss: 1.8381e-09
Epoch 313/512
448/448 - 0s - loss: 1.8137e-08 - val_loss: 1.8139e-09
Epoch 314/512
448/448 - 0s - loss: 1.7671e-08 - val_loss: 1.7993e-09
Epoch 315/512
448/448 - 0s - loss: 1.7335e-08 - val_loss: 1.7792e-09
Epoch 316/512
448/448 - 0s - loss: 1.6607e-08 - val_loss: 1.7503e-09
Epoch 317/512
448/448 - 0s - loss: 1.5480e-08 - val_loss: 1.7328e-09
Epoch 318/512
448/448 - 0s - loss: 1.5112e-08 - val_loss: 1.7200e-09
Epoch 319/512
448/448 - 0s - loss: 1.4967e-08 - val_loss: 1.6922e-09
Epoch 320/512
448/448 - 0s - loss: 1.3658e-08 - val_loss: 1.6739e-09
Epoch 321/512
448/448 - 0s - loss: 1.3414e-08 - val_loss: 1.6574e-09
Epoch 322/512
448/448 - 0s - loss: 1.3007e-08 - val_loss: 1.6404e-09
Epoch 323/512
448/448 - 0s - loss: 1.2585e-08 - val_loss: 1.6212e-09
Epoch 324/512
448/448 - 0s - loss: 1.2041e-08 - val_loss: 1.6007e-09
Epoch 325/512
448/448 - 0s - loss: 1.1256e-08 - val_loss: 1.5860e-09
Epoch 326/512
448/448 - 0s - loss: 1.1252e-08 - val_loss: 1.5676e-09
Epoch 327/512
448/448 - 0s - loss: 1.0597e-08 - val_loss: 1.5534e-09
Epoch 328/512
448/448 - 0s - loss: 1.0480e-08 - val_loss: 1.5339e-09
Epoch 329/512
448/448 - 0s - loss: 9.8473e-09 - val_loss: 1.5168e-09
Epoch 330/512
448/448 - 0s - loss: 9.5372e-09 - val_loss: 1.4996e-09
Epoch 331/512
448/448 - 0s - loss: 9.1777e-09 - val_loss: 1.4840e-09
Epoch 332/512
448/448 - 0s - loss: 8.8332e-09 - val_loss: 1.4708e-09
Epoch 333/512
448/448 - 0s - loss: 8.6702e-09 - val_loss: 1.4558e-09
Epoch 334/512
448/448 - 0s - loss: 8.2951e-09 - val_loss: 1.4394e-09
Epoch 335/512
448/448 - 0s - loss: 7.9065e-09 - val_loss: 1.4239e-09
Epoch 336/512
448/448 - 0s - loss: 7.7101e-09 - val_loss: 1.4137e-09
Epoch 337/512
448/448 - 0s - loss: 7.5793e-09 - val_loss: 1.4010e-09
Epoch 338/512
448/448 - 0s - loss: 7.3458e-09 - val_loss: 1.3827e-09
Epoch 339/512
448/448 - 0s - loss: 6.8722e-09 - val_loss: 1.3669e-09
Epoch 340/512
448/448 - 0s - loss: 6.6359e-09 - val_loss: 1.3563e-09
Epoch 341/512
448/448 - 0s - loss: 6.5674e-09 - val_loss: 1.3462e-09
Epoch 342/512
448/448 - 0s - loss: 6.4544e-09 - val_loss: 1.3310e-09
Epoch 343/512
448/448 - 0s - loss: 6.0185e-09 - val_loss: 1.3175e-09
Epoch 344/512
448/448 - 0s - loss: 5.9520e-09 - val_loss: 1.3086e-09
Epoch 345/512
448/448 - 0s - loss: 5.8467e-09 - val_loss: 1.2971e-09
Epoch 346/512
448/448 - 0s - loss: 5.6384e-09 - val_loss: 1.2824e-09
Epoch 347/512
448/448 - 0s - loss: 5.3947e-09 - val_loss: 1.2686e-09
Epoch 348/512
448/448 - 0s - loss: 5.1525e-09 - val_loss: 1.2580e-09
Epoch 349/512
448/448 - 0s - loss: 5.0811e-09 - val_loss: 1.2472e-09
Epoch 350/512
448/448 - 0s - loss: 4.9476e-09 - val_loss: 1.2339e-09
Epoch 351/512
448/448 - 0s - loss: 4.7112e-09 - val_loss: 1.2254e-09
Epoch 352/512
448/448 - 0s - loss: 4.7353e-09 - val_loss: 1.2171e-09
Epoch 353/512
448/448 - 0s - loss: 4.6355e-09 - val_loss: 1.2042e-09
Epoch 354/512
448/448 - 0s - loss: 4.3823e-09 - val_loss: 1.1917e-09
Epoch 355/512
448/448 - 0s - loss: 4.1719e-09 - val_loss: 1.1825e-09
Epoch 356/512
448/448 - 0s - loss: 4.1746e-09 - val_loss: 1.1753e-09
Epoch 357/512
448/448 - 0s - loss: 4.1366e-09 - val_loss: 1.1634e-09
Epoch 358/512
448/448 - 0s - loss: 3.9898e-09 - val_loss: 1.1514e-09
Epoch 359/512
448/448 - 0s - loss: 3.7822e-09 - val_loss: 1.1405e-09
Epoch 360/512
448/448 - 0s - loss: 3.6554e-09 - val_loss: 1.1308e-09
Epoch 361/512
448/448 - 0s - loss: 3.5932e-09 - val_loss: 1.1229e-09
Epoch 362/512
448/448 - 0s - loss: 3.5478e-09 - val_loss: 1.1162e-09
Epoch 363/512
448/448 - 0s - loss: 3.5212e-09 - val_loss: 1.1081e-09
Epoch 364/512
448/448 - 0s - loss: 3.4389e-09 - val_loss: 1.0966e-09
Epoch 365/512
448/448 - 0s - loss: 3.2396e-09 - val_loss: 1.0860e-09
Epoch 366/512
448/448 - 0s - loss: 3.1722e-09 - val_loss: 1.0783e-09
Epoch 367/512
448/448 - 0s - loss: 3.0923e-09 - val_loss: 1.0710e-09
Epoch 368/512
448/448 - 0s - loss: 3.0694e-09 - val_loss: 1.0640e-09
Epoch 369/512
448/448 - 0s - loss: 3.0532e-09 - val_loss: 1.0549e-09
Epoch 370/512
448/448 - 0s - loss: 2.9448e-09 - val_loss: 1.0439e-09
Epoch 371/512
448/448 - 0s - loss: 2.7817e-09 - val_loss: 1.0351e-09
Epoch 372/512
448/448 - 0s - loss: 2.7164e-09 - val_loss: 1.0295e-09
Epoch 373/512
448/448 - 0s - loss: 2.7370e-09 - val_loss: 1.0219e-09
Epoch 374/512
448/448 - 0s - loss: 2.6593e-09 - val_loss: 1.0137e-09
Epoch 375/512
448/448 - 0s - loss: 2.5822e-09 - val_loss: 1.0053e-09
Epoch 376/512
448/448 - 0s - loss: 2.5061e-09 - val_loss: 9.9820e-10
Epoch 377/512
448/448 - 0s - loss: 2.4659e-09 - val_loss: 9.9223e-10
Epoch 378/512
448/448 - 0s - loss: 2.4750e-09 - val_loss: 9.8493e-10
Epoch 379/512
448/448 - 0s - loss: 2.3841e-09 - val_loss: 9.7742e-10
Epoch 380/512
448/448 - 0s - loss: 2.3478e-09 - val_loss: 9.6899e-10
Epoch 381/512
448/448 - 0s - loss: 2.2362e-09 - val_loss: 9.6173e-10
Epoch 382/512
448/448 - 0s - loss: 2.2097e-09 - val_loss: 9.5430e-10
Epoch 383/512
448/448 - 0s - loss: 2.1619e-09 - val_loss: 9.4913e-10
Epoch 384/512
448/448 - 0s - loss: 2.1388e-09 - val_loss: 9.4385e-10
Epoch 385/512
448/448 - 0s - loss: 2.1861e-09 - val_loss: 9.3658e-10
Epoch 386/512
448/448 - 0s - loss: 2.0925e-09 - val_loss: 9.2766e-10
Epoch 387/512
448/448 - 0s - loss: 1.9711e-09 - val_loss: 9.1992e-10
Epoch 388/512
448/448 - 0s - loss: 1.9196e-09 - val_loss: 9.1581e-10
Epoch 389/512
448/448 - 0s - loss: 1.9649e-09 - val_loss: 9.1074e-10
Epoch 390/512
448/448 - 0s - loss: 1.9495e-09 - val_loss: 9.0395e-10
Epoch 391/512
448/448 - 0s - loss: 1.8747e-09 - val_loss: 8.9649e-10
Epoch 392/512
448/448 - 0s - loss: 1.8235e-09 - val_loss: 8.8997e-10
Epoch 393/512
448/448 - 0s - loss: 1.7930e-09 - val_loss: 8.8595e-10
Epoch 394/512
448/448 - 0s - loss: 1.8034e-09 - val_loss: 8.7964e-10
Epoch 395/512
448/448 - 0s - loss: 1.7475e-09 - val_loss: 8.7254e-10
Epoch 396/512
448/448 - 0s - loss: 1.6877e-09 - val_loss: 8.6616e-10
Epoch 397/512
448/448 - 0s - loss: 1.6584e-09 - val_loss: 8.6186e-10
Epoch 398/512
448/448 - 0s - loss: 1.6723e-09 - val_loss: 8.5658e-10
Epoch 399/512
448/448 - 0s - loss: 1.6379e-09 - val_loss: 8.5186e-10
Epoch 400/512
448/448 - 0s - loss: 1.6137e-09 - val_loss: 8.4499e-10
Epoch 401/512
448/448 - 0s - loss: 1.5627e-09 - val_loss: 8.4006e-10
Epoch 402/512
448/448 - 0s - loss: 1.5536e-09 - val_loss: 8.3358e-10
Epoch 403/512
448/448 - 0s - loss: 1.5045e-09 - val_loss: 8.2773e-10
Epoch 404/512
448/448 - 0s - loss: 1.4646e-09 - val_loss: 8.2469e-10
Epoch 405/512
448/448 - 0s - loss: 1.5035e-09 - val_loss: 8.2063e-10
Epoch 406/512
448/448 - 0s - loss: 1.4696e-09 - val_loss: 8.1345e-10
Epoch 407/512
448/448 - 0s - loss: 1.4034e-09 - val_loss: 8.0845e-10
Epoch 408/512
448/448 - 0s - loss: 1.3968e-09 - val_loss: 8.0397e-10
Epoch 409/512
448/448 - 0s - loss: 1.3743e-09 - val_loss: 7.9993e-10
Epoch 410/512
448/448 - 0s - loss: 1.3842e-09 - val_loss: 7.9501e-10
Epoch 411/512
448/448 - 0s - loss: 1.3614e-09 - val_loss: 7.9029e-10
Epoch 412/512
448/448 - 0s - loss: 1.3374e-09 - val_loss: 7.8544e-10
Epoch 413/512
448/448 - 0s - loss: 1.3094e-09 - val_loss: 7.7983e-10
Epoch 414/512
448/448 - 0s - loss: 1.2678e-09 - val_loss: 7.7402e-10
Epoch 415/512
448/448 - 0s - loss: 1.2398e-09 - val_loss: 7.6935e-10
Epoch 416/512
448/448 - 0s - loss: 1.2304e-09 - val_loss: 7.6581e-10
Epoch 417/512
448/448 - 0s - loss: 1.2249e-09 - val_loss: 7.6219e-10
Epoch 418/512
448/448 - 0s - loss: 1.2163e-09 - val_loss: 7.5824e-10
Epoch 419/512
448/448 - 0s - loss: 1.2196e-09 - val_loss: 7.5419e-10
Epoch 420/512
448/448 - 0s - loss: 1.1910e-09 - val_loss: 7.4930e-10
Epoch 421/512
448/448 - 0s - loss: 1.1691e-09 - val_loss: 7.4434e-10
Epoch 422/512
448/448 - 0s - loss: 1.1307e-09 - val_loss: 7.4004e-10
Epoch 423/512
448/448 - 0s - loss: 1.1207e-09 - val_loss: 7.3675e-10
Epoch 424/512
448/448 - 0s - loss: 1.1280e-09 - val_loss: 7.3247e-10
Epoch 425/512
448/448 - 0s - loss: 1.1129e-09 - val_loss: 7.2779e-10
Epoch 426/512
448/448 - 0s - loss: 1.0798e-09 - val_loss: 7.2321e-10
Epoch 427/512
448/448 - 0s - loss: 1.0517e-09 - val_loss: 7.2011e-10
Epoch 428/512
448/448 - 0s - loss: 1.0615e-09 - val_loss: 7.1550e-10
Epoch 429/512
448/448 - 0s - loss: 1.0410e-09 - val_loss: 7.1231e-10
Epoch 430/512
448/448 - 0s - loss: 1.0361e-09 - val_loss: 7.0869e-10
Epoch 431/512
448/448 - 0s - loss: 1.0204e-09 - val_loss: 7.0445e-10
Epoch 432/512
448/448 - 0s - loss: 9.9735e-10 - val_loss: 7.0188e-10
Epoch 433/512
448/448 - 0s - loss: 1.0229e-09 - val_loss: 6.9786e-10
Epoch 434/512
448/448 - 0s - loss: 1.0007e-09 - val_loss: 6.9446e-10
Epoch 435/512
448/448 - 0s - loss: 9.9310e-10 - val_loss: 6.9049e-10
Epoch 436/512
448/448 - 0s - loss: 9.7503e-10 - val_loss: 6.8640e-10
Epoch 437/512
448/448 - 0s - loss: 9.3568e-10 - val_loss: 6.8218e-10
Epoch 438/512
448/448 - 0s - loss: 9.3663e-10 - val_loss: 6.7920e-10
Epoch 439/512
448/448 - 0s - loss: 9.2651e-10 - val_loss: 6.7525e-10
Epoch 440/512
448/448 - 0s - loss: 9.0976e-10 - val_loss: 6.7131e-10
Epoch 441/512
448/448 - 0s - loss: 9.0145e-10 - val_loss: 6.6856e-10
Epoch 442/512
448/448 - 0s - loss: 8.9798e-10 - val_loss: 6.6597e-10
Epoch 443/512
448/448 - 0s - loss: 8.9459e-10 - val_loss: 6.6227e-10
Epoch 444/512
448/448 - 0s - loss: 8.7798e-10 - val_loss: 6.5879e-10
Epoch 445/512
448/448 - 0s - loss: 8.6810e-10 - val_loss: 6.5493e-10
Epoch 446/512
448/448 - 0s - loss: 8.5667e-10 - val_loss: 6.5191e-10
Epoch 447/512
448/448 - 0s - loss: 8.5272e-10 - val_loss: 6.4917e-10
Epoch 448/512
448/448 - 0s - loss: 8.4091e-10 - val_loss: 6.4575e-10
Epoch 449/512
448/448 - 0s - loss: 8.2934e-10 - val_loss: 6.4224e-10
Epoch 450/512
448/448 - 0s - loss: 8.2464e-10 - val_loss: 6.3970e-10
Epoch 451/512
448/448 - 0s - loss: 8.1697e-10 - val_loss: 6.3605e-10
Epoch 452/512
448/448 - 0s - loss: 7.9808e-10 - val_loss: 6.3420e-10
Epoch 453/512
448/448 - 0s - loss: 8.1135e-10 - val_loss: 6.3017e-10
Epoch 454/512
448/448 - 0s - loss: 7.9326e-10 - val_loss: 6.2710e-10
Epoch 455/512
448/448 - 0s - loss: 7.7191e-10 - val_loss: 6.2372e-10
Epoch 456/512
448/448 - 0s - loss: 7.7262e-10 - val_loss: 6.2180e-10
Epoch 457/512
448/448 - 0s - loss: 7.6452e-10 - val_loss: 6.1835e-10
Epoch 458/512
448/448 - 0s - loss: 7.4831e-10 - val_loss: 6.1518e-10
Epoch 459/512
448/448 - 0s - loss: 7.4294e-10 - val_loss: 6.1234e-10
Epoch 460/512
448/448 - 0s - loss: 7.3402e-10 - val_loss: 6.0962e-10
Epoch 461/512
448/448 - 0s - loss: 7.3197e-10 - val_loss: 6.0700e-10
Epoch 462/512
448/448 - 0s - loss: 7.3129e-10 - val_loss: 6.0361e-10
Epoch 463/512
448/448 - 0s - loss: 7.2642e-10 - val_loss: 6.0093e-10
Epoch 464/512
448/448 - 0s - loss: 7.0471e-10 - val_loss: 5.9785e-10
Epoch 465/512
448/448 - 0s - loss: 7.0142e-10 - val_loss: 5.9540e-10
Epoch 466/512
448/448 - 0s - loss: 7.0407e-10 - val_loss: 5.9296e-10
Epoch 467/512
448/448 - 0s - loss: 6.8702e-10 - val_loss: 5.8977e-10
Epoch 468/512
448/448 - 0s - loss: 6.7889e-10 - val_loss: 5.8778e-10
Epoch 469/512
448/448 - 0s - loss: 6.8529e-10 - val_loss: 5.8589e-10
Epoch 470/512
448/448 - 0s - loss: 6.8796e-10 - val_loss: 5.8312e-10
Epoch 471/512
448/448 - 0s - loss: 6.6890e-10 - val_loss: 5.8021e-10
Epoch 472/512
448/448 - 0s - loss: 6.6600e-10 - val_loss: 5.7700e-10
Epoch 473/512
448/448 - 0s - loss: 6.5586e-10 - val_loss: 5.7575e-10
Epoch 474/512
448/448 - 0s - loss: 6.5872e-10 - val_loss: 5.7346e-10
Epoch 475/512
448/448 - 0s - loss: 6.6072e-10 - val_loss: 5.7021e-10
Epoch 476/512
448/448 - 0s - loss: 6.4332e-10 - val_loss: 5.6699e-10
Epoch 477/512
448/448 - 0s - loss: 6.3147e-10 - val_loss: 5.6430e-10
Epoch 478/512
448/448 - 0s - loss: 6.2163e-10 - val_loss: 5.6226e-10
Epoch 479/512
448/448 - 0s - loss: 6.2384e-10 - val_loss: 5.5965e-10
Epoch 480/512
448/448 - 0s - loss: 6.1233e-10 - val_loss: 5.5721e-10
Epoch 481/512
448/448 - 0s - loss: 6.0355e-10 - val_loss: 5.5498e-10
Epoch 482/512
448/448 - 0s - loss: 6.0824e-10 - val_loss: 5.5372e-10
Epoch 483/512
448/448 - 0s - loss: 6.0805e-10 - val_loss: 5.5092e-10
Epoch 484/512
448/448 - 0s - loss: 5.9500e-10 - val_loss: 5.4865e-10
Epoch 485/512
448/448 - 0s - loss: 5.9249e-10 - val_loss: 5.4612e-10
Epoch 486/512
448/448 - 0s - loss: 5.9122e-10 - val_loss: 5.4446e-10
Epoch 487/512
448/448 - 0s - loss: 5.8447e-10 - val_loss: 5.4154e-10
Epoch 488/512
448/448 - 0s - loss: 5.7009e-10 - val_loss: 5.3894e-10
Epoch 489/512
448/448 - 0s - loss: 5.6553e-10 - val_loss: 5.3750e-10
Epoch 490/512
448/448 - 0s - loss: 5.7025e-10 - val_loss: 5.3505e-10
Epoch 491/512
448/448 - 0s - loss: 5.7171e-10 - val_loss: 5.3361e-10
Epoch 492/512
448/448 - 0s - loss: 5.7492e-10 - val_loss: 5.3193e-10
Epoch 493/512
448/448 - 0s - loss: 5.7476e-10 - val_loss: 5.2958e-10
Epoch 494/512
448/448 - 0s - loss: 5.6284e-10 - val_loss: 5.2627e-10
Epoch 495/512
448/448 - 0s - loss: 5.4600e-10 - val_loss: 5.2375e-10
Epoch 496/512
448/448 - 0s - loss: 5.3961e-10 - val_loss: 5.2225e-10
Epoch 497/512
448/448 - 0s - loss: 5.3180e-10 - val_loss: 5.1944e-10
Epoch 498/512
448/448 - 0s - loss: 5.3033e-10 - val_loss: 5.1797e-10
Epoch 499/512
448/448 - 0s - loss: 5.2730e-10 - val_loss: 5.1614e-10
Epoch 500/512
448/448 - 0s - loss: 5.3103e-10 - val_loss: 5.1451e-10
Epoch 501/512
448/448 - 0s - loss: 5.3010e-10 - val_loss: 5.1252e-10
Epoch 502/512
448/448 - 0s - loss: 5.3087e-10 - val_loss: 5.1036e-10
Epoch 503/512
448/448 - 0s - loss: 5.2317e-10 - val_loss: 5.0858e-10
Epoch 504/512
448/448 - 0s - loss: 5.1941e-10 - val_loss: 5.0710e-10
Epoch 505/512
448/448 - 0s - loss: 5.1796e-10 - val_loss: 5.0443e-10
Epoch 506/512
448/448 - 0s - loss: 5.1137e-10 - val_loss: 5.0273e-10
Epoch 507/512
448/448 - 0s - loss: 5.0517e-10 - val_loss: 5.0058e-10
Epoch 508/512
448/448 - 0s - loss: 4.9773e-10 - val_loss: 4.9800e-10
Epoch 509/512
448/448 - 0s - loss: 4.8900e-10 - val_loss: 4.9632e-10
Epoch 510/512
448/448 - 0s - loss: 4.8473e-10 - val_loss: 4.9488e-10
Epoch 511/512
448/448 - 0s - loss: 4.8736e-10 - val_loss: 4.9354e-10
Epoch 512/512
448/448 - 0s - loss: 4.9215e-10 - val_loss: 4.9163e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2127e-08 - val_loss: 2.5548e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5495e-08 - val_loss: 1.7261e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0493e-09 - val_loss: 3.7429e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.3863e-10 - val_loss: 3.5232e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8165e-10 - val_loss: 1.0546e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1518e-09 - val_loss: 6.0754e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5379e-09 - val_loss: 7.6111e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0112e-09 - val_loss: 2.6600e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1951e-09 - val_loss: 1.5378e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6247e-09 - val_loss: 1.9538e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5161e-09 - val_loss: 3.6933e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4279e-09 - val_loss: 4.6301e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4478e-09 - val_loss: 3.2044e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9487e-09 - val_loss: 2.2715e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2904e-09 - val_loss: 2.2607e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5109e-09 - val_loss: 2.8010e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1008e-09 - val_loss: 3.1719e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2445e-09 - val_loss: 2.8768e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8204e-09 - val_loss: 2.4187e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4238e-09 - val_loss: 2.2441e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3491e-09 - val_loss: 2.3440e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4773e-09 - val_loss: 2.4611e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5630e-09 - val_loss: 2.4411e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4792e-09 - val_loss: 2.2521e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2798e-09 - val_loss: 2.1123e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1709e-09 - val_loss: 2.0890e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1608e-09 - val_loss: 2.0866e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1568e-09 - val_loss: 2.0516e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1100e-09 - val_loss: 1.9899e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0368e-09 - val_loss: 1.9236e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9692e-09 - val_loss: 1.8740e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9234e-09 - val_loss: 1.8443e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8989e-09 - val_loss: 1.8076e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8583e-09 - val_loss: 1.7627e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8098e-09 - val_loss: 1.7134e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7576e-09 - val_loss: 1.6600e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7035e-09 - val_loss: 1.6326e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6814e-09 - val_loss: 1.6036e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6468e-09 - val_loss: 1.5785e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6183e-09 - val_loss: 1.5451e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5791e-09 - val_loss: 1.4967e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5314e-09 - val_loss: 1.4570e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4971e-09 - val_loss: 1.4366e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4765e-09 - val_loss: 1.4123e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4437e-09 - val_loss: 1.3695e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4078e-09 - val_loss: 1.3545e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3874e-09 - val_loss: 1.3122e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3408e-09 - val_loss: 1.2906e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3234e-09 - val_loss: 1.2706e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3023e-09 - val_loss: 1.2426e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2715e-09 - val_loss: 1.2249e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2574e-09 - val_loss: 1.2130e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2356e-09 - val_loss: 1.1796e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2031e-09 - val_loss: 1.1368e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1637e-09 - val_loss: 1.1123e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1387e-09 - val_loss: 1.1022e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1307e-09 - val_loss: 1.0843e-09
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1114e-09 - val_loss: 1.0770e-09
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0972e-09 - val_loss: 1.0537e-09
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0770e-09 - val_loss: 1.0314e-09
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0490e-09 - val_loss: 9.8865e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0050e-09 - val_loss: 9.6725e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9661e-10 - val_loss: 9.6988e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9452e-10 - val_loss: 9.5399e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7298e-10 - val_loss: 9.4050e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6546e-10 - val_loss: 9.3090e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4836e-10 - val_loss: 9.1430e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3014e-10 - val_loss: 8.9079e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0614e-10 - val_loss: 8.6778e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8375e-10 - val_loss: 8.5386e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7643e-10 - val_loss: 8.5456e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7708e-10 - val_loss: 8.3652e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5204e-10 - val_loss: 8.1979e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3235e-10 - val_loss: 7.9794e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1842e-10 - val_loss: 7.9748e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1590e-10 - val_loss: 7.8761e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0660e-10 - val_loss: 7.7467e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8610e-10 - val_loss: 7.5353e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7157e-10 - val_loss: 7.5188e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6668e-10 - val_loss: 7.4409e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5403e-10 - val_loss: 7.2525e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3680e-10 - val_loss: 7.1168e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2536e-10 - val_loss: 7.0049e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1280e-10 - val_loss: 6.9387e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0753e-10 - val_loss: 6.8808e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0030e-10 - val_loss: 6.7434e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8371e-10 - val_loss: 6.5564e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6906e-10 - val_loss: 6.5495e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6878e-10 - val_loss: 6.4522e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5938e-10 - val_loss: 6.4097e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5221e-10 - val_loss: 6.2846e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3515e-10 - val_loss: 6.1894e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2903e-10 - val_loss: 6.1132e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1922e-10 - val_loss: 6.0229e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1419e-10 - val_loss: 5.9328e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0007e-10 - val_loss: 5.7977e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9183e-10 - val_loss: 5.7409e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8551e-10 - val_loss: 5.7414e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8272e-10 - val_loss: 5.6086e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7027e-10 - val_loss: 5.4931e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5978e-10 - val_loss: 5.5205e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6290e-10 - val_loss: 5.5135e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5884e-10 - val_loss: 5.3758e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4459e-10 - val_loss: 5.2361e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3142e-10 - val_loss: 5.1736e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2349e-10 - val_loss: 5.1478e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2026e-10 - val_loss: 5.0381e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1093e-10 - val_loss: 4.9795e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0773e-10 - val_loss: 4.9781e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0511e-10 - val_loss: 4.9010e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9459e-10 - val_loss: 4.8323e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9146e-10 - val_loss: 4.7943e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8649e-10 - val_loss: 4.7492e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8396e-10 - val_loss: 4.7182e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7683e-10 - val_loss: 4.5640e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6217e-10 - val_loss: 4.4843e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5232e-10 - val_loss: 4.3890e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4545e-10 - val_loss: 4.3825e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4678e-10 - val_loss: 4.4747e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5956e-10 - val_loss: 4.5327e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5860e-10 - val_loss: 4.4567e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4853e-10 - val_loss: 4.3186e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3577e-10 - val_loss: 4.1943e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2303e-10 - val_loss: 4.0991e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1541e-10 - val_loss: 4.0934e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1691e-10 - val_loss: 4.1561e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2194e-10 - val_loss: 4.1026e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1692e-10 - val_loss: 4.1296e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1791e-10 - val_loss: 4.0526e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0466e-10 - val_loss: 3.8973e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9279e-10 - val_loss: 3.8088e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8867e-10 - val_loss: 3.8524e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9231e-10 - val_loss: 3.8814e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9306e-10 - val_loss: 3.8389e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8653e-10 - val_loss: 3.7402e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7787e-10 - val_loss: 3.6658e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7379e-10 - val_loss: 3.6925e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7284e-10 - val_loss: 3.6825e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7455e-10 - val_loss: 3.6763e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7157e-10 - val_loss: 3.6184e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6658e-10 - val_loss: 3.5775e-10
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.6051e-10 - val_loss: 3.4939e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.5056e-10 - val_loss: 3.4246e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.4630e-10 - val_loss: 3.4014e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.4305e-10 - val_loss: 3.3679e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.3974e-10 - val_loss: 3.2960e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3509e-10 - val_loss: 3.3246e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3742e-10 - val_loss: 3.3787e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4030e-10 - val_loss: 3.3142e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.3721e-10 - val_loss: 3.2891e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.3112e-10 - val_loss: 3.2038e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2515e-10 - val_loss: 3.2464e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2810e-10 - val_loss: 3.1977e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.2230e-10 - val_loss: 3.0993e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.1240e-10 - val_loss: 3.0980e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.1322e-10 - val_loss: 3.0647e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1116e-10 - val_loss: 3.0727e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.1132e-10 - val_loss: 3.0062e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 3.0329e-10 - val_loss: 2.9695e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9943e-10 - val_loss: 2.9333e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9685e-10 - val_loss: 2.9205e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9560e-10 - val_loss: 2.9063e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9426e-10 - val_loss: 2.8862e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.9153e-10 - val_loss: 2.8429e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8648e-10 - val_loss: 2.8269e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8650e-10 - val_loss: 2.8329e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8573e-10 - val_loss: 2.8162e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8500e-10 - val_loss: 2.8595e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8827e-10 - val_loss: 2.7985e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.8334e-10 - val_loss: 2.7717e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.7997e-10 - val_loss: 2.7382e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.7644e-10 - val_loss: 2.6649e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6716e-10 - val_loss: 2.6009e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.6124e-10 - val_loss: 2.5159e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5379e-10 - val_loss: 2.5173e-10
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5516e-10 - val_loss: 2.5711e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6196e-10 - val_loss: 2.6175e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6511e-10 - val_loss: 2.5759e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5907e-10 - val_loss: 2.5253e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.5407e-10 - val_loss: 2.4965e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5203e-10 - val_loss: 2.5038e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5372e-10 - val_loss: 2.5438e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5691e-10 - val_loss: 2.5172e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.5106e-10 - val_loss: 2.4594e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4625e-10 - val_loss: 2.4194e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4406e-10 - val_loss: 2.3999e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4270e-10 - val_loss: 2.3966e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4276e-10 - val_loss: 2.4117e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4308e-10 - val_loss: 2.3732e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.4026e-10 - val_loss: 2.3649e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3901e-10 - val_loss: 2.3522e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3726e-10 - val_loss: 2.3336e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3599e-10 - val_loss: 2.3561e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3772e-10 - val_loss: 2.3720e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3942e-10 - val_loss: 2.3413e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.3431e-10 - val_loss: 2.2665e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2685e-10 - val_loss: 2.2170e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2341e-10 - val_loss: 2.1834e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2177e-10 - val_loss: 2.2022e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2204e-10 - val_loss: 2.1906e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2264e-10 - val_loss: 2.2253e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2454e-10 - val_loss: 2.2097e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2204e-10 - val_loss: 2.1951e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.2099e-10 - val_loss: 2.1607e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.1687e-10 - val_loss: 2.1182e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.1311e-10 - val_loss: 2.0992e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1180e-10 - val_loss: 2.1080e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1166e-10 - val_loss: 2.1101e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1374e-10 - val_loss: 2.1239e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.1470e-10 - val_loss: 2.0799e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0839e-10 - val_loss: 1.9941e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0102e-10 - val_loss: 1.9980e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0163e-10 - val_loss: 2.0125e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0468e-10 - val_loss: 2.0360e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0426e-10 - val_loss: 2.0050e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 2.0059e-10 - val_loss: 1.9561e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9641e-10 - val_loss: 1.9401e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9522e-10 - val_loss: 1.9383e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9533e-10 - val_loss: 1.9331e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9517e-10 - val_loss: 1.9158e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9319e-10 - val_loss: 1.9043e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9166e-10 - val_loss: 1.8859e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8915e-10 - val_loss: 1.8766e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8933e-10 - val_loss: 1.8912e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9120e-10 - val_loss: 1.8921e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.9068e-10 - val_loss: 1.8761e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8833e-10 - val_loss: 1.8573e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8729e-10 - val_loss: 1.8631e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8666e-10 - val_loss: 1.8367e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8481e-10 - val_loss: 1.7940e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.8102e-10 - val_loss: 1.7809e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7976e-10 - val_loss: 1.8059e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8207e-10 - val_loss: 1.7928e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7908e-10 - val_loss: 1.7498e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7593e-10 - val_loss: 1.7256e-10
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7572e-10 - val_loss: 1.7751e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7912e-10 - val_loss: 1.7813e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8032e-10 - val_loss: 1.8146e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8265e-10 - val_loss: 1.7939e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7902e-10 - val_loss: 1.7709e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7713e-10 - val_loss: 1.7331e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7560e-10 - val_loss: 1.7245e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7300e-10 - val_loss: 1.6964e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.7103e-10 - val_loss: 1.6868e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6898e-10 - val_loss: 1.6649e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6774e-10 - val_loss: 1.6526e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6469e-10 - val_loss: 1.6225e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6246e-10 - val_loss: 1.6261e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6512e-10 - val_loss: 1.6660e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6910e-10 - val_loss: 1.6508e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6638e-10 - val_loss: 1.6766e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6929e-10 - val_loss: 1.6454e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6503e-10 - val_loss: 1.6062e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6141e-10 - val_loss: 1.6103e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.6343e-10 - val_loss: 1.5932e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5936e-10 - val_loss: 1.5611e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5689e-10 - val_loss: 1.5506e-10
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5728e-10 - val_loss: 1.5602e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5556e-10 - val_loss: 1.5440e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5625e-10 - val_loss: 1.5644e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5706e-10 - val_loss: 1.5628e-10
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5681e-10 - val_loss: 1.5601e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5546e-10 - val_loss: 1.5269e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5399e-10 - val_loss: 1.5365e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5353e-10 - val_loss: 1.5139e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5225e-10 - val_loss: 1.5319e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5386e-10 - val_loss: 1.5011e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5134e-10 - val_loss: 1.5077e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5256e-10 - val_loss: 1.5087e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5192e-10 - val_loss: 1.5264e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5300e-10 - val_loss: 1.5161e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.5083e-10 - val_loss: 1.4903e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4898e-10 - val_loss: 1.4737e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4813e-10 - val_loss: 1.4832e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4991e-10 - val_loss: 1.4707e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4726e-10 - val_loss: 1.4341e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4376e-10 - val_loss: 1.4118e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4257e-10 - val_loss: 1.4329e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4427e-10 - val_loss: 1.4277e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4378e-10 - val_loss: 1.4340e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4374e-10 - val_loss: 1.4274e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4411e-10 - val_loss: 1.4348e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4467e-10 - val_loss: 1.4457e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4447e-10 - val_loss: 1.4290e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4248e-10 - val_loss: 1.4145e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4161e-10 - val_loss: 1.4084e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4208e-10 - val_loss: 1.4065e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.4115e-10 - val_loss: 1.3782e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3692e-10 - val_loss: 1.3487e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3693e-10 - val_loss: 1.3649e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3748e-10 - val_loss: 1.3513e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3507e-10 - val_loss: 1.3240e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3235e-10 - val_loss: 1.3173e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3222e-10 - val_loss: 1.3153e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3068e-10 - val_loss: 1.2851e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3049e-10 - val_loss: 1.3039e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3129e-10 - val_loss: 1.2988e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3138e-10 - val_loss: 1.3296e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3462e-10 - val_loss: 1.3382e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3445e-10 - val_loss: 1.3355e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3428e-10 - val_loss: 1.3355e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3421e-10 - val_loss: 1.3199e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3227e-10 - val_loss: 1.2964e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.3021e-10 - val_loss: 1.2830e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2720e-10 - val_loss: 1.2476e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2521e-10 - val_loss: 1.2539e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2757e-10 - val_loss: 1.2753e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2721e-10 - val_loss: 1.2484e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2598e-10 - val_loss: 1.2730e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2876e-10 - val_loss: 1.2869e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2936e-10 - val_loss: 1.2700e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2827e-10 - val_loss: 1.2906e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2908e-10 - val_loss: 1.2563e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2547e-10 - val_loss: 1.2247e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2359e-10 - val_loss: 1.2518e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2617e-10 - val_loss: 1.2515e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2482e-10 - val_loss: 1.2385e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2532e-10 - val_loss: 1.2485e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2467e-10 - val_loss: 1.2226e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.2167e-10 - val_loss: 1.1902e-10
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1915e-10 - val_loss: 1.1829e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1788e-10 - val_loss: 1.1742e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1850e-10 - val_loss: 1.1874e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1832e-10 - val_loss: 1.1697e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1860e-10 - val_loss: 1.2038e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2187e-10 - val_loss: 1.2139e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2249e-10 - val_loss: 1.2229e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2266e-10 - val_loss: 1.2077e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2021e-10 - val_loss: 1.1735e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1730e-10 - val_loss: 1.1584e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1559e-10 - val_loss: 1.1283e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1420e-10 - val_loss: 1.1463e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1530e-10 - val_loss: 1.1531e-10
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1612e-10 - val_loss: 1.1673e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1829e-10 - val_loss: 1.1832e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1975e-10 - val_loss: 1.1967e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2062e-10 - val_loss: 1.1865e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1887e-10 - val_loss: 1.1555e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1615e-10 - val_loss: 1.1534e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1458e-10 - val_loss: 1.1109e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1168e-10 - val_loss: 1.1190e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1221e-10 - val_loss: 1.1114e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1165e-10 - val_loss: 1.1076e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1267e-10 - val_loss: 1.1502e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1564e-10 - val_loss: 1.1499e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1474e-10 - val_loss: 1.1322e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1308e-10 - val_loss: 1.1238e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1284e-10 - val_loss: 1.1325e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1324e-10 - val_loss: 1.1207e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.1154e-10 - val_loss: 1.0850e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0964e-10 - val_loss: 1.1045e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1148e-10 - val_loss: 1.1102e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1077e-10 - val_loss: 1.0910e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0929e-10 - val_loss: 1.0778e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0737e-10 - val_loss: 1.0643e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0705e-10 - val_loss: 1.0687e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0726e-10 - val_loss: 1.0643e-10
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0682e-10 - val_loss: 1.0511e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0543e-10 - val_loss: 1.0710e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0757e-10 - val_loss: 1.0828e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0922e-10 - val_loss: 1.0914e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0953e-10 - val_loss: 1.0824e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0888e-10 - val_loss: 1.0883e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0916e-10 - val_loss: 1.0755e-10
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0684e-10 - val_loss: 1.0358e-10
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0381e-10 - val_loss: 1.0345e-10
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0342e-10 - val_loss: 1.0324e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0413e-10 - val_loss: 1.0344e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0403e-10 - val_loss: 1.0339e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0440e-10 - val_loss: 1.0434e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0453e-10 - val_loss: 1.0328e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0367e-10 - val_loss: 1.0386e-10
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0360e-10 - val_loss: 1.0159e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0170e-10 - val_loss: 1.0242e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0299e-10 - val_loss: 1.0305e-10
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 1.0219e-10 - val_loss: 9.9363e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.9785e-11 - val_loss: 9.8063e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.7749e-11 - val_loss: 9.6068e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.6519e-11 - val_loss: 9.3828e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.3576e-11 - val_loss: 9.2629e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3883e-11 - val_loss: 9.4348e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5091e-11 - val_loss: 9.5499e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6083e-11 - val_loss: 9.6026e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6538e-11 - val_loss: 9.6119e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6563e-11 - val_loss: 9.7269e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7344e-11 - val_loss: 9.7315e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7242e-11 - val_loss: 9.6864e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7585e-11 - val_loss: 9.8684e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9094e-11 - val_loss: 9.7346e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7663e-11 - val_loss: 9.7558e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7918e-11 - val_loss: 9.6742e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6168e-11 - val_loss: 9.3894e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.4041e-11 - val_loss: 9.1889e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2568e-11 - val_loss: 9.2628e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4190e-11 - val_loss: 9.4544e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3941e-11 - val_loss: 9.2832e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3249e-11 - val_loss: 9.2681e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.2803e-11 - val_loss: 9.1389e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2525e-11 - val_loss: 9.4089e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4176e-11 - val_loss: 9.3531e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4043e-11 - val_loss: 9.4619e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4873e-11 - val_loss: 9.4123e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3681e-11 - val_loss: 9.3847e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4002e-11 - val_loss: 9.3571e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4782e-11 - val_loss: 9.3690e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3545e-11 - val_loss: 9.2082e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1973e-11 - val_loss: 9.3105e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3435e-11 - val_loss: 9.3711e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3544e-11 - val_loss: 9.2676e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3024e-11 - val_loss: 9.2288e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 9.1032e-11 - val_loss: 8.8641e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.8536e-11 - val_loss: 8.6575e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6594e-11 - val_loss: 8.6663e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7245e-11 - val_loss: 8.8609e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9586e-11 - val_loss: 8.8932e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8861e-11 - val_loss: 8.8316e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8326e-11 - val_loss: 8.7667e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7190e-11 - val_loss: 8.7937e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8915e-11 - val_loss: 8.9698e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9943e-11 - val_loss: 8.7825e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.7190e-11 - val_loss: 8.6464e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.6395e-11 - val_loss: 8.6434e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7320e-11 - val_loss: 8.6942e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7541e-11 - val_loss: 8.8229e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9163e-11 - val_loss: 8.8385e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.8161e-11 - val_loss: 8.6169e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.6333e-11 - val_loss: 8.5633e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5791e-11 - val_loss: 8.6486e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7142e-11 - val_loss: 8.7895e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8310e-11 - val_loss: 8.8543e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8358e-11 - val_loss: 8.9157e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9617e-11 - val_loss: 8.9407e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8862e-11 - val_loss: 8.5813e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.5559e-11 - val_loss: 8.4515e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.3848e-11 - val_loss: 8.1116e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1878e-11 - val_loss: 8.3112e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3265e-11 - val_loss: 8.3474e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4048e-11 - val_loss: 8.5591e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5814e-11 - val_loss: 8.6757e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6963e-11 - val_loss: 8.8154e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8727e-11 - val_loss: 8.9478e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8648e-11 - val_loss: 8.6099e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5106e-11 - val_loss: 8.2442e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2643e-11 - val_loss: 8.2435e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2791e-11 - val_loss: 8.2482e-11
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 8.1961e-11 - val_loss: 7.9878e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.9596e-11 - val_loss: 7.8787e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9294e-11 - val_loss: 7.9851e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1397e-11 - val_loss: 8.2458e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3183e-11 - val_loss: 8.2764e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3687e-11 - val_loss: 8.4127e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5042e-11 - val_loss: 8.5188e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5195e-11 - val_loss: 8.3965e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4303e-11 - val_loss: 8.3968e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4259e-11 - val_loss: 8.3793e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3754e-11 - val_loss: 8.3727e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4737e-11 - val_loss: 8.4592e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4573e-11 - val_loss: 8.3442e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3260e-11 - val_loss: 8.1101e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1674e-11 - val_loss: 8.1500e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1781e-11 - val_loss: 8.0835e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0903e-11 - val_loss: 7.9332e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.9509e-11 - val_loss: 7.8550e-11
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.8441e-11 - val_loss: 7.7265e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.6862e-11 - val_loss: 7.6457e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7207e-11 - val_loss: 7.7509e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8762e-11 - val_loss: 8.0280e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0770e-11 - val_loss: 8.1687e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1846e-11 - val_loss: 7.9958e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9466e-11 - val_loss: 7.7471e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7475e-11 - val_loss: 7.7575e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8675e-11 - val_loss: 8.0148e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0884e-11 - val_loss: 8.1032e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1066e-11 - val_loss: 8.0384e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9325e-11 - val_loss: 7.7100e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.7118e-11 - val_loss: 7.6332e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.6181e-11 - val_loss: 7.5730e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.5578e-11 - val_loss: 7.5180e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5855e-11 - val_loss: 7.6382e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6905e-11 - val_loss: 7.6540e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7228e-11 - val_loss: 7.7265e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7775e-11 - val_loss: 7.7227e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7147e-11 - val_loss: 7.7231e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6937e-11 - val_loss: 7.5330e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5473e-11 - val_loss: 7.6083e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6642e-11 - val_loss: 7.7533e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8196e-11 - val_loss: 7.8563e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7955e-11 - val_loss: 7.7152e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7120e-11 - val_loss: 7.5657e-11
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.5152e-11 - val_loss: 7.4007e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4580e-11 - val_loss: 7.4955e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5178e-11 - val_loss: 7.5376e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5930e-11 - val_loss: 7.7030e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7704e-11 - val_loss: 7.7109e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6994e-11 - val_loss: 7.6057e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5665e-11 - val_loss: 7.4945e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4483e-11 - val_loss: 7.4520e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4818e-11 - val_loss: 7.4243e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.4176e-11 - val_loss: 7.2801e-11
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.2476e-11 - val_loss: 7.1519e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.1457e-11 - val_loss: 7.1022e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1285e-11 - val_loss: 7.1591e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1980e-11 - val_loss: 7.2245e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2833e-11 - val_loss: 7.3707e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4308e-11 - val_loss: 7.4305e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4050e-11 - val_loss: 7.2801e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.2378e-11 - val_loss: 7.0917e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.4-curve-secp384r1-extra-out/multiplication_weights.h5
448/448 - 0s - loss: 7.0829e-11 - val_loss: 6.9404e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9944e-11 - val_loss: 7.1140e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2229e-11 - val_loss: 7.2750e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2667e-11 - val_loss: 7.2682e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2586e-11 - val_loss: 7.1753e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.343 | eve: 9.702 | bob: 9.216Epoch   0:   0% | abe: 9.307 | eve: 9.715 | bob: 9.188Epoch   0:   1% | abe: 9.288 | eve: 9.716 | bob: 9.179Epoch   0:   2% | abe: 9.240 | eve: 9.714 | bob: 9.138Epoch   0:   2% | abe: 9.230 | eve: 9.710 | bob: 9.136Epoch   0:   3% | abe: 9.220 | eve: 9.713 | bob: 9.132Epoch   0:   4% | abe: 9.210 | eve: 9.714 | bob: 9.128Epoch   0:   4% | abe: 9.201 | eve: 9.722 | bob: 9.123Epoch   0:   5% | abe: 9.184 | eve: 9.732 | bob: 9.110Epoch   0:   6% | abe: 9.163 | eve: 9.732 | bob: 9.092Epoch   0:   6% | abe: 9.153 | eve: 9.732 | bob: 9.084Epoch   0:   7% | abe: 9.148 | eve: 9.739 | bob: 9.082Epoch   0:   8% | abe: 9.144 | eve: 9.741 | bob: 9.080Epoch   0:   8% | abe: 9.137 | eve: 9.745 | bob: 9.075Epoch   0:   9% | abe: 9.135 | eve: 9.752 | bob: 9.074Epoch   0:  10% | abe: 9.127 | eve: 9.752 | bob: 9.067Epoch   0:  10% | abe: 9.125 | eve: 9.753 | bob: 9.066Epoch   0:  11% | abe: 9.122 | eve: 9.756 | bob: 9.064Epoch   0:  12% | abe: 9.116 | eve: 9.759 | bob: 9.058Epoch   0:  13% | abe: 9.113 | eve: 9.764 | bob: 9.056Epoch   0:  13% | abe: 9.113 | eve: 9.768 | bob: 9.056Epoch   0:  14% | abe: 9.111 | eve: 9.774 | bob: 9.054Epoch   0:  15% | abe: 9.108 | eve: 9.776 | bob: 9.051Epoch   0:  15% | abe: 9.105 | eve: 9.783 | bob: 9.048Epoch   0:  16% | abe: 9.102 | eve: 9.786 | bob: 9.046Epoch   0:  17% | abe: 9.098 | eve: 9.787 | bob: 9.042Epoch   0:  17% | abe: 9.097 | eve: 9.790 | bob: 9.040Epoch   0:  18% | abe: 9.098 | eve: 9.789 | bob: 9.041Epoch   0:  19% | abe: 9.098 | eve: 9.791 | bob: 9.042Epoch   0:  19% | abe: 9.100 | eve: 9.795 | bob: 9.043Epoch   0:  20% | abe: 9.096 | eve: 9.798 | bob: 9.039Epoch   0:  21% | abe: 9.095 | eve: 9.800 | bob: 9.038Epoch   0:  21% | abe: 9.095 | eve: 9.804 | bob: 9.037Epoch   0:  22% | abe: 9.092 | eve: 9.812 | bob: 9.034Epoch   0:  23% | abe: 9.090 | eve: 9.815 | bob: 9.032Epoch   0:  23% | abe: 9.089 | eve: 9.819 | bob: 9.031Epoch   0:  24% | abe: 9.088 | eve: 9.822 | bob: 9.030Epoch   0:  25% | abe: 9.088 | eve: 9.827 | bob: 9.029Epoch   0:  26% | abe: 9.086 | eve: 9.832 | bob: 9.026Epoch   0:  26% | abe: 9.087 | eve: 9.835 | bob: 9.028Epoch   0:  27% | abe: 9.086 | eve: 9.836 | bob: 9.026Epoch   0:  28% | abe: 9.085 | eve: 9.840 | bob: 9.025Epoch   0:  28% | abe: 9.084 | eve: 9.845 | bob: 9.024Epoch   0:  29% | abe: 9.084 | eve: 9.846 | bob: 9.023Epoch   0:  30% | abe: 9.082 | eve: 9.851 | bob: 9.021Epoch   0:  30% | abe: 9.080 | eve: 9.853 | bob: 9.019Epoch   0:  31% | abe: 9.080 | eve: 9.858 | bob: 9.019Epoch   0:  32% | abe: 9.080 | eve: 9.861 | bob: 9.019Epoch   0:  32% | abe: 9.078 | eve: 9.864 | bob: 9.016Epoch   0:  33% | abe: 9.077 | eve: 9.868 | bob: 9.015Epoch   0:  34% | abe: 9.075 | eve: 9.870 | bob: 9.013Epoch   0:  34% | abe: 9.075 | eve: 9.874 | bob: 9.013Epoch   0:  35% | abe: 9.074 | eve: 9.877 | bob: 9.011Epoch   0:  36% | abe: 9.072 | eve: 9.880 | bob: 9.010Epoch   0:  36% | abe: 9.071 | eve: 9.882 | bob: 9.009Epoch   0:  37% | abe: 9.070 | eve: 9.886 | bob: 9.007Epoch   0:  38% | abe: 9.070 | eve: 9.887 | bob: 9.007Epoch   0:  39% | abe: 9.069 | eve: 9.889 | bob: 9.006Epoch   0:  39% | abe: 9.069 | eve: 9.892 | bob: 9.006Epoch   0:  40% | abe: 9.068 | eve: 9.896 | bob: 9.005Epoch   0:  41% | abe: 9.068 | eve: 9.898 | bob: 9.004Epoch   0:  41% | abe: 9.067 | eve: 9.900 | bob: 9.003Epoch   0:  42% | abe: 9.066 | eve: 9.902 | bob: 9.002Epoch   0:  43% | abe: 9.066 | eve: 9.905 | bob: 9.001Epoch   0:  43% | abe: 9.065 | eve: 9.908 | bob: 9.001Epoch   0:  44% | abe: 9.064 | eve: 9.911 | bob: 9.000Epoch   0:  45% | abe: 9.064 | eve: 9.914 | bob: 8.999Epoch   0:  45% | abe: 9.063 | eve: 9.916 | bob: 8.998Epoch   0:  46% | abe: 9.062 | eve: 9.919 | bob: 8.996Epoch   0:  47% | abe: 9.061 | eve: 9.922 | bob: 8.995Epoch   0:  47% | abe: 9.060 | eve: 9.924 | bob: 8.994Epoch   0:  48% | abe: 9.059 | eve: 9.928 | bob: 8.993Epoch   0:  49% | abe: 9.058 | eve: 9.931 | bob: 8.992Epoch   0:  50% | abe: 9.058 | eve: 9.933 | bob: 8.991Epoch   0:  50% | abe: 9.057 | eve: 9.936 | bob: 8.990Epoch   0:  51% | abe: 9.057 | eve: 9.938 | bob: 8.990Epoch   0:  52% | abe: 9.056 | eve: 9.940 | bob: 8.989Epoch   0:  52% | abe: 9.055 | eve: 9.943 | bob: 8.988Epoch   0:  53% | abe: 9.055 | eve: 9.945 | bob: 8.988Epoch   0:  54% | abe: 9.054 | eve: 9.946 | bob: 8.986Epoch   0:  54% | abe: 9.054 | eve: 9.949 | bob: 8.987Epoch   0:  55% | abe: 9.053 | eve: 9.952 | bob: 8.985Epoch   0:  56% | abe: 9.053 | eve: 9.954 | bob: 8.986Epoch   0:  56% | abe: 9.053 | eve: 9.957 | bob: 8.985Epoch   0:  57% | abe: 9.052 | eve: 9.959 | bob: 8.984Epoch   0:  58% | abe: 9.052 | eve: 9.960 | bob: 8.984Epoch   0:  58% | abe: 9.052 | eve: 9.963 | bob: 8.984Epoch   0:  59% | abe: 9.051 | eve: 9.965 | bob: 8.983Epoch   0:  60% | abe: 9.051 | eve: 9.967 | bob: 8.982Epoch   0:  60% | abe: 9.051 | eve: 9.968 | bob: 8.983Epoch   0:  61% | abe: 9.049 | eve: 9.970 | bob: 8.981Epoch   0:  62% | abe: 9.049 | eve: 9.972 | bob: 8.980Epoch   0:  63% | abe: 9.048 | eve: 9.974 | bob: 8.980Epoch   0:  63% | abe: 9.048 | eve: 9.976 | bob: 8.979Epoch   0:  64% | abe: 9.046 | eve: 9.979 | bob: 8.978Epoch   0:  65% | abe: 9.046 | eve: 9.981 | bob: 8.977Epoch   0:  65% | abe: 9.046 | eve: 9.982 | bob: 8.977Epoch   0:  66% | abe: 9.045 | eve: 9.983 | bob: 8.976Epoch   0:  67% | abe: 9.045 | eve: 9.986 | bob: 8.976Epoch   0:  67% | abe: 9.045 | eve: 9.987 | bob: 8.976Epoch   0:  68% | abe: 9.045 | eve: 9.989 | bob: 8.975Epoch   0:  69% | abe: 9.044 | eve: 9.990 | bob: 8.974Epoch   0:  69% | abe: 9.043 | eve: 9.992 | bob: 8.973Epoch   0:  70% | abe: 9.042 | eve: 9.994 | bob: 8.972Epoch   0:  71% | abe: 9.042 | eve: 9.996 | bob: 8.972Epoch   0:  71% | abe: 9.041 | eve: 9.997 | bob: 8.971Epoch   0:  72% | abe: 9.040 | eve: 9.999 | bob: 8.971Epoch   0:  73% | abe: 9.040 | eve: 10.001 | bob: 8.970Epoch   0:  73% | abe: 9.039 | eve: 10.003 | bob: 8.969Epoch   0:  74% | abe: 9.039 | eve: 10.005 | bob: 8.968Epoch   0:  75% | abe: 9.039 | eve: 10.006 | bob: 8.968Epoch   0:  76% | abe: 9.038 | eve: 10.007 | bob: 8.968Epoch   0:  76% | abe: 9.038 | eve: 10.008 | bob: 8.967Epoch   0:  77% | abe: 9.037 | eve: 10.009 | bob: 8.966Epoch   0:  78% | abe: 9.036 | eve: 10.012 | bob: 8.966Epoch   0:  78% | abe: 9.036 | eve: 10.013 | bob: 8.965Epoch   0:  79% | abe: 9.035 | eve: 10.015 | bob: 8.965Epoch   0:  80% | abe: 9.035 | eve: 10.016 | bob: 8.964Epoch   0:  80% | abe: 9.035 | eve: 10.018 | bob: 8.964Epoch   0:  81% | abe: 9.034 | eve: 10.019 | bob: 8.964Epoch   0:  82% | abe: 9.034 | eve: 10.021 | bob: 8.963Epoch   0:  82% | abe: 9.033 | eve: 10.022 | bob: 8.962Epoch   0:  83% | abe: 9.032 | eve: 10.022 | bob: 8.961Epoch   0:  84% | abe: 9.032 | eve: 10.024 | bob: 8.961Epoch   0:  84% | abe: 9.032 | eve: 10.025 | bob: 8.960Epoch   0:  85% | abe: 9.031 | eve: 10.027 | bob: 8.960Epoch   0:  86% | abe: 9.031 | eve: 10.028 | bob: 8.960Epoch   0:  86% | abe: 9.031 | eve: 10.029 | bob: 8.960Epoch   0:  87% | abe: 9.031 | eve: 10.030 | bob: 8.960Epoch   0:  88% | abe: 9.031 | eve: 10.031 | bob: 8.959Epoch   0:  89% | abe: 9.031 | eve: 10.032 | bob: 8.959Epoch   0:  89% | abe: 9.030 | eve: 10.034 | bob: 8.958Epoch   0:  90% | abe: 9.029 | eve: 10.035 | bob: 8.958Epoch   0:  91% | abe: 9.029 | eve: 10.036 | bob: 8.958Epoch   0:  91% | abe: 9.028 | eve: 10.037 | bob: 8.956Epoch   0:  92% | abe: 9.028 | eve: 10.039 | bob: 8.956Epoch   0:  93% | abe: 9.027 | eve: 10.040 | bob: 8.955Epoch   0:  93% | abe: 9.027 | eve: 10.041 | bob: 8.955Epoch   0:  94% | abe: 9.027 | eve: 10.043 | bob: 8.955Epoch   0:  95% | abe: 9.026 | eve: 10.044 | bob: 8.954Epoch   0:  95% | abe: 9.026 | eve: 10.045 | bob: 8.954Epoch   0:  96% | abe: 9.026 | eve: 10.046 | bob: 8.954Epoch   0:  97% | abe: 9.025 | eve: 10.047 | bob: 8.953Epoch   0:  97% | abe: 9.025 | eve: 10.048 | bob: 8.953Epoch   0:  98% | abe: 9.025 | eve: 10.049 | bob: 8.953Epoch   0:  99% | abe: 9.024 | eve: 10.050 | bob: 8.952
New best Bob loss 8.95231377285538 at epoch 0
Epoch   1:   0% | abe: 8.983 | eve: 10.125 | bob: 8.904Epoch   1:   0% | abe: 8.960 | eve: 10.157 | bob: 8.881Epoch   1:   1% | abe: 8.989 | eve: 10.148 | bob: 8.911Epoch   1:   2% | abe: 8.985 | eve: 10.135 | bob: 8.907Epoch   1:   2% | abe: 8.977 | eve: 10.137 | bob: 8.899Epoch   1:   3% | abe: 8.972 | eve: 10.138 | bob: 8.895Epoch   1:   4% | abe: 8.969 | eve: 10.133 | bob: 8.891Epoch   1:   4% | abe: 8.956 | eve: 10.148 | bob: 8.878Epoch   1:   5% | abe: 8.956 | eve: 10.158 | bob: 8.877Epoch   1:   6% | abe: 8.959 | eve: 10.154 | bob: 8.880Epoch   1:   6% | abe: 8.958 | eve: 10.152 | bob: 8.879Epoch   1:   7% | abe: 8.954 | eve: 10.155 | bob: 8.875Epoch   1:   8% | abe: 8.951 | eve: 10.155 | bob: 8.872Epoch   1:   8% | abe: 8.957 | eve: 10.154 | bob: 8.878Epoch   1:   9% | abe: 8.959 | eve: 10.156 | bob: 8.880Epoch   1:  10% | abe: 8.961 | eve: 10.156 | bob: 8.882Epoch   1:  10% | abe: 8.959 | eve: 10.159 | bob: 8.880Epoch   1:  11% | abe: 8.960 | eve: 10.160 | bob: 8.881Epoch   1:  12% | abe: 8.961 | eve: 10.162 | bob: 8.881Epoch   1:  13% | abe: 8.958 | eve: 10.161 | bob: 8.879Epoch   1:  13% | abe: 8.956 | eve: 10.160 | bob: 8.877Epoch   1:  14% | abe: 8.953 | eve: 10.157 | bob: 8.874Epoch   1:  15% | abe: 8.951 | eve: 10.158 | bob: 8.872Epoch   1:  15% | abe: 8.950 | eve: 10.162 | bob: 8.871Epoch   1:  16% | abe: 8.951 | eve: 10.163 | bob: 8.871Epoch   1:  17% | abe: 8.949 | eve: 10.160 | bob: 8.870Epoch   1:  17% | abe: 8.949 | eve: 10.160 | bob: 8.869Epoch   1:  18% | abe: 8.949 | eve: 10.161 | bob: 8.869Epoch   1:  19% | abe: 8.948 | eve: 10.165 | bob: 8.868Epoch   1:  19% | abe: 8.947 | eve: 10.162 | bob: 8.868Epoch   1:  20% | abe: 8.946 | eve: 10.159 | bob: 8.867Epoch   1:  21% | abe: 8.948 | eve: 10.158 | bob: 8.868Epoch   1:  21% | abe: 8.949 | eve: 10.158 | bob: 8.869Epoch   1:  22% | abe: 8.948 | eve: 10.159 | bob: 8.869Epoch   1:  23% | abe: 8.947 | eve: 10.159 | bob: 8.868Epoch   1:  23% | abe: 8.947 | eve: 10.159 | bob: 8.867Epoch   1:  24% | abe: 8.948 | eve: 10.159 | bob: 8.868Epoch   1:  25% | abe: 8.946 | eve: 10.160 | bob: 8.866Epoch   1:  26% | abe: 8.945 | eve: 10.160 | bob: 8.865Epoch   1:  26% | abe: 8.944 | eve: 10.161 | bob: 8.864Epoch   1:  27% | abe: 8.944 | eve: 10.162 | bob: 8.864Epoch   1:  28% | abe: 8.943 | eve: 10.163 | bob: 8.863Epoch   1:  28% | abe: 8.944 | eve: 10.164 | bob: 8.864Epoch   1:  29% | abe: 8.945 | eve: 10.163 | bob: 8.864Epoch   1:  30% | abe: 8.945 | eve: 10.165 | bob: 8.865Epoch   1:  30% | abe: 8.944 | eve: 10.165 | bob: 8.864Epoch   1:  31% | abe: 8.944 | eve: 10.164 | bob: 8.864Epoch   1:  32% | abe: 8.945 | eve: 10.165 | bob: 8.865Epoch   1:  32% | abe: 8.947 | eve: 10.165 | bob: 8.866Epoch   1:  33% | abe: 8.947 | eve: 10.165 | bob: 8.867Epoch   1:  34% | abe: 8.948 | eve: 10.165 | bob: 8.867Epoch   1:  34% | abe: 8.948 | eve: 10.164 | bob: 8.868Epoch   1:  35% | abe: 8.948 | eve: 10.164 | bob: 8.867Epoch   1:  36% | abe: 8.947 | eve: 10.164 | bob: 8.867Epoch   1:  36% | abe: 8.947 | eve: 10.163 | bob: 8.867Epoch   1:  37% | abe: 8.947 | eve: 10.165 | bob: 8.867Epoch   1:  38% | abe: 8.947 | eve: 10.165 | bob: 8.866Epoch   1:  39% | abe: 8.946 | eve: 10.166 | bob: 8.866Epoch   1:  39% | abe: 8.946 | eve: 10.168 | bob: 8.866Epoch   1:  40% | abe: 8.947 | eve: 10.166 | bob: 8.867Epoch   1:  41% | abe: 8.946 | eve: 10.167 | bob: 8.866Epoch   1:  41% | abe: 8.946 | eve: 10.168 | bob: 8.866Epoch   1:  42% | abe: 8.945 | eve: 10.168 | bob: 8.865Epoch   1:  43% | abe: 8.944 | eve: 10.168 | bob: 8.864Epoch   1:  43% | abe: 8.943 | eve: 10.168 | bob: 8.863Epoch   1:  44% | abe: 8.943 | eve: 10.168 | bob: 8.863Epoch   1:  45% | abe: 8.942 | eve: 10.168 | bob: 8.862Epoch   1:  45% | abe: 8.942 | eve: 10.168 | bob: 8.862Epoch   1:  46% | abe: 8.941 | eve: 10.168 | bob: 8.860Epoch   1:  47% | abe: 8.940 | eve: 10.167 | bob: 8.859Epoch   1:  47% | abe: 8.939 | eve: 10.166 | bob: 8.859Epoch   1:  48% | abe: 8.938 | eve: 10.166 | bob: 8.858Epoch   1:  49% | abe: 8.938 | eve: 10.165 | bob: 8.858Epoch   1:  50% | abe: 8.937 | eve: 10.164 | bob: 8.857Epoch   1:  50% | abe: 8.937 | eve: 10.165 | bob: 8.856Epoch   1:  51% | abe: 8.937 | eve: 10.166 | bob: 8.856Epoch   1:  52% | abe: 8.936 | eve: 10.167 | bob: 8.855Epoch   1:  52% | abe: 8.936 | eve: 10.167 | bob: 8.856Epoch   1:  53% | abe: 8.936 | eve: 10.167 | bob: 8.855Epoch   1:  54% | abe: 8.935 | eve: 10.166 | bob: 8.854Epoch   1:  54% | abe: 8.935 | eve: 10.167 | bob: 8.854Epoch   1:  55% | abe: 8.934 | eve: 10.167 | bob: 8.854Epoch   1:  56% | abe: 8.933 | eve: 10.169 | bob: 8.853Epoch   1:  56% | abe: 8.933 | eve: 10.169 | bob: 8.853Epoch   1:  57% | abe: 8.933 | eve: 10.170 | bob: 8.853Epoch   1:  58% | abe: 8.933 | eve: 10.171 | bob: 8.853Epoch   1:  58% | abe: 8.933 | eve: 10.170 | bob: 8.852Epoch   1:  59% | abe: 8.932 | eve: 10.170 | bob: 8.851Epoch   1:  60% | abe: 8.931 | eve: 10.169 | bob: 8.850Epoch   1:  60% | abe: 8.930 | eve: 10.169 | bob: 8.850Epoch   1:  61% | abe: 8.930 | eve: 10.168 | bob: 8.849Epoch   1:  62% | abe: 8.930 | eve: 10.168 | bob: 8.849Epoch   1:  63% | abe: 8.929 | eve: 10.168 | bob: 8.848Epoch   1:  63% | abe: 8.929 | eve: 10.167 | bob: 8.848Epoch   1:  64% | abe: 8.929 | eve: 10.167 | bob: 8.848Epoch   1:  65% | abe: 8.928 | eve: 10.167 | bob: 8.847Epoch   1:  65% | abe: 8.928 | eve: 10.168 | bob: 8.847Epoch   1:  66% | abe: 8.927 | eve: 10.168 | bob: 8.846Epoch   1:  67% | abe: 8.927 | eve: 10.168 | bob: 8.846Epoch   1:  67% | abe: 8.926 | eve: 10.168 | bob: 8.845Epoch   1:  68% | abe: 8.926 | eve: 10.169 | bob: 8.845Epoch   1:  69% | abe: 8.926 | eve: 10.169 | bob: 8.845Epoch   1:  69% | abe: 8.926 | eve: 10.168 | bob: 8.845Epoch   1:  70% | abe: 8.926 | eve: 10.169 | bob: 8.844Epoch   1:  71% | abe: 8.925 | eve: 10.168 | bob: 8.844Epoch   1:  71% | abe: 8.925 | eve: 10.168 | bob: 8.844Epoch   1:  72% | abe: 8.925 | eve: 10.169 | bob: 8.844Epoch   1:  73% | abe: 8.925 | eve: 10.169 | bob: 8.843Epoch   1:  73% | abe: 8.924 | eve: 10.169 | bob: 8.843Epoch   1:  74% | abe: 8.924 | eve: 10.170 | bob: 8.843Epoch   1:  75% | abe: 8.924 | eve: 10.170 | bob: 8.843Epoch   1:  76% | abe: 8.924 | eve: 10.171 | bob: 8.843Epoch   1:  76% | abe: 8.923 | eve: 10.171 | bob: 8.842Epoch   1:  77% | abe: 8.923 | eve: 10.170 | bob: 8.842Epoch   1:  78% | abe: 8.923 | eve: 10.170 | bob: 8.842Epoch   1:  78% | abe: 8.922 | eve: 10.170 | bob: 8.841Epoch   1:  79% | abe: 8.922 | eve: 10.170 | bob: 8.840Epoch   1:  80% | abe: 8.921 | eve: 10.171 | bob: 8.840Epoch   1:  80% | abe: 8.921 | eve: 10.171 | bob: 8.839Epoch   1:  81% | abe: 8.921 | eve: 10.171 | bob: 8.839Epoch   1:  82% | abe: 8.921 | eve: 10.171 | bob: 8.839Epoch   1:  82% | abe: 8.920 | eve: 10.172 | bob: 8.839Epoch   1:  83% | abe: 8.920 | eve: 10.172 | bob: 8.838Epoch   1:  84% | abe: 8.919 | eve: 10.172 | bob: 8.837Epoch   1:  84% | abe: 8.919 | eve: 10.172 | bob: 8.837Epoch   1:  85% | abe: 8.919 | eve: 10.171 | bob: 8.837Epoch   1:  86% | abe: 8.919 | eve: 10.171 | bob: 8.837Epoch   1:  86% | abe: 8.917 | eve: 10.171 | bob: 8.836Epoch   1:  87% | abe: 8.917 | eve: 10.171 | bob: 8.835Epoch   1:  88% | abe: 8.917 | eve: 10.171 | bob: 8.835Epoch   1:  89% | abe: 8.916 | eve: 10.171 | bob: 8.834Epoch   1:  89% | abe: 8.916 | eve: 10.170 | bob: 8.834Epoch   1:  90% | abe: 8.916 | eve: 10.170 | bob: 8.834Epoch   1:  91% | abe: 8.915 | eve: 10.171 | bob: 8.833Epoch   1:  91% | abe: 8.914 | eve: 10.170 | bob: 8.832Epoch   1:  92% | abe: 8.914 | eve: 10.171 | bob: 8.832Epoch   1:  93% | abe: 8.914 | eve: 10.170 | bob: 8.832Epoch   1:  93% | abe: 8.914 | eve: 10.170 | bob: 8.832Epoch   1:  94% | abe: 8.913 | eve: 10.170 | bob: 8.831Epoch   1:  95% | abe: 8.913 | eve: 10.171 | bob: 8.831Epoch   1:  95% | abe: 8.912 | eve: 10.171 | bob: 8.830Epoch   1:  96% | abe: 8.912 | eve: 10.170 | bob: 8.830Epoch   1:  97% | abe: 8.912 | eve: 10.170 | bob: 8.830Epoch   1:  97% | abe: 8.912 | eve: 10.170 | bob: 8.830Epoch   1:  98% | abe: 8.912 | eve: 10.171 | bob: 8.830Epoch   1:  99% | abe: 8.911 | eve: 10.171 | bob: 8.829
New best Bob loss 8.829407570150284 at epoch 1
Epoch   2:   0% | abe: 8.847 | eve: 10.107 | bob: 8.765Epoch   2:   0% | abe: 8.838 | eve: 10.142 | bob: 8.755Epoch   2:   1% | abe: 8.835 | eve: 10.160 | bob: 8.752Epoch   2:   2% | abe: 8.850 | eve: 10.160 | bob: 8.767Epoch   2:   2% | abe: 8.836 | eve: 10.160 | bob: 8.753Epoch   2:   3% | abe: 8.832 | eve: 10.155 | bob: 8.749Epoch   2:   4% | abe: 8.840 | eve: 10.169 | bob: 8.756Epoch   2:   4% | abe: 8.842 | eve: 10.168 | bob: 8.758Epoch   2:   5% | abe: 8.839 | eve: 10.162 | bob: 8.756Epoch   2:   6% | abe: 8.842 | eve: 10.159 | bob: 8.758Epoch   2:   6% | abe: 8.842 | eve: 10.154 | bob: 8.758Epoch   2:   7% | abe: 8.839 | eve: 10.156 | bob: 8.755Epoch   2:   8% | abe: 8.837 | eve: 10.156 | bob: 8.754Epoch   2:   8% | abe: 8.840 | eve: 10.153 | bob: 8.757Epoch   2:   9% | abe: 8.842 | eve: 10.151 | bob: 8.758Epoch   2:  10% | abe: 8.841 | eve: 10.142 | bob: 8.758Epoch   2:  10% | abe: 8.839 | eve: 10.139 | bob: 8.756Epoch   2:  11% | abe: 8.843 | eve: 10.134 | bob: 8.759Epoch   2:  12% | abe: 8.843 | eve: 10.136 | bob: 8.760Epoch   2:  13% | abe: 8.846 | eve: 10.137 | bob: 8.763Epoch   2:  13% | abe: 8.847 | eve: 10.135 | bob: 8.764Epoch   2:  14% | abe: 8.846 | eve: 10.137 | bob: 8.764Epoch   2:  15% | abe: 8.846 | eve: 10.140 | bob: 8.764Epoch   2:  15% | abe: 8.846 | eve: 10.139 | bob: 8.763Epoch   2:  16% | abe: 8.844 | eve: 10.137 | bob: 8.762Epoch   2:  17% | abe: 8.843 | eve: 10.136 | bob: 8.761Epoch   2:  17% | abe: 8.842 | eve: 10.133 | bob: 8.760Epoch   2:  18% | abe: 8.841 | eve: 10.136 | bob: 8.758Epoch   2:  19% | abe: 8.839 | eve: 10.139 | bob: 8.757Epoch   2:  19% | abe: 8.840 | eve: 10.141 | bob: 8.758Epoch   2:  20% | abe: 8.841 | eve: 10.139 | bob: 8.759Epoch   2:  21% | abe: 8.842 | eve: 10.142 | bob: 8.759Epoch   2:  21% | abe: 8.841 | eve: 10.144 | bob: 8.759Epoch   2:  22% | abe: 8.839 | eve: 10.145 | bob: 8.757Epoch   2:  23% | abe: 8.838 | eve: 10.146 | bob: 8.755Epoch   2:  23% | abe: 8.835 | eve: 10.146 | bob: 8.753Epoch   2:  24% | abe: 8.834 | eve: 10.147 | bob: 8.752Epoch   2:  25% | abe: 8.835 | eve: 10.150 | bob: 8.752Epoch   2:  26% | abe: 8.833 | eve: 10.149 | bob: 8.751Epoch   2:  26% | abe: 8.833 | eve: 10.148 | bob: 8.751Epoch   2:  27% | abe: 8.832 | eve: 10.146 | bob: 8.750Epoch   2:  28% | abe: 8.831 | eve: 10.147 | bob: 8.749Epoch   2:  28% | abe: 8.832 | eve: 10.146 | bob: 8.750Epoch   2:  29% | abe: 8.831 | eve: 10.149 | bob: 8.749Epoch   2:  30% | abe: 8.832 | eve: 10.149 | bob: 8.750Epoch   2:  30% | abe: 8.831 | eve: 10.150 | bob: 8.750Epoch   2:  31% | abe: 8.831 | eve: 10.152 | bob: 8.749Epoch   2:  32% | abe: 8.831 | eve: 10.153 | bob: 8.749Epoch   2:  32% | abe: 8.830 | eve: 10.152 | bob: 8.748Epoch   2:  33% | abe: 8.830 | eve: 10.153 | bob: 8.748Epoch   2:  34% | abe: 8.829 | eve: 10.154 | bob: 8.748Epoch   2:  34% | abe: 8.830 | eve: 10.152 | bob: 8.749Epoch   2:  35% | abe: 8.829 | eve: 10.152 | bob: 8.748Epoch   2:  36% | abe: 8.829 | eve: 10.153 | bob: 8.747Epoch   2:  36% | abe: 8.829 | eve: 10.152 | bob: 8.747Epoch   2:  37% | abe: 8.828 | eve: 10.150 | bob: 8.747Epoch   2:  38% | abe: 8.828 | eve: 10.150 | bob: 8.747Epoch   2:  39% | abe: 8.828 | eve: 10.150 | bob: 8.747Epoch   2:  39% | abe: 8.827 | eve: 10.151 | bob: 8.746Epoch   2:  40% | abe: 8.826 | eve: 10.151 | bob: 8.745Epoch   2:  41% | abe: 8.827 | eve: 10.152 | bob: 8.746Epoch   2:  41% | abe: 8.826 | eve: 10.153 | bob: 8.745Epoch   2:  42% | abe: 8.826 | eve: 10.153 | bob: 8.745Epoch   2:  43% | abe: 8.825 | eve: 10.154 | bob: 8.744Epoch   2:  43% | abe: 8.825 | eve: 10.155 | bob: 8.744Epoch   2:  44% | abe: 8.825 | eve: 10.155 | bob: 8.744Epoch   2:  45% | abe: 8.825 | eve: 10.155 | bob: 8.744Epoch   2:  45% | abe: 8.824 | eve: 10.155 | bob: 8.743Epoch   2:  46% | abe: 8.823 | eve: 10.154 | bob: 8.742Epoch   2:  47% | abe: 8.823 | eve: 10.155 | bob: 8.742Epoch   2:  47% | abe: 8.823 | eve: 10.156 | bob: 8.743Epoch   2:  48% | abe: 8.822 | eve: 10.157 | bob: 8.742Epoch   2:  49% | abe: 8.822 | eve: 10.157 | bob: 8.741Epoch   2:  50% | abe: 8.821 | eve: 10.156 | bob: 8.740Epoch   2:  50% | abe: 8.821 | eve: 10.156 | bob: 8.740Epoch   2:  51% | abe: 8.820 | eve: 10.155 | bob: 8.740Epoch   2:  52% | abe: 8.820 | eve: 10.155 | bob: 8.739Epoch   2:  52% | abe: 8.820 | eve: 10.156 | bob: 8.739Epoch   2:  53% | abe: 8.819 | eve: 10.157 | bob: 8.739Epoch   2:  54% | abe: 8.819 | eve: 10.157 | bob: 8.739Epoch   2:  54% | abe: 8.820 | eve: 10.157 | bob: 8.739Epoch   2:  55% | abe: 8.819 | eve: 10.157 | bob: 8.738Epoch   2:  56% | abe: 8.818 | eve: 10.158 | bob: 8.737Epoch   2:  56% | abe: 8.817 | eve: 10.156 | bob: 8.737Epoch   2:  57% | abe: 8.818 | eve: 10.155 | bob: 8.737Epoch   2:  58% | abe: 8.817 | eve: 10.155 | bob: 8.737Epoch   2:  58% | abe: 8.817 | eve: 10.155 | bob: 8.736Epoch   2:  59% | abe: 8.816 | eve: 10.156 | bob: 8.736Epoch   2:  60% | abe: 8.815 | eve: 10.157 | bob: 8.735Epoch   2:  60% | abe: 8.815 | eve: 10.157 | bob: 8.735Epoch   2:  61% | abe: 8.815 | eve: 10.157 | bob: 8.735Epoch   2:  62% | abe: 8.814 | eve: 10.157 | bob: 8.734Epoch   2:  63% | abe: 8.814 | eve: 10.158 | bob: 8.734Epoch   2:  63% | abe: 8.813 | eve: 10.157 | bob: 8.733Epoch   2:  64% | abe: 8.813 | eve: 10.158 | bob: 8.733Epoch   2:  65% | abe: 8.812 | eve: 10.158 | bob: 8.732Epoch   2:  65% | abe: 8.812 | eve: 10.158 | bob: 8.732Epoch   2:  66% | abe: 8.811 | eve: 10.158 | bob: 8.732Epoch   2:  67% | abe: 8.811 | eve: 10.158 | bob: 8.732Epoch   2:  67% | abe: 8.811 | eve: 10.158 | bob: 8.731Epoch   2:  68% | abe: 8.811 | eve: 10.159 | bob: 8.731Epoch   2:  69% | abe: 8.810 | eve: 10.160 | bob: 8.731Epoch   2:  69% | abe: 8.809 | eve: 10.159 | bob: 8.730Epoch   2:  70% | abe: 8.808 | eve: 10.159 | bob: 8.729Epoch   2:  71% | abe: 8.808 | eve: 10.158 | bob: 8.729Epoch   2:  71% | abe: 8.808 | eve: 10.158 | bob: 8.728Epoch   2:  72% | abe: 8.808 | eve: 10.158 | bob: 8.728Epoch   2:  73% | abe: 8.808 | eve: 10.158 | bob: 8.728Epoch   2:  73% | abe: 8.807 | eve: 10.160 | bob: 8.728Epoch   2:  74% | abe: 8.807 | eve: 10.161 | bob: 8.727Epoch   2:  75% | abe: 8.807 | eve: 10.161 | bob: 8.727Epoch   2:  76% | abe: 8.806 | eve: 10.161 | bob: 8.727Epoch   2:  76% | abe: 8.806 | eve: 10.160 | bob: 8.727Epoch   2:  77% | abe: 8.806 | eve: 10.160 | bob: 8.727Epoch   2:  78% | abe: 8.806 | eve: 10.160 | bob: 8.727Epoch   2:  78% | abe: 8.805 | eve: 10.160 | bob: 8.726Epoch   2:  79% | abe: 8.805 | eve: 10.161 | bob: 8.725Epoch   2:  80% | abe: 8.805 | eve: 10.162 | bob: 8.726Epoch   2:  80% | abe: 8.804 | eve: 10.162 | bob: 8.725Epoch   2:  81% | abe: 8.803 | eve: 10.161 | bob: 8.724Epoch   2:  82% | abe: 8.803 | eve: 10.161 | bob: 8.724Epoch   2:  82% | abe: 8.803 | eve: 10.162 | bob: 8.724Epoch   2:  83% | abe: 8.803 | eve: 10.163 | bob: 8.724Epoch   2:  84% | abe: 8.802 | eve: 10.162 | bob: 8.723Epoch   2:  84% | abe: 8.802 | eve: 10.162 | bob: 8.723Epoch   2:  85% | abe: 8.802 | eve: 10.162 | bob: 8.723Epoch   2:  86% | abe: 8.802 | eve: 10.162 | bob: 8.723Epoch   2:  86% | abe: 8.801 | eve: 10.162 | bob: 8.722Epoch   2:  87% | abe: 8.801 | eve: 10.161 | bob: 8.722Epoch   2:  88% | abe: 8.801 | eve: 10.161 | bob: 8.722Epoch   2:  89% | abe: 8.800 | eve: 10.161 | bob: 8.722Epoch   2:  89% | abe: 8.800 | eve: 10.161 | bob: 8.722Epoch   2:  90% | abe: 8.799 | eve: 10.161 | bob: 8.721Epoch   2:  91% | abe: 8.799 | eve: 10.161 | bob: 8.720Epoch   2:  91% | abe: 8.798 | eve: 10.161 | bob: 8.720Epoch   2:  92% | abe: 8.797 | eve: 10.162 | bob: 8.719Epoch   2:  93% | abe: 8.796 | eve: 10.162 | bob: 8.718Epoch   2:  93% | abe: 8.796 | eve: 10.162 | bob: 8.718Epoch   2:  94% | abe: 8.796 | eve: 10.162 | bob: 8.717Epoch   2:  95% | abe: 8.796 | eve: 10.162 | bob: 8.717Epoch   2:  95% | abe: 8.795 | eve: 10.162 | bob: 8.717Epoch   2:  96% | abe: 8.795 | eve: 10.162 | bob: 8.717Epoch   2:  97% | abe: 8.794 | eve: 10.162 | bob: 8.716Epoch   2:  97% | abe: 8.794 | eve: 10.163 | bob: 8.716Epoch   2:  98% | abe: 8.793 | eve: 10.163 | bob: 8.715Epoch   2:  99% | abe: 8.793 | eve: 10.163 | bob: 8.715
New best Bob loss 8.714690200484641 at epoch 2
Epoch   3:   0% | abe: 8.762 | eve: 10.210 | bob: 8.689Epoch   3:   0% | abe: 8.772 | eve: 10.152 | bob: 8.699Epoch   3:   1% | abe: 8.748 | eve: 10.151 | bob: 8.674Epoch   3:   2% | abe: 8.734 | eve: 10.168 | bob: 8.659Epoch   3:   2% | abe: 8.750 | eve: 10.183 | bob: 8.676Epoch   3:   3% | abe: 8.743 | eve: 10.190 | bob: 8.669Epoch   3:   4% | abe: 8.749 | eve: 10.180 | bob: 8.676Epoch   3:   4% | abe: 8.749 | eve: 10.177 | bob: 8.676Epoch   3:   5% | abe: 8.756 | eve: 10.164 | bob: 8.683Epoch   3:   6% | abe: 8.755 | eve: 10.168 | bob: 8.682Epoch   3:   6% | abe: 8.756 | eve: 10.175 | bob: 8.683Epoch   3:   7% | abe: 8.751 | eve: 10.173 | bob: 8.678Epoch   3:   8% | abe: 8.751 | eve: 10.176 | bob: 8.678Epoch   3:   8% | abe: 8.751 | eve: 10.172 | bob: 8.679Epoch   3:   9% | abe: 8.746 | eve: 10.176 | bob: 8.673Epoch   3:  10% | abe: 8.746 | eve: 10.176 | bob: 8.674Epoch   3:  10% | abe: 8.743 | eve: 10.174 | bob: 8.671Epoch   3:  11% | abe: 8.742 | eve: 10.176 | bob: 8.669Epoch   3:  12% | abe: 8.740 | eve: 10.175 | bob: 8.668Epoch   3:  13% | abe: 8.741 | eve: 10.173 | bob: 8.669Epoch   3:  13% | abe: 8.743 | eve: 10.169 | bob: 8.671Epoch   3:  14% | abe: 8.742 | eve: 10.168 | bob: 8.669Epoch   3:  15% | abe: 8.742 | eve: 10.165 | bob: 8.669Epoch   3:  15% | abe: 8.743 | eve: 10.163 | bob: 8.671Epoch   3:  16% | abe: 8.740 | eve: 10.165 | bob: 8.668Epoch   3:  17% | abe: 8.740 | eve: 10.165 | bob: 8.668Epoch   3:  17% | abe: 8.739 | eve: 10.166 | bob: 8.666Epoch   3:  18% | abe: 8.737 | eve: 10.167 | bob: 8.665Epoch   3:  19% | abe: 8.737 | eve: 10.168 | bob: 8.664Epoch   3:  19% | abe: 8.736 | eve: 10.168 | bob: 8.664Epoch   3:  20% | abe: 8.735 | eve: 10.167 | bob: 8.663Epoch   3:  21% | abe: 8.734 | eve: 10.162 | bob: 8.662Epoch   3:  21% | abe: 8.733 | eve: 10.163 | bob: 8.661Epoch   3:  22% | abe: 8.732 | eve: 10.161 | bob: 8.659Epoch   3:  23% | abe: 8.732 | eve: 10.163 | bob: 8.660Epoch   3:  23% | abe: 8.732 | eve: 10.164 | bob: 8.660Epoch   3:  24% | abe: 8.730 | eve: 10.164 | bob: 8.658Epoch   3:  25% | abe: 8.730 | eve: 10.163 | bob: 8.657Epoch   3:  26% | abe: 8.730 | eve: 10.164 | bob: 8.657Epoch   3:  26% | abe: 8.729 | eve: 10.163 | bob: 8.657Epoch   3:  27% | abe: 8.730 | eve: 10.164 | bob: 8.658Epoch   3:  28% | abe: 8.729 | eve: 10.165 | bob: 8.657Epoch   3:  28% | abe: 8.728 | eve: 10.165 | bob: 8.656Epoch   3:  29% | abe: 8.728 | eve: 10.163 | bob: 8.656Epoch   3:  30% | abe: 8.729 | eve: 10.163 | bob: 8.657Epoch   3:  30% | abe: 8.728 | eve: 10.163 | bob: 8.656Epoch   3:  31% | abe: 8.727 | eve: 10.162 | bob: 8.655Epoch   3:  32% | abe: 8.728 | eve: 10.161 | bob: 8.656Epoch   3:  32% | abe: 8.728 | eve: 10.159 | bob: 8.656Epoch   3:  33% | abe: 8.728 | eve: 10.159 | bob: 8.656Epoch   3:  34% | abe: 8.728 | eve: 10.158 | bob: 8.656Epoch   3:  34% | abe: 8.728 | eve: 10.156 | bob: 8.656