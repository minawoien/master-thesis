WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-14 21:53:08.068598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-14 21:53:08.152677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-04-14 21:53:08.153456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:53:08.156931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:53:08.159755: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:53:08.160732: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:53:08.164134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:53:08.166302: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:53:08.172362: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:53:08.181880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:53:08.182312: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-14 21:53:08.193760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-14 21:53:08.196741: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2bf0760 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:53:08.196815: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-14 21:53:08.511822: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x208aaa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:53:08.511894: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-14 21:53:08.519597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-04-14 21:53:08.519722: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:53:08.519774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:53:08.519805: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:53:08.519838: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:53:08.519869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:53:08.519901: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:53:08.519932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:53:08.527526: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:53:08.527609: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:53:08.530678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-14 21:53:08.530700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-14 21:53:08.530726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-14 21:53:08.538864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-14 21:53:11.941175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.7342 - val_loss: 0.0031
Epoch 2/512
448/448 - 0s - loss: 0.1959 - val_loss: 4.7839e-04
Epoch 3/512
448/448 - 0s - loss: 0.0273 - val_loss: 4.6028e-05
Epoch 4/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.6344e-06
Epoch 5/512
448/448 - 0s - loss: 4.3261e-04 - val_loss: 2.8666e-06
Epoch 6/512
448/448 - 0s - loss: 2.5372e-04 - val_loss: 1.9051e-06
Epoch 7/512
448/448 - 0s - loss: 1.6497e-04 - val_loss: 1.1595e-06
Epoch 8/512
448/448 - 0s - loss: 9.7144e-05 - val_loss: 6.2462e-07
Epoch 9/512
448/448 - 0s - loss: 5.0262e-05 - val_loss: 2.8834e-07
Epoch 10/512
448/448 - 0s - loss: 2.2098e-05 - val_loss: 1.0971e-07
Epoch 11/512
448/448 - 0s - loss: 7.9271e-06 - val_loss: 3.2636e-08
Epoch 12/512
448/448 - 0s - loss: 2.1992e-06 - val_loss: 7.1103e-09
Epoch 13/512
448/448 - 0s - loss: 4.4153e-07 - val_loss: 1.0714e-09
Epoch 14/512
448/448 - 0s - loss: 8.1428e-08 - val_loss: 3.4905e-09
Epoch 15/512
448/448 - 0s - loss: 1.0094e-05 - val_loss: 3.4803e-06
Epoch 16/512
448/448 - 0s - loss: 0.0033 - val_loss: 2.3778e-05
Epoch 17/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.1162e-06
Epoch 18/512
448/448 - 0s - loss: 9.3625e-05 - val_loss: 1.3389e-06
Epoch 19/512
448/448 - 0s - loss: 3.3091e-04 - val_loss: 1.6131e-05
Epoch 20/512
448/448 - 0s - loss: 0.0023 - val_loss: 1.4599e-05
Epoch 21/512
448/448 - 0s - loss: 8.8846e-04 - val_loss: 3.6570e-06
Epoch 22/512
448/448 - 0s - loss: 3.9003e-04 - val_loss: 6.9406e-06
Epoch 23/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.8108e-05
Epoch 24/512
448/448 - 0s - loss: 0.0015 - val_loss: 7.8983e-06
Epoch 25/512
448/448 - 0s - loss: 6.5986e-04 - val_loss: 6.3561e-06
Epoch 26/512
448/448 - 0s - loss: 7.6871e-04 - val_loss: 1.2030e-05
Epoch 27/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0867e-05
Epoch 28/512
448/448 - 0s - loss: 9.0902e-04 - val_loss: 7.4399e-06
Epoch 29/512
448/448 - 0s - loss: 7.5780e-04 - val_loss: 9.0998e-06
Epoch 30/512
448/448 - 0s - loss: 9.7926e-04 - val_loss: 1.0677e-05
Epoch 31/512
448/448 - 0s - loss: 9.7931e-04 - val_loss: 8.6212e-06
Epoch 32/512
448/448 - 0s - loss: 8.1673e-04 - val_loss: 8.3477e-06
Epoch 33/512
448/448 - 0s - loss: 8.6509e-04 - val_loss: 9.4234e-06
Epoch 34/512
448/448 - 0s - loss: 9.2089e-04 - val_loss: 9.2059e-06
Epoch 35/512
448/448 - 0s - loss: 8.6461e-04 - val_loss: 8.5297e-06
Epoch 36/512
448/448 - 0s - loss: 8.3387e-04 - val_loss: 8.6312e-06
Epoch 37/512
448/448 - 0s - loss: 8.5554e-04 - val_loss: 8.7175e-06
Epoch 38/512
448/448 - 0s - loss: 8.4932e-04 - val_loss: 8.4654e-06
Epoch 39/512
448/448 - 0s - loss: 8.2485e-04 - val_loss: 8.3279e-06
Epoch 40/512
448/448 - 0s - loss: 8.1706e-04 - val_loss: 8.5645e-06
Epoch 41/512
448/448 - 0s - loss: 8.3424e-04 - val_loss: 8.3179e-06
Epoch 42/512
448/448 - 0s - loss: 7.9996e-04 - val_loss: 8.1031e-06
Epoch 43/512
448/448 - 0s - loss: 7.9701e-04 - val_loss: 8.0835e-06
Epoch 44/512
448/448 - 0s - loss: 7.9814e-04 - val_loss: 7.9710e-06
Epoch 45/512
448/448 - 0s - loss: 7.9022e-04 - val_loss: 7.6834e-06
Epoch 46/512
448/448 - 0s - loss: 7.6192e-04 - val_loss: 7.8622e-06
Epoch 47/512
448/448 - 0s - loss: 7.7891e-04 - val_loss: 8.0385e-06
Epoch 48/512
448/448 - 0s - loss: 7.8237e-04 - val_loss: 7.5087e-06
Epoch 49/512
448/448 - 0s - loss: 7.3627e-04 - val_loss: 7.5261e-06
Epoch 50/512
448/448 - 0s - loss: 7.5656e-04 - val_loss: 7.6359e-06
Epoch 51/512
448/448 - 0s - loss: 7.4517e-04 - val_loss: 7.7104e-06
Epoch 52/512
448/448 - 0s - loss: 7.5068e-04 - val_loss: 7.4504e-06
Epoch 53/512
448/448 - 0s - loss: 7.2610e-04 - val_loss: 7.2881e-06
Epoch 54/512
448/448 - 0s - loss: 7.2340e-04 - val_loss: 7.3312e-06
Epoch 55/512
448/448 - 0s - loss: 7.2648e-04 - val_loss: 7.1800e-06
Epoch 56/512
448/448 - 0s - loss: 7.1162e-04 - val_loss: 7.1240e-06
Epoch 57/512
448/448 - 0s - loss: 7.0926e-04 - val_loss: 7.0993e-06
Epoch 58/512
448/448 - 0s - loss: 7.0252e-04 - val_loss: 7.1327e-06
Epoch 59/512
448/448 - 0s - loss: 7.0436e-04 - val_loss: 7.0110e-06
Epoch 60/512
448/448 - 0s - loss: 6.9018e-04 - val_loss: 6.8779e-06
Epoch 61/512
448/448 - 0s - loss: 6.8700e-04 - val_loss: 6.7684e-06
Epoch 62/512
448/448 - 0s - loss: 6.7343e-04 - val_loss: 6.9063e-06
Epoch 63/512
448/448 - 0s - loss: 6.8549e-04 - val_loss: 6.8198e-06
Epoch 64/512
448/448 - 0s - loss: 6.7273e-04 - val_loss: 6.5917e-06
Epoch 65/512
448/448 - 0s - loss: 6.5849e-04 - val_loss: 6.5734e-06
Epoch 66/512
448/448 - 0s - loss: 6.5887e-04 - val_loss: 6.6312e-06
Epoch 67/512
448/448 - 0s - loss: 6.5942e-04 - val_loss: 6.5731e-06
Epoch 68/512
448/448 - 0s - loss: 6.5056e-04 - val_loss: 6.4121e-06
Epoch 69/512
448/448 - 0s - loss: 6.3899e-04 - val_loss: 6.5337e-06
Epoch 70/512
448/448 - 0s - loss: 6.4880e-04 - val_loss: 6.4619e-06
Epoch 71/512
448/448 - 0s - loss: 6.3559e-04 - val_loss: 6.2883e-06
Epoch 72/512
448/448 - 0s - loss: 6.2557e-04 - val_loss: 6.3605e-06
Epoch 73/512
448/448 - 0s - loss: 6.2912e-04 - val_loss: 6.3871e-06
Epoch 74/512
448/448 - 0s - loss: 6.3015e-04 - val_loss: 6.1522e-06
Epoch 75/512
448/448 - 0s - loss: 6.0931e-04 - val_loss: 6.1599e-06
Epoch 76/512
448/448 - 0s - loss: 6.1299e-04 - val_loss: 6.2464e-06
Epoch 77/512
448/448 - 0s - loss: 6.1828e-04 - val_loss: 6.0116e-06
Epoch 78/512
448/448 - 0s - loss: 5.9618e-04 - val_loss: 5.9211e-06
Epoch 79/512
448/448 - 0s - loss: 5.9399e-04 - val_loss: 6.1284e-06
Epoch 80/512
448/448 - 0s - loss: 6.1011e-04 - val_loss: 5.9650e-06
Epoch 81/512
448/448 - 0s - loss: 5.8291e-04 - val_loss: 5.9284e-06
Epoch 82/512
448/448 - 0s - loss: 5.8920e-04 - val_loss: 5.9037e-06
Epoch 83/512
448/448 - 0s - loss: 5.8215e-04 - val_loss: 5.8898e-06
Epoch 84/512
448/448 - 0s - loss: 5.7966e-04 - val_loss: 5.8125e-06
Epoch 85/512
448/448 - 0s - loss: 5.7515e-04 - val_loss: 5.7439e-06
Epoch 86/512
448/448 - 0s - loss: 5.7223e-04 - val_loss: 5.6330e-06
Epoch 87/512
448/448 - 0s - loss: 5.5718e-04 - val_loss: 5.8234e-06
Epoch 88/512
448/448 - 0s - loss: 5.7442e-04 - val_loss: 5.8206e-06
Epoch 89/512
448/448 - 0s - loss: 5.6977e-04 - val_loss: 5.3611e-06
Epoch 90/512
448/448 - 0s - loss: 5.3584e-04 - val_loss: 5.4701e-06
Epoch 91/512
448/448 - 0s - loss: 5.5379e-04 - val_loss: 5.6718e-06
Epoch 92/512
448/448 - 0s - loss: 5.5868e-04 - val_loss: 5.4553e-06
Epoch 93/512
448/448 - 0s - loss: 5.3827e-04 - val_loss: 5.3130e-06
Epoch 94/512
448/448 - 0s - loss: 5.3109e-04 - val_loss: 5.5028e-06
Epoch 95/512
448/448 - 0s - loss: 5.4873e-04 - val_loss: 5.2984e-06
Epoch 96/512
448/448 - 0s - loss: 5.2226e-04 - val_loss: 5.2970e-06
Epoch 97/512
448/448 - 0s - loss: 5.2744e-04 - val_loss: 5.4806e-06
Epoch 98/512
448/448 - 0s - loss: 5.3797e-04 - val_loss: 5.2473e-06
Epoch 99/512
448/448 - 0s - loss: 5.1435e-04 - val_loss: 5.1221e-06
Epoch 100/512
448/448 - 0s - loss: 5.1581e-04 - val_loss: 5.0971e-06
Epoch 101/512
448/448 - 0s - loss: 5.1191e-04 - val_loss: 5.1314e-06
Epoch 102/512
448/448 - 0s - loss: 5.1390e-04 - val_loss: 5.0573e-06
Epoch 103/512
448/448 - 0s - loss: 5.0231e-04 - val_loss: 5.0975e-06
Epoch 104/512
448/448 - 0s - loss: 5.0878e-04 - val_loss: 4.9674e-06
Epoch 105/512
448/448 - 0s - loss: 4.9057e-04 - val_loss: 5.0663e-06
Epoch 106/512
448/448 - 0s - loss: 5.0660e-04 - val_loss: 4.9097e-06
Epoch 107/512
448/448 - 0s - loss: 4.8257e-04 - val_loss: 4.9529e-06
Epoch 108/512
448/448 - 0s - loss: 4.9574e-04 - val_loss: 4.8997e-06
Epoch 109/512
448/448 - 0s - loss: 4.8222e-04 - val_loss: 4.8696e-06
Epoch 110/512
448/448 - 0s - loss: 4.8164e-04 - val_loss: 4.9160e-06
Epoch 111/512
448/448 - 0s - loss: 4.8329e-04 - val_loss: 4.7891e-06
Epoch 112/512
448/448 - 0s - loss: 4.7208e-04 - val_loss: 4.7171e-06
Epoch 113/512
448/448 - 0s - loss: 4.6895e-04 - val_loss: 4.7514e-06
Epoch 114/512
448/448 - 0s - loss: 4.7157e-04 - val_loss: 4.7154e-06
Epoch 115/512
448/448 - 0s - loss: 4.6564e-04 - val_loss: 4.6134e-06
Epoch 116/512
448/448 - 0s - loss: 4.5986e-04 - val_loss: 4.5528e-06
Epoch 117/512
448/448 - 0s - loss: 4.5816e-04 - val_loss: 4.5456e-06
Epoch 118/512
448/448 - 0s - loss: 4.5670e-04 - val_loss: 4.4967e-06
Epoch 119/512
448/448 - 0s - loss: 4.4593e-04 - val_loss: 4.6161e-06
Epoch 120/512
448/448 - 0s - loss: 4.5837e-04 - val_loss: 4.5162e-06
Epoch 121/512
448/448 - 0s - loss: 4.4245e-04 - val_loss: 4.4234e-06
Epoch 122/512
448/448 - 0s - loss: 4.4101e-04 - val_loss: 4.4004e-06
Epoch 123/512
448/448 - 0s - loss: 4.3801e-04 - val_loss: 4.4121e-06
Epoch 124/512
448/448 - 0s - loss: 4.3972e-04 - val_loss: 4.3653e-06
Epoch 125/512
448/448 - 0s - loss: 4.3106e-04 - val_loss: 4.2959e-06
Epoch 126/512
448/448 - 0s - loss: 4.2796e-04 - val_loss: 4.3124e-06
Epoch 127/512
448/448 - 0s - loss: 4.2626e-04 - val_loss: 4.3385e-06
Epoch 128/512
448/448 - 0s - loss: 4.2752e-04 - val_loss: 4.2396e-06
Epoch 129/512
448/448 - 0s - loss: 4.1709e-04 - val_loss: 4.2342e-06
Epoch 130/512
448/448 - 0s - loss: 4.2074e-04 - val_loss: 4.1987e-06
Epoch 131/512
448/448 - 0s - loss: 4.1477e-04 - val_loss: 4.1167e-06
Epoch 132/512
448/448 - 0s - loss: 4.1105e-04 - val_loss: 4.0492e-06
Epoch 133/512
448/448 - 0s - loss: 4.0239e-04 - val_loss: 4.1993e-06
Epoch 134/512
448/448 - 0s - loss: 4.1512e-04 - val_loss: 4.1159e-06
Epoch 135/512
448/448 - 0s - loss: 4.0485e-04 - val_loss: 3.8830e-06
Epoch 136/512
448/448 - 0s - loss: 3.8955e-04 - val_loss: 3.9507e-06
Epoch 137/512
448/448 - 0s - loss: 4.0087e-04 - val_loss: 4.0366e-06
Epoch 138/512
448/448 - 0s - loss: 3.9808e-04 - val_loss: 3.9787e-06
Epoch 139/512
448/448 - 0s - loss: 3.9348e-04 - val_loss: 3.8075e-06
Epoch 140/512
448/448 - 0s - loss: 3.7729e-04 - val_loss: 3.9699e-06
Epoch 141/512
448/448 - 0s - loss: 3.9860e-04 - val_loss: 3.8555e-06
Epoch 142/512
448/448 - 0s - loss: 3.7861e-04 - val_loss: 3.8004e-06
Epoch 143/512
448/448 - 0s - loss: 3.8230e-04 - val_loss: 3.7639e-06
Epoch 144/512
448/448 - 0s - loss: 3.7573e-04 - val_loss: 3.7529e-06
Epoch 145/512
448/448 - 0s - loss: 3.7629e-04 - val_loss: 3.7596e-06
Epoch 146/512
448/448 - 0s - loss: 3.7466e-04 - val_loss: 3.6760e-06
Epoch 147/512
448/448 - 0s - loss: 3.6924e-04 - val_loss: 3.5967e-06
Epoch 148/512
448/448 - 0s - loss: 3.6210e-04 - val_loss: 3.7213e-06
Epoch 149/512
448/448 - 0s - loss: 3.7273e-04 - val_loss: 3.6872e-06
Epoch 150/512
448/448 - 0s - loss: 3.6364e-04 - val_loss: 3.5536e-06
Epoch 151/512
448/448 - 0s - loss: 3.5732e-04 - val_loss: 3.5297e-06
Epoch 152/512
448/448 - 0s - loss: 3.5524e-04 - val_loss: 3.6147e-06
Epoch 153/512
448/448 - 0s - loss: 3.5991e-04 - val_loss: 3.6015e-06
Epoch 154/512
448/448 - 0s - loss: 3.5629e-04 - val_loss: 3.4391e-06
Epoch 155/512
448/448 - 0s - loss: 3.4276e-04 - val_loss: 3.4713e-06
Epoch 156/512
448/448 - 0s - loss: 3.4955e-04 - val_loss: 3.5659e-06
Epoch 157/512
448/448 - 0s - loss: 3.5236e-04 - val_loss: 3.4605e-06
Epoch 158/512
448/448 - 0s - loss: 3.4121e-04 - val_loss: 3.3568e-06
Epoch 159/512
448/448 - 0s - loss: 3.3698e-04 - val_loss: 3.4184e-06
Epoch 160/512
448/448 - 0s - loss: 3.4228e-04 - val_loss: 3.4746e-06
Epoch 161/512
448/448 - 0s - loss: 3.3980e-04 - val_loss: 3.4093e-06
Epoch 162/512
448/448 - 0s - loss: 3.3708e-04 - val_loss: 3.2433e-06
Epoch 163/512
448/448 - 0s - loss: 3.2351e-04 - val_loss: 3.3644e-06
Epoch 164/512
448/448 - 0s - loss: 3.3679e-04 - val_loss: 3.3870e-06
Epoch 165/512
448/448 - 0s - loss: 3.3299e-04 - val_loss: 3.1796e-06
Epoch 166/512
448/448 - 0s - loss: 3.1612e-04 - val_loss: 3.2551e-06
Epoch 167/512
448/448 - 0s - loss: 3.2748e-04 - val_loss: 3.3355e-06
Epoch 168/512
448/448 - 0s - loss: 3.3051e-04 - val_loss: 3.1266e-06
Epoch 169/512
448/448 - 0s - loss: 3.0988e-04 - val_loss: 3.1725e-06
Epoch 170/512
448/448 - 0s - loss: 3.2147e-04 - val_loss: 3.2435e-06
Epoch 171/512
448/448 - 0s - loss: 3.2107e-04 - val_loss: 3.1378e-06
Epoch 172/512
448/448 - 0s - loss: 3.1160e-04 - val_loss: 3.0722e-06
Epoch 173/512
448/448 - 0s - loss: 3.0783e-04 - val_loss: 3.1782e-06
Epoch 174/512
448/448 - 0s - loss: 3.1679e-04 - val_loss: 3.1149e-06
Epoch 175/512
448/448 - 0s - loss: 3.0698e-04 - val_loss: 3.0509e-06
Epoch 176/512
448/448 - 0s - loss: 3.0608e-04 - val_loss: 3.0520e-06
Epoch 177/512
448/448 - 0s - loss: 3.0552e-04 - val_loss: 3.0587e-06
Epoch 178/512
448/448 - 0s - loss: 3.0292e-04 - val_loss: 3.0760e-06
Epoch 179/512
448/448 - 0s - loss: 3.0567e-04 - val_loss: 2.9725e-06
Epoch 180/512
448/448 - 0s - loss: 2.9325e-04 - val_loss: 3.0333e-06
Epoch 181/512
448/448 - 0s - loss: 3.0247e-04 - val_loss: 2.9998e-06
Epoch 182/512
448/448 - 0s - loss: 2.9852e-04 - val_loss: 2.8870e-06
Epoch 183/512
448/448 - 0s - loss: 2.8783e-04 - val_loss: 2.9688e-06
Epoch 184/512
448/448 - 0s - loss: 2.9817e-04 - val_loss: 2.9490e-06
Epoch 185/512
448/448 - 0s - loss: 2.9079e-04 - val_loss: 2.8955e-06
Epoch 186/512
448/448 - 0s - loss: 2.8741e-04 - val_loss: 2.9232e-06
Epoch 187/512
448/448 - 0s - loss: 2.9124e-04 - val_loss: 2.8808e-06
Epoch 188/512
448/448 - 0s - loss: 2.8540e-04 - val_loss: 2.8009e-06
Epoch 189/512
448/448 - 0s - loss: 2.8115e-04 - val_loss: 2.8621e-06
Epoch 190/512
448/448 - 0s - loss: 2.8829e-04 - val_loss: 2.8250e-06
Epoch 191/512
448/448 - 0s - loss: 2.8267e-04 - val_loss: 2.7206e-06
Epoch 192/512
448/448 - 0s - loss: 2.7413e-04 - val_loss: 2.8237e-06
Epoch 193/512
448/448 - 0s - loss: 2.8443e-04 - val_loss: 2.8115e-06
Epoch 194/512
448/448 - 0s - loss: 2.7818e-04 - val_loss: 2.7372e-06
Epoch 195/512
448/448 - 0s - loss: 2.7260e-04 - val_loss: 2.7960e-06
Epoch 196/512
448/448 - 0s - loss: 2.7725e-04 - val_loss: 2.8014e-06
Epoch 197/512
448/448 - 0s - loss: 2.7657e-04 - val_loss: 2.6712e-06
Epoch 198/512
448/448 - 0s - loss: 2.6500e-04 - val_loss: 2.7265e-06
Epoch 199/512
448/448 - 0s - loss: 2.7476e-04 - val_loss: 2.7328e-06
Epoch 200/512
448/448 - 0s - loss: 2.7143e-04 - val_loss: 2.6453e-06
Epoch 201/512
448/448 - 0s - loss: 2.6364e-04 - val_loss: 2.6955e-06
Epoch 202/512
448/448 - 0s - loss: 2.6914e-04 - val_loss: 2.7304e-06
Epoch 203/512
448/448 - 0s - loss: 2.6791e-04 - val_loss: 2.6649e-06
Epoch 204/512
448/448 - 0s - loss: 2.6438e-04 - val_loss: 2.5775e-06
Epoch 205/512
448/448 - 0s - loss: 2.5856e-04 - val_loss: 2.6194e-06
Epoch 206/512
448/448 - 0s - loss: 2.6397e-04 - val_loss: 2.6232e-06
Epoch 207/512
448/448 - 0s - loss: 2.6117e-04 - val_loss: 2.5803e-06
Epoch 208/512
448/448 - 0s - loss: 2.5668e-04 - val_loss: 2.6297e-06
Epoch 209/512
448/448 - 0s - loss: 2.6190e-04 - val_loss: 2.5825e-06
Epoch 210/512
448/448 - 0s - loss: 2.5461e-04 - val_loss: 2.5676e-06
Epoch 211/512
448/448 - 0s - loss: 2.5742e-04 - val_loss: 2.5310e-06
Epoch 212/512
448/448 - 0s - loss: 2.5366e-04 - val_loss: 2.4912e-06
Epoch 213/512
448/448 - 0s - loss: 2.5142e-04 - val_loss: 2.5514e-06
Epoch 214/512
448/448 - 0s - loss: 2.5445e-04 - val_loss: 2.5851e-06
Epoch 215/512
448/448 - 0s - loss: 2.5339e-04 - val_loss: 2.5583e-06
Epoch 216/512
448/448 - 0s - loss: 2.5116e-04 - val_loss: 2.5004e-06
Epoch 217/512
448/448 - 0s - loss: 2.4858e-04 - val_loss: 2.4748e-06
Epoch 218/512
448/448 - 0s - loss: 2.4816e-04 - val_loss: 2.4933e-06
Epoch 219/512
448/448 - 0s - loss: 2.4845e-04 - val_loss: 2.4900e-06
Epoch 220/512
448/448 - 0s - loss: 2.4596e-04 - val_loss: 2.4843e-06
Epoch 221/512
448/448 - 0s - loss: 2.4750e-04 - val_loss: 2.4437e-06
Epoch 222/512
448/448 - 0s - loss: 2.4479e-04 - val_loss: 2.3759e-06
Epoch 223/512
448/448 - 0s - loss: 2.3865e-04 - val_loss: 2.4450e-06
Epoch 224/512
448/448 - 0s - loss: 2.4633e-04 - val_loss: 2.4663e-06
Epoch 225/512
448/448 - 0s - loss: 2.4376e-04 - val_loss: 2.3978e-06
Epoch 226/512
448/448 - 0s - loss: 2.3822e-04 - val_loss: 2.4148e-06
Epoch 227/512
448/448 - 0s - loss: 2.4137e-04 - val_loss: 2.4369e-06
Epoch 228/512
448/448 - 0s - loss: 2.4381e-04 - val_loss: 2.3023e-06
Epoch 229/512
448/448 - 0s - loss: 2.3027e-04 - val_loss: 2.3604e-06
Epoch 230/512
448/448 - 0s - loss: 2.4029e-04 - val_loss: 2.4125e-06
Epoch 231/512
448/448 - 0s - loss: 2.3887e-04 - val_loss: 2.3852e-06
Epoch 232/512
448/448 - 0s - loss: 2.3422e-04 - val_loss: 2.4153e-06
Epoch 233/512
448/448 - 0s - loss: 2.3872e-04 - val_loss: 2.3323e-06
Epoch 234/512
448/448 - 0s - loss: 2.3093e-04 - val_loss: 2.3034e-06
Epoch 235/512
448/448 - 0s - loss: 2.3324e-04 - val_loss: 2.3231e-06
Epoch 236/512
448/448 - 0s - loss: 2.3288e-04 - val_loss: 2.3371e-06
Epoch 237/512
448/448 - 0s - loss: 2.3321e-04 - val_loss: 2.2662e-06
Epoch 238/512
448/448 - 0s - loss: 2.2666e-04 - val_loss: 2.3252e-06
Epoch 239/512
448/448 - 0s - loss: 2.3200e-04 - val_loss: 2.3847e-06
Epoch 240/512
448/448 - 0s - loss: 2.3421e-04 - val_loss: 2.2743e-06
Epoch 241/512
448/448 - 0s - loss: 2.2631e-04 - val_loss: 2.2121e-06
Epoch 242/512
448/448 - 0s - loss: 2.2348e-04 - val_loss: 2.3256e-06
Epoch 243/512
448/448 - 0s - loss: 2.3271e-04 - val_loss: 2.3076e-06
Epoch 244/512
448/448 - 0s - loss: 2.2773e-04 - val_loss: 2.1792e-06
Epoch 245/512
448/448 - 0s - loss: 2.1666e-04 - val_loss: 2.3359e-06
Epoch 246/512
448/448 - 0s - loss: 2.3353e-04 - val_loss: 2.3270e-06
Epoch 247/512
448/448 - 0s - loss: 2.2598e-04 - val_loss: 2.1798e-06
Epoch 248/512
448/448 - 0s - loss: 2.1760e-04 - val_loss: 2.2085e-06
Epoch 249/512
448/448 - 0s - loss: 2.2505e-04 - val_loss: 2.2566e-06
Epoch 250/512
448/448 - 0s - loss: 2.2386e-04 - val_loss: 2.2420e-06
Epoch 251/512
448/448 - 0s - loss: 2.2056e-04 - val_loss: 2.2518e-06
Epoch 252/512
448/448 - 0s - loss: 2.2334e-04 - val_loss: 2.2156e-06
Epoch 253/512
448/448 - 0s - loss: 2.2007e-04 - val_loss: 2.1709e-06
Epoch 254/512
448/448 - 0s - loss: 2.1771e-04 - val_loss: 2.1896e-06
Epoch 255/512
448/448 - 0s - loss: 2.2125e-04 - val_loss: 2.1728e-06
Epoch 256/512
448/448 - 0s - loss: 2.1734e-04 - val_loss: 2.1740e-06
Epoch 257/512
448/448 - 0s - loss: 2.1839e-04 - val_loss: 2.1908e-06
Epoch 258/512
448/448 - 0s - loss: 2.2000e-04 - val_loss: 2.1156e-06
Epoch 259/512
448/448 - 0s - loss: 2.1201e-04 - val_loss: 2.1701e-06
Epoch 260/512
448/448 - 0s - loss: 2.1984e-04 - val_loss: 2.1507e-06
Epoch 261/512
448/448 - 0s - loss: 2.1427e-04 - val_loss: 2.1374e-06
Epoch 262/512
448/448 - 0s - loss: 2.1368e-04 - val_loss: 2.1887e-06
Epoch 263/512
448/448 - 0s - loss: 2.1519e-04 - val_loss: 2.2520e-06
Epoch 264/512
448/448 - 0s - loss: 2.2011e-04 - val_loss: 2.0973e-06
Epoch 265/512
448/448 - 0s - loss: 2.0615e-04 - val_loss: 2.1085e-06
Epoch 266/512
448/448 - 0s - loss: 2.1458e-04 - val_loss: 2.1467e-06
Epoch 267/512
448/448 - 0s - loss: 2.1268e-04 - val_loss: 2.1484e-06
Epoch 268/512
448/448 - 0s - loss: 2.1389e-04 - val_loss: 2.0894e-06
Epoch 269/512
448/448 - 0s - loss: 2.0790e-04 - val_loss: 2.1092e-06
Epoch 270/512
448/448 - 0s - loss: 2.1122e-04 - val_loss: 2.1646e-06
Epoch 271/512
448/448 - 0s - loss: 2.1367e-04 - val_loss: 2.0951e-06
Epoch 272/512
448/448 - 0s - loss: 2.0612e-04 - val_loss: 2.0948e-06
Epoch 273/512
448/448 - 0s - loss: 2.1054e-04 - val_loss: 2.0693e-06
Epoch 274/512
448/448 - 0s - loss: 2.0858e-04 - val_loss: 2.0273e-06
Epoch 275/512
448/448 - 0s - loss: 2.0532e-04 - val_loss: 2.0664e-06
Epoch 276/512
448/448 - 0s - loss: 2.0784e-04 - val_loss: 2.1193e-06
Epoch 277/512
448/448 - 0s - loss: 2.1061e-04 - val_loss: 2.0527e-06
Epoch 278/512
448/448 - 0s - loss: 2.0172e-04 - val_loss: 2.1050e-06
Epoch 279/512
448/448 - 0s - loss: 2.1027e-04 - val_loss: 2.0602e-06
Epoch 280/512
448/448 - 0s - loss: 2.0340e-04 - val_loss: 2.0452e-06
Epoch 281/512
448/448 - 0s - loss: 2.0473e-04 - val_loss: 2.0548e-06
Epoch 282/512
448/448 - 0s - loss: 2.0416e-04 - val_loss: 2.0520e-06
Epoch 283/512
448/448 - 0s - loss: 2.0321e-04 - val_loss: 2.0824e-06
Epoch 284/512
448/448 - 0s - loss: 2.0621e-04 - val_loss: 2.0275e-06
Epoch 285/512
448/448 - 0s - loss: 2.0136e-04 - val_loss: 1.9731e-06
Epoch 286/512
448/448 - 0s - loss: 1.9835e-04 - val_loss: 2.0586e-06
Epoch 287/512
448/448 - 0s - loss: 2.0535e-04 - val_loss: 2.0701e-06
Epoch 288/512
448/448 - 0s - loss: 2.0247e-04 - val_loss: 2.0039e-06
Epoch 289/512
448/448 - 0s - loss: 1.9783e-04 - val_loss: 2.0136e-06
Epoch 290/512
448/448 - 0s - loss: 2.0123e-04 - val_loss: 2.0371e-06
Epoch 291/512
448/448 - 0s - loss: 2.0189e-04 - val_loss: 2.0036e-06
Epoch 292/512
448/448 - 0s - loss: 1.9821e-04 - val_loss: 2.0073e-06
Epoch 293/512
448/448 - 0s - loss: 1.9923e-04 - val_loss: 2.0191e-06
Epoch 294/512
448/448 - 0s - loss: 2.0247e-04 - val_loss: 1.8718e-06
Epoch 295/512
448/448 - 0s - loss: 1.8919e-04 - val_loss: 1.9716e-06
Epoch 296/512
448/448 - 0s - loss: 2.0036e-04 - val_loss: 2.0950e-06
Epoch 297/512
448/448 - 0s - loss: 2.0374e-04 - val_loss: 1.9495e-06
Epoch 298/512
448/448 - 0s - loss: 1.9250e-04 - val_loss: 1.8768e-06
Epoch 299/512
448/448 - 0s - loss: 1.8969e-04 - val_loss: 2.0368e-06
Epoch 300/512
448/448 - 0s - loss: 2.0328e-04 - val_loss: 2.0161e-06
Epoch 301/512
448/448 - 0s - loss: 1.9624e-04 - val_loss: 1.8706e-06
Epoch 302/512
448/448 - 0s - loss: 1.8605e-04 - val_loss: 1.9747e-06
Epoch 303/512
448/448 - 0s - loss: 2.0055e-04 - val_loss: 1.9377e-06
Epoch 304/512
448/448 - 0s - loss: 1.9083e-04 - val_loss: 1.9254e-06
Epoch 305/512
448/448 - 0s - loss: 1.9312e-04 - val_loss: 1.9478e-06
Epoch 306/512
448/448 - 0s - loss: 1.9260e-04 - val_loss: 1.9650e-06
Epoch 307/512
448/448 - 0s - loss: 1.9344e-04 - val_loss: 1.9330e-06
Epoch 308/512
448/448 - 0s - loss: 1.9188e-04 - val_loss: 1.8610e-06
Epoch 309/512
448/448 - 0s - loss: 1.8694e-04 - val_loss: 1.9159e-06
Epoch 310/512
448/448 - 0s - loss: 1.9410e-04 - val_loss: 1.9021e-06
Epoch 311/512
448/448 - 0s - loss: 1.8758e-04 - val_loss: 1.9335e-06
Epoch 312/512
448/448 - 0s - loss: 1.9285e-04 - val_loss: 1.8755e-06
Epoch 313/512
448/448 - 0s - loss: 1.8646e-04 - val_loss: 1.8617e-06
Epoch 314/512
448/448 - 0s - loss: 1.8777e-04 - val_loss: 1.8980e-06
Epoch 315/512
448/448 - 0s - loss: 1.8856e-04 - val_loss: 1.9222e-06
Epoch 316/512
448/448 - 0s - loss: 1.9057e-04 - val_loss: 1.8232e-06
Epoch 317/512
448/448 - 0s - loss: 1.8051e-04 - val_loss: 1.8844e-06
Epoch 318/512
448/448 - 0s - loss: 1.9039e-04 - val_loss: 1.8746e-06
Epoch 319/512
448/448 - 0s - loss: 1.8545e-04 - val_loss: 1.8206e-06
Epoch 320/512
448/448 - 0s - loss: 1.8195e-04 - val_loss: 1.8557e-06
Epoch 321/512
448/448 - 0s - loss: 1.8641e-04 - val_loss: 1.8674e-06
Epoch 322/512
448/448 - 0s - loss: 1.8550e-04 - val_loss: 1.8023e-06
Epoch 323/512
448/448 - 0s - loss: 1.8049e-04 - val_loss: 1.8368e-06
Epoch 324/512
448/448 - 0s - loss: 1.8503e-04 - val_loss: 1.8339e-06
Epoch 325/512
448/448 - 0s - loss: 1.8192e-04 - val_loss: 1.8348e-06
Epoch 326/512
448/448 - 0s - loss: 1.8176e-04 - val_loss: 1.8324e-06
Epoch 327/512
448/448 - 0s - loss: 1.8123e-04 - val_loss: 1.8146e-06
Epoch 328/512
448/448 - 0s - loss: 1.8194e-04 - val_loss: 1.7648e-06
Epoch 329/512
448/448 - 0s - loss: 1.7599e-04 - val_loss: 1.8127e-06
Epoch 330/512
448/448 - 0s - loss: 1.8252e-04 - val_loss: 1.8171e-06
Epoch 331/512
448/448 - 0s - loss: 1.7918e-04 - val_loss: 1.7527e-06
Epoch 332/512
448/448 - 0s - loss: 1.7553e-04 - val_loss: 1.7701e-06
Epoch 333/512
448/448 - 0s - loss: 1.7821e-04 - val_loss: 1.8058e-06
Epoch 334/512
448/448 - 0s - loss: 1.7799e-04 - val_loss: 1.8065e-06
Epoch 335/512
448/448 - 0s - loss: 1.7848e-04 - val_loss: 1.7201e-06
Epoch 336/512
448/448 - 0s - loss: 1.7108e-04 - val_loss: 1.7600e-06
Epoch 337/512
448/448 - 0s - loss: 1.7634e-04 - val_loss: 1.7788e-06
Epoch 338/512
448/448 - 0s - loss: 1.7645e-04 - val_loss: 1.7286e-06
Epoch 339/512
448/448 - 0s - loss: 1.7232e-04 - val_loss: 1.7068e-06
Epoch 340/512
448/448 - 0s - loss: 1.7112e-04 - val_loss: 1.7513e-06
Epoch 341/512
448/448 - 0s - loss: 1.7569e-04 - val_loss: 1.7068e-06
Epoch 342/512
448/448 - 0s - loss: 1.7083e-04 - val_loss: 1.6634e-06
Epoch 343/512
448/448 - 0s - loss: 1.6828e-04 - val_loss: 1.7230e-06
Epoch 344/512
448/448 - 0s - loss: 1.7339e-04 - val_loss: 1.6853e-06
Epoch 345/512
448/448 - 0s - loss: 1.6857e-04 - val_loss: 1.6387e-06
Epoch 346/512
448/448 - 0s - loss: 1.6654e-04 - val_loss: 1.6609e-06
Epoch 347/512
448/448 - 0s - loss: 1.6667e-04 - val_loss: 1.7608e-06
Epoch 348/512
448/448 - 0s - loss: 1.7395e-04 - val_loss: 1.6644e-06
Epoch 349/512
448/448 - 0s - loss: 1.6367e-04 - val_loss: 1.5917e-06
Epoch 350/512
448/448 - 0s - loss: 1.6298e-04 - val_loss: 1.6645e-06
Epoch 351/512
448/448 - 0s - loss: 1.6834e-04 - val_loss: 1.6669e-06
Epoch 352/512
448/448 - 0s - loss: 1.6417e-04 - val_loss: 1.6308e-06
Epoch 353/512
448/448 - 0s - loss: 1.6170e-04 - val_loss: 1.6886e-06
Epoch 354/512
448/448 - 0s - loss: 1.6779e-04 - val_loss: 1.6235e-06
Epoch 355/512
448/448 - 0s - loss: 1.6091e-04 - val_loss: 1.5554e-06
Epoch 356/512
448/448 - 0s - loss: 1.5813e-04 - val_loss: 1.5896e-06
Epoch 357/512
448/448 - 0s - loss: 1.6025e-04 - val_loss: 1.6791e-06
Epoch 358/512
448/448 - 0s - loss: 1.6494e-04 - val_loss: 1.6137e-06
Epoch 359/512
448/448 - 0s - loss: 1.5735e-04 - val_loss: 1.5420e-06
Epoch 360/512
448/448 - 0s - loss: 1.5594e-04 - val_loss: 1.5633e-06
Epoch 361/512
448/448 - 0s - loss: 1.5785e-04 - val_loss: 1.5841e-06
Epoch 362/512
448/448 - 0s - loss: 1.5666e-04 - val_loss: 1.6045e-06
Epoch 363/512
448/448 - 0s - loss: 1.5905e-04 - val_loss: 1.5255e-06
Epoch 364/512
448/448 - 0s - loss: 1.5095e-04 - val_loss: 1.5188e-06
Epoch 365/512
448/448 - 0s - loss: 1.5343e-04 - val_loss: 1.5592e-06
Epoch 366/512
448/448 - 0s - loss: 1.5452e-04 - val_loss: 1.5564e-06
Epoch 367/512
448/448 - 0s - loss: 1.5353e-04 - val_loss: 1.5025e-06
Epoch 368/512
448/448 - 0s - loss: 1.4947e-04 - val_loss: 1.4857e-06
Epoch 369/512
448/448 - 0s - loss: 1.4830e-04 - val_loss: 1.5515e-06
Epoch 370/512
448/448 - 0s - loss: 1.5378e-04 - val_loss: 1.4900e-06
Epoch 371/512
448/448 - 0s - loss: 1.4697e-04 - val_loss: 1.4259e-06
Epoch 372/512
448/448 - 0s - loss: 1.4443e-04 - val_loss: 1.4824e-06
Epoch 373/512
448/448 - 0s - loss: 1.4978e-04 - val_loss: 1.4796e-06
Epoch 374/512
448/448 - 0s - loss: 1.4628e-04 - val_loss: 1.4369e-06
Epoch 375/512
448/448 - 0s - loss: 1.4365e-04 - val_loss: 1.4013e-06
Epoch 376/512
448/448 - 0s - loss: 1.4218e-04 - val_loss: 1.4187e-06
Epoch 377/512
448/448 - 0s - loss: 1.4297e-04 - val_loss: 1.4241e-06
Epoch 378/512
448/448 - 0s - loss: 1.4154e-04 - val_loss: 1.4185e-06
Epoch 379/512
448/448 - 0s - loss: 1.4235e-04 - val_loss: 1.3488e-06
Epoch 380/512
448/448 - 0s - loss: 1.3475e-04 - val_loss: 1.3917e-06
Epoch 381/512
448/448 - 0s - loss: 1.4068e-04 - val_loss: 1.4152e-06
Epoch 382/512
448/448 - 0s - loss: 1.4062e-04 - val_loss: 1.3108e-06
Epoch 383/512
448/448 - 0s - loss: 1.3157e-04 - val_loss: 1.3468e-06
Epoch 384/512
448/448 - 0s - loss: 1.3680e-04 - val_loss: 1.3760e-06
Epoch 385/512
448/448 - 0s - loss: 1.3597e-04 - val_loss: 1.3299e-06
Epoch 386/512
448/448 - 0s - loss: 1.3254e-04 - val_loss: 1.2881e-06
Epoch 387/512
448/448 - 0s - loss: 1.2871e-04 - val_loss: 1.3522e-06
Epoch 388/512
448/448 - 0s - loss: 1.3544e-04 - val_loss: 1.2959e-06
Epoch 389/512
448/448 - 0s - loss: 1.2822e-04 - val_loss: 1.2507e-06
Epoch 390/512
448/448 - 0s - loss: 1.2671e-04 - val_loss: 1.2852e-06
Epoch 391/512
448/448 - 0s - loss: 1.2846e-04 - val_loss: 1.3162e-06
Epoch 392/512
448/448 - 0s - loss: 1.2985e-04 - val_loss: 1.2362e-06
Epoch 393/512
448/448 - 0s - loss: 1.2190e-04 - val_loss: 1.2124e-06
Epoch 394/512
448/448 - 0s - loss: 1.2360e-04 - val_loss: 1.2571e-06
Epoch 395/512
448/448 - 0s - loss: 1.2510e-04 - val_loss: 1.2307e-06
Epoch 396/512
448/448 - 0s - loss: 1.2283e-04 - val_loss: 1.1436e-06
Epoch 397/512
448/448 - 0s - loss: 1.1547e-04 - val_loss: 1.2210e-06
Epoch 398/512
448/448 - 0s - loss: 1.2322e-04 - val_loss: 1.2378e-06
Epoch 399/512
448/448 - 0s - loss: 1.2136e-04 - val_loss: 1.1191e-06
Epoch 400/512
448/448 - 0s - loss: 1.1220e-04 - val_loss: 1.1119e-06
Epoch 401/512
448/448 - 0s - loss: 1.1382e-04 - val_loss: 1.2130e-06
Epoch 402/512
448/448 - 0s - loss: 1.2081e-04 - val_loss: 1.1380e-06
Epoch 403/512
448/448 - 0s - loss: 1.1041e-04 - val_loss: 1.0936e-06
Epoch 404/512
448/448 - 0s - loss: 1.1069e-04 - val_loss: 1.1242e-06
Epoch 405/512
448/448 - 0s - loss: 1.1323e-04 - val_loss: 1.0990e-06
Epoch 406/512
448/448 - 0s - loss: 1.0934e-04 - val_loss: 1.0660e-06
Epoch 407/512
448/448 - 0s - loss: 1.0738e-04 - val_loss: 1.0709e-06
Epoch 408/512
448/448 - 0s - loss: 1.0799e-04 - val_loss: 1.0579e-06
Epoch 409/512
448/448 - 0s - loss: 1.0606e-04 - val_loss: 1.0331e-06
Epoch 410/512
448/448 - 0s - loss: 1.0422e-04 - val_loss: 1.0287e-06
Epoch 411/512
448/448 - 0s - loss: 1.0416e-04 - val_loss: 9.8801e-07
Epoch 412/512
448/448 - 0s - loss: 1.0062e-04 - val_loss: 9.8482e-07
Epoch 413/512
448/448 - 0s - loss: 1.0011e-04 - val_loss: 1.0062e-06
Epoch 414/512
448/448 - 0s - loss: 1.0078e-04 - val_loss: 1.0009e-06
Epoch 415/512
448/448 - 0s - loss: 9.9973e-05 - val_loss: 9.2034e-07
Epoch 416/512
448/448 - 0s - loss: 9.3467e-05 - val_loss: 9.2591e-07
Epoch 417/512
448/448 - 0s - loss: 9.5266e-05 - val_loss: 9.7149e-07
Epoch 418/512
448/448 - 0s - loss: 9.6836e-05 - val_loss: 9.4313e-07
Epoch 419/512
448/448 - 0s - loss: 9.2933e-05 - val_loss: 8.9319e-07
Epoch 420/512
448/448 - 0s - loss: 8.9808e-05 - val_loss: 9.0493e-07
Epoch 421/512
448/448 - 0s - loss: 9.1779e-05 - val_loss: 8.8162e-07
Epoch 422/512
448/448 - 0s - loss: 8.7915e-05 - val_loss: 8.8513e-07
Epoch 423/512
448/448 - 0s - loss: 8.9025e-05 - val_loss: 8.6383e-07
Epoch 424/512
448/448 - 0s - loss: 8.5875e-05 - val_loss: 8.6017e-07
Epoch 425/512
448/448 - 0s - loss: 8.6774e-05 - val_loss: 8.2067e-07
Epoch 426/512
448/448 - 0s - loss: 8.2837e-05 - val_loss: 8.0260e-07
Epoch 427/512
448/448 - 0s - loss: 8.2065e-05 - val_loss: 8.1346e-07
Epoch 428/512
448/448 - 0s - loss: 8.2327e-05 - val_loss: 8.1121e-07
Epoch 429/512
448/448 - 0s - loss: 8.0983e-05 - val_loss: 7.7264e-07
Epoch 430/512
448/448 - 0s - loss: 7.7394e-05 - val_loss: 7.6842e-07
Epoch 431/512
448/448 - 0s - loss: 7.7642e-05 - val_loss: 7.6723e-07
Epoch 432/512
448/448 - 0s - loss: 7.7027e-05 - val_loss: 7.4223e-07
Epoch 433/512
448/448 - 0s - loss: 7.4658e-05 - val_loss: 7.2432e-07
Epoch 434/512
448/448 - 0s - loss: 7.3314e-05 - val_loss: 7.3785e-07
Epoch 435/512
448/448 - 0s - loss: 7.3662e-05 - val_loss: 7.1807e-07
Epoch 436/512
448/448 - 0s - loss: 7.0828e-05 - val_loss: 6.9988e-07
Epoch 437/512
448/448 - 0s - loss: 7.0101e-05 - val_loss: 6.8832e-07
Epoch 438/512
448/448 - 0s - loss: 6.8846e-05 - val_loss: 6.7354e-07
Epoch 439/512
448/448 - 0s - loss: 6.7472e-05 - val_loss: 6.6439e-07
Epoch 440/512
448/448 - 0s - loss: 6.6983e-05 - val_loss: 6.3152e-07
Epoch 441/512
448/448 - 0s - loss: 6.3933e-05 - val_loss: 6.2456e-07
Epoch 442/512
448/448 - 0s - loss: 6.3507e-05 - val_loss: 6.4608e-07
Epoch 443/512
448/448 - 0s - loss: 6.5187e-05 - val_loss: 5.8326e-07
Epoch 444/512
448/448 - 0s - loss: 5.8744e-05 - val_loss: 5.7400e-07
Epoch 445/512
448/448 - 0s - loss: 5.9710e-05 - val_loss: 6.1668e-07
Epoch 446/512
448/448 - 0s - loss: 6.2118e-05 - val_loss: 5.6273e-07
Epoch 447/512
448/448 - 0s - loss: 5.6321e-05 - val_loss: 5.3415e-07
Epoch 448/512
448/448 - 0s - loss: 5.4833e-05 - val_loss: 5.7536e-07
Epoch 449/512
448/448 - 0s - loss: 5.8167e-05 - val_loss: 5.5785e-07
Epoch 450/512
448/448 - 0s - loss: 5.4816e-05 - val_loss: 4.9773e-07
Epoch 451/512
448/448 - 0s - loss: 5.0168e-05 - val_loss: 5.2457e-07
Epoch 452/512
448/448 - 0s - loss: 5.3537e-05 - val_loss: 5.3720e-07
Epoch 453/512
448/448 - 0s - loss: 5.2698e-05 - val_loss: 4.8040e-07
Epoch 454/512
448/448 - 0s - loss: 4.7767e-05 - val_loss: 4.7460e-07
Epoch 455/512
448/448 - 0s - loss: 4.8919e-05 - val_loss: 4.8696e-07
Epoch 456/512
448/448 - 0s - loss: 4.8824e-05 - val_loss: 4.6695e-07
Epoch 457/512
448/448 - 0s - loss: 4.6523e-05 - val_loss: 4.4472e-07
Epoch 458/512
448/448 - 0s - loss: 4.4979e-05 - val_loss: 4.4070e-07
Epoch 459/512
448/448 - 0s - loss: 4.4507e-05 - val_loss: 4.4363e-07
Epoch 460/512
448/448 - 0s - loss: 4.4396e-05 - val_loss: 4.2105e-07
Epoch 461/512
448/448 - 0s - loss: 4.1860e-05 - val_loss: 4.0548e-07
Epoch 462/512
448/448 - 0s - loss: 4.1389e-05 - val_loss: 4.0187e-07
Epoch 463/512
448/448 - 0s - loss: 4.0715e-05 - val_loss: 3.9230e-07
Epoch 464/512
448/448 - 0s - loss: 3.9683e-05 - val_loss: 3.7902e-07
Epoch 465/512
448/448 - 0s - loss: 3.8250e-05 - val_loss: 3.7087e-07
Epoch 466/512
448/448 - 0s - loss: 3.7888e-05 - val_loss: 3.5999e-07
Epoch 467/512
448/448 - 0s - loss: 3.6598e-05 - val_loss: 3.4893e-07
Epoch 468/512
448/448 - 0s - loss: 3.5450e-05 - val_loss: 3.5089e-07
Epoch 469/512
448/448 - 0s - loss: 3.5383e-05 - val_loss: 3.4348e-07
Epoch 470/512
448/448 - 0s - loss: 3.4054e-05 - val_loss: 3.2510e-07
Epoch 471/512
448/448 - 0s - loss: 3.2867e-05 - val_loss: 3.1698e-07
Epoch 472/512
448/448 - 0s - loss: 3.2381e-05 - val_loss: 3.0855e-07
Epoch 473/512
448/448 - 0s - loss: 3.1336e-05 - val_loss: 3.0449e-07
Epoch 474/512
448/448 - 0s - loss: 3.0809e-05 - val_loss: 2.9530e-07
Epoch 475/512
448/448 - 0s - loss: 2.9557e-05 - val_loss: 2.9049e-07
Epoch 476/512
448/448 - 0s - loss: 2.9236e-05 - val_loss: 2.8267e-07
Epoch 477/512
448/448 - 0s - loss: 2.8475e-05 - val_loss: 2.6599e-07
Epoch 478/512
448/448 - 0s - loss: 2.6930e-05 - val_loss: 2.6216e-07
Epoch 479/512
448/448 - 0s - loss: 2.6637e-05 - val_loss: 2.6579e-07
Epoch 480/512
448/448 - 0s - loss: 2.6796e-05 - val_loss: 2.4434e-07
Epoch 481/512
448/448 - 0s - loss: 2.4320e-05 - val_loss: 2.3908e-07
Epoch 482/512
448/448 - 0s - loss: 2.4571e-05 - val_loss: 2.4298e-07
Epoch 483/512
448/448 - 0s - loss: 2.4280e-05 - val_loss: 2.3141e-07
Epoch 484/512
448/448 - 0s - loss: 2.3187e-05 - val_loss: 2.1284e-07
Epoch 485/512
448/448 - 0s - loss: 2.1785e-05 - val_loss: 2.1290e-07
Epoch 486/512
448/448 - 0s - loss: 2.2014e-05 - val_loss: 2.1144e-07
Epoch 487/512
448/448 - 0s - loss: 2.1301e-05 - val_loss: 2.0482e-07
Epoch 488/512
448/448 - 0s - loss: 2.0647e-05 - val_loss: 1.9455e-07
Epoch 489/512
448/448 - 0s - loss: 1.9581e-05 - val_loss: 1.9136e-07
Epoch 490/512
448/448 - 0s - loss: 1.9518e-05 - val_loss: 1.8549e-07
Epoch 491/512
448/448 - 0s - loss: 1.8839e-05 - val_loss: 1.7566e-07
Epoch 492/512
448/448 - 0s - loss: 1.7929e-05 - val_loss: 1.6981e-07
Epoch 493/512
448/448 - 0s - loss: 1.7527e-05 - val_loss: 1.6561e-07
Epoch 494/512
448/448 - 0s - loss: 1.6853e-05 - val_loss: 1.6922e-07
Epoch 495/512
448/448 - 0s - loss: 1.7145e-05 - val_loss: 1.5384e-07
Epoch 496/512
448/448 - 0s - loss: 1.5487e-05 - val_loss: 1.4566e-07
Epoch 497/512
448/448 - 0s - loss: 1.5150e-05 - val_loss: 1.5213e-07
Epoch 498/512
448/448 - 0s - loss: 1.5604e-05 - val_loss: 1.4246e-07
Epoch 499/512
448/448 - 0s - loss: 1.4242e-05 - val_loss: 1.3360e-07
Epoch 500/512
448/448 - 0s - loss: 1.3740e-05 - val_loss: 1.3680e-07
Epoch 501/512
448/448 - 0s - loss: 1.4056e-05 - val_loss: 1.2971e-07
Epoch 502/512
448/448 - 0s - loss: 1.2942e-05 - val_loss: 1.2303e-07
Epoch 503/512
448/448 - 0s - loss: 1.2552e-05 - val_loss: 1.2262e-07
Epoch 504/512
448/448 - 0s - loss: 1.2515e-05 - val_loss: 1.1658e-07
Epoch 505/512
448/448 - 0s - loss: 1.1739e-05 - val_loss: 1.1162e-07
Epoch 506/512
448/448 - 0s - loss: 1.1423e-05 - val_loss: 1.0985e-07
Epoch 507/512
448/448 - 0s - loss: 1.1191e-05 - val_loss: 1.0524e-07
Epoch 508/512
448/448 - 0s - loss: 1.0637e-05 - val_loss: 1.0056e-07
Epoch 509/512
448/448 - 0s - loss: 1.0290e-05 - val_loss: 9.7460e-08
Epoch 510/512
448/448 - 0s - loss: 9.9528e-06 - val_loss: 9.6070e-08
Epoch 511/512
448/448 - 0s - loss: 9.8033e-06 - val_loss: 9.0044e-08
Epoch 512/512
448/448 - 0s - loss: 9.1406e-06 - val_loss: 8.6572e-08
2024-04-14 21:53:26.788737: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5884e-06 - val_loss: 6.5701e-06
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00001
448/448 - 0s - loss: 7.3031e-06 - val_loss: 8.7396e-06
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00001
448/448 - 0s - loss: 9.4441e-06 - val_loss: 8.6322e-06
Epoch 4/512

Epoch 00004: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0564e-06 - val_loss: 6.4233e-06
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00001
448/448 - 0s - loss: 6.5558e-06 - val_loss: 6.8335e-06
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00001
448/448 - 0s - loss: 7.5110e-06 - val_loss: 7.7660e-06
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00001
448/448 - 0s - loss: 7.6918e-06 - val_loss: 6.4327e-06
Epoch 8/512

Epoch 00008: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.3011e-06 - val_loss: 5.8466e-06
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00001
448/448 - 0s - loss: 6.2432e-06 - val_loss: 6.4989e-06
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00001
448/448 - 0s - loss: 6.7451e-06 - val_loss: 6.1416e-06
Epoch 11/512

Epoch 00011: val_loss improved from 0.00001 to 0.00001, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0393e-06 - val_loss: 5.3420e-06
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00001
448/448 - 0s - loss: 5.5020e-06 - val_loss: 5.4696e-06
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00001
448/448 - 0s - loss: 5.7526e-06 - val_loss: 5.5488e-06
Epoch 14/512

Epoch 00014: val_loss improved from 0.00001 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5738e-06 - val_loss: 4.9635e-06
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0144e-06 - val_loss: 4.7313e-06
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9434e-06 - val_loss: 4.8456e-06
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9664e-06 - val_loss: 4.5756e-06
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6026e-06 - val_loss: 4.2127e-06
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3384e-06 - val_loss: 4.1807e-06
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3334e-06 - val_loss: 4.1056e-06
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1681e-06 - val_loss: 3.8114e-06
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8824e-06 - val_loss: 3.6582e-06
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7858e-06 - val_loss: 3.6111e-06
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7022e-06 - val_loss: 3.4263e-06
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4885e-06 - val_loss: 3.2347e-06
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3310e-06 - val_loss: 3.1646e-06
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2595e-06 - val_loss: 3.0570e-06
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1187e-06 - val_loss: 2.8846e-06
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9565e-06 - val_loss: 2.7623e-06
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8510e-06 - val_loss: 2.6956e-06
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7704e-06 - val_loss: 2.5664e-06
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6261e-06 - val_loss: 2.4379e-06
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5082e-06 - val_loss: 2.3695e-06
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4418e-06 - val_loss: 2.2703e-06
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3243e-06 - val_loss: 2.1547e-06
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2181e-06 - val_loss: 2.0721e-06
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1397e-06 - val_loss: 1.9967e-06
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0532e-06 - val_loss: 1.9092e-06
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9612e-06 - val_loss: 1.8264e-06
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8799e-06 - val_loss: 1.7539e-06
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8053e-06 - val_loss: 1.6805e-06
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7281e-06 - val_loss: 1.6065e-06
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6539e-06 - val_loss: 1.5333e-06
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5807e-06 - val_loss: 1.4760e-06
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5219e-06 - val_loss: 1.4120e-06
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4535e-06 - val_loss: 1.3463e-06
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3884e-06 - val_loss: 1.2896e-06
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3315e-06 - val_loss: 1.2375e-06
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2766e-06 - val_loss: 1.1815e-06
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2181e-06 - val_loss: 1.1318e-06
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1684e-06 - val_loss: 1.0813e-06
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1162e-06 - val_loss: 1.0319e-06
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0660e-06 - val_loss: 9.8994e-07
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0236e-06 - val_loss: 9.4482e-07
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7548e-07 - val_loss: 9.0505e-07
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.3484e-07 - val_loss: 8.6569e-07
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.9347e-07 - val_loss: 8.2539e-07
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.5222e-07 - val_loss: 7.9062e-07
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.1764e-07 - val_loss: 7.5504e-07
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8032e-07 - val_loss: 7.1719e-07
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4259e-07 - val_loss: 6.8954e-07
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.1466e-07 - val_loss: 6.5896e-07
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8060e-07 - val_loss: 6.2640e-07
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.4831e-07 - val_loss: 5.9974e-07
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.2142e-07 - val_loss: 5.7422e-07
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9410e-07 - val_loss: 5.4554e-07
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6486e-07 - val_loss: 5.2044e-07
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3975e-07 - val_loss: 4.9986e-07
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1821e-07 - val_loss: 4.7522e-07
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9161e-07 - val_loss: 4.5281e-07
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6971e-07 - val_loss: 4.3330e-07
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4972e-07 - val_loss: 4.1377e-07
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2827e-07 - val_loss: 3.9468e-07
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0890e-07 - val_loss: 3.7590e-07
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8948e-07 - val_loss: 3.5918e-07
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7267e-07 - val_loss: 3.4147e-07
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5427e-07 - val_loss: 3.2531e-07
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3778e-07 - val_loss: 3.1274e-07
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2448e-07 - val_loss: 2.9660e-07
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0692e-07 - val_loss: 2.8249e-07
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9363e-07 - val_loss: 2.6966e-07
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7988e-07 - val_loss: 2.5709e-07
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6722e-07 - val_loss: 2.4401e-07
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5351e-07 - val_loss: 2.3380e-07
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4324e-07 - val_loss: 2.2254e-07
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3098e-07 - val_loss: 2.1103e-07
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1936e-07 - val_loss: 2.0212e-07
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1026e-07 - val_loss: 1.9328e-07
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0058e-07 - val_loss: 1.8281e-07
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8986e-07 - val_loss: 1.7418e-07
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8121e-07 - val_loss: 1.6707e-07
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7374e-07 - val_loss: 1.5833e-07
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6427e-07 - val_loss: 1.4966e-07
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5613e-07 - val_loss: 1.4391e-07
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4991e-07 - val_loss: 1.3730e-07
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4250e-07 - val_loss: 1.2980e-07
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3497e-07 - val_loss: 1.2383e-07
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2902e-07 - val_loss: 1.1810e-07
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2292e-07 - val_loss: 1.1213e-07
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1661e-07 - val_loss: 1.0698e-07
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1148e-07 - val_loss: 1.0179e-07
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0588e-07 - val_loss: 9.6629e-08
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0058e-07 - val_loss: 9.2151e-08
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.6015e-08 - val_loss: 8.7861e-08
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1433e-08 - val_loss: 8.3456e-08
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.6800e-08 - val_loss: 7.9370e-08
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2651e-08 - val_loss: 7.5485e-08
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8601e-08 - val_loss: 7.2089e-08
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5016e-08 - val_loss: 6.8404e-08
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.1107e-08 - val_loss: 6.4953e-08
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7607e-08 - val_loss: 6.2055e-08
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.4633e-08 - val_loss: 5.8920e-08
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1276e-08 - val_loss: 5.5910e-08
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8243e-08 - val_loss: 5.3232e-08
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5426e-08 - val_loss: 5.0876e-08
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2889e-08 - val_loss: 4.8235e-08
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0169e-08 - val_loss: 4.5797e-08
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7662e-08 - val_loss: 4.3535e-08
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5351e-08 - val_loss: 4.1526e-08
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3211e-08 - val_loss: 3.9543e-08
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1089e-08 - val_loss: 3.7429e-08
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8950e-08 - val_loss: 3.5735e-08
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7159e-08 - val_loss: 3.4070e-08
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5371e-08 - val_loss: 3.2207e-08
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3422e-08 - val_loss: 3.0614e-08
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1942e-08 - val_loss: 2.9263e-08
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0429e-08 - val_loss: 2.7888e-08
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8875e-08 - val_loss: 2.6359e-08
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7394e-08 - val_loss: 2.5129e-08
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6092e-08 - val_loss: 2.3944e-08
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4891e-08 - val_loss: 2.2818e-08
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3626e-08 - val_loss: 2.1582e-08
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2402e-08 - val_loss: 2.0564e-08
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1341e-08 - val_loss: 1.9648e-08
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0396e-08 - val_loss: 1.8625e-08
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9277e-08 - val_loss: 1.7664e-08
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8312e-08 - val_loss: 1.6917e-08
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7575e-08 - val_loss: 1.6165e-08
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6713e-08 - val_loss: 1.5190e-08
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5718e-08 - val_loss: 1.4505e-08
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5050e-08 - val_loss: 1.3846e-08
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4314e-08 - val_loss: 1.3206e-08
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3679e-08 - val_loss: 1.2586e-08
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2986e-08 - val_loss: 1.1895e-08
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2292e-08 - val_loss: 1.1406e-08
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1832e-08 - val_loss: 1.0899e-08
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1223e-08 - val_loss: 1.0291e-08
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0618e-08 - val_loss: 9.7901e-09
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0151e-08 - val_loss: 9.4288e-09
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7263e-09 - val_loss: 8.9581e-09
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.2194e-09 - val_loss: 8.4828e-09
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.7304e-09 - val_loss: 8.0707e-09
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3342e-09 - val_loss: 7.7769e-09
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0342e-09 - val_loss: 7.4433e-09
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.6404e-09 - val_loss: 7.0397e-09
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2113e-09 - val_loss: 6.6476e-09
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8516e-09 - val_loss: 6.4537e-09
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.6720e-09 - val_loss: 6.2114e-09
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.3634e-09 - val_loss: 5.8678e-09
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9910e-09 - val_loss: 5.5182e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6722e-09 - val_loss: 5.3372e-09
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5228e-09 - val_loss: 5.2017e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3416e-09 - val_loss: 4.9323e-09
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0236e-09 - val_loss: 4.5892e-09
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6826e-09 - val_loss: 4.3832e-09
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5221e-09 - val_loss: 4.2856e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4209e-09 - val_loss: 4.1778e-09
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2704e-09 - val_loss: 3.9570e-09
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0245e-09 - val_loss: 3.6851e-09
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7516e-09 - val_loss: 3.4982e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6076e-09 - val_loss: 3.4407e-09
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5451e-09 - val_loss: 3.3468e-09
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4167e-09 - val_loss: 3.1969e-09
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2676e-09 - val_loss: 3.0544e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0940e-09 - val_loss: 2.8510e-09
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9036e-09 - val_loss: 2.7449e-09
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8243e-09 - val_loss: 2.6985e-09
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7653e-09 - val_loss: 2.6093e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6559e-09 - val_loss: 2.4781e-09
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5157e-09 - val_loss: 2.3533e-09
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4056e-09 - val_loss: 2.2896e-09
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3391e-09 - val_loss: 2.2201e-09
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2576e-09 - val_loss: 2.1223e-09
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1593e-09 - val_loss: 2.0350e-09
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0655e-09 - val_loss: 1.9536e-09
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9941e-09 - val_loss: 1.8963e-09
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9241e-09 - val_loss: 1.8141e-09
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8472e-09 - val_loss: 1.7481e-09
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7667e-09 - val_loss: 1.6527e-09
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6828e-09 - val_loss: 1.6187e-09
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6551e-09 - val_loss: 1.5906e-09
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6185e-09 - val_loss: 1.5366e-09
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5548e-09 - val_loss: 1.4577e-09
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4752e-09 - val_loss: 1.4000e-09
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4243e-09 - val_loss: 1.3602e-09
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3834e-09 - val_loss: 1.3236e-09
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3460e-09 - val_loss: 1.2815e-09
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2894e-09 - val_loss: 1.2179e-09
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2295e-09 - val_loss: 1.1672e-09
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1836e-09 - val_loss: 1.1414e-09
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1698e-09 - val_loss: 1.1397e-09
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1623e-09 - val_loss: 1.1130e-09
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1202e-09 - val_loss: 1.0498e-09
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0555e-09 - val_loss: 1.0022e-09
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0159e-09 - val_loss: 9.7964e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9337e-10 - val_loss: 9.6059e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7747e-10 - val_loss: 9.3658e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.4987e-10 - val_loss: 9.0701e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.0785e-10 - val_loss: 8.4958e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.5413e-10 - val_loss: 8.1451e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2720e-10 - val_loss: 8.0308e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2245e-10 - val_loss: 8.1188e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2526e-10 - val_loss: 8.0042e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.1495e-10 - val_loss: 7.8534e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8897e-10 - val_loss: 7.4925e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5214e-10 - val_loss: 7.1011e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.1036e-10 - val_loss: 6.7523e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8468e-10 - val_loss: 6.6857e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8126e-10 - val_loss: 6.6447e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7222e-10 - val_loss: 6.5055e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5848e-10 - val_loss: 6.2351e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.2136e-10 - val_loss: 5.8690e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9030e-10 - val_loss: 5.6574e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7187e-10 - val_loss: 5.5925e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7040e-10 - val_loss: 5.5808e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6615e-10 - val_loss: 5.5087e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5469e-10 - val_loss: 5.3281e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3774e-10 - val_loss: 5.2539e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3492e-10 - val_loss: 5.2305e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3017e-10 - val_loss: 5.1580e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1732e-10 - val_loss: 4.9417e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9572e-10 - val_loss: 4.7643e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7995e-10 - val_loss: 4.6449e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6924e-10 - val_loss: 4.5541e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5944e-10 - val_loss: 4.3824e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3551e-10 - val_loss: 4.1397e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1436e-10 - val_loss: 3.9909e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0461e-10 - val_loss: 3.9928e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0731e-10 - val_loss: 4.0300e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0678e-10 - val_loss: 3.9323e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9674e-10 - val_loss: 3.8645e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9078e-10 - val_loss: 3.7991e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8309e-10 - val_loss: 3.7329e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7529e-10 - val_loss: 3.6319e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6694e-10 - val_loss: 3.6287e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6980e-10 - val_loss: 3.6478e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6770e-10 - val_loss: 3.5682e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5613e-10 - val_loss: 3.4199e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4417e-10 - val_loss: 3.3213e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3325e-10 - val_loss: 3.2610e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3061e-10 - val_loss: 3.2164e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2249e-10 - val_loss: 3.1042e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1126e-10 - val_loss: 3.0055e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9960e-10 - val_loss: 2.8528e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8645e-10 - val_loss: 2.7896e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8083e-10 - val_loss: 2.7863e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8375e-10 - val_loss: 2.8220e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8424e-10 - val_loss: 2.7551e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7665e-10 - val_loss: 2.6953e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7072e-10 - val_loss: 2.6527e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6741e-10 - val_loss: 2.6008e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6267e-10 - val_loss: 2.5931e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6111e-10 - val_loss: 2.5782e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6357e-10 - val_loss: 2.6259e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6558e-10 - val_loss: 2.5924e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6067e-10 - val_loss: 2.5330e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5359e-10 - val_loss: 2.4549e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4530e-10 - val_loss: 2.3690e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3208e-10 - val_loss: 2.1767e-10
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1801e-10 - val_loss: 2.1136e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1345e-10 - val_loss: 2.1197e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1417e-10 - val_loss: 2.1227e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1403e-10 - val_loss: 2.1241e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1417e-10 - val_loss: 2.1024e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1184e-10 - val_loss: 2.0937e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1085e-10 - val_loss: 2.0778e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0916e-10 - val_loss: 2.0503e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0590e-10 - val_loss: 1.9972e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9930e-10 - val_loss: 1.9303e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9504e-10 - val_loss: 1.9287e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9366e-10 - val_loss: 1.9158e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9148e-10 - val_loss: 1.8837e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9241e-10 - val_loss: 1.9579e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9893e-10 - val_loss: 1.9973e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0089e-10 - val_loss: 1.9605e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9442e-10 - val_loss: 1.8770e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8778e-10 - val_loss: 1.8297e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8025e-10 - val_loss: 1.7142e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6962e-10 - val_loss: 1.6276e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6261e-10 - val_loss: 1.5919e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6108e-10 - val_loss: 1.5975e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6065e-10 - val_loss: 1.5799e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5905e-10 - val_loss: 1.5792e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5909e-10 - val_loss: 1.6035e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6168e-10 - val_loss: 1.6116e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6364e-10 - val_loss: 1.6320e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6173e-10 - val_loss: 1.5418e-10
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5275e-10 - val_loss: 1.4820e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4838e-10 - val_loss: 1.4486e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4432e-10 - val_loss: 1.4122e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4125e-10 - val_loss: 1.3880e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3949e-10 - val_loss: 1.4008e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4421e-10 - val_loss: 1.5144e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5531e-10 - val_loss: 1.5969e-10
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6098e-10 - val_loss: 1.5981e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6042e-10 - val_loss: 1.5705e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5652e-10 - val_loss: 1.4581e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4134e-10 - val_loss: 1.3164e-10
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2909e-10 - val_loss: 1.2339e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2251e-10 - val_loss: 1.1977e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2014e-10 - val_loss: 1.1944e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2056e-10 - val_loss: 1.2217e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2441e-10 - val_loss: 1.2577e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2748e-10 - val_loss: 1.2985e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3030e-10 - val_loss: 1.2893e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2875e-10 - val_loss: 1.2527e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2440e-10 - val_loss: 1.2091e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2056e-10 - val_loss: 1.1885e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1891e-10 - val_loss: 1.1764e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1784e-10 - val_loss: 1.1609e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1680e-10 - val_loss: 1.1750e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1812e-10 - val_loss: 1.1741e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1801e-10 - val_loss: 1.1660e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1705e-10 - val_loss: 1.2108e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2305e-10 - val_loss: 1.2461e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2720e-10 - val_loss: 1.2761e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2858e-10 - val_loss: 1.2645e-10
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2322e-10 - val_loss: 1.1522e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1253e-10 - val_loss: 1.0578e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0470e-10 - val_loss: 1.0038e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9734e-11 - val_loss: 9.7193e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7825e-11 - val_loss: 9.7706e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8291e-11 - val_loss: 9.8623e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9633e-11 - val_loss: 9.9533e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0148e-10 - val_loss: 1.0275e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0345e-10 - val_loss: 1.0431e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0541e-10 - val_loss: 1.0526e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0511e-10 - val_loss: 1.0225e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0169e-10 - val_loss: 1.0049e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0236e-10 - val_loss: 1.0399e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0472e-10 - val_loss: 1.0292e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0184e-10 - val_loss: 9.9088e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.8153e-11 - val_loss: 9.4925e-11
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.4051e-11 - val_loss: 9.2020e-11
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1626e-11 - val_loss: 9.1387e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3878e-11 - val_loss: 9.7730e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0032e-10 - val_loss: 1.0244e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0429e-10 - val_loss: 1.0656e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0745e-10 - val_loss: 1.0446e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0274e-10 - val_loss: 9.6380e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.5053e-11 - val_loss: 9.0272e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.9515e-11 - val_loss: 8.7279e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.6484e-11 - val_loss: 8.3705e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3258e-11 - val_loss: 8.2894e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3001e-11 - val_loss: 8.3710e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4346e-11 - val_loss: 8.4178e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4750e-11 - val_loss: 8.5777e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5585e-11 - val_loss: 8.5193e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5048e-11 - val_loss: 8.3995e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4633e-11 - val_loss: 8.4065e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4373e-11 - val_loss: 8.3756e-11
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3345e-11 - val_loss: 8.2602e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3080e-11 - val_loss: 8.3618e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4149e-11 - val_loss: 8.4186e-11
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3521e-11 - val_loss: 8.1202e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.1155e-11 - val_loss: 7.9513e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9590e-11 - val_loss: 8.0602e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3133e-11 - val_loss: 8.7458e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9242e-11 - val_loss: 9.1420e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2666e-11 - val_loss: 9.4402e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1443e-11 - val_loss: 8.4875e-11
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2804e-11 - val_loss: 7.7920e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.7021e-11 - val_loss: 7.4852e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4382e-11 - val_loss: 7.1921e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2048e-11 - val_loss: 7.1703e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.2004e-11 - val_loss: 7.1335e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.1700e-11 - val_loss: 7.0843e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1461e-11 - val_loss: 7.2003e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2775e-11 - val_loss: 7.3682e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4242e-11 - val_loss: 7.4242e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5118e-11 - val_loss: 7.5399e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5337e-11 - val_loss: 7.4230e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4365e-11 - val_loss: 7.4066e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3672e-11 - val_loss: 7.2403e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2425e-11 - val_loss: 7.2170e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2327e-11 - val_loss: 7.2379e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3170e-11 - val_loss: 7.4161e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3800e-11 - val_loss: 7.3033e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2956e-11 - val_loss: 7.1565e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1762e-11 - val_loss: 7.3555e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5432e-11 - val_loss: 7.8076e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8999e-11 - val_loss: 8.0585e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0809e-11 - val_loss: 7.7641e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5646e-11 - val_loss: 7.2013e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0917e-11 - val_loss: 6.8094e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.6930e-11 - val_loss: 6.4486e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.3793e-11 - val_loss: 6.2488e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2582e-11 - val_loss: 6.2653e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3000e-11 - val_loss: 6.3120e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3325e-11 - val_loss: 6.3075e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3637e-11 - val_loss: 6.4634e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4575e-11 - val_loss: 6.5537e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5884e-11 - val_loss: 6.5247e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5545e-11 - val_loss: 6.5263e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4826e-11 - val_loss: 6.3310e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3255e-11 - val_loss: 6.3036e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.2750e-11 - val_loss: 6.1800e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1408e-11 - val_loss: 6.0364e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0854e-11 - val_loss: 6.1258e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1774e-11 - val_loss: 6.1652e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1442e-11 - val_loss: 6.1014e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0638e-11 - val_loss: 5.9675e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0576e-11 - val_loss: 6.3524e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5377e-11 - val_loss: 6.8784e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0390e-11 - val_loss: 7.2736e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1352e-11 - val_loss: 6.8457e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7613e-11 - val_loss: 6.5576e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4635e-11 - val_loss: 6.2555e-11
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1535e-11 - val_loss: 5.9611e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9136e-11 - val_loss: 5.8016e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7984e-11 - val_loss: 5.6893e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6412e-11 - val_loss: 5.5826e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5597e-11 - val_loss: 5.5519e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5989e-11 - val_loss: 5.7733e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7948e-11 - val_loss: 5.8794e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8626e-11 - val_loss: 5.7841e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8503e-11 - val_loss: 5.9493e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9299e-11 - val_loss: 5.8451e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8250e-11 - val_loss: 5.7340e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7474e-11 - val_loss: 5.7732e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7595e-11 - val_loss: 5.7764e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7907e-11 - val_loss: 5.7956e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7798e-11 - val_loss: 5.6876e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6247e-11 - val_loss: 5.5671e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6117e-11 - val_loss: 5.7050e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6502e-11 - val_loss: 5.6855e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8199e-11 - val_loss: 6.0852e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2912e-11 - val_loss: 6.5982e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6744e-11 - val_loss: 6.5473e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5057e-11 - val_loss: 6.3200e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2295e-11 - val_loss: 5.9621e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8306e-11 - val_loss: 5.5766e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5087e-11 - val_loss: 5.4010e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3520e-11 - val_loss: 5.2318e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2600e-11 - val_loss: 5.2017e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2024e-11 - val_loss: 5.2435e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2326e-11 - val_loss: 5.2016e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1697e-11 - val_loss: 5.0812e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0969e-11 - val_loss: 5.0611e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0968e-11 - val_loss: 5.1583e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1645e-11 - val_loss: 5.1598e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1728e-11 - val_loss: 5.2115e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2565e-11 - val_loss: 5.3703e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3733e-11 - val_loss: 5.3724e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3829e-11 - val_loss: 5.4065e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4559e-11 - val_loss: 5.3961e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3605e-11 - val_loss: 5.2542e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2516e-11 - val_loss: 5.2381e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1859e-11 - val_loss: 5.1069e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2236e-11 - val_loss: 5.5004e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6675e-11 - val_loss: 5.8340e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9133e-11 - val_loss: 5.8322e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7055e-11 - val_loss: 5.3993e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2951e-11 - val_loss: 5.1258e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1038e-11 - val_loss: 5.0642e-11
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0353e-11 - val_loss: 4.9606e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9261e-11 - val_loss: 4.9033e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9565e-11 - val_loss: 4.9425e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9017e-11 - val_loss: 4.8552e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8629e-11 - val_loss: 4.8935e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8740e-11 - val_loss: 4.8541e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8541e-11 - val_loss: 4.7866e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8143e-11 - val_loss: 4.8186e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7981e-11 - val_loss: 4.7400e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7898e-11 - val_loss: 4.8935e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9220e-11 - val_loss: 4.9136e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8917e-11 - val_loss: 4.7892e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7823e-11 - val_loss: 4.7783e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7786e-11 - val_loss: 4.7799e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8205e-11 - val_loss: 4.8543e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8550e-11 - val_loss: 4.8870e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8686e-11 - val_loss: 4.8509e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8635e-11 - val_loss: 4.8808e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9564e-11 - val_loss: 5.2493e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4166e-11 - val_loss: 5.6388e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6971e-11 - val_loss: 5.5983e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5674e-11 - val_loss: 5.4643e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3954e-11 - val_loss: 5.2837e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1989e-11 - val_loss: 4.9739e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9271e-11 - val_loss: 4.7983e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7796e-11 - val_loss: 4.6868e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6568e-11 - val_loss: 4.5780e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5301e-11 - val_loss: 4.4407e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4477e-11 - val_loss: 4.3872e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3383e-11 - val_loss: 4.3045e-11
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2711e-11 - val_loss: 4.2218e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2535e-11 - val_loss: 4.3051e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3331e-11 - val_loss: 4.3497e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3593e-11 - val_loss: 4.4088e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4434e-11 - val_loss: 4.5078e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5592e-11 - val_loss: 4.6092e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6128e-11 - val_loss: 4.6650e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6321e-11 - val_loss: 4.5254e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5108e-11 - val_loss: 4.4487e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4159e-11 - val_loss: 4.4038e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4012e-11 - val_loss: 4.3768e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3881e-11 - val_loss: 4.3462e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4214e-11 - val_loss: 4.6797e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8066e-11 - val_loss: 5.0695e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2169e-11 - val_loss: 5.1946e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1354e-11 - val_loss: 5.0276e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9639e-11 - val_loss: 4.7798e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 26324.7433 - val_loss: 0.5794
Epoch 2/512
448/448 - 0s - loss: 0.6028 - val_loss: 0.5765
Epoch 3/512
448/448 - 0s - loss: 0.5991 - val_loss: 0.5746
Epoch 4/512
448/448 - 0s - loss: 0.5957 - val_loss: 0.5721
Epoch 5/512
448/448 - 0s - loss: 0.5945 - val_loss: 0.5708
Epoch 6/512
448/448 - 0s - loss: 0.5911 - val_loss: 0.5694
Epoch 7/512
448/448 - 0s - loss: 0.5884 - val_loss: 0.5679
Epoch 8/512
448/448 - 0s - loss: 0.5850 - val_loss: 0.5661
Epoch 9/512
448/448 - 0s - loss: 0.5813 - val_loss: 0.5640
Epoch 10/512
448/448 - 0s - loss: 0.5780 - val_loss: 0.5612
Epoch 11/512
448/448 - 0s - loss: 0.5743 - val_loss: 0.5579
Epoch 12/512
448/448 - 0s - loss: 0.5703 - val_loss: 0.5539
Epoch 13/512
448/448 - 0s - loss: 0.5657 - val_loss: 0.5494
Epoch 14/512
448/448 - 0s - loss: 0.5594 - val_loss: 0.5436
Epoch 15/512
448/448 - 0s - loss: 0.5522 - val_loss: 0.5374
Epoch 16/512
448/448 - 0s - loss: 0.5426 - val_loss: 0.5291
Epoch 17/512
448/448 - 0s - loss: 0.5329 - val_loss: 0.5191
Epoch 18/512
448/448 - 0s - loss: 0.5217 - val_loss: 0.5071
Epoch 19/512
448/448 - 0s - loss: 0.5080 - val_loss: 0.4929
Epoch 20/512
448/448 - 0s - loss: 0.4948 - val_loss: 0.4759
Epoch 21/512
448/448 - 0s - loss: 0.4750 - val_loss: 0.4572
Epoch 22/512
448/448 - 0s - loss: 0.4549 - val_loss: 0.4365
Epoch 23/512
448/448 - 0s - loss: 0.4329 - val_loss: 0.4132
Epoch 24/512
448/448 - 0s - loss: 0.4085 - val_loss: 0.3881
Epoch 25/512
448/448 - 0s - loss: 0.3862 - val_loss: 0.3619
Epoch 26/512
448/448 - 0s - loss: 0.3568 - val_loss: 0.3345
Epoch 27/512
448/448 - 0s - loss: 0.3281 - val_loss: 0.3052
Epoch 28/512
448/448 - 0s - loss: 0.2991 - val_loss: 0.2763
Epoch 29/512
448/448 - 0s - loss: 0.2683 - val_loss: 0.2473
Epoch 30/512
448/448 - 0s - loss: 0.2379 - val_loss: 0.2191
Epoch 31/512
448/448 - 0s - loss: 0.2084 - val_loss: 0.1923
Epoch 32/512
448/448 - 0s - loss: 0.1801 - val_loss: 0.1675
Epoch 33/512
448/448 - 0s - loss: 0.1533 - val_loss: 0.1449
Epoch 34/512
448/448 - 0s - loss: 0.1296 - val_loss: 0.1249
Epoch 35/512
448/448 - 0s - loss: 0.1081 - val_loss: 0.1070
Epoch 36/512
448/448 - 0s - loss: 0.0902 - val_loss: 0.0913
Epoch 37/512
448/448 - 0s - loss: 0.0742 - val_loss: 0.0771
Epoch 38/512
448/448 - 0s - loss: 0.0609 - val_loss: 0.0651
Epoch 39/512
448/448 - 0s - loss: 0.0489 - val_loss: 0.0546
Epoch 40/512
448/448 - 0s - loss: 0.0380 - val_loss: 0.0439
Epoch 41/512
448/448 - 0s - loss: 0.0189 - val_loss: 0.0395
Epoch 42/512
448/448 - 0s - loss: 0.0378 - val_loss: 0.0299
Epoch 43/512
448/448 - 0s - loss: 0.0109 - val_loss: 0.0154
Epoch 44/512
448/448 - 0s - loss: 0.0334 - val_loss: 0.0224
Epoch 45/512
448/448 - 0s - loss: 0.0184 - val_loss: 0.0165
Epoch 46/512
448/448 - 0s - loss: 0.0119 - val_loss: 0.0143
Epoch 47/512
448/448 - 0s - loss: 0.0091 - val_loss: 0.0123
Epoch 48/512
448/448 - 0s - loss: 0.0077 - val_loss: 0.0101
Epoch 49/512
448/448 - 0s - loss: 0.0068 - val_loss: 0.0082
Epoch 50/512
448/448 - 0s - loss: 0.0060 - val_loss: 0.0065
Epoch 51/512
448/448 - 0s - loss: 0.0058 - val_loss: 0.0051
Epoch 52/512
448/448 - 0s - loss: 0.0057 - val_loss: 0.0042
Epoch 53/512
448/448 - 0s - loss: 0.0061 - val_loss: 0.0035
Epoch 54/512
448/448 - 0s - loss: 0.0065 - val_loss: 0.0031
Epoch 55/512
448/448 - 0s - loss: 0.0065 - val_loss: 0.0029
Epoch 56/512
448/448 - 0s - loss: 0.0066 - val_loss: 0.0028
Epoch 57/512
448/448 - 0s - loss: 0.0066 - val_loss: 0.0027
Epoch 58/512
448/448 - 0s - loss: 0.0068 - val_loss: 0.0026
Epoch 59/512
448/448 - 0s - loss: 0.0069 - val_loss: 0.0026
Epoch 60/512
448/448 - 0s - loss: 0.0071 - val_loss: 0.0026
Epoch 61/512
448/448 - 0s - loss: 0.0069 - val_loss: 0.0025
Epoch 62/512
448/448 - 0s - loss: 0.0074 - val_loss: 0.0026
Epoch 63/512
448/448 - 0s - loss: 0.0072 - val_loss: 0.0026
Epoch 64/512
448/448 - 0s - loss: 0.0073 - val_loss: 0.0027
Epoch 65/512
448/448 - 0s - loss: 0.0074 - val_loss: 0.0027
Epoch 66/512
448/448 - 0s - loss: 0.0073 - val_loss: 0.0022
Epoch 67/512
448/448 - 0s - loss: 0.0069 - val_loss: 0.0022
Epoch 68/512
448/448 - 0s - loss: 0.0065 - val_loss: 0.0023
Epoch 69/512
448/448 - 0s - loss: 0.0079 - val_loss: 0.0016
Epoch 70/512
448/448 - 0s - loss: 0.0076 - val_loss: 0.0021
Epoch 71/512
448/448 - 0s - loss: 0.0095 - val_loss: 0.0031
Epoch 72/512
448/448 - 0s - loss: 0.0080 - val_loss: 0.0028
Epoch 73/512
448/448 - 0s - loss: 0.0088 - val_loss: 0.0027
Epoch 74/512
448/448 - 0s - loss: 0.0093 - val_loss: 0.0031
Epoch 75/512
448/448 - 0s - loss: 0.0095 - val_loss: 0.0071
Epoch 76/512
448/448 - 0s - loss: 0.0087 - val_loss: 0.0017
Epoch 77/512
448/448 - 0s - loss: 0.0130 - val_loss: 0.0035
Epoch 78/512
448/448 - 0s - loss: 0.0136 - val_loss: 0.0014
Epoch 79/512
448/448 - 0s - loss: 0.0079 - val_loss: 0.0010
Epoch 80/512
448/448 - 0s - loss: 0.0088 - val_loss: 0.0014
Epoch 81/512
448/448 - 0s - loss: 0.0086 - val_loss: 0.0015
Epoch 82/512
448/448 - 0s - loss: 0.0085 - val_loss: 0.0012
Epoch 83/512
448/448 - 0s - loss: 0.0085 - val_loss: 0.0013
Epoch 84/512
448/448 - 0s - loss: 0.0073 - val_loss: 0.0015
Epoch 85/512
448/448 - 0s - loss: 0.0076 - val_loss: 0.0016
Epoch 86/512
448/448 - 0s - loss: 0.0076 - val_loss: 0.0016
Epoch 87/512
448/448 - 0s - loss: 0.0072 - val_loss: 0.0016
Epoch 88/512
448/448 - 0s - loss: 0.0069 - val_loss: 0.0016
Epoch 89/512
448/448 - 0s - loss: 0.0067 - val_loss: 0.0015
Epoch 90/512
448/448 - 0s - loss: 0.0070 - val_loss: 0.0015
Epoch 91/512
448/448 - 0s - loss: 0.0070 - val_loss: 0.0020
Epoch 92/512
448/448 - 0s - loss: 0.0060 - val_loss: 0.0018
Epoch 93/512
448/448 - 0s - loss: 0.0061 - val_loss: 0.0017
Epoch 94/512
448/448 - 0s - loss: 0.0062 - val_loss: 0.0018
Epoch 95/512
448/448 - 0s - loss: 0.0060 - val_loss: 0.0018
Epoch 96/512
448/448 - 0s - loss: 0.0057 - val_loss: 0.0015
Epoch 97/512
448/448 - 0s - loss: 0.0061 - val_loss: 0.0014
Epoch 98/512
448/448 - 0s - loss: 0.0056 - val_loss: 0.0016
Epoch 99/512
448/448 - 0s - loss: 0.0053 - val_loss: 0.0016
Epoch 100/512
448/448 - 0s - loss: 0.0054 - val_loss: 0.0016
Epoch 101/512
448/448 - 0s - loss: 0.0053 - val_loss: 0.0016
Epoch 102/512
448/448 - 0s - loss: 0.0052 - val_loss: 0.0014
Epoch 103/512
448/448 - 0s - loss: 0.0054 - val_loss: 0.0014
Epoch 104/512
448/448 - 0s - loss: 0.0049 - val_loss: 0.0014
Epoch 105/512
448/448 - 0s - loss: 0.0048 - val_loss: 0.0015
Epoch 106/512
448/448 - 0s - loss: 0.0049 - val_loss: 0.0015
Epoch 107/512
448/448 - 0s - loss: 0.0047 - val_loss: 0.0014
Epoch 108/512
448/448 - 0s - loss: 0.0048 - val_loss: 0.0013
Epoch 109/512
448/448 - 0s - loss: 0.0045 - val_loss: 0.0013
Epoch 110/512
448/448 - 0s - loss: 0.0043 - val_loss: 0.0014
Epoch 111/512
448/448 - 0s - loss: 0.0046 - val_loss: 0.0014
Epoch 112/512
448/448 - 0s - loss: 0.0043 - val_loss: 0.0012
Epoch 113/512
448/448 - 0s - loss: 0.0042 - val_loss: 0.0012
Epoch 114/512
448/448 - 0s - loss: 0.0041 - val_loss: 0.0012
Epoch 115/512
448/448 - 0s - loss: 0.0041 - val_loss: 0.0013
Epoch 116/512
448/448 - 0s - loss: 0.0041 - val_loss: 0.0012
Epoch 117/512
448/448 - 0s - loss: 0.0039 - val_loss: 0.0011
Epoch 118/512
448/448 - 0s - loss: 0.0038 - val_loss: 0.0011
Epoch 119/512
448/448 - 0s - loss: 0.0038 - val_loss: 0.0011
Epoch 120/512
448/448 - 0s - loss: 0.0038 - val_loss: 0.0011
Epoch 121/512
448/448 - 0s - loss: 0.0038 - val_loss: 0.0010
Epoch 122/512
448/448 - 0s - loss: 0.0036 - val_loss: 0.0010
Epoch 123/512
448/448 - 0s - loss: 0.0037 - val_loss: 0.0011
Epoch 124/512
448/448 - 0s - loss: 0.0036 - val_loss: 0.0011
Epoch 125/512
448/448 - 0s - loss: 0.0035 - val_loss: 9.7575e-04
Epoch 126/512
448/448 - 0s - loss: 0.0035 - val_loss: 9.5466e-04
Epoch 127/512
448/448 - 0s - loss: 0.0035 - val_loss: 0.0010
Epoch 128/512
448/448 - 0s - loss: 0.0035 - val_loss: 9.9761e-04
Epoch 129/512
448/448 - 0s - loss: 0.0033 - val_loss: 9.0941e-04
Epoch 130/512
448/448 - 0s - loss: 0.0034 - val_loss: 9.0610e-04
Epoch 131/512
448/448 - 0s - loss: 0.0034 - val_loss: 9.6991e-04
Epoch 132/512
448/448 - 0s - loss: 0.0033 - val_loss: 9.1841e-04
Epoch 133/512
448/448 - 0s - loss: 0.0032 - val_loss: 8.6247e-04
Epoch 134/512
448/448 - 0s - loss: 0.0033 - val_loss: 8.8958e-04
Epoch 135/512
448/448 - 0s - loss: 0.0032 - val_loss: 9.3156e-04
Epoch 136/512
448/448 - 0s - loss: 0.0032 - val_loss: 8.3765e-04
Epoch 137/512
448/448 - 0s - loss: 0.0032 - val_loss: 8.2531e-04
Epoch 138/512
448/448 - 0s - loss: 0.0031 - val_loss: 8.7109e-04
Epoch 139/512
448/448 - 0s - loss: 0.0031 - val_loss: 8.3410e-04
Epoch 140/512
448/448 - 0s - loss: 0.0030 - val_loss: 7.8276e-04
Epoch 141/512
448/448 - 0s - loss: 0.0031 - val_loss: 8.0692e-04
Epoch 142/512
448/448 - 0s - loss: 0.0030 - val_loss: 8.3493e-04
Epoch 143/512
448/448 - 0s - loss: 0.0030 - val_loss: 7.5305e-04
Epoch 144/512
448/448 - 0s - loss: 0.0029 - val_loss: 7.5567e-04
Epoch 145/512
448/448 - 0s - loss: 0.0029 - val_loss: 7.9306e-04
Epoch 146/512
448/448 - 0s - loss: 0.0030 - val_loss: 7.3194e-04
Epoch 147/512
448/448 - 0s - loss: 0.0030 - val_loss: 7.1178e-04
Epoch 148/512
448/448 - 0s - loss: 0.0029 - val_loss: 7.5461e-04
Epoch 149/512
448/448 - 0s - loss: 0.0028 - val_loss: 7.0596e-04
Epoch 150/512
448/448 - 0s - loss: 0.0029 - val_loss: 6.7959e-04
Epoch 151/512
448/448 - 0s - loss: 0.0029 - val_loss: 7.4014e-04
Epoch 152/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.8272e-04
Epoch 153/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.6609e-04
Epoch 154/512
448/448 - 0s - loss: 0.0028 - val_loss: 7.0278e-04
Epoch 155/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.5917e-04
Epoch 156/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.3351e-04
Epoch 157/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.7923e-04
Epoch 158/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.3335e-04
Epoch 159/512
448/448 - 0s - loss: 0.0028 - val_loss: 6.1905e-04
Epoch 160/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.6595e-04
Epoch 161/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.0624e-04
Epoch 162/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.0819e-04
Epoch 163/512
448/448 - 0s - loss: 0.0026 - val_loss: 6.3966e-04
Epoch 164/512
448/448 - 0s - loss: 0.0027 - val_loss: 5.8449e-04
Epoch 165/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.0288e-04
Epoch 166/512
448/448 - 0s - loss: 0.0026 - val_loss: 6.2411e-04
Epoch 167/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.7377e-04
Epoch 168/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.9439e-04
Epoch 169/512
448/448 - 0s - loss: 0.0027 - val_loss: 6.0399e-04
Epoch 170/512
448/448 - 0s - loss: 0.0027 - val_loss: 5.5083e-04
Epoch 171/512
448/448 - 0s - loss: 0.0027 - val_loss: 5.9368e-04
Epoch 172/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.6975e-04
Epoch 173/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.5258e-04
Epoch 174/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.9941e-04
Epoch 175/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.3865e-04
Epoch 176/512
448/448 - 0s - loss: 0.0027 - val_loss: 5.5437e-04
Epoch 177/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.7867e-04
Epoch 178/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.2990e-04
Epoch 179/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.6410e-04
Epoch 180/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.4691e-04
Epoch 181/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.2727e-04
Epoch 182/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.7220e-04
Epoch 183/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.1865e-04
Epoch 184/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3567e-04
Epoch 185/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.4289e-04
Epoch 186/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.1145e-04
Epoch 187/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.6460e-04
Epoch 188/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.0914e-04
Epoch 189/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.2411e-04
Epoch 190/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.4548e-04
Epoch 191/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.0325e-04
Epoch 192/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.5100e-04
Epoch 193/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.0928e-04
Epoch 194/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.1190e-04
Epoch 195/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.5042e-04
Epoch 196/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8826e-04
Epoch 197/512
448/448 - 0s - loss: 0.0026 - val_loss: 5.4451e-04
Epoch 198/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.9929e-04
Epoch 199/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.1765e-04
Epoch 200/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3211e-04
Epoch 201/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.9049e-04
Epoch 202/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.4350e-04
Epoch 203/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8804e-04
Epoch 204/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.2727e-04
Epoch 205/512
448/448 - 0s - loss: 0.0026 - val_loss: 4.9953e-04
Epoch 206/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.0102e-04
Epoch 207/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.2108e-04
Epoch 208/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8736e-04
Epoch 209/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.4390e-04
Epoch 210/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.7846e-04
Epoch 211/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3938e-04
Epoch 212/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8418e-04
Epoch 213/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.1684e-04
Epoch 214/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.0829e-04
Epoch 215/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.9695e-04
Epoch 216/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3024e-04
Epoch 217/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8143e-04
Epoch 218/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.4663e-04
Epoch 219/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7770e-04
Epoch 220/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3647e-04
Epoch 221/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.7772e-04
Epoch 222/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.4593e-04
Epoch 223/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.7553e-04
Epoch 224/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3406e-04
Epoch 225/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.9168e-04
Epoch 226/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.2684e-04
Epoch 227/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8504e-04
Epoch 228/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.2409e-04
Epoch 229/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8838e-04
Epoch 230/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.2437e-04
Epoch 231/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8336e-04
Epoch 232/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.3328e-04
Epoch 233/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8510e-04
Epoch 234/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.3050e-04
Epoch 235/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8433e-04
Epoch 236/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.4683e-04
Epoch 237/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7829e-04
Epoch 238/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.6512e-04
Epoch 239/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7790e-04
Epoch 240/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.5236e-04
Epoch 241/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8076e-04
Epoch 242/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.5075e-04
Epoch 243/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8421e-04
Epoch 244/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.2160e-04
Epoch 245/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.1353e-04
Epoch 246/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8752e-04
Epoch 247/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.5232e-04
Epoch 248/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.7724e-04
Epoch 249/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.6770e-04
Epoch 250/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8560e-04
Epoch 251/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.3216e-04
Epoch 252/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.0742e-04
Epoch 253/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.8673e-04
Epoch 254/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.5954e-04
Epoch 255/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.8024e-04
Epoch 256/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.5084e-04
Epoch 257/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.0518e-04
Epoch 258/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.8532e-04
Epoch 259/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.8129e-04
Epoch 260/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7746e-04
Epoch 261/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.2020e-04
Epoch 262/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.5527e-04
Epoch 263/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.7078e-04
Epoch 264/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.5321e-04
Epoch 265/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.2060e-04
Epoch 266/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7412e-04
Epoch 267/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.7722e-04
Epoch 268/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.9979e-04
Epoch 269/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7358e-04
Epoch 270/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.7235e-04
Epoch 271/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.0583e-04
Epoch 272/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.6785e-04
Epoch 273/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.7104e-04
Epoch 274/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.0877e-04
Epoch 275/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.6226e-04
Epoch 276/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.3855e-04
Epoch 277/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.4517e-04
Epoch 278/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.5491e-04
Epoch 279/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.1822e-04
Epoch 280/512
448/448 - 0s - loss: 0.0021 - val_loss: 5.7085e-04
Epoch 281/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.6751e-04
Epoch 282/512
448/448 - 0s - loss: 0.0025 - val_loss: 4.8001e-04
Epoch 283/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.6467e-04
Epoch 284/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.1947e-04
Epoch 285/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.5050e-04
Epoch 286/512
448/448 - 0s - loss: 0.0024 - val_loss: 5.0989e-04
Epoch 287/512
448/448 - 0s - loss: 0.0021 - val_loss: 5.3892e-04
Epoch 288/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.0440e-04
Epoch 289/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.5916e-04
Epoch 290/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.3652e-04
Epoch 291/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.3671e-04
Epoch 292/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.5311e-04
Epoch 293/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.7747e-04
Epoch 294/512
448/448 - 0s - loss: 0.0021 - val_loss: 5.2316e-04
Epoch 295/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.3194e-04
Epoch 296/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.5750e-04
Epoch 297/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.7808e-04
Epoch 298/512
448/448 - 0s - loss: 0.0021 - val_loss: 5.1588e-04
Epoch 299/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.8870e-04
Epoch 300/512
448/448 - 0s - loss: 0.0024 - val_loss: 4.6166e-04
Epoch 301/512
448/448 - 0s - loss: 0.0021 - val_loss: 5.0867e-04
Epoch 302/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.2642e-04
Epoch 303/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.5609e-04
Epoch 304/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.5914e-04
Epoch 305/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.8946e-04
Epoch 306/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.6164e-04
Epoch 307/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.5932e-04
Epoch 308/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.9207e-04
Epoch 309/512
448/448 - 0s - loss: 0.0023 - val_loss: 5.2008e-04
Epoch 310/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.6012e-04
Epoch 311/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.5353e-04
Epoch 312/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.9181e-04
Epoch 313/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.5024e-04
Epoch 314/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.4687e-04
Epoch 315/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.8248e-04
Epoch 316/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.6045e-04
Epoch 317/512
448/448 - 0s - loss: 0.0023 - val_loss: 4.5470e-04
Epoch 318/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.8280e-04
Epoch 319/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.9236e-04
Epoch 320/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.6936e-04
Epoch 321/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.4866e-04
Epoch 322/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.8978e-04
Epoch 323/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.4238e-04
Epoch 324/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.3506e-04
Epoch 325/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.8305e-04
Epoch 326/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.4341e-04
Epoch 327/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.4107e-04
Epoch 328/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.6128e-04
Epoch 329/512
448/448 - 0s - loss: 0.0022 - val_loss: 4.5972e-04
Epoch 330/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.4465e-04
Epoch 331/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.3208e-04
Epoch 332/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.8829e-04
Epoch 333/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.2511e-04
Epoch 334/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1822e-04
Epoch 335/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.7114e-04
Epoch 336/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.2857e-04
Epoch 337/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.2481e-04
Epoch 338/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.3114e-04
Epoch 339/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.6651e-04
Epoch 340/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.1253e-04
Epoch 341/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.0102e-04
Epoch 342/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.8224e-04
Epoch 343/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.9976e-04
Epoch 344/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.0113e-04
Epoch 345/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.3548e-04
Epoch 346/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.3500e-04
Epoch 347/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.9600e-04
Epoch 348/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.8326e-04
Epoch 349/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.6759e-04
Epoch 350/512
448/448 - 0s - loss: 0.0021 - val_loss: 3.7520e-04
Epoch 351/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.8418e-04
Epoch 352/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.2387e-04
Epoch 353/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.3482e-04
Epoch 354/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.7758e-04
Epoch 355/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.6157e-04
Epoch 356/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.5671e-04
Epoch 357/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.5842e-04
Epoch 358/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.6536e-04
Epoch 359/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.0241e-04
Epoch 360/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.4302e-04
Epoch 361/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.5153e-04
Epoch 362/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.3684e-04
Epoch 363/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.4482e-04
Epoch 364/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.6679e-04
Epoch 365/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.5340e-04
Epoch 366/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.4576e-04
Epoch 367/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.5741e-04
Epoch 368/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.2417e-04
Epoch 369/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.3404e-04
Epoch 370/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.8236e-04
Epoch 371/512
448/448 - 0s - loss: 0.0020 - val_loss: 4.0519e-04
Epoch 372/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.2293e-04
Epoch 373/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.1697e-04
Epoch 374/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.5026e-04
Epoch 375/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.6901e-04
Epoch 376/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.3271e-04
Epoch 377/512
448/448 - 0s - loss: 0.0020 - val_loss: 3.1605e-04
Epoch 378/512
448/448 - 0s - loss: 0.0018 - val_loss: 4.3436e-04
Epoch 379/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.2078e-04
Epoch 380/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.2690e-04
Epoch 381/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.3892e-04
Epoch 382/512
448/448 - 0s - loss: 0.0018 - val_loss: 4.7444e-04
Epoch 383/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.9050e-04
Epoch 384/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.9638e-04
Epoch 385/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.5370e-04
Epoch 386/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.8667e-04
Epoch 387/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.8615e-04
Epoch 388/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.9908e-04
Epoch 389/512
448/448 - 0s - loss: 0.0017 - val_loss: 4.0312e-04
Epoch 390/512
448/448 - 0s - loss: 0.0019 - val_loss: 4.2189e-04
Epoch 391/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.8346e-04
Epoch 392/512
448/448 - 0s - loss: 0.0020 - val_loss: 2.7469e-04
Epoch 393/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.7206e-04
Epoch 394/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.7058e-04
Epoch 395/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.8955e-04
Epoch 396/512
448/448 - 0s - loss: 0.0020 - val_loss: 2.8167e-04
Epoch 397/512
448/448 - 0s - loss: 0.0017 - val_loss: 4.3942e-04
Epoch 398/512
448/448 - 0s - loss: 0.0019 - val_loss: 3.6462e-04
Epoch 399/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9632e-04
Epoch 400/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.7044e-04
Epoch 401/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.8146e-04
Epoch 402/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.3851e-04
Epoch 403/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.9899e-04
Epoch 404/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.7638e-04
Epoch 405/512
448/448 - 0s - loss: 0.0017 - val_loss: 4.7390e-04
Epoch 406/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.2463e-04
Epoch 407/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9023e-04
Epoch 408/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.7006e-04
Epoch 409/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.8214e-04
Epoch 410/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.3075e-04
Epoch 411/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9838e-04
Epoch 412/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.6584e-04
Epoch 413/512
448/448 - 0s - loss: 0.0017 - val_loss: 4.6609e-04
Epoch 414/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.1296e-04
Epoch 415/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.7919e-04
Epoch 416/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.6145e-04
Epoch 417/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.8780e-04
Epoch 418/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.5103e-04
Epoch 419/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.7624e-04
Epoch 420/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.5052e-04
Epoch 421/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.9858e-04
Epoch 422/512
448/448 - 0s - loss: 0.0018 - val_loss: 3.4348e-04
Epoch 423/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.6137e-04
Epoch 424/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.4542e-04
Epoch 425/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.9160e-04
Epoch 426/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.5117e-04
Epoch 427/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.5585e-04
Epoch 428/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.3873e-04
Epoch 429/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.6089e-04
Epoch 430/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.6949e-04
Epoch 431/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.3492e-04
Epoch 432/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.3691e-04
Epoch 433/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.4972e-04
Epoch 434/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.7618e-04
Epoch 435/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2753e-04
Epoch 436/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.3200e-04
Epoch 437/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.3166e-04
Epoch 438/512
448/448 - 0s - loss: 0.0017 - val_loss: 4.0674e-04
Epoch 439/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.1974e-04
Epoch 440/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.2442e-04
Epoch 441/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9934e-04
Epoch 442/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.9066e-04
Epoch 443/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.3045e-04
Epoch 444/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.4106e-04
Epoch 445/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9445e-04
Epoch 446/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.9995e-04
Epoch 447/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2113e-04
Epoch 448/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.2605e-04
Epoch 449/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.7998e-04
Epoch 450/512
448/448 - 0s - loss: 0.0016 - val_loss: 4.4037e-04
Epoch 451/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2745e-04
Epoch 452/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.2392e-04
Epoch 453/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.4990e-04
Epoch 454/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.6888e-04
Epoch 455/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.8676e-04
Epoch 456/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.7259e-04
Epoch 457/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.2595e-04
Epoch 458/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.9615e-04
Epoch 459/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.9275e-04
Epoch 460/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.2724e-04
Epoch 461/512
448/448 - 0s - loss: 0.0019 - val_loss: 2.1646e-04
Epoch 462/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.5337e-04
Epoch 463/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.6032e-04
Epoch 464/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.0108e-04
Epoch 465/512
448/448 - 0s - loss: 0.0018 - val_loss: 1.9795e-04
Epoch 466/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.8224e-04
Epoch 467/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.4679e-04
Epoch 468/512
448/448 - 0s - loss: 0.0015 - val_loss: 2.3879e-04
Epoch 469/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.3832e-04
Epoch 470/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.4807e-04
Epoch 471/512
448/448 - 0s - loss: 0.0015 - val_loss: 4.1526e-04
Epoch 472/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.3719e-04
Epoch 473/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2057e-04
Epoch 474/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.1811e-04
Epoch 475/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.3413e-04
Epoch 476/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.2894e-04
Epoch 477/512
448/448 - 0s - loss: 0.0016 - val_loss: 1.9638e-04
Epoch 478/512
448/448 - 0s - loss: 0.0019 - val_loss: 1.9094e-04
Epoch 479/512
448/448 - 0s - loss: 0.0015 - val_loss: 2.8733e-04
Epoch 480/512
448/448 - 0s - loss: 0.0017 - val_loss: 3.5169e-04
Epoch 481/512
448/448 - 0s - loss: 0.0016 - val_loss: 1.8656e-04
Epoch 482/512
448/448 - 0s - loss: 0.0018 - val_loss: 1.9185e-04
Epoch 483/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.6557e-04
Epoch 484/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.5794e-04
Epoch 485/512
448/448 - 0s - loss: 0.0016 - val_loss: 1.9803e-04
Epoch 486/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.0219e-04
Epoch 487/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.3654e-04
Epoch 488/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.8211e-04
Epoch 489/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.3596e-04
Epoch 490/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.1079e-04
Epoch 491/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.0225e-04
Epoch 492/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.1942e-04
Epoch 493/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.9439e-04
Epoch 494/512
448/448 - 0s - loss: 0.0015 - val_loss: 1.8073e-04
Epoch 495/512
448/448 - 0s - loss: 0.0019 - val_loss: 1.7742e-04
Epoch 496/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.8382e-04
Epoch 497/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.3081e-04
Epoch 498/512
448/448 - 0s - loss: 0.0016 - val_loss: 1.7165e-04
Epoch 499/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.7253e-04
Epoch 500/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.4020e-04
Epoch 501/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.4524e-04
Epoch 502/512
448/448 - 0s - loss: 0.0015 - val_loss: 2.2363e-04
Epoch 503/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.0796e-04
Epoch 504/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.9382e-04
Epoch 505/512
448/448 - 0s - loss: 0.0015 - val_loss: 3.0224e-04
Epoch 506/512
448/448 - 0s - loss: 0.0016 - val_loss: 2.7672e-04
Epoch 507/512
448/448 - 0s - loss: 0.0015 - val_loss: 1.7118e-04
Epoch 508/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.6623e-04
Epoch 509/512
448/448 - 0s - loss: 0.0015 - val_loss: 2.7736e-04
Epoch 510/512
448/448 - 0s - loss: 0.0016 - val_loss: 3.2553e-04
Epoch 511/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.7591e-04
Epoch 512/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.5955e-04
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00055, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 0.0047 - val_loss: 5.5185e-04
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00055
448/448 - 0s - loss: 4.5408e-04 - val_loss: 6.4100e-04
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.4676e-04 - val_loss: 0.0022
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0018 - val_loss: 0.0014
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0014
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0013 - val_loss: 0.0014
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0013
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0013
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.9636e-04 - val_loss: 0.0011
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.5349e-04 - val_loss: 0.0011
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00055
448/448 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.4137e-04 - val_loss: 0.0010
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.9550e-04 - val_loss: 0.0011
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.4058e-04 - val_loss: 0.0011
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.3664e-04 - val_loss: 9.9744e-04
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.8554e-04 - val_loss: 9.7102e-04
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.6106e-04 - val_loss: 0.0010
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00055
448/448 - 0s - loss: 9.0449e-04 - val_loss: 9.9528e-04
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.6565e-04 - val_loss: 9.3008e-04
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.3570e-04 - val_loss: 9.2899e-04
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.1732e-04 - val_loss: 9.8104e-04
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.8492e-04 - val_loss: 9.3978e-04
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.0970e-04 - val_loss: 9.0056e-04
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.6747e-04 - val_loss: 8.9946e-04
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.3026e-04 - val_loss: 9.3801e-04
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.0712e-04 - val_loss: 8.6704e-04
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.5815e-04 - val_loss: 8.0711e-04
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.6003e-04 - val_loss: 9.2290e-04
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.0596e-04 - val_loss: 8.2692e-04
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.3347e-04 - val_loss: 7.8491e-04
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.0502e-04 - val_loss: 8.9600e-04
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00055
448/448 - 0s - loss: 8.1674e-04 - val_loss: 8.3742e-04
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.1782e-04 - val_loss: 7.6034e-04
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.6266e-04 - val_loss: 8.2401e-04
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.7952e-04 - val_loss: 8.2709e-04
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.1119e-04 - val_loss: 8.0426e-04
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4924e-04 - val_loss: 7.6257e-04
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.3943e-04 - val_loss: 8.3146e-04
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.0851e-04 - val_loss: 7.9601e-04
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.2686e-04 - val_loss: 7.0509e-04
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.1597e-04 - val_loss: 8.3858e-04
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00055
448/448 - 0s - loss: 7.0762e-04 - val_loss: 7.5902e-04
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.0673e-04 - val_loss: 6.7561e-04
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.8574e-04 - val_loss: 8.2193e-04
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.9758e-04 - val_loss: 7.7293e-04
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.0479e-04 - val_loss: 6.4125e-04
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.5460e-04 - val_loss: 8.0275e-04
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.8598e-04 - val_loss: 7.6172e-04
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.8838e-04 - val_loss: 6.2419e-04
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.5098e-04 - val_loss: 7.9488e-04
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.7602e-04 - val_loss: 7.4185e-04
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.7014e-04 - val_loss: 6.1097e-04
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4053e-04 - val_loss: 7.6534e-04
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.5383e-04 - val_loss: 7.2958e-04
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.5548e-04 - val_loss: 6.0372e-04
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4588e-04 - val_loss: 7.5770e-04
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4955e-04 - val_loss: 6.7616e-04
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.2849e-04 - val_loss: 6.1658e-04
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4145e-04 - val_loss: 7.2175e-04
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.2905e-04 - val_loss: 6.5506e-04
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.1957e-04 - val_loss: 6.4341e-04
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.5353e-04 - val_loss: 6.8270e-04
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.0349e-04 - val_loss: 6.2036e-04
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.0884e-04 - val_loss: 6.5090e-04
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4998e-04 - val_loss: 6.8402e-04
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00055
448/448 - 0s - loss: 5.9550e-04 - val_loss: 5.5284e-04
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00055
448/448 - 0s - loss: 4.9474e-04 - val_loss: 6.7441e-04
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00055
448/448 - 0s - loss: 6.4921e-04 - val_loss: 6.4330e-04
Epoch 76/512

Epoch 00076: val_loss improved from 0.00055 to 0.00054, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7937e-04 - val_loss: 5.3685e-04
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00054
448/448 - 0s - loss: 4.8892e-04 - val_loss: 6.8670e-04
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00054
448/448 - 0s - loss: 6.4561e-04 - val_loss: 6.4775e-04
Epoch 79/512

Epoch 00079: val_loss improved from 0.00054 to 0.00050, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7788e-04 - val_loss: 5.0040e-04
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00050
448/448 - 0s - loss: 4.8734e-04 - val_loss: 6.7923e-04
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00050
448/448 - 0s - loss: 6.2220e-04 - val_loss: 6.5586e-04
Epoch 82/512

Epoch 00082: val_loss improved from 0.00050 to 0.00048, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.5695e-04 - val_loss: 4.8458e-04
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.0433e-04 - val_loss: 6.9833e-04
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00048
448/448 - 0s - loss: 6.1600e-04 - val_loss: 6.6191e-04
Epoch 85/512

Epoch 00085: val_loss improved from 0.00048 to 0.00048, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.2811e-04 - val_loss: 4.7667e-04
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.1623e-04 - val_loss: 6.7633e-04
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00048
448/448 - 0s - loss: 6.1072e-04 - val_loss: 7.1059e-04
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.8895e-04 - val_loss: 5.4517e-04
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.6381e-04 - val_loss: 6.4452e-04
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.7591e-04 - val_loss: 5.9473e-04
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.6336e-04 - val_loss: 5.9888e-04
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00048
448/448 - 0s - loss: 6.3335e-04 - val_loss: 6.4641e-04
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.4207e-04 - val_loss: 5.6010e-04
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.5044e-04 - val_loss: 5.9393e-04
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00048
448/448 - 0s - loss: 6.1144e-04 - val_loss: 6.5714e-04
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.4894e-04 - val_loss: 5.1956e-04
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.4559e-04 - val_loss: 5.8118e-04
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.9137e-04 - val_loss: 6.1661e-04
Epoch 99/512

Epoch 00099: val_loss improved from 0.00048 to 0.00048, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.6320e-04 - val_loss: 4.7602e-04
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.5069e-04 - val_loss: 6.1237e-04
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.9012e-04 - val_loss: 5.7238e-04
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.9134e-04 - val_loss: 6.7530e-04
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.4338e-04 - val_loss: 5.2136e-04
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.2352e-04 - val_loss: 6.1412e-04
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.4904e-04 - val_loss: 5.5294e-04
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00048
448/448 - 0s - loss: 4.6322e-04 - val_loss: 5.3761e-04
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00048
448/448 - 0s - loss: 5.3414e-04 - val_loss: 6.0937e-04
Epoch 108/512

Epoch 00108: val_loss improved from 0.00048 to 0.00044, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.7681e-04 - val_loss: 4.4026e-04
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.3761e-04 - val_loss: 6.1904e-04
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.7366e-04 - val_loss: 5.5876e-04
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.9853e-04 - val_loss: 6.1118e-04
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6816e-04 - val_loss: 5.5885e-04
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.5762e-04 - val_loss: 5.9149e-04
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.3164e-04 - val_loss: 5.1022e-04
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6309e-04 - val_loss: 5.9896e-04
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.5622e-04 - val_loss: 5.9357e-04
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.3146e-04 - val_loss: 4.9063e-04
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6280e-04 - val_loss: 5.8672e-04
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.3401e-04 - val_loss: 6.0737e-04
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.2412e-04 - val_loss: 5.4912e-04
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.0255e-04 - val_loss: 4.7013e-04
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.5486e-04 - val_loss: 6.5813e-04
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00044
448/448 - 0s - loss: 6.0897e-04 - val_loss: 5.6842e-04
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.5301e-04 - val_loss: 4.5673e-04
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.5535e-04 - val_loss: 5.9336e-04
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.1062e-04 - val_loss: 6.7386e-04
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.7398e-04 - val_loss: 5.8605e-04
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6810e-04 - val_loss: 4.4032e-04
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6045e-04 - val_loss: 6.5477e-04
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.6788e-04 - val_loss: 5.7779e-04
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.7091e-04 - val_loss: 4.7041e-04
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6961e-04 - val_loss: 6.7608e-04
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.7254e-04 - val_loss: 5.5074e-04
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6858e-04 - val_loss: 5.3936e-04
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.5889e-04 - val_loss: 5.4058e-04
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.0998e-04 - val_loss: 6.5396e-04
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.6809e-04 - val_loss: 5.3791e-04
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.3865e-04 - val_loss: 5.2895e-04
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.7919e-04 - val_loss: 5.8232e-04
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.4035e-04 - val_loss: 5.4444e-04
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.8908e-04 - val_loss: 5.0413e-04
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.6172e-04 - val_loss: 5.6166e-04
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.1400e-04 - val_loss: 5.8616e-04
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.2459e-04 - val_loss: 5.8828e-04
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.4022e-04 - val_loss: 5.2403e-04
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.0399e-04 - val_loss: 5.9490e-04
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00044
448/448 - 0s - loss: 5.3634e-04 - val_loss: 5.1539e-04
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.3695e-04 - val_loss: 5.0010e-04
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00044
448/448 - 0s - loss: 4.9129e-04 - val_loss: 6.1320e-04
Epoch 150/512

Epoch 00150: val_loss improved from 0.00044 to 0.00043, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.2755e-04 - val_loss: 4.3262e-04
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.4071e-04 - val_loss: 5.6805e-04
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.3805e-04 - val_loss: 5.6578e-04
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.6228e-04 - val_loss: 4.8837e-04
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.7331e-04 - val_loss: 5.5668e-04
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.9974e-04 - val_loss: 6.2106e-04
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.1303e-04 - val_loss: 5.5126e-04
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.5014e-04 - val_loss: 5.4924e-04
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.3283e-04 - val_loss: 5.4428e-04
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.7341e-04 - val_loss: 4.6002e-04
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.6390e-04 - val_loss: 5.1102e-04
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.8189e-04 - val_loss: 6.0326e-04
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.1352e-04 - val_loss: 4.8103e-04
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.4481e-04 - val_loss: 4.8298e-04
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.2474e-04 - val_loss: 4.7361e-04
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.5641e-04 - val_loss: 5.5477e-04
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00043
448/448 - 0s - loss: 5.0499e-04 - val_loss: 5.4444e-04
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.5407e-04 - val_loss: 5.1026e-04
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.7598e-04 - val_loss: 4.9544e-04
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00043
448/448 - 0s - loss: 4.6922e-04 - val_loss: 5.6056e-04
Epoch 170/512

Epoch 00170: val_loss improved from 0.00043 to 0.00042, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 5.1755e-04 - val_loss: 4.2172e-04
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2361e-04 - val_loss: 6.4140e-04
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.6665e-04 - val_loss: 5.3432e-04
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4947e-04 - val_loss: 4.8428e-04
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3091e-04 - val_loss: 5.2496e-04
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.1961e-04 - val_loss: 5.5814e-04
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7786e-04 - val_loss: 5.3682e-04
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7599e-04 - val_loss: 4.8344e-04
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6329e-04 - val_loss: 5.6305e-04
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8799e-04 - val_loss: 5.3129e-04
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6490e-04 - val_loss: 4.9546e-04
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5693e-04 - val_loss: 5.0999e-04
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8822e-04 - val_loss: 6.1305e-04
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.3144e-04 - val_loss: 5.2159e-04
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5232e-04 - val_loss: 4.7041e-04
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4982e-04 - val_loss: 5.4152e-04
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8651e-04 - val_loss: 5.4015e-04
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7526e-04 - val_loss: 5.1636e-04
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8419e-04 - val_loss: 4.5343e-04
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2675e-04 - val_loss: 5.3925e-04
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.2023e-04 - val_loss: 6.0747e-04
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4133e-04 - val_loss: 4.7544e-04
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7959e-04 - val_loss: 5.2964e-04
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8137e-04 - val_loss: 6.3485e-04
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5458e-04 - val_loss: 4.7522e-04
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8080e-04 - val_loss: 5.8863e-04
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9664e-04 - val_loss: 4.9086e-04
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3255e-04 - val_loss: 4.9739e-04
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7035e-04 - val_loss: 5.3100e-04
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8936e-04 - val_loss: 5.4744e-04
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3691e-04 - val_loss: 4.9901e-04
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9095e-04 - val_loss: 5.9093e-04
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.0872e-04 - val_loss: 5.2684e-04
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2444e-04 - val_loss: 4.5045e-04
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6478e-04 - val_loss: 5.7909e-04
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9053e-04 - val_loss: 4.9963e-04
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5621e-04 - val_loss: 4.9688e-04
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7169e-04 - val_loss: 5.0537e-04
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4866e-04 - val_loss: 5.0114e-04
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9575e-04 - val_loss: 5.2269e-04
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5660e-04 - val_loss: 5.7969e-04
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7034e-04 - val_loss: 4.8896e-04
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5968e-04 - val_loss: 5.3245e-04
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9147e-04 - val_loss: 5.4741e-04
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6294e-04 - val_loss: 4.4740e-04
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4905e-04 - val_loss: 5.3062e-04
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9394e-04 - val_loss: 5.1370e-04
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4799e-04 - val_loss: 4.5030e-04
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4560e-04 - val_loss: 5.4412e-04
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9425e-04 - val_loss: 5.2748e-04
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4386e-04 - val_loss: 4.4320e-04
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4790e-04 - val_loss: 5.3650e-04
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9894e-04 - val_loss: 5.7363e-04
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3149e-04 - val_loss: 4.5849e-04
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6566e-04 - val_loss: 5.4889e-04
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9425e-04 - val_loss: 4.7938e-04
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.1595e-04 - val_loss: 4.7966e-04
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7321e-04 - val_loss: 5.3546e-04
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.0431e-04 - val_loss: 4.7644e-04
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2832e-04 - val_loss: 4.9939e-04
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5852e-04 - val_loss: 5.3686e-04
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8189e-04 - val_loss: 4.5361e-04
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3087e-04 - val_loss: 4.9887e-04
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8631e-04 - val_loss: 5.2852e-04
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6024e-04 - val_loss: 4.4635e-04
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6306e-04 - val_loss: 4.8881e-04
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5809e-04 - val_loss: 5.1141e-04
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7900e-04 - val_loss: 4.9910e-04
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4554e-04 - val_loss: 4.9488e-04
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4849e-04 - val_loss: 4.8904e-04
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5148e-04 - val_loss: 5.0785e-04
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.0282e-04 - val_loss: 4.2468e-04
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0853e-04 - val_loss: 5.0854e-04
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.0288e-04 - val_loss: 5.3492e-04
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6014e-04 - val_loss: 4.3760e-04
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3484e-04 - val_loss: 5.2240e-04
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8361e-04 - val_loss: 4.9001e-04
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4817e-04 - val_loss: 5.0388e-04
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3168e-04 - val_loss: 4.6077e-04
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8626e-04 - val_loss: 5.7105e-04
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8993e-04 - val_loss: 4.7853e-04
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2512e-04 - val_loss: 4.2978e-04
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4188e-04 - val_loss: 5.6169e-04
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9269e-04 - val_loss: 4.8656e-04
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4767e-04 - val_loss: 4.7242e-04
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4928e-04 - val_loss: 5.4032e-04
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3511e-04 - val_loss: 5.0366e-04
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8651e-04 - val_loss: 5.3742e-04
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6690e-04 - val_loss: 4.5311e-04
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0585e-04 - val_loss: 5.1717e-04
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8566e-04 - val_loss: 5.3507e-04
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4708e-04 - val_loss: 4.4495e-04
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2247e-04 - val_loss: 5.0123e-04
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00042
448/448 - 0s - loss: 5.0288e-04 - val_loss: 5.2030e-04
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4799e-04 - val_loss: 5.2366e-04
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2971e-04 - val_loss: 4.6475e-04
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5031e-04 - val_loss: 5.3002e-04
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9183e-04 - val_loss: 5.2549e-04
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.1365e-04 - val_loss: 4.7914e-04
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8674e-04 - val_loss: 5.6678e-04
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6569e-04 - val_loss: 4.4871e-04
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0454e-04 - val_loss: 4.7865e-04
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7028e-04 - val_loss: 5.4802e-04
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5068e-04 - val_loss: 5.1654e-04
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8044e-04 - val_loss: 5.1725e-04
Epoch 275/512

Epoch 00275: val_loss improved from 0.00042 to 0.00042, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.7691e-04 - val_loss: 4.1694e-04
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0783e-04 - val_loss: 5.2394e-04
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8081e-04 - val_loss: 4.8279e-04
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5343e-04 - val_loss: 4.8757e-04
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2981e-04 - val_loss: 4.9257e-04
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7161e-04 - val_loss: 5.4414e-04
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8463e-04 - val_loss: 5.0945e-04
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2041e-04 - val_loss: 4.4262e-04
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4022e-04 - val_loss: 5.2074e-04
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8023e-04 - val_loss: 4.6125e-04
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.1783e-04 - val_loss: 5.6886e-04
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6139e-04 - val_loss: 4.9013e-04
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6489e-04 - val_loss: 5.4780e-04
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5961e-04 - val_loss: 4.4688e-04
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3359e-04 - val_loss: 5.4557e-04
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6072e-04 - val_loss: 4.8071e-04
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5554e-04 - val_loss: 5.1133e-04
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6061e-04 - val_loss: 5.7152e-04
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2823e-04 - val_loss: 4.4805e-04
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5371e-04 - val_loss: 5.3791e-04
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8629e-04 - val_loss: 5.4334e-04
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0734e-04 - val_loss: 4.6134e-04
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8219e-04 - val_loss: 5.6550e-04
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.6383e-04 - val_loss: 4.5520e-04
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2048e-04 - val_loss: 4.5090e-04
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.2314e-04 - val_loss: 5.0427e-04
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8906e-04 - val_loss: 4.8901e-04
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3622e-04 - val_loss: 5.5664e-04
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3224e-04 - val_loss: 4.8205e-04
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.5383e-04 - val_loss: 5.1957e-04
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.7921e-04 - val_loss: 5.1160e-04
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3014e-04 - val_loss: 4.2460e-04
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.3230e-04 - val_loss: 5.3515e-04
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.8183e-04 - val_loss: 4.8765e-04
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.4603e-04 - val_loss: 4.2929e-04
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.0620e-04 - val_loss: 5.2365e-04
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00042
448/448 - 0s - loss: 4.9301e-04 - val_loss: 4.9577e-04
Epoch 312/512

Epoch 00312: val_loss improved from 0.00042 to 0.00040, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.6273e-04 - val_loss: 4.0290e-04
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9753e-04 - val_loss: 5.0459e-04
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9200e-04 - val_loss: 5.1864e-04
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3462e-04 - val_loss: 4.3635e-04
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2042e-04 - val_loss: 5.2263e-04
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7526e-04 - val_loss: 4.2126e-04
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1350e-04 - val_loss: 5.6430e-04
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00040
448/448 - 0s - loss: 5.1698e-04 - val_loss: 4.9746e-04
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2638e-04 - val_loss: 4.2562e-04
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9755e-04 - val_loss: 5.2411e-04
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00040
448/448 - 0s - loss: 5.0419e-04 - val_loss: 5.0566e-04
Epoch 323/512

Epoch 00323: val_loss improved from 0.00040 to 0.00040, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.5487e-04 - val_loss: 4.0097e-04
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9016e-04 - val_loss: 5.1639e-04
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9543e-04 - val_loss: 4.8634e-04
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4188e-04 - val_loss: 5.6763e-04
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1581e-04 - val_loss: 4.6597e-04
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6269e-04 - val_loss: 5.3106e-04
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6873e-04 - val_loss: 4.8821e-04
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0898e-04 - val_loss: 4.7503e-04
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5038e-04 - val_loss: 5.6025e-04
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00040
448/448 - 0s - loss: 5.0027e-04 - val_loss: 4.6150e-04
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0857e-04 - val_loss: 4.4066e-04
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1444e-04 - val_loss: 5.0265e-04
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8280e-04 - val_loss: 5.1080e-04
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4681e-04 - val_loss: 4.4364e-04
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4800e-04 - val_loss: 4.2738e-04
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1795e-04 - val_loss: 5.2727e-04
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8780e-04 - val_loss: 4.8517e-04
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2667e-04 - val_loss: 4.6042e-04
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1849e-04 - val_loss: 5.0810e-04
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9957e-04 - val_loss: 5.0744e-04
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5614e-04 - val_loss: 4.2636e-04
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9322e-04 - val_loss: 4.7888e-04
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6691e-04 - val_loss: 5.1070e-04
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5437e-04 - val_loss: 4.5745e-04
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3243e-04 - val_loss: 4.8807e-04
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1351e-04 - val_loss: 5.0000e-04
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9188e-04 - val_loss: 4.9124e-04
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3821e-04 - val_loss: 5.1955e-04
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2786e-04 - val_loss: 4.6733e-04
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3714e-04 - val_loss: 5.0675e-04
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6474e-04 - val_loss: 4.6220e-04
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3121e-04 - val_loss: 4.9179e-04
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0286e-04 - val_loss: 5.0654e-04
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00040
448/448 - 0s - loss: 5.1133e-04 - val_loss: 5.3468e-04
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4918e-04 - val_loss: 4.5466e-04
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8543e-04 - val_loss: 4.5241e-04
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7089e-04 - val_loss: 5.5703e-04
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6773e-04 - val_loss: 4.4133e-04
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1304e-04 - val_loss: 4.1919e-04
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1695e-04 - val_loss: 5.5525e-04
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00040
448/448 - 0s - loss: 5.0566e-04 - val_loss: 5.4715e-04
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1693e-04 - val_loss: 4.3727e-04
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1956e-04 - val_loss: 5.3479e-04
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8223e-04 - val_loss: 4.9980e-04
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2541e-04 - val_loss: 4.3686e-04
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1272e-04 - val_loss: 4.9195e-04
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7073e-04 - val_loss: 5.1295e-04
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3464e-04 - val_loss: 4.6746e-04
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6503e-04 - val_loss: 4.4449e-04
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1561e-04 - val_loss: 5.0829e-04
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5685e-04 - val_loss: 4.6699e-04
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3038e-04 - val_loss: 4.7282e-04
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5079e-04 - val_loss: 4.2712e-04
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0860e-04 - val_loss: 5.2129e-04
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9022e-04 - val_loss: 5.1516e-04
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4061e-04 - val_loss: 4.0885e-04
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9965e-04 - val_loss: 5.2465e-04
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9354e-04 - val_loss: 5.0631e-04
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3192e-04 - val_loss: 4.3214e-04
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8977e-04 - val_loss: 4.7991e-04
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7896e-04 - val_loss: 5.3801e-04
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5709e-04 - val_loss: 4.1425e-04
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2407e-04 - val_loss: 4.0626e-04
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1222e-04 - val_loss: 5.2035e-04
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8390e-04 - val_loss: 4.9235e-04
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3519e-04 - val_loss: 4.4236e-04
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8662e-04 - val_loss: 4.6259e-04
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8264e-04 - val_loss: 5.3600e-04
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6572e-04 - val_loss: 5.0274e-04
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.7119e-04 - val_loss: 4.5238e-04
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7864e-04 - val_loss: 5.7485e-04
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6326e-04 - val_loss: 4.0809e-04
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9205e-04 - val_loss: 4.8597e-04
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5993e-04 - val_loss: 4.5626e-04
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1998e-04 - val_loss: 5.3295e-04
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7546e-04 - val_loss: 4.5395e-04
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4477e-04 - val_loss: 4.5481e-04
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1765e-04 - val_loss: 4.8769e-04
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4999e-04 - val_loss: 4.7784e-04
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3864e-04 - val_loss: 4.6690e-04
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3441e-04 - val_loss: 4.7348e-04
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1298e-04 - val_loss: 5.0391e-04
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9993e-04 - val_loss: 5.0771e-04
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3529e-04 - val_loss: 4.7169e-04
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9031e-04 - val_loss: 4.6200e-04
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5527e-04 - val_loss: 5.3565e-04
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7110e-04 - val_loss: 4.4338e-04
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1631e-04 - val_loss: 4.9244e-04
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9738e-04 - val_loss: 4.8904e-04
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9466e-04 - val_loss: 5.1389e-04
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4024e-04 - val_loss: 4.6732e-04
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8039e-04 - val_loss: 4.5100e-04
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5646e-04 - val_loss: 5.4124e-04
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7351e-04 - val_loss: 4.7804e-04
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8077e-04 - val_loss: 4.1216e-04
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4591e-04 - val_loss: 5.6026e-04
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8625e-04 - val_loss: 5.1963e-04
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9648e-04 - val_loss: 4.0708e-04
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1985e-04 - val_loss: 5.3879e-04
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7857e-04 - val_loss: 4.4754e-04
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2829e-04 - val_loss: 4.8588e-04
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9722e-04 - val_loss: 5.0693e-04
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9051e-04 - val_loss: 5.0096e-04
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2922e-04 - val_loss: 4.3461e-04
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9503e-04 - val_loss: 4.6876e-04
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3477e-04 - val_loss: 5.1921e-04
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8748e-04 - val_loss: 4.7333e-04
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2863e-04 - val_loss: 4.7635e-04
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8752e-04 - val_loss: 4.7998e-04
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8043e-04 - val_loss: 5.1832e-04
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5218e-04 - val_loss: 5.1853e-04
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9605e-04 - val_loss: 4.4829e-04
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4517e-04 - val_loss: 5.2116e-04
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6546e-04 - val_loss: 4.4575e-04
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9908e-04 - val_loss: 4.9988e-04
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5454e-04 - val_loss: 4.4278e-04
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2399e-04 - val_loss: 5.1001e-04
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5828e-04 - val_loss: 4.8065e-04
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2557e-04 - val_loss: 4.0471e-04
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0942e-04 - val_loss: 5.2921e-04
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.8404e-04 - val_loss: 4.6363e-04
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2584e-04 - val_loss: 4.1676e-04
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9153e-04 - val_loss: 4.9110e-04
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6847e-04 - val_loss: 4.8973e-04
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4413e-04 - val_loss: 4.5051e-04
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.8859e-04 - val_loss: 4.6584e-04
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7852e-04 - val_loss: 5.0914e-04
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4842e-04 - val_loss: 4.6752e-04
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9366e-04 - val_loss: 4.3740e-04
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4429e-04 - val_loss: 5.2619e-04
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5196e-04 - val_loss: 4.5721e-04
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0287e-04 - val_loss: 4.1636e-04
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2852e-04 - val_loss: 5.6458e-04
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9081e-04 - val_loss: 4.1814e-04
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9839e-04 - val_loss: 5.0665e-04
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2606e-04 - val_loss: 4.8211e-04
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4537e-04 - val_loss: 4.8180e-04
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3331e-04 - val_loss: 4.4878e-04
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2729e-04 - val_loss: 4.4256e-04
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9817e-04 - val_loss: 5.1785e-04
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.9572e-04 - val_loss: 4.7313e-04
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.1649e-04 - val_loss: 4.8341e-04
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9848e-04 - val_loss: 4.4559e-04
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4289e-04 - val_loss: 4.9772e-04
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4846e-04 - val_loss: 4.8423e-04
Epoch 468/512

Epoch 00468: val_loss improved from 0.00040 to 0.00040, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.2691e-04 - val_loss: 3.9833e-04
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0983e-04 - val_loss: 5.2285e-04
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.7182e-04 - val_loss: 4.6295e-04
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2285e-04 - val_loss: 4.2770e-04
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9109e-04 - val_loss: 4.7528e-04
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6699e-04 - val_loss: 5.1070e-04
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4279e-04 - val_loss: 4.2209e-04
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.7820e-04 - val_loss: 4.6578e-04
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6779e-04 - val_loss: 5.0071e-04
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.4098e-04 - val_loss: 5.8015e-04
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9856e-04 - val_loss: 4.3319e-04
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3928e-04 - val_loss: 5.4091e-04
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5480e-04 - val_loss: 4.3319e-04
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0011e-04 - val_loss: 4.6776e-04
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.2585e-04 - val_loss: 4.8680e-04
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.6540e-04 - val_loss: 4.4718e-04
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3564e-04 - val_loss: 4.9079e-04
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00040
448/448 - 0s - loss: 3.9933e-04 - val_loss: 4.5732e-04
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5050e-04 - val_loss: 5.3071e-04
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.3609e-04 - val_loss: 4.2759e-04
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.0664e-04 - val_loss: 4.8228e-04
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00040
448/448 - 0s - loss: 4.5570e-04 - val_loss: 4.9573e-04
Epoch 490/512

Epoch 00490: val_loss improved from 0.00040 to 0.00037, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.3576e-04 - val_loss: 3.6848e-04
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.8611e-04 - val_loss: 5.3046e-04
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.9047e-04 - val_loss: 4.4533e-04
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.1187e-04 - val_loss: 4.7184e-04
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.9638e-04 - val_loss: 4.5643e-04
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.5269e-04 - val_loss: 4.9316e-04
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.3167e-04 - val_loss: 4.4822e-04
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.0721e-04 - val_loss: 3.9833e-04
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.0592e-04 - val_loss: 5.5019e-04
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.9104e-04 - val_loss: 4.8139e-04
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.2015e-04 - val_loss: 3.9917e-04
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.8090e-04 - val_loss: 4.7229e-04
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.5435e-04 - val_loss: 4.9719e-04
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.5541e-04 - val_loss: 4.1539e-04
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.6989e-04 - val_loss: 4.7226e-04
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.7880e-04 - val_loss: 5.1241e-04
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.3491e-04 - val_loss: 4.3324e-04
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.7312e-04 - val_loss: 4.1664e-04
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.3325e-04 - val_loss: 5.4475e-04
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.7132e-04 - val_loss: 4.8946e-04
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.0574e-04 - val_loss: 3.8631e-04
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00037
448/448 - 0s - loss: 3.9518e-04 - val_loss: 5.3783e-04
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00037
448/448 - 0s - loss: 4.8502e-04 - val_loss: 4.4743e-04
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.695 | eve: 9.698 | bob: 9.587Epoch   0:   0% | abe: 9.663 | eve: 9.718 | bob: 9.565Epoch   0:   1% | abe: 9.650 | eve: 9.713 | bob: 9.563Epoch   0:   2% | abe: 9.601 | eve: 9.705 | bob: 9.522Epoch   0:   2% | abe: 9.589 | eve: 9.696 | bob: 9.517Epoch   0:   3% | abe: 9.573 | eve: 9.694 | bob: 9.508Epoch   0:   4% | abe: 9.557 | eve: 9.689 | bob: 9.498Epoch   0:   4% | abe: 9.542 | eve: 9.692 | bob: 9.488Epoch   0:   5% | abe: 9.524 | eve: 9.698 | bob: 9.473Epoch   0:   6% | abe: 9.497 | eve: 9.697 | bob: 9.450Epoch   0:   6% | abe: 9.483 | eve: 9.695 | bob: 9.439Epoch   0:   7% | abe: 9.472 | eve: 9.700 | bob: 9.430Epoch   0:   8% | abe: 9.462 | eve: 9.699 | bob: 9.423Epoch   0:   8% | abe: 9.450 | eve: 9.700 | bob: 9.414Epoch   0:   9% | abe: 9.441 | eve: 9.705 | bob: 9.407Epoch   0:  10% | abe: 9.427 | eve: 9.701 | bob: 9.394Epoch   0:  10% | abe: 9.418 | eve: 9.699 | bob: 9.387Epoch   0:  11% | abe: 9.408 | eve: 9.699 | bob: 9.379Epoch   0:  12% | abe: 9.395 | eve: 9.698 | bob: 9.367Epoch   0:  13% | abe: 9.386 | eve: 9.702 | bob: 9.359Epoch   0:  13% | abe: 9.378 | eve: 9.702 | bob: 9.353Epoch   0:  14% | abe: 9.369 | eve: 9.706 | bob: 9.346Epoch   0:  15% | abe: 9.359 | eve: 9.705 | bob: 9.337Epoch   0:  15% | abe: 9.348 | eve: 9.710 | bob: 9.328Epoch   0:  16% | abe: 9.339 | eve: 9.710 | bob: 9.320Epoch   0:  17% | abe: 9.328 | eve: 9.708 | bob: 9.311Epoch   0:  17% | abe: 9.320 | eve: 9.709 | bob: 9.304Epoch   0:  18% | abe: 9.314 | eve: 9.705 | bob: 9.299Epoch   0:  19% | abe: 9.308 | eve: 9.704 | bob: 9.295Epoch   0:  19% | abe: 9.302 | eve: 9.705 | bob: 9.291Epoch   0:  20% | abe: 9.293 | eve: 9.706 | bob: 9.283Epoch   0:  21% | abe: 9.286 | eve: 9.705 | bob: 9.278Epoch   0:  21% | abe: 9.281 | eve: 9.706 | bob: 9.274Epoch   0:  22% | abe: 9.273 | eve: 9.710 | bob: 9.268Epoch   0:  23% | abe: 9.266 | eve: 9.710 | bob: 9.263Epoch   0:  23% | abe: 9.261 | eve: 9.712 | bob: 9.259Epoch   0:  24% | abe: 9.255 | eve: 9.711 | bob: 9.255Epoch   0:  25% | abe: 9.250 | eve: 9.713 | bob: 9.252Epoch   0:  26% | abe: 9.244 | eve: 9.715 | bob: 9.247Epoch   0:  26% | abe: 9.242 | eve: 9.715 | bob: 9.246Epoch   0:  27% | abe: 9.237 | eve: 9.714 | bob: 9.243Epoch   0:  28% | abe: 9.232 | eve: 9.715 | bob: 9.240Epoch   0:  28% | abe: 9.228 | eve: 9.717 | bob: 9.236Epoch   0:  29% | abe: 9.224 | eve: 9.716 | bob: 9.234Epoch   0:  30% | abe: 9.219 | eve: 9.718 | bob: 9.231Epoch   0:  30% | abe: 9.215 | eve: 9.717 | bob: 9.227Epoch   0:  31% | abe: 9.212 | eve: 9.719 | bob: 9.225Epoch   0:  32% | abe: 9.209 | eve: 9.720 | bob: 9.224Epoch   0:  32% | abe: 9.204 | eve: 9.720 | bob: 9.220Epoch   0:  33% | abe: 9.201 | eve: 9.722 | bob: 9.218Epoch   0:  34% | abe: 9.197 | eve: 9.722 | bob: 9.215Epoch   0:  34% | abe: 9.194 | eve: 9.724 | bob: 9.214Epoch   0:  35% | abe: 9.191 | eve: 9.725 | bob: 9.211Epoch   0:  36% | abe: 9.188 | eve: 9.726 | bob: 9.209Epoch   0:  36% | abe: 9.185 | eve: 9.725 | bob: 9.207Epoch   0:  37% | abe: 9.182 | eve: 9.727 | bob: 9.205Epoch   0:  38% | abe: 9.180 | eve: 9.727 | bob: 9.204Epoch   0:  39% | abe: 9.178 | eve: 9.727 | bob: 9.203Epoch   0:  39% | abe: 9.176 | eve: 9.727 | bob: 9.202Epoch   0:  40% | abe: 9.173 | eve: 9.729 | bob: 9.201Epoch   0:  41% | abe: 9.171 | eve: 9.729 | bob: 9.200Epoch   0:  41% | abe: 9.169 | eve: 9.729 | bob: 9.198Epoch   0:  42% | abe: 9.166 | eve: 9.729 | bob: 9.197Epoch   0:  43% | abe: 9.164 | eve: 9.730 | bob: 9.196Epoch   0:  43% | abe: 9.163 | eve: 9.732 | bob: 9.195Epoch   0:  44% | abe: 9.161 | eve: 9.732 | bob: 9.194Epoch   0:  45% | abe: 9.159 | eve: 9.734 | bob: 9.193Epoch   0:  45% | abe: 9.157 | eve: 9.733 | bob: 9.191Epoch   0:  46% | abe: 9.154 | eve: 9.735 | bob: 9.190Epoch   0:  47% | abe: 9.152 | eve: 9.736 | bob: 9.188Epoch   0:  47% | abe: 9.150 | eve: 9.736 | bob: 9.187Epoch   0:  48% | abe: 9.149 | eve: 9.738 | bob: 9.186Epoch   0:  49% | abe: 9.147 | eve: 9.739 | bob: 9.185Epoch   0:  50% | abe: 9.145 | eve: 9.740 | bob: 9.185Epoch   0:  50% | abe: 9.143 | eve: 9.742 | bob: 9.184Epoch   0:  51% | abe: 9.142 | eve: 9.742 | bob: 9.183Epoch   0:  52% | abe: 9.140 | eve: 9.742 | bob: 9.182Epoch   0:  52% | abe: 9.138 | eve: 9.743 | bob: 9.181Epoch   0:  53% | abe: 9.137 | eve: 9.744 | bob: 9.181Epoch   0:  54% | abe: 9.135 | eve: 9.743 | bob: 9.180Epoch   0:  54% | abe: 9.135 | eve: 9.745 | bob: 9.180Epoch   0:  55% | abe: 9.133 | eve: 9.746 | bob: 9.179Epoch   0:  56% | abe: 9.133 | eve: 9.746 | bob: 9.180Epoch   0:  56% | abe: 9.132 | eve: 9.748 | bob: 9.179Epoch   0:  57% | abe: 9.130 | eve: 9.748 | bob: 9.178Epoch   0:  58% | abe: 9.129 | eve: 9.748 | bob: 9.178Epoch   0:  58% | abe: 9.129 | eve: 9.749 | bob: 9.178Epoch   0:  59% | abe: 9.126 | eve: 9.750 | bob: 9.177Epoch   0:  60% | abe: 9.126 | eve: 9.750 | bob: 9.177Epoch   0:  60% | abe: 9.125 | eve: 9.750 | bob: 9.177Epoch   0:  61% | abe: 9.123 | eve: 9.751 | bob: 9.175Epoch   0:  62% | abe: 9.122 | eve: 9.751 | bob: 9.174Epoch   0:  63% | abe: 9.120 | eve: 9.752 | bob: 9.174Epoch   0:  63% | abe: 9.119 | eve: 9.753 | bob: 9.174Epoch   0:  64% | abe: 9.117 | eve: 9.754 | bob: 9.172Epoch   0:  65% | abe: 9.116 | eve: 9.755 | bob: 9.172Epoch   0:  65% | abe: 9.115 | eve: 9.755 | bob: 9.171Epoch   0:  66% | abe: 9.115 | eve: 9.756 | bob: 9.171Epoch   0:  67% | abe: 9.114 | eve: 9.757 | bob: 9.171Epoch   0:  67% | abe: 9.113 | eve: 9.757 | bob: 9.171Epoch   0:  68% | abe: 9.112 | eve: 9.757 | bob: 9.171Epoch   0:  69% | abe: 9.111 | eve: 9.758 | bob: 9.170Epoch   0:  69% | abe: 9.109 | eve: 9.759 | bob: 9.169Epoch   0:  70% | abe: 9.108 | eve: 9.759 | bob: 9.168Epoch   0:  71% | abe: 9.108 | eve: 9.761 | bob: 9.169Epoch   0:  71% | abe: 9.106 | eve: 9.760 | bob: 9.168Epoch   0:  72% | abe: 9.105 | eve: 9.761 | bob: 9.168Epoch   0:  73% | abe: 9.104 | eve: 9.763 | bob: 9.167Epoch   0:  73% | abe: 9.103 | eve: 9.764 | bob: 9.166Epoch   0:  74% | abe: 9.102 | eve: 9.764 | bob: 9.166Epoch   0:  75% | abe: 9.102 | eve: 9.764 | bob: 9.167Epoch   0:  76% | abe: 9.101 | eve: 9.765 | bob: 9.166Epoch   0:  76% | abe: 9.100 | eve: 9.765 | bob: 9.166Epoch   0:  77% | abe: 9.099 | eve: 9.766 | bob: 9.165Epoch   0:  78% | abe: 9.098 | eve: 9.767 | bob: 9.165Epoch   0:  78% | abe: 9.097 | eve: 9.767 | bob: 9.165Epoch   0:  79% | abe: 9.096 | eve: 9.768 | bob: 9.164Epoch   0:  80% | abe: 9.096 | eve: 9.769 | bob: 9.164Epoch   0:  80% | abe: 9.095 | eve: 9.770 | bob: 9.164Epoch   0:  81% | abe: 9.095 | eve: 9.770 | bob: 9.164Epoch   0:  82% | abe: 9.094 | eve: 9.771 | bob: 9.164Epoch   0:  82% | abe: 9.093 | eve: 9.772 | bob: 9.164Epoch   0:  83% | abe: 9.091 | eve: 9.772 | bob: 9.163Epoch   0:  84% | abe: 9.091 | eve: 9.773 | bob: 9.162Epoch   0:  84% | abe: 9.090 | eve: 9.774 | bob: 9.163Epoch   0:  85% | abe: 9.090 | eve: 9.775 | bob: 9.163Epoch   0:  86% | abe: 9.089 | eve: 9.775 | bob: 9.162Epoch   0:  86% | abe: 9.088 | eve: 9.776 | bob: 9.163Epoch   0:  87% | abe: 9.088 | eve: 9.776 | bob: 9.163Epoch   0:  88% | abe: 9.088 | eve: 9.777 | bob: 9.163Epoch   0:  89% | abe: 9.087 | eve: 9.777 | bob: 9.163Epoch   0:  89% | abe: 9.086 | eve: 9.778 | bob: 9.162Epoch   0:  90% | abe: 9.086 | eve: 9.778 | bob: 9.163Epoch   0:  91% | abe: 9.085 | eve: 9.779 | bob: 9.162Epoch   0:  91% | abe: 9.084 | eve: 9.780 | bob: 9.162Epoch   0:  92% | abe: 9.083 | eve: 9.781 | bob: 9.161Epoch   0:  93% | abe: 9.082 | eve: 9.781 | bob: 9.161Epoch   0:  93% | abe: 9.082 | eve: 9.782 | bob: 9.161Epoch   0:  94% | abe: 9.081 | eve: 9.783 | bob: 9.161Epoch   0:  95% | abe: 9.080 | eve: 9.784 | bob: 9.161Epoch   0:  95% | abe: 9.080 | eve: 9.785 | bob: 9.161Epoch   0:  96% | abe: 9.080 | eve: 9.786 | bob: 9.161Epoch   0:  97% | abe: 9.079 | eve: 9.786 | bob: 9.161Epoch   0:  97% | abe: 9.079 | eve: 9.786 | bob: 9.161Epoch   0:  98% | abe: 9.078 | eve: 9.787 | bob: 9.161Epoch   0:  99% | abe: 9.077 | eve: 9.787 | bob: 9.161
New best Bob loss 9.160847027153425 at epoch 0
Epoch   1:   0% | abe: 8.995 | eve: 9.806 | bob: 9.152Epoch   1:   0% | abe: 8.978 | eve: 9.835 | bob: 9.132Epoch   1:   1% | abe: 9.006 | eve: 9.828 | bob: 9.162Epoch   1:   2% | abe: 9.001 | eve: 9.818 | bob: 9.157Epoch   1:   2% | abe: 8.996 | eve: 9.820 | bob: 9.151Epoch   1:   3% | abe: 8.991 | eve: 9.821 | bob: 9.148Epoch   1:   4% | abe: 8.987 | eve: 9.815 | bob: 9.142Epoch   1:   4% | abe: 8.976 | eve: 9.831 | bob: 9.130Epoch   1:   5% | abe: 8.976 | eve: 9.841 | bob: 9.130Epoch   1:   6% | abe: 8.978 | eve: 9.836 | bob: 9.133Epoch   1:   6% | abe: 8.978 | eve: 9.832 | bob: 9.132Epoch   1:   7% | abe: 8.976 | eve: 9.838 | bob: 9.130Epoch   1:   8% | abe: 8.972 | eve: 9.838 | bob: 9.126Epoch   1:   8% | abe: 8.978 | eve: 9.837 | bob: 9.132Epoch   1:   9% | abe: 8.980 | eve: 9.839 | bob: 9.135Epoch   1:  10% | abe: 8.983 | eve: 9.839 | bob: 9.138Epoch   1:  10% | abe: 8.981 | eve: 9.841 | bob: 9.135Epoch   1:  11% | abe: 8.981 | eve: 9.842 | bob: 9.136Epoch   1:  12% | abe: 8.981 | eve: 9.845 | bob: 9.137Epoch   1:  13% | abe: 8.979 | eve: 9.844 | bob: 9.134Epoch   1:  13% | abe: 8.978 | eve: 9.844 | bob: 9.134Epoch   1:  14% | abe: 8.974 | eve: 9.842 | bob: 9.130Epoch   1:  15% | abe: 8.973 | eve: 9.842 | bob: 9.130Epoch   1:  15% | abe: 8.972 | eve: 9.846 | bob: 9.129Epoch   1:  16% | abe: 8.972 | eve: 9.847 | bob: 9.129Epoch   1:  17% | abe: 8.971 | eve: 9.845 | bob: 9.128Epoch   1:  17% | abe: 8.970 | eve: 9.845 | bob: 9.129Epoch   1:  18% | abe: 8.970 | eve: 9.846 | bob: 9.129Epoch   1:  19% | abe: 8.970 | eve: 9.850 | bob: 9.129Epoch   1:  19% | abe: 8.970 | eve: 9.847 | bob: 9.129Epoch   1:  20% | abe: 8.969 | eve: 9.845 | bob: 9.129Epoch   1:  21% | abe: 8.970 | eve: 9.845 | bob: 9.131Epoch   1:  21% | abe: 8.971 | eve: 9.844 | bob: 9.132Epoch   1:  22% | abe: 8.971 | eve: 9.846 | bob: 9.133Epoch   1:  23% | abe: 8.970 | eve: 9.846 | bob: 9.132Epoch   1:  23% | abe: 8.970 | eve: 9.847 | bob: 9.132Epoch   1:  24% | abe: 8.970 | eve: 9.847 | bob: 9.133Epoch   1:  25% | abe: 8.969 | eve: 9.848 | bob: 9.131Epoch   1:  26% | abe: 8.968 | eve: 9.848 | bob: 9.131Epoch   1:  26% | abe: 8.967 | eve: 9.849 | bob: 9.131Epoch   1:  27% | abe: 8.967 | eve: 9.851 | bob: 9.131Epoch   1:  28% | abe: 8.967 | eve: 9.852 | bob: 9.131Epoch   1:  28% | abe: 8.967 | eve: 9.853 | bob: 9.131Epoch   1:  29% | abe: 8.968 | eve: 9.852 | bob: 9.132Epoch   1:  30% | abe: 8.968 | eve: 9.854 | bob: 9.132Epoch   1:  30% | abe: 8.967 | eve: 9.855 | bob: 9.132Epoch   1:  31% | abe: 8.967 | eve: 9.854 | bob: 9.132Epoch   1:  32% | abe: 8.969 | eve: 9.855 | bob: 9.134Epoch   1:  32% | abe: 8.970 | eve: 9.855 | bob: 9.135Epoch   1:  33% | abe: 8.970 | eve: 9.855 | bob: 9.136Epoch   1:  34% | abe: 8.971 | eve: 9.856 | bob: 9.137Epoch   1:  34% | abe: 8.971 | eve: 9.855 | bob: 9.137Epoch   1:  35% | abe: 8.970 | eve: 9.855 | bob: 9.137Epoch   1:  36% | abe: 8.970 | eve: 9.855 | bob: 9.137Epoch   1:  36% | abe: 8.970 | eve: 9.855 | bob: 9.137Epoch   1:  37% | abe: 8.970 | eve: 9.856 | bob: 9.138Epoch   1:  38% | abe: 8.969 | eve: 9.857 | bob: 9.137Epoch   1:  39% | abe: 8.969 | eve: 9.858 | bob: 9.137Epoch   1:  39% | abe: 8.969 | eve: 9.860 | bob: 9.138Epoch   1:  40% | abe: 8.970 | eve: 9.858 | bob: 9.139Epoch   1:  41% | abe: 8.969 | eve: 9.859 | bob: 9.138Epoch   1:  41% | abe: 8.969 | eve: 9.860 | bob: 9.138Epoch   1:  42% | abe: 8.968 | eve: 9.861 | bob: 9.138Epoch   1:  43% | abe: 8.967 | eve: 9.861 | bob: 9.137Epoch   1:  43% | abe: 8.967 | eve: 9.861 | bob: 9.137Epoch   1:  44% | abe: 8.966 | eve: 9.861 | bob: 9.136Epoch   1:  45% | abe: 8.966 | eve: 9.862 | bob: 9.136Epoch   1:  45% | abe: 8.965 | eve: 9.862 | bob: 9.136Epoch   1:  46% | abe: 8.964 | eve: 9.862 | bob: 9.135Epoch   1:  47% | abe: 8.963 | eve: 9.861 | bob: 9.134Epoch   1:  47% | abe: 8.963 | eve: 9.861 | bob: 9.134Epoch   1:  48% | abe: 8.962 | eve: 9.861 | bob: 9.133Epoch   1:  49% | abe: 8.962 | eve: 9.860 | bob: 9.133Epoch   1:  50% | abe: 8.961 | eve: 9.860 | bob: 9.133Epoch   1:  50% | abe: 8.961 | eve: 9.861 | bob: 9.133Epoch   1:  51% | abe: 8.961 | eve: 9.862 | bob: 9.133Epoch   1:  52% | abe: 8.960 | eve: 9.863 | bob: 9.132Epoch   1:  52% | abe: 8.961 | eve: 9.863 | bob: 9.133Epoch   1:  53% | abe: 8.960 | eve: 9.863 | bob: 9.133Epoch   1:  54% | abe: 8.959 | eve: 9.863 | bob: 9.133Epoch   1:  54% | abe: 8.959 | eve: 9.864 | bob: 9.133Epoch   1:  55% | abe: 8.959 | eve: 9.864 | bob: 9.132Epoch   1:  56% | abe: 8.958 | eve: 9.865 | bob: 9.132Epoch   1:  56% | abe: 8.958 | eve: 9.866 | bob: 9.132Epoch   1:  57% | abe: 8.958 | eve: 9.867 | bob: 9.133Epoch   1:  58% | abe: 8.958 | eve: 9.868 | bob: 9.133Epoch   1:  58% | abe: 8.958 | eve: 9.867 | bob: 9.133Epoch   1:  59% | abe: 8.957 | eve: 9.867 | bob: 9.133Epoch   1:  60% | abe: 8.956 | eve: 9.866 | bob: 9.132Epoch   1:  60% | abe: 8.956 | eve: 9.867 | bob: 9.131Epoch   1:  61% | abe: 8.955 | eve: 9.866 | bob: 9.131Epoch   1:  62% | abe: 8.955 | eve: 9.866 | bob: 9.132Epoch   1:  63% | abe: 8.955 | eve: 9.866 | bob: 9.131Epoch   1:  63% | abe: 8.954 | eve: 9.865 | bob: 9.131Epoch   1:  64% | abe: 8.954 | eve: 9.865 | bob: 9.131Epoch   1:  65% | abe: 8.954 | eve: 9.866 | bob: 9.131Epoch   1:  65% | abe: 8.953 | eve: 9.866 | bob: 9.130Epoch   1:  66% | abe: 8.952 | eve: 9.866 | bob: 9.130Epoch   1:  67% | abe: 8.953 | eve: 9.866 | bob: 9.130Epoch   1:  67% | abe: 8.952 | eve: 9.867 | bob: 9.130Epoch   1:  68% | abe: 8.952 | eve: 9.867 | bob: 9.130Epoch   1:  69% | abe: 8.952 | eve: 9.868 | bob: 9.131Epoch   1:  69% | abe: 8.952 | eve: 9.867 | bob: 9.131Epoch   1:  70% | abe: 8.951 | eve: 9.867 | bob: 9.130Epoch   1:  71% | abe: 8.951 | eve: 9.868 | bob: 9.130Epoch   1:  71% | abe: 8.951 | eve: 9.868 | bob: 9.131Epoch   1:  72% | abe: 8.951 | eve: 9.869 | bob: 9.131Epoch   1:  73% | abe: 8.951 | eve: 9.869 | bob: 9.131Epoch   1:  73% | abe: 8.951 | eve: 9.869 | bob: 9.130Epoch   1:  74% | abe: 8.950 | eve: 9.870 | bob: 9.131Epoch   1:  75% | abe: 8.951 | eve: 9.870 | bob: 9.131Epoch   1:  76% | abe: 8.951 | eve: 9.871 | bob: 9.131Epoch   1:  76% | abe: 8.950 | eve: 9.871 | bob: 9.131Epoch   1:  77% | abe: 8.950 | eve: 9.870 | bob: 9.131Epoch   1:  78% | abe: 8.950 | eve: 9.871 | bob: 9.131Epoch   1:  78% | abe: 8.949 | eve: 9.871 | bob: 9.131Epoch   1:  79% | abe: 8.949 | eve: 9.871 | bob: 9.130Epoch   1:  80% | abe: 8.948 | eve: 9.872 | bob: 9.130Epoch   1:  80% | abe: 8.948 | eve: 9.872 | bob: 9.131Epoch   1:  81% | abe: 8.948 | eve: 9.872 | bob: 9.131Epoch   1:  82% | abe: 8.948 | eve: 9.872 | bob: 9.131Epoch   1:  82% | abe: 8.948 | eve: 9.873 | bob: 9.131Epoch   1:  83% | abe: 8.947 | eve: 9.873 | bob: 9.130Epoch   1:  84% | abe: 8.947 | eve: 9.873 | bob: 9.130Epoch   1:  84% | abe: 8.947 | eve: 9.873 | bob: 9.130Epoch   1:  85% | abe: 8.947 | eve: 9.872 | bob: 9.130Epoch   1:  86% | abe: 8.946 | eve: 9.873 | bob: 9.131Epoch   1:  86% | abe: 8.945 | eve: 9.873 | bob: 9.130Epoch   1:  87% | abe: 8.945 | eve: 9.873 | bob: 9.130Epoch   1:  88% | abe: 8.945 | eve: 9.873 | bob: 9.130Epoch   1:  89% | abe: 8.945 | eve: 9.873 | bob: 9.129Epoch   1:  89% | abe: 8.944 | eve: 9.872 | bob: 9.129Epoch   1:  90% | abe: 8.944 | eve: 9.872 | bob: 9.130Epoch   1:  91% | abe: 8.944 | eve: 9.873 | bob: 9.129Epoch   1:  91% | abe: 8.943 | eve: 9.873 | bob: 9.129Epoch   1:  92% | abe: 8.943 | eve: 9.873 | bob: 9.129Epoch   1:  93% | abe: 8.942 | eve: 9.873 | bob: 9.129Epoch   1:  93% | abe: 8.942 | eve: 9.873 | bob: 9.129Epoch   1:  94% | abe: 8.942 | eve: 9.873 | bob: 9.128Epoch   1:  95% | abe: 8.942 | eve: 9.874 | bob: 9.128Epoch   1:  95% | abe: 8.941 | eve: 9.874 | bob: 9.128Epoch   1:  96% | abe: 8.941 | eve: 9.874 | bob: 9.128Epoch   1:  97% | abe: 8.941 | eve: 9.874 | bob: 9.129Epoch   1:  97% | abe: 8.941 | eve: 9.874 | bob: 9.129Epoch   1:  98% | abe: 8.941 | eve: 9.875 | bob: 9.129Epoch   1:  99% | abe: 8.941 | eve: 9.875 | bob: 9.129
New best Bob loss 9.129009094015 at epoch 1
Training complete.
cipher1 + cipher2
[[0.7109953  1.2454937  0.8911344  ... 0.9155617  1.0612904  1.1241478 ]
 [0.7160115  1.2409751  0.87019074 ... 0.7402556  1.0699359  1.1410444 ]
 [0.777534   1.2032278  0.8858304  ... 0.7736392  1.04307    1.1480749 ]
 ...
 [0.71582186 1.23527    0.97796273 ... 0.79801726 1.0940268  1.1601536 ]
 [0.76311123 1.2319746  0.9914143  ... 0.86302185 1.0679085  1.1264306 ]
 [0.7679939  1.1910923  0.90819037 ... 0.7655271  1.1388054  1.125129  ]]
HO addition:
[[0.7109561  1.2454257  0.8910853  ... 0.91551155 1.0612323  1.1240863 ]
 [0.71597195 1.2409075  0.87014276 ... 0.74021477 1.0698775  1.140982  ]
 [0.7774912  1.2031621  0.88578165 ... 0.7735966  1.0430129  1.1480122 ]
 ...
 [0.7157823  1.2352028  0.9779091  ... 0.79797333 1.093967   1.1600902 ]
 [0.7630692  1.2319074  0.9913599  ... 0.86297446 1.0678502  1.1263691 ]
 [0.7679516  1.1910273  0.9081404  ... 0.7654849  1.1387432  1.1250674 ]]
cipher1 * cipher2
[[0.11751994 0.37794706 0.19332089 ... 0.20950873 0.28028947 0.31393844]
 [0.11970552 0.3761047  0.1829754  ... 0.13047598 0.28137988 0.32349244]
 [0.14698838 0.35630637 0.19262259 ... 0.14531656 0.2686532  0.3269927 ]
 ...
 [0.11962663 0.37305358 0.23857512 ... 0.15660687 0.29629114 0.3324571 ]
 [0.1411235  0.37152013 0.24532066 ... 0.18554093 0.27893585 0.31578824]
 [0.1431999  0.34967065 0.20304337 ... 0.14185217 0.3199067  0.31499383]]
HO multiplication
[[0.15145007 0.32539457 0.17619741 ... 0.2204749  0.2617356  0.2767288 ]
 [0.15242787 0.32214475 0.1711794  ... 0.15775844 0.2853379  0.27923563]
 [0.16683333 0.30657184 0.18077724 ... 0.16597936 0.27382964 0.28382382]
 ...
 [0.15239568 0.32005522 0.21105936 ... 0.17419595 0.27876067 0.29394177]
 [0.16446903 0.3181217  0.21462208 ... 0.19391507 0.29099828 0.2724853 ]
 [0.16552815 0.30245164 0.18486394 ... 0.16424067 0.2924563  0.27283752]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 1.31%
Bob decrypted addition: [[0.8435026  0.9375412  0.923604   ... 0.9207783  0.78413993 0.8079195 ]
 [0.84418565 0.9273789  0.9007424  ... 1.0028177  0.8601273  0.8141737 ]
 [0.8439988  0.95042145 0.9175758  ... 0.9359647  0.79367733 0.8555841 ]
 ...
 [0.8456931  0.91621876 0.89332163 ... 0.9338594  0.7803532  0.81185776]
 [0.8465594  0.86477584 0.8699997  ... 0.9539527  0.7960825  0.82105565]
 [0.85347813 0.879615   0.90020055 ... 0.94695467 0.7918603  0.8239201 ]]
Bob decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits addition: 3551
Total number of bits addition: 7168
Decryption accuracy addition: 49.539620535714285%
Bob decrypted multiplication: [[0.8474483  1.0008351  0.9385816  ... 0.9636018  0.8160037  0.8134867 ]
 [0.84767747 1.0004897  0.92108345 ... 1.055035   0.91042405 0.82652825]
 [0.85801935 1.010431   0.9463117  ... 0.9739215  0.8326184  0.8965247 ]
 ...
 [0.86937034 1.0181003  0.9384116  ... 0.95321834 0.8056264  0.82044184]
 [0.85705245 0.9894383  0.93234366 ... 0.9859492  0.8322946  0.8398069 ]
 [0.92935985 1.0493865  0.970337   ... 0.97635144 0.82855356 0.8428439 ]]
Bob decrypted bits multiplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits multiplication: 1818
Total number of bits multiplication: 7168
Decryption accuracy multiplication: 25.362723214285715%
Eve decrypted addition: [[1.005741   0.9975083  0.98430073 ... 0.9609038  0.93573433 1.0156578 ]
 [1.0517192  0.9860375  0.9408512  ... 1.0089288  0.91118795 1.0238761 ]
 [1.0742724  0.9940935  1.0213382  ... 0.97138447 0.89502    1.0292091 ]
 ...
 [1.0826395  0.99485654 0.97907513 ... 0.9919928  0.8681766  1.0451216 ]
 [1.061866   0.9946232  0.9303713  ... 0.99184763 1.0289112  1.019357  ]
 [1.0498142  0.9726647  0.957968   ... 1.009657   0.9573578  1.069672  ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 3551
Total number of bits addition: 7168
Decryption accuracy by Eve addition: 49.539620535714285%
Eve decrypted mulitplication: [[0.9746187  1.0157318  0.96777534 ... 0.94382054 0.9212163  1.0256371 ]
 [1.0597554  0.99359775 0.91207737 ... 1.0078237  0.88602227 1.0346751 ]
 [1.081319   0.9980921  0.99164575 ... 0.9742444  0.8565994  1.0582424 ]
 ...
 [1.0877106  1.002417   0.9677072  ... 1.0015951  0.8412111  1.0594356 ]
 [1.0724071  0.986171   0.8801699  ... 1.0066222  1.0536028  1.0370978 ]
 [1.0495352  0.97339517 0.94413936 ... 1.0164341  0.93637055 1.0943714 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 1818
Total number of bits mulitplication: 7168
Decryption accuracy by Eve mulitplication: 25.362723214285715%
Bob decrypted P1: [[0.84331185 0.99222493 0.9388842  ... 0.96178854 0.8018296  0.80998445]
 [0.84384537 0.99108577 0.9225364  ... 1.0523695  0.89440197 0.81919694]
 [0.84638727 0.99604994 0.9360066  ... 0.97281194 0.8148525  0.8806506 ]
 ...
 [0.8499619  0.9956732  0.9275645  ... 0.9543441  0.7948316  0.816036  ]
 [0.84609175 0.97536397 0.9272711  ... 0.98344684 0.8201729  0.83115566]
 [0.8753902  1.0094819  0.9568934  ... 0.9737559  0.8131348  0.83500624]]
Bob decrypted bits P1: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P1: 3593
Total number of bits P1: 7168
Decryption accuracy P1: 50.12555803571429%
Bob decrypted P2: [[0.84608966 0.9614964  0.92859626 ... 0.95069873 0.80340594 0.8115036 ]
 [0.8468921  0.95202005 0.90731037 ... 1.0348219  0.89070797 0.8219478 ]
 [0.852939   0.9730502  0.93548036 ... 0.95758784 0.8239954  0.88629127]
 ...
 [0.8598887  0.95816475 0.91948503 ... 0.94417775 0.79494107 0.81569904]
 [0.8545957  0.8986898  0.8892719  ... 0.9674951  0.8182816  0.8376274 ]
 [0.90345967 0.95242244 0.9261989  ... 0.96166587 0.8158244  0.8378292 ]]
Bob decrypted bits P2: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P2: 3594
Total number of bits P2: 7168
Decryption accuracy P2: 50.13950892857143%
