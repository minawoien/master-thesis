WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-16 13:00:29.143497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-16 13:00:29.217495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:09:00.0
2024-04-16 13:00:29.218190: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-16 13:00:29.221532: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-16 13:00:29.224360: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-16 13:00:29.225566: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-16 13:00:29.229382: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-16 13:00:29.232288: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-16 13:00:29.240537: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-16 13:00:29.245216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-16 13:00:29.245667: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-16 13:00:29.263480: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-16 13:00:29.266543: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4edb860 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-16 13:00:29.266585: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-16 13:00:29.470290: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4632430 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-16 13:00:29.470359: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-16 13:00:29.478124: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:09:00.0
2024-04-16 13:00:29.478298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-16 13:00:29.478340: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-16 13:00:29.478372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-16 13:00:29.478403: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-16 13:00:29.478433: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-16 13:00:29.478463: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-16 13:00:29.478495: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-16 13:00:29.487467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-16 13:00:29.487563: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-16 13:00:29.496258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-16 13:00:29.496282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-16 13:00:29.496290: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-16 13:00:29.500459: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-16 13:00:32.836977: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9216 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4580 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1467 - val_loss: 3.9261e-04
Epoch 4/512
448/448 - 0s - loss: 0.0224 - val_loss: 4.0255e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5914e-06
Epoch 6/512
448/448 - 0s - loss: 7.4120e-04 - val_loss: 5.8663e-06
Epoch 7/512
448/448 - 0s - loss: 5.3897e-04 - val_loss: 4.4209e-06
Epoch 8/512
448/448 - 0s - loss: 3.9909e-04 - val_loss: 3.1225e-06
Epoch 9/512
448/448 - 0s - loss: 2.7528e-04 - val_loss: 2.0270e-06
Epoch 10/512
448/448 - 0s - loss: 1.7352e-04 - val_loss: 1.1817e-06
Epoch 11/512
448/448 - 0s - loss: 9.7604e-05 - val_loss: 6.0266e-07
Epoch 12/512
448/448 - 0s - loss: 4.7642e-05 - val_loss: 2.5942e-07
Epoch 13/512
448/448 - 0s - loss: 1.9446e-05 - val_loss: 9.0003e-08
Epoch 14/512
448/448 - 0s - loss: 6.3276e-06 - val_loss: 2.3737e-08
Epoch 15/512
448/448 - 0s - loss: 1.5474e-06 - val_loss: 4.4267e-09
Epoch 16/512
448/448 - 0s - loss: 2.6939e-07 - val_loss: 2.1684e-09
Epoch 17/512
448/448 - 0s - loss: 1.6381e-05 - val_loss: 9.2735e-06
Epoch 18/512
448/448 - 0s - loss: 0.0053 - val_loss: 9.9167e-06
Epoch 19/512
448/448 - 0s - loss: 4.2267e-04 - val_loss: 4.0488e-07
Epoch 20/512
448/448 - 0s - loss: 3.5427e-05 - val_loss: 5.4234e-07
Epoch 21/512
448/448 - 0s - loss: 1.6023e-04 - val_loss: 1.0953e-05
Epoch 22/512
448/448 - 0s - loss: 0.0027 - val_loss: 1.9875e-05
Epoch 23/512
448/448 - 0s - loss: 0.0012 - val_loss: 3.1197e-06
Epoch 24/512
448/448 - 0s - loss: 3.0806e-04 - val_loss: 4.8056e-06
Epoch 25/512
448/448 - 0s - loss: 9.1550e-04 - val_loss: 2.0177e-05
Epoch 26/512
448/448 - 0s - loss: 0.0019 - val_loss: 9.4510e-06
Epoch 27/512
448/448 - 0s - loss: 7.4911e-04 - val_loss: 5.7710e-06
Epoch 28/512
448/448 - 0s - loss: 7.1225e-04 - val_loss: 1.1350e-05
Epoch 29/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.2726e-05
Epoch 30/512
448/448 - 0s - loss: 0.0011 - val_loss: 7.4928e-06
Epoch 31/512
448/448 - 0s - loss: 7.6084e-04 - val_loss: 8.7661e-06
Epoch 32/512
448/448 - 0s - loss: 0.0010 - val_loss: 1.1538e-05
Epoch 33/512
448/448 - 0s - loss: 0.0011 - val_loss: 9.2251e-06
Epoch 34/512
448/448 - 0s - loss: 8.8877e-04 - val_loss: 8.2604e-06
Epoch 35/512
448/448 - 0s - loss: 8.9777e-04 - val_loss: 9.6578e-06
Epoch 36/512
448/448 - 0s - loss: 0.0010 - val_loss: 9.5059e-06
Epoch 37/512
448/448 - 0s - loss: 9.3858e-04 - val_loss: 8.5101e-06
Epoch 38/512
448/448 - 0s - loss: 8.7255e-04 - val_loss: 8.8992e-06
Epoch 39/512
448/448 - 0s - loss: 9.2828e-04 - val_loss: 9.1114e-06
Epoch 40/512
448/448 - 0s - loss: 9.1422e-04 - val_loss: 8.6132e-06
Epoch 41/512
448/448 - 0s - loss: 8.6940e-04 - val_loss: 8.5711e-06
Epoch 42/512
448/448 - 0s - loss: 8.8135e-04 - val_loss: 8.5728e-06
Epoch 43/512
448/448 - 0s - loss: 8.6548e-04 - val_loss: 8.5552e-06
Epoch 44/512
448/448 - 0s - loss: 8.5924e-04 - val_loss: 8.4436e-06
Epoch 45/512
448/448 - 0s - loss: 8.4852e-04 - val_loss: 8.1771e-06
Epoch 46/512
448/448 - 0s - loss: 8.2818e-04 - val_loss: 8.1425e-06
Epoch 47/512
448/448 - 0s - loss: 8.2351e-04 - val_loss: 8.1909e-06
Epoch 48/512
448/448 - 0s - loss: 8.2838e-04 - val_loss: 7.8661e-06
Epoch 49/512
448/448 - 0s - loss: 7.9863e-04 - val_loss: 7.6467e-06
Epoch 50/512
448/448 - 0s - loss: 7.8251e-04 - val_loss: 7.9659e-06
Epoch 51/512
448/448 - 0s - loss: 8.0960e-04 - val_loss: 7.7010e-06
Epoch 52/512
448/448 - 0s - loss: 7.6659e-04 - val_loss: 7.5748e-06
Epoch 53/512
448/448 - 0s - loss: 7.6851e-04 - val_loss: 7.4901e-06
Epoch 54/512
448/448 - 0s - loss: 7.5983e-04 - val_loss: 7.5106e-06
Epoch 55/512
448/448 - 0s - loss: 7.5876e-04 - val_loss: 7.3363e-06
Epoch 56/512
448/448 - 0s - loss: 7.3836e-04 - val_loss: 7.3447e-06
Epoch 57/512
448/448 - 0s - loss: 7.4179e-04 - val_loss: 7.2417e-06
Epoch 58/512
448/448 - 0s - loss: 7.2683e-04 - val_loss: 7.0756e-06
Epoch 59/512
448/448 - 0s - loss: 7.1444e-04 - val_loss: 7.0848e-06
Epoch 60/512
448/448 - 0s - loss: 7.1906e-04 - val_loss: 7.0162e-06
Epoch 61/512
448/448 - 0s - loss: 7.0611e-04 - val_loss: 6.8470e-06
Epoch 62/512
448/448 - 0s - loss: 6.9370e-04 - val_loss: 6.8220e-06
Epoch 63/512
448/448 - 0s - loss: 6.9297e-04 - val_loss: 6.8271e-06
Epoch 64/512
448/448 - 0s - loss: 6.8779e-04 - val_loss: 6.6259e-06
Epoch 65/512
448/448 - 0s - loss: 6.6433e-04 - val_loss: 6.8625e-06
Epoch 66/512
448/448 - 0s - loss: 6.8844e-04 - val_loss: 6.6606e-06
Epoch 67/512
448/448 - 0s - loss: 6.5699e-04 - val_loss: 6.4728e-06
Epoch 68/512
448/448 - 0s - loss: 6.5667e-04 - val_loss: 6.3549e-06
Epoch 69/512
448/448 - 0s - loss: 6.4457e-04 - val_loss: 6.4339e-06
Epoch 70/512
448/448 - 0s - loss: 6.5261e-04 - val_loss: 6.2440e-06
Epoch 71/512
448/448 - 0s - loss: 6.2568e-04 - val_loss: 6.3515e-06
Epoch 72/512
448/448 - 0s - loss: 6.4296e-04 - val_loss: 6.2153e-06
Epoch 73/512
448/448 - 0s - loss: 6.2088e-04 - val_loss: 6.0836e-06
Epoch 74/512
448/448 - 0s - loss: 6.1569e-04 - val_loss: 6.0997e-06
Epoch 75/512
448/448 - 0s - loss: 6.1281e-04 - val_loss: 6.1318e-06
Epoch 76/512
448/448 - 0s - loss: 6.1578e-04 - val_loss: 5.8580e-06
Epoch 77/512
448/448 - 0s - loss: 5.8893e-04 - val_loss: 5.8887e-06
Epoch 78/512
448/448 - 0s - loss: 5.9428e-04 - val_loss: 6.1083e-06
Epoch 79/512
448/448 - 0s - loss: 6.1334e-04 - val_loss: 5.6918e-06
Epoch 80/512
448/448 - 0s - loss: 5.6814e-04 - val_loss: 5.5395e-06
Epoch 81/512
448/448 - 0s - loss: 5.6771e-04 - val_loss: 5.8622e-06
Epoch 82/512
448/448 - 0s - loss: 5.9162e-04 - val_loss: 5.7299e-06
Epoch 83/512
448/448 - 0s - loss: 5.6359e-04 - val_loss: 5.5960e-06
Epoch 84/512
448/448 - 0s - loss: 5.6169e-04 - val_loss: 5.6035e-06
Epoch 85/512
448/448 - 0s - loss: 5.6293e-04 - val_loss: 5.4941e-06
Epoch 86/512
448/448 - 0s - loss: 5.5116e-04 - val_loss: 5.3908e-06
Epoch 87/512
448/448 - 0s - loss: 5.4307e-04 - val_loss: 5.4300e-06
Epoch 88/512
448/448 - 0s - loss: 5.5011e-04 - val_loss: 5.3295e-06
Epoch 89/512
448/448 - 0s - loss: 5.3499e-04 - val_loss: 5.2594e-06
Epoch 90/512
448/448 - 0s - loss: 5.2948e-04 - val_loss: 5.3399e-06
Epoch 91/512
448/448 - 0s - loss: 5.3600e-04 - val_loss: 5.2596e-06
Epoch 92/512
448/448 - 0s - loss: 5.2476e-04 - val_loss: 5.0864e-06
Epoch 93/512
448/448 - 0s - loss: 5.1046e-04 - val_loss: 5.1123e-06
Epoch 94/512
448/448 - 0s - loss: 5.1654e-04 - val_loss: 5.2397e-06
Epoch 95/512
448/448 - 0s - loss: 5.2105e-04 - val_loss: 4.9064e-06
Epoch 96/512
448/448 - 0s - loss: 4.9130e-04 - val_loss: 4.8650e-06
Epoch 97/512
448/448 - 0s - loss: 4.9690e-04 - val_loss: 5.0776e-06
Epoch 98/512
448/448 - 0s - loss: 5.0681e-04 - val_loss: 5.0238e-06
Epoch 99/512
448/448 - 0s - loss: 4.9440e-04 - val_loss: 4.7662e-06
Epoch 100/512
448/448 - 0s - loss: 4.7887e-04 - val_loss: 4.7748e-06
Epoch 101/512
448/448 - 0s - loss: 4.8181e-04 - val_loss: 4.9235e-06
Epoch 102/512
448/448 - 0s - loss: 4.9094e-04 - val_loss: 4.7432e-06
Epoch 103/512
448/448 - 0s - loss: 4.6868e-04 - val_loss: 4.6344e-06
Epoch 104/512
448/448 - 0s - loss: 4.6603e-04 - val_loss: 4.7447e-06
Epoch 105/512
448/448 - 0s - loss: 4.7633e-04 - val_loss: 4.6155e-06
Epoch 106/512
448/448 - 0s - loss: 4.5780e-04 - val_loss: 4.5694e-06
Epoch 107/512
448/448 - 0s - loss: 4.5863e-04 - val_loss: 4.5625e-06
Epoch 108/512
448/448 - 0s - loss: 4.5508e-04 - val_loss: 4.5228e-06
Epoch 109/512
448/448 - 0s - loss: 4.5321e-04 - val_loss: 4.3680e-06
Epoch 110/512
448/448 - 0s - loss: 4.3700e-04 - val_loss: 4.4704e-06
Epoch 111/512
448/448 - 0s - loss: 4.5089e-04 - val_loss: 4.3656e-06
Epoch 112/512
448/448 - 0s - loss: 4.3615e-04 - val_loss: 4.2338e-06
Epoch 113/512
448/448 - 0s - loss: 4.2756e-04 - val_loss: 4.3370e-06
Epoch 114/512
448/448 - 0s - loss: 4.4003e-04 - val_loss: 4.2119e-06
Epoch 115/512
448/448 - 0s - loss: 4.2089e-04 - val_loss: 4.1994e-06
Epoch 116/512
448/448 - 0s - loss: 4.2453e-04 - val_loss: 4.1723e-06
Epoch 117/512
448/448 - 0s - loss: 4.1648e-04 - val_loss: 4.2138e-06
Epoch 118/512
448/448 - 0s - loss: 4.2238e-04 - val_loss: 4.0678e-06
Epoch 119/512
448/448 - 0s - loss: 4.0552e-04 - val_loss: 4.0555e-06
Epoch 120/512
448/448 - 0s - loss: 4.0913e-04 - val_loss: 4.1393e-06
Epoch 121/512
448/448 - 0s - loss: 4.1107e-04 - val_loss: 3.9932e-06
Epoch 122/512
448/448 - 0s - loss: 3.9885e-04 - val_loss: 3.8380e-06
Epoch 123/512
448/448 - 0s - loss: 3.8707e-04 - val_loss: 4.0130e-06
Epoch 124/512
448/448 - 0s - loss: 4.0302e-04 - val_loss: 3.9949e-06
Epoch 125/512
448/448 - 0s - loss: 3.9436e-04 - val_loss: 3.7546e-06
Epoch 126/512
448/448 - 0s - loss: 3.7434e-04 - val_loss: 3.8585e-06
Epoch 127/512
448/448 - 0s - loss: 3.8880e-04 - val_loss: 3.9124e-06
Epoch 128/512
448/448 - 0s - loss: 3.8682e-04 - val_loss: 3.6727e-06
Epoch 129/512
448/448 - 0s - loss: 3.6434e-04 - val_loss: 3.7390e-06
Epoch 130/512
448/448 - 0s - loss: 3.7826e-04 - val_loss: 3.7614e-06
Epoch 131/512
448/448 - 0s - loss: 3.7230e-04 - val_loss: 3.6320e-06
Epoch 132/512
448/448 - 0s - loss: 3.6372e-04 - val_loss: 3.5470e-06
Epoch 133/512
448/448 - 0s - loss: 3.5903e-04 - val_loss: 3.6320e-06
Epoch 134/512
448/448 - 0s - loss: 3.6432e-04 - val_loss: 3.5803e-06
Epoch 135/512
448/448 - 0s - loss: 3.5493e-04 - val_loss: 3.5273e-06
Epoch 136/512
448/448 - 0s - loss: 3.5189e-04 - val_loss: 3.5153e-06
Epoch 137/512
448/448 - 0s - loss: 3.4946e-04 - val_loss: 3.5248e-06
Epoch 138/512
448/448 - 0s - loss: 3.5098e-04 - val_loss: 3.3886e-06
Epoch 139/512
448/448 - 0s - loss: 3.3766e-04 - val_loss: 3.3754e-06
Epoch 140/512
448/448 - 0s - loss: 3.3978e-04 - val_loss: 3.4237e-06
Epoch 141/512
448/448 - 0s - loss: 3.3946e-04 - val_loss: 3.3799e-06
Epoch 142/512
448/448 - 0s - loss: 3.3622e-04 - val_loss: 3.2454e-06
Epoch 143/512
448/448 - 0s - loss: 3.2318e-04 - val_loss: 3.2750e-06
Epoch 144/512
448/448 - 0s - loss: 3.2938e-04 - val_loss: 3.3127e-06
Epoch 145/512
448/448 - 0s - loss: 3.2705e-04 - val_loss: 3.2252e-06
Epoch 146/512
448/448 - 0s - loss: 3.1855e-04 - val_loss: 3.1773e-06
Epoch 147/512
448/448 - 0s - loss: 3.1789e-04 - val_loss: 3.1293e-06
Epoch 148/512
448/448 - 0s - loss: 3.1388e-04 - val_loss: 3.1275e-06
Epoch 149/512
448/448 - 0s - loss: 3.1229e-04 - val_loss: 3.1333e-06
Epoch 150/512
448/448 - 0s - loss: 3.1109e-04 - val_loss: 3.0679e-06
Epoch 151/512
448/448 - 0s - loss: 3.0323e-04 - val_loss: 3.0607e-06
Epoch 152/512
448/448 - 0s - loss: 3.0425e-04 - val_loss: 3.0395e-06
Epoch 153/512
448/448 - 0s - loss: 3.0211e-04 - val_loss: 2.9174e-06
Epoch 154/512
448/448 - 0s - loss: 2.9033e-04 - val_loss: 3.0472e-06
Epoch 155/512
448/448 - 0s - loss: 3.0479e-04 - val_loss: 2.9094e-06
Epoch 156/512
448/448 - 0s - loss: 2.8480e-04 - val_loss: 2.8809e-06
Epoch 157/512
448/448 - 0s - loss: 2.8930e-04 - val_loss: 2.9001e-06
Epoch 158/512
448/448 - 0s - loss: 2.8624e-04 - val_loss: 2.9012e-06
Epoch 159/512
448/448 - 0s - loss: 2.8661e-04 - val_loss: 2.7885e-06
Epoch 160/512
448/448 - 0s - loss: 2.7690e-04 - val_loss: 2.7370e-06
Epoch 161/512
448/448 - 0s - loss: 2.7476e-04 - val_loss: 2.7471e-06
Epoch 162/512
448/448 - 0s - loss: 2.7587e-04 - val_loss: 2.7798e-06
Epoch 163/512
448/448 - 0s - loss: 2.7532e-04 - val_loss: 2.6589e-06
Epoch 164/512
448/448 - 0s - loss: 2.6344e-04 - val_loss: 2.6516e-06
Epoch 165/512
448/448 - 0s - loss: 2.6796e-04 - val_loss: 2.6443e-06
Epoch 166/512
448/448 - 0s - loss: 2.6421e-04 - val_loss: 2.5972e-06
Epoch 167/512
448/448 - 0s - loss: 2.5824e-04 - val_loss: 2.6174e-06
Epoch 168/512
448/448 - 0s - loss: 2.6103e-04 - val_loss: 2.5720e-06
Epoch 169/512
448/448 - 0s - loss: 2.5419e-04 - val_loss: 2.5388e-06
Epoch 170/512
448/448 - 0s - loss: 2.5404e-04 - val_loss: 2.4466e-06
Epoch 171/512
448/448 - 0s - loss: 2.4453e-04 - val_loss: 2.5229e-06
Epoch 172/512
448/448 - 0s - loss: 2.5267e-04 - val_loss: 2.4966e-06
Epoch 173/512
448/448 - 0s - loss: 2.4457e-04 - val_loss: 2.4344e-06
Epoch 174/512
448/448 - 0s - loss: 2.4067e-04 - val_loss: 2.4039e-06
Epoch 175/512
448/448 - 0s - loss: 2.3963e-04 - val_loss: 2.3941e-06
Epoch 176/512
448/448 - 0s - loss: 2.3816e-04 - val_loss: 2.3487e-06
Epoch 177/512
448/448 - 0s - loss: 2.3263e-04 - val_loss: 2.3494e-06
Epoch 178/512
448/448 - 0s - loss: 2.3313e-04 - val_loss: 2.3276e-06
Epoch 179/512
448/448 - 0s - loss: 2.2914e-04 - val_loss: 2.3045e-06
Epoch 180/512
448/448 - 0s - loss: 2.2840e-04 - val_loss: 2.2322e-06
Epoch 181/512
448/448 - 0s - loss: 2.2240e-04 - val_loss: 2.2045e-06
Epoch 182/512
448/448 - 0s - loss: 2.2168e-04 - val_loss: 2.2367e-06
Epoch 183/512
448/448 - 0s - loss: 2.2092e-04 - val_loss: 2.2296e-06
Epoch 184/512
448/448 - 0s - loss: 2.1983e-04 - val_loss: 2.1098e-06
Epoch 185/512
448/448 - 0s - loss: 2.0930e-04 - val_loss: 2.1399e-06
Epoch 186/512
448/448 - 0s - loss: 2.1404e-04 - val_loss: 2.1321e-06
Epoch 187/512
448/448 - 0s - loss: 2.1128e-04 - val_loss: 2.0500e-06
Epoch 188/512
448/448 - 0s - loss: 2.0347e-04 - val_loss: 2.0681e-06
Epoch 189/512
448/448 - 0s - loss: 2.0690e-04 - val_loss: 2.0724e-06
Epoch 190/512
448/448 - 0s - loss: 2.0479e-04 - val_loss: 1.9938e-06
Epoch 191/512
448/448 - 0s - loss: 1.9792e-04 - val_loss: 1.9424e-06
Epoch 192/512
448/448 - 0s - loss: 1.9589e-04 - val_loss: 1.9704e-06
Epoch 193/512
448/448 - 0s - loss: 1.9656e-04 - val_loss: 1.9925e-06
Epoch 194/512
448/448 - 0s - loss: 1.9681e-04 - val_loss: 1.8868e-06
Epoch 195/512
448/448 - 0s - loss: 1.8635e-04 - val_loss: 1.8802e-06
Epoch 196/512
448/448 - 0s - loss: 1.8925e-04 - val_loss: 1.8825e-06
Epoch 197/512
448/448 - 0s - loss: 1.8724e-04 - val_loss: 1.8482e-06
Epoch 198/512
448/448 - 0s - loss: 1.8288e-04 - val_loss: 1.8320e-06
Epoch 199/512
448/448 - 0s - loss: 1.8336e-04 - val_loss: 1.7793e-06
Epoch 200/512
448/448 - 0s - loss: 1.7750e-04 - val_loss: 1.7535e-06
Epoch 201/512
448/448 - 0s - loss: 1.7602e-04 - val_loss: 1.7707e-06
Epoch 202/512
448/448 - 0s - loss: 1.7581e-04 - val_loss: 1.7658e-06
Epoch 203/512
448/448 - 0s - loss: 1.7350e-04 - val_loss: 1.7344e-06
Epoch 204/512
448/448 - 0s - loss: 1.7116e-04 - val_loss: 1.6692e-06
Epoch 205/512
448/448 - 0s - loss: 1.6505e-04 - val_loss: 1.6861e-06
Epoch 206/512
448/448 - 0s - loss: 1.6792e-04 - val_loss: 1.6541e-06
Epoch 207/512
448/448 - 0s - loss: 1.6351e-04 - val_loss: 1.6113e-06
Epoch 208/512
448/448 - 0s - loss: 1.6014e-04 - val_loss: 1.6205e-06
Epoch 209/512
448/448 - 0s - loss: 1.6178e-04 - val_loss: 1.5778e-06
Epoch 210/512
448/448 - 0s - loss: 1.5581e-04 - val_loss: 1.5544e-06
Epoch 211/512
448/448 - 0s - loss: 1.5424e-04 - val_loss: 1.5653e-06
Epoch 212/512
448/448 - 0s - loss: 1.5508e-04 - val_loss: 1.5261e-06
Epoch 213/512
448/448 - 0s - loss: 1.5028e-04 - val_loss: 1.4797e-06
Epoch 214/512
448/448 - 0s - loss: 1.4617e-04 - val_loss: 1.5032e-06
Epoch 215/512
448/448 - 0s - loss: 1.4856e-04 - val_loss: 1.4789e-06
Epoch 216/512
448/448 - 0s - loss: 1.4502e-04 - val_loss: 1.4077e-06
Epoch 217/512
448/448 - 0s - loss: 1.3962e-04 - val_loss: 1.4126e-06
Epoch 218/512
448/448 - 0s - loss: 1.4055e-04 - val_loss: 1.4304e-06
Epoch 219/512
448/448 - 0s - loss: 1.4156e-04 - val_loss: 1.3316e-06
Epoch 220/512
448/448 - 0s - loss: 1.3080e-04 - val_loss: 1.3665e-06
Epoch 221/512
448/448 - 0s - loss: 1.3672e-04 - val_loss: 1.3650e-06
Epoch 222/512
448/448 - 0s - loss: 1.3374e-04 - val_loss: 1.2792e-06
Epoch 223/512
448/448 - 0s - loss: 1.2603e-04 - val_loss: 1.2953e-06
Epoch 224/512
448/448 - 0s - loss: 1.2880e-04 - val_loss: 1.3376e-06
Epoch 225/512
448/448 - 0s - loss: 1.2914e-04 - val_loss: 1.2757e-06
Epoch 226/512
448/448 - 0s - loss: 1.2331e-04 - val_loss: 1.2128e-06
Epoch 227/512
448/448 - 0s - loss: 1.1890e-04 - val_loss: 1.2595e-06
Epoch 228/512
448/448 - 0s - loss: 1.2436e-04 - val_loss: 1.2107e-06
Epoch 229/512
448/448 - 0s - loss: 1.1742e-04 - val_loss: 1.1309e-06
Epoch 230/512
448/448 - 0s - loss: 1.1231e-04 - val_loss: 1.1744e-06
Epoch 231/512
448/448 - 0s - loss: 1.1725e-04 - val_loss: 1.1652e-06
Epoch 232/512
448/448 - 0s - loss: 1.1274e-04 - val_loss: 1.1090e-06
Epoch 233/512
448/448 - 0s - loss: 1.1014e-04 - val_loss: 1.0546e-06
Epoch 234/512
448/448 - 0s - loss: 1.0492e-04 - val_loss: 1.1091e-06
Epoch 235/512
448/448 - 0s - loss: 1.0974e-04 - val_loss: 1.1018e-06
Epoch 236/512
448/448 - 0s - loss: 1.0657e-04 - val_loss: 1.0143e-06
Epoch 237/512
448/448 - 0s - loss: 9.9455e-05 - val_loss: 1.0206e-06
Epoch 238/512
448/448 - 0s - loss: 1.0159e-04 - val_loss: 1.0471e-06
Epoch 239/512
448/448 - 0s - loss: 1.0245e-04 - val_loss: 9.6670e-07
Epoch 240/512
448/448 - 0s - loss: 9.4119e-05 - val_loss: 9.7396e-07
Epoch 241/512
448/448 - 0s - loss: 9.7027e-05 - val_loss: 9.8385e-07
Epoch 242/512
448/448 - 0s - loss: 9.5706e-05 - val_loss: 9.2415e-07
Epoch 243/512
448/448 - 0s - loss: 9.0341e-05 - val_loss: 8.9982e-07
Epoch 244/512
448/448 - 0s - loss: 8.9744e-05 - val_loss: 9.2053e-07
Epoch 245/512
448/448 - 0s - loss: 9.0193e-05 - val_loss: 9.0380e-07
Epoch 246/512
448/448 - 0s - loss: 8.7597e-05 - val_loss: 8.5935e-07
Epoch 247/512
448/448 - 0s - loss: 8.3703e-05 - val_loss: 8.6182e-07
Epoch 248/512
448/448 - 0s - loss: 8.4368e-05 - val_loss: 8.6765e-07
Epoch 249/512
448/448 - 0s - loss: 8.3864e-05 - val_loss: 8.0098e-07
Epoch 250/512
448/448 - 0s - loss: 7.8220e-05 - val_loss: 7.8213e-07
Epoch 251/512
448/448 - 0s - loss: 7.7986e-05 - val_loss: 8.1881e-07
Epoch 252/512
448/448 - 0s - loss: 8.0468e-05 - val_loss: 7.5392e-07
Epoch 253/512
448/448 - 0s - loss: 7.2770e-05 - val_loss: 7.4070e-07
Epoch 254/512
448/448 - 0s - loss: 7.4147e-05 - val_loss: 7.5618e-07
Epoch 255/512
448/448 - 0s - loss: 7.3545e-05 - val_loss: 7.3154e-07
Epoch 256/512
448/448 - 0s - loss: 7.0883e-05 - val_loss: 6.9262e-07
Epoch 257/512
448/448 - 0s - loss: 6.8594e-05 - val_loss: 6.8130e-07
Epoch 258/512
448/448 - 0s - loss: 6.7625e-05 - val_loss: 6.9086e-07
Epoch 259/512
448/448 - 0s - loss: 6.7756e-05 - val_loss: 6.6607e-07
Epoch 260/512
448/448 - 0s - loss: 6.4562e-05 - val_loss: 6.4241e-07
Epoch 261/512
448/448 - 0s - loss: 6.3175e-05 - val_loss: 6.3612e-07
Epoch 262/512
448/448 - 0s - loss: 6.2218e-05 - val_loss: 6.3214e-07
Epoch 263/512
448/448 - 0s - loss: 6.1641e-05 - val_loss: 5.9775e-07
Epoch 264/512
448/448 - 0s - loss: 5.8381e-05 - val_loss: 5.9042e-07
Epoch 265/512
448/448 - 0s - loss: 5.8040e-05 - val_loss: 5.9533e-07
Epoch 266/512
448/448 - 0s - loss: 5.7921e-05 - val_loss: 5.6464e-07
Epoch 267/512
448/448 - 0s - loss: 5.4920e-05 - val_loss: 5.3608e-07
Epoch 268/512
448/448 - 0s - loss: 5.2716e-05 - val_loss: 5.5621e-07
Epoch 269/512
448/448 - 0s - loss: 5.4565e-05 - val_loss: 5.3710e-07
Epoch 270/512
448/448 - 0s - loss: 5.1539e-05 - val_loss: 4.9942e-07
Epoch 271/512
448/448 - 0s - loss: 4.9301e-05 - val_loss: 4.8683e-07
Epoch 272/512
448/448 - 0s - loss: 4.8082e-05 - val_loss: 5.1378e-07
Epoch 273/512
448/448 - 0s - loss: 4.9920e-05 - val_loss: 4.8255e-07
Epoch 274/512
448/448 - 0s - loss: 4.6039e-05 - val_loss: 4.4616e-07
Epoch 275/512
448/448 - 0s - loss: 4.3876e-05 - val_loss: 4.6678e-07
Epoch 276/512
448/448 - 0s - loss: 4.6075e-05 - val_loss: 4.4898e-07
Epoch 277/512
448/448 - 0s - loss: 4.3205e-05 - val_loss: 4.0959e-07
Epoch 278/512
448/448 - 0s - loss: 4.0351e-05 - val_loss: 4.1973e-07
Epoch 279/512
448/448 - 0s - loss: 4.1729e-05 - val_loss: 4.2363e-07
Epoch 280/512
448/448 - 0s - loss: 4.0603e-05 - val_loss: 3.9199e-07
Epoch 281/512
448/448 - 0s - loss: 3.7477e-05 - val_loss: 3.9595e-07
Epoch 282/512
448/448 - 0s - loss: 3.8840e-05 - val_loss: 3.8151e-07
Epoch 283/512
448/448 - 0s - loss: 3.6362e-05 - val_loss: 3.6858e-07
Epoch 284/512
448/448 - 0s - loss: 3.5649e-05 - val_loss: 3.6453e-07
Epoch 285/512
448/448 - 0s - loss: 3.5055e-05 - val_loss: 3.5102e-07
Epoch 286/512
448/448 - 0s - loss: 3.3693e-05 - val_loss: 3.3789e-07
Epoch 287/512
448/448 - 0s - loss: 3.3019e-05 - val_loss: 3.2420e-07
Epoch 288/512
448/448 - 0s - loss: 3.1759e-05 - val_loss: 3.1567e-07
Epoch 289/512
448/448 - 0s - loss: 3.0760e-05 - val_loss: 3.1810e-07
Epoch 290/512
448/448 - 0s - loss: 3.1023e-05 - val_loss: 2.9608e-07
Epoch 291/512
448/448 - 0s - loss: 2.8630e-05 - val_loss: 2.8639e-07
Epoch 292/512
448/448 - 0s - loss: 2.8412e-05 - val_loss: 2.8490e-07
Epoch 293/512
448/448 - 0s - loss: 2.7823e-05 - val_loss: 2.7935e-07
Epoch 294/512
448/448 - 0s - loss: 2.7018e-05 - val_loss: 2.6806e-07
Epoch 295/512
448/448 - 0s - loss: 2.6023e-05 - val_loss: 2.5699e-07
Epoch 296/512
448/448 - 0s - loss: 2.5187e-05 - val_loss: 2.4945e-07
Epoch 297/512
448/448 - 0s - loss: 2.4151e-05 - val_loss: 2.5398e-07
Epoch 298/512
448/448 - 0s - loss: 2.4845e-05 - val_loss: 2.3315e-07
Epoch 299/512
448/448 - 0s - loss: 2.2289e-05 - val_loss: 2.2422e-07
Epoch 300/512
448/448 - 0s - loss: 2.2180e-05 - val_loss: 2.3194e-07
Epoch 301/512
448/448 - 0s - loss: 2.2414e-05 - val_loss: 2.1941e-07
Epoch 302/512
448/448 - 0s - loss: 2.1002e-05 - val_loss: 2.0180e-07
Epoch 303/512
448/448 - 0s - loss: 1.9612e-05 - val_loss: 2.0871e-07
Epoch 304/512
448/448 - 0s - loss: 2.0331e-05 - val_loss: 2.0557e-07
Epoch 305/512
448/448 - 0s - loss: 1.9397e-05 - val_loss: 1.8676e-07
Epoch 306/512
448/448 - 0s - loss: 1.7980e-05 - val_loss: 1.8063e-07
Epoch 307/512
448/448 - 0s - loss: 1.7722e-05 - val_loss: 1.8657e-07
Epoch 308/512
448/448 - 0s - loss: 1.7950e-05 - val_loss: 1.7632e-07
Epoch 309/512
448/448 - 0s - loss: 1.6561e-05 - val_loss: 1.6633e-07
Epoch 310/512
448/448 - 0s - loss: 1.6116e-05 - val_loss: 1.6206e-07
Epoch 311/512
448/448 - 0s - loss: 1.5724e-05 - val_loss: 1.5820e-07
Epoch 312/512
448/448 - 0s - loss: 1.5208e-05 - val_loss: 1.5387e-07
Epoch 313/512
448/448 - 0s - loss: 1.4873e-05 - val_loss: 1.4394e-07
Epoch 314/512
448/448 - 0s - loss: 1.3893e-05 - val_loss: 1.4122e-07
Epoch 315/512
448/448 - 0s - loss: 1.3791e-05 - val_loss: 1.3877e-07
Epoch 316/512
448/448 - 0s - loss: 1.3288e-05 - val_loss: 1.3519e-07
Epoch 317/512
448/448 - 0s - loss: 1.2960e-05 - val_loss: 1.2727e-07
Epoch 318/512
448/448 - 0s - loss: 1.2202e-05 - val_loss: 1.2431e-07
Epoch 319/512
448/448 - 0s - loss: 1.2014e-05 - val_loss: 1.2035e-07
Epoch 320/512
448/448 - 0s - loss: 1.1515e-05 - val_loss: 1.1633e-07
Epoch 321/512
448/448 - 0s - loss: 1.1186e-05 - val_loss: 1.1141e-07
Epoch 322/512
448/448 - 0s - loss: 1.0682e-05 - val_loss: 1.0862e-07
Epoch 323/512
448/448 - 0s - loss: 1.0380e-05 - val_loss: 1.0594e-07
Epoch 324/512
448/448 - 0s - loss: 1.0032e-05 - val_loss: 1.0239e-07
Epoch 325/512
448/448 - 0s - loss: 9.7989e-06 - val_loss: 9.4352e-08
Epoch 326/512
448/448 - 0s - loss: 9.0205e-06 - val_loss: 9.3176e-08
Epoch 327/512
448/448 - 0s - loss: 9.0756e-06 - val_loss: 9.2371e-08
Epoch 328/512
448/448 - 0s - loss: 8.7608e-06 - val_loss: 8.6907e-08
Epoch 329/512
448/448 - 0s - loss: 8.2703e-06 - val_loss: 8.1782e-08
Epoch 330/512
448/448 - 0s - loss: 7.9109e-06 - val_loss: 8.1052e-08
Epoch 331/512
448/448 - 0s - loss: 7.8377e-06 - val_loss: 7.9358e-08
Epoch 332/512
448/448 - 0s - loss: 7.5279e-06 - val_loss: 7.4402e-08
Epoch 333/512
448/448 - 0s - loss: 7.1249e-06 - val_loss: 7.0678e-08
Epoch 334/512
448/448 - 0s - loss: 6.8596e-06 - val_loss: 6.9559e-08
Epoch 335/512
448/448 - 0s - loss: 6.7044e-06 - val_loss: 6.7956e-08
Epoch 336/512
448/448 - 0s - loss: 6.4522e-06 - val_loss: 6.4330e-08
Epoch 337/512
448/448 - 0s - loss: 6.1355e-06 - val_loss: 6.1694e-08
Epoch 338/512
448/448 - 0s - loss: 5.9089e-06 - val_loss: 6.0442e-08
Epoch 339/512
448/448 - 0s - loss: 5.8093e-06 - val_loss: 5.7242e-08
Epoch 340/512
448/448 - 0s - loss: 5.4495e-06 - val_loss: 5.4065e-08
Epoch 341/512
448/448 - 0s - loss: 5.2160e-06 - val_loss: 5.4085e-08
Epoch 342/512
448/448 - 0s - loss: 5.1648e-06 - val_loss: 5.2434e-08
Epoch 343/512
448/448 - 0s - loss: 4.9361e-06 - val_loss: 4.8581e-08
Epoch 344/512
448/448 - 0s - loss: 4.6207e-06 - val_loss: 4.6550e-08
Epoch 345/512
448/448 - 0s - loss: 4.4834e-06 - val_loss: 4.6424e-08
Epoch 346/512
448/448 - 0s - loss: 4.4621e-06 - val_loss: 4.2799e-08
Epoch 347/512
448/448 - 0s - loss: 4.0822e-06 - val_loss: 4.0541e-08
Epoch 348/512
448/448 - 0s - loss: 3.9608e-06 - val_loss: 4.1140e-08
Epoch 349/512
448/448 - 0s - loss: 3.9195e-06 - val_loss: 4.0501e-08
Epoch 350/512
448/448 - 0s - loss: 3.7936e-06 - val_loss: 3.6010e-08
Epoch 351/512
448/448 - 0s - loss: 3.3848e-06 - val_loss: 3.5486e-08
Epoch 352/512
448/448 - 0s - loss: 3.4711e-06 - val_loss: 3.5202e-08
Epoch 353/512
448/448 - 0s - loss: 3.3018e-06 - val_loss: 3.2845e-08
Epoch 354/512
448/448 - 0s - loss: 3.1056e-06 - val_loss: 3.1240e-08
Epoch 355/512
448/448 - 0s - loss: 2.9864e-06 - val_loss: 3.0873e-08
Epoch 356/512
448/448 - 0s - loss: 2.9086e-06 - val_loss: 3.0271e-08
Epoch 357/512
448/448 - 0s - loss: 2.8626e-06 - val_loss: 2.6737e-08
Epoch 358/512
448/448 - 0s - loss: 2.5236e-06 - val_loss: 2.6657e-08
Epoch 359/512
448/448 - 0s - loss: 2.6030e-06 - val_loss: 2.6680e-08
Epoch 360/512
448/448 - 0s - loss: 2.5067e-06 - val_loss: 2.4221e-08
Epoch 361/512
448/448 - 0s - loss: 2.2736e-06 - val_loss: 2.3534e-08
Epoch 362/512
448/448 - 0s - loss: 2.2459e-06 - val_loss: 2.3856e-08
Epoch 363/512
448/448 - 0s - loss: 2.2315e-06 - val_loss: 2.1708e-08
Epoch 364/512
448/448 - 0s - loss: 2.0365e-06 - val_loss: 1.9917e-08
Epoch 365/512
448/448 - 0s - loss: 1.9033e-06 - val_loss: 2.0728e-08
Epoch 366/512
448/448 - 0s - loss: 1.9950e-06 - val_loss: 1.9528e-08
Epoch 367/512
448/448 - 0s - loss: 1.8159e-06 - val_loss: 1.7169e-08
Epoch 368/512
448/448 - 0s - loss: 1.6389e-06 - val_loss: 1.7902e-08
Epoch 369/512
448/448 - 0s - loss: 1.7280e-06 - val_loss: 1.7916e-08
Epoch 370/512
448/448 - 0s - loss: 1.6432e-06 - val_loss: 1.5663e-08
Epoch 371/512
448/448 - 0s - loss: 1.4669e-06 - val_loss: 1.4925e-08
Epoch 372/512
448/448 - 0s - loss: 1.4490e-06 - val_loss: 1.5377e-08
Epoch 373/512
448/448 - 0s - loss: 1.4589e-06 - val_loss: 1.4034e-08
Epoch 374/512
448/448 - 0s - loss: 1.2974e-06 - val_loss: 1.3265e-08
Epoch 375/512
448/448 - 0s - loss: 1.2690e-06 - val_loss: 1.3537e-08
Epoch 376/512
448/448 - 0s - loss: 1.2614e-06 - val_loss: 1.2863e-08
Epoch 377/512
448/448 - 0s - loss: 1.1901e-06 - val_loss: 1.1347e-08
Epoch 378/512
448/448 - 0s - loss: 1.0788e-06 - val_loss: 1.1003e-08
Epoch 379/512
448/448 - 0s - loss: 1.0653e-06 - val_loss: 1.1386e-08
Epoch 380/512
448/448 - 0s - loss: 1.0704e-06 - val_loss: 1.0383e-08
Epoch 381/512
448/448 - 0s - loss: 9.6241e-07 - val_loss: 9.4862e-09
Epoch 382/512
448/448 - 0s - loss: 9.1088e-07 - val_loss: 9.6267e-09
Epoch 383/512
448/448 - 0s - loss: 9.2633e-07 - val_loss: 9.2213e-09
Epoch 384/512
448/448 - 0s - loss: 8.4895e-07 - val_loss: 8.6612e-09
Epoch 385/512
448/448 - 0s - loss: 8.1874e-07 - val_loss: 8.2927e-09
Epoch 386/512
448/448 - 0s - loss: 7.8722e-07 - val_loss: 7.7663e-09
Epoch 387/512
448/448 - 0s - loss: 7.3675e-07 - val_loss: 7.5854e-09
Epoch 388/512
448/448 - 0s - loss: 7.2359e-07 - val_loss: 7.2511e-09
Epoch 389/512
448/448 - 0s - loss: 6.8051e-07 - val_loss: 6.8789e-09
Epoch 390/512
448/448 - 0s - loss: 6.5313e-07 - val_loss: 6.5400e-09
Epoch 391/512
448/448 - 0s - loss: 6.2211e-07 - val_loss: 6.2428e-09
Epoch 392/512
448/448 - 0s - loss: 5.8984e-07 - val_loss: 6.1484e-09
Epoch 393/512
448/448 - 0s - loss: 5.7854e-07 - val_loss: 5.7328e-09
Epoch 394/512
448/448 - 0s - loss: 5.3375e-07 - val_loss: 5.4860e-09
Epoch 395/512
448/448 - 0s - loss: 5.1686e-07 - val_loss: 5.4365e-09
Epoch 396/512
448/448 - 0s - loss: 5.1031e-07 - val_loss: 4.8982e-09
Epoch 397/512
448/448 - 0s - loss: 4.5902e-07 - val_loss: 4.5979e-09
Epoch 398/512
448/448 - 0s - loss: 4.4387e-07 - val_loss: 4.6938e-09
Epoch 399/512
448/448 - 0s - loss: 4.4686e-07 - val_loss: 4.3927e-09
Epoch 400/512
448/448 - 0s - loss: 4.0363e-07 - val_loss: 4.0886e-09
Epoch 401/512
448/448 - 0s - loss: 3.8727e-07 - val_loss: 4.0190e-09
Epoch 402/512
448/448 - 0s - loss: 3.7656e-07 - val_loss: 3.9046e-09
Epoch 403/512
448/448 - 0s - loss: 3.6473e-07 - val_loss: 3.4865e-09
Epoch 404/512
448/448 - 0s - loss: 3.2525e-07 - val_loss: 3.4249e-09
Epoch 405/512
448/448 - 0s - loss: 3.2760e-07 - val_loss: 3.4134e-09
Epoch 406/512
448/448 - 0s - loss: 3.1844e-07 - val_loss: 3.0470e-09
Epoch 407/512
448/448 - 0s - loss: 2.8427e-07 - val_loss: 2.9151e-09
Epoch 408/512
448/448 - 0s - loss: 2.7956e-07 - val_loss: 2.9584e-09
Epoch 409/512
448/448 - 0s - loss: 2.7534e-07 - val_loss: 2.7916e-09
Epoch 410/512
448/448 - 0s - loss: 2.5783e-07 - val_loss: 2.4720e-09
Epoch 411/512
448/448 - 0s - loss: 2.3243e-07 - val_loss: 2.4813e-09
Epoch 412/512
448/448 - 0s - loss: 2.3719e-07 - val_loss: 2.4702e-09
Epoch 413/512
448/448 - 0s - loss: 2.2985e-07 - val_loss: 2.1438e-09
Epoch 414/512
448/448 - 0s - loss: 1.9949e-07 - val_loss: 2.0674e-09
Epoch 415/512
448/448 - 0s - loss: 2.0045e-07 - val_loss: 2.1499e-09
Epoch 416/512
448/448 - 0s - loss: 1.9970e-07 - val_loss: 1.9942e-09
Epoch 417/512
448/448 - 0s - loss: 1.8122e-07 - val_loss: 1.7954e-09
Epoch 418/512
448/448 - 0s - loss: 1.6754e-07 - val_loss: 1.8097e-09
Epoch 419/512
448/448 - 0s - loss: 1.7122e-07 - val_loss: 1.7263e-09
Epoch 420/512
448/448 - 0s - loss: 1.5865e-07 - val_loss: 1.5584e-09
Epoch 421/512
448/448 - 0s - loss: 1.4497e-07 - val_loss: 1.5373e-09
Epoch 422/512
448/448 - 0s - loss: 1.4561e-07 - val_loss: 1.5189e-09
Epoch 423/512
448/448 - 0s - loss: 1.4051e-07 - val_loss: 1.3530e-09
Epoch 424/512
448/448 - 0s - loss: 1.2436e-07 - val_loss: 1.3135e-09
Epoch 425/512
448/448 - 0s - loss: 1.2548e-07 - val_loss: 1.3047e-09
Epoch 426/512
448/448 - 0s - loss: 1.2154e-07 - val_loss: 1.1700e-09
Epoch 427/512
448/448 - 0s - loss: 1.0802e-07 - val_loss: 1.1208e-09
Epoch 428/512
448/448 - 0s - loss: 1.0703e-07 - val_loss: 1.1240e-09
Epoch 429/512
448/448 - 0s - loss: 1.0385e-07 - val_loss: 1.0720e-09
Epoch 430/512
448/448 - 0s - loss: 9.8296e-08 - val_loss: 9.6714e-10
Epoch 431/512
448/448 - 0s - loss: 9.0103e-08 - val_loss: 9.2894e-10
Epoch 432/512
448/448 - 0s - loss: 8.7910e-08 - val_loss: 9.1411e-10
Epoch 433/512
448/448 - 0s - loss: 8.5514e-08 - val_loss: 8.4879e-10
Epoch 434/512
448/448 - 0s - loss: 7.8615e-08 - val_loss: 7.8900e-10
Epoch 435/512
448/448 - 0s - loss: 7.4914e-08 - val_loss: 7.7415e-10
Epoch 436/512
448/448 - 0s - loss: 7.2736e-08 - val_loss: 7.4884e-10
Epoch 437/512
448/448 - 0s - loss: 6.9334e-08 - val_loss: 7.0680e-10
Epoch 438/512
448/448 - 0s - loss: 6.5356e-08 - val_loss: 6.7390e-10
Epoch 439/512
448/448 - 0s - loss: 6.2629e-08 - val_loss: 6.3497e-10
Epoch 440/512
448/448 - 0s - loss: 5.8764e-08 - val_loss: 6.1233e-10
Epoch 441/512
448/448 - 0s - loss: 5.6996e-08 - val_loss: 5.8080e-10
Epoch 442/512
448/448 - 0s - loss: 5.3680e-08 - val_loss: 5.5378e-10
Epoch 443/512
448/448 - 0s - loss: 5.1308e-08 - val_loss: 5.2857e-10
Epoch 444/512
448/448 - 0s - loss: 4.9378e-08 - val_loss: 4.8851e-10
Epoch 445/512
448/448 - 0s - loss: 4.5597e-08 - val_loss: 4.6432e-10
Epoch 446/512
448/448 - 0s - loss: 4.3740e-08 - val_loss: 4.5996e-10
Epoch 447/512
448/448 - 0s - loss: 4.3036e-08 - val_loss: 4.3485e-10
Epoch 448/512
448/448 - 0s - loss: 4.0025e-08 - val_loss: 4.0487e-10
Epoch 449/512
448/448 - 0s - loss: 3.7732e-08 - val_loss: 3.8991e-10
Epoch 450/512
448/448 - 0s - loss: 3.6633e-08 - val_loss: 3.7347e-10
Epoch 451/512
448/448 - 0s - loss: 3.4594e-08 - val_loss: 3.5378e-10
Epoch 452/512
448/448 - 0s - loss: 3.2780e-08 - val_loss: 3.3610e-10
Epoch 453/512
448/448 - 0s - loss: 3.1550e-08 - val_loss: 3.1486e-10
Epoch 454/512
448/448 - 0s - loss: 2.9692e-08 - val_loss: 2.9743e-10
Epoch 455/512
448/448 - 0s - loss: 2.7931e-08 - val_loss: 2.9585e-10
Epoch 456/512
448/448 - 0s - loss: 2.7535e-08 - val_loss: 2.8599e-10
Epoch 457/512
448/448 - 0s - loss: 2.6250e-08 - val_loss: 2.6017e-10
Epoch 458/512
448/448 - 0s - loss: 2.4207e-08 - val_loss: 2.4005e-10
Epoch 459/512
448/448 - 0s - loss: 2.2646e-08 - val_loss: 2.4042e-10
Epoch 460/512
448/448 - 0s - loss: 2.2696e-08 - val_loss: 2.3546e-10
Epoch 461/512
448/448 - 0s - loss: 2.1798e-08 - val_loss: 2.1418e-10
Epoch 462/512
448/448 - 0s - loss: 2.0023e-08 - val_loss: 1.9784e-10
Epoch 463/512
448/448 - 0s - loss: 1.8672e-08 - val_loss: 1.9576e-10
Epoch 464/512
448/448 - 0s - loss: 1.8678e-08 - val_loss: 1.8921e-10
Epoch 465/512
448/448 - 0s - loss: 1.7503e-08 - val_loss: 1.7838e-10
Epoch 466/512
448/448 - 0s - loss: 1.6469e-08 - val_loss: 1.7297e-10
Epoch 467/512
448/448 - 0s - loss: 1.6213e-08 - val_loss: 1.6110e-10
Epoch 468/512
448/448 - 0s - loss: 1.5108e-08 - val_loss: 1.4920e-10
Epoch 469/512
448/448 - 0s - loss: 1.4010e-08 - val_loss: 1.4744e-10
Epoch 470/512
448/448 - 0s - loss: 1.3901e-08 - val_loss: 1.4407e-10
Epoch 471/512
448/448 - 0s - loss: 1.3515e-08 - val_loss: 1.3213e-10
Epoch 472/512
448/448 - 0s - loss: 1.2170e-08 - val_loss: 1.2484e-10
Epoch 473/512
448/448 - 0s - loss: 1.1899e-08 - val_loss: 1.2140e-10
Epoch 474/512
448/448 - 0s - loss: 1.1571e-08 - val_loss: 1.1451e-10
Epoch 475/512
448/448 - 0s - loss: 1.0755e-08 - val_loss: 1.1080e-10
Epoch 476/512
448/448 - 0s - loss: 1.0461e-08 - val_loss: 1.0731e-10
Epoch 477/512
448/448 - 0s - loss: 9.9826e-09 - val_loss: 1.0185e-10
Epoch 478/512
448/448 - 0s - loss: 9.6143e-09 - val_loss: 9.4682e-11
Epoch 479/512
448/448 - 0s - loss: 8.8665e-09 - val_loss: 8.9659e-11
Epoch 480/512
448/448 - 0s - loss: 8.4742e-09 - val_loss: 8.8897e-11
Epoch 481/512
448/448 - 0s - loss: 8.4492e-09 - val_loss: 8.5771e-11
Epoch 482/512
448/448 - 0s - loss: 8.0147e-09 - val_loss: 8.0069e-11
Epoch 483/512
448/448 - 0s - loss: 7.4779e-09 - val_loss: 7.5230e-11
Epoch 484/512
448/448 - 0s - loss: 7.0785e-09 - val_loss: 7.3754e-11
Epoch 485/512
448/448 - 0s - loss: 7.0683e-09 - val_loss: 6.9632e-11
Epoch 486/512
448/448 - 0s - loss: 6.6119e-09 - val_loss: 6.4183e-11
Epoch 487/512
448/448 - 0s - loss: 6.1325e-09 - val_loss: 6.3012e-11
Epoch 488/512
448/448 - 0s - loss: 6.0248e-09 - val_loss: 6.3668e-11
Epoch 489/512
448/448 - 0s - loss: 6.0212e-09 - val_loss: 6.0010e-11
Epoch 490/512
448/448 - 0s - loss: 5.5749e-09 - val_loss: 5.4897e-11
Epoch 491/512
448/448 - 0s - loss: 5.1864e-09 - val_loss: 5.3242e-11
Epoch 492/512
448/448 - 0s - loss: 5.0734e-09 - val_loss: 5.1578e-11
Epoch 493/512
448/448 - 0s - loss: 4.9099e-09 - val_loss: 5.0032e-11
Epoch 494/512
448/448 - 0s - loss: 4.7573e-09 - val_loss: 4.7086e-11
Epoch 495/512
448/448 - 0s - loss: 4.4761e-09 - val_loss: 4.4571e-11
Epoch 496/512
448/448 - 0s - loss: 4.2739e-09 - val_loss: 4.3347e-11
Epoch 497/512
448/448 - 0s - loss: 4.0895e-09 - val_loss: 4.3213e-11
Epoch 498/512
448/448 - 0s - loss: 4.0807e-09 - val_loss: 4.1032e-11
Epoch 499/512
448/448 - 0s - loss: 3.8717e-09 - val_loss: 3.7574e-11
Epoch 500/512
448/448 - 0s - loss: 3.5969e-09 - val_loss: 3.5682e-11
Epoch 501/512
448/448 - 0s - loss: 3.4352e-09 - val_loss: 3.5378e-11
Epoch 502/512
448/448 - 0s - loss: 3.4227e-09 - val_loss: 3.4987e-11
Epoch 503/512
448/448 - 0s - loss: 3.3341e-09 - val_loss: 3.3520e-11
Epoch 504/512
448/448 - 0s - loss: 3.1594e-09 - val_loss: 3.1432e-11
Epoch 505/512
448/448 - 0s - loss: 2.9922e-09 - val_loss: 3.0290e-11
Epoch 506/512
448/448 - 0s - loss: 2.8664e-09 - val_loss: 2.9402e-11
Epoch 507/512
448/448 - 0s - loss: 2.8329e-09 - val_loss: 2.8161e-11
Epoch 508/512
448/448 - 0s - loss: 2.6930e-09 - val_loss: 2.7373e-11
Epoch 509/512
448/448 - 0s - loss: 2.5986e-09 - val_loss: 2.6416e-11
Epoch 510/512
448/448 - 0s - loss: 2.5334e-09 - val_loss: 2.4981e-11
Epoch 511/512
448/448 - 0s - loss: 2.3764e-09 - val_loss: 2.4195e-11
Epoch 512/512
448/448 - 0s - loss: 2.3091e-09 - val_loss: 2.3597e-11
2024-04-16 13:00:47.641138: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.3154e-09 - val_loss: 2.2765e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.1371e-09 - val_loss: 2.0949e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.0066e-09 - val_loss: 1.9866e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9212e-09 - val_loss: 1.9844e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9344e-09 - val_loss: 1.9659e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.8831e-09 - val_loss: 1.9084e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.8300e-09 - val_loss: 1.8238e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.7370e-09 - val_loss: 1.7366e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.6697e-09 - val_loss: 1.6605e-09
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.5928e-09 - val_loss: 1.5958e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.5486e-09 - val_loss: 1.5836e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.5312e-09 - val_loss: 1.5605e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4835e-09 - val_loss: 1.4836e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4266e-09 - val_loss: 1.4387e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.3790e-09 - val_loss: 1.3676e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.3231e-09 - val_loss: 1.3283e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2739e-09 - val_loss: 1.2881e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2458e-09 - val_loss: 1.2603e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2196e-09 - val_loss: 1.2422e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2001e-09 - val_loss: 1.1949e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.1484e-09 - val_loss: 1.1458e-09
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.1043e-09 - val_loss: 1.1146e-09
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0782e-09 - val_loss: 1.0736e-09
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0388e-09 - val_loss: 1.0639e-09
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0283e-09 - val_loss: 1.0277e-09
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.9199e-10 - val_loss: 9.9931e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.6031e-10 - val_loss: 9.7122e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.4349e-10 - val_loss: 9.4254e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.1393e-10 - val_loss: 9.2362e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.9067e-10 - val_loss: 8.9461e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.6517e-10 - val_loss: 8.7267e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.4026e-10 - val_loss: 8.4383e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.1657e-10 - val_loss: 8.1478e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.9258e-10 - val_loss: 7.9443e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.6988e-10 - val_loss: 7.8100e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.5669e-10 - val_loss: 7.4942e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.2908e-10 - val_loss: 7.3896e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.2625e-10 - val_loss: 7.2525e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.9432e-10 - val_loss: 7.0511e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.8405e-10 - val_loss: 6.9651e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.7429e-10 - val_loss: 6.8485e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.6264e-10 - val_loss: 6.6370e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.3493e-10 - val_loss: 6.3511e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.2005e-10 - val_loss: 6.2126e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.0225e-10 - val_loss: 6.0933e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.9154e-10 - val_loss: 6.0043e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.9138e-10 - val_loss: 5.8653e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.6993e-10 - val_loss: 5.7220e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.5465e-10 - val_loss: 5.5775e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.4279e-10 - val_loss: 5.4706e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.3199e-10 - val_loss: 5.1943e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.0489e-10 - val_loss: 4.9973e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9124e-10 - val_loss: 5.0057e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9270e-10 - val_loss: 5.1120e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.9943e-10 - val_loss: 4.9566e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.8134e-10 - val_loss: 4.7378e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.5817e-10 - val_loss: 4.5980e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.4531e-10 - val_loss: 4.4286e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3812e-10 - val_loss: 4.4553e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3449e-10 - val_loss: 4.4537e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3656e-10 - val_loss: 4.4446e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.3413e-10 - val_loss: 4.3031e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.1613e-10 - val_loss: 4.1375e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.0237e-10 - val_loss: 4.0415e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.9744e-10 - val_loss: 4.0204e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.8888e-10 - val_loss: 3.8465e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.7694e-10 - val_loss: 3.7682e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.6607e-10 - val_loss: 3.6612e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.5963e-10 - val_loss: 3.6386e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.5515e-10 - val_loss: 3.6332e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.5384e-10 - val_loss: 3.4676e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.3946e-10 - val_loss: 3.4658e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.3830e-10 - val_loss: 3.3813e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.3349e-10 - val_loss: 3.3351e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.2560e-10 - val_loss: 3.3189e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.2270e-10 - val_loss: 3.1581e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.0780e-10 - val_loss: 3.1223e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.0657e-10 - val_loss: 3.0356e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9838e-10 - val_loss: 3.0609e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0222e-10 - val_loss: 3.0493e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.9683e-10 - val_loss: 2.9905e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.9197e-10 - val_loss: 2.9059e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.8433e-10 - val_loss: 2.8547e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.7943e-10 - val_loss: 2.7695e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.7072e-10 - val_loss: 2.7153e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.6562e-10 - val_loss: 2.6594e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6256e-10 - val_loss: 2.6858e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6451e-10 - val_loss: 2.6798e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6490e-10 - val_loss: 2.7096e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.6315e-10 - val_loss: 2.5570e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.5055e-10 - val_loss: 2.4036e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.3492e-10 - val_loss: 2.3472e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.2917e-10 - val_loss: 2.2800e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2718e-10 - val_loss: 2.3105e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3060e-10 - val_loss: 2.3162e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2850e-10 - val_loss: 2.2954e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.2642e-10 - val_loss: 2.2665e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.2054e-10 - val_loss: 2.1276e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.0946e-10 - val_loss: 2.0800e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.0636e-10 - val_loss: 2.0717e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0398e-10 - val_loss: 2.1209e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1180e-10 - val_loss: 2.1861e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.1306e-10 - val_loss: 2.0636e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.0257e-10 - val_loss: 2.0563e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.0430e-10 - val_loss: 2.0445e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9702e-10 - val_loss: 1.9952e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9773e-10 - val_loss: 1.9734e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9397e-10 - val_loss: 1.9035e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8795e-10 - val_loss: 1.9148e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.8870e-10 - val_loss: 1.8667e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.8041e-10 - val_loss: 1.7691e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7699e-10 - val_loss: 1.8078e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.7764e-10 - val_loss: 1.7395e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.6868e-10 - val_loss: 1.6794e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6991e-10 - val_loss: 1.7833e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7900e-10 - val_loss: 1.8140e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7595e-10 - val_loss: 1.7372e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7383e-10 - val_loss: 1.7552e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.7156e-10 - val_loss: 1.6499e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.6037e-10 - val_loss: 1.5658e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5643e-10 - val_loss: 1.6231e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6002e-10 - val_loss: 1.5893e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.5541e-10 - val_loss: 1.4908e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4726e-10 - val_loss: 1.5516e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5369e-10 - val_loss: 1.5058e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4925e-10 - val_loss: 1.4959e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4855e-10 - val_loss: 1.5268e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5149e-10 - val_loss: 1.5159e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4820e-10 - val_loss: 1.4561e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4067e-10 - val_loss: 1.3943e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4197e-10 - val_loss: 1.4686e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4293e-10 - val_loss: 1.3852e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.3411e-10 - val_loss: 1.3246e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3218e-10 - val_loss: 1.3674e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3592e-10 - val_loss: 1.3484e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.3249e-10 - val_loss: 1.2867e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2527e-10 - val_loss: 1.2334e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2144e-10 - val_loss: 1.2361e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2354e-10 - val_loss: 1.2665e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2490e-10 - val_loss: 1.2593e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2302e-10 - val_loss: 1.2291e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2209e-10 - val_loss: 1.2642e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2429e-10 - val_loss: 1.2304e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2191e-10 - val_loss: 1.2227e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.1869e-10 - val_loss: 1.1529e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.1081e-10 - val_loss: 1.0265e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0094e-10 - val_loss: 1.0312e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0301e-10 - val_loss: 1.0791e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0808e-10 - val_loss: 1.1198e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1314e-10 - val_loss: 1.1406e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1373e-10 - val_loss: 1.1874e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1755e-10 - val_loss: 1.1760e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1589e-10 - val_loss: 1.1561e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1318e-10 - val_loss: 1.1191e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0847e-10 - val_loss: 1.0487e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0233e-10 - val_loss: 1.0362e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0207e-10 - val_loss: 1.0145e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0035e-10 - val_loss: 1.0329e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0328e-10 - val_loss: 1.0573e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0131e-10 - val_loss: 1.0079e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.9588e-11 - val_loss: 9.9557e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.8371e-11 - val_loss: 9.8591e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.8215e-11 - val_loss: 9.7572e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8217e-11 - val_loss: 9.7752e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.5357e-11 - val_loss: 9.6220e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.4039e-11 - val_loss: 9.2642e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1642e-11 - val_loss: 9.3810e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4212e-11 - val_loss: 9.9025e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9098e-11 - val_loss: 9.7431e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.2989e-11 - val_loss: 8.6599e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.4944e-11 - val_loss: 8.4477e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3382e-11 - val_loss: 8.6564e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5806e-11 - val_loss: 8.7766e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7904e-11 - val_loss: 8.6658e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.3086e-11 - val_loss: 8.0751e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.9413e-11 - val_loss: 7.9092e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0174e-11 - val_loss: 8.4614e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5637e-11 - val_loss: 9.0087e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0549e-11 - val_loss: 9.4715e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2737e-11 - val_loss: 8.8133e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.3710e-11 - val_loss: 7.7449e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.4516e-11 - val_loss: 6.9617e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9402e-11 - val_loss: 7.3138e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4555e-11 - val_loss: 8.0935e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1373e-11 - val_loss: 8.4352e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3914e-11 - val_loss: 8.2609e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8734e-11 - val_loss: 7.3735e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.1442e-11 - val_loss: 6.8205e-11
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.6316e-11 - val_loss: 6.7965e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9400e-11 - val_loss: 7.4681e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5693e-11 - val_loss: 7.9756e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0298e-11 - val_loss: 8.1757e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0981e-11 - val_loss: 7.6670e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3321e-11 - val_loss: 7.0732e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.7794e-11 - val_loss: 6.4424e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.2954e-11 - val_loss: 6.4357e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5617e-11 - val_loss: 7.0286e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1467e-11 - val_loss: 7.5360e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4912e-11 - val_loss: 7.4047e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2426e-11 - val_loss: 6.8957e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.6247e-11 - val_loss: 6.2427e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.1629e-11 - val_loss: 5.9568e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.8701e-11 - val_loss: 5.6871e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6808e-11 - val_loss: 6.1077e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2323e-11 - val_loss: 6.5966e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6301e-11 - val_loss: 7.0278e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2569e-11 - val_loss: 7.3552e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0287e-11 - val_loss: 6.4520e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2522e-11 - val_loss: 5.9986e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8169e-11 - val_loss: 5.6918e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.6425e-11 - val_loss: 5.4800e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4683e-11 - val_loss: 5.7770e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9585e-11 - val_loss: 6.4791e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6195e-11 - val_loss: 6.9638e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0610e-11 - val_loss: 7.2849e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1535e-11 - val_loss: 6.6547e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2765e-11 - val_loss: 5.6644e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.4455e-11 - val_loss: 5.2086e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.0793e-11 - val_loss: 5.1135e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.0165e-11 - val_loss: 4.9669e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9383e-11 - val_loss: 5.1892e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3343e-11 - val_loss: 5.7125e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8160e-11 - val_loss: 6.1907e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3229e-11 - val_loss: 6.3478e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1778e-11 - val_loss: 5.8159e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5852e-11 - val_loss: 5.2171e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 5.0617e-11 - val_loss: 4.8058e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.7140e-11 - val_loss: 4.6752e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.5297e-11 - val_loss: 4.3725e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3745e-11 - val_loss: 4.5395e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5989e-11 - val_loss: 4.9881e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2436e-11 - val_loss: 5.8229e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9098e-11 - val_loss: 6.1998e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1219e-11 - val_loss: 6.0148e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6837e-11 - val_loss: 5.1735e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9450e-11 - val_loss: 4.5951e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.4173e-11 - val_loss: 4.1765e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.0669e-11 - val_loss: 3.8629e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.7790e-11 - val_loss: 3.8005e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7746e-11 - val_loss: 3.8264e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8671e-11 - val_loss: 4.0524e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1393e-11 - val_loss: 4.6262e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8179e-11 - val_loss: 5.2195e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2990e-11 - val_loss: 5.5388e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4942e-11 - val_loss: 5.5223e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2902e-11 - val_loss: 4.8296e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6369e-11 - val_loss: 4.3713e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2690e-11 - val_loss: 4.0226e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9080e-11 - val_loss: 3.8016e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.7553e-11 - val_loss: 3.6686e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.5790e-11 - val_loss: 3.5270e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6280e-11 - val_loss: 3.7203e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6961e-11 - val_loss: 3.6775e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7135e-11 - val_loss: 3.9426e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2249e-11 - val_loss: 4.6465e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7833e-11 - val_loss: 4.9723e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0572e-11 - val_loss: 5.1003e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9080e-11 - val_loss: 4.5666e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3526e-11 - val_loss: 4.1036e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9507e-11 - val_loss: 3.7505e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.6042e-11 - val_loss: 3.3974e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3754e-11 - val_loss: 3.4759e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5198e-11 - val_loss: 3.5507e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.4686e-11 - val_loss: 3.3346e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4075e-11 - val_loss: 3.6097e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6092e-11 - val_loss: 3.7587e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8391e-11 - val_loss: 3.9178e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8848e-11 - val_loss: 4.2669e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3745e-11 - val_loss: 4.6362e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6310e-11 - val_loss: 4.6673e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6131e-11 - val_loss: 4.2493e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9971e-11 - val_loss: 3.7964e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.6218e-11 - val_loss: 3.2794e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.2187e-11 - val_loss: 3.2292e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2088e-11 - val_loss: 3.3280e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.2598e-11 - val_loss: 3.1986e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.1040e-11 - val_loss: 3.1542e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1264e-11 - val_loss: 3.2046e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0932e-11 - val_loss: 3.2703e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3083e-11 - val_loss: 3.2750e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2829e-11 - val_loss: 3.4398e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4908e-11 - val_loss: 3.6525e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6992e-11 - val_loss: 3.8814e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0314e-11 - val_loss: 4.3823e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4272e-11 - val_loss: 4.6485e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6401e-11 - val_loss: 4.6192e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4237e-11 - val_loss: 4.1238e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9531e-11 - val_loss: 3.6200e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4123e-11 - val_loss: 3.1770e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 3.0595e-11 - val_loss: 2.8965e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9000e-11 - val_loss: 2.9539e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9187e-11 - val_loss: 2.9680e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9487e-11 - val_loss: 2.9739e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9354e-11 - val_loss: 3.0927e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1338e-11 - val_loss: 3.2001e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1191e-11 - val_loss: 3.0737e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0459e-11 - val_loss: 2.9376e-11
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.8629e-11 - val_loss: 2.8681e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9248e-11 - val_loss: 3.1070e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0775e-11 - val_loss: 3.0723e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0439e-11 - val_loss: 2.9350e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.9361e-11 - val_loss: 2.8571e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.6750e-11 - val_loss: 2.4685e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5230e-11 - val_loss: 2.7188e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7826e-11 - val_loss: 2.9609e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9858e-11 - val_loss: 2.9993e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0264e-11 - val_loss: 3.1865e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2348e-11 - val_loss: 3.3869e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3794e-11 - val_loss: 3.2718e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2105e-11 - val_loss: 3.0463e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9449e-11 - val_loss: 2.8489e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7865e-11 - val_loss: 2.8539e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8755e-11 - val_loss: 2.8550e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8647e-11 - val_loss: 2.8617e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7988e-11 - val_loss: 2.8676e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9073e-11 - val_loss: 3.0757e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0243e-11 - val_loss: 3.0757e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0913e-11 - val_loss: 3.0664e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0727e-11 - val_loss: 3.0085e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9767e-11 - val_loss: 2.9270e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8863e-11 - val_loss: 2.8186e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8076e-11 - val_loss: 2.7827e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8286e-11 - val_loss: 2.6998e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.5772e-11 - val_loss: 2.2852e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 2.1993e-11 - val_loss: 2.0162e-11
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.9443e-11 - val_loss: 1.8134e-11
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.7355e-11 - val_loss: 1.5764e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6638e-11 - val_loss: 1.8519e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9405e-11 - val_loss: 2.1533e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2008e-11 - val_loss: 2.3145e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4107e-11 - val_loss: 2.6624e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7259e-11 - val_loss: 2.8526e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8883e-11 - val_loss: 3.0191e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0272e-11 - val_loss: 2.9550e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8585e-11 - val_loss: 2.6924e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6549e-11 - val_loss: 2.6655e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6865e-11 - val_loss: 2.8109e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8594e-11 - val_loss: 2.8562e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8159e-11 - val_loss: 2.7110e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6966e-11 - val_loss: 2.7980e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7125e-11 - val_loss: 2.6381e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6577e-11 - val_loss: 2.6755e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6729e-11 - val_loss: 2.6643e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6757e-11 - val_loss: 2.5968e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5715e-11 - val_loss: 2.6159e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5375e-11 - val_loss: 2.4688e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4608e-11 - val_loss: 2.5305e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5803e-11 - val_loss: 2.5934e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4095e-11 - val_loss: 1.9743e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8404e-11 - val_loss: 1.6556e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.5856e-11 - val_loss: 1.4981e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4568e-11 - val_loss: 1.4218e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.4128e-11 - val_loss: 1.3880e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.3456e-11 - val_loss: 1.2873e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2662e-11 - val_loss: 1.2510e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3424e-11 - val_loss: 1.6003e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7470e-11 - val_loss: 2.0942e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2042e-11 - val_loss: 2.4141e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4830e-11 - val_loss: 2.5928e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6319e-11 - val_loss: 2.7256e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6536e-11 - val_loss: 2.6418e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6694e-11 - val_loss: 2.7177e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7038e-11 - val_loss: 2.6876e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6110e-11 - val_loss: 2.4927e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3986e-11 - val_loss: 2.3512e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2912e-11 - val_loss: 2.1824e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1267e-11 - val_loss: 2.0723e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1528e-11 - val_loss: 2.3336e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3503e-11 - val_loss: 2.2612e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2384e-11 - val_loss: 2.2199e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1384e-11 - val_loss: 2.0208e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9978e-11 - val_loss: 2.0588e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0311e-11 - val_loss: 1.9726e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0242e-11 - val_loss: 2.0352e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0119e-11 - val_loss: 2.0651e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1344e-11 - val_loss: 2.2345e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2707e-11 - val_loss: 2.2659e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2238e-11 - val_loss: 2.0841e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9066e-11 - val_loss: 1.6065e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5260e-11 - val_loss: 1.3780e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3324e-11 - val_loss: 1.2598e-11
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.2242e-11 - val_loss: 1.1125e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0866e-11 - val_loss: 1.0337e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0275e-11 - val_loss: 1.0496e-11
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0253e-11 - val_loss: 9.8061e-12
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.8195e-12 - val_loss: 9.7580e-12
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8912e-12 - val_loss: 1.0260e-11
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 1.0118e-11 - val_loss: 9.4759e-12
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4306e-12 - val_loss: 9.6117e-12
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7936e-12 - val_loss: 1.0476e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1281e-11 - val_loss: 1.3196e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3778e-11 - val_loss: 1.5639e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6444e-11 - val_loss: 1.8512e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9580e-11 - val_loss: 2.1812e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2898e-11 - val_loss: 2.3580e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2886e-11 - val_loss: 2.2234e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2570e-11 - val_loss: 2.2817e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3358e-11 - val_loss: 2.4641e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3836e-11 - val_loss: 2.2765e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1977e-11 - val_loss: 2.1130e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0196e-11 - val_loss: 1.9317e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9071e-11 - val_loss: 1.8748e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9267e-11 - val_loss: 1.9357e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8698e-11 - val_loss: 1.8349e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8689e-11 - val_loss: 1.9290e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9447e-11 - val_loss: 2.0707e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0714e-11 - val_loss: 2.1108e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1147e-11 - val_loss: 2.1626e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0905e-11 - val_loss: 2.0622e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0772e-11 - val_loss: 2.0609e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0053e-11 - val_loss: 1.9950e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9852e-11 - val_loss: 1.9460e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8981e-11 - val_loss: 1.7926e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7775e-11 - val_loss: 1.7372e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7444e-11 - val_loss: 1.8004e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7417e-11 - val_loss: 1.6172e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5061e-11 - val_loss: 1.3650e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2861e-11 - val_loss: 1.1852e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1453e-11 - val_loss: 1.0915e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0695e-11 - val_loss: 1.0159e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 9.5201e-12 - val_loss: 8.7052e-12
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.6448e-12 - val_loss: 8.2660e-12
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1144e-12 - val_loss: 8.2703e-12
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4149e-12 - val_loss: 9.7506e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0073e-11 - val_loss: 1.0416e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9129e-12 - val_loss: 9.3475e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2345e-12 - val_loss: 9.6540e-12
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9303e-12 - val_loss: 1.0446e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0725e-11 - val_loss: 1.0629e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0396e-11 - val_loss: 1.0055e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0076e-11 - val_loss: 9.6702e-12
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6411e-12 - val_loss: 9.4914e-12
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0975e-12 - val_loss: 8.7909e-12
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 8.6268e-12 - val_loss: 8.2653e-12
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.9552e-12 - val_loss: 7.4041e-12
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.1236e-12 - val_loss: 6.9726e-12
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 7.0563e-12 - val_loss: 6.6595e-12
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7195e-12 - val_loss: 6.7480e-12
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9197e-12 - val_loss: 9.3213e-12
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0906e-11 - val_loss: 1.4327e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5532e-11 - val_loss: 1.7735e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8283e-11 - val_loss: 1.9934e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0612e-11 - val_loss: 2.1555e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1274e-11 - val_loss: 2.0791e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0223e-11 - val_loss: 1.9986e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9716e-11 - val_loss: 2.0015e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9786e-11 - val_loss: 1.9195e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8742e-11 - val_loss: 1.7870e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7527e-11 - val_loss: 1.7046e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6739e-11 - val_loss: 1.6211e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6017e-11 - val_loss: 1.5261e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4829e-11 - val_loss: 1.4889e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4719e-11 - val_loss: 1.5245e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5434e-11 - val_loss: 1.6143e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6633e-11 - val_loss: 1.6326e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6011e-11 - val_loss: 1.5598e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5530e-11 - val_loss: 1.5891e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5995e-11 - val_loss: 1.6300e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6070e-11 - val_loss: 1.5828e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5685e-11 - val_loss: 1.5641e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6036e-11 - val_loss: 1.6109e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5745e-11 - val_loss: 1.5433e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5350e-11 - val_loss: 1.5455e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4867e-11 - val_loss: 1.4345e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4558e-11 - val_loss: 1.5630e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5868e-11 - val_loss: 1.6251e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5623e-11 - val_loss: 1.5473e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5669e-11 - val_loss: 1.4425e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3168e-11 - val_loss: 1.1410e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0810e-11 - val_loss: 9.2539e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5124e-12 - val_loss: 7.4798e-12
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.9231e-12 - val_loss: 6.2915e-12
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 6.0380e-12 - val_loss: 5.0506e-12
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.9923e-12 - val_loss: 4.8644e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9098e-12 - val_loss: 5.1271e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0259e-12 - val_loss: 5.0335e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0603e-12 - val_loss: 5.2307e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8291e-12 - val_loss: 6.8139e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9057e-12 - val_loss: 6.8983e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8510e-12 - val_loss: 7.3437e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2926e-12 - val_loss: 7.2100e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1014e-12 - val_loss: 6.5768e-12
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5004e-12 - val_loss: 6.5805e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5660e-12 - val_loss: 6.3091e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1643e-12 - val_loss: 6.2232e-12
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3162e-12 - val_loss: 7.3034e-12
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6312e-12 - val_loss: 7.6523e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6367e-12 - val_loss: 7.2921e-12
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2853e-12 - val_loss: 7.2062e-12
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3182e-12 - val_loss: 7.1338e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9897e-12 - val_loss: 6.3648e-12
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2223e-12 - val_loss: 6.7679e-12
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9265e-12 - val_loss: 6.8917e-12
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9883e-12 - val_loss: 6.6627e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5703e-12 - val_loss: 6.8582e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6869e-12 - val_loss: 6.4355e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1701e-12 - val_loss: 6.0666e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1543e-12 - val_loss: 6.0798e-12
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0348e-12 - val_loss: 5.5754e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5275e-12 - val_loss: 5.5953e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4515e-12 - val_loss: 5.1289e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0162e-12 - val_loss: 5.2634e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1565e-12 - val_loss: 5.1421e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1865e-12 - val_loss: 4.9811e-12
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/addition_weights.h5
448/448 - 0s - loss: 4.7415e-12 - val_loss: 4.5177e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7220e-12 - val_loss: 5.7379e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4539e-12 - val_loss: 7.9977e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8763e-12 - val_loss: 1.1382e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2225e-11 - val_loss: 1.3121e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3480e-11 - val_loss: 1.4240e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4765e-11 - val_loss: 1.5605e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5605e-11 - val_loss: 1.5941e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0458 - val_loss: 7.6840e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1005e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.1607e-04
Epoch 4/512
448/448 - 0s - loss: 9.8927e-04 - val_loss: 2.0978e-04
Epoch 5/512
448/448 - 0s - loss: 0.0023 - val_loss: 1.4290e-04
Epoch 6/512
448/448 - 0s - loss: 9.0051e-04 - val_loss: 1.3691e-04
Epoch 7/512
448/448 - 0s - loss: 5.4983e-04 - val_loss: 1.0960e-04
Epoch 8/512
448/448 - 0s - loss: 8.5366e-04 - val_loss: 4.4471e-05
Epoch 9/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.2565e-05
Epoch 10/512
448/448 - 0s - loss: 5.8277e-04 - val_loss: 2.0827e-05
Epoch 11/512
448/448 - 0s - loss: 5.2848e-04 - val_loss: 1.0808e-05
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0345e-05
Epoch 13/512
448/448 - 0s - loss: 6.5704e-04 - val_loss: 1.2001e-05
Epoch 14/512
448/448 - 0s - loss: 3.9414e-04 - val_loss: 1.4078e-05
Epoch 15/512
448/448 - 0s - loss: 8.0332e-04 - val_loss: 1.9086e-05
Epoch 16/512
448/448 - 0s - loss: 6.1878e-04 - val_loss: 2.0255e-05
Epoch 17/512
448/448 - 0s - loss: 3.2356e-04 - val_loss: 2.0614e-05
Epoch 18/512
448/448 - 0s - loss: 5.4739e-04 - val_loss: 2.2240e-05
Epoch 19/512
448/448 - 0s - loss: 5.2166e-04 - val_loss: 2.2600e-05
Epoch 20/512
448/448 - 0s - loss: 2.7650e-04 - val_loss: 2.0962e-05
Epoch 21/512
448/448 - 0s - loss: 3.7195e-04 - val_loss: 2.1045e-05
Epoch 22/512
448/448 - 0s - loss: 4.0989e-04 - val_loss: 2.0686e-05
Epoch 23/512
448/448 - 0s - loss: 2.4161e-04 - val_loss: 1.8118e-05
Epoch 24/512
448/448 - 0s - loss: 2.7154e-04 - val_loss: 1.7447e-05
Epoch 25/512
448/448 - 0s - loss: 3.0057e-04 - val_loss: 1.6441e-05
Epoch 26/512
448/448 - 0s - loss: 2.0600e-04 - val_loss: 1.3944e-05
Epoch 27/512
448/448 - 0s - loss: 2.1356e-04 - val_loss: 1.2794e-05
Epoch 28/512
448/448 - 0s - loss: 2.2262e-04 - val_loss: 1.1326e-05
Epoch 29/512
448/448 - 0s - loss: 1.7885e-04 - val_loss: 9.6926e-06
Epoch 30/512
448/448 - 0s - loss: 1.6812e-04 - val_loss: 8.4387e-06
Epoch 31/512
448/448 - 0s - loss: 1.7358e-04 - val_loss: 7.4868e-06
Epoch 32/512
448/448 - 0s - loss: 1.5273e-04 - val_loss: 6.6947e-06
Epoch 33/512
448/448 - 0s - loss: 1.4122e-04 - val_loss: 6.2403e-06
Epoch 34/512
448/448 - 0s - loss: 1.4087e-04 - val_loss: 6.0156e-06
Epoch 35/512
448/448 - 0s - loss: 1.2868e-04 - val_loss: 5.9137e-06
Epoch 36/512
448/448 - 0s - loss: 1.2326e-04 - val_loss: 5.8558e-06
Epoch 37/512
448/448 - 0s - loss: 1.1951e-04 - val_loss: 5.7976e-06
Epoch 38/512
448/448 - 0s - loss: 1.1262e-04 - val_loss: 5.6873e-06
Epoch 39/512
448/448 - 0s - loss: 1.0680e-04 - val_loss: 5.5271e-06
Epoch 40/512
448/448 - 0s - loss: 1.0373e-04 - val_loss: 5.3744e-06
Epoch 41/512
448/448 - 0s - loss: 1.0173e-04 - val_loss: 5.1386e-06
Epoch 42/512
448/448 - 0s - loss: 9.6154e-05 - val_loss: 4.6666e-06
Epoch 43/512
448/448 - 0s - loss: 9.2132e-05 - val_loss: 4.2500e-06
Epoch 44/512
448/448 - 0s - loss: 9.0666e-05 - val_loss: 3.5347e-06
Epoch 45/512
448/448 - 0s - loss: 8.8804e-05 - val_loss: 3.1061e-06
Epoch 46/512
448/448 - 0s - loss: 8.3691e-05 - val_loss: 3.0745e-06
Epoch 47/512
448/448 - 0s - loss: 8.2291e-05 - val_loss: 2.9165e-06
Epoch 48/512
448/448 - 0s - loss: 7.9431e-05 - val_loss: 2.7845e-06
Epoch 49/512
448/448 - 0s - loss: 7.7835e-05 - val_loss: 2.5467e-06
Epoch 50/512
448/448 - 0s - loss: 7.4902e-05 - val_loss: 2.3830e-06
Epoch 51/512
448/448 - 0s - loss: 7.4102e-05 - val_loss: 2.1476e-06
Epoch 52/512
448/448 - 0s - loss: 7.2482e-05 - val_loss: 1.9816e-06
Epoch 53/512
448/448 - 0s - loss: 6.9191e-05 - val_loss: 1.8366e-06
Epoch 54/512
448/448 - 0s - loss: 6.8529e-05 - val_loss: 1.7014e-06
Epoch 55/512
448/448 - 0s - loss: 6.7280e-05 - val_loss: 1.6014e-06
Epoch 56/512
448/448 - 0s - loss: 6.4102e-05 - val_loss: 1.5338e-06
Epoch 57/512
448/448 - 0s - loss: 6.3480e-05 - val_loss: 1.4372e-06
Epoch 58/512
448/448 - 0s - loss: 6.2819e-05 - val_loss: 1.3219e-06
Epoch 59/512
448/448 - 0s - loss: 6.0464e-05 - val_loss: 1.2416e-06
Epoch 60/512
448/448 - 0s - loss: 5.9611e-05 - val_loss: 1.1135e-06
Epoch 61/512
448/448 - 0s - loss: 5.8592e-05 - val_loss: 1.0471e-06
Epoch 62/512
448/448 - 0s - loss: 5.7677e-05 - val_loss: 9.7978e-07
Epoch 63/512
448/448 - 0s - loss: 5.5038e-05 - val_loss: 9.5260e-07
Epoch 64/512
448/448 - 0s - loss: 5.4346e-05 - val_loss: 8.8957e-07
Epoch 65/512
448/448 - 0s - loss: 5.4755e-05 - val_loss: 8.1229e-07
Epoch 66/512
448/448 - 0s - loss: 5.2543e-05 - val_loss: 7.7277e-07
Epoch 67/512
448/448 - 0s - loss: 5.1922e-05 - val_loss: 7.1282e-07
Epoch 68/512
448/448 - 0s - loss: 5.0825e-05 - val_loss: 6.7536e-07
Epoch 69/512
448/448 - 0s - loss: 4.9764e-05 - val_loss: 6.4094e-07
Epoch 70/512
448/448 - 0s - loss: 4.8680e-05 - val_loss: 6.1317e-07
Epoch 71/512
448/448 - 0s - loss: 4.8670e-05 - val_loss: 5.8082e-07
Epoch 72/512
448/448 - 0s - loss: 4.6516e-05 - val_loss: 5.5922e-07
Epoch 73/512
448/448 - 0s - loss: 4.6439e-05 - val_loss: 5.3350e-07
Epoch 74/512
448/448 - 0s - loss: 4.6049e-05 - val_loss: 5.0284e-07
Epoch 75/512
448/448 - 0s - loss: 4.4376e-05 - val_loss: 4.7998e-07
Epoch 76/512
448/448 - 0s - loss: 4.4295e-05 - val_loss: 4.4632e-07
Epoch 77/512
448/448 - 0s - loss: 4.3537e-05 - val_loss: 4.2847e-07
Epoch 78/512
448/448 - 0s - loss: 4.3145e-05 - val_loss: 4.0275e-07
Epoch 79/512
448/448 - 0s - loss: 4.1228e-05 - val_loss: 3.9343e-07
Epoch 80/512
448/448 - 0s - loss: 4.1327e-05 - val_loss: 3.8424e-07
Epoch 81/512
448/448 - 0s - loss: 4.1203e-05 - val_loss: 3.5825e-07
Epoch 82/512
448/448 - 0s - loss: 3.9507e-05 - val_loss: 3.4610e-07
Epoch 83/512
448/448 - 0s - loss: 3.9368e-05 - val_loss: 3.3808e-07
Epoch 84/512
448/448 - 0s - loss: 3.8871e-05 - val_loss: 3.1943e-07
Epoch 85/512
448/448 - 0s - loss: 3.8213e-05 - val_loss: 3.0397e-07
Epoch 86/512
448/448 - 0s - loss: 3.6968e-05 - val_loss: 3.0039e-07
Epoch 87/512
448/448 - 0s - loss: 3.7276e-05 - val_loss: 2.8736e-07
Epoch 88/512
448/448 - 0s - loss: 3.6458e-05 - val_loss: 2.7277e-07
Epoch 89/512
448/448 - 0s - loss: 3.5310e-05 - val_loss: 2.6934e-07
Epoch 90/512
448/448 - 0s - loss: 3.5632e-05 - val_loss: 2.5837e-07
Epoch 91/512
448/448 - 0s - loss: 3.4471e-05 - val_loss: 2.4651e-07
Epoch 92/512
448/448 - 0s - loss: 3.3467e-05 - val_loss: 2.4809e-07
Epoch 93/512
448/448 - 0s - loss: 3.3547e-05 - val_loss: 2.4473e-07
Epoch 94/512
448/448 - 0s - loss: 3.3471e-05 - val_loss: 2.2947e-07
Epoch 95/512
448/448 - 0s - loss: 3.2268e-05 - val_loss: 2.1992e-07
Epoch 96/512
448/448 - 0s - loss: 3.1940e-05 - val_loss: 2.1719e-07
Epoch 97/512
448/448 - 0s - loss: 3.1413e-05 - val_loss: 2.1255e-07
Epoch 98/512
448/448 - 0s - loss: 3.1214e-05 - val_loss: 2.0497e-07
Epoch 99/512
448/448 - 0s - loss: 3.0013e-05 - val_loss: 2.0196e-07
Epoch 100/512
448/448 - 0s - loss: 2.9936e-05 - val_loss: 2.0049e-07
Epoch 101/512
448/448 - 0s - loss: 2.9780e-05 - val_loss: 1.9457e-07
Epoch 102/512
448/448 - 0s - loss: 2.8870e-05 - val_loss: 1.8827e-07
Epoch 103/512
448/448 - 0s - loss: 2.8446e-05 - val_loss: 1.8408e-07
Epoch 104/512
448/448 - 0s - loss: 2.7976e-05 - val_loss: 1.8265e-07
Epoch 105/512
448/448 - 0s - loss: 2.7758e-05 - val_loss: 1.7752e-07
Epoch 106/512
448/448 - 0s - loss: 2.7083e-05 - val_loss: 1.7114e-07
Epoch 107/512
448/448 - 0s - loss: 2.6422e-05 - val_loss: 1.6893e-07
Epoch 108/512
448/448 - 0s - loss: 2.6378e-05 - val_loss: 1.6541e-07
Epoch 109/512
448/448 - 0s - loss: 2.5833e-05 - val_loss: 1.6241e-07
Epoch 110/512
448/448 - 0s - loss: 2.5283e-05 - val_loss: 1.5877e-07
Epoch 111/512
448/448 - 0s - loss: 2.4586e-05 - val_loss: 1.5783e-07
Epoch 112/512
448/448 - 0s - loss: 2.4807e-05 - val_loss: 1.5208e-07
Epoch 113/512
448/448 - 0s - loss: 2.3640e-05 - val_loss: 1.5048e-07
Epoch 114/512
448/448 - 0s - loss: 2.3751e-05 - val_loss: 1.4806e-07
Epoch 115/512
448/448 - 0s - loss: 2.3167e-05 - val_loss: 1.4491e-07
Epoch 116/512
448/448 - 0s - loss: 2.2811e-05 - val_loss: 1.4171e-07
Epoch 117/512
448/448 - 0s - loss: 2.2270e-05 - val_loss: 1.3911e-07
Epoch 118/512
448/448 - 0s - loss: 2.1874e-05 - val_loss: 1.3720e-07
Epoch 119/512
448/448 - 0s - loss: 2.1744e-05 - val_loss: 1.3260e-07
Epoch 120/512
448/448 - 0s - loss: 2.0809e-05 - val_loss: 1.3177e-07
Epoch 121/512
448/448 - 0s - loss: 2.1123e-05 - val_loss: 1.2753e-07
Epoch 122/512
448/448 - 0s - loss: 2.0151e-05 - val_loss: 1.2435e-07
Epoch 123/512
448/448 - 0s - loss: 1.9976e-05 - val_loss: 1.2146e-07
Epoch 124/512
448/448 - 0s - loss: 1.9268e-05 - val_loss: 1.2324e-07
Epoch 125/512
448/448 - 0s - loss: 1.9709e-05 - val_loss: 1.1864e-07
Epoch 126/512
448/448 - 0s - loss: 1.8657e-05 - val_loss: 1.1454e-07
Epoch 127/512
448/448 - 0s - loss: 1.8268e-05 - val_loss: 1.1455e-07
Epoch 128/512
448/448 - 0s - loss: 1.8370e-05 - val_loss: 1.1088e-07
Epoch 129/512
448/448 - 0s - loss: 1.7672e-05 - val_loss: 1.0736e-07
Epoch 130/512
448/448 - 0s - loss: 1.7199e-05 - val_loss: 1.0710e-07
Epoch 131/512
448/448 - 0s - loss: 1.7255e-05 - val_loss: 1.0349e-07
Epoch 132/512
448/448 - 0s - loss: 1.6555e-05 - val_loss: 1.0119e-07
Epoch 133/512
448/448 - 0s - loss: 1.6460e-05 - val_loss: 9.7339e-08
Epoch 134/512
448/448 - 0s - loss: 1.5857e-05 - val_loss: 9.4573e-08
Epoch 135/512
448/448 - 0s - loss: 1.5585e-05 - val_loss: 9.3444e-08
Epoch 136/512
448/448 - 0s - loss: 1.5413e-05 - val_loss: 9.1760e-08
Epoch 137/512
448/448 - 0s - loss: 1.4996e-05 - val_loss: 8.8973e-08
Epoch 138/512
448/448 - 0s - loss: 1.4693e-05 - val_loss: 8.5934e-08
Epoch 139/512
448/448 - 0s - loss: 1.4275e-05 - val_loss: 8.4380e-08
Epoch 140/512
448/448 - 0s - loss: 1.4101e-05 - val_loss: 8.2054e-08
Epoch 141/512
448/448 - 0s - loss: 1.3709e-05 - val_loss: 7.9298e-08
Epoch 142/512
448/448 - 0s - loss: 1.3326e-05 - val_loss: 7.7805e-08
Epoch 143/512
448/448 - 0s - loss: 1.3086e-05 - val_loss: 7.7027e-08
Epoch 144/512
448/448 - 0s - loss: 1.3062e-05 - val_loss: 7.3229e-08
Epoch 145/512
448/448 - 0s - loss: 1.2259e-05 - val_loss: 7.1466e-08
Epoch 146/512
448/448 - 0s - loss: 1.2146e-05 - val_loss: 7.2165e-08
Epoch 147/512
448/448 - 0s - loss: 1.2291e-05 - val_loss: 6.8561e-08
Epoch 148/512
448/448 - 0s - loss: 1.1535e-05 - val_loss: 6.5028e-08
Epoch 149/512
448/448 - 0s - loss: 1.1180e-05 - val_loss: 6.5387e-08
Epoch 150/512
448/448 - 0s - loss: 1.1357e-05 - val_loss: 6.2955e-08
Epoch 151/512
448/448 - 0s - loss: 1.0738e-05 - val_loss: 6.0835e-08
Epoch 152/512
448/448 - 0s - loss: 1.0479e-05 - val_loss: 6.0319e-08
Epoch 153/512
448/448 - 0s - loss: 1.0453e-05 - val_loss: 5.8407e-08
Epoch 154/512
448/448 - 0s - loss: 1.0059e-05 - val_loss: 5.5587e-08
Epoch 155/512
448/448 - 0s - loss: 9.5960e-06 - val_loss: 5.5689e-08
Epoch 156/512
448/448 - 0s - loss: 9.7069e-06 - val_loss: 5.4932e-08
Epoch 157/512
448/448 - 0s - loss: 9.4512e-06 - val_loss: 5.1280e-08
Epoch 158/512
448/448 - 0s - loss: 8.8056e-06 - val_loss: 5.0534e-08
Epoch 159/512
448/448 - 0s - loss: 8.8942e-06 - val_loss: 4.9934e-08
Epoch 160/512
448/448 - 0s - loss: 8.6638e-06 - val_loss: 4.8311e-08
Epoch 161/512
448/448 - 0s - loss: 8.3224e-06 - val_loss: 4.6874e-08
Epoch 162/512
448/448 - 0s - loss: 8.1849e-06 - val_loss: 4.5359e-08
Epoch 163/512
448/448 - 0s - loss: 7.8790e-06 - val_loss: 4.4375e-08
Epoch 164/512
448/448 - 0s - loss: 7.7136e-06 - val_loss: 4.3678e-08
Epoch 165/512
448/448 - 0s - loss: 7.5601e-06 - val_loss: 4.2198e-08
Epoch 166/512
448/448 - 0s - loss: 7.2302e-06 - val_loss: 4.1660e-08
Epoch 167/512
448/448 - 0s - loss: 7.1207e-06 - val_loss: 4.1045e-08
Epoch 168/512
448/448 - 0s - loss: 6.9962e-06 - val_loss: 3.8708e-08
Epoch 169/512
448/448 - 0s - loss: 6.5664e-06 - val_loss: 3.7470e-08
Epoch 170/512
448/448 - 0s - loss: 6.5018e-06 - val_loss: 3.7002e-08
Epoch 171/512
448/448 - 0s - loss: 6.3380e-06 - val_loss: 3.6308e-08
Epoch 172/512
448/448 - 0s - loss: 6.1500e-06 - val_loss: 3.5313e-08
Epoch 173/512
448/448 - 0s - loss: 5.9305e-06 - val_loss: 3.4860e-08
Epoch 174/512
448/448 - 0s - loss: 5.8506e-06 - val_loss: 3.3522e-08
Epoch 175/512
448/448 - 0s - loss: 5.5815e-06 - val_loss: 3.2115e-08
Epoch 176/512
448/448 - 0s - loss: 5.4131e-06 - val_loss: 3.1354e-08
Epoch 177/512
448/448 - 0s - loss: 5.2492e-06 - val_loss: 3.1042e-08
Epoch 178/512
448/448 - 0s - loss: 5.1423e-06 - val_loss: 3.0711e-08
Epoch 179/512
448/448 - 0s - loss: 5.0379e-06 - val_loss: 2.9127e-08
Epoch 180/512
448/448 - 0s - loss: 4.7250e-06 - val_loss: 2.8343e-08
Epoch 181/512
448/448 - 0s - loss: 4.6571e-06 - val_loss: 2.7898e-08
Epoch 182/512
448/448 - 0s - loss: 4.5217e-06 - val_loss: 2.7522e-08
Epoch 183/512
448/448 - 0s - loss: 4.4122e-06 - val_loss: 2.6621e-08
Epoch 184/512
448/448 - 0s - loss: 4.2565e-06 - val_loss: 2.5117e-08
Epoch 185/512
448/448 - 0s - loss: 3.9916e-06 - val_loss: 2.4964e-08
Epoch 186/512
448/448 - 0s - loss: 3.9912e-06 - val_loss: 2.4853e-08
Epoch 187/512
448/448 - 0s - loss: 3.8829e-06 - val_loss: 2.3954e-08
Epoch 188/512
448/448 - 0s - loss: 3.6883e-06 - val_loss: 2.3193e-08
Epoch 189/512
448/448 - 0s - loss: 3.5652e-06 - val_loss: 2.2841e-08
Epoch 190/512
448/448 - 0s - loss: 3.5006e-06 - val_loss: 2.2218e-08
Epoch 191/512
448/448 - 0s - loss: 3.3631e-06 - val_loss: 2.1354e-08
Epoch 192/512
448/448 - 0s - loss: 3.2021e-06 - val_loss: 2.1041e-08
Epoch 193/512
448/448 - 0s - loss: 3.1703e-06 - val_loss: 2.0483e-08
Epoch 194/512
448/448 - 0s - loss: 3.0105e-06 - val_loss: 2.0010e-08
Epoch 195/512
448/448 - 0s - loss: 2.9292e-06 - val_loss: 1.9657e-08
Epoch 196/512
448/448 - 0s - loss: 2.8400e-06 - val_loss: 1.9078e-08
Epoch 197/512
448/448 - 0s - loss: 2.7367e-06 - val_loss: 1.8410e-08
Epoch 198/512
448/448 - 0s - loss: 2.6183e-06 - val_loss: 1.7986e-08
Epoch 199/512
448/448 - 0s - loss: 2.5452e-06 - val_loss: 1.7521e-08
Epoch 200/512
448/448 - 0s - loss: 2.4414e-06 - val_loss: 1.7212e-08
Epoch 201/512
448/448 - 0s - loss: 2.3912e-06 - val_loss: 1.6559e-08
Epoch 202/512
448/448 - 0s - loss: 2.2504e-06 - val_loss: 1.6254e-08
Epoch 203/512
448/448 - 0s - loss: 2.2037e-06 - val_loss: 1.6175e-08
Epoch 204/512
448/448 - 0s - loss: 2.1616e-06 - val_loss: 1.5498e-08
Epoch 205/512
448/448 - 0s - loss: 2.0241e-06 - val_loss: 1.4988e-08
Epoch 206/512
448/448 - 0s - loss: 1.9517e-06 - val_loss: 1.4889e-08
Epoch 207/512
448/448 - 0s - loss: 1.9356e-06 - val_loss: 1.4389e-08
Epoch 208/512
448/448 - 0s - loss: 1.8156e-06 - val_loss: 1.3984e-08
Epoch 209/512
448/448 - 0s - loss: 1.7607e-06 - val_loss: 1.3755e-08
Epoch 210/512
448/448 - 0s - loss: 1.7057e-06 - val_loss: 1.3379e-08
Epoch 211/512
448/448 - 0s - loss: 1.6362e-06 - val_loss: 1.2990e-08
Epoch 212/512
448/448 - 0s - loss: 1.5628e-06 - val_loss: 1.2841e-08
Epoch 213/512
448/448 - 0s - loss: 1.5465e-06 - val_loss: 1.2221e-08
Epoch 214/512
448/448 - 0s - loss: 1.4202e-06 - val_loss: 1.2016e-08
Epoch 215/512
448/448 - 0s - loss: 1.4044e-06 - val_loss: 1.2026e-08
Epoch 216/512
448/448 - 0s - loss: 1.3825e-06 - val_loss: 1.1523e-08
Epoch 217/512
448/448 - 0s - loss: 1.2782e-06 - val_loss: 1.1147e-08
Epoch 218/512
448/448 - 0s - loss: 1.2355e-06 - val_loss: 1.1046e-08
Epoch 219/512
448/448 - 0s - loss: 1.2242e-06 - val_loss: 1.0553e-08
Epoch 220/512
448/448 - 0s - loss: 1.1188e-06 - val_loss: 1.0483e-08
Epoch 221/512
448/448 - 0s - loss: 1.1240e-06 - val_loss: 1.0397e-08
Epoch 222/512
448/448 - 0s - loss: 1.0836e-06 - val_loss: 9.8847e-09
Epoch 223/512
448/448 - 0s - loss: 9.9716e-07 - val_loss: 9.6821e-09
Epoch 224/512
448/448 - 0s - loss: 9.8064e-07 - val_loss: 9.6850e-09
Epoch 225/512
448/448 - 0s - loss: 9.6691e-07 - val_loss: 9.3715e-09
Epoch 226/512
448/448 - 0s - loss: 9.0706e-07 - val_loss: 8.8403e-09
Epoch 227/512
448/448 - 0s - loss: 8.3354e-07 - val_loss: 8.9131e-09
Epoch 228/512
448/448 - 0s - loss: 8.6247e-07 - val_loss: 8.8088e-09
Epoch 229/512
448/448 - 0s - loss: 8.1307e-07 - val_loss: 8.3040e-09
Epoch 230/512
448/448 - 0s - loss: 7.4166e-07 - val_loss: 8.2472e-09
Epoch 231/512
448/448 - 0s - loss: 7.5046e-07 - val_loss: 8.1550e-09
Epoch 232/512
448/448 - 0s - loss: 7.1632e-07 - val_loss: 7.8446e-09
Epoch 233/512
448/448 - 0s - loss: 6.7074e-07 - val_loss: 7.5947e-09
Epoch 234/512
448/448 - 0s - loss: 6.4182e-07 - val_loss: 7.5627e-09
Epoch 235/512
448/448 - 0s - loss: 6.3596e-07 - val_loss: 7.3781e-09
Epoch 236/512
448/448 - 0s - loss: 5.9681e-07 - val_loss: 7.1293e-09
Epoch 237/512
448/448 - 0s - loss: 5.6151e-07 - val_loss: 7.1092e-09
Epoch 238/512
448/448 - 0s - loss: 5.6190e-07 - val_loss: 6.9864e-09
Epoch 239/512
448/448 - 0s - loss: 5.3520e-07 - val_loss: 6.6354e-09
Epoch 240/512
448/448 - 0s - loss: 4.8883e-07 - val_loss: 6.5452e-09
Epoch 241/512
448/448 - 0s - loss: 4.8535e-07 - val_loss: 6.5308e-09
Epoch 242/512
448/448 - 0s - loss: 4.7477e-07 - val_loss: 6.2700e-09
Epoch 243/512
448/448 - 0s - loss: 4.3318e-07 - val_loss: 6.1541e-09
Epoch 244/512
448/448 - 0s - loss: 4.2642e-07 - val_loss: 6.0804e-09
Epoch 245/512
448/448 - 0s - loss: 4.1264e-07 - val_loss: 5.9040e-09
Epoch 246/512
448/448 - 0s - loss: 3.8769e-07 - val_loss: 5.7363e-09
Epoch 247/512
448/448 - 0s - loss: 3.6833e-07 - val_loss: 5.6762e-09
Epoch 248/512
448/448 - 0s - loss: 3.5969e-07 - val_loss: 5.6313e-09
Epoch 249/512
448/448 - 0s - loss: 3.5143e-07 - val_loss: 5.3877e-09
Epoch 250/512
448/448 - 0s - loss: 3.1909e-07 - val_loss: 5.2358e-09
Epoch 251/512
448/448 - 0s - loss: 3.0983e-07 - val_loss: 5.2172e-09
Epoch 252/512
448/448 - 0s - loss: 3.0451e-07 - val_loss: 5.1368e-09
Epoch 253/512
448/448 - 0s - loss: 2.9196e-07 - val_loss: 4.8871e-09
Epoch 254/512
448/448 - 0s - loss: 2.6321e-07 - val_loss: 4.8462e-09
Epoch 255/512
448/448 - 0s - loss: 2.6455e-07 - val_loss: 4.8482e-09
Epoch 256/512
448/448 - 0s - loss: 2.5891e-07 - val_loss: 4.6378e-09
Epoch 257/512
448/448 - 0s - loss: 2.3369e-07 - val_loss: 4.5014e-09
Epoch 258/512
448/448 - 0s - loss: 2.2543e-07 - val_loss: 4.5061e-09
Epoch 259/512
448/448 - 0s - loss: 2.2476e-07 - val_loss: 4.4361e-09
Epoch 260/512
448/448 - 0s - loss: 2.1334e-07 - val_loss: 4.2421e-09
Epoch 261/512
448/448 - 0s - loss: 1.9275e-07 - val_loss: 4.1970e-09
Epoch 262/512
448/448 - 0s - loss: 1.9117e-07 - val_loss: 4.2228e-09
Epoch 263/512
448/448 - 0s - loss: 1.9220e-07 - val_loss: 4.0434e-09
Epoch 264/512
448/448 - 0s - loss: 1.6976e-07 - val_loss: 3.9156e-09
Epoch 265/512
448/448 - 0s - loss: 1.6295e-07 - val_loss: 3.9269e-09
Epoch 266/512
448/448 - 0s - loss: 1.6561e-07 - val_loss: 3.8445e-09
Epoch 267/512
448/448 - 0s - loss: 1.5246e-07 - val_loss: 3.7266e-09
Epoch 268/512
448/448 - 0s - loss: 1.4326e-07 - val_loss: 3.6845e-09
Epoch 269/512
448/448 - 0s - loss: 1.4023e-07 - val_loss: 3.6514e-09
Epoch 270/512
448/448 - 0s - loss: 1.3631e-07 - val_loss: 3.5439e-09
Epoch 271/512
448/448 - 0s - loss: 1.2516e-07 - val_loss: 3.4640e-09
Epoch 272/512
448/448 - 0s - loss: 1.2026e-07 - val_loss: 3.4361e-09
Epoch 273/512
448/448 - 0s - loss: 1.1887e-07 - val_loss: 3.3609e-09
Epoch 274/512
448/448 - 0s - loss: 1.1036e-07 - val_loss: 3.2797e-09
Epoch 275/512
448/448 - 0s - loss: 1.0522e-07 - val_loss: 3.2390e-09
Epoch 276/512
448/448 - 0s - loss: 1.0251e-07 - val_loss: 3.1783e-09
Epoch 277/512
448/448 - 0s - loss: 9.6389e-08 - val_loss: 3.1187e-09
Epoch 278/512
448/448 - 0s - loss: 9.1445e-08 - val_loss: 3.1003e-09
Epoch 279/512
448/448 - 0s - loss: 9.1348e-08 - val_loss: 3.0198e-09
Epoch 280/512
448/448 - 0s - loss: 8.3162e-08 - val_loss: 2.9528e-09
Epoch 281/512
448/448 - 0s - loss: 8.0019e-08 - val_loss: 2.9130e-09
Epoch 282/512
448/448 - 0s - loss: 7.7527e-08 - val_loss: 2.8663e-09
Epoch 283/512
448/448 - 0s - loss: 7.3617e-08 - val_loss: 2.8175e-09
Epoch 284/512
448/448 - 0s - loss: 7.0919e-08 - val_loss: 2.7547e-09
Epoch 285/512
448/448 - 0s - loss: 6.6434e-08 - val_loss: 2.7089e-09
Epoch 286/512
448/448 - 0s - loss: 6.3876e-08 - val_loss: 2.6851e-09
Epoch 287/512
448/448 - 0s - loss: 6.2618e-08 - val_loss: 2.6234e-09
Epoch 288/512
448/448 - 0s - loss: 5.7283e-08 - val_loss: 2.5835e-09
Epoch 289/512
448/448 - 0s - loss: 5.6290e-08 - val_loss: 2.5489e-09
Epoch 290/512
448/448 - 0s - loss: 5.3906e-08 - val_loss: 2.5057e-09
Epoch 291/512
448/448 - 0s - loss: 5.1163e-08 - val_loss: 2.4661e-09
Epoch 292/512
448/448 - 0s - loss: 4.8864e-08 - val_loss: 2.4267e-09
Epoch 293/512
448/448 - 0s - loss: 4.6795e-08 - val_loss: 2.3886e-09
Epoch 294/512
448/448 - 0s - loss: 4.4622e-08 - val_loss: 2.3560e-09
Epoch 295/512
448/448 - 0s - loss: 4.2872e-08 - val_loss: 2.3189e-09
Epoch 296/512
448/448 - 0s - loss: 4.0914e-08 - val_loss: 2.2765e-09
Epoch 297/512
448/448 - 0s - loss: 3.8246e-08 - val_loss: 2.2578e-09
Epoch 298/512
448/448 - 0s - loss: 3.8118e-08 - val_loss: 2.2287e-09
Epoch 299/512
448/448 - 0s - loss: 3.6067e-08 - val_loss: 2.1829e-09
Epoch 300/512
448/448 - 0s - loss: 3.3934e-08 - val_loss: 2.1397e-09
Epoch 301/512
448/448 - 0s - loss: 3.1549e-08 - val_loss: 2.1352e-09
Epoch 302/512
448/448 - 0s - loss: 3.2478e-08 - val_loss: 2.1084e-09
Epoch 303/512
448/448 - 0s - loss: 3.0374e-08 - val_loss: 2.0607e-09
Epoch 304/512
448/448 - 0s - loss: 2.7999e-08 - val_loss: 2.0321e-09
Epoch 305/512
448/448 - 0s - loss: 2.7206e-08 - val_loss: 2.0118e-09
Epoch 306/512
448/448 - 0s - loss: 2.6388e-08 - val_loss: 1.9979e-09
Epoch 307/512
448/448 - 0s - loss: 2.6076e-08 - val_loss: 1.9492e-09
Epoch 308/512
448/448 - 0s - loss: 2.3148e-08 - val_loss: 1.9278e-09
Epoch 309/512
448/448 - 0s - loss: 2.2937e-08 - val_loss: 1.9122e-09
Epoch 310/512
448/448 - 0s - loss: 2.2549e-08 - val_loss: 1.8904e-09
Epoch 311/512
448/448 - 0s - loss: 2.1389e-08 - val_loss: 1.8539e-09
Epoch 312/512
448/448 - 0s - loss: 2.0038e-08 - val_loss: 1.8283e-09
Epoch 313/512
448/448 - 0s - loss: 1.9099e-08 - val_loss: 1.8193e-09
Epoch 314/512
448/448 - 0s - loss: 1.9242e-08 - val_loss: 1.8003e-09
Epoch 315/512
448/448 - 0s - loss: 1.8216e-08 - val_loss: 1.7696e-09
Epoch 316/512
448/448 - 0s - loss: 1.7058e-08 - val_loss: 1.7457e-09
Epoch 317/512
448/448 - 0s - loss: 1.6406e-08 - val_loss: 1.7209e-09
Epoch 318/512
448/448 - 0s - loss: 1.5696e-08 - val_loss: 1.7100e-09
Epoch 319/512
448/448 - 0s - loss: 1.5491e-08 - val_loss: 1.6898e-09
Epoch 320/512
448/448 - 0s - loss: 1.4683e-08 - val_loss: 1.6681e-09
Epoch 321/512
448/448 - 0s - loss: 1.3883e-08 - val_loss: 1.6515e-09
Epoch 322/512
448/448 - 0s - loss: 1.3649e-08 - val_loss: 1.6339e-09
Epoch 323/512
448/448 - 0s - loss: 1.3179e-08 - val_loss: 1.6123e-09
Epoch 324/512
448/448 - 0s - loss: 1.2479e-08 - val_loss: 1.5856e-09
Epoch 325/512
448/448 - 0s - loss: 1.1551e-08 - val_loss: 1.5762e-09
Epoch 326/512
448/448 - 0s - loss: 1.1750e-08 - val_loss: 1.5680e-09
Epoch 327/512
448/448 - 0s - loss: 1.1626e-08 - val_loss: 1.5424e-09
Epoch 328/512
448/448 - 0s - loss: 1.0571e-08 - val_loss: 1.5200e-09
Epoch 329/512
448/448 - 0s - loss: 1.0090e-08 - val_loss: 1.5098e-09
Epoch 330/512
448/448 - 0s - loss: 1.0075e-08 - val_loss: 1.5006e-09
Epoch 331/512
448/448 - 0s - loss: 9.9102e-09 - val_loss: 1.4795e-09
Epoch 332/512
448/448 - 0s - loss: 9.1755e-09 - val_loss: 1.4595e-09
Epoch 333/512
448/448 - 0s - loss: 8.6971e-09 - val_loss: 1.4465e-09
Epoch 334/512
448/448 - 0s - loss: 8.6052e-09 - val_loss: 1.4355e-09
Epoch 335/512
448/448 - 0s - loss: 8.4880e-09 - val_loss: 1.4202e-09
Epoch 336/512
448/448 - 0s - loss: 8.1151e-09 - val_loss: 1.3988e-09
Epoch 337/512
448/448 - 0s - loss: 7.5461e-09 - val_loss: 1.3858e-09
Epoch 338/512
448/448 - 0s - loss: 7.3981e-09 - val_loss: 1.3763e-09
Epoch 339/512
448/448 - 0s - loss: 7.3115e-09 - val_loss: 1.3676e-09
Epoch 340/512
448/448 - 0s - loss: 7.2064e-09 - val_loss: 1.3508e-09
Epoch 341/512
448/448 - 0s - loss: 6.7582e-09 - val_loss: 1.3342e-09
Epoch 342/512
448/448 - 0s - loss: 6.4233e-09 - val_loss: 1.3233e-09
Epoch 343/512
448/448 - 0s - loss: 6.2718e-09 - val_loss: 1.3148e-09
Epoch 344/512
448/448 - 0s - loss: 6.3292e-09 - val_loss: 1.3037e-09
Epoch 345/512
448/448 - 0s - loss: 6.0577e-09 - val_loss: 1.2864e-09
Epoch 346/512
448/448 - 0s - loss: 5.6839e-09 - val_loss: 1.2702e-09
Epoch 347/512
448/448 - 0s - loss: 5.4407e-09 - val_loss: 1.2625e-09
Epoch 348/512
448/448 - 0s - loss: 5.4255e-09 - val_loss: 1.2540e-09
Epoch 349/512
448/448 - 0s - loss: 5.3708e-09 - val_loss: 1.2446e-09
Epoch 350/512
448/448 - 0s - loss: 5.2052e-09 - val_loss: 1.2308e-09
Epoch 351/512
448/448 - 0s - loss: 4.9560e-09 - val_loss: 1.2156e-09
Epoch 352/512
448/448 - 0s - loss: 4.6828e-09 - val_loss: 1.2025e-09
Epoch 353/512
448/448 - 0s - loss: 4.5897e-09 - val_loss: 1.1954e-09
Epoch 354/512
448/448 - 0s - loss: 4.5442e-09 - val_loss: 1.1875e-09
Epoch 355/512
448/448 - 0s - loss: 4.4718e-09 - val_loss: 1.1769e-09
Epoch 356/512
448/448 - 0s - loss: 4.3130e-09 - val_loss: 1.1652e-09
Epoch 357/512
448/448 - 0s - loss: 4.1594e-09 - val_loss: 1.1545e-09
Epoch 358/512
448/448 - 0s - loss: 4.0153e-09 - val_loss: 1.1457e-09
Epoch 359/512
448/448 - 0s - loss: 3.9540e-09 - val_loss: 1.1387e-09
Epoch 360/512
448/448 - 0s - loss: 3.9360e-09 - val_loss: 1.1277e-09
Epoch 361/512
448/448 - 0s - loss: 3.7433e-09 - val_loss: 1.1154e-09
Epoch 362/512
448/448 - 0s - loss: 3.5726e-09 - val_loss: 1.1069e-09
Epoch 363/512
448/448 - 0s - loss: 3.5260e-09 - val_loss: 1.1004e-09
Epoch 364/512
448/448 - 0s - loss: 3.4905e-09 - val_loss: 1.0911e-09
Epoch 365/512
448/448 - 0s - loss: 3.4104e-09 - val_loss: 1.0821e-09
Epoch 366/512
448/448 - 0s - loss: 3.3124e-09 - val_loss: 1.0728e-09
Epoch 367/512
448/448 - 0s - loss: 3.1552e-09 - val_loss: 1.0624e-09
Epoch 368/512
448/448 - 0s - loss: 3.0788e-09 - val_loss: 1.0561e-09
Epoch 369/512
448/448 - 0s - loss: 3.0749e-09 - val_loss: 1.0497e-09
Epoch 370/512
448/448 - 0s - loss: 3.0448e-09 - val_loss: 1.0385e-09
Epoch 371/512
448/448 - 0s - loss: 2.8800e-09 - val_loss: 1.0296e-09
Epoch 372/512
448/448 - 0s - loss: 2.8035e-09 - val_loss: 1.0232e-09
Epoch 373/512
448/448 - 0s - loss: 2.7832e-09 - val_loss: 1.0169e-09
Epoch 374/512
448/448 - 0s - loss: 2.7643e-09 - val_loss: 1.0086e-09
Epoch 375/512
448/448 - 0s - loss: 2.6757e-09 - val_loss: 9.9828e-10
Epoch 376/512
448/448 - 0s - loss: 2.5484e-09 - val_loss: 9.9121e-10
Epoch 377/512
448/448 - 0s - loss: 2.5013e-09 - val_loss: 9.8290e-10
Epoch 378/512
448/448 - 0s - loss: 2.4286e-09 - val_loss: 9.7650e-10
Epoch 379/512
448/448 - 0s - loss: 2.4202e-09 - val_loss: 9.6992e-10
Epoch 380/512
448/448 - 0s - loss: 2.3702e-09 - val_loss: 9.6375e-10
Epoch 381/512
448/448 - 0s - loss: 2.3339e-09 - val_loss: 9.5720e-10
Epoch 382/512
448/448 - 0s - loss: 2.2735e-09 - val_loss: 9.4913e-10
Epoch 383/512
448/448 - 0s - loss: 2.2254e-09 - val_loss: 9.4296e-10
Epoch 384/512
448/448 - 0s - loss: 2.1961e-09 - val_loss: 9.3706e-10
Epoch 385/512
448/448 - 0s - loss: 2.1598e-09 - val_loss: 9.2853e-10
Epoch 386/512
448/448 - 0s - loss: 2.0738e-09 - val_loss: 9.2260e-10
Epoch 387/512
448/448 - 0s - loss: 2.0573e-09 - val_loss: 9.1607e-10
Epoch 388/512
448/448 - 0s - loss: 1.9764e-09 - val_loss: 9.1046e-10
Epoch 389/512
448/448 - 0s - loss: 1.9928e-09 - val_loss: 9.0325e-10
Epoch 390/512
448/448 - 0s - loss: 1.9337e-09 - val_loss: 8.9681e-10
Epoch 391/512
448/448 - 0s - loss: 1.8767e-09 - val_loss: 8.9050e-10
Epoch 392/512
448/448 - 0s - loss: 1.8301e-09 - val_loss: 8.8677e-10
Epoch 393/512
448/448 - 0s - loss: 1.8714e-09 - val_loss: 8.8010e-10
Epoch 394/512
448/448 - 0s - loss: 1.8083e-09 - val_loss: 8.7554e-10
Epoch 395/512
448/448 - 0s - loss: 1.8210e-09 - val_loss: 8.6804e-10
Epoch 396/512
448/448 - 0s - loss: 1.7062e-09 - val_loss: 8.6060e-10
Epoch 397/512
448/448 - 0s - loss: 1.6573e-09 - val_loss: 8.5561e-10
Epoch 398/512
448/448 - 0s - loss: 1.6506e-09 - val_loss: 8.5103e-10
Epoch 399/512
448/448 - 0s - loss: 1.6480e-09 - val_loss: 8.4613e-10
Epoch 400/512
448/448 - 0s - loss: 1.6467e-09 - val_loss: 8.4201e-10
Epoch 401/512
448/448 - 0s - loss: 1.6336e-09 - val_loss: 8.3486e-10
Epoch 402/512
448/448 - 0s - loss: 1.5626e-09 - val_loss: 8.2789e-10
Epoch 403/512
448/448 - 0s - loss: 1.4965e-09 - val_loss: 8.2285e-10
Epoch 404/512
448/448 - 0s - loss: 1.4783e-09 - val_loss: 8.1698e-10
Epoch 405/512
448/448 - 0s - loss: 1.4537e-09 - val_loss: 8.1335e-10
Epoch 406/512
448/448 - 0s - loss: 1.4676e-09 - val_loss: 8.0950e-10
Epoch 407/512
448/448 - 0s - loss: 1.4637e-09 - val_loss: 8.0458e-10
Epoch 408/512
448/448 - 0s - loss: 1.4449e-09 - val_loss: 7.9826e-10
Epoch 409/512
448/448 - 0s - loss: 1.3638e-09 - val_loss: 7.9316e-10
Epoch 410/512
448/448 - 0s - loss: 1.3619e-09 - val_loss: 7.8829e-10
Epoch 411/512
448/448 - 0s - loss: 1.3561e-09 - val_loss: 7.8425e-10
Epoch 412/512
448/448 - 0s - loss: 1.3404e-09 - val_loss: 7.7969e-10
Epoch 413/512
448/448 - 0s - loss: 1.2942e-09 - val_loss: 7.7445e-10
Epoch 414/512
448/448 - 0s - loss: 1.2977e-09 - val_loss: 7.7126e-10
Epoch 415/512
448/448 - 0s - loss: 1.2837e-09 - val_loss: 7.6678e-10
Epoch 416/512
448/448 - 0s - loss: 1.2678e-09 - val_loss: 7.6108e-10
Epoch 417/512
448/448 - 0s - loss: 1.2281e-09 - val_loss: 7.5773e-10
Epoch 418/512
448/448 - 0s - loss: 1.2123e-09 - val_loss: 7.5262e-10
Epoch 419/512
448/448 - 0s - loss: 1.1951e-09 - val_loss: 7.4826e-10
Epoch 420/512
448/448 - 0s - loss: 1.1792e-09 - val_loss: 7.4473e-10
Epoch 421/512
448/448 - 0s - loss: 1.1731e-09 - val_loss: 7.4110e-10
Epoch 422/512
448/448 - 0s - loss: 1.1675e-09 - val_loss: 7.3693e-10
Epoch 423/512
448/448 - 0s - loss: 1.1500e-09 - val_loss: 7.3267e-10
Epoch 424/512
448/448 - 0s - loss: 1.1320e-09 - val_loss: 7.2712e-10
Epoch 425/512
448/448 - 0s - loss: 1.0911e-09 - val_loss: 7.2314e-10
Epoch 426/512
448/448 - 0s - loss: 1.0830e-09 - val_loss: 7.1880e-10
Epoch 427/512
448/448 - 0s - loss: 1.0614e-09 - val_loss: 7.1593e-10
Epoch 428/512
448/448 - 0s - loss: 1.0714e-09 - val_loss: 7.1244e-10
Epoch 429/512
448/448 - 0s - loss: 1.0783e-09 - val_loss: 7.0918e-10
Epoch 430/512
448/448 - 0s - loss: 1.0555e-09 - val_loss: 7.0377e-10
Epoch 431/512
448/448 - 0s - loss: 1.0087e-09 - val_loss: 6.9941e-10
Epoch 432/512
448/448 - 0s - loss: 9.9272e-10 - val_loss: 6.9589e-10
Epoch 433/512
448/448 - 0s - loss: 9.8084e-10 - val_loss: 6.9253e-10
Epoch 434/512
448/448 - 0s - loss: 9.7432e-10 - val_loss: 6.8944e-10
Epoch 435/512
448/448 - 0s - loss: 9.8737e-10 - val_loss: 6.8613e-10
Epoch 436/512
448/448 - 0s - loss: 9.7512e-10 - val_loss: 6.8214e-10
Epoch 437/512
448/448 - 0s - loss: 9.4980e-10 - val_loss: 6.7740e-10
Epoch 438/512
448/448 - 0s - loss: 9.1670e-10 - val_loss: 6.7381e-10
Epoch 439/512
448/448 - 0s - loss: 9.1146e-10 - val_loss: 6.7163e-10
Epoch 440/512
448/448 - 0s - loss: 9.2650e-10 - val_loss: 6.6915e-10
Epoch 441/512
448/448 - 0s - loss: 9.4831e-10 - val_loss: 6.6584e-10
Epoch 442/512
448/448 - 0s - loss: 9.2685e-10 - val_loss: 6.6186e-10
Epoch 443/512
448/448 - 0s - loss: 8.9702e-10 - val_loss: 6.5607e-10
Epoch 444/512
448/448 - 0s - loss: 8.3418e-10 - val_loss: 6.5260e-10
Epoch 445/512
448/448 - 0s - loss: 8.4253e-10 - val_loss: 6.5114e-10
Epoch 446/512
448/448 - 0s - loss: 8.5844e-10 - val_loss: 6.4860e-10
Epoch 447/512
448/448 - 0s - loss: 8.7123e-10 - val_loss: 6.4571e-10
Epoch 448/512
448/448 - 0s - loss: 8.6370e-10 - val_loss: 6.4301e-10
Epoch 449/512
448/448 - 0s - loss: 8.4820e-10 - val_loss: 6.3809e-10
Epoch 450/512
448/448 - 0s - loss: 8.0854e-10 - val_loss: 6.3419e-10
Epoch 451/512
448/448 - 0s - loss: 7.8877e-10 - val_loss: 6.3066e-10
Epoch 452/512
448/448 - 0s - loss: 7.8089e-10 - val_loss: 6.2881e-10
Epoch 453/512
448/448 - 0s - loss: 7.9127e-10 - val_loss: 6.2671e-10
Epoch 454/512
448/448 - 0s - loss: 8.0694e-10 - val_loss: 6.2401e-10
Epoch 455/512
448/448 - 0s - loss: 7.9442e-10 - val_loss: 6.2047e-10
Epoch 456/512
448/448 - 0s - loss: 7.8618e-10 - val_loss: 6.1804e-10
Epoch 457/512
448/448 - 0s - loss: 7.6402e-10 - val_loss: 6.1388e-10
Epoch 458/512
448/448 - 0s - loss: 7.4348e-10 - val_loss: 6.1153e-10
Epoch 459/512
448/448 - 0s - loss: 7.4661e-10 - val_loss: 6.0911e-10
Epoch 460/512
448/448 - 0s - loss: 7.4884e-10 - val_loss: 6.0572e-10
Epoch 461/512
448/448 - 0s - loss: 7.3359e-10 - val_loss: 6.0363e-10
Epoch 462/512
448/448 - 0s - loss: 7.3524e-10 - val_loss: 6.0170e-10
Epoch 463/512
448/448 - 0s - loss: 7.3349e-10 - val_loss: 5.9779e-10
Epoch 464/512
448/448 - 0s - loss: 7.1630e-10 - val_loss: 5.9384e-10
Epoch 465/512
448/448 - 0s - loss: 6.9711e-10 - val_loss: 5.9169e-10
Epoch 466/512
448/448 - 0s - loss: 6.9757e-10 - val_loss: 5.8954e-10
Epoch 467/512
448/448 - 0s - loss: 6.9291e-10 - val_loss: 5.8682e-10
Epoch 468/512
448/448 - 0s - loss: 6.9647e-10 - val_loss: 5.8497e-10
Epoch 469/512
448/448 - 0s - loss: 6.9540e-10 - val_loss: 5.8250e-10
Epoch 470/512
448/448 - 0s - loss: 6.8693e-10 - val_loss: 5.7924e-10
Epoch 471/512
448/448 - 0s - loss: 6.6658e-10 - val_loss: 5.7576e-10
Epoch 472/512
448/448 - 0s - loss: 6.4847e-10 - val_loss: 5.7356e-10
Epoch 473/512
448/448 - 0s - loss: 6.5023e-10 - val_loss: 5.7284e-10
Epoch 474/512
448/448 - 0s - loss: 6.7314e-10 - val_loss: 5.6981e-10
Epoch 475/512
448/448 - 0s - loss: 6.6514e-10 - val_loss: 5.6690e-10
Epoch 476/512
448/448 - 0s - loss: 6.4353e-10 - val_loss: 5.6374e-10
Epoch 477/512
448/448 - 0s - loss: 6.2950e-10 - val_loss: 5.6075e-10
Epoch 478/512
448/448 - 0s - loss: 6.1526e-10 - val_loss: 5.5842e-10
Epoch 479/512
448/448 - 0s - loss: 6.1131e-10 - val_loss: 5.5640e-10
Epoch 480/512
448/448 - 0s - loss: 6.0959e-10 - val_loss: 5.5507e-10
Epoch 481/512
448/448 - 0s - loss: 6.1703e-10 - val_loss: 5.5206e-10
Epoch 482/512
448/448 - 0s - loss: 5.9914e-10 - val_loss: 5.4893e-10
Epoch 483/512
448/448 - 0s - loss: 5.8578e-10 - val_loss: 5.4779e-10
Epoch 484/512
448/448 - 0s - loss: 6.0386e-10 - val_loss: 5.4587e-10
Epoch 485/512
448/448 - 0s - loss: 5.9829e-10 - val_loss: 5.4317e-10
Epoch 486/512
448/448 - 0s - loss: 5.8823e-10 - val_loss: 5.4046e-10
Epoch 487/512
448/448 - 0s - loss: 5.8760e-10 - val_loss: 5.3827e-10
Epoch 488/512
448/448 - 0s - loss: 5.7480e-10 - val_loss: 5.3655e-10
Epoch 489/512
448/448 - 0s - loss: 5.7392e-10 - val_loss: 5.3420e-10
Epoch 490/512
448/448 - 0s - loss: 5.6974e-10 - val_loss: 5.3231e-10
Epoch 491/512
448/448 - 0s - loss: 5.6881e-10 - val_loss: 5.3015e-10
Epoch 492/512
448/448 - 0s - loss: 5.6404e-10 - val_loss: 5.2759e-10
Epoch 493/512
448/448 - 0s - loss: 5.5153e-10 - val_loss: 5.2519e-10
Epoch 494/512
448/448 - 0s - loss: 5.4742e-10 - val_loss: 5.2291e-10
Epoch 495/512
448/448 - 0s - loss: 5.3962e-10 - val_loss: 5.2060e-10
Epoch 496/512
448/448 - 0s - loss: 5.2975e-10 - val_loss: 5.1883e-10
Epoch 497/512
448/448 - 0s - loss: 5.3562e-10 - val_loss: 5.1715e-10
Epoch 498/512
448/448 - 0s - loss: 5.3513e-10 - val_loss: 5.1494e-10
Epoch 499/512
448/448 - 0s - loss: 5.3423e-10 - val_loss: 5.1301e-10
Epoch 500/512
448/448 - 0s - loss: 5.2352e-10 - val_loss: 5.1045e-10
Epoch 501/512
448/448 - 0s - loss: 5.1493e-10 - val_loss: 5.0846e-10
Epoch 502/512
448/448 - 0s - loss: 5.1319e-10 - val_loss: 5.0710e-10
Epoch 503/512
448/448 - 0s - loss: 5.1590e-10 - val_loss: 5.0513e-10
Epoch 504/512
448/448 - 0s - loss: 5.1600e-10 - val_loss: 5.0398e-10
Epoch 505/512
448/448 - 0s - loss: 5.1703e-10 - val_loss: 5.0163e-10
Epoch 506/512
448/448 - 0s - loss: 5.0546e-10 - val_loss: 4.9939e-10
Epoch 507/512
448/448 - 0s - loss: 4.9958e-10 - val_loss: 4.9715e-10
Epoch 508/512
448/448 - 0s - loss: 4.8907e-10 - val_loss: 4.9577e-10
Epoch 509/512
448/448 - 0s - loss: 4.9932e-10 - val_loss: 4.9463e-10
Epoch 510/512
448/448 - 0s - loss: 4.9386e-10 - val_loss: 4.9195e-10
Epoch 511/512
448/448 - 0s - loss: 4.8640e-10 - val_loss: 4.9004e-10
Epoch 512/512
448/448 - 0s - loss: 4.8886e-10 - val_loss: 4.8859e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.8748e-09 - val_loss: 2.2435e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4467e-08 - val_loss: 1.9395e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1801e-09 - val_loss: 4.1473e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.6511e-10 - val_loss: 3.4638e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4122e-10 - val_loss: 8.5581e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5917e-09 - val_loss: 4.2411e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3165e-09 - val_loss: 7.0116e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8642e-09 - val_loss: 2.8031e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2920e-09 - val_loss: 1.4486e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4349e-09 - val_loss: 1.5301e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8629e-09 - val_loss: 2.6817e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3263e-09 - val_loss: 3.8776e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9404e-09 - val_loss: 3.1474e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9180e-09 - val_loss: 2.1677e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1047e-09 - val_loss: 1.9248e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0527e-09 - val_loss: 2.1876e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4233e-09 - val_loss: 2.6136e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7494e-09 - val_loss: 2.6208e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6157e-09 - val_loss: 2.2667e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2293e-09 - val_loss: 1.9806e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0219e-09 - val_loss: 1.9755e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0717e-09 - val_loss: 2.0911e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1935e-09 - val_loss: 2.1328e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1830e-09 - val_loss: 2.0210e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0415e-09 - val_loss: 1.8969e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9365e-09 - val_loss: 1.8274e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8820e-09 - val_loss: 1.8281e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8877e-09 - val_loss: 1.7764e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8337e-09 - val_loss: 1.7253e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7773e-09 - val_loss: 1.6898e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7426e-09 - val_loss: 1.6791e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7267e-09 - val_loss: 1.6356e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6733e-09 - val_loss: 1.5764e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6185e-09 - val_loss: 1.5443e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5866e-09 - val_loss: 1.5373e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5811e-09 - val_loss: 1.5185e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5485e-09 - val_loss: 1.4395e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4718e-09 - val_loss: 1.3972e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4357e-09 - val_loss: 1.3846e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4332e-09 - val_loss: 1.3830e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4252e-09 - val_loss: 1.3727e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3992e-09 - val_loss: 1.3194e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3463e-09 - val_loss: 1.2711e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3045e-09 - val_loss: 1.2407e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2748e-09 - val_loss: 1.2445e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2787e-09 - val_loss: 1.2363e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2668e-09 - val_loss: 1.2085e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2270e-09 - val_loss: 1.1773e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2037e-09 - val_loss: 1.1508e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1672e-09 - val_loss: 1.1024e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1309e-09 - val_loss: 1.1025e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1287e-09 - val_loss: 1.0938e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1193e-09 - val_loss: 1.0816e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1054e-09 - val_loss: 1.0353e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0554e-09 - val_loss: 1.0158e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0366e-09 - val_loss: 9.8441e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0062e-09 - val_loss: 9.7752e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0079e-09 - val_loss: 9.7538e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0038e-09 - val_loss: 9.6683e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9098e-10 - val_loss: 9.5788e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6987e-10 - val_loss: 9.3018e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4962e-10 - val_loss: 9.0488e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2035e-10 - val_loss: 8.8289e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0117e-10 - val_loss: 8.7978e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9763e-10 - val_loss: 8.7415e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8924e-10 - val_loss: 8.7391e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9514e-10 - val_loss: 8.4113e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5113e-10 - val_loss: 8.2002e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3375e-10 - val_loss: 7.8927e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0454e-10 - val_loss: 7.8546e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0487e-10 - val_loss: 7.8558e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0242e-10 - val_loss: 7.7296e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8718e-10 - val_loss: 7.5970e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7025e-10 - val_loss: 7.5358e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6584e-10 - val_loss: 7.3601e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4594e-10 - val_loss: 7.2527e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4023e-10 - val_loss: 7.1066e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2429e-10 - val_loss: 6.9884e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1565e-10 - val_loss: 6.9595e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1080e-10 - val_loss: 6.9792e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0988e-10 - val_loss: 6.9535e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0306e-10 - val_loss: 6.7441e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8142e-10 - val_loss: 6.4584e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5703e-10 - val_loss: 6.4350e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5726e-10 - val_loss: 6.4233e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5530e-10 - val_loss: 6.3218e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4682e-10 - val_loss: 6.2047e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3418e-10 - val_loss: 6.1844e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3058e-10 - val_loss: 6.0955e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1840e-10 - val_loss: 6.0745e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1746e-10 - val_loss: 6.0294e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0648e-10 - val_loss: 5.7697e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8107e-10 - val_loss: 5.5810e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6662e-10 - val_loss: 5.5394e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6644e-10 - val_loss: 5.6568e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7953e-10 - val_loss: 5.6853e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7619e-10 - val_loss: 5.5162e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5608e-10 - val_loss: 5.3452e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4275e-10 - val_loss: 5.2780e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3745e-10 - val_loss: 5.2011e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3005e-10 - val_loss: 5.1117e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1957e-10 - val_loss: 5.0034e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1206e-10 - val_loss: 4.9918e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0823e-10 - val_loss: 4.9286e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0320e-10 - val_loss: 4.8863e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9974e-10 - val_loss: 4.9047e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9972e-10 - val_loss: 4.9083e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9894e-10 - val_loss: 4.8311e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8834e-10 - val_loss: 4.7792e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8584e-10 - val_loss: 4.7438e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8099e-10 - val_loss: 4.6648e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7046e-10 - val_loss: 4.5119e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5676e-10 - val_loss: 4.4310e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4653e-10 - val_loss: 4.3636e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4280e-10 - val_loss: 4.3529e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4328e-10 - val_loss: 4.3694e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4604e-10 - val_loss: 4.4500e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5360e-10 - val_loss: 4.4392e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5131e-10 - val_loss: 4.2823e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2969e-10 - val_loss: 4.1175e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1360e-10 - val_loss: 4.0610e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1239e-10 - val_loss: 4.1288e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2268e-10 - val_loss: 4.1366e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2129e-10 - val_loss: 4.1520e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1980e-10 - val_loss: 4.0111e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0574e-10 - val_loss: 3.9664e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0039e-10 - val_loss: 3.7880e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8418e-10 - val_loss: 3.7734e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8105e-10 - val_loss: 3.7755e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8568e-10 - val_loss: 3.8663e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9276e-10 - val_loss: 3.8466e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8936e-10 - val_loss: 3.8011e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8301e-10 - val_loss: 3.7441e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8098e-10 - val_loss: 3.6622e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7049e-10 - val_loss: 3.6226e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6618e-10 - val_loss: 3.6038e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6447e-10 - val_loss: 3.5953e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6146e-10 - val_loss: 3.5339e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5557e-10 - val_loss: 3.4871e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.5161e-10 - val_loss: 3.3995e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.4401e-10 - val_loss: 3.3174e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3793e-10 - val_loss: 3.3462e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3817e-10 - val_loss: 3.3867e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4498e-10 - val_loss: 3.3827e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4367e-10 - val_loss: 3.3505e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.3546e-10 - val_loss: 3.2354e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.2797e-10 - val_loss: 3.1609e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.1822e-10 - val_loss: 3.1379e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.1842e-10 - val_loss: 3.1344e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.1872e-10 - val_loss: 3.1301e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1956e-10 - val_loss: 3.1572e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1646e-10 - val_loss: 3.1313e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.1569e-10 - val_loss: 3.0894e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.0999e-10 - val_loss: 3.0193e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 3.0546e-10 - val_loss: 2.9466e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9957e-10 - val_loss: 2.9481e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.9610e-10 - val_loss: 2.8753e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9084e-10 - val_loss: 2.9304e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9775e-10 - val_loss: 2.9244e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9249e-10 - val_loss: 2.8755e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.9396e-10 - val_loss: 2.8308e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.8537e-10 - val_loss: 2.8186e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8752e-10 - val_loss: 2.9130e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9692e-10 - val_loss: 2.9563e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9872e-10 - val_loss: 2.8553e-10
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.8629e-10 - val_loss: 2.7888e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.7985e-10 - val_loss: 2.7015e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.7256e-10 - val_loss: 2.6856e-10
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7292e-10 - val_loss: 2.6925e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.7225e-10 - val_loss: 2.6578e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6866e-10 - val_loss: 2.6671e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.6886e-10 - val_loss: 2.6533e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6918e-10 - val_loss: 2.6589e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7044e-10 - val_loss: 2.6872e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7137e-10 - val_loss: 2.6887e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.6583e-10 - val_loss: 2.5398e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.5661e-10 - val_loss: 2.4935e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.5059e-10 - val_loss: 2.4423e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5089e-10 - val_loss: 2.5194e-10
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5649e-10 - val_loss: 2.5083e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5250e-10 - val_loss: 2.4887e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.4965e-10 - val_loss: 2.4220e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4651e-10 - val_loss: 2.4925e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5073e-10 - val_loss: 2.4667e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4858e-10 - val_loss: 2.4359e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.4480e-10 - val_loss: 2.4091e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.4246e-10 - val_loss: 2.3843e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.3929e-10 - val_loss: 2.3407e-10
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3744e-10 - val_loss: 2.3590e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4039e-10 - val_loss: 2.3730e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3902e-10 - val_loss: 2.3680e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3773e-10 - val_loss: 2.3454e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.3595e-10 - val_loss: 2.3073e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.3142e-10 - val_loss: 2.2639e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.2678e-10 - val_loss: 2.2544e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.2829e-10 - val_loss: 2.2348e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.2463e-10 - val_loss: 2.2004e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2291e-10 - val_loss: 2.2377e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2669e-10 - val_loss: 2.2677e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3113e-10 - val_loss: 2.2755e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2976e-10 - val_loss: 2.2253e-10
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2497e-10 - val_loss: 2.2015e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.2064e-10 - val_loss: 2.1787e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.2132e-10 - val_loss: 2.1572e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.1566e-10 - val_loss: 2.1126e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.0991e-10 - val_loss: 2.0528e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0880e-10 - val_loss: 2.0592e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0896e-10 - val_loss: 2.0961e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1131e-10 - val_loss: 2.1165e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1424e-10 - val_loss: 2.0787e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.0933e-10 - val_loss: 2.0519e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.0682e-10 - val_loss: 2.0023e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.0167e-10 - val_loss: 1.9792e-10
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0035e-10 - val_loss: 1.9886e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0030e-10 - val_loss: 2.0028e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0177e-10 - val_loss: 2.0059e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 2.0083e-10 - val_loss: 1.9518e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9772e-10 - val_loss: 1.9721e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.9811e-10 - val_loss: 1.9342e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.9513e-10 - val_loss: 1.9312e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9581e-10 - val_loss: 1.9388e-10
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9518e-10 - val_loss: 1.9785e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9787e-10 - val_loss: 1.9363e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.9572e-10 - val_loss: 1.9077e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.9227e-10 - val_loss: 1.8650e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.8784e-10 - val_loss: 1.8199e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.8277e-10 - val_loss: 1.7777e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8075e-10 - val_loss: 1.8345e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8558e-10 - val_loss: 1.8393e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8627e-10 - val_loss: 1.8598e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8685e-10 - val_loss: 1.8437e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8439e-10 - val_loss: 1.8130e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8281e-10 - val_loss: 1.8176e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8254e-10 - val_loss: 1.8009e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8206e-10 - val_loss: 1.7833e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7771e-10 - val_loss: 1.7451e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7696e-10 - val_loss: 1.7537e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7512e-10 - val_loss: 1.7196e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7450e-10 - val_loss: 1.7190e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7516e-10 - val_loss: 1.7403e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7647e-10 - val_loss: 1.7494e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7488e-10 - val_loss: 1.7092e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7208e-10 - val_loss: 1.6998e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7172e-10 - val_loss: 1.7419e-10
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7580e-10 - val_loss: 1.7325e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7238e-10 - val_loss: 1.7101e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.7181e-10 - val_loss: 1.6894e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6949e-10 - val_loss: 1.6808e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6929e-10 - val_loss: 1.6734e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6844e-10 - val_loss: 1.6787e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6928e-10 - val_loss: 1.6754e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6686e-10 - val_loss: 1.6365e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6571e-10 - val_loss: 1.6429e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6432e-10 - val_loss: 1.6221e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6344e-10 - val_loss: 1.6039e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6185e-10 - val_loss: 1.6397e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6664e-10 - val_loss: 1.6220e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6126e-10 - val_loss: 1.5748e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5912e-10 - val_loss: 1.6069e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6095e-10 - val_loss: 1.5871e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6032e-10 - val_loss: 1.6038e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.6177e-10 - val_loss: 1.5727e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.5787e-10 - val_loss: 1.5258e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5324e-10 - val_loss: 1.5406e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5663e-10 - val_loss: 1.5615e-10
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5768e-10 - val_loss: 1.5681e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5718e-10 - val_loss: 1.5545e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5690e-10 - val_loss: 1.5656e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5560e-10 - val_loss: 1.5260e-10
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.5295e-10 - val_loss: 1.5061e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5249e-10 - val_loss: 1.5375e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5480e-10 - val_loss: 1.5267e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5305e-10 - val_loss: 1.5142e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.5164e-10 - val_loss: 1.4862e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4920e-10 - val_loss: 1.4873e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.5020e-10 - val_loss: 1.4769e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4757e-10 - val_loss: 1.4622e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4759e-10 - val_loss: 1.4619e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4691e-10 - val_loss: 1.4585e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4646e-10 - val_loss: 1.4719e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4842e-10 - val_loss: 1.4744e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4763e-10 - val_loss: 1.4601e-10
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4654e-10 - val_loss: 1.4446e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4458e-10 - val_loss: 1.4063e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4049e-10 - val_loss: 1.3873e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3922e-10 - val_loss: 1.3814e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3890e-10 - val_loss: 1.4092e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4276e-10 - val_loss: 1.4393e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4491e-10 - val_loss: 1.4181e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4212e-10 - val_loss: 1.4092e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4186e-10 - val_loss: 1.4066e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.4119e-10 - val_loss: 1.3736e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3691e-10 - val_loss: 1.3512e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3559e-10 - val_loss: 1.3395e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3529e-10 - val_loss: 1.3526e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3709e-10 - val_loss: 1.3629e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3746e-10 - val_loss: 1.3764e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3840e-10 - val_loss: 1.3771e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3701e-10 - val_loss: 1.3438e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3470e-10 - val_loss: 1.3348e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3184e-10 - val_loss: 1.2961e-10
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2989e-10 - val_loss: 1.3046e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3085e-10 - val_loss: 1.3062e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3172e-10 - val_loss: 1.3266e-10
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3336e-10 - val_loss: 1.3449e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3658e-10 - val_loss: 1.3522e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3442e-10 - val_loss: 1.3358e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3550e-10 - val_loss: 1.3329e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3418e-10 - val_loss: 1.3175e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3143e-10 - val_loss: 1.3106e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3171e-10 - val_loss: 1.3057e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.3019e-10 - val_loss: 1.2556e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2621e-10 - val_loss: 1.2641e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2683e-10 - val_loss: 1.2651e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2633e-10 - val_loss: 1.2670e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2735e-10 - val_loss: 1.2621e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.2642e-10 - val_loss: 1.2388e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2471e-10 - val_loss: 1.2473e-10
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2449e-10 - val_loss: 1.2390e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.2411e-10 - val_loss: 1.2187e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2230e-10 - val_loss: 1.2249e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2318e-10 - val_loss: 1.2264e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2340e-10 - val_loss: 1.2398e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2473e-10 - val_loss: 1.2312e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.2362e-10 - val_loss: 1.2039e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1874e-10 - val_loss: 1.1621e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1626e-10 - val_loss: 1.1625e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1652e-10 - val_loss: 1.1708e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1668e-10 - val_loss: 1.1808e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1845e-10 - val_loss: 1.1752e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1815e-10 - val_loss: 1.1909e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2061e-10 - val_loss: 1.2052e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2059e-10 - val_loss: 1.1883e-10
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1954e-10 - val_loss: 1.2101e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2194e-10 - val_loss: 1.2092e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2121e-10 - val_loss: 1.1869e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1902e-10 - val_loss: 1.1538e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1651e-10 - val_loss: 1.1645e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1691e-10 - val_loss: 1.1809e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1699e-10 - val_loss: 1.1429e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1646e-10 - val_loss: 1.1759e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1844e-10 - val_loss: 1.2087e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2120e-10 - val_loss: 1.2105e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1989e-10 - val_loss: 1.1758e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1785e-10 - val_loss: 1.1790e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1753e-10 - val_loss: 1.1740e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1592e-10 - val_loss: 1.1364e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1311e-10 - val_loss: 1.1174e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1198e-10 - val_loss: 1.1262e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1250e-10 - val_loss: 1.1323e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1373e-10 - val_loss: 1.1416e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1462e-10 - val_loss: 1.1559e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1581e-10 - val_loss: 1.1415e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1427e-10 - val_loss: 1.1246e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1282e-10 - val_loss: 1.1115e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.1088e-10 - val_loss: 1.0862e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0945e-10 - val_loss: 1.1070e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1222e-10 - val_loss: 1.1122e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1310e-10 - val_loss: 1.1349e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1481e-10 - val_loss: 1.1536e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1511e-10 - val_loss: 1.1316e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1312e-10 - val_loss: 1.1176e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1184e-10 - val_loss: 1.0891e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0862e-10 - val_loss: 1.0806e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0884e-10 - val_loss: 1.0989e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1004e-10 - val_loss: 1.0922e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1051e-10 - val_loss: 1.1038e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0928e-10 - val_loss: 1.0818e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0883e-10 - val_loss: 1.0928e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1017e-10 - val_loss: 1.0950e-10
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0958e-10 - val_loss: 1.0704e-10
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0682e-10 - val_loss: 1.0599e-10
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0613e-10 - val_loss: 1.0500e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0642e-10 - val_loss: 1.0759e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0707e-10 - val_loss: 1.0567e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0623e-10 - val_loss: 1.0697e-10
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0768e-10 - val_loss: 1.0649e-10
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0774e-10 - val_loss: 1.0772e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0730e-10 - val_loss: 1.0543e-10
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0511e-10 - val_loss: 1.0390e-10
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0352e-10 - val_loss: 1.0238e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0257e-10 - val_loss: 1.0282e-10
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0369e-10 - val_loss: 1.0399e-10
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0416e-10 - val_loss: 1.0349e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0337e-10 - val_loss: 1.0233e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0342e-10 - val_loss: 1.0508e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0588e-10 - val_loss: 1.0552e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0595e-10 - val_loss: 1.0451e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0433e-10 - val_loss: 1.0292e-10
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 1.0159e-10 - val_loss: 9.8741e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.9037e-11 - val_loss: 9.7941e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8799e-11 - val_loss: 9.9331e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.9599e-11 - val_loss: 9.7687e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.8118e-11 - val_loss: 9.7328e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.7228e-11 - val_loss: 9.7046e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7765e-11 - val_loss: 9.8992e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9408e-11 - val_loss: 9.9576e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9980e-11 - val_loss: 1.0083e-10
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0088e-10 - val_loss: 1.0002e-10
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9691e-11 - val_loss: 9.8194e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8547e-11 - val_loss: 9.7704e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8074e-11 - val_loss: 9.7053e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.6428e-11 - val_loss: 9.5338e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.5735e-11 - val_loss: 9.4495e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.4648e-11 - val_loss: 9.4346e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.3866e-11 - val_loss: 9.3794e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5090e-11 - val_loss: 9.6217e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7245e-11 - val_loss: 9.7204e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7248e-11 - val_loss: 9.5664e-11
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.5808e-11 - val_loss: 9.3717e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.3067e-11 - val_loss: 9.2119e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2858e-11 - val_loss: 9.4447e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5491e-11 - val_loss: 9.5148e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4664e-11 - val_loss: 9.3983e-11
Epoch 415/512

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.3163e-11 - val_loss: 9.1686e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.2065e-11 - val_loss: 9.1490e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2814e-11 - val_loss: 9.5699e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5700e-11 - val_loss: 9.6704e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5644e-11 - val_loss: 9.2757e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.2175e-11 - val_loss: 9.0953e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.1423e-11 - val_loss: 9.0398e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.0075e-11 - val_loss: 9.0262e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0568e-11 - val_loss: 9.0482e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0639e-11 - val_loss: 9.1299e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2052e-11 - val_loss: 9.2993e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3211e-11 - val_loss: 9.2338e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1998e-11 - val_loss: 9.0952e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2122e-11 - val_loss: 9.2053e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 9.2093e-11 - val_loss: 8.9542e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.9707e-11 - val_loss: 8.9143e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.9096e-11 - val_loss: 8.8191e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9338e-11 - val_loss: 8.9400e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.9158e-11 - val_loss: 8.7530e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.7749e-11 - val_loss: 8.7327e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8275e-11 - val_loss: 8.9629e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9367e-11 - val_loss: 8.8271e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9079e-11 - val_loss: 8.9750e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0182e-11 - val_loss: 9.0661e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.9674e-11 - val_loss: 8.6712e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.6180e-11 - val_loss: 8.6099e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6241e-11 - val_loss: 8.6251e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6910e-11 - val_loss: 8.7647e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7466e-11 - val_loss: 8.7247e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7639e-11 - val_loss: 8.6838e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7249e-11 - val_loss: 8.6834e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6716e-11 - val_loss: 8.7656e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7967e-11 - val_loss: 8.7889e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8517e-11 - val_loss: 8.8423e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8519e-11 - val_loss: 8.9378e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8407e-11 - val_loss: 8.7522e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.6852e-11 - val_loss: 8.4607e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.4502e-11 - val_loss: 8.3608e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4210e-11 - val_loss: 8.5044e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5415e-11 - val_loss: 8.4857e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5717e-11 - val_loss: 8.5717e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6125e-11 - val_loss: 8.6595e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6605e-11 - val_loss: 8.5841e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5589e-11 - val_loss: 8.4851e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5489e-11 - val_loss: 8.5623e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6276e-11 - val_loss: 8.6398e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6132e-11 - val_loss: 8.5722e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6048e-11 - val_loss: 8.5488e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.4888e-11 - val_loss: 8.2654e-11
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.2256e-11 - val_loss: 8.0896e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1159e-11 - val_loss: 8.1263e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2331e-11 - val_loss: 8.3894e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4542e-11 - val_loss: 8.5295e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5816e-11 - val_loss: 8.4574e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4002e-11 - val_loss: 8.2651e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2461e-11 - val_loss: 8.1885e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2296e-11 - val_loss: 8.2106e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1836e-11 - val_loss: 8.2512e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3407e-11 - val_loss: 8.3279e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3875e-11 - val_loss: 8.5233e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5566e-11 - val_loss: 8.4775e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4014e-11 - val_loss: 8.2400e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1417e-11 - val_loss: 8.1005e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 8.0689e-11 - val_loss: 7.8927e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.8927e-11 - val_loss: 7.8848e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.8139e-11 - val_loss: 7.7910e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8736e-11 - val_loss: 8.0437e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0799e-11 - val_loss: 7.9432e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9629e-11 - val_loss: 7.9748e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9370e-11 - val_loss: 8.0039e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1109e-11 - val_loss: 8.1709e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1351e-11 - val_loss: 8.1204e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1533e-11 - val_loss: 8.1515e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2194e-11 - val_loss: 8.1540e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0986e-11 - val_loss: 8.0078e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9432e-11 - val_loss: 7.9122e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.8725e-11 - val_loss: 7.7726e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8546e-11 - val_loss: 7.9004e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.9035e-11 - val_loss: 7.7572e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.6813e-11 - val_loss: 7.5879e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.5425e-11 - val_loss: 7.4371e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4256e-11 - val_loss: 7.5360e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6098e-11 - val_loss: 7.5940e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6637e-11 - val_loss: 7.6347e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7008e-11 - val_loss: 7.7678e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8557e-11 - val_loss: 7.8804e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8711e-11 - val_loss: 7.9303e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9498e-11 - val_loss: 7.9731e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9834e-11 - val_loss: 7.9537e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9457e-11 - val_loss: 7.8093e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8308e-11 - val_loss: 7.7854e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7501e-11 - val_loss: 7.6395e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6057e-11 - val_loss: 7.4613e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4886e-11 - val_loss: 7.4506e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.4497e-11 - val_loss: 7.3631e-11
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.2934e-11 - val_loss: 7.1651e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.1271e-11 - val_loss: 7.1177e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.5-cuvre-secp256r1-72/multiplication_weights.h5
448/448 - 0s - loss: 7.0869e-11 - val_loss: 7.0593e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.368 | eve: 9.698 | bob: 9.248Epoch   0:   0% | abe: 9.342 | eve: 9.707 | bob: 9.225Epoch   0:   1% | abe: 9.329 | eve: 9.703 | bob: 9.218Epoch   0:   2% | abe: 9.284 | eve: 9.702 | bob: 9.177Epoch   0:   2% | abe: 9.274 | eve: 9.695 | bob: 9.172Epoch   0:   3% | abe: 9.264 | eve: 9.695 | bob: 9.166Epoch   0:   4% | abe: 9.254 | eve: 9.693 | bob: 9.161Epoch   0:   4% | abe: 9.244 | eve: 9.698 | bob: 9.154Epoch   0:   5% | abe: 9.226 | eve: 9.705 | bob: 9.139Epoch   0:   6% | abe: 9.202 | eve: 9.705 | bob: 9.118Epoch   0:   6% | abe: 9.189 | eve: 9.705 | bob: 9.108Epoch   0:   7% | abe: 9.182 | eve: 9.710 | bob: 9.104Epoch   0:   8% | abe: 9.176 | eve: 9.710 | bob: 9.100Epoch   0:   8% | abe: 9.168 | eve: 9.713 | bob: 9.094Epoch   0:   9% | abe: 9.163 | eve: 9.720 | bob: 9.090Epoch   0:  10% | abe: 9.153 | eve: 9.720 | bob: 9.082Epoch   0:  10% | abe: 9.150 | eve: 9.721 | bob: 9.079Epoch   0:  11% | abe: 9.145 | eve: 9.724 | bob: 9.075Epoch   0:  12% | abe: 9.136 | eve: 9.726 | bob: 9.068Epoch   0:  13% | abe: 9.132 | eve: 9.731 | bob: 9.064Epoch   0:  13% | abe: 9.131 | eve: 9.734 | bob: 9.063Epoch   0:  14% | abe: 9.128 | eve: 9.741 | bob: 9.061Epoch   0:  15% | abe: 9.124 | eve: 9.742 | bob: 9.057Epoch   0:  15% | abe: 9.120 | eve: 9.749 | bob: 9.053Epoch   0:  16% | abe: 9.117 | eve: 9.752 | bob: 9.051Epoch   0:  17% | abe: 9.112 | eve: 9.753 | bob: 9.046Epoch   0:  17% | abe: 9.110 | eve: 9.756 | bob: 9.044Epoch   0:  18% | abe: 9.110 | eve: 9.755 | bob: 9.045Epoch   0:  19% | abe: 9.110 | eve: 9.757 | bob: 9.045Epoch   0:  19% | abe: 9.111 | eve: 9.760 | bob: 9.046Epoch   0:  20% | abe: 9.108 | eve: 9.764 | bob: 9.042Epoch   0:  21% | abe: 9.106 | eve: 9.766 | bob: 9.040Epoch   0:  21% | abe: 9.105 | eve: 9.770 | bob: 9.040Epoch   0:  22% | abe: 9.102 | eve: 9.778 | bob: 9.036Epoch   0:  23% | abe: 9.100 | eve: 9.781 | bob: 9.034Epoch   0:  23% | abe: 9.098 | eve: 9.785 | bob: 9.033Epoch   0:  24% | abe: 9.097 | eve: 9.788 | bob: 9.031Epoch   0:  25% | abe: 9.096 | eve: 9.793 | bob: 9.030Epoch   0:  26% | abe: 9.094 | eve: 9.798 | bob: 9.028Epoch   0:  26% | abe: 9.095 | eve: 9.800 | bob: 9.029Epoch   0:  27% | abe: 9.093 | eve: 9.802 | bob: 9.027Epoch   0:  28% | abe: 9.092 | eve: 9.806 | bob: 9.026Epoch   0:  28% | abe: 9.091 | eve: 9.810 | bob: 9.025Epoch   0:  29% | abe: 9.091 | eve: 9.812 | bob: 9.024Epoch   0:  30% | abe: 9.089 | eve: 9.817 | bob: 9.022Epoch   0:  30% | abe: 9.087 | eve: 9.819 | bob: 9.020Epoch   0:  31% | abe: 9.086 | eve: 9.823 | bob: 9.020Epoch   0:  32% | abe: 9.086 | eve: 9.826 | bob: 9.019Epoch   0:  32% | abe: 9.084 | eve: 9.829 | bob: 9.017Epoch   0:  33% | abe: 9.083 | eve: 9.833 | bob: 9.015Epoch   0:  34% | abe: 9.080 | eve: 9.836 | bob: 9.013Epoch   0:  34% | abe: 9.081 | eve: 9.839 | bob: 9.013Epoch   0:  35% | abe: 9.079 | eve: 9.842 | bob: 9.011Epoch   0:  36% | abe: 9.078 | eve: 9.845 | bob: 9.010Epoch   0:  36% | abe: 9.077 | eve: 9.847 | bob: 9.008Epoch   0:  37% | abe: 9.075 | eve: 9.851 | bob: 9.007Epoch   0:  38% | abe: 9.075 | eve: 9.853 | bob: 9.006Epoch   0:  39% | abe: 9.074 | eve: 9.854 | bob: 9.005Epoch   0:  39% | abe: 9.074 | eve: 9.857 | bob: 9.005Epoch   0:  40% | abe: 9.073 | eve: 9.860 | bob: 9.004Epoch   0:  41% | abe: 9.072 | eve: 9.863 | bob: 9.003Epoch   0:  41% | abe: 9.071 | eve: 9.865 | bob: 9.002Epoch   0:  42% | abe: 9.070 | eve: 9.866 | bob: 9.001Epoch   0:  43% | abe: 9.070 | eve: 9.869 | bob: 9.000Epoch   0:  43% | abe: 9.069 | eve: 9.872 | bob: 8.999Epoch   0:  44% | abe: 9.068 | eve: 9.875 | bob: 8.998Epoch   0:  45% | abe: 9.068 | eve: 9.878 | bob: 8.997Epoch   0:  45% | abe: 9.066 | eve: 9.879 | bob: 8.996Epoch   0:  46% | abe: 9.065 | eve: 9.882 | bob: 8.995Epoch   0:  47% | abe: 9.064 | eve: 9.884 | bob: 8.993Epoch   0:  47% | abe: 9.063 | eve: 9.886 | bob: 8.992Epoch   0:  48% | abe: 9.063 | eve: 9.890 | bob: 8.991Epoch   0:  49% | abe: 9.061 | eve: 9.892 | bob: 8.990Epoch   0:  50% | abe: 9.061 | eve: 9.894 | bob: 8.989Epoch   0:  50% | abe: 9.060 | eve: 9.897 | bob: 8.988Epoch   0:  51% | abe: 9.060 | eve: 9.898 | bob: 8.988Epoch   0:  52% | abe: 9.059 | eve: 9.900 | bob: 8.987Epoch   0:  52% | abe: 9.058 | eve: 9.902 | bob: 8.986Epoch   0:  53% | abe: 9.058 | eve: 9.904 | bob: 8.985Epoch   0:  54% | abe: 9.057 | eve: 9.904 | bob: 8.984Epoch   0:  54% | abe: 9.057 | eve: 9.907 | bob: 8.984Epoch   0:  55% | abe: 9.056 | eve: 9.909 | bob: 8.983Epoch   0:  56% | abe: 9.056 | eve: 9.911 | bob: 8.984Epoch   0:  56% | abe: 9.056 | eve: 9.913 | bob: 8.983Epoch   0:  57% | abe: 9.055 | eve: 9.914 | bob: 8.982Epoch   0:  58% | abe: 9.055 | eve: 9.915 | bob: 8.982Epoch   0:  58% | abe: 9.056 | eve: 9.917 | bob: 8.982Epoch   0:  59% | abe: 9.054 | eve: 9.918 | bob: 8.981Epoch   0:  60% | abe: 9.054 | eve: 9.920 | bob: 8.980Epoch   0:  60% | abe: 9.054 | eve: 9.920 | bob: 8.981Epoch   0:  61% | abe: 9.052 | eve: 9.921 | bob: 8.979Epoch   0:  62% | abe: 9.052 | eve: 9.923 | bob: 8.978Epoch   0:  63% | abe: 9.051 | eve: 9.924 | bob: 8.977Epoch   0:  63% | abe: 9.051 | eve: 9.925 | bob: 8.977Epoch   0:  64% | abe: 9.049 | eve: 9.927 | bob: 8.975Epoch   0:  65% | abe: 9.049 | eve: 9.929 | bob: 8.975Epoch   0:  65% | abe: 9.048 | eve: 9.930 | bob: 8.974Epoch   0:  66% | abe: 9.048 | eve: 9.930 | bob: 8.974Epoch   0:  67% | abe: 9.048 | eve: 9.933 | bob: 8.974Epoch   0:  67% | abe: 9.048 | eve: 9.933 | bob: 8.973Epoch   0:  68% | abe: 9.047 | eve: 9.934 | bob: 8.973Epoch   0:  69% | abe: 9.046 | eve: 9.935 | bob: 8.972Epoch   0:  69% | abe: 9.045 | eve: 9.936 | bob: 8.970Epoch   0:  70% | abe: 9.045 | eve: 9.938 | bob: 8.970Epoch   0:  71% | abe: 9.045 | eve: 9.940 | bob: 8.970Epoch   0:  71% | abe: 9.044 | eve: 9.940 | bob: 8.969Epoch   0:  72% | abe: 9.043 | eve: 9.941 | bob: 8.968Epoch   0:  73% | abe: 9.042 | eve: 9.943 | bob: 8.967Epoch   0:  73% | abe: 9.041 | eve: 9.945 | bob: 8.966Epoch   0:  74% | abe: 9.041 | eve: 9.945 | bob: 8.966Epoch   0:  75% | abe: 9.041 | eve: 9.946 | bob: 8.966Epoch   0:  76% | abe: 9.041 | eve: 9.947 | bob: 8.965Epoch   0:  76% | abe: 9.041 | eve: 9.948 | bob: 8.965Epoch   0:  77% | abe: 9.040 | eve: 9.948 | bob: 8.964Epoch   0:  78% | abe: 9.039 | eve: 9.950 | bob: 8.963Epoch   0:  78% | abe: 9.039 | eve: 9.951 | bob: 8.963Epoch   0:  79% | abe: 9.038 | eve: 9.952 | bob: 8.962Epoch   0:  80% | abe: 9.038 | eve: 9.954 | bob: 8.962Epoch   0:  80% | abe: 9.037 | eve: 9.955 | bob: 8.961Epoch   0:  81% | abe: 9.037 | eve: 9.955 | bob: 8.961Epoch   0:  82% | abe: 9.037 | eve: 9.957 | bob: 8.961Epoch   0:  82% | abe: 9.036 | eve: 9.957 | bob: 8.960Epoch   0:  83% | abe: 9.035 | eve: 9.958 | bob: 8.959Epoch   0:  84% | abe: 9.034 | eve: 9.959 | bob: 8.958Epoch   0:  84% | abe: 9.034 | eve: 9.960 | bob: 8.958Epoch   0:  85% | abe: 9.034 | eve: 9.961 | bob: 8.958Epoch   0:  86% | abe: 9.034 | eve: 9.962 | bob: 8.957Epoch   0:  86% | abe: 9.034 | eve: 9.963 | bob: 8.957Epoch   0:  87% | abe: 9.034 | eve: 9.963 | bob: 8.957Epoch   0:  88% | abe: 9.034 | eve: 9.964 | bob: 8.957Epoch   0:  89% | abe: 9.033 | eve: 9.965 | bob: 8.957Epoch   0:  89% | abe: 9.032 | eve: 9.966 | bob: 8.956Epoch   0:  90% | abe: 9.032 | eve: 9.966 | bob: 8.956Epoch   0:  91% | abe: 9.032 | eve: 9.967 | bob: 8.955Epoch   0:  91% | abe: 9.031 | eve: 9.968 | bob: 8.954Epoch   0:  92% | abe: 9.031 | eve: 9.970 | bob: 8.954Epoch   0:  93% | abe: 9.030 | eve: 9.970 | bob: 8.953Epoch   0:  93% | abe: 9.030 | eve: 9.971 | bob: 8.953Epoch   0:  94% | abe: 9.030 | eve: 9.972 | bob: 8.953Epoch   0:  95% | abe: 9.029 | eve: 9.974 | bob: 8.952Epoch   0:  95% | abe: 9.029 | eve: 9.974 | bob: 8.952Epoch   0:  96% | abe: 9.029 | eve: 9.975 | bob: 8.952Epoch   0:  97% | abe: 9.028 | eve: 9.976 | bob: 8.951Epoch   0:  97% | abe: 9.028 | eve: 9.976 | bob: 8.951Epoch   0:  98% | abe: 9.028 | eve: 9.976 | bob: 8.951Epoch   0:  99% | abe: 9.028 | eve: 9.977 | bob: 8.950
New best Bob loss 8.95047011555568 at epoch 0
Epoch   1:   0% | abe: 8.993 | eve: 9.998 | bob: 8.909Epoch   1:   0% | abe: 8.968 | eve: 10.036 | bob: 8.886Epoch   1:   1% | abe: 8.997 | eve: 10.029 | bob: 8.915Epoch   1:   2% | abe: 8.991 | eve: 10.016 | bob: 8.909Epoch   1:   2% | abe: 8.985 | eve: 10.017 | bob: 8.904Epoch   1:   3% | abe: 8.980 | eve: 10.018 | bob: 8.899Epoch   1:   4% | abe: 8.975 | eve: 10.013 | bob: 8.894Epoch   1:   4% | abe: 8.963 | eve: 10.028 | bob: 8.880Epoch   1:   5% | abe: 8.962 | eve: 10.038 | bob: 8.880Epoch   1:   6% | abe: 8.966 | eve: 10.032 | bob: 8.883Epoch   1:   6% | abe: 8.965 | eve: 10.030 | bob: 8.883Epoch   1:   7% | abe: 8.962 | eve: 10.035 | bob: 8.880Epoch   1:   8% | abe: 8.959 | eve: 10.035 | bob: 8.877Epoch   1:   8% | abe: 8.965 | eve: 10.033 | bob: 8.882Epoch   1:   9% | abe: 8.967 | eve: 10.036 | bob: 8.885Epoch   1:  10% | abe: 8.970 | eve: 10.034 | bob: 8.888Epoch   1:  10% | abe: 8.967 | eve: 10.037 | bob: 8.885Epoch   1:  11% | abe: 8.968 | eve: 10.038 | bob: 8.886Epoch   1:  12% | abe: 8.968 | eve: 10.040 | bob: 8.886Epoch   1:  13% | abe: 8.966 | eve: 10.039 | bob: 8.884Epoch   1:  13% | abe: 8.964 | eve: 10.038 | bob: 8.882Epoch   1:  14% | abe: 8.961 | eve: 10.035 | bob: 8.879Epoch   1:  15% | abe: 8.959 | eve: 10.036 | bob: 8.878Epoch   1:  15% | abe: 8.958 | eve: 10.040 | bob: 8.876Epoch   1:  16% | abe: 8.959 | eve: 10.040 | bob: 8.877Epoch   1:  17% | abe: 8.957 | eve: 10.037 | bob: 8.875Epoch   1:  17% | abe: 8.957 | eve: 10.037 | bob: 8.875Epoch   1:  18% | abe: 8.957 | eve: 10.038 | bob: 8.875Epoch   1:  19% | abe: 8.956 | eve: 10.042 | bob: 8.874Epoch   1:  19% | abe: 8.956 | eve: 10.039 | bob: 8.874Epoch   1:  20% | abe: 8.955 | eve: 10.036 | bob: 8.873Epoch   1:  21% | abe: 8.957 | eve: 10.036 | bob: 8.875Epoch   1:  21% | abe: 8.957 | eve: 10.035 | bob: 8.876Epoch   1:  22% | abe: 8.957 | eve: 10.036 | bob: 8.876Epoch   1:  23% | abe: 8.956 | eve: 10.036 | bob: 8.875Epoch   1:  23% | abe: 8.956 | eve: 10.036 | bob: 8.875Epoch   1:  24% | abe: 8.957 | eve: 10.036 | bob: 8.875Epoch   1:  25% | abe: 8.955 | eve: 10.037 | bob: 8.873Epoch   1:  26% | abe: 8.954 | eve: 10.037 | bob: 8.873Epoch   1:  26% | abe: 8.954 | eve: 10.038 | bob: 8.872Epoch   1:  27% | abe: 8.954 | eve: 10.039 | bob: 8.872Epoch   1:  28% | abe: 8.953 | eve: 10.039 | bob: 8.872Epoch   1:  28% | abe: 8.953 | eve: 10.040 | bob: 8.872Epoch   1:  29% | abe: 8.954 | eve: 10.039 | bob: 8.873Epoch   1:  30% | abe: 8.954 | eve: 10.041 | bob: 8.873Epoch   1:  30% | abe: 8.954 | eve: 10.041 | bob: 8.873Epoch   1:  31% | abe: 8.954 | eve: 10.040 | bob: 8.872Epoch   1:  32% | abe: 8.955 | eve: 10.041 | bob: 8.874Epoch   1:  32% | abe: 8.956 | eve: 10.041 | bob: 8.875Epoch   1:  33% | abe: 8.957 | eve: 10.041 | bob: 8.876Epoch   1:  34% | abe: 8.958 | eve: 10.040 | bob: 8.876Epoch   1:  34% | abe: 8.958 | eve: 10.040 | bob: 8.877Epoch   1:  35% | abe: 8.958 | eve: 10.039 | bob: 8.877Epoch   1:  36% | abe: 8.958 | eve: 10.039 | bob: 8.877Epoch   1:  36% | abe: 8.957 | eve: 10.038 | bob: 8.877Epoch   1:  37% | abe: 8.958 | eve: 10.040 | bob: 8.877Epoch   1:  38% | abe: 8.957 | eve: 10.040 | bob: 8.876Epoch   1:  39% | abe: 8.957 | eve: 10.042 | bob: 8.876Epoch   1:  39% | abe: 8.957 | eve: 10.043 | bob: 8.876Epoch   1:  40% | abe: 8.957 | eve: 10.041 | bob: 8.877Epoch   1:  41% | abe: 8.957 | eve: 10.042 | bob: 8.876Epoch   1:  41% | abe: 8.957 | eve: 10.042 | bob: 8.876Epoch   1:  42% | abe: 8.956 | eve: 10.042 | bob: 8.875Epoch   1:  43% | abe: 8.955 | eve: 10.042 | bob: 8.875Epoch   1:  43% | abe: 8.955 | eve: 10.042 | bob: 8.874Epoch   1:  44% | abe: 8.954 | eve: 10.042 | bob: 8.874Epoch   1:  45% | abe: 8.954 | eve: 10.042 | bob: 8.873Epoch   1:  45% | abe: 8.954 | eve: 10.042 | bob: 8.873Epoch   1:  46% | abe: 8.952 | eve: 10.042 | bob: 8.872Epoch   1:  47% | abe: 8.952 | eve: 10.041 | bob: 8.871Epoch   1:  47% | abe: 8.951 | eve: 10.040 | bob: 8.871Epoch   1:  48% | abe: 8.950 | eve: 10.040 | bob: 8.870Epoch   1:  49% | abe: 8.950 | eve: 10.039 | bob: 8.870Epoch   1:  50% | abe: 8.950 | eve: 10.038 | bob: 8.869Epoch   1:  50% | abe: 8.949 | eve: 10.039 | bob: 8.869Epoch   1:  51% | abe: 8.949 | eve: 10.040 | bob: 8.869Epoch   1:  52% | abe: 8.949 | eve: 10.040 | bob: 8.868Epoch   1:  52% | abe: 8.949 | eve: 10.040 | bob: 8.869Epoch   1:  53% | abe: 8.949 | eve: 10.040 | bob: 8.868Epoch   1:  54% | abe: 8.948 | eve: 10.039 | bob: 8.868Epoch   1:  54% | abe: 8.948 | eve: 10.040 | bob: 8.868Epoch   1:  55% | abe: 8.948 | eve: 10.040 | bob: 8.867Epoch   1:  56% | abe: 8.947 | eve: 10.042 | bob: 8.867Epoch   1:  56% | abe: 8.947 | eve: 10.042 | bob: 8.867Epoch   1:  57% | abe: 8.947 | eve: 10.043 | bob: 8.867Epoch   1:  58% | abe: 8.947 | eve: 10.043 | bob: 8.867Epoch   1:  58% | abe: 8.947 | eve: 10.043 | bob: 8.867Epoch   1:  59% | abe: 8.946 | eve: 10.043 | bob: 8.866Epoch   1:  60% | abe: 8.946 | eve: 10.042 | bob: 8.866Epoch   1:  60% | abe: 8.945 | eve: 10.042 | bob: 8.865Epoch   1:  61% | abe: 8.945 | eve: 10.041 | bob: 8.865Epoch   1:  62% | abe: 8.945 | eve: 10.041 | bob: 8.865Epoch   1:  63% | abe: 8.944 | eve: 10.041 | bob: 8.864Epoch   1:  63% | abe: 8.944 | eve: 10.040 | bob: 8.864Epoch   1:  64% | abe: 8.944 | eve: 10.040 | bob: 8.864Epoch   1:  65% | abe: 8.944 | eve: 10.040 | bob: 8.864Epoch   1:  65% | abe: 8.943 | eve: 10.040 | bob: 8.863Epoch   1:  66% | abe: 8.942 | eve: 10.040 | bob: 8.863Epoch   1:  67% | abe: 8.943 | eve: 10.040 | bob: 8.863Epoch   1:  67% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  68% | abe: 8.942 | eve: 10.041 | bob: 8.862Epoch   1:  69% | abe: 8.942 | eve: 10.041 | bob: 8.863Epoch   1:  69% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  70% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  71% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  71% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  72% | abe: 8.942 | eve: 10.041 | bob: 8.862Epoch   1:  73% | abe: 8.942 | eve: 10.040 | bob: 8.862Epoch   1:  73% | abe: 8.941 | eve: 10.041 | bob: 8.862Epoch   1:  74% | abe: 8.941 | eve: 10.041 | bob: 8.862Epoch   1:  75% | abe: 8.941 | eve: 10.041 | bob: 8.862Epoch   1:  76% | abe: 8.942 | eve: 10.042 | bob: 8.862Epoch   1:  76% | abe: 8.941 | eve: 10.041 | bob: 8.862Epoch   1:  77% | abe: 8.941 | eve: 10.041 | bob: 8.862Epoch   1:  78% | abe: 8.941 | eve: 10.041 | bob: 8.861Epoch   1:  78% | abe: 8.940 | eve: 10.041 | bob: 8.861Epoch   1:  79% | abe: 8.940 | eve: 10.041 | bob: 8.861Epoch   1:  80% | abe: 8.939 | eve: 10.041 | bob: 8.860Epoch   1:  80% | abe: 8.939 | eve: 10.042 | bob: 8.860Epoch   1:  81% | abe: 8.940 | eve: 10.042 | bob: 8.860Epoch   1:  82% | abe: 8.940 | eve: 10.042 | bob: 8.860Epoch   1:  82% | abe: 8.939 | eve: 10.042 | bob: 8.860Epoch   1:  83% | abe: 8.939 | eve: 10.042 | bob: 8.859Epoch   1:  84% | abe: 8.938 | eve: 10.042 | bob: 8.859Epoch   1:  84% | abe: 8.938 | eve: 10.042 | bob: 8.859Epoch   1:  85% | abe: 8.938 | eve: 10.041 | bob: 8.859Epoch   1:  86% | abe: 8.938 | eve: 10.041 | bob: 8.859Epoch   1:  86% | abe: 8.937 | eve: 10.041 | bob: 8.858Epoch   1:  87% | abe: 8.937 | eve: 10.041 | bob: 8.858Epoch   1:  88% | abe: 8.937 | eve: 10.041 | bob: 8.857Epoch   1:  89% | abe: 8.936 | eve: 10.040 | bob: 8.857Epoch   1:  89% | abe: 8.936 | eve: 10.040 | bob: 8.857Epoch   1:  90% | abe: 8.936 | eve: 10.040 | bob: 8.857Epoch   1:  91% | abe: 8.936 | eve: 10.040 | bob: 8.857Epoch   1:  91% | abe: 8.935 | eve: 10.040 | bob: 8.856Epoch   1:  92% | abe: 8.935 | eve: 10.040 | bob: 8.856Epoch   1:  93% | abe: 8.935 | eve: 10.039 | bob: 8.856Epoch   1:  93% | abe: 8.935 | eve: 10.039 | bob: 8.856Epoch   1:  94% | abe: 8.934 | eve: 10.040 | bob: 8.855Epoch   1:  95% | abe: 8.934 | eve: 10.040 | bob: 8.855Epoch   1:  95% | abe: 8.934 | eve: 10.040 | bob: 8.855Epoch   1:  96% | abe: 8.934 | eve: 10.039 | bob: 8.855Epoch   1:  97% | abe: 8.934 | eve: 10.039 | bob: 8.855Epoch   1:  97% | abe: 8.934 | eve: 10.040 | bob: 8.855Epoch   1:  98% | abe: 8.934 | eve: 10.040 | bob: 8.855Epoch   1:  99% | abe: 8.934 | eve: 10.040 | bob: 8.855
New best Bob loss 8.854817332285492 at epoch 1
Epoch   2:   0% | abe: 8.888 | eve: 9.978 | bob: 8.812Epoch   2:   0% | abe: 8.879 | eve: 10.014 | bob: 8.802Epoch   2:   1% | abe: 8.878 | eve: 10.031 | bob: 8.801Epoch   2:   2% | abe: 8.893 | eve: 10.029 | bob: 8.818Epoch   2:   2% | abe: 8.881 | eve: 10.025 | bob: 8.805Epoch   2:   3% | abe: 8.878 | eve: 10.019 | bob: 8.801Epoch   2:   4% | abe: 8.884 | eve: 10.034 | bob: 8.808Epoch   2:   4% | abe: 8.888 | eve: 10.031 | bob: 8.811Epoch   2:   5% | abe: 8.885 | eve: 10.025 | bob: 8.808Epoch   2:   6% | abe: 8.887 | eve: 10.023 | bob: 8.810Epoch   2:   6% | abe: 8.886 | eve: 10.018 | bob: 8.810Epoch   2:   7% | abe: 8.883 | eve: 10.020 | bob: 8.807Epoch   2:   8% | abe: 8.882 | eve: 10.020 | bob: 8.806Epoch   2:   8% | abe: 8.886 | eve: 10.016 | bob: 8.809Epoch   2:   9% | abe: 8.887 | eve: 10.013 | bob: 8.810Epoch   2:  10% | abe: 8.886 | eve: 10.005 | bob: 8.810Epoch   2:  10% | abe: 8.885 | eve: 10.002 | bob: 8.808Epoch   2:  11% | abe: 8.888 | eve: 9.997 | bob: 8.811Epoch   2:  12% | abe: 8.888 | eve: 9.998 | bob: 8.812Epoch   2:  13% | abe: 8.892 | eve: 9.999 | bob: 8.815Epoch   2:  13% | abe: 8.893 | eve: 9.997 | bob: 8.817Epoch   2:  14% | abe: 8.892 | eve: 9.999 | bob: 8.816Epoch   2:  15% | abe: 8.892 | eve: 10.003 | bob: 8.816Epoch   2:  15% | abe: 8.892 | eve: 10.002 | bob: 8.816Epoch   2:  16% | abe: 8.890 | eve: 10.000 | bob: 8.814Epoch   2:  17% | abe: 8.889 | eve: 9.999 | bob: 8.813Epoch   2:  17% | abe: 8.888 | eve: 9.996 | bob: 8.812Epoch   2:  18% | abe: 8.887 | eve: 9.999 | bob: 8.811Epoch   2:  19% | abe: 8.885 | eve: 10.002 | bob: 8.809Epoch   2:  19% | abe: 8.887 | eve: 10.005 | bob: 8.811Epoch   2:  20% | abe: 8.888 | eve: 10.003 | bob: 8.812Epoch   2:  21% | abe: 8.888 | eve: 10.006 | bob: 8.812Epoch   2:  21% | abe: 8.888 | eve: 10.008 | bob: 8.812Epoch   2:  22% | abe: 8.886 | eve: 10.009 | bob: 8.810Epoch   2:  23% | abe: 8.885 | eve: 10.010 | bob: 8.809Epoch   2:  23% | abe: 8.882 | eve: 10.011 | bob: 8.806Epoch   2:  24% | abe: 8.882 | eve: 10.011 | bob: 8.806Epoch   2:  25% | abe: 8.882 | eve: 10.015 | bob: 8.806Epoch   2:  26% | abe: 8.881 | eve: 10.013 | bob: 8.805Epoch   2:  26% | abe: 8.881 | eve: 10.013 | bob: 8.805Epoch   2:  27% | abe: 8.880 | eve: 10.011 | bob: 8.804Epoch   2:  28% | abe: 8.879 | eve: 10.012 | bob: 8.803Epoch   2:  28% | abe: 8.880 | eve: 10.011 | bob: 8.804Epoch   2:  29% | abe: 8.879 | eve: 10.014 | bob: 8.803Epoch   2:  30% | abe: 8.881 | eve: 10.014 | bob: 8.805Epoch   2:  30% | abe: 8.880 | eve: 10.015 | bob: 8.804Epoch   2:  31% | abe: 8.880 | eve: 10.017 | bob: 8.804Epoch   2:  32% | abe: 8.880 | eve: 10.018 | bob: 8.804Epoch   2:  32% | abe: 8.879 | eve: 10.017 | bob: 8.803Epoch   2:  33% | abe: 8.879 | eve: 10.018 | bob: 8.803Epoch   2:  34% | abe: 8.878 | eve: 10.018 | bob: 8.802Epoch   2:  34% | abe: 8.879 | eve: 10.017 | bob: 8.804Epoch   2:  35% | abe: 8.879 | eve: 10.017 | bob: 8.803Epoch   2:  36% | abe: 8.878 | eve: 10.018 | bob: 8.802Epoch   2:  36% | abe: 8.878 | eve: 10.017 | bob: 8.803Epoch   2:  37% | abe: 8.878 | eve: 10.015 | bob: 8.802Epoch   2:  38% | abe: 8.878 | eve: 10.016 | bob: 8.802Epoch   2:  39% | abe: 8.878 | eve: 10.015 | bob: 8.802Epoch   2:  39% | abe: 8.878 | eve: 10.017 | bob: 8.802Epoch   2:  40% | abe: 8.877 | eve: 10.016 | bob: 8.801Epoch   2:  41% | abe: 8.877 | eve: 10.017 | bob: 8.802Epoch   2:  41% | abe: 8.876 | eve: 10.018 | bob: 8.801Epoch   2:  42% | abe: 8.876 | eve: 10.019 | bob: 8.801Epoch   2:  43% | abe: 8.876 | eve: 10.019 | bob: 8.800Epoch   2:  43% | abe: 8.876 | eve: 10.020 | bob: 8.800Epoch   2:  44% | abe: 8.876 | eve: 10.021 | bob: 8.800Epoch   2:  45% | abe: 8.875 | eve: 10.020 | bob: 8.800Epoch   2:  45% | abe: 8.875 | eve: 10.021 | bob: 8.799Epoch   2:  46% | abe: 8.874 | eve: 10.020 | bob: 8.798Epoch   2:  47% | abe: 8.874 | eve: 10.021 | bob: 8.798Epoch   2:  47% | abe: 8.875 | eve: 10.021 | bob: 8.799Epoch   2:  48% | abe: 8.874 | eve: 10.022 | bob: 8.798Epoch   2:  49% | abe: 8.874 | eve: 10.022 | bob: 8.798Epoch   2:  50% | abe: 8.873 | eve: 10.022 | bob: 8.797Epoch   2:  50% | abe: 8.872 | eve: 10.021 | bob: 8.797Epoch   2:  51% | abe: 8.872 | eve: 10.021 | bob: 8.797Epoch   2:  52% | abe: 8.872 | eve: 10.020 | bob: 8.796Epoch   2:  52% | abe: 8.872 | eve: 10.021 | bob: 8.796Epoch   2:  53% | abe: 8.871 | eve: 10.022 | bob: 8.796Epoch   2:  54% | abe: 8.871 | eve: 10.023 | bob: 8.796Epoch   2:  54% | abe: 8.872 | eve: 10.023 | bob: 8.797Epoch   2:  55% | abe: 8.871 | eve: 10.023 | bob: 8.796Epoch   2:  56% | abe: 8.870 | eve: 10.023 | bob: 8.795Epoch   2:  56% | abe: 8.870 | eve: 10.022 | bob: 8.795Epoch   2:  57% | abe: 8.870 | eve: 10.021 | bob: 8.795Epoch   2:  58% | abe: 8.870 | eve: 10.021 | bob: 8.795Epoch   2:  58% | abe: 8.870 | eve: 10.020 | bob: 8.795Epoch   2:  59% | abe: 8.869 | eve: 10.021 | bob: 8.794Epoch   2:  60% | abe: 8.869 | eve: 10.022 | bob: 8.794Epoch   2:  60% | abe: 8.869 | eve: 10.023 | bob: 8.794Epoch   2:  61% | abe: 8.869 | eve: 10.022 | bob: 8.794Epoch   2:  62% | abe: 8.868 | eve: 10.022 | bob: 8.793Epoch   2:  63% | abe: 8.868 | eve: 10.023 | bob: 8.793Epoch   2:  63% | abe: 8.867 | eve: 10.022 | bob: 8.792Epoch   2:  64% | abe: 8.867 | eve: 10.023 | bob: 8.792Epoch   2:  65% | abe: 8.867 | eve: 10.022 | bob: 8.792Epoch   2:  65% | abe: 8.866 | eve: 10.023 | bob: 8.791Epoch   2:  66% | abe: 8.866 | eve: 10.022 | bob: 8.791Epoch   2:  67% | abe: 8.866 | eve: 10.023 | bob: 8.791Epoch   2:  67% | abe: 8.866 | eve: 10.023 | bob: 8.791Epoch   2:  68% | abe: 8.866 | eve: 10.023 | bob: 8.791Epoch   2:  69% | abe: 8.866 | eve: 10.024 | bob: 8.791Epoch   2:  69% | abe: 8.865 | eve: 10.023 | bob: 8.790Epoch   2:  70% | abe: 8.864 | eve: 10.023 | bob: 8.789Epoch   2:  71% | abe: 8.864 | eve: 10.022 | bob: 8.789Epoch   2:  71% | abe: 8.864 | eve: 10.022 | bob: 8.789Epoch   2:  72% | abe: 8.864 | eve: 10.022 | bob: 8.789Epoch   2:  73% | abe: 8.864 | eve: 10.022 | bob: 8.789Epoch   2:  73% | abe: 8.863 | eve: 10.023 | bob: 8.789Epoch   2:  74% | abe: 8.863 | eve: 10.024 | bob: 8.788Epoch   2:  75% | abe: 8.863 | eve: 10.025 | bob: 8.789Epoch   2:  76% | abe: 8.863 | eve: 10.024 | bob: 8.788Epoch   2:  76% | abe: 8.863 | eve: 10.024 | bob: 8.788Epoch   2:  77% | abe: 8.863 | eve: 10.023 | bob: 8.788Epoch   2:  78% | abe: 8.863 | eve: 10.023 | bob: 8.788Epoch   2:  78% | abe: 8.862 | eve: 10.023 | bob: 8.788Epoch   2:  79% | abe: 8.862 | eve: 10.024 | bob: 8.787Epoch   2:  80% | abe: 8.862 | eve: 10.025 | bob: 8.788Epoch   2:  80% | abe: 8.861 | eve: 10.024 | bob: 8.787Epoch   2:  81% | abe: 8.861 | eve: 10.024 | bob: 8.787Epoch   2:  82% | abe: 8.861 | eve: 10.024 | bob: 8.787Epoch   2:  82% | abe: 8.860 | eve: 10.025 | bob: 8.786Epoch   2:  83% | abe: 8.860 | eve: 10.025 | bob: 8.786Epoch   2:  84% | abe: 8.860 | eve: 10.024 | bob: 8.786Epoch   2:  84% | abe: 8.860 | eve: 10.024 | bob: 8.786Epoch   2:  85% | abe: 8.860 | eve: 10.024 | bob: 8.786Epoch   2:  86% | abe: 8.860 | eve: 10.024 | bob: 8.786Epoch   2:  86% | abe: 8.859 | eve: 10.024 | bob: 8.785Epoch   2:  87% | abe: 8.859 | eve: 10.023 | bob: 8.785Epoch   2:  88% | abe: 8.859 | eve: 10.023 | bob: 8.785Epoch   2:  89% | abe: 8.859 | eve: 10.023 | bob: 8.785Epoch   2:  89% | abe: 8.859 | eve: 10.023 | bob: 8.785Epoch   2:  90% | abe: 8.858 | eve: 10.023 | bob: 8.784Epoch   2:  91% | abe: 8.858 | eve: 10.023 | bob: 8.784Epoch   2:  91% | abe: 8.857 | eve: 10.022 | bob: 8.783Epoch   2:  92% | abe: 8.856 | eve: 10.023 | bob: 8.783Epoch   2:  93% | abe: 8.856 | eve: 10.023 | bob: 8.782Epoch   2:  93% | abe: 8.855 | eve: 10.023 | bob: 8.782Epoch   2:  94% | abe: 8.855 | eve: 10.023 | bob: 8.781Epoch   2:  95% | abe: 8.855 | eve: 10.024 | bob: 8.781Epoch   2:  95% | abe: 8.855 | eve: 10.023 | bob: 8.781Epoch   2:  96% | abe: 8.854 | eve: 10.023 | bob: 8.781Epoch   2:  97% | abe: 8.854 | eve: 10.023 | bob: 8.780Epoch   2:  97% | abe: 8.854 | eve: 10.024 | bob: 8.780Epoch   2:  98% | abe: 8.853 | eve: 10.024 | bob: 8.779Epoch   2:  99% | abe: 8.853 | eve: 10.024 | bob: 8.779
New best Bob loss 8.779146426600866 at epoch 2
Epoch   3:   0% | abe: 8.835 | eve: 10.073 | bob: 8.763Epoch   3:   0% | abe: 8.846 | eve: 10.010 | bob: 8.775Epoch   3:   1% | abe: 8.819 | eve: 10.005 | bob: 8.747Epoch   3:   2% | abe: 8.805 | eve: 10.015 | bob: 8.732Epoch   3:   2% | abe: 8.820 | eve: 10.031 | bob: 8.748Epoch   3:   3% | abe: 8.815 | eve: 10.037 | bob: 8.742Epoch   3:   4% | abe: 8.821 | eve: 10.027 | bob: 8.749Epoch   3:   4% | abe: 8.821 | eve: 10.025 | bob: 8.749Epoch   3:   5% | abe: 8.828 | eve: 10.014 | bob: 8.756Epoch   3:   6% | abe: 8.827 | eve: 10.016 | bob: 8.755Epoch   3:   6% | abe: 8.828 | eve: 10.022 | bob: 8.756Epoch   3:   7% | abe: 8.824 | eve: 10.020 | bob: 8.752Epoch   3:   8% | abe: 8.824 | eve: 10.022 | bob: 8.752Epoch   3:   8% | abe: 8.824 | eve: 10.018 | bob: 8.752Epoch   3:   9% | abe: 8.819 | eve: 10.022 | bob: 8.746Epoch   3:  10% | abe: 8.820 | eve: 10.022 | bob: 8.747Epoch   3:  10% | abe: 8.817 | eve: 10.020 | bob: 8.744Epoch   3:  11% | abe: 8.815 | eve: 10.021 | bob: 8.742Epoch   3:  12% | abe: 8.813 | eve: 10.021 | bob: 8.740Epoch   3:  13% | abe: 8.815 | eve: 10.018 | bob: 8.742Epoch   3:  13% | abe: 8.816 | eve: 10.015 | bob: 8.743Epoch   3:  14% | abe: 8.815 | eve: 10.014 | bob: 8.741Epoch   3:  15% | abe: 8.814 | eve: 10.010 | bob: 8.741Epoch   3:  15% | abe: 8.815 | eve: 10.009 | bob: 8.742Epoch   3:  16% | abe: 8.813 | eve: 10.011 | bob: 8.740Epoch   3:  17% | abe: 8.812 | eve: 10.011 | bob: 8.739Epoch   3:  17% | abe: 8.811 | eve: 10.012 | bob: 8.738Epoch   3:  18% | abe: 8.810 | eve: 10.013 | bob: 8.736Epoch   3:  19% | abe: 8.809 | eve: 10.014 | bob: 8.735Epoch   3:  19% | abe: 8.809 | eve: 10.014 | bob: 8.735Epoch   3:  20% | abe: 8.808 | eve: 10.012 | bob: 8.734Epoch   3:  21% | abe: 8.807 | eve: 10.008 | bob: 8.733Epoch   3:  21% | abe: 8.806 | eve: 10.009 | bob: 8.732Epoch   3:  22% | abe: 8.805 | eve: 10.008 | bob: 8.731Epoch   3:  23% | abe: 8.805 | eve: 10.009 | bob: 8.731Epoch   3:  23% | abe: 8.805 | eve: 10.010 | bob: 8.731Epoch   3:  24% | abe: 8.803 | eve: 10.011 | bob: 8.730Epoch   3:  25% | abe: 8.803 | eve: 10.010 | bob: 8.729Epoch   3:  26% | abe: 8.803 | eve: 10.011 | bob: 8.729Epoch   3:  26% | abe: 8.802 | eve: 10.009 | bob: 8.729Epoch   3:  27% | abe: 8.803 | eve: 10.011 | bob: 8.729Epoch   3:  28% | abe: 8.802 | eve: 10.011 | bob: 8.728Epoch   3:  28% | abe: 8.801 | eve: 10.012 | bob: 8.727Epoch   3:  29% | abe: 8.801 | eve: 10.010 | bob: 8.727Epoch   3:  30% | abe: 8.802 | eve: 10.009 | bob: 8.728Epoch   3:  30% | abe: 8.801 | eve: 10.010 | bob: 8.728Epoch   3:  31% | abe: 8.800 | eve: 10.008 | bob: 8.726Epoch   3:  32% | abe: 8.801 | eve: 10.008 | bob: 8.727Epoch   3:  32% | abe: 8.801 | eve: 10.006 | bob: 8.727Epoch   3:  33% | abe: 8.801 | eve: 10.005 | bob: 8.727Epoch   3:  34% | abe: 8.801 | eve: 10.004 | bob: 8.727Epoch   3:  34% | abe: 8.802 | eve: 10.002 | bob: 8.728Epoch   3:  35% | abe: 8.801 | eve: 10.002 | bob: 8.727Epoch   3:  36% | abe: 8.801 | eve: 10.002 | bob: 8.727Epoch   3:  36% | abe: 8.800 | eve: 10.004 | bob: 8.726Epoch   3:  37% | abe: 8.798 | eve: 10.003 | bob: 8.724Epoch   3:  38% | abe: 8.798 | eve: 10.002 | bob: 8.724Epoch   3:  39% | abe: 8.798 | eve: 10.002 | bob: 8.724Epoch   3:  39% | abe: 8.797 | eve: 10.001 | bob: 8.723Epoch   3:  40% | abe: 8.797 | eve: 10.001 | bob: 8.723Epoch   3:  41% | abe: 8.797 | eve: 10.003 | bob: 8.723Epoch   3:  41% | abe: 8.796 | eve: 10.002 | bob: 8.722Epoch   3:  42% | abe: 8.796 | eve: 10.001 | bob: 8.722Epoch   3:  43% | abe: 8.796 | eve: 10.001 | bob: 8.721Epoch   3:  43% | abe: 8.795 | eve: 10.000 | bob: 8.720Epoch   3:  44% | abe: 8.794 | eve: 10.001 | bob: 8.720Epoch   3:  45% | abe: 8.794 | eve: 10.000 | bob: 8.720Epoch   3:  45% | abe: 8.794 | eve: 10.000 | bob: 8.720Epoch   3:  46% | abe: 8.795 | eve: 10.002 | bob: 8.720Epoch   3:  47% | abe: 8.794 | eve: 10.000 | bob: 8.719Epoch   3:  47% | abe: 8.794 | eve: 10.001 | bob: 8.719Epoch   3:  48% | abe: 8.793 | eve: 10.001 | bob: 8.719Epoch   3:  49% | abe: 8.793 | eve: 10.002 | bob: 8.719Epoch   3:  50% | abe: 8.792 | eve: 10.001 | bob: 8.718Epoch   3:  50% | abe: 8.793 | eve: 10.002 | bob: 8.718Epoch   3:  51% | abe: 8.793 | eve: 10.002 | bob: 8.718Epoch   3:  52% | abe: 8.793 | eve: 10.003 | bob: 8.718Epoch   3:  52% | abe: 8.793 | eve: 10.002 | bob: 8.719Epoch   3:  53% | abe: 8.793 | eve: 10.001 | bob: 8.718Epoch   3:  54% | abe: 8.792 | eve: 10.001 | bob: 8.718Epoch   3:  54% | abe: 8.792 | eve: 9.999 | bob: 8.718Epoch   3:  55% | abe: 8.792 | eve: 9.998 | bob: 8.718Epoch   3:  56% | abe: 8.792 | eve: 9.998 | bob: 8.717Epoch   3:  56% | abe: 8.791 | eve: 9.998 | bob: 8.717Epoch   3:  57% | abe: 8.791 | eve: 9.998 | bob: 8.716Epoch   3:  58% | abe: 8.791 | eve: 9.997 | bob: 8.716Epoch   3:  58% | abe: 8.791 | eve: 9.997 | bob: 8.716Epoch   3:  59% | abe: 8.790 | eve: 9.997 | bob: 8.715Epoch   3:  60% | abe: 8.790 | eve: 9.998 | bob: 8.715Epoch   3:  60% | abe: 8.791 | eve: 9.998 | bob: 8.716Epoch   3:  61% | abe: 8.791 | eve: 9.998 | bob: 8.715Epoch   3:  62% | abe: 8.790 | eve: 9.997 | bob: 8.715Epoch   3:  63% | abe: 8.789 | eve: 9.997 | bob: 8.714Epoch   3:  63% | abe: 8.789 | eve: 9.997 | bob: 8.713Epoch   3:  64% | abe: 8.788 | eve: 9.997 | bob: 8.713Epoch   3:  65% | abe: 8.788 | eve: 9.997 | bob: 8.712Epoch   3:  65% | abe: 8.788 | eve: 9.997 | bob: 8.712Epoch   3:  66% | abe: 8.788 | eve: 9.997 | bob: 8.712Epoch   3:  67% | abe: 8.787 | eve: 9.997 | bob: 8.712Epoch   3:  67% | abe: 8.787 | eve: 9.998 | bob: 8.711Epoch   3:  68% | abe: 8.786 | eve: 9.997 | bob: 8.711Epoch   3:  69% | abe: 8.787 | eve: 9.997 | bob: 8.711Epoch   3:  69% | abe: 8.786 | eve: 9.997 | bob: 8.711Epoch   3:  70% | abe: 8.786 | eve: 9.997 | bob: 8.710Epoch   3:  71% | abe: 8.786 | eve: 9.997 | bob: 8.710Epoch   3:  71% | abe: 8.785 | eve: 9.996 | bob: 8.709Epoch   3:  72% | abe: 8.785 | eve: 9.996 | bob: 8.709Epoch   3:  73% | abe: 8.785 | eve: 9.996 | bob: 8.709Epoch   3:  73% | abe: 8.784 | eve: 9.996 | bob: 8.709Epoch   3:  74% | abe: 8.784 | eve: 9.996 | bob: 8.708Epoch   3:  75% | abe: 8.784 | eve: 9.995 | bob: 8.708Epoch   3:  76% | abe: 8.783 | eve: 9.994 | bob: 8.708Epoch   3:  76% | abe: 8.783 | eve: 9.994 | bob: 8.707Epoch   3:  77% | abe: 8.783 | eve: 9.993 | bob: 8.707Epoch   3:  78% | abe: 8.782 | eve: 9.993 | bob: 8.707Epoch   3:  78% | abe: 8.782 | eve: 9.993 | bob: 8.706Epoch   3:  79% | abe: 8.781 | eve: 9.993 | bob: 8.706Epoch   3:  80% | abe: 8.781 | eve: 9.993 | bob: 8.705Epoch   3:  80% | abe: 8.780 | eve: 9.993 | bob: 8.704Epoch   3:  81% | abe: 8.779 | eve: 9.993 | bob: 8.704Epoch   3:  82% | abe: 8.780 | eve: 9.993 | bob: 8.704Epoch   3:  82% | abe: 8.779 | eve: 9.993 | bob: 8.703Epoch   3:  83% | abe: 8.779 | eve: 9.993 | bob: 8.703Epoch   3:  84% | abe: 8.779 | eve: 9.992 | bob: 8.703Epoch   3:  84% | abe: 8.778 | eve: 9.992 | bob: 8.702Epoch   3:  85% | abe: 8.778 | eve: 9.992 | bob: 8.702Epoch   3:  86% | abe: 8.778 | eve: 9.992 | bob: 8.702Epoch   3:  86% | abe: 8.777 | eve: 9.991 | bob: 8.701Epoch   3:  87% | abe: 8.777 | eve: 9.991 | bob: 8.701Epoch   3:  88% | abe: 8.776 | eve: 9.992 | bob: 8.700Epoch   3:  89% | abe: 8.776 | eve: 9.992 | bob: 8.700Epoch   3:  89% | abe: 8.776 | eve: 9.992 | bob: 8.700Epoch   3:  90% | abe: 8.776 | eve: 9.991 | bob: 8.700Epoch   3:  91% | abe: 8.776 | eve: 9.992 | bob: 8.700Epoch   3:  91% | abe: 8.776 | eve: 9.992 | bob: 8.700Epoch   3:  92% | abe: 8.775 | eve: 9.992 | bob: 8.699Epoch   3:  93% | abe: 8.775 | eve: 9.992 | bob: 8.699Epoch   3:  93% | abe: 8.775 | eve: 9.992 | bob: 8.699Epoch   3:  94% | abe: 8.775 | eve: 9.992 | bob: 8.699Epoch   3:  95% | abe: 8.774 | eve: 9.992 | bob: 8.698Epoch   3:  95% | abe: 8.774 | eve: 9.992 | bob: 8.698Epoch   3:  96% | abe: 8.773 | eve: 9.992 | bob: 8.697Epoch   3:  97% | abe: 8.773 | eve: 9.993 | bob: 8.697Epoch   3:  97% | abe: 8.773 | eve: 9.993 | bob: 8.697Epoch   3:  98% | abe: 8.773 | eve: 9.993 | bob: 8.697Epoch   3:  99% | abe: 8.772 | eve: 9.993 | bob: 8.696
New best Bob loss 8.696422925001373 at epoch 3
Epoch   4:   0% | abe: 8.758 | eve: 10.019 | bob: 8.685Epoch   4:   0% | abe: 8.761 | eve: 9.994 | bob: 8.687Epoch   4:   1% | abe: 8.721 | eve: 9.990 | bob: 8.646Epoch   4:   2% | abe: 8.720 | eve: 9.979 | bob: 8.646Epoch   4:   2% | abe: 8.719 | eve: 9.976 | bob: 8.644Epoch   4:   3% | abe: 8.723 | eve: 9.985 | bob: 8.648Epoch   4:   4% | abe: 8.725 | eve: 9.987 | bob: 8.650Epoch   4:   4% | abe: 8.722 | eve: 9.987 | bob: 8.647Epoch   4:   5% | abe: 8.723 | eve: 9.994 | bob: 8.648Epoch   4:   6% | abe: 8.725 | eve: 9.992 | bob: 8.650Epoch   4:   6% | abe: 8.723 | eve: 9.986 | bob: 8.649Epoch   4:   7% | abe: 8.723 | eve: 9.991 | bob: 8.648Epoch   4:   8% | abe: 8.723 | eve: 9.993 | bob: 8.648Epoch   4:   8% | abe: 8.722 | eve: 9.985 | bob: 8.647Epoch   4:   9% | abe: 8.722 | eve: 9.980 | bob: 8.647Epoch   4:  10% | abe: 8.722 | eve: 9.977 | bob: 8.647Epoch   4:  10% | abe: 8.718 | eve: 9.977 | bob: 8.643Epoch   4:  11% | abe: 8.719 | eve: 9.980 | bob: 8.644Epoch   4:  12% | abe: 8.719 | eve: 9.980 | bob: 8.645Epoch   4:  13% | abe: 8.718 | eve: 9.980 | bob: 8.644Epoch   4:  13% | abe: 8.718 | eve: 9.976 | bob: 8.643Epoch   4:  14% | abe: 8.718 | eve: 9.974 | bob: 8.643Epoch   4:  15% | abe: 8.717 | eve: 9.970 | bob: 8.643Epoch   4:  15% | abe: 8.716 | eve: 9.970 | bob: 8.642Epoch   4:  16% | abe: 8.715 | eve: 9.968 | bob: 8.640Epoch   4:  17% | abe: 8.716 | eve: 9.965 | bob: 8.641Epoch   4:  17% | abe: 8.716 | eve: 9.965 | bob: 8.641Epoch   4:  18% | abe: 8.716 | eve: 9.962 | bob: 8.641Epoch   4:  19% | abe: 8.715 | eve: 9.962 | bob: 8.641Epoch   4:  19% | abe: 8.716 | eve: 9.962 | bob: 8.642Epoch   4:  20% | abe: 8.716 | eve: 9.965 | bob: 8.642Epoch   4:  21% | abe: 8.715 | eve: 9.966 | bob: 8.641Epoch   4:  21% | abe: 8.715 | eve: 9.969 | bob: 8.640Epoch   4:  22% | abe: 8.715 | eve: 9.967 | bob: 8.640Epoch   4:  23% | abe: 8.714 | eve: 9.965 | bob: 8.639Epoch   4:  23% | abe: 8.712 | eve: 9.967 | bob: 8.637Epoch   4:  24% | abe: 8.712 | eve: 9.966 | bob: 8.638Epoch   4:  25% | abe: 8.711 | eve: 9.965 | bob: 8.636Epoch   4:  26% | abe: 8.710 | eve: 9.965 | bob: 8.635Epoch   4:  26% | abe: 8.710 | eve: 9.968 | bob: 8.635Epoch   4:  27% | abe: 8.709 | eve: 9.969 | bob: 8.635Epoch   4:  28% | abe: 8.710 | eve: 9.970 | bob: 8.635Epoch   4:  28% | abe: 8.709 | eve: 9.970 | bob: 8.635Epoch   4:  29% | abe: 8.710 | eve: 9.971 | bob: 8.635Epoch   4:  30% | abe: 8.708 | eve: 9.970 | bob: 8.633Epoch   4:  30% | abe: 8.708 | eve: 9.970 | bob: 8.633Epoch   4:  31% | abe: 8.706 | eve: 9.972 | bob: 8.631Epoch   4:  32% | abe: 8.705 | eve: 9.972 | bob: 8.630Epoch   4:  32% | abe: 8.704 | eve: 9.972 | bob: 8.629Epoch   4:  33% | abe: 8.705 | eve: 9.972 | bob: 8.630Epoch   4:  34% | abe: 8.706 | eve: 9.971 | bob: 8.631Epoch   4:  34% | abe: 8.706 | eve: 9.970 | bob: 8.631Epoch   4:  35% | abe: 8.705 | eve: 9.971 | bob: 8.631Epoch   4:  36% | abe: 8.705 | eve: 9.971 | bob: 8.630Epoch   4:  36% | abe: 8.705 | eve: 9.972 | bob: 8.630Epoch   4:  37% | abe: 8.706 | eve: 9.971 | bob: 8.631Epoch   4:  38% | abe: 8.705 | eve: 9.971 | bob: 8.631Epoch   4:  39% | abe: 8.705 | eve: 9.969 | bob: 8.630Epoch   4:  39% | abe: 8.705 | eve: 9.968 | bob: 8.630Epoch   4:  40% | abe: 8.703 | eve: 9.967 | bob: 8.629Epoch   4:  41% | abe: 8.702 | eve: 9.967 | bob: 8.627Epoch   4:  41% | abe: 8.702 | eve: 9.968 | bob: 8.627Epoch   4:  42% | abe: 8.702 | eve: 9.967 | bob: 8.627Epoch   4:  43% | abe: 8.701 | eve: 9.967 | bob: 8.627Epoch   4:  43% | abe: 8.701 | eve: 9.967 | bob: 8.627Epoch   4:  44% | abe: 8.701 | eve: 9.967 | bob: 8.627Epoch   4:  45% | abe: 8.700 | eve: 9.966 | bob: 8.626Epoch   4:  45% | abe: 8.700 | eve: 9.965 | bob: 8.626Epoch   4:  46% | abe: 8.699 | eve: 9.964 | bob: 8.625Epoch   4:  47% | abe: 8.699 | eve: 9.963 | bob: 8.625Epoch   4:  47% | abe: 8.698 | eve: 9.962 | bob: 8.624Epoch   4:  48% | abe: 8.698 | eve: 9.961 | bob: 8.623Epoch   4:  49% | abe: 8.697 | eve: 9.961 | bob: 8.623Epoch   4:  50% | abe: 8.697 | eve: 9.961 | bob: 8.622Epoch   4:  50% | abe: 8.697 | eve: 9.961 | bob: 8.622Epoch   4:  51% | abe: 8.697 | eve: 9.961 | bob: 8.623Epoch   4:  52% | abe: 8.697 | eve: 9.961 | bob: 8.623Epoch   4:  52% | abe: 8.697 | eve: 9.960 | bob: 8.622Epoch   4:  53% | abe: 8.696 | eve: 9.960 | bob: 8.622Epoch   4:  54% | abe: 8.697 | eve: 9.961 | bob: 8.623Epoch   4:  54% | abe: 8.696 | eve: 9.961 | bob: 8.622Epoch   4:  55% | abe: 8.697 | eve: 9.960 | bob: 8.622Epoch   4:  56% | abe: 8.696 | eve: 9.960 | bob: 8.622Epoch   4:  56% | abe: 8.696 | eve: 9.960 | bob: 8.622Epoch   4:  57% | abe: 8.696 | eve: 9.960 | bob: 8.622Epoch   4:  58% | abe: 8.695 | eve: 9.959 | bob: 8.621Epoch   4:  58% | abe: 8.695 | eve: 9.958 | bob: 8.621Epoch   4:  59% | abe: 8.695 | eve: 9.958 | bob: 8.621Epoch   4:  60% | abe: 8.694 | eve: 9.958 | bob: 8.620Epoch   4:  60% | abe: 8.693 | eve: 9.958 | bob: 8.619Epoch   4:  61% | abe: 8.693 | eve: 9.958 | bob: 8.619Epoch   4:  62% | abe: 8.692 | eve: 9.958 | bob: 8.618Epoch   4:  63% | abe: 8.692 | eve: 9.958 | bob: 8.618Epoch   4:  63% | abe: 8.692 | eve: 9.957 | bob: 8.618Epoch   4:  64% | abe: 8.692 | eve: 9.957 | bob: 8.618Epoch   4:  65% | abe: 8.692 | eve: 9.957 | bob: 8.618Epoch   4:  65% | abe: 8.692 | eve: 9.957 | bob: 8.618Epoch   4:  66% | abe: 8.691 | eve: 9.957 | bob: 8.617Epoch   4:  67% | abe: 8.691 | eve: 9.956 | bob: 8.617Epoch   4:  67% | abe: 8.691 | eve: 9.957 | bob: 8.617Epoch   4:  68% | abe: 8.690 | eve: 9.956 | bob: 8.616Epoch   4:  69% | abe: 8.690 | eve: 9.955 | bob: 8.616Epoch   4:  69% | abe: 8.690 | eve: 9.955 | bob: 8.616Epoch   4:  70% | abe: 8.690 | eve: 9.955 | bob: 8.616Epoch   4:  71% | abe: 8.689 | eve: 9.954 | bob: 8.616Epoch   4:  71% | abe: 8.689 | eve: 9.954 | bob: 8.615Epoch   4:  72% | abe: 8.689 | eve: 9.953 | bob: 8.616Epoch   4:  73% | abe: 8.689 | eve: 9.953 | bob: 8.615Epoch   4:  73% | abe: 8.689 | eve: 9.952 | bob: 8.615Epoch   4:  74% | abe: 8.689 | eve: 9.951 | bob: 8.615Epoch   4:  75% | abe: 8.689 | eve: 9.950 | bob: 8.615Epoch   4:  76% | abe: 8.688 | eve: 9.950 | bob: 8.615Epoch   4:  76% | abe: 8.688 | eve: 9.950 | bob: 8.615Epoch   4:  77% | abe: 8.688 | eve: 9.949 | bob: 8.615Epoch   4:  78% | abe: 8.688 | eve: 9.949 | bob: 8.614Epoch   4:  78% | abe: 8.688 | eve: 9.949 | bob: 8.614Epoch   4:  79% | abe: 8.688 | eve: 9.948 | bob: 8.614Epoch   4:  80% | abe: 8.687 | eve: 9.948 | bob: 8.614Epoch   4:  80% | abe: 8.687 | eve: 9.947 | bob: 8.613Epoch   4:  81% | abe: 8.687 | eve: 9.948 | bob: 8.613Epoch   4:  82% | abe: 8.686 | eve: 9.948 | bob: 8.613Epoch   4:  82% | abe: 8.686 | eve: 9.948 | bob: 8.612Epoch   4:  83% | abe: 8.685 | eve: 9.948 | bob: 8.612Epoch   4:  84% | abe: 8.685 | eve: 9.947 | bob: 8.611Epoch   4:  84% | abe: 8.684 | eve: 9.947 | bob: 8.611Epoch   4:  85% | abe: 8.684 | eve: 9.948 | bob: 8.611Epoch   4:  86% | abe: 8.684 | eve: 9.947 | bob: 8.611Epoch   4:  86% | abe: 8.684 | eve: 9.946 | bob: 8.610Epoch   4:  87% | abe: 8.683 | eve: 9.946 | bob: 8.610Epoch   4:  88% | abe: 8.682 | eve: 9.947 | bob: 8.609Epoch   4:  89% | abe: 8.682 | eve: 9.946 | bob: 8.609Epoch   4:  89% | abe: 8.682 | eve: 9.946 | bob: 8.609Epoch   4:  90% | abe: 8.681 | eve: 9.945 | bob: 8.608Epoch   4:  91% | abe: 8.681 | eve: 9.945 | bob: 8.608Epoch   4:  91% | abe: 8.681 | eve: 9.945 | bob: 8.607Epoch   4:  92% | abe: 8.680 | eve: 9.945 | bob: 8.607Epoch   4:  93% | abe: 8.680 | eve: 9.946 | bob: 8.607Epoch   4:  93% | abe: 8.680 | eve: 9.946 | bob: 8.606Epoch   4:  94% | abe: 8.679 | eve: 9.945 | bob: 8.606Epoch   4:  95% | abe: 8.679 | eve: 9.946 | bob: 8.606Epoch   4:  95% | abe: 8.679 | eve: 9.945 | bob: 8.606Epoch   4:  96% | abe: 8.678 | eve: 9.946 | bob: 8.605Epoch   4:  97% | abe: 8.678 | eve: 9.945 | bob: 8.605Epoch   4:  97% | abe: 8.677 | eve: 9.945 | bob: 8.604Epoch   4:  98% | abe: 8.677 | eve: 9.945 | bob: 8.604Epoch   4:  99% | abe: 8.676 | eve: 9.945 | bob: 8.603
New best Bob loss 8.602996344269853 at epoch 4
Epoch   5:   0% | abe: 8.684 | eve: 9.872 | bob: 8.612Epoch   5:   0% | abe: 8.650 | eve: 9.869 | bob: 8.576Epoch   5:   1% | abe: 8.636 | eve: 9.919 | bob: 8.562Epoch   5:   2% | abe: 8.644 | eve: 9.921 | bob: 8.570Epoch   5:   2% | abe: 8.645 | eve: 9.916 | bob: 8.570Epoch   5:   3% | abe: 8.642 | eve: 9.923 | bob: 8.568Epoch   5:   4% | abe: 8.637 | eve: 9.924 | bob: 8.563Epoch   5:   4% | abe: 8.638 | eve: 9.929 | bob: 8.564Epoch   5:   5% | abe: 8.630 | eve: 9.923 | bob: 8.556Epoch   5:   6% | abe: 8.622 | eve: 9.918 | bob: 8.547Epoch   5:   6% | abe: 8.612 | eve: 9.916 | bob: 8.538Epoch   5:   7% | abe: 8.610 | eve: 9.919 | bob: 8.536Epoch   5:   8% | abe: 8.609 | eve: 9.920 | bob: 8.535Epoch   5:   8% | abe: 8.610 | eve: 9.919 | bob: 8.536Epoch   5:   9% | abe: 8.612 | eve: 9.917 | bob: 8.538Epoch   5:  10% | abe: 8.610 | eve: 9.914 | bob: 8.536Epoch   5:  10% | abe: 8.611 | eve: 9.913 | bob: 8.537Epoch   5:  11% | abe: 8.609 | eve: 9.913 | bob: 8.535Epoch   5:  12% | abe: 8.610 | eve: 9.919 | bob: 8.536Epoch   5:  13% | abe: 8.609 | eve: 9.922 | bob: 8.535Epoch   5:  13% | abe: 8.605 | eve: 9.928 | bob: 8.531Epoch   5:  14% | abe: 8.606 | eve: 9.927 | bob: 8.532Epoch   5:  15% | abe: 8.605 | eve: 9.924 | bob: 8.532Epoch   5:  15% | abe: 8.604 | eve: 9.924 | bob: 8.530Epoch   5:  16% | abe: 8.603 | eve: 9.924 | bob: 8.529Epoch   5:  17% | abe: 8.602 | eve: 9.924 | bob: 8.528Epoch   5:  17% | abe: 8.603 | eve: 9.925 | bob: 8.529Epoch   5:  18% | abe: 8.604 | eve: 9.927 | bob: 8.531Epoch   5:  19% | abe: 8.602 | eve: 9.925 | bob: 8.529Epoch   5:  19% | abe: 8.601 | eve: 9.925 | bob: 8.528Epoch   5:  20% | abe: 8.601 | eve: 9.920 | bob: 8.528Epoch   5:  21% | abe: 8.601 | eve: 9.919 | bob: 8.527Epoch   5:  21% | abe: 8.600 | eve: 9.917 | bob: 8.527Epoch   5:  22% | abe: 8.599 | eve: 9.918 | bob: 8.525Epoch   5:  23% | abe: 8.599 | eve: 9.917 | bob: 8.525Epoch   5:  23% | abe: 8.598 | eve: 9.917 | bob: 8.525Epoch   5:  24% | abe: 8.598 | eve: 9.915 | bob: 8.525Epoch   5:  25% | abe: 8.597 | eve: 9.914 | bob: 8.524Epoch   5:  26% | abe: 8.595 | eve: 9.913 | bob: 8.522Epoch   5:  26% | abe: 8.594 | eve: 9.912 | bob: 8.521Epoch   5:  27% | abe: 8.594 | eve: 9.911 | bob: 8.522Epoch   5:  28% | abe: 8.594 | eve: 9.911 | bob: 8.521Epoch   5:  28% | abe: 8.594 | eve: 9.910 | bob: 8.521Epoch   5:  29% | abe: 8.592 | eve: 9.911 | bob: 8.519Epoch   5:  30% | abe: 8.592 | eve: 9.912 | bob: 8.519Epoch   5:  30% | abe: 8.592 | eve: 9.913 | bob: 8.519Epoch   5:  31% | abe: 8.591 | eve: 9.912 | bob: 8.519Epoch   5:  32% | abe: 8.591 | eve: 9.912 | bob: 8.518Epoch   5:  32% | abe: 8.591 | eve: 9.912 | bob: 8.518Epoch   5:  33% | abe: 8.590 | eve: 9.914 | bob: 8.518Epoch   5:  34% | abe: 8.589 | eve: 9.915 | bob: 8.516Epoch   5:  34% | abe: 8.589 | eve: 9.916 | bob: 8.516Epoch   5:  35% | abe: 8.588 | eve: 9.914 | bob: 8.516Epoch   5:  36% | abe: 8.588 | eve: 9.914 | bob: 8.515Epoch   5:  36% | abe: 8.588 | eve: 9.915 | bob: 8.516Epoch   5:  37% | abe: 8.588 | eve: 9.916 | bob: 8.516Epoch   5:  38% | abe: 8.588 | eve: 9.916 | bob: 8.515Epoch   5:  39% | abe: 8.587 | eve: 9.917 | bob: 8.515Epoch   5:  39% | abe: 8.586 | eve: 9.918 | bob: 8.514Epoch   5:  40% | abe: 8.586 | eve: 9.918 | bob: 8.513Epoch   5:  41% | abe: 8.585 | eve: 9.919 | bob: 8.512Epoch   5:  41% | abe: 8.584 | eve: 9.919 | bob: 8.511Epoch   5:  42% | abe: 8.583 | eve: 9.920 | bob: 8.511Epoch   5:  43% | abe: 8.583 | eve: 9.920 | bob: 8.510Epoch   5:  43% | abe: 8.583 | eve: 9.919 | bob: 8.511Epoch   5:  44% | abe: 8.583 | eve: 9.919 | bob: 8.511Epoch   5:  45% | abe: 8.582 | eve: 9.919 | bob: 8.510Epoch   5:  45% | abe: 8.581 | eve: 9.919 | bob: 8.509Epoch   5:  46% | abe: 8.580 | eve: 9.921 | bob: 8.508Epoch   5:  47% | abe: 8.579 | eve: 9.922 | bob: 8.507Epoch   5:  47% | abe: 8.579 | eve: 9.922 | bob: 8.507Epoch   5:  48% | abe: 8.579 | eve: 9.922 | bob: 8.507Epoch   5:  49% | abe: 8.579 | eve: 9.922 | bob: 8.507Epoch   5:  50% | abe: 8.579 | eve: 9.921 | bob: 8.507Epoch   5:  50% | abe: 8.578 | eve: 9.922 | bob: 8.506Epoch   5:  51% | abe: 8.577 | eve: 9.922 | bob: 8.505Epoch   5:  52% | abe: 8.577 | eve: 9.922 | bob: 8.505Epoch   5:  52% | abe: 8.576 | eve: 9.921 | bob: 8.504Epoch   5:  53% | abe: 8.576 | eve: 9.922 | bob: 8.504Epoch   5:  54% | abe: 8.576 | eve: 9.921 | bob: 8.504Epoch   5:  54% | abe: 8.575 | eve: 9.921 | bob: 8.503Epoch   5:  55% | abe: 8.575 | eve: 9.921 | bob: 8.503Epoch   5:  56% | abe: 8.574 | eve: 9.920 | bob: 8.502Epoch   5:  56% | abe: 8.573 | eve: 9.921 | bob: 8.501Epoch   5:  57% | abe: 8.573 | eve: 9.920 | bob: 8.501Epoch   5:  58% | abe: 8.573 | eve: 9.920 | bob: 8.501Epoch   5:  58% | abe: 8.573 | eve: 9.919 | bob: 8.501Epoch   5:  59% | abe: 8.572 | eve: 9.919 | bob: 8.500Epoch   5:  60% | abe: 8.572 | eve: 9.918 | bob: 8.500Epoch   5:  60% | abe: 8.571 | eve: 9.917 | bob: 8.499Epoch   5:  61% | abe: 8.570 | eve: 9.917 | bob: 8.498Epoch   5:  62% | abe: 8.570 | eve: 9.918 | bob: 8.498Epoch   5:  63% | abe: 8.570 | eve: 9.918 | bob: 8.498Epoch   5:  63% | abe: 8.569 | eve: 9.917 | bob: 8.498Epoch   5:  64% | abe: 8.569 | eve: 9.917 | bob: 8.498Epoch   5:  65% | abe: 8.569 | eve: 9.917 | bob: 8.497Epoch   5:  65% | abe: 8.569 | eve: 9.917 | bob: 8.497Epoch   5:  66% | abe: 8.569 | eve: 9.917 | bob: 8.497Epoch   5:  67% | abe: 8.568 | eve: 9.917 | bob: 8.497Epoch   5:  67% | abe: 8.567 | eve: 9.918 | bob: 8.496Epoch   5:  68% | abe: 8.567 | eve: 9.917 | bob: 8.496Epoch   5:  69% | abe: 8.567 | eve: 9.917 | bob: 8.495Epoch   5:  69% | abe: 8.566 | eve: 9.917 | bob: 8.495Epoch   5:  70% | abe: 8.566 | eve: 9.917 | bob: 8.494Epoch   5:  71% | abe: 8.565 | eve: 9.917 | bob: 8.494Epoch   5:  71% | abe: 8.565 | eve: 9.917 | bob: 8.494Epoch   5:  72% | abe: 8.565 | eve: 9.916 | bob: 8.493Epoch   5:  73% | abe: 8.564 | eve: 9.917 | bob: 8.492Epoch   5:  73% | abe: 8.563 | eve: 9.917 | bob: 8.492Epoch   5:  74% | abe: 8.562 | eve: 9.916 | bob: 8.491Epoch   5:  75% | abe: 8.562 | eve: 9.916 | bob: 8.491Epoch   5:  76% | abe: 8.562 | eve: 9.915 | bob: 8.490Epoch   5:  76% | abe: 8.561 | eve: 9.915 | bob: 8.489Epoch   5:  77% | abe: 8.560 | eve: 9.916 | bob: 8.489Epoch   5:  78% | abe: 8.560 | eve: 9.916 | bob: 8.488Epoch   5:  78% | abe: 8.559 | eve: 9.916 | bob: 8.488Epoch   5:  79% | abe: 8.559 | eve: 9.916 | bob: 8.487Epoch   5:  80% | abe: 8.558 | eve: 9.916 | bob: 8.486Epoch   5:  80% | abe: 8.557 | eve: 9.915 | bob: 8.486Epoch   5:  81% | abe: 8.557 | eve: 9.915 | bob: 8.485Epoch   5:  82% | abe: 8.556 | eve: 9.915 | bob: 8.485Epoch   5:  82% | abe: 8.555 | eve: 9.915 | bob: 8.484Epoch   5:  83% | abe: 8.555 | eve: 9.915 | bob: 8.484Epoch   5:  84% | abe: 8.554 | eve: 9.915 | bob: 8.483Epoch   5:  84% | abe: 8.553 | eve: 9.914 | bob: 8.482Epoch   5:  85% | abe: 8.553 | eve: 9.914 | bob: 8.482Epoch   5:  86% | abe: 8.552 | eve: 9.914 | bob: 8.481Epoch   5:  86% | abe: 8.551 | eve: 9.913 | bob: 8.480Epoch   5:  87% | abe: 8.551 | eve: 9.913 | bob: 8.480Epoch   5:  88% | abe: 8.550 | eve: 9.913 | bob: 8.479Epoch   5:  89% | abe: 8.549 | eve: 9.914 | bob: 8.478Epoch   5:  89% | abe: 8.549 | eve: 9.914 | bob: 8.478Epoch   5:  90% | abe: 8.548 | eve: 9.914 | bob: 8.477Epoch   5:  91% | abe: 8.547 | eve: 9.915 | bob: 8.476Epoch   5:  91% | abe: 8.547 | eve: 9.914 | bob: 8.476Epoch   5:  92% | abe: 8.547 | eve: 9.913 | bob: 8.476Epoch   5:  93% | abe: 8.546 | eve: 9.913 | bob: 8.475Epoch   5:  93% | abe: 8.546 | eve: 9.913 | bob: 8.475Epoch   5:  94% | abe: 8.545 | eve: 9.913 | bob: 8.474Epoch   5:  95% | abe: 8.545 | eve: 9.912 | bob: 8.474Epoch   5:  95% | abe: 8.544 | eve: 9.913 | bob: 8.473Epoch   5:  96% | abe: 8.544 | eve: 9.913 | bob: 8.473Epoch   5:  97% | abe: 8.543 | eve: 9.913 | bob: 8.472Epoch   5:  97% | abe: 8.543 | eve: 9.913 | bob: 8.472Epoch   5:  98% | abe: 8.542 | eve: 9.913 | bob: 8.472Epoch   5:  99% | abe: 8.542 | eve: 9.912 | bob: 8.471
New best Bob loss 8.470928539600134 at epoch 5
Epoch   6:   0% | abe: 8.478 | eve: 9.904 | bob: 8.411Epoch   6:   0% | abe: 8.466 | eve: 9.916 | bob: 8.398Epoch   6:   1% | abe: 8.472 | eve: 9.917 | bob: 8.405Epoch   6:   2% | abe: 8.472 | eve: 9.911 | bob: 8.405Epoch   6:   2% | abe: 8.462 | eve: 9.913 | bob: 8.395Epoch   6:   3% | abe: 8.471 | eve: 9.900 | bob: 8.404Epoch   6:   4% | abe: 8.465 | eve: 9.897 | bob: 8.399Epoch   6:   4% | abe: 8.472 | eve: 9.893 | bob: 8.406Epoch   6:   5% | abe: 8.469 | eve: 9.887 | bob: 8.403Epoch   6:   6% | abe: 8.474 | eve: 9.890 | bob: 8.407Epoch   6:   6% | abe: 8.470 | eve: 9.894 | bob: 8.403Epoch   6:   7% | abe: 8.468 | eve: 9.897 | bob: 8.401Epoch   6:   8% | abe: 8.468 | eve: 9.902 | bob: 8.402Epoch   6:   8% | abe: 8.468 | eve: 9.904 | bob: 8.402Epoch   6:   9% | abe: 8.469 | eve: 9.906 | bob: 8.403Epoch   6:  10% | abe: 8.466 | eve: 9.910 | bob: 8.400Epoch   6:  10% | abe: 8.464 | eve: 9.907 | bob: 8.398Epoch   6:  11% | abe: 8.464 | eve: 9.909 | bob: 8.397Epoch   6:  12% | abe: 8.465 | eve: 9.912 | bob: 8.398Epoch   6:  13% | abe: 8.463 | eve: 9.911 | bob: 8.396Epoch   6:  13% | abe: 8.461 | eve: 9.912 | bob: 8.394Epoch   6:  14% | abe: 8.462 | eve: 9.913 | bob: 8.396Epoch   6:  15% | abe: 8.463 | eve: 9.915 | bob: 8.397Epoch   6:  15% | abe: 8.463 | eve: 9.911 | bob: 8.397Epoch   6:  16% | abe: 8.461 | eve: 9.912 | bob: 8.395Epoch   6:  17% | abe: 8.460 | eve: 9.913 | bob: 8.394Epoch   6:  17% | abe: 8.459 | eve: 9.913 | bob: 8.393Epoch   6:  18% | abe: 8.458 | eve: 9.912 | bob: 8.392Epoch   6:  19% | abe: 8.458 | eve: 9.911 | bob: 8.391Epoch   6:  19% | abe: 8.458 | eve: 9.910 | bob: 8.391Epoch   6:  20% | abe: 8.456 | eve: 9.909 | bob: 8.389Epoch   6:  21% | abe: 8.455 | eve: 9.909 | bob: 8.389Epoch   6:  21% | abe: 8.455 | eve: 9.910 | bob: 8.388Epoch   6:  22% | abe: 8.455 | eve: 9.910 | bob: 8.388Epoch   6:  23% | abe: 8.454 | eve: 9.908 | bob: 8.387Epoch   6:  23% | abe: 8.453 | eve: 9.909 | bob: 8.386Epoch   6:  24% | abe: 8.452 | eve: 9.908 | bob: 8.386Epoch   6:  25% | abe: 8.452 | eve: 9.907 | bob: 8.386Epoch   6:  26% | abe: 8.452 | eve: 9.906 | bob: 8.385Epoch   6:  26% | abe: 8.451 | eve: 9.908 | bob: 8.384Epoch   6:  27% | abe: 8.449 | eve: 9.907 | bob: 8.383Epoch   6:  28% | abe: 8.450 | eve: 9.909 | bob: 8.383Epoch   6:  28% | abe: 8.450 | eve: 9.911 | bob: 8.384Epoch   6:  29% | abe: 8.450 | eve: 9.911 | bob: 8.384Epoch   6:  30% | abe: 8.449 | eve: 9.910 | bob: 8.383Epoch   6:  30% | abe: 8.450 | eve: 9.908 | bob: 8.384Epoch   6:  31% | abe: 8.450 | eve: 9.908 | bob: 8.384Epoch   6:  32% | abe: 8.449 | eve: 9.908 | bob: 8.383Epoch   6:  32% | abe: 8.448 | eve: 9.910 | bob: 8.382Epoch   6:  33% | abe: 8.448 | eve: 9.910 | bob: 8.382Epoch   6:  34% | abe: 8.447 | eve: 9.909 | bob: 8.381Epoch   6:  34% | abe: 8.446 | eve: 9.910 | bob: 8.380Epoch   6:  35% | abe: 8.445 | eve: 9.909 | bob: 8.378Epoch   6:  36% | abe: 8.444 | eve: 9.908 | bob: 8.378Epoch   6:  36% | abe: 8.444 | eve: 9.907 | bob: 8.378Epoch   6:  37% | abe: 8.443 | eve: 9.908 | bob: 8.376Epoch   6:  38% | abe: 8.443 | eve: 9.908 | bob: 8.376Epoch   6:  39% | abe: 8.443 | eve: 9.908 | bob: 8.377Epoch   6:  39% | abe: 8.442 | eve: 9.907 | bob: 8.376Epoch   6:  40% | abe: 8.441 | eve: 9.909 | bob: 8.375Epoch   6:  41% | abe: 8.441 | eve: 9.908 | bob: 8.375Epoch   6:  41% | abe: 8.440 | eve: 9.908 | bob: 8.374Epoch   6:  42% | abe: 8.439 | eve: 9.911 | bob: 8.373Epoch   6:  43% | abe: 8.439 | eve: 9.911 | bob: 8.373Epoch   6:  43% | abe: 8.438 | eve: 9.912 | bob: 8.372Epoch   6:  44% | abe: 8.437 | eve: 9.912 | bob: 8.371Epoch   6:  45% | abe: 8.436 | eve: 9.914 | bob: 8.370Epoch   6:  45% | abe: 8.436 | eve: 9.915 | bob: 8.370Epoch   6:  46% | abe: 8.436 | eve: 9.915 | bob: 8.370Epoch   6:  47% | abe: 8.434 | eve: 9.914 | bob: 8.368Epoch   6:  47% | abe: 8.433 | eve: 9.915 | bob: 8.367Epoch   6:  48% | abe: 8.433 | eve: 9.917 | bob: 8.367Epoch   6:  49% | abe: 8.433 | eve: 9.916 | bob: 8.367Epoch   6:  50% | abe: 8.433 | eve: 9.916 | bob: 8.367Epoch   6:  50% | abe: 8.432 | eve: 9.916 | bob: 8.366Epoch   6:  51% | abe: 8.431 | eve: 9.916 | bob: 8.365Epoch   6:  52% | abe: 8.431 | eve: 9.916 | bob: 8.365Epoch   6:  52% | abe: 8.430 | eve: 9.916 | bob: 8.364Epoch   6:  53% | abe: 8.430 | eve: 9.916 | bob: 8.364Epoch   6:  54% | abe: 8.429 | eve: 9.916 | bob: 8.364Epoch   6:  54% | abe: 8.429 | eve: 9.916 | bob: 8.363Epoch   6:  55% | abe: 8.428 | eve: 9.916 | bob: 8.362Epoch   6:  56% | abe: 8.427 | eve: 9.916 | bob: 8.362Epoch   6:  56% | abe: 8.427 | eve: 9.916 | bob: 8.361Epoch   6:  57% | abe: 8.426 | eve: 9.917 | bob: 8.361Epoch   6:  58% | abe: 8.426 | eve: 9.916 | bob: 8.361Epoch   6:  58% | abe: 8.427 | eve: 9.916 | bob: 8.361Epoch   6:  59% | abe: 8.426 | eve: 9.915 | bob: 8.360Epoch   6:  60% | abe: 8.425 | eve: 9.915 | bob: 8.360Epoch   6:  60% | abe: 8.425 | eve: 9.916 | bob: 8.360Epoch   6:  61% | abe: 8.424 | eve: 9.916 | bob: 8.359Epoch   6:  62% | abe: 8.423 | eve: 9.916 | bob: 8.358Epoch   6:  63% | abe: 8.423 | eve: 9.916 | bob: 8.358Epoch   6:  63% | abe: 8.422 | eve: 9.916 | bob: 8.357Epoch   6:  64% | abe: 8.422 | eve: 9.916 | bob: 8.356Epoch   6:  65% | abe: 8.421 | eve: 9.916 | bob: 8.356Epoch   6:  65% | abe: 8.421 | eve: 9.916 | bob: 8.355Epoch   6:  66% | abe: 8.420 | eve: 9.916 | bob: 8.355Epoch   6:  67% | abe: 8.420 | eve: 9.916 | bob: 8.355Epoch   6:  67% | abe: 8.419 | eve: 9.916 | bob: 8.354Epoch   6:  68% | abe: 8.420 | eve: 9.917 | bob: 8.354Epoch   6:  69% | abe: 8.419 | eve: 9.917 | bob: 8.354Epoch   6:  69% | abe: 8.418 | eve: 9.917 | bob: 8.353Epoch   6:  70% | abe: 8.417 | eve: 9.917 | bob: 8.352