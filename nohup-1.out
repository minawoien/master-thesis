WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-07 19:46:22.495954: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-07 19:46:22.627514: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-07 19:46:22.628412: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-07 19:46:22.631149: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-07 19:46:22.633482: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-07 19:46:22.634481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-07 19:46:22.637429: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-07 19:46:22.639885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-07 19:46:22.646118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-07 19:46:22.654328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-07 19:46:22.655072: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-07 19:46:22.672466: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-07 19:46:22.675776: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4726f50 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-07 19:46:22.675808: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-07 19:46:23.233140: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x40f0740 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-07 19:46:23.233228: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-07 19:46:23.238158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-07 19:46:23.238278: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-07 19:46:23.238328: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-07 19:46:23.238369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-07 19:46:23.238421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-07 19:46:23.238466: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-07 19:46:23.238506: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-07 19:46:23.238554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-07 19:46:23.245914: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-07 19:46:23.246017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-07 19:46:23.253920: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-07 19:46:23.253962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-07 19:46:23.253974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-07 19:46:23.258627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-07 19:46:26.539070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6243 - val_loss: 0.0036
Epoch 2/512
512/512 - 0s - loss: 0.2799 - val_loss: 0.0017
Epoch 3/512
512/512 - 0s - loss: 0.1268 - val_loss: 5.5271e-04
Epoch 4/512
512/512 - 0s - loss: 0.0290 - val_loss: 3.7775e-05
Epoch 5/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.4730e-06
Epoch 6/512
512/512 - 0s - loss: 1.1365e-04 - val_loss: 8.6904e-07
Epoch 7/512
512/512 - 0s - loss: 7.8226e-05 - val_loss: 6.4251e-07
Epoch 8/512
512/512 - 0s - loss: 5.6512e-05 - val_loss: 4.4274e-07
Epoch 9/512
512/512 - 0s - loss: 3.7829e-05 - val_loss: 2.7922e-07
Epoch 10/512
512/512 - 0s - loss: 2.3024e-05 - val_loss: 1.5753e-07
Epoch 11/512
512/512 - 0s - loss: 1.2438e-05 - val_loss: 7.7262e-08
Epoch 12/512
512/512 - 0s - loss: 5.7867e-06 - val_loss: 3.1778e-08
Epoch 13/512
512/512 - 0s - loss: 2.2362e-06 - val_loss: 1.0807e-08
Epoch 14/512
512/512 - 0s - loss: 1.3234e-06 - val_loss: 1.0623e-07
Epoch 15/512
512/512 - 0s - loss: 6.4841e-04 - val_loss: 4.2661e-05
Epoch 16/512
512/512 - 0s - loss: 0.0023 - val_loss: 2.8699e-06
Epoch 17/512
512/512 - 0s - loss: 1.6624e-04 - val_loss: 1.0344e-06
Epoch 18/512
512/512 - 0s - loss: 1.8248e-04 - val_loss: 5.6597e-06
Epoch 19/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.9991e-05
Epoch 20/512
512/512 - 0s - loss: 0.0012 - val_loss: 4.5550e-06
Epoch 21/512
512/512 - 0s - loss: 3.8294e-04 - val_loss: 4.3401e-06
Epoch 22/512
512/512 - 0s - loss: 6.8103e-04 - val_loss: 1.2793e-05
Epoch 23/512
512/512 - 0s - loss: 0.0013 - val_loss: 8.9336e-06
Epoch 24/512
512/512 - 0s - loss: 6.7744e-04 - val_loss: 5.1071e-06
Epoch 25/512
512/512 - 0s - loss: 5.7346e-04 - val_loss: 8.1415e-06
Epoch 26/512
512/512 - 0s - loss: 9.5273e-04 - val_loss: 1.0082e-05
Epoch 27/512
512/512 - 0s - loss: 8.4654e-04 - val_loss: 6.4315e-06
Epoch 28/512
512/512 - 0s - loss: 6.1050e-04 - val_loss: 6.6947e-06
Epoch 29/512
512/512 - 0s - loss: 7.5159e-04 - val_loss: 8.6589e-06
Epoch 30/512
512/512 - 0s - loss: 8.2247e-04 - val_loss: 7.2935e-06
Epoch 31/512
512/512 - 0s - loss: 6.6706e-04 - val_loss: 6.4282e-06
Epoch 32/512
512/512 - 0s - loss: 6.6434e-04 - val_loss: 7.3686e-06
Epoch 33/512
512/512 - 0s - loss: 7.4214e-04 - val_loss: 7.2689e-06
Epoch 34/512
512/512 - 0s - loss: 6.8117e-04 - val_loss: 6.3335e-06
Epoch 35/512
512/512 - 0s - loss: 6.3034e-04 - val_loss: 6.5493e-06
Epoch 36/512
512/512 - 0s - loss: 6.6898e-04 - val_loss: 6.8025e-06
Epoch 37/512
512/512 - 0s - loss: 6.6027e-04 - val_loss: 6.2864e-06
Epoch 38/512
512/512 - 0s - loss: 6.0780e-04 - val_loss: 6.2326e-06
Epoch 39/512
512/512 - 0s - loss: 6.3221e-04 - val_loss: 6.2159e-06
Epoch 40/512
512/512 - 0s - loss: 6.0762e-04 - val_loss: 6.0090e-06
Epoch 41/512
512/512 - 0s - loss: 5.9546e-04 - val_loss: 5.9085e-06
Epoch 42/512
512/512 - 0s - loss: 5.8445e-04 - val_loss: 5.9224e-06
Epoch 43/512
512/512 - 0s - loss: 5.8347e-04 - val_loss: 5.7456e-06
Epoch 44/512
512/512 - 0s - loss: 5.6225e-04 - val_loss: 5.5607e-06
Epoch 45/512
512/512 - 0s - loss: 5.5140e-04 - val_loss: 5.5155e-06
Epoch 46/512
512/512 - 0s - loss: 5.5099e-04 - val_loss: 5.4098e-06
Epoch 47/512
512/512 - 0s - loss: 5.3189e-04 - val_loss: 5.2703e-06
Epoch 48/512
512/512 - 0s - loss: 5.2373e-04 - val_loss: 5.2456e-06
Epoch 49/512
512/512 - 0s - loss: 5.2105e-04 - val_loss: 5.0921e-06
Epoch 50/512
512/512 - 0s - loss: 4.9973e-04 - val_loss: 4.9725e-06
Epoch 51/512
512/512 - 0s - loss: 4.9388e-04 - val_loss: 5.0393e-06
Epoch 52/512
512/512 - 0s - loss: 4.9645e-04 - val_loss: 4.8496e-06
Epoch 53/512
512/512 - 0s - loss: 4.7488e-04 - val_loss: 4.6740e-06
Epoch 54/512
512/512 - 0s - loss: 4.6483e-04 - val_loss: 4.6424e-06
Epoch 55/512
512/512 - 0s - loss: 4.5842e-04 - val_loss: 4.6820e-06
Epoch 56/512
512/512 - 0s - loss: 4.5858e-04 - val_loss: 4.5290e-06
Epoch 57/512
512/512 - 0s - loss: 4.4294e-04 - val_loss: 4.2977e-06
Epoch 58/512
512/512 - 0s - loss: 4.2639e-04 - val_loss: 4.3095e-06
Epoch 59/512
512/512 - 0s - loss: 4.3166e-04 - val_loss: 4.2553e-06
Epoch 60/512
512/512 - 0s - loss: 4.1697e-04 - val_loss: 4.1169e-06
Epoch 61/512
512/512 - 0s - loss: 4.0757e-04 - val_loss: 4.0640e-06
Epoch 62/512
512/512 - 0s - loss: 4.0300e-04 - val_loss: 3.9553e-06
Epoch 63/512
512/512 - 0s - loss: 3.9316e-04 - val_loss: 3.8702e-06
Epoch 64/512
512/512 - 0s - loss: 3.8558e-04 - val_loss: 3.7871e-06
Epoch 65/512
512/512 - 0s - loss: 3.7685e-04 - val_loss: 3.7371e-06
Epoch 66/512
512/512 - 0s - loss: 3.6994e-04 - val_loss: 3.6929e-06
Epoch 67/512
512/512 - 0s - loss: 3.6584e-04 - val_loss: 3.5817e-06
Epoch 68/512
512/512 - 0s - loss: 3.5496e-04 - val_loss: 3.4701e-06
Epoch 69/512
512/512 - 0s - loss: 3.4645e-04 - val_loss: 3.4307e-06
Epoch 70/512
512/512 - 0s - loss: 3.4196e-04 - val_loss: 3.3964e-06
Epoch 71/512
512/512 - 0s - loss: 3.3781e-04 - val_loss: 3.2439e-06
Epoch 72/512
512/512 - 0s - loss: 3.2234e-04 - val_loss: 3.1948e-06
Epoch 73/512
512/512 - 0s - loss: 3.1876e-04 - val_loss: 3.2242e-06
Epoch 74/512
512/512 - 0s - loss: 3.2309e-04 - val_loss: 3.0257e-06
Epoch 75/512
512/512 - 0s - loss: 2.9947e-04 - val_loss: 2.9465e-06
Epoch 76/512
512/512 - 0s - loss: 2.9778e-04 - val_loss: 3.0182e-06
Epoch 77/512
512/512 - 0s - loss: 3.0076e-04 - val_loss: 2.9220e-06
Epoch 78/512
512/512 - 0s - loss: 2.8910e-04 - val_loss: 2.7234e-06
Epoch 79/512
512/512 - 0s - loss: 2.7409e-04 - val_loss: 2.7186e-06
Epoch 80/512
512/512 - 0s - loss: 2.7515e-04 - val_loss: 2.8151e-06
Epoch 81/512
512/512 - 0s - loss: 2.7751e-04 - val_loss: 2.6555e-06
Epoch 82/512
512/512 - 0s - loss: 2.5964e-04 - val_loss: 2.5072e-06
Epoch 83/512
512/512 - 0s - loss: 2.5370e-04 - val_loss: 2.5287e-06
Epoch 84/512
512/512 - 0s - loss: 2.5363e-04 - val_loss: 2.5263e-06
Epoch 85/512
512/512 - 0s - loss: 2.5040e-04 - val_loss: 2.3813e-06
Epoch 86/512
512/512 - 0s - loss: 2.3604e-04 - val_loss: 2.3085e-06
Epoch 87/512
512/512 - 0s - loss: 2.3387e-04 - val_loss: 2.3343e-06
Epoch 88/512
512/512 - 0s - loss: 2.3332e-04 - val_loss: 2.2528e-06
Epoch 89/512
512/512 - 0s - loss: 2.2156e-04 - val_loss: 2.1872e-06
Epoch 90/512
512/512 - 0s - loss: 2.1831e-04 - val_loss: 2.1733e-06
Epoch 91/512
512/512 - 0s - loss: 2.1623e-04 - val_loss: 2.1048e-06
Epoch 92/512
512/512 - 0s - loss: 2.0871e-04 - val_loss: 2.0057e-06
Epoch 93/512
512/512 - 0s - loss: 2.0258e-04 - val_loss: 1.9641e-06
Epoch 94/512
512/512 - 0s - loss: 1.9727e-04 - val_loss: 1.9544e-06
Epoch 95/512
512/512 - 0s - loss: 1.9692e-04 - val_loss: 1.8908e-06
Epoch 96/512
512/512 - 0s - loss: 1.8921e-04 - val_loss: 1.7976e-06
Epoch 97/512
512/512 - 0s - loss: 1.7983e-04 - val_loss: 1.8276e-06
Epoch 98/512
512/512 - 0s - loss: 1.8466e-04 - val_loss: 1.7931e-06
Epoch 99/512
512/512 - 0s - loss: 1.7670e-04 - val_loss: 1.6620e-06
Epoch 100/512
512/512 - 0s - loss: 1.6715e-04 - val_loss: 1.6396e-06
Epoch 101/512
512/512 - 0s - loss: 1.6752e-04 - val_loss: 1.6365e-06
Epoch 102/512
512/512 - 0s - loss: 1.6341e-04 - val_loss: 1.5875e-06
Epoch 103/512
512/512 - 0s - loss: 1.5694e-04 - val_loss: 1.5609e-06
Epoch 104/512
512/512 - 0s - loss: 1.5574e-04 - val_loss: 1.5142e-06
Epoch 105/512
512/512 - 0s - loss: 1.5132e-04 - val_loss: 1.4411e-06
Epoch 106/512
512/512 - 0s - loss: 1.4349e-04 - val_loss: 1.4267e-06
Epoch 107/512
512/512 - 0s - loss: 1.4376e-04 - val_loss: 1.4097e-06
Epoch 108/512
512/512 - 0s - loss: 1.4155e-04 - val_loss: 1.3046e-06
Epoch 109/512
512/512 - 0s - loss: 1.2980e-04 - val_loss: 1.3053e-06
Epoch 110/512
512/512 - 0s - loss: 1.3342e-04 - val_loss: 1.3182e-06
Epoch 111/512
512/512 - 0s - loss: 1.3108e-04 - val_loss: 1.2111e-06
Epoch 112/512
512/512 - 0s - loss: 1.2173e-04 - val_loss: 1.1453e-06
Epoch 113/512
512/512 - 0s - loss: 1.1858e-04 - val_loss: 1.1746e-06
Epoch 114/512
512/512 - 0s - loss: 1.2076e-04 - val_loss: 1.1455e-06
Epoch 115/512
512/512 - 0s - loss: 1.1378e-04 - val_loss: 1.0762e-06
Epoch 116/512
512/512 - 0s - loss: 1.0898e-04 - val_loss: 1.0731e-06
Epoch 117/512
512/512 - 0s - loss: 1.0893e-04 - val_loss: 1.0617e-06
Epoch 118/512
512/512 - 0s - loss: 1.0615e-04 - val_loss: 1.0100e-06
Epoch 119/512
512/512 - 0s - loss: 1.0060e-04 - val_loss: 9.7127e-07
Epoch 120/512
512/512 - 0s - loss: 9.8754e-05 - val_loss: 9.5941e-07
Epoch 121/512
512/512 - 0s - loss: 9.6827e-05 - val_loss: 9.2565e-07
Epoch 122/512
512/512 - 0s - loss: 9.3682e-05 - val_loss: 8.8153e-07
Epoch 123/512
512/512 - 0s - loss: 8.8926e-05 - val_loss: 8.6835e-07
Epoch 124/512
512/512 - 0s - loss: 8.8436e-05 - val_loss: 8.5849e-07
Epoch 125/512
512/512 - 0s - loss: 8.6501e-05 - val_loss: 8.0523e-07
Epoch 126/512
512/512 - 0s - loss: 8.1179e-05 - val_loss: 7.8064e-07
Epoch 127/512
512/512 - 0s - loss: 7.9453e-05 - val_loss: 7.9259e-07
Epoch 128/512
512/512 - 0s - loss: 8.0207e-05 - val_loss: 7.5128e-07
Epoch 129/512
512/512 - 0s - loss: 7.4491e-05 - val_loss: 7.0321e-07
Epoch 130/512
512/512 - 0s - loss: 7.1902e-05 - val_loss: 7.0452e-07
Epoch 131/512
512/512 - 0s - loss: 7.1930e-05 - val_loss: 6.8579e-07
Epoch 132/512
512/512 - 0s - loss: 6.8568e-05 - val_loss: 6.5253e-07
Epoch 133/512
512/512 - 0s - loss: 6.6219e-05 - val_loss: 6.3625e-07
Epoch 134/512
512/512 - 0s - loss: 6.4370e-05 - val_loss: 6.2055e-07
Epoch 135/512
512/512 - 0s - loss: 6.2662e-05 - val_loss: 5.9813e-07
Epoch 136/512
512/512 - 0s - loss: 6.0397e-05 - val_loss: 5.7540e-07
Epoch 137/512
512/512 - 0s - loss: 5.8440e-05 - val_loss: 5.5202e-07
Epoch 138/512
512/512 - 0s - loss: 5.6249e-05 - val_loss: 5.4193e-07
Epoch 139/512
512/512 - 0s - loss: 5.5090e-05 - val_loss: 5.2789e-07
Epoch 140/512
512/512 - 0s - loss: 5.2950e-05 - val_loss: 5.0866e-07
Epoch 141/512
512/512 - 0s - loss: 5.1382e-05 - val_loss: 4.8603e-07
Epoch 142/512
512/512 - 0s - loss: 4.9558e-05 - val_loss: 4.6459e-07
Epoch 143/512
512/512 - 0s - loss: 4.7464e-05 - val_loss: 4.5790e-07
Epoch 144/512
512/512 - 0s - loss: 4.6776e-05 - val_loss: 4.4623e-07
Epoch 145/512
512/512 - 0s - loss: 4.4941e-05 - val_loss: 4.2438e-07
Epoch 146/512
512/512 - 0s - loss: 4.3080e-05 - val_loss: 4.0666e-07
Epoch 147/512
512/512 - 0s - loss: 4.1545e-05 - val_loss: 4.0166e-07
Epoch 148/512
512/512 - 0s - loss: 4.0770e-05 - val_loss: 3.9259e-07
Epoch 149/512
512/512 - 0s - loss: 3.9592e-05 - val_loss: 3.6654e-07
Epoch 150/512
512/512 - 0s - loss: 3.6975e-05 - val_loss: 3.5501e-07
Epoch 151/512
512/512 - 0s - loss: 3.6677e-05 - val_loss: 3.4771e-07
Epoch 152/512
512/512 - 0s - loss: 3.5368e-05 - val_loss: 3.3185e-07
Epoch 153/512
512/512 - 0s - loss: 3.3675e-05 - val_loss: 3.1891e-07
Epoch 154/512
512/512 - 0s - loss: 3.2722e-05 - val_loss: 3.1221e-07
Epoch 155/512
512/512 - 0s - loss: 3.1767e-05 - val_loss: 3.0352e-07
Epoch 156/512
512/512 - 0s - loss: 3.0827e-05 - val_loss: 2.8475e-07
Epoch 157/512
512/512 - 0s - loss: 2.8966e-05 - val_loss: 2.7320e-07
Epoch 158/512
512/512 - 0s - loss: 2.8219e-05 - val_loss: 2.7248e-07
Epoch 159/512
512/512 - 0s - loss: 2.7847e-05 - val_loss: 2.6239e-07
Epoch 160/512
512/512 - 0s - loss: 2.6566e-05 - val_loss: 2.4183e-07
Epoch 161/512
512/512 - 0s - loss: 2.4748e-05 - val_loss: 2.3772e-07
Epoch 162/512
512/512 - 0s - loss: 2.4827e-05 - val_loss: 2.3447e-07
Epoch 163/512
512/512 - 0s - loss: 2.3851e-05 - val_loss: 2.2189e-07
Epoch 164/512
512/512 - 0s - loss: 2.2393e-05 - val_loss: 2.1521e-07
Epoch 165/512
512/512 - 0s - loss: 2.2147e-05 - val_loss: 2.0681e-07
Epoch 166/512
512/512 - 0s - loss: 2.1217e-05 - val_loss: 1.9659e-07
Epoch 167/512
512/512 - 0s - loss: 2.0086e-05 - val_loss: 1.9023e-07
Epoch 168/512
512/512 - 0s - loss: 1.9542e-05 - val_loss: 1.8906e-07
Epoch 169/512
512/512 - 0s - loss: 1.9022e-05 - val_loss: 1.8190e-07
Epoch 170/512
512/512 - 0s - loss: 1.8405e-05 - val_loss: 1.6837e-07
Epoch 171/512
512/512 - 0s - loss: 1.7116e-05 - val_loss: 1.6217e-07
Epoch 172/512
512/512 - 0s - loss: 1.6834e-05 - val_loss: 1.5973e-07
Epoch 173/512
512/512 - 0s - loss: 1.6384e-05 - val_loss: 1.5181e-07
Epoch 174/512
512/512 - 0s - loss: 1.5502e-05 - val_loss: 1.4400e-07
Epoch 175/512
512/512 - 0s - loss: 1.4876e-05 - val_loss: 1.3960e-07
Epoch 176/512
512/512 - 0s - loss: 1.4472e-05 - val_loss: 1.3439e-07
Epoch 177/512
512/512 - 0s - loss: 1.3700e-05 - val_loss: 1.3063e-07
Epoch 178/512
512/512 - 0s - loss: 1.3521e-05 - val_loss: 1.2338e-07
Epoch 179/512
512/512 - 0s - loss: 1.2580e-05 - val_loss: 1.1826e-07
Epoch 180/512
512/512 - 0s - loss: 1.2290e-05 - val_loss: 1.1714e-07
Epoch 181/512
512/512 - 0s - loss: 1.1945e-05 - val_loss: 1.1222e-07
Epoch 182/512
512/512 - 0s - loss: 1.1310e-05 - val_loss: 1.0644e-07
Epoch 183/512
512/512 - 0s - loss: 1.0891e-05 - val_loss: 1.0192e-07
Epoch 184/512
512/512 - 0s - loss: 1.0480e-05 - val_loss: 9.7802e-08
Epoch 185/512
512/512 - 0s - loss: 1.0064e-05 - val_loss: 9.3997e-08
Epoch 186/512
512/512 - 0s - loss: 9.6169e-06 - val_loss: 9.0671e-08
Epoch 187/512
512/512 - 0s - loss: 9.3604e-06 - val_loss: 8.6598e-08
Epoch 188/512
512/512 - 0s - loss: 8.8492e-06 - val_loss: 8.2304e-08
Epoch 189/512
512/512 - 0s - loss: 8.4947e-06 - val_loss: 8.0139e-08
Epoch 190/512
512/512 - 0s - loss: 8.2815e-06 - val_loss: 7.6532e-08
Epoch 191/512
512/512 - 0s - loss: 7.8626e-06 - val_loss: 7.2799e-08
Epoch 192/512
512/512 - 0s - loss: 7.4504e-06 - val_loss: 7.0849e-08
Epoch 193/512
512/512 - 0s - loss: 7.3037e-06 - val_loss: 6.8735e-08
Epoch 194/512
512/512 - 0s - loss: 7.0279e-06 - val_loss: 6.3970e-08
Epoch 195/512
512/512 - 0s - loss: 6.5799e-06 - val_loss: 6.0653e-08
Epoch 196/512
512/512 - 0s - loss: 6.3468e-06 - val_loss: 6.0154e-08
Epoch 197/512
512/512 - 0s - loss: 6.1741e-06 - val_loss: 5.8059e-08
Epoch 198/512
512/512 - 0s - loss: 5.9574e-06 - val_loss: 5.3719e-08
Epoch 199/512
512/512 - 0s - loss: 5.5485e-06 - val_loss: 5.0586e-08
Epoch 200/512
512/512 - 0s - loss: 5.3485e-06 - val_loss: 4.9919e-08
Epoch 201/512
512/512 - 0s - loss: 5.1835e-06 - val_loss: 4.8920e-08
Epoch 202/512
512/512 - 0s - loss: 5.0274e-06 - val_loss: 4.5899e-08
Epoch 203/512
512/512 - 0s - loss: 4.7112e-06 - val_loss: 4.3058e-08
Epoch 204/512
512/512 - 0s - loss: 4.4853e-06 - val_loss: 4.2621e-08
Epoch 205/512
512/512 - 0s - loss: 4.4481e-06 - val_loss: 4.0400e-08
Epoch 206/512
512/512 - 0s - loss: 4.1312e-06 - val_loss: 3.7797e-08
Epoch 207/512
512/512 - 0s - loss: 3.9332e-06 - val_loss: 3.7377e-08
Epoch 208/512
512/512 - 0s - loss: 3.9025e-06 - val_loss: 3.5862e-08
Epoch 209/512
512/512 - 0s - loss: 3.6342e-06 - val_loss: 3.3805e-08
Epoch 210/512
512/512 - 0s - loss: 3.5003e-06 - val_loss: 3.2138e-08
Epoch 211/512
512/512 - 0s - loss: 3.3440e-06 - val_loss: 3.0918e-08
Epoch 212/512
512/512 - 0s - loss: 3.1970e-06 - val_loss: 2.9944e-08
Epoch 213/512
512/512 - 0s - loss: 3.1069e-06 - val_loss: 2.8444e-08
Epoch 214/512
512/512 - 0s - loss: 2.9422e-06 - val_loss: 2.6625e-08
Epoch 215/512
512/512 - 0s - loss: 2.7753e-06 - val_loss: 2.5598e-08
Epoch 216/512
512/512 - 0s - loss: 2.6741e-06 - val_loss: 2.5319e-08
Epoch 217/512
512/512 - 0s - loss: 2.6234e-06 - val_loss: 2.3802e-08
Epoch 218/512
512/512 - 0s - loss: 2.4279e-06 - val_loss: 2.2393e-08
Epoch 219/512
512/512 - 0s - loss: 2.3450e-06 - val_loss: 2.1546e-08
Epoch 220/512
512/512 - 0s - loss: 2.2302e-06 - val_loss: 2.1151e-08
Epoch 221/512
512/512 - 0s - loss: 2.1846e-06 - val_loss: 2.0025e-08
Epoch 222/512
512/512 - 0s - loss: 2.0414e-06 - val_loss: 1.8681e-08
Epoch 223/512
512/512 - 0s - loss: 1.9420e-06 - val_loss: 1.8240e-08
Epoch 224/512
512/512 - 0s - loss: 1.9048e-06 - val_loss: 1.7319e-08
Epoch 225/512
512/512 - 0s - loss: 1.7897e-06 - val_loss: 1.6082e-08
Epoch 226/512
512/512 - 0s - loss: 1.6835e-06 - val_loss: 1.5798e-08
Epoch 227/512
512/512 - 0s - loss: 1.6584e-06 - val_loss: 1.5113e-08
Epoch 228/512
512/512 - 0s - loss: 1.5664e-06 - val_loss: 1.4079e-08
Epoch 229/512
512/512 - 0s - loss: 1.4674e-06 - val_loss: 1.3747e-08
Epoch 230/512
512/512 - 0s - loss: 1.4329e-06 - val_loss: 1.3464e-08
Epoch 231/512
512/512 - 0s - loss: 1.3928e-06 - val_loss: 1.2326e-08
Epoch 232/512
512/512 - 0s - loss: 1.2685e-06 - val_loss: 1.1583e-08
Epoch 233/512
512/512 - 0s - loss: 1.2348e-06 - val_loss: 1.1514e-08
Epoch 234/512
512/512 - 0s - loss: 1.2021e-06 - val_loss: 1.1028e-08
Epoch 235/512
512/512 - 0s - loss: 1.1322e-06 - val_loss: 1.0194e-08
Epoch 236/512
512/512 - 0s - loss: 1.0659e-06 - val_loss: 9.7896e-09
Epoch 237/512
512/512 - 0s - loss: 1.0326e-06 - val_loss: 9.5437e-09
Epoch 238/512
512/512 - 0s - loss: 9.9251e-07 - val_loss: 8.9733e-09
Epoch 239/512
512/512 - 0s - loss: 9.3240e-07 - val_loss: 8.4777e-09
Epoch 240/512
512/512 - 0s - loss: 8.8876e-07 - val_loss: 8.2111e-09
Epoch 241/512
512/512 - 0s - loss: 8.5675e-07 - val_loss: 7.8654e-09
Epoch 242/512
512/512 - 0s - loss: 8.1830e-07 - val_loss: 7.4695e-09
Epoch 243/512
512/512 - 0s - loss: 7.7829e-07 - val_loss: 7.0110e-09
Epoch 244/512
512/512 - 0s - loss: 7.3323e-07 - val_loss: 6.7296e-09
Epoch 245/512
512/512 - 0s - loss: 7.0703e-07 - val_loss: 6.5370e-09
Epoch 246/512
512/512 - 0s - loss: 6.8116e-07 - val_loss: 6.1188e-09
Epoch 247/512
512/512 - 0s - loss: 6.3406e-07 - val_loss: 5.8471e-09
Epoch 248/512
512/512 - 0s - loss: 6.0883e-07 - val_loss: 5.7289e-09
Epoch 249/512
512/512 - 0s - loss: 5.9954e-07 - val_loss: 5.3225e-09
Epoch 250/512
512/512 - 0s - loss: 5.4663e-07 - val_loss: 4.9956e-09
Epoch 251/512
512/512 - 0s - loss: 5.3119e-07 - val_loss: 4.8110e-09
Epoch 252/512
512/512 - 0s - loss: 5.0318e-07 - val_loss: 4.6354e-09
Epoch 253/512
512/512 - 0s - loss: 4.8949e-07 - val_loss: 4.3661e-09
Epoch 254/512
512/512 - 0s - loss: 4.5050e-07 - val_loss: 4.1959e-09
Epoch 255/512
512/512 - 0s - loss: 4.4152e-07 - val_loss: 4.0407e-09
Epoch 256/512
512/512 - 0s - loss: 4.2055e-07 - val_loss: 3.7464e-09
Epoch 257/512
512/512 - 0s - loss: 3.9305e-07 - val_loss: 3.5535e-09
Epoch 258/512
512/512 - 0s - loss: 3.7269e-07 - val_loss: 3.5235e-09
Epoch 259/512
512/512 - 0s - loss: 3.7056e-07 - val_loss: 3.3493e-09
Epoch 260/512
512/512 - 0s - loss: 3.4148e-07 - val_loss: 3.0980e-09
Epoch 261/512
512/512 - 0s - loss: 3.2382e-07 - val_loss: 2.9633e-09
Epoch 262/512
512/512 - 0s - loss: 3.1472e-07 - val_loss: 2.8391e-09
Epoch 263/512
512/512 - 0s - loss: 2.9642e-07 - val_loss: 2.6810e-09
Epoch 264/512
512/512 - 0s - loss: 2.8463e-07 - val_loss: 2.4947e-09
Epoch 265/512
512/512 - 0s - loss: 2.6265e-07 - val_loss: 2.4498e-09
Epoch 266/512
512/512 - 0s - loss: 2.5860e-07 - val_loss: 2.4222e-09
Epoch 267/512
512/512 - 0s - loss: 2.5173e-07 - val_loss: 2.1749e-09
Epoch 268/512
512/512 - 0s - loss: 2.2505e-07 - val_loss: 2.0391e-09
Epoch 269/512
512/512 - 0s - loss: 2.1903e-07 - val_loss: 2.0480e-09
Epoch 270/512
512/512 - 0s - loss: 2.1491e-07 - val_loss: 1.9625e-09
Epoch 271/512
512/512 - 0s - loss: 2.0171e-07 - val_loss: 1.7964e-09
Epoch 272/512
512/512 - 0s - loss: 1.8846e-07 - val_loss: 1.7101e-09
Epoch 273/512
512/512 - 0s - loss: 1.8106e-07 - val_loss: 1.6691e-09
Epoch 274/512
512/512 - 0s - loss: 1.7559e-07 - val_loss: 1.5786e-09
Epoch 275/512
512/512 - 0s - loss: 1.6449e-07 - val_loss: 1.4527e-09
Epoch 276/512
512/512 - 0s - loss: 1.5247e-07 - val_loss: 1.4328e-09
Epoch 277/512
512/512 - 0s - loss: 1.5225e-07 - val_loss: 1.3997e-09
Epoch 278/512
512/512 - 0s - loss: 1.4419e-07 - val_loss: 1.2833e-09
Epoch 279/512
512/512 - 0s - loss: 1.3369e-07 - val_loss: 1.1890e-09
Epoch 280/512
512/512 - 0s - loss: 1.2612e-07 - val_loss: 1.1727e-09
Epoch 281/512
512/512 - 0s - loss: 1.2401e-07 - val_loss: 1.1393e-09
Epoch 282/512
512/512 - 0s - loss: 1.1880e-07 - val_loss: 1.0285e-09
Epoch 283/512
512/512 - 0s - loss: 1.0751e-07 - val_loss: 9.8506e-10
Epoch 284/512
512/512 - 0s - loss: 1.0544e-07 - val_loss: 9.7910e-10
Epoch 285/512
512/512 - 0s - loss: 1.0183e-07 - val_loss: 9.2828e-10
Epoch 286/512
512/512 - 0s - loss: 9.6259e-08 - val_loss: 8.6047e-10
Epoch 287/512
512/512 - 0s - loss: 8.9407e-08 - val_loss: 8.2078e-10
Epoch 288/512
512/512 - 0s - loss: 8.7080e-08 - val_loss: 7.8475e-10
Epoch 289/512
512/512 - 0s - loss: 8.2821e-08 - val_loss: 7.3653e-10
Epoch 290/512
512/512 - 0s - loss: 7.8157e-08 - val_loss: 6.9713e-10
Epoch 291/512
512/512 - 0s - loss: 7.3710e-08 - val_loss: 6.7442e-10
Epoch 292/512
512/512 - 0s - loss: 7.1593e-08 - val_loss: 6.4805e-10
Epoch 293/512
512/512 - 0s - loss: 6.8026e-08 - val_loss: 6.0036e-10
Epoch 294/512
512/512 - 0s - loss: 6.3707e-08 - val_loss: 5.7253e-10
Epoch 295/512
512/512 - 0s - loss: 6.1065e-08 - val_loss: 5.5499e-10
Epoch 296/512
512/512 - 0s - loss: 5.8478e-08 - val_loss: 5.2658e-10
Epoch 297/512
512/512 - 0s - loss: 5.5567e-08 - val_loss: 4.9882e-10
Epoch 298/512
512/512 - 0s - loss: 5.2372e-08 - val_loss: 4.7823e-10
Epoch 299/512
512/512 - 0s - loss: 5.0159e-08 - val_loss: 4.6497e-10
Epoch 300/512
512/512 - 0s - loss: 4.8444e-08 - val_loss: 4.3902e-10
Epoch 301/512
512/512 - 0s - loss: 4.5515e-08 - val_loss: 4.0949e-10
Epoch 302/512
512/512 - 0s - loss: 4.2949e-08 - val_loss: 3.9144e-10
Epoch 303/512
512/512 - 0s - loss: 4.1154e-08 - val_loss: 3.7981e-10
Epoch 304/512
512/512 - 0s - loss: 3.9806e-08 - val_loss: 3.5595e-10
Epoch 305/512
512/512 - 0s - loss: 3.7017e-08 - val_loss: 3.4115e-10
Epoch 306/512
512/512 - 0s - loss: 3.5776e-08 - val_loss: 3.2838e-10
Epoch 307/512
512/512 - 0s - loss: 3.4424e-08 - val_loss: 3.0613e-10
Epoch 308/512
512/512 - 0s - loss: 3.2137e-08 - val_loss: 2.8535e-10
Epoch 309/512
512/512 - 0s - loss: 3.0090e-08 - val_loss: 2.8246e-10
Epoch 310/512
512/512 - 0s - loss: 3.0106e-08 - val_loss: 2.7356e-10
Epoch 311/512
512/512 - 0s - loss: 2.8616e-08 - val_loss: 2.4575e-10
Epoch 312/512
512/512 - 0s - loss: 2.5794e-08 - val_loss: 2.3337e-10
Epoch 313/512
512/512 - 0s - loss: 2.5037e-08 - val_loss: 2.3559e-10
Epoch 314/512
512/512 - 0s - loss: 2.5168e-08 - val_loss: 2.2093e-10
Epoch 315/512
512/512 - 0s - loss: 2.2961e-08 - val_loss: 2.0351e-10
Epoch 316/512
512/512 - 0s - loss: 2.1376e-08 - val_loss: 1.9956e-10
Epoch 317/512
512/512 - 0s - loss: 2.1350e-08 - val_loss: 1.9589e-10
Epoch 318/512
512/512 - 0s - loss: 2.0669e-08 - val_loss: 1.7955e-10
Epoch 319/512
512/512 - 0s - loss: 1.8550e-08 - val_loss: 1.6914e-10
Epoch 320/512
512/512 - 0s - loss: 1.7932e-08 - val_loss: 1.6876e-10
Epoch 321/512
512/512 - 0s - loss: 1.7844e-08 - val_loss: 1.6452e-10
Epoch 322/512
512/512 - 0s - loss: 1.6899e-08 - val_loss: 1.5302e-10
Epoch 323/512
512/512 - 0s - loss: 1.5818e-08 - val_loss: 1.4238e-10
Epoch 324/512
512/512 - 0s - loss: 1.4941e-08 - val_loss: 1.3741e-10
Epoch 325/512
512/512 - 0s - loss: 1.4578e-08 - val_loss: 1.3381e-10
Epoch 326/512
512/512 - 0s - loss: 1.4106e-08 - val_loss: 1.2538e-10
Epoch 327/512
512/512 - 0s - loss: 1.3000e-08 - val_loss: 1.2018e-10
Epoch 328/512
512/512 - 0s - loss: 1.2637e-08 - val_loss: 1.1722e-10
Epoch 329/512
512/512 - 0s - loss: 1.2344e-08 - val_loss: 1.1146e-10
Epoch 330/512
512/512 - 0s - loss: 1.1617e-08 - val_loss: 1.0398e-10
Epoch 331/512
512/512 - 0s - loss: 1.0856e-08 - val_loss: 1.0030e-10
Epoch 332/512
512/512 - 0s - loss: 1.0605e-08 - val_loss: 9.8425e-11
Epoch 333/512
512/512 - 0s - loss: 1.0327e-08 - val_loss: 9.4085e-11
Epoch 334/512
512/512 - 0s - loss: 9.8028e-09 - val_loss: 8.8572e-11
Epoch 335/512
512/512 - 0s - loss: 9.2887e-09 - val_loss: 8.3281e-11
Epoch 336/512
512/512 - 0s - loss: 8.6773e-09 - val_loss: 8.1369e-11
Epoch 337/512
512/512 - 0s - loss: 8.6436e-09 - val_loss: 7.9614e-11
Epoch 338/512
512/512 - 0s - loss: 8.2430e-09 - val_loss: 7.6662e-11
Epoch 339/512
512/512 - 0s - loss: 7.9813e-09 - val_loss: 7.2820e-11
Epoch 340/512
512/512 - 0s - loss: 7.5819e-09 - val_loss: 6.7535e-11
Epoch 341/512
512/512 - 0s - loss: 7.0421e-09 - val_loss: 6.5376e-11
Epoch 342/512
512/512 - 0s - loss: 6.8967e-09 - val_loss: 6.4920e-11
Epoch 343/512
512/512 - 0s - loss: 6.7821e-09 - val_loss: 6.2210e-11
Epoch 344/512
512/512 - 0s - loss: 6.4006e-09 - val_loss: 5.9345e-11
Epoch 345/512
512/512 - 0s - loss: 6.1694e-09 - val_loss: 5.6388e-11
Epoch 346/512
512/512 - 0s - loss: 5.8913e-09 - val_loss: 5.4183e-11
Epoch 347/512
512/512 - 0s - loss: 5.6428e-09 - val_loss: 5.3169e-11
Epoch 348/512
512/512 - 0s - loss: 5.5739e-09 - val_loss: 5.0775e-11
Epoch 349/512
512/512 - 0s - loss: 5.2849e-09 - val_loss: 4.8134e-11
Epoch 350/512
512/512 - 0s - loss: 4.9928e-09 - val_loss: 4.6730e-11
Epoch 351/512
512/512 - 0s - loss: 4.8438e-09 - val_loss: 4.5832e-11
Epoch 352/512
512/512 - 0s - loss: 4.8086e-09 - val_loss: 4.4487e-11
Epoch 353/512
512/512 - 0s - loss: 4.5745e-09 - val_loss: 4.2251e-11
Epoch 354/512
512/512 - 0s - loss: 4.3430e-09 - val_loss: 4.0231e-11
Epoch 355/512
512/512 - 0s - loss: 4.1598e-09 - val_loss: 3.9004e-11
Epoch 356/512
512/512 - 0s - loss: 4.0868e-09 - val_loss: 3.8014e-11
Epoch 357/512
512/512 - 0s - loss: 3.9450e-09 - val_loss: 3.6805e-11
Epoch 358/512
512/512 - 0s - loss: 3.8031e-09 - val_loss: 3.5541e-11
Epoch 359/512
512/512 - 0s - loss: 3.6629e-09 - val_loss: 3.3809e-11
Epoch 360/512
512/512 - 0s - loss: 3.4795e-09 - val_loss: 3.2194e-11
Epoch 361/512
512/512 - 0s - loss: 3.3376e-09 - val_loss: 3.1293e-11
Epoch 362/512
512/512 - 0s - loss: 3.2477e-09 - val_loss: 3.0782e-11
Epoch 363/512
512/512 - 0s - loss: 3.1801e-09 - val_loss: 3.0352e-11
Epoch 364/512
512/512 - 0s - loss: 3.1330e-09 - val_loss: 2.8896e-11
Epoch 365/512
512/512 - 0s - loss: 2.9750e-09 - val_loss: 2.7594e-11
Epoch 366/512
512/512 - 0s - loss: 2.8687e-09 - val_loss: 2.6567e-11
Epoch 367/512
512/512 - 0s - loss: 2.7381e-09 - val_loss: 2.5931e-11
Epoch 368/512
512/512 - 0s - loss: 2.7043e-09 - val_loss: 2.5518e-11
Epoch 369/512
512/512 - 0s - loss: 2.6239e-09 - val_loss: 2.4701e-11
Epoch 370/512
512/512 - 0s - loss: 2.5384e-09 - val_loss: 2.3654e-11
Epoch 371/512
512/512 - 0s - loss: 2.4666e-09 - val_loss: 2.2611e-11
Epoch 372/512
512/512 - 0s - loss: 2.3132e-09 - val_loss: 2.1930e-11
Epoch 373/512
512/512 - 0s - loss: 2.2393e-09 - val_loss: 2.1649e-11
Epoch 374/512
512/512 - 0s - loss: 2.2494e-09 - val_loss: 2.1270e-11
Epoch 375/512
512/512 - 0s - loss: 2.1959e-09 - val_loss: 2.0454e-11
Epoch 376/512
512/512 - 0s - loss: 2.0955e-09 - val_loss: 1.9418e-11
Epoch 377/512
512/512 - 0s - loss: 2.0151e-09 - val_loss: 1.9001e-11
Epoch 378/512
512/512 - 0s - loss: 1.9475e-09 - val_loss: 1.8733e-11
Epoch 379/512
512/512 - 0s - loss: 1.9515e-09 - val_loss: 1.8230e-11
Epoch 380/512
512/512 - 0s - loss: 1.8734e-09 - val_loss: 1.7607e-11
Epoch 381/512
512/512 - 0s - loss: 1.8243e-09 - val_loss: 1.7143e-11
Epoch 382/512
512/512 - 0s - loss: 1.7678e-09 - val_loss: 1.6543e-11
Epoch 383/512
512/512 - 0s - loss: 1.7021e-09 - val_loss: 1.5954e-11
Epoch 384/512
512/512 - 0s - loss: 1.6710e-09 - val_loss: 1.5350e-11
Epoch 385/512
512/512 - 0s - loss: 1.5876e-09 - val_loss: 1.5237e-11
Epoch 386/512
512/512 - 0s - loss: 1.5817e-09 - val_loss: 1.5173e-11
Epoch 387/512
512/512 - 0s - loss: 1.5619e-09 - val_loss: 1.4738e-11
Epoch 388/512
512/512 - 0s - loss: 1.5150e-09 - val_loss: 1.4284e-11
Epoch 389/512
512/512 - 0s - loss: 1.4665e-09 - val_loss: 1.3758e-11
Epoch 390/512
512/512 - 0s - loss: 1.4163e-09 - val_loss: 1.3395e-11
Epoch 391/512
512/512 - 0s - loss: 1.3741e-09 - val_loss: 1.2967e-11
Epoch 392/512
512/512 - 0s - loss: 1.3305e-09 - val_loss: 1.2778e-11
Epoch 393/512
512/512 - 0s - loss: 1.3159e-09 - val_loss: 1.2714e-11
Epoch 394/512
512/512 - 0s - loss: 1.3154e-09 - val_loss: 1.2505e-11
Epoch 395/512
512/512 - 0s - loss: 1.2685e-09 - val_loss: 1.1814e-11
Epoch 396/512
512/512 - 0s - loss: 1.2168e-09 - val_loss: 1.1356e-11
Epoch 397/512
512/512 - 0s - loss: 1.1638e-09 - val_loss: 1.1206e-11
Epoch 398/512
512/512 - 0s - loss: 1.1703e-09 - val_loss: 1.1183e-11
Epoch 399/512
512/512 - 0s - loss: 1.1516e-09 - val_loss: 1.0836e-11
Epoch 400/512
512/512 - 0s - loss: 1.1082e-09 - val_loss: 1.0501e-11
Epoch 401/512
512/512 - 0s - loss: 1.0740e-09 - val_loss: 1.0244e-11
Epoch 402/512
512/512 - 0s - loss: 1.0426e-09 - val_loss: 9.8358e-12
Epoch 403/512
512/512 - 0s - loss: 1.0037e-09 - val_loss: 9.6512e-12
Epoch 404/512
512/512 - 0s - loss: 9.9458e-10 - val_loss: 9.6374e-12
Epoch 405/512
512/512 - 0s - loss: 9.9925e-10 - val_loss: 9.4245e-12
Epoch 406/512
512/512 - 0s - loss: 9.6583e-10 - val_loss: 9.0304e-12
Epoch 407/512
512/512 - 0s - loss: 9.1863e-10 - val_loss: 8.8583e-12
Epoch 408/512
512/512 - 0s - loss: 8.9929e-10 - val_loss: 8.5654e-12
Epoch 409/512
512/512 - 0s - loss: 8.8288e-10 - val_loss: 8.4464e-12
Epoch 410/512
512/512 - 0s - loss: 8.7126e-10 - val_loss: 8.4137e-12
Epoch 411/512
512/512 - 0s - loss: 8.6468e-10 - val_loss: 8.4206e-12
Epoch 412/512
512/512 - 0s - loss: 8.6002e-10 - val_loss: 8.3130e-12
Epoch 413/512
512/512 - 0s - loss: 8.4500e-10 - val_loss: 7.8641e-12
Epoch 414/512
512/512 - 0s - loss: 8.0329e-10 - val_loss: 7.5449e-12
Epoch 415/512
512/512 - 0s - loss: 7.7253e-10 - val_loss: 7.4208e-12
Epoch 416/512
512/512 - 0s - loss: 7.6501e-10 - val_loss: 7.3259e-12
Epoch 417/512
512/512 - 0s - loss: 7.4557e-10 - val_loss: 7.4550e-12
Epoch 418/512
512/512 - 0s - loss: 7.6187e-10 - val_loss: 7.3798e-12
Epoch 419/512
512/512 - 0s - loss: 7.5649e-10 - val_loss: 7.0579e-12
Epoch 420/512
512/512 - 0s - loss: 7.1033e-10 - val_loss: 6.6499e-12
Epoch 421/512
512/512 - 0s - loss: 6.7441e-10 - val_loss: 6.4538e-12
Epoch 422/512
512/512 - 0s - loss: 6.5573e-10 - val_loss: 6.5139e-12
Epoch 423/512
512/512 - 0s - loss: 6.6606e-10 - val_loss: 6.5895e-12
Epoch 424/512
512/512 - 0s - loss: 6.8188e-10 - val_loss: 6.5649e-12
Epoch 425/512
512/512 - 0s - loss: 6.6326e-10 - val_loss: 6.3008e-12
Epoch 426/512
512/512 - 0s - loss: 6.3939e-10 - val_loss: 6.1616e-12
Epoch 427/512
512/512 - 0s - loss: 6.2603e-10 - val_loss: 5.8754e-12
Epoch 428/512
512/512 - 0s - loss: 5.9526e-10 - val_loss: 5.8113e-12
Epoch 429/512
512/512 - 0s - loss: 5.9798e-10 - val_loss: 5.5669e-12
Epoch 430/512
512/512 - 0s - loss: 5.6968e-10 - val_loss: 5.3997e-12
Epoch 431/512
512/512 - 0s - loss: 5.5057e-10 - val_loss: 5.2914e-12
Epoch 432/512
512/512 - 0s - loss: 5.4587e-10 - val_loss: 5.4086e-12
Epoch 433/512
512/512 - 0s - loss: 5.6463e-10 - val_loss: 5.4813e-12
Epoch 434/512
512/512 - 0s - loss: 5.6513e-10 - val_loss: 5.4095e-12
Epoch 435/512
512/512 - 0s - loss: 5.4039e-10 - val_loss: 5.2867e-12
Epoch 436/512
512/512 - 0s - loss: 5.3949e-10 - val_loss: 5.2195e-12
Epoch 437/512
512/512 - 0s - loss: 5.2702e-10 - val_loss: 4.9936e-12
Epoch 438/512
512/512 - 0s - loss: 4.9641e-10 - val_loss: 4.8108e-12
Epoch 439/512
512/512 - 0s - loss: 4.8636e-10 - val_loss: 4.7802e-12
Epoch 440/512
512/512 - 0s - loss: 4.9195e-10 - val_loss: 4.7229e-12
Epoch 441/512
512/512 - 0s - loss: 4.8281e-10 - val_loss: 4.6420e-12
Epoch 442/512
512/512 - 0s - loss: 4.7165e-10 - val_loss: 4.5224e-12
Epoch 443/512
512/512 - 0s - loss: 4.6248e-10 - val_loss: 4.5054e-12
Epoch 444/512
512/512 - 0s - loss: 4.6163e-10 - val_loss: 4.5223e-12
Epoch 445/512
512/512 - 0s - loss: 4.6707e-10 - val_loss: 4.4109e-12
Epoch 446/512
512/512 - 0s - loss: 4.4935e-10 - val_loss: 4.2687e-12
Epoch 447/512
512/512 - 0s - loss: 4.3242e-10 - val_loss: 4.2209e-12
Epoch 448/512
512/512 - 0s - loss: 4.2679e-10 - val_loss: 4.1311e-12
Epoch 449/512
512/512 - 0s - loss: 4.2033e-10 - val_loss: 4.0687e-12
Epoch 450/512
512/512 - 0s - loss: 4.1919e-10 - val_loss: 4.0033e-12
Epoch 451/512
512/512 - 0s - loss: 4.1354e-10 - val_loss: 4.0027e-12
Epoch 452/512
512/512 - 0s - loss: 4.0410e-10 - val_loss: 3.9171e-12
Epoch 453/512
512/512 - 0s - loss: 3.9413e-10 - val_loss: 3.8327e-12
Epoch 454/512
512/512 - 0s - loss: 3.9215e-10 - val_loss: 3.8307e-12
Epoch 455/512
512/512 - 0s - loss: 3.8486e-10 - val_loss: 3.7401e-12
Epoch 456/512
512/512 - 0s - loss: 3.8492e-10 - val_loss: 3.6752e-12
Epoch 457/512
512/512 - 0s - loss: 3.7636e-10 - val_loss: 3.5546e-12
Epoch 458/512
512/512 - 0s - loss: 3.6110e-10 - val_loss: 3.4501e-12
Epoch 459/512
512/512 - 0s - loss: 3.5670e-10 - val_loss: 3.4752e-12
Epoch 460/512
512/512 - 0s - loss: 3.5885e-10 - val_loss: 3.5240e-12
Epoch 461/512
512/512 - 0s - loss: 3.5427e-10 - val_loss: 3.4720e-12
Epoch 462/512
512/512 - 0s - loss: 3.5528e-10 - val_loss: 3.4720e-12
Epoch 463/512
512/512 - 0s - loss: 3.4712e-10 - val_loss: 3.3952e-12
Epoch 464/512
512/512 - 0s - loss: 3.4494e-10 - val_loss: 3.2354e-12
Epoch 465/512
512/512 - 0s - loss: 3.2454e-10 - val_loss: 3.1090e-12
Epoch 466/512
512/512 - 0s - loss: 3.2189e-10 - val_loss: 3.0986e-12
Epoch 467/512
512/512 - 0s - loss: 3.1665e-10 - val_loss: 3.0844e-12
Epoch 468/512
512/512 - 0s - loss: 3.1401e-10 - val_loss: 3.0610e-12
Epoch 469/512
512/512 - 0s - loss: 3.1370e-10 - val_loss: 3.1234e-12
Epoch 470/512
512/512 - 0s - loss: 3.1901e-10 - val_loss: 3.1471e-12
Epoch 471/512
512/512 - 0s - loss: 3.2281e-10 - val_loss: 3.0455e-12
Epoch 472/512
512/512 - 0s - loss: 3.0600e-10 - val_loss: 2.8971e-12
Epoch 473/512
512/512 - 0s - loss: 2.9288e-10 - val_loss: 2.8865e-12
Epoch 474/512
512/512 - 0s - loss: 2.8928e-10 - val_loss: 2.8100e-12
Epoch 475/512
512/512 - 0s - loss: 2.9228e-10 - val_loss: 2.8303e-12
Epoch 476/512
512/512 - 0s - loss: 2.8472e-10 - val_loss: 2.7604e-12
Epoch 477/512
512/512 - 0s - loss: 2.8259e-10 - val_loss: 2.7404e-12
Epoch 478/512
512/512 - 0s - loss: 2.7922e-10 - val_loss: 2.7163e-12
Epoch 479/512
512/512 - 0s - loss: 2.7495e-10 - val_loss: 2.7012e-12
Epoch 480/512
512/512 - 0s - loss: 2.7430e-10 - val_loss: 2.6273e-12
Epoch 481/512
512/512 - 0s - loss: 2.6468e-10 - val_loss: 2.5932e-12
Epoch 482/512
512/512 - 0s - loss: 2.6287e-10 - val_loss: 2.5410e-12
Epoch 483/512
512/512 - 0s - loss: 2.5195e-10 - val_loss: 2.5202e-12
Epoch 484/512
512/512 - 0s - loss: 2.5574e-10 - val_loss: 2.5110e-12
Epoch 485/512
512/512 - 0s - loss: 2.6078e-10 - val_loss: 2.5229e-12
Epoch 486/512
512/512 - 0s - loss: 2.5159e-10 - val_loss: 2.4210e-12
Epoch 487/512
512/512 - 0s - loss: 2.4321e-10 - val_loss: 2.3423e-12
Epoch 488/512
512/512 - 0s - loss: 2.3811e-10 - val_loss: 2.2813e-12
Epoch 489/512
512/512 - 0s - loss: 2.2756e-10 - val_loss: 2.1997e-12
Epoch 490/512
512/512 - 0s - loss: 2.2326e-10 - val_loss: 2.2094e-12
Epoch 491/512
512/512 - 0s - loss: 2.2974e-10 - val_loss: 2.2629e-12
Epoch 492/512
512/512 - 0s - loss: 2.3113e-10 - val_loss: 2.1670e-12
Epoch 493/512
512/512 - 0s - loss: 2.2188e-10 - val_loss: 2.1716e-12
Epoch 494/512
512/512 - 0s - loss: 2.1999e-10 - val_loss: 2.0806e-12
Epoch 495/512
512/512 - 0s - loss: 2.0816e-10 - val_loss: 2.0068e-12
Epoch 496/512
512/512 - 0s - loss: 2.0230e-10 - val_loss: 1.9534e-12
Epoch 497/512
512/512 - 0s - loss: 1.9842e-10 - val_loss: 1.9536e-12
Epoch 498/512
512/512 - 0s - loss: 2.0347e-10 - val_loss: 2.0853e-12
Epoch 499/512
512/512 - 0s - loss: 2.1464e-10 - val_loss: 2.0753e-12
Epoch 500/512
512/512 - 0s - loss: 2.0881e-10 - val_loss: 2.0527e-12
Epoch 501/512
512/512 - 0s - loss: 2.0951e-10 - val_loss: 1.9926e-12
Epoch 502/512
512/512 - 0s - loss: 2.0290e-10 - val_loss: 1.9267e-12
Epoch 503/512
512/512 - 0s - loss: 1.9175e-10 - val_loss: 1.8523e-12
Epoch 504/512
512/512 - 0s - loss: 1.8659e-10 - val_loss: 1.8320e-12
Epoch 505/512
512/512 - 0s - loss: 1.8759e-10 - val_loss: 1.8329e-12
Epoch 506/512
512/512 - 0s - loss: 1.8589e-10 - val_loss: 1.8990e-12
Epoch 507/512
512/512 - 0s - loss: 1.9635e-10 - val_loss: 1.9573e-12
Epoch 508/512
512/512 - 0s - loss: 2.0274e-10 - val_loss: 1.9968e-12
Epoch 509/512
512/512 - 0s - loss: 1.9843e-10 - val_loss: 1.8852e-12
Epoch 510/512
512/512 - 0s - loss: 1.8787e-10 - val_loss: 1.8360e-12
Epoch 511/512
512/512 - 0s - loss: 1.8685e-10 - val_loss: 1.8175e-12
Epoch 512/512
512/512 - 0s - loss: 1.8475e-10 - val_loss: 1.7475e-12
2024-04-07 19:46:40.897029: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1151e-10 - val_loss: 5.5127e-10
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1719e-10 - val_loss: 8.4452e-10
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2214e-10 - val_loss: 6.3105e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3984e-10 - val_loss: 3.8168e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3903e-10 - val_loss: 2.7588e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6658e-10 - val_loss: 2.5218e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6782e-10 - val_loss: 2.8242e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0573e-10 - val_loss: 3.4184e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7305e-10 - val_loss: 3.9220e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1478e-10 - val_loss: 4.2294e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2984e-10 - val_loss: 4.0101e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9404e-10 - val_loss: 3.5557e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5038e-10 - val_loss: 3.2208e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2111e-10 - val_loss: 3.0271e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1649e-10 - val_loss: 3.1411e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2343e-10 - val_loss: 3.2358e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3159e-10 - val_loss: 3.2877e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4116e-10 - val_loss: 3.3295e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3907e-10 - val_loss: 3.2901e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3356e-10 - val_loss: 3.2148e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2653e-10 - val_loss: 3.1151e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1742e-10 - val_loss: 3.0716e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0777e-10 - val_loss: 2.9310e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9696e-10 - val_loss: 2.8373e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8773e-10 - val_loss: 2.7554e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8367e-10 - val_loss: 2.7536e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8004e-10 - val_loss: 2.7605e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8112e-10 - val_loss: 2.8068e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8200e-10 - val_loss: 2.6629e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7059e-10 - val_loss: 2.6472e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6509e-10 - val_loss: 2.5323e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5983e-10 - val_loss: 2.5415e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6439e-10 - val_loss: 2.6603e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7437e-10 - val_loss: 2.6838e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7183e-10 - val_loss: 2.6307e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6088e-10 - val_loss: 2.4354e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4072e-10 - val_loss: 2.2475e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3064e-10 - val_loss: 2.3255e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3827e-10 - val_loss: 2.3415e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4060e-10 - val_loss: 2.4217e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5015e-10 - val_loss: 2.4639e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5009e-10 - val_loss: 2.3371e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3349e-10 - val_loss: 2.2456e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2593e-10 - val_loss: 2.1801e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2186e-10 - val_loss: 2.1573e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2078e-10 - val_loss: 2.1541e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1703e-10 - val_loss: 2.1296e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1798e-10 - val_loss: 2.1528e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1842e-10 - val_loss: 2.1232e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1674e-10 - val_loss: 2.1609e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2059e-10 - val_loss: 2.1053e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1439e-10 - val_loss: 2.1099e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1448e-10 - val_loss: 2.0935e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1085e-10 - val_loss: 2.0361e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0385e-10 - val_loss: 1.9671e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9901e-10 - val_loss: 1.9141e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9372e-10 - val_loss: 1.8970e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9179e-10 - val_loss: 1.8663e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9202e-10 - val_loss: 1.9010e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9628e-10 - val_loss: 1.9793e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0028e-10 - val_loss: 1.8860e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9114e-10 - val_loss: 1.8478e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8619e-10 - val_loss: 1.7625e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7748e-10 - val_loss: 1.7200e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7442e-10 - val_loss: 1.7213e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7480e-10 - val_loss: 1.7127e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7372e-10 - val_loss: 1.7366e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7622e-10 - val_loss: 1.7488e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7749e-10 - val_loss: 1.7566e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7708e-10 - val_loss: 1.7247e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7365e-10 - val_loss: 1.7242e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7506e-10 - val_loss: 1.7050e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7242e-10 - val_loss: 1.6966e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7270e-10 - val_loss: 1.6911e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7316e-10 - val_loss: 1.7140e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6935e-10 - val_loss: 1.5829e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5488e-10 - val_loss: 1.4850e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5036e-10 - val_loss: 1.4618e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4580e-10 - val_loss: 1.4095e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4148e-10 - val_loss: 1.3368e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3643e-10 - val_loss: 1.4080e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4248e-10 - val_loss: 1.4410e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5079e-10 - val_loss: 1.5240e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5688e-10 - val_loss: 1.5393e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5651e-10 - val_loss: 1.5418e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5431e-10 - val_loss: 1.4879e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5005e-10 - val_loss: 1.4617e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4556e-10 - val_loss: 1.3566e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3511e-10 - val_loss: 1.3004e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3196e-10 - val_loss: 1.2739e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2762e-10 - val_loss: 1.2777e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2972e-10 - val_loss: 1.2698e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3108e-10 - val_loss: 1.3152e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3206e-10 - val_loss: 1.2749e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3151e-10 - val_loss: 1.3084e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3195e-10 - val_loss: 1.2698e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2955e-10 - val_loss: 1.2600e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2864e-10 - val_loss: 1.2649e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3080e-10 - val_loss: 1.3062e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3050e-10 - val_loss: 1.2664e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2925e-10 - val_loss: 1.2653e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2551e-10 - val_loss: 1.1918e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2029e-10 - val_loss: 1.2182e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2294e-10 - val_loss: 1.1807e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2147e-10 - val_loss: 1.1975e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2023e-10 - val_loss: 1.1203e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1160e-10 - val_loss: 1.1525e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1681e-10 - val_loss: 1.1626e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1793e-10 - val_loss: 1.1300e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1368e-10 - val_loss: 1.1242e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1466e-10 - val_loss: 1.1779e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2086e-10 - val_loss: 1.2352e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2139e-10 - val_loss: 1.1582e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1491e-10 - val_loss: 1.0633e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0267e-10 - val_loss: 9.6264e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0085e-10 - val_loss: 9.9927e-11
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0225e-10 - val_loss: 9.9429e-11
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0180e-10 - val_loss: 1.0276e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0956e-10 - val_loss: 1.1620e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1557e-10 - val_loss: 1.1329e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1566e-10 - val_loss: 1.1151e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0951e-10 - val_loss: 1.0514e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0839e-10 - val_loss: 1.0463e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0258e-10 - val_loss: 9.7267e-11
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8265e-11 - val_loss: 9.5860e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7648e-11 - val_loss: 9.6206e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6406e-11 - val_loss: 9.6147e-11
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5592e-11 - val_loss: 9.5187e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7105e-11 - val_loss: 9.7069e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7539e-11 - val_loss: 9.5472e-11
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6108e-11 - val_loss: 9.3381e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7029e-11 - val_loss: 1.0120e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0052e-10 - val_loss: 9.4266e-11
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5682e-11 - val_loss: 8.9677e-11
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8042e-11 - val_loss: 8.5239e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6878e-11 - val_loss: 8.5444e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6913e-11 - val_loss: 8.5282e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7012e-11 - val_loss: 8.5916e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7017e-11 - val_loss: 8.4398e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8009e-11 - val_loss: 9.0613e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0323e-11 - val_loss: 8.8366e-11
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7069e-11 - val_loss: 8.1264e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3304e-11 - val_loss: 8.2046e-11
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3691e-11 - val_loss: 8.0917e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2473e-11 - val_loss: 8.1189e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2926e-11 - val_loss: 8.1242e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3166e-11 - val_loss: 8.3472e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5648e-11 - val_loss: 8.5075e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6147e-11 - val_loss: 8.6112e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6526e-11 - val_loss: 8.3934e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4174e-11 - val_loss: 8.3616e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2997e-11 - val_loss: 8.2141e-11
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1083e-11 - val_loss: 7.4228e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6983e-11 - val_loss: 8.1629e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2347e-11 - val_loss: 8.3941e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8729e-11 - val_loss: 9.0554e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2154e-11 - val_loss: 9.1000e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9628e-11 - val_loss: 8.4191e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1990e-11 - val_loss: 7.7062e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4612e-11 - val_loss: 7.0530e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0929e-11 - val_loss: 6.7128e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8601e-11 - val_loss: 6.8140e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9726e-11 - val_loss: 7.0660e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1554e-11 - val_loss: 7.0689e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2454e-11 - val_loss: 7.2559e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4537e-11 - val_loss: 7.4748e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4524e-11 - val_loss: 7.2142e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3011e-11 - val_loss: 7.3076e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1685e-11 - val_loss: 7.0776e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0126e-11 - val_loss: 7.0043e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1401e-11 - val_loss: 6.8001e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9880e-11 - val_loss: 6.8320e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9544e-11 - val_loss: 7.0241e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0737e-11 - val_loss: 6.9268e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9301e-11 - val_loss: 7.0314e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2057e-11 - val_loss: 6.8557e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9337e-11 - val_loss: 6.8538e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8991e-11 - val_loss: 6.7581e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7896e-11 - val_loss: 6.8707e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8899e-11 - val_loss: 6.6401e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5674e-11 - val_loss: 6.5320e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6450e-11 - val_loss: 6.1174e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9898e-11 - val_loss: 5.8731e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0758e-11 - val_loss: 5.7932e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9177e-11 - val_loss: 6.1962e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4482e-11 - val_loss: 6.4328e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6850e-11 - val_loss: 6.7959e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8643e-11 - val_loss: 6.6619e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6401e-11 - val_loss: 6.2879e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2033e-11 - val_loss: 6.1075e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1137e-11 - val_loss: 5.8093e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9554e-11 - val_loss: 6.0891e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1031e-11 - val_loss: 5.9233e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1462e-11 - val_loss: 6.0530e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1272e-11 - val_loss: 6.1094e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8651e-11 - val_loss: 5.3215e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4267e-11 - val_loss: 5.4920e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8867e-11 - val_loss: 6.4296e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7231e-11 - val_loss: 6.6999e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6761e-11 - val_loss: 6.5002e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3268e-11 - val_loss: 5.9233e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0239e-11 - val_loss: 6.1143e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1773e-11 - val_loss: 5.8823e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7301e-11 - val_loss: 5.4959e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3019e-11 - val_loss: 4.8939e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7774e-11 - val_loss: 4.5854e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5203e-11 - val_loss: 4.5064e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6402e-11 - val_loss: 4.5873e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6397e-11 - val_loss: 4.7653e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9996e-11 - val_loss: 5.3191e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5365e-11 - val_loss: 5.8001e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1568e-11 - val_loss: 6.3355e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4005e-11 - val_loss: 5.9800e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0689e-11 - val_loss: 5.8210e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9884e-11 - val_loss: 6.0849e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8566e-11 - val_loss: 5.5059e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2810e-11 - val_loss: 4.6990e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6223e-11 - val_loss: 4.2443e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4182e-11 - val_loss: 4.6545e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8244e-11 - val_loss: 5.0651e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1075e-11 - val_loss: 5.0299e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1482e-11 - val_loss: 5.0971e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9237e-11 - val_loss: 4.8367e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0084e-11 - val_loss: 5.0611e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9431e-11 - val_loss: 4.8822e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9919e-11 - val_loss: 4.6953e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5704e-11 - val_loss: 4.0780e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1199e-11 - val_loss: 3.8907e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9357e-11 - val_loss: 4.0790e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2311e-11 - val_loss: 4.4132e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5455e-11 - val_loss: 4.4092e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3018e-11 - val_loss: 4.3015e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5623e-11 - val_loss: 4.8796e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1182e-11 - val_loss: 5.2683e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2397e-11 - val_loss: 5.0026e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8356e-11 - val_loss: 4.5017e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3925e-11 - val_loss: 4.1960e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2958e-11 - val_loss: 4.5247e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7970e-11 - val_loss: 5.0992e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9403e-11 - val_loss: 4.6361e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4323e-11 - val_loss: 4.1351e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9838e-11 - val_loss: 3.9343e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0216e-11 - val_loss: 4.0402e-11
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9741e-11 - val_loss: 3.8907e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0417e-11 - val_loss: 4.1784e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3869e-11 - val_loss: 4.5812e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6091e-11 - val_loss: 4.4016e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5302e-11 - val_loss: 4.5870e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5034e-11 - val_loss: 4.1516e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2661e-11 - val_loss: 4.3805e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3290e-11 - val_loss: 4.2198e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3194e-11 - val_loss: 4.3180e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2215e-11 - val_loss: 3.9674e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0958e-11 - val_loss: 3.8700e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8438e-11 - val_loss: 3.9136e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0844e-11 - val_loss: 4.3514e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4717e-11 - val_loss: 4.2095e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0669e-11 - val_loss: 3.7361e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8187e-11 - val_loss: 3.8901e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0673e-11 - val_loss: 4.1431e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9997e-11 - val_loss: 3.7876e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8956e-11 - val_loss: 4.1258e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3999e-11 - val_loss: 4.5491e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4122e-11 - val_loss: 4.3127e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4846e-11 - val_loss: 4.6597e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5981e-11 - val_loss: 4.2564e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3558e-11 - val_loss: 4.4324e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3638e-11 - val_loss: 4.1920e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0747e-11 - val_loss: 3.8120e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7163e-11 - val_loss: 3.4120e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3738e-11 - val_loss: 3.4603e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6281e-11 - val_loss: 3.6767e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6091e-11 - val_loss: 3.6203e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7239e-11 - val_loss: 3.6503e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8279e-11 - val_loss: 4.0391e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1321e-11 - val_loss: 3.9436e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9731e-11 - val_loss: 3.9376e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9875e-11 - val_loss: 3.8488e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7954e-11 - val_loss: 3.5501e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3811e-11 - val_loss: 3.1930e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1055e-11 - val_loss: 2.9677e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8972e-11 - val_loss: 2.8624e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0023e-11 - val_loss: 3.1124e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1557e-11 - val_loss: 3.0404e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1508e-11 - val_loss: 3.2117e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3204e-11 - val_loss: 3.3800e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5286e-11 - val_loss: 3.6361e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4417e-11 - val_loss: 3.2985e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4337e-11 - val_loss: 3.5587e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6223e-11 - val_loss: 3.4531e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4469e-11 - val_loss: 3.5006e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5675e-11 - val_loss: 3.4587e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3646e-11 - val_loss: 3.1477e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0442e-11 - val_loss: 2.9119e-11
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8792e-11 - val_loss: 2.7318e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6569e-11 - val_loss: 2.5938e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7366e-11 - val_loss: 2.9546e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0402e-11 - val_loss: 3.1067e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3009e-11 - val_loss: 3.3734e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3915e-11 - val_loss: 3.4671e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6541e-11 - val_loss: 3.8915e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0172e-11 - val_loss: 3.9019e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8509e-11 - val_loss: 3.7552e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5307e-11 - val_loss: 3.3472e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4767e-11 - val_loss: 3.6264e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6389e-11 - val_loss: 3.3290e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1768e-11 - val_loss: 2.8938e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7867e-11 - val_loss: 2.6816e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5842e-11 - val_loss: 2.5631e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5714e-11 - val_loss: 2.4693e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6342e-11 - val_loss: 2.8253e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9166e-11 - val_loss: 3.0731e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2051e-11 - val_loss: 3.3087e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4884e-11 - val_loss: 3.7832e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0282e-11 - val_loss: 4.2163e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3480e-11 - val_loss: 4.2599e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1235e-11 - val_loss: 3.8700e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7404e-11 - val_loss: 3.4721e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4603e-11 - val_loss: 3.2004e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1126e-11 - val_loss: 2.9140e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9183e-11 - val_loss: 2.8507e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8734e-11 - val_loss: 2.9256e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0590e-11 - val_loss: 3.1912e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0646e-11 - val_loss: 2.8523e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8283e-11 - val_loss: 2.5545e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6047e-11 - val_loss: 2.5961e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6629e-11 - val_loss: 2.7548e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7338e-11 - val_loss: 2.5458e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5384e-11 - val_loss: 2.5483e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6370e-11 - val_loss: 2.6951e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8269e-11 - val_loss: 2.9083e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0505e-11 - val_loss: 3.1846e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2236e-11 - val_loss: 2.9327e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8744e-11 - val_loss: 2.8229e-11
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6568e-11 - val_loss: 2.4429e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4948e-11 - val_loss: 2.3928e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3115e-11 - val_loss: 2.1525e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2821e-11 - val_loss: 2.4352e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4316e-11 - val_loss: 2.4686e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6002e-11 - val_loss: 2.7459e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8521e-11 - val_loss: 3.1561e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2181e-11 - val_loss: 3.2400e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2367e-11 - val_loss: 3.0482e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9748e-11 - val_loss: 2.8077e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7056e-11 - val_loss: 2.5335e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5377e-11 - val_loss: 2.4459e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3405e-11 - val_loss: 2.1639e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3088e-11 - val_loss: 2.4396e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4416e-11 - val_loss: 2.4678e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5868e-11 - val_loss: 2.7221e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8109e-11 - val_loss: 2.8648e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7668e-11 - val_loss: 2.6724e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7196e-11 - val_loss: 2.8337e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8710e-11 - val_loss: 2.9609e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9883e-11 - val_loss: 3.0421e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0218e-11 - val_loss: 2.9260e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8981e-11 - val_loss: 2.7632e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6613e-11 - val_loss: 2.4781e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5023e-11 - val_loss: 2.3946e-11
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2676e-11 - val_loss: 2.1067e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1597e-11 - val_loss: 2.2257e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3492e-11 - val_loss: 2.4615e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6859e-11 - val_loss: 2.8872e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9847e-11 - val_loss: 2.8872e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9963e-11 - val_loss: 3.0700e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9812e-11 - val_loss: 2.7762e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7593e-11 - val_loss: 2.7510e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6605e-11 - val_loss: 2.4515e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4644e-11 - val_loss: 2.5052e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3560e-11 - val_loss: 2.1686e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2161e-11 - val_loss: 2.4094e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4578e-11 - val_loss: 2.4027e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4881e-11 - val_loss: 2.7020e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7424e-11 - val_loss: 2.8955e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9895e-11 - val_loss: 3.0142e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0119e-11 - val_loss: 2.8385e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7908e-11 - val_loss: 2.6631e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5757e-11 - val_loss: 2.4618e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4348e-11 - val_loss: 2.3672e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2989e-11 - val_loss: 2.1428e-11
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1336e-11 - val_loss: 2.0350e-11
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9705e-11 - val_loss: 1.9302e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8926e-11 - val_loss: 2.0019e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0572e-11 - val_loss: 2.0942e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1068e-11 - val_loss: 2.0730e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1939e-11 - val_loss: 2.3730e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3902e-11 - val_loss: 2.4159e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4057e-11 - val_loss: 2.2566e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2642e-11 - val_loss: 2.3665e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4987e-11 - val_loss: 2.5422e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4372e-11 - val_loss: 2.2178e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1854e-11 - val_loss: 2.2084e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2041e-11 - val_loss: 2.0662e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9606e-11 - val_loss: 1.7925e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8069e-11 - val_loss: 1.8529e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8105e-11 - val_loss: 1.6307e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5858e-11 - val_loss: 1.5634e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5696e-11 - val_loss: 1.5284e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6207e-11 - val_loss: 1.8386e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8856e-11 - val_loss: 1.8593e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8629e-11 - val_loss: 1.9827e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1115e-11 - val_loss: 2.2154e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2294e-11 - val_loss: 2.4036e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4226e-11 - val_loss: 2.4879e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5223e-11 - val_loss: 2.6144e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7422e-11 - val_loss: 2.7855e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6424e-11 - val_loss: 2.4695e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4985e-11 - val_loss: 2.4663e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3671e-11 - val_loss: 2.2653e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1621e-11 - val_loss: 2.1193e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9767e-11 - val_loss: 1.8340e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8367e-11 - val_loss: 1.7626e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7928e-11 - val_loss: 1.9316e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0251e-11 - val_loss: 2.0702e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0672e-11 - val_loss: 1.8625e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8704e-11 - val_loss: 1.8722e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8778e-11 - val_loss: 1.9046e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0179e-11 - val_loss: 2.1168e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1308e-11 - val_loss: 2.1693e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2566e-11 - val_loss: 2.4279e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4756e-11 - val_loss: 2.4296e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3301e-11 - val_loss: 2.1400e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0454e-11 - val_loss: 1.8269e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7744e-11 - val_loss: 1.6760e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7159e-11 - val_loss: 1.7646e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7666e-11 - val_loss: 1.6364e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5944e-11 - val_loss: 1.5388e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5447e-11 - val_loss: 1.5050e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6067e-11 - val_loss: 1.7643e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8028e-11 - val_loss: 1.7772e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8577e-11 - val_loss: 2.0180e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1128e-11 - val_loss: 2.1537e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1770e-11 - val_loss: 2.1682e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1644e-11 - val_loss: 2.1428e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1697e-11 - val_loss: 2.1421e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1453e-11 - val_loss: 2.0119e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0031e-11 - val_loss: 1.8422e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7932e-11 - val_loss: 1.7558e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7942e-11 - val_loss: 1.8680e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9696e-11 - val_loss: 2.0694e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0653e-11 - val_loss: 2.0347e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9540e-11 - val_loss: 1.8199e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8140e-11 - val_loss: 1.8136e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8260e-11 - val_loss: 1.7472e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6654e-11 - val_loss: 1.5105e-11
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4803e-11 - val_loss: 1.4878e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5115e-11 - val_loss: 1.6470e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7752e-11 - val_loss: 1.8342e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8916e-11 - val_loss: 2.0372e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0587e-11 - val_loss: 2.0709e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1316e-11 - val_loss: 2.1320e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0659e-11 - val_loss: 1.8265e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7380e-11 - val_loss: 1.6098e-11
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5710e-11 - val_loss: 1.4586e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5121e-11 - val_loss: 1.5800e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5785e-11 - val_loss: 1.5048e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5295e-11 - val_loss: 1.6460e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7224e-11 - val_loss: 1.8436e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8193e-11 - val_loss: 1.8014e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7507e-11 - val_loss: 1.6191e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6014e-11 - val_loss: 1.5357e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5149e-11 - val_loss: 1.5251e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6438e-11 - val_loss: 1.7519e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7821e-11 - val_loss: 1.8004e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9087e-11 - val_loss: 2.0477e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0671e-11 - val_loss: 2.0875e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1428e-11 - val_loss: 2.0923e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9550e-11 - val_loss: 1.8553e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8810e-11 - val_loss: 1.9658e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0566e-11 - val_loss: 2.1321e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0848e-11 - val_loss: 1.8559e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8361e-11 - val_loss: 1.7937e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7501e-11 - val_loss: 1.7783e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7218e-11 - val_loss: 1.5372e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5196e-11 - val_loss: 1.4641e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4751e-11 - val_loss: 1.4714e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5246e-11 - val_loss: 1.6662e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7627e-11 - val_loss: 1.7788e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7871e-11 - val_loss: 1.7343e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6287e-11 - val_loss: 1.5236e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5211e-11 - val_loss: 1.4735e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5244e-11 - val_loss: 1.6166e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6789e-11 - val_loss: 1.7266e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7866e-11 - val_loss: 1.7562e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8113e-11 - val_loss: 1.9865e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0031e-11 - val_loss: 2.0031e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9961e-11 - val_loss: 1.8258e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7784e-11 - val_loss: 1.6984e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6545e-11 - val_loss: 1.5632e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5571e-11 - val_loss: 1.5006e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5232e-11 - val_loss: 1.5163e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5349e-11 - val_loss: 1.5235e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4590e-11 - val_loss: 1.3348e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2995e-11 - val_loss: 1.2481e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2987e-11 - val_loss: 1.4420e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4969e-11 - val_loss: 1.4947e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5227e-11 - val_loss: 1.4466e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4942e-11 - val_loss: 1.5603e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6361e-11 - val_loss: 1.8036e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8882e-11 - val_loss: 1.8726e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8751e-11 - val_loss: 1.9089e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9328e-11 - val_loss: 2.0287e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0752e-11 - val_loss: 2.0772e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9770e-11 - val_loss: 1.8789e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8274e-11 - val_loss: 1.7234e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6087e-11 - val_loss: 1.5187e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4826e-11 - val_loss: 1.4087e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4665e-11 - val_loss: 1.5348e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4940e-11 - val_loss: 1.3920e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3642e-11 - val_loss: 1.3375e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2680e-11 - val_loss: 1.1723e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1485e-11 - val_loss: 1.0562e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 0s - loss: 13.4830 - val_loss: 0.3457
Epoch 2/512
512/512 - 0s - loss: 0.4115 - val_loss: 0.1539
Epoch 3/512
512/512 - 0s - loss: 0.1908 - val_loss: 0.0893
Epoch 4/512
512/512 - 0s - loss: 0.1067 - val_loss: 0.0826
Epoch 5/512
512/512 - 0s - loss: 0.0849 - val_loss: 0.0787
Epoch 6/512
512/512 - 0s - loss: 0.0701 - val_loss: 0.0727
Epoch 7/512
512/512 - 0s - loss: 0.0600 - val_loss: 0.0667
Epoch 8/512
512/512 - 0s - loss: 0.0788 - val_loss: 0.0823
Epoch 9/512
512/512 - 0s - loss: 0.0664 - val_loss: 0.0695
Epoch 10/512
512/512 - 0s - loss: 0.0483 - val_loss: 0.0546
Epoch 11/512
512/512 - 0s - loss: 0.0375 - val_loss: 0.0409
Epoch 12/512
512/512 - 0s - loss: 0.0338 - val_loss: 0.0381
Epoch 13/512
512/512 - 0s - loss: 0.0331 - val_loss: 0.0356
Epoch 14/512
512/512 - 0s - loss: 0.0325 - val_loss: 0.0384
Epoch 15/512
512/512 - 0s - loss: 0.0345 - val_loss: 0.0294
Epoch 16/512
512/512 - 0s - loss: 0.0311 - val_loss: 0.0294
Epoch 17/512
512/512 - 0s - loss: 0.0306 - val_loss: 0.0277
Epoch 18/512
512/512 - 0s - loss: 0.0305 - val_loss: 0.0357
Epoch 19/512
512/512 - 0s - loss: 0.0336 - val_loss: 0.0225
Epoch 20/512
512/512 - 0s - loss: 0.0290 - val_loss: 0.0234
Epoch 21/512
512/512 - 0s - loss: 0.0280 - val_loss: 0.0225
Epoch 22/512
512/512 - 0s - loss: 0.0268 - val_loss: 0.0206
Epoch 23/512
512/512 - 0s - loss: 0.0255 - val_loss: 0.0179
Epoch 24/512
512/512 - 0s - loss: 0.0240 - val_loss: 0.0144
Epoch 25/512
512/512 - 0s - loss: 0.0223 - val_loss: 0.0082
Epoch 26/512
512/512 - 0s - loss: 0.0224 - val_loss: 0.0075
Epoch 27/512
512/512 - 0s - loss: 0.0191 - val_loss: 0.0062
Epoch 28/512
512/512 - 0s - loss: 0.0192 - val_loss: 0.0175
Epoch 29/512
512/512 - 0s - loss: 0.0585 - val_loss: 0.0504
Epoch 30/512
512/512 - 0s - loss: 0.0294 - val_loss: 0.0039
Epoch 31/512
512/512 - 0s - loss: 0.0140 - val_loss: 0.0025
Epoch 32/512
512/512 - 0s - loss: 0.0122 - val_loss: 0.0013
Epoch 33/512
512/512 - 0s - loss: 0.0115 - val_loss: 0.0292
Epoch 34/512
512/512 - 0s - loss: 0.1230 - val_loss: 0.0518
Epoch 35/512
512/512 - 0s - loss: 0.2189 - val_loss: 0.0797
Epoch 36/512
512/512 - 0s - loss: 0.0697 - val_loss: 0.0618
Epoch 37/512
512/512 - 0s - loss: 0.0561 - val_loss: 0.0466
Epoch 38/512
512/512 - 0s - loss: 0.0418 - val_loss: 0.0191
Epoch 39/512
512/512 - 0s - loss: 0.0240 - val_loss: 0.0071
Epoch 40/512
512/512 - 0s - loss: 0.0163 - val_loss: 0.0020
Epoch 41/512
512/512 - 0s - loss: 0.0114 - val_loss: 0.0019
Epoch 42/512
512/512 - 0s - loss: 0.0094 - val_loss: 0.0015
Epoch 43/512
512/512 - 0s - loss: 0.0085 - val_loss: 0.0020
Epoch 44/512
512/512 - 0s - loss: 0.0106 - val_loss: 6.1107e-04
Epoch 45/512
512/512 - 0s - loss: 0.0069 - val_loss: 2.3931e-04
Epoch 46/512
512/512 - 0s - loss: 0.0200 - val_loss: 0.0032
Epoch 47/512
512/512 - 0s - loss: 0.0117 - val_loss: 1.1471e-04
Epoch 48/512
512/512 - 0s - loss: 0.0051 - val_loss: 2.9566e-05
Epoch 49/512
512/512 - 0s - loss: 0.0070 - val_loss: 0.0037
Epoch 50/512
512/512 - 0s - loss: 0.0176 - val_loss: 7.1346e-04
Epoch 51/512
512/512 - 0s - loss: 0.0056 - val_loss: 5.5449e-05
Epoch 52/512
512/512 - 0s - loss: 0.0027 - val_loss: 2.6768e-05
Epoch 53/512
512/512 - 0s - loss: 0.0040 - val_loss: 6.4949e-04
Epoch 54/512
512/512 - 0s - loss: 0.0060 - val_loss: 3.8291e-05
Epoch 55/512
512/512 - 0s - loss: 0.0061 - val_loss: 1.0368e-04
Epoch 56/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.1529e-04
Epoch 57/512
512/512 - 0s - loss: 0.0045 - val_loss: 0.0022
Epoch 58/512
512/512 - 0s - loss: 0.0045 - val_loss: 6.5064e-05
Epoch 59/512
512/512 - 0s - loss: 0.0025 - val_loss: 1.3658e-04
Epoch 60/512
512/512 - 0s - loss: 0.0036 - val_loss: 8.8418e-05
Epoch 61/512
512/512 - 0s - loss: 0.0029 - val_loss: 9.9139e-05
Epoch 62/512
512/512 - 0s - loss: 0.0030 - val_loss: 4.9017e-05
Epoch 63/512
512/512 - 0s - loss: 0.0023 - val_loss: 6.7953e-05
Epoch 64/512
512/512 - 0s - loss: 0.0029 - val_loss: 7.6828e-05
Epoch 65/512
512/512 - 0s - loss: 0.0033 - val_loss: 3.2152e-04
Epoch 66/512
512/512 - 0s - loss: 0.0020 - val_loss: 1.9222e-05
Epoch 67/512
512/512 - 0s - loss: 0.0022 - val_loss: 2.3406e-05
Epoch 68/512
512/512 - 0s - loss: 0.0024 - val_loss: 3.0656e-05
Epoch 69/512
512/512 - 0s - loss: 0.0019 - val_loss: 3.8843e-05
Epoch 70/512
512/512 - 0s - loss: 0.0019 - val_loss: 3.7834e-05
Epoch 71/512
512/512 - 0s - loss: 0.0020 - val_loss: 3.7652e-05
Epoch 72/512
512/512 - 0s - loss: 0.0018 - val_loss: 3.8125e-05
Epoch 73/512
512/512 - 0s - loss: 0.0016 - val_loss: 3.7352e-05
Epoch 74/512
512/512 - 0s - loss: 0.0017 - val_loss: 3.7461e-05
Epoch 75/512
512/512 - 0s - loss: 0.0016 - val_loss: 3.9215e-05
Epoch 76/512
512/512 - 0s - loss: 0.0015 - val_loss: 4.0633e-05
Epoch 77/512
512/512 - 0s - loss: 0.0015 - val_loss: 4.3931e-05
Epoch 78/512
512/512 - 0s - loss: 0.0015 - val_loss: 4.7683e-05
Epoch 79/512
512/512 - 0s - loss: 0.0015 - val_loss: 5.0301e-05
Epoch 80/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.3801e-05
Epoch 81/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.6416e-05
Epoch 82/512
512/512 - 0s - loss: 0.0013 - val_loss: 5.8577e-05
Epoch 83/512
512/512 - 0s - loss: 0.0013 - val_loss: 6.0611e-05
Epoch 84/512
512/512 - 0s - loss: 0.0012 - val_loss: 6.2986e-05
Epoch 85/512
512/512 - 0s - loss: 0.0012 - val_loss: 6.6201e-05
Epoch 86/512
512/512 - 0s - loss: 0.0011 - val_loss: 7.0426e-05
Epoch 87/512
512/512 - 0s - loss: 0.0011 - val_loss: 7.5160e-05
Epoch 88/512
512/512 - 0s - loss: 9.9580e-04 - val_loss: 8.0564e-05
Epoch 89/512
512/512 - 0s - loss: 9.1881e-04 - val_loss: 8.6536e-05
Epoch 90/512
512/512 - 0s - loss: 8.6773e-04 - val_loss: 9.3854e-05
Epoch 91/512
512/512 - 0s - loss: 8.3287e-04 - val_loss: 1.0181e-04
Epoch 92/512
512/512 - 0s - loss: 7.7039e-04 - val_loss: 1.0966e-04
Epoch 93/512
512/512 - 0s - loss: 7.0560e-04 - val_loss: 1.1879e-04
Epoch 94/512
512/512 - 0s - loss: 6.8852e-04 - val_loss: 1.2926e-04
Epoch 95/512
512/512 - 0s - loss: 6.4289e-04 - val_loss: 1.3974e-04
Epoch 96/512
512/512 - 0s - loss: 5.7551e-04 - val_loss: 1.4971e-04
Epoch 97/512
512/512 - 0s - loss: 5.4741e-04 - val_loss: 1.6137e-04
Epoch 98/512
512/512 - 0s - loss: 5.2914e-04 - val_loss: 1.7376e-04
Epoch 99/512
512/512 - 0s - loss: 4.7638e-04 - val_loss: 1.8525e-04
Epoch 100/512
512/512 - 0s - loss: 4.3635e-04 - val_loss: 1.9713e-04
Epoch 101/512
512/512 - 0s - loss: 4.1722e-04 - val_loss: 2.1056e-04
Epoch 102/512
512/512 - 0s - loss: 3.9035e-04 - val_loss: 2.2359e-04
Epoch 103/512
512/512 - 0s - loss: 3.4770e-04 - val_loss: 2.3520e-04
Epoch 104/512
512/512 - 0s - loss: 3.2775e-04 - val_loss: 2.4875e-04
Epoch 105/512
512/512 - 0s - loss: 3.0425e-04 - val_loss: 2.6088e-04
Epoch 106/512
512/512 - 0s - loss: 2.7136e-04 - val_loss: 2.7197e-04
Epoch 107/512
512/512 - 0s - loss: 2.5298e-04 - val_loss: 2.8401e-04
Epoch 108/512
512/512 - 0s - loss: 2.3391e-04 - val_loss: 2.9558e-04
Epoch 109/512
512/512 - 0s - loss: 2.1001e-04 - val_loss: 3.0528e-04
Epoch 110/512
512/512 - 0s - loss: 1.9163e-04 - val_loss: 3.1493e-04
Epoch 111/512
512/512 - 0s - loss: 1.7643e-04 - val_loss: 3.2382e-04
Epoch 112/512
512/512 - 0s - loss: 1.5725e-04 - val_loss: 3.3078e-04
Epoch 113/512
512/512 - 0s - loss: 1.4241e-04 - val_loss: 3.3718e-04
Epoch 114/512
512/512 - 0s - loss: 1.3001e-04 - val_loss: 3.4298e-04
Epoch 115/512
512/512 - 0s - loss: 1.1908e-04 - val_loss: 3.4719e-04
Epoch 116/512
512/512 - 0s - loss: 1.0534e-04 - val_loss: 3.4946e-04
Epoch 117/512
512/512 - 0s - loss: 9.3288e-05 - val_loss: 3.5111e-04
Epoch 118/512
512/512 - 0s - loss: 8.8453e-05 - val_loss: 3.5305e-04
Epoch 119/512
512/512 - 0s - loss: 7.9721e-05 - val_loss: 3.5281e-04
Epoch 120/512
512/512 - 0s - loss: 6.9433e-05 - val_loss: 3.5080e-04
Epoch 121/512
512/512 - 0s - loss: 6.3484e-05 - val_loss: 3.4892e-04
Epoch 122/512
512/512 - 0s - loss: 5.8830e-05 - val_loss: 3.4629e-04
Epoch 123/512
512/512 - 0s - loss: 5.2718e-05 - val_loss: 3.4231e-04
Epoch 124/512
512/512 - 0s - loss: 4.6066e-05 - val_loss: 3.3718e-04
Epoch 125/512
512/512 - 0s - loss: 4.2871e-05 - val_loss: 3.3278e-04
Epoch 126/512
512/512 - 0s - loss: 3.9929e-05 - val_loss: 3.2732e-04
Epoch 127/512
512/512 - 0s - loss: 3.4675e-05 - val_loss: 3.2060e-04
Epoch 128/512
512/512 - 0s - loss: 3.1580e-05 - val_loss: 3.1401e-04
Epoch 129/512
512/512 - 0s - loss: 2.9309e-05 - val_loss: 3.0742e-04
Epoch 130/512
512/512 - 0s - loss: 2.6315e-05 - val_loss: 3.0001e-04
Epoch 131/512
512/512 - 0s - loss: 2.3595e-05 - val_loss: 2.9234e-04
Epoch 132/512
512/512 - 0s - loss: 2.1743e-05 - val_loss: 2.8484e-04
Epoch 133/512
512/512 - 0s - loss: 2.0103e-05 - val_loss: 2.7733e-04
Epoch 134/512
512/512 - 0s - loss: 1.7844e-05 - val_loss: 2.6915e-04
Epoch 135/512
512/512 - 0s - loss: 1.6174e-05 - val_loss: 2.6121e-04
Epoch 136/512
512/512 - 0s - loss: 1.5030e-05 - val_loss: 2.5348e-04
Epoch 137/512
512/512 - 0s - loss: 1.3901e-05 - val_loss: 2.4574e-04
Epoch 138/512
512/512 - 0s - loss: 1.2502e-05 - val_loss: 2.3781e-04
Epoch 139/512
512/512 - 0s - loss: 1.1304e-05 - val_loss: 2.3014e-04
Epoch 140/512
512/512 - 0s - loss: 1.0697e-05 - val_loss: 2.2269e-04
Epoch 141/512
512/512 - 0s - loss: 9.6203e-06 - val_loss: 2.1517e-04
Epoch 142/512
512/512 - 0s - loss: 8.6324e-06 - val_loss: 2.0783e-04
Epoch 143/512
512/512 - 0s - loss: 8.2296e-06 - val_loss: 2.0094e-04
Epoch 144/512
512/512 - 0s - loss: 7.6420e-06 - val_loss: 1.9404e-04
Epoch 145/512
512/512 - 0s - loss: 6.7056e-06 - val_loss: 1.8715e-04
Epoch 146/512
512/512 - 0s - loss: 6.2704e-06 - val_loss: 1.8074e-04
Epoch 147/512
512/512 - 0s - loss: 6.0040e-06 - val_loss: 1.7456e-04
Epoch 148/512
512/512 - 0s - loss: 5.5152e-06 - val_loss: 1.6848e-04
Epoch 149/512
512/512 - 0s - loss: 4.9425e-06 - val_loss: 1.6252e-04
Epoch 150/512
512/512 - 0s - loss: 4.6111e-06 - val_loss: 1.5691e-04
Epoch 151/512
512/512 - 0s - loss: 4.3961e-06 - val_loss: 1.5154e-04
Epoch 152/512
512/512 - 0s - loss: 4.0628e-06 - val_loss: 1.4631e-04
Epoch 153/512
512/512 - 0s - loss: 3.7192e-06 - val_loss: 1.4127e-04
Epoch 154/512
512/512 - 0s - loss: 3.5032e-06 - val_loss: 1.3649e-04
Epoch 155/512
512/512 - 0s - loss: 3.2754e-06 - val_loss: 1.3188e-04
Epoch 156/512
512/512 - 0s - loss: 3.0511e-06 - val_loss: 1.2749e-04
Epoch 157/512
512/512 - 0s - loss: 2.8699e-06 - val_loss: 1.2327e-04
Epoch 158/512
512/512 - 0s - loss: 2.6726e-06 - val_loss: 1.1923e-04
Epoch 159/512
512/512 - 0s - loss: 2.5276e-06 - val_loss: 1.1539e-04
Epoch 160/512
512/512 - 0s - loss: 2.3641e-06 - val_loss: 1.1170e-04
Epoch 161/512
512/512 - 0s - loss: 2.2369e-06 - val_loss: 1.0820e-04
Epoch 162/512
512/512 - 0s - loss: 2.1246e-06 - val_loss: 1.0486e-04
Epoch 163/512
512/512 - 0s - loss: 1.9861e-06 - val_loss: 1.0164e-04
Epoch 164/512
512/512 - 0s - loss: 1.8738e-06 - val_loss: 9.8596e-05
Epoch 165/512
512/512 - 0s - loss: 1.8069e-06 - val_loss: 9.5701e-05
Epoch 166/512
512/512 - 0s - loss: 1.6967e-06 - val_loss: 9.2910e-05
Epoch 167/512
512/512 - 0s - loss: 1.6016e-06 - val_loss: 9.0249e-05
Epoch 168/512
512/512 - 0s - loss: 1.5334e-06 - val_loss: 8.7732e-05
Epoch 169/512
512/512 - 0s - loss: 1.4784e-06 - val_loss: 8.5328e-05
Epoch 170/512
512/512 - 0s - loss: 1.3945e-06 - val_loss: 8.3004e-05
Epoch 171/512
512/512 - 0s - loss: 1.3257e-06 - val_loss: 8.0796e-05
Epoch 172/512
512/512 - 0s - loss: 1.2828e-06 - val_loss: 7.8708e-05
Epoch 173/512
512/512 - 0s - loss: 1.2384e-06 - val_loss: 7.6700e-05
Epoch 174/512
512/512 - 0s - loss: 1.1688e-06 - val_loss: 7.4759e-05
Epoch 175/512
512/512 - 0s - loss: 1.1221e-06 - val_loss: 7.2924e-05
Epoch 176/512
512/512 - 0s - loss: 1.0912e-06 - val_loss: 7.1171e-05
Epoch 177/512
512/512 - 0s - loss: 1.0524e-06 - val_loss: 6.9489e-05
Epoch 178/512
512/512 - 0s - loss: 1.0065e-06 - val_loss: 6.7871e-05
Epoch 179/512
512/512 - 0s - loss: 9.7389e-07 - val_loss: 6.6328e-05
Epoch 180/512
512/512 - 0s - loss: 9.4402e-07 - val_loss: 6.4850e-05
Epoch 181/512
512/512 - 0s - loss: 9.0994e-07 - val_loss: 6.3428e-05
Epoch 182/512
512/512 - 0s - loss: 8.8056e-07 - val_loss: 6.2066e-05
Epoch 183/512
512/512 - 0s - loss: 8.5493e-07 - val_loss: 6.0760e-05
Epoch 184/512
512/512 - 0s - loss: 8.2894e-07 - val_loss: 5.9504e-05
Epoch 185/512
512/512 - 0s - loss: 8.0454e-07 - val_loss: 5.8296e-05
Epoch 186/512
512/512 - 0s - loss: 7.8264e-07 - val_loss: 5.7138e-05
Epoch 187/512
512/512 - 0s - loss: 7.6069e-07 - val_loss: 5.6022e-05
Epoch 188/512
512/512 - 0s - loss: 7.3956e-07 - val_loss: 5.4948e-05
Epoch 189/512
512/512 - 0s - loss: 7.2049e-07 - val_loss: 5.3914e-05
Epoch 190/512
512/512 - 0s - loss: 7.0264e-07 - val_loss: 5.2920e-05
Epoch 191/512
512/512 - 0s - loss: 6.8528e-07 - val_loss: 5.1961e-05
Epoch 192/512
512/512 - 0s - loss: 6.6870e-07 - val_loss: 5.1034e-05
Epoch 193/512
512/512 - 0s - loss: 6.5198e-07 - val_loss: 5.0141e-05
Epoch 194/512
512/512 - 0s - loss: 6.3751e-07 - val_loss: 4.9280e-05
Epoch 195/512
512/512 - 0s - loss: 6.2404e-07 - val_loss: 4.8448e-05
Epoch 196/512
512/512 - 0s - loss: 6.1093e-07 - val_loss: 4.7646e-05
Epoch 197/512
512/512 - 0s - loss: 5.9721e-07 - val_loss: 4.6870e-05
Epoch 198/512
512/512 - 0s - loss: 5.8494e-07 - val_loss: 4.6118e-05
Epoch 199/512
512/512 - 0s - loss: 5.7231e-07 - val_loss: 4.5393e-05
Epoch 200/512
512/512 - 0s - loss: 5.6176e-07 - val_loss: 4.4689e-05
Epoch 201/512
512/512 - 0s - loss: 5.5075e-07 - val_loss: 4.4008e-05
Epoch 202/512
512/512 - 0s - loss: 5.4053e-07 - val_loss: 4.3351e-05
Epoch 203/512
512/512 - 0s - loss: 5.3074e-07 - val_loss: 4.2713e-05
Epoch 204/512
512/512 - 0s - loss: 5.2100e-07 - val_loss: 4.2094e-05
Epoch 205/512
512/512 - 0s - loss: 5.1178e-07 - val_loss: 4.1494e-05
Epoch 206/512
512/512 - 0s - loss: 5.0267e-07 - val_loss: 4.0912e-05
Epoch 207/512
512/512 - 0s - loss: 4.9422e-07 - val_loss: 4.0348e-05
Epoch 208/512
512/512 - 0s - loss: 4.8606e-07 - val_loss: 3.9800e-05
Epoch 209/512
512/512 - 0s - loss: 4.7799e-07 - val_loss: 3.9268e-05
Epoch 210/512
512/512 - 0s - loss: 4.7047e-07 - val_loss: 3.8751e-05
Epoch 211/512
512/512 - 0s - loss: 4.6333e-07 - val_loss: 3.8250e-05
Epoch 212/512
512/512 - 0s - loss: 4.5629e-07 - val_loss: 3.7762e-05
Epoch 213/512
512/512 - 0s - loss: 4.4921e-07 - val_loss: 3.7286e-05
Epoch 214/512
512/512 - 0s - loss: 4.4246e-07 - val_loss: 3.6823e-05
Epoch 215/512
512/512 - 0s - loss: 4.3585e-07 - val_loss: 3.6372e-05
Epoch 216/512
512/512 - 0s - loss: 4.2958e-07 - val_loss: 3.5935e-05
Epoch 217/512
512/512 - 0s - loss: 4.2370e-07 - val_loss: 3.5509e-05
Epoch 218/512
512/512 - 0s - loss: 4.1805e-07 - val_loss: 3.5094e-05
Epoch 219/512
512/512 - 0s - loss: 4.1265e-07 - val_loss: 3.4690e-05
Epoch 220/512
512/512 - 0s - loss: 4.0693e-07 - val_loss: 3.4295e-05
Epoch 221/512
512/512 - 0s - loss: 4.0147e-07 - val_loss: 3.3910e-05
Epoch 222/512
512/512 - 0s - loss: 3.9623e-07 - val_loss: 3.3535e-05
Epoch 223/512
512/512 - 0s - loss: 3.9122e-07 - val_loss: 3.3169e-05
Epoch 224/512
512/512 - 0s - loss: 3.8635e-07 - val_loss: 3.2812e-05
Epoch 225/512
512/512 - 0s - loss: 3.8160e-07 - val_loss: 3.2464e-05
Epoch 226/512
512/512 - 0s - loss: 3.7701e-07 - val_loss: 3.2124e-05
Epoch 227/512
512/512 - 0s - loss: 3.7253e-07 - val_loss: 3.1791e-05
Epoch 228/512
512/512 - 0s - loss: 3.6811e-07 - val_loss: 3.1466e-05
Epoch 229/512
512/512 - 0s - loss: 3.6387e-07 - val_loss: 3.1149e-05
Epoch 230/512
512/512 - 0s - loss: 3.5975e-07 - val_loss: 3.0839e-05
Epoch 231/512
512/512 - 0s - loss: 3.5573e-07 - val_loss: 3.0535e-05
Epoch 232/512
512/512 - 0s - loss: 3.5180e-07 - val_loss: 3.0239e-05
Epoch 233/512
512/512 - 0s - loss: 3.4800e-07 - val_loss: 2.9950e-05
Epoch 234/512
512/512 - 0s - loss: 3.4426e-07 - val_loss: 2.9666e-05
Epoch 235/512
512/512 - 0s - loss: 3.4062e-07 - val_loss: 2.9388e-05
Epoch 236/512
512/512 - 0s - loss: 3.3706e-07 - val_loss: 2.9116e-05
Epoch 237/512
512/512 - 0s - loss: 3.3357e-07 - val_loss: 2.8850e-05
Epoch 238/512
512/512 - 0s - loss: 3.3018e-07 - val_loss: 2.8589e-05
Epoch 239/512
512/512 - 0s - loss: 3.2687e-07 - val_loss: 2.8334e-05
Epoch 240/512
512/512 - 0s - loss: 3.2363e-07 - val_loss: 2.8084e-05
Epoch 241/512
512/512 - 0s - loss: 3.2046e-07 - val_loss: 2.7839e-05
Epoch 242/512
512/512 - 0s - loss: 3.1737e-07 - val_loss: 2.7598e-05
Epoch 243/512
512/512 - 0s - loss: 3.1434e-07 - val_loss: 2.7363e-05
Epoch 244/512
512/512 - 0s - loss: 3.1137e-07 - val_loss: 2.7132e-05
Epoch 245/512
512/512 - 0s - loss: 3.0846e-07 - val_loss: 2.6906e-05
Epoch 246/512
512/512 - 0s - loss: 3.0562e-07 - val_loss: 2.6683e-05
Epoch 247/512
512/512 - 0s - loss: 3.0284e-07 - val_loss: 2.6465e-05
Epoch 248/512
512/512 - 0s - loss: 3.0011e-07 - val_loss: 2.6252e-05
Epoch 249/512
512/512 - 0s - loss: 2.9744e-07 - val_loss: 2.6042e-05
Epoch 250/512
512/512 - 0s - loss: 2.9482e-07 - val_loss: 2.5836e-05
Epoch 251/512
512/512 - 0s - loss: 2.9226e-07 - val_loss: 2.5633e-05
Epoch 252/512
512/512 - 0s - loss: 2.8974e-07 - val_loss: 2.5435e-05
Epoch 253/512
512/512 - 0s - loss: 2.8728e-07 - val_loss: 2.5239e-05
Epoch 254/512
512/512 - 0s - loss: 2.8486e-07 - val_loss: 2.5048e-05
Epoch 255/512
512/512 - 0s - loss: 2.8249e-07 - val_loss: 2.4859e-05
Epoch 256/512
512/512 - 0s - loss: 2.8016e-07 - val_loss: 2.4674e-05
Epoch 257/512
512/512 - 0s - loss: 2.7787e-07 - val_loss: 2.4492e-05
Epoch 258/512
512/512 - 0s - loss: 2.7563e-07 - val_loss: 2.4313e-05
Epoch 259/512
512/512 - 0s - loss: 2.7343e-07 - val_loss: 2.4137e-05
Epoch 260/512
512/512 - 0s - loss: 2.7127e-07 - val_loss: 2.3964e-05
Epoch 261/512
512/512 - 0s - loss: 2.6914e-07 - val_loss: 2.3794e-05
Epoch 262/512
512/512 - 0s - loss: 2.6706e-07 - val_loss: 2.3627e-05
Epoch 263/512
512/512 - 0s - loss: 2.6501e-07 - val_loss: 2.3463e-05
Epoch 264/512
512/512 - 0s - loss: 2.6300e-07 - val_loss: 2.3301e-05
Epoch 265/512
512/512 - 0s - loss: 2.6102e-07 - val_loss: 2.3142e-05
Epoch 266/512
512/512 - 0s - loss: 2.5908e-07 - val_loss: 2.2985e-05
Epoch 267/512
512/512 - 0s - loss: 2.5717e-07 - val_loss: 2.2830e-05
Epoch 268/512
512/512 - 0s - loss: 2.5529e-07 - val_loss: 2.2678e-05
Epoch 269/512
512/512 - 0s - loss: 2.5345e-07 - val_loss: 2.2529e-05
Epoch 270/512
512/512 - 0s - loss: 2.5163e-07 - val_loss: 2.2382e-05
Epoch 271/512
512/512 - 0s - loss: 2.4985e-07 - val_loss: 2.2236e-05
Epoch 272/512
512/512 - 0s - loss: 2.4809e-07 - val_loss: 2.2094e-05
Epoch 273/512
512/512 - 0s - loss: 2.4636e-07 - val_loss: 2.1953e-05
Epoch 274/512
512/512 - 0s - loss: 2.4466e-07 - val_loss: 2.1814e-05
Epoch 275/512
512/512 - 0s - loss: 2.4299e-07 - val_loss: 2.1678e-05
Epoch 276/512
512/512 - 0s - loss: 2.4134e-07 - val_loss: 2.1543e-05
Epoch 277/512
512/512 - 0s - loss: 2.3972e-07 - val_loss: 2.1410e-05
Epoch 278/512
512/512 - 0s - loss: 2.3812e-07 - val_loss: 2.1279e-05
Epoch 279/512
512/512 - 0s - loss: 2.3655e-07 - val_loss: 2.1151e-05
Epoch 280/512
512/512 - 0s - loss: 2.3500e-07 - val_loss: 2.1023e-05
Epoch 281/512
512/512 - 0s - loss: 2.3348e-07 - val_loss: 2.0898e-05
Epoch 282/512
512/512 - 0s - loss: 2.3197e-07 - val_loss: 2.0775e-05
Epoch 283/512
512/512 - 0s - loss: 2.3050e-07 - val_loss: 2.0653e-05
Epoch 284/512
512/512 - 0s - loss: 2.2904e-07 - val_loss: 2.0533e-05
Epoch 285/512
512/512 - 0s - loss: 2.2760e-07 - val_loss: 2.0414e-05
Epoch 286/512
512/512 - 0s - loss: 2.2619e-07 - val_loss: 2.0297e-05
Epoch 287/512
512/512 - 0s - loss: 2.2479e-07 - val_loss: 2.0182e-05
Epoch 288/512
512/512 - 0s - loss: 2.2342e-07 - val_loss: 2.0068e-05
Epoch 289/512
512/512 - 0s - loss: 2.2206e-07 - val_loss: 1.9956e-05
Epoch 290/512
512/512 - 0s - loss: 2.2072e-07 - val_loss: 1.9845e-05
Epoch 291/512
512/512 - 0s - loss: 2.1941e-07 - val_loss: 1.9735e-05
Epoch 292/512
512/512 - 0s - loss: 2.1811e-07 - val_loss: 1.9627e-05
Epoch 293/512
512/512 - 0s - loss: 2.1682e-07 - val_loss: 1.9521e-05
Epoch 294/512
512/512 - 0s - loss: 2.1556e-07 - val_loss: 1.9416e-05
Epoch 295/512
512/512 - 0s - loss: 2.1431e-07 - val_loss: 1.9312e-05
Epoch 296/512
512/512 - 0s - loss: 2.1308e-07 - val_loss: 1.9209e-05
Epoch 297/512
512/512 - 0s - loss: 2.1187e-07 - val_loss: 1.9108e-05
Epoch 298/512
512/512 - 0s - loss: 2.1067e-07 - val_loss: 1.9008e-05
Epoch 299/512
512/512 - 0s - loss: 2.0949e-07 - val_loss: 1.8909e-05
Epoch 300/512
512/512 - 0s - loss: 2.0832e-07 - val_loss: 1.8811e-05
Epoch 301/512
512/512 - 0s - loss: 2.0717e-07 - val_loss: 1.8715e-05
Epoch 302/512
512/512 - 0s - loss: 2.0603e-07 - val_loss: 1.8620e-05
Epoch 303/512
512/512 - 0s - loss: 2.0491e-07 - val_loss: 1.8525e-05
Epoch 304/512
512/512 - 0s - loss: 2.0380e-07 - val_loss: 1.8432e-05
Epoch 305/512
512/512 - 0s - loss: 2.0271e-07 - val_loss: 1.8341e-05
Epoch 306/512
512/512 - 0s - loss: 2.0163e-07 - val_loss: 1.8250e-05
Epoch 307/512
512/512 - 0s - loss: 2.0056e-07 - val_loss: 1.8160e-05
Epoch 308/512
512/512 - 0s - loss: 1.9951e-07 - val_loss: 1.8071e-05
Epoch 309/512
512/512 - 0s - loss: 1.9847e-07 - val_loss: 1.7984e-05
Epoch 310/512
512/512 - 0s - loss: 1.9744e-07 - val_loss: 1.7897e-05
Epoch 311/512
512/512 - 0s - loss: 1.9643e-07 - val_loss: 1.7811e-05
Epoch 312/512
512/512 - 0s - loss: 1.9542e-07 - val_loss: 1.7727e-05
Epoch 313/512
512/512 - 0s - loss: 1.9443e-07 - val_loss: 1.7643e-05
Epoch 314/512
512/512 - 0s - loss: 1.9345e-07 - val_loss: 1.7560e-05
Epoch 315/512
512/512 - 0s - loss: 1.9249e-07 - val_loss: 1.7478e-05
Epoch 316/512
512/512 - 0s - loss: 1.9153e-07 - val_loss: 1.7397e-05
Epoch 317/512
512/512 - 0s - loss: 1.9059e-07 - val_loss: 1.7317e-05
Epoch 318/512
512/512 - 0s - loss: 1.8965e-07 - val_loss: 1.7238e-05
Epoch 319/512
512/512 - 0s - loss: 1.8873e-07 - val_loss: 1.7159e-05
Epoch 320/512
512/512 - 0s - loss: 1.8782e-07 - val_loss: 1.7082e-05
Epoch 321/512
512/512 - 0s - loss: 1.8691e-07 - val_loss: 1.7005e-05
Epoch 322/512
512/512 - 0s - loss: 1.8602e-07 - val_loss: 1.6929e-05
Epoch 323/512
512/512 - 0s - loss: 1.8514e-07 - val_loss: 1.6854e-05
Epoch 324/512
512/512 - 0s - loss: 1.8427e-07 - val_loss: 1.6780e-05
Epoch 325/512
512/512 - 0s - loss: 1.8340e-07 - val_loss: 1.6706e-05
Epoch 326/512
512/512 - 0s - loss: 1.8255e-07 - val_loss: 1.6633e-05
Epoch 327/512
512/512 - 0s - loss: 1.8171e-07 - val_loss: 1.6562e-05
Epoch 328/512
512/512 - 0s - loss: 1.8087e-07 - val_loss: 1.6490e-05
Epoch 329/512
512/512 - 0s - loss: 1.8005e-07 - val_loss: 1.6420e-05
Epoch 330/512
512/512 - 0s - loss: 1.7923e-07 - val_loss: 1.6350e-05
Epoch 331/512
512/512 - 0s - loss: 1.7842e-07 - val_loss: 1.6281e-05
Epoch 332/512
512/512 - 0s - loss: 1.7762e-07 - val_loss: 1.6212e-05
Epoch 333/512
512/512 - 0s - loss: 1.7683e-07 - val_loss: 1.6145e-05
Epoch 334/512
512/512 - 0s - loss: 1.7605e-07 - val_loss: 1.6078e-05
Epoch 335/512
512/512 - 0s - loss: 1.7527e-07 - val_loss: 1.6011e-05
Epoch 336/512
512/512 - 0s - loss: 1.7451e-07 - val_loss: 1.5946e-05
Epoch 337/512
512/512 - 0s - loss: 1.7375e-07 - val_loss: 1.5880e-05
Epoch 338/512
512/512 - 0s - loss: 1.7300e-07 - val_loss: 1.5816e-05
Epoch 339/512
512/512 - 0s - loss: 1.7226e-07 - val_loss: 1.5752e-05
Epoch 340/512
512/512 - 0s - loss: 1.7152e-07 - val_loss: 1.5689e-05
Epoch 341/512
512/512 - 0s - loss: 1.7079e-07 - val_loss: 1.5626e-05
Epoch 342/512
512/512 - 0s - loss: 1.7007e-07 - val_loss: 1.5564e-05
Epoch 343/512
512/512 - 0s - loss: 1.6936e-07 - val_loss: 1.5503e-05
Epoch 344/512
512/512 - 0s - loss: 1.6865e-07 - val_loss: 1.5442e-05
Epoch 345/512
512/512 - 0s - loss: 1.6795e-07 - val_loss: 1.5382e-05
Epoch 346/512
512/512 - 0s - loss: 1.6726e-07 - val_loss: 1.5322e-05
Epoch 347/512
512/512 - 0s - loss: 1.6657e-07 - val_loss: 1.5263e-05
Epoch 348/512
512/512 - 0s - loss: 1.6589e-07 - val_loss: 1.5204e-05
Epoch 349/512
512/512 - 0s - loss: 1.6522e-07 - val_loss: 1.5146e-05
Epoch 350/512
512/512 - 0s - loss: 1.6456e-07 - val_loss: 1.5088e-05
Epoch 351/512
512/512 - 0s - loss: 1.6390e-07 - val_loss: 1.5031e-05
Epoch 352/512
512/512 - 0s - loss: 1.6324e-07 - val_loss: 1.4975e-05
Epoch 353/512
512/512 - 0s - loss: 1.6260e-07 - val_loss: 1.4919e-05
Epoch 354/512
512/512 - 0s - loss: 1.6195e-07 - val_loss: 1.4863e-05
Epoch 355/512
512/512 - 0s - loss: 1.6132e-07 - val_loss: 1.4808e-05
Epoch 356/512
512/512 - 0s - loss: 1.6069e-07 - val_loss: 1.4754e-05
Epoch 357/512
512/512 - 0s - loss: 1.6007e-07 - val_loss: 1.4700e-05
Epoch 358/512
512/512 - 0s - loss: 1.5945e-07 - val_loss: 1.4646e-05
Epoch 359/512
512/512 - 0s - loss: 1.5884e-07 - val_loss: 1.4593e-05
Epoch 360/512
512/512 - 0s - loss: 1.5823e-07 - val_loss: 1.4540e-05
Epoch 361/512
512/512 - 0s - loss: 1.5763e-07 - val_loss: 1.4488e-05
Epoch 362/512
512/512 - 0s - loss: 1.5703e-07 - val_loss: 1.4436e-05
Epoch 363/512
512/512 - 0s - loss: 1.5644e-07 - val_loss: 1.4385e-05
Epoch 364/512
512/512 - 0s - loss: 1.5586e-07 - val_loss: 1.4334e-05
Epoch 365/512
512/512 - 0s - loss: 1.5528e-07 - val_loss: 1.4284e-05
Epoch 366/512
512/512 - 0s - loss: 1.5470e-07 - val_loss: 1.4234e-05
Epoch 367/512
512/512 - 0s - loss: 1.5413e-07 - val_loss: 1.4184e-05
Epoch 368/512
512/512 - 0s - loss: 1.5357e-07 - val_loss: 1.4135e-05
Epoch 369/512
512/512 - 0s - loss: 1.5301e-07 - val_loss: 1.4086e-05
Epoch 370/512
512/512 - 0s - loss: 1.5245e-07 - val_loss: 1.4038e-05
Epoch 371/512
512/512 - 0s - loss: 1.5190e-07 - val_loss: 1.3990e-05
Epoch 372/512
512/512 - 0s - loss: 1.5136e-07 - val_loss: 1.3942e-05
Epoch 373/512
512/512 - 0s - loss: 1.5082e-07 - val_loss: 1.3895e-05
Epoch 374/512
512/512 - 0s - loss: 1.5028e-07 - val_loss: 1.3848e-05
Epoch 375/512
512/512 - 0s - loss: 1.4975e-07 - val_loss: 1.3802e-05
Epoch 376/512
512/512 - 0s - loss: 1.4922e-07 - val_loss: 1.3756e-05
Epoch 377/512
512/512 - 0s - loss: 1.4870e-07 - val_loss: 1.3710e-05
Epoch 378/512
512/512 - 0s - loss: 1.4818e-07 - val_loss: 1.3665e-05
Epoch 379/512
512/512 - 0s - loss: 1.4767e-07 - val_loss: 1.3620e-05
Epoch 380/512
512/512 - 0s - loss: 1.4716e-07 - val_loss: 1.3575e-05
Epoch 381/512
512/512 - 0s - loss: 1.4666e-07 - val_loss: 1.3531e-05
Epoch 382/512
512/512 - 0s - loss: 1.4615e-07 - val_loss: 1.3487e-05
Epoch 383/512
512/512 - 0s - loss: 1.4566e-07 - val_loss: 1.3443e-05
Epoch 384/512
512/512 - 0s - loss: 1.4516e-07 - val_loss: 1.3400e-05
Epoch 385/512
512/512 - 0s - loss: 1.4467e-07 - val_loss: 1.3357e-05
Epoch 386/512
512/512 - 0s - loss: 1.4419e-07 - val_loss: 1.3315e-05
Epoch 387/512
512/512 - 0s - loss: 1.4371e-07 - val_loss: 1.3272e-05
Epoch 388/512
512/512 - 0s - loss: 1.4323e-07 - val_loss: 1.3231e-05
Epoch 389/512
512/512 - 0s - loss: 1.4276e-07 - val_loss: 1.3189e-05
Epoch 390/512
512/512 - 0s - loss: 1.4229e-07 - val_loss: 1.3148e-05
Epoch 391/512
512/512 - 0s - loss: 1.4182e-07 - val_loss: 1.3107e-05
Epoch 392/512
512/512 - 0s - loss: 1.4136e-07 - val_loss: 1.3066e-05
Epoch 393/512
512/512 - 0s - loss: 1.4090e-07 - val_loss: 1.3026e-05
Epoch 394/512
512/512 - 0s - loss: 1.4044e-07 - val_loss: 1.2986e-05
Epoch 395/512
512/512 - 0s - loss: 1.3999e-07 - val_loss: 1.2946e-05
Epoch 396/512
512/512 - 0s - loss: 1.3954e-07 - val_loss: 1.2906e-05
Epoch 397/512
512/512 - 0s - loss: 1.3910e-07 - val_loss: 1.2867e-05
Epoch 398/512
512/512 - 0s - loss: 1.3866e-07 - val_loss: 1.2828e-05
Epoch 399/512
512/512 - 0s - loss: 1.3822e-07 - val_loss: 1.2789e-05
Epoch 400/512
512/512 - 0s - loss: 1.3778e-07 - val_loss: 1.2751e-05
Epoch 401/512
512/512 - 0s - loss: 1.3735e-07 - val_loss: 1.2713e-05
Epoch 402/512
512/512 - 0s - loss: 1.3692e-07 - val_loss: 1.2675e-05
Epoch 403/512
512/512 - 0s - loss: 1.3650e-07 - val_loss: 1.2638e-05
Epoch 404/512
512/512 - 0s - loss: 1.3608e-07 - val_loss: 1.2601e-05
Epoch 405/512
512/512 - 0s - loss: 1.3566e-07 - val_loss: 1.2564e-05
Epoch 406/512
512/512 - 0s - loss: 1.3524e-07 - val_loss: 1.2527e-05
Epoch 407/512
512/512 - 0s - loss: 1.3483e-07 - val_loss: 1.2490e-05
Epoch 408/512
512/512 - 0s - loss: 1.3442e-07 - val_loss: 1.2454e-05
Epoch 409/512
512/512 - 0s - loss: 1.3401e-07 - val_loss: 1.2418e-05
Epoch 410/512
512/512 - 0s - loss: 1.3361e-07 - val_loss: 1.2383e-05
Epoch 411/512
512/512 - 0s - loss: 1.3321e-07 - val_loss: 1.2347e-05
Epoch 412/512
512/512 - 0s - loss: 1.3281e-07 - val_loss: 1.2312e-05
Epoch 413/512
512/512 - 0s - loss: 1.3242e-07 - val_loss: 1.2277e-05
Epoch 414/512
512/512 - 0s - loss: 1.3202e-07 - val_loss: 1.2242e-05
Epoch 415/512
512/512 - 0s - loss: 1.3163e-07 - val_loss: 1.2208e-05
Epoch 416/512
512/512 - 0s - loss: 1.3125e-07 - val_loss: 1.2173e-05
Epoch 417/512
512/512 - 0s - loss: 1.3086e-07 - val_loss: 1.2139e-05
Epoch 418/512
512/512 - 0s - loss: 1.3048e-07 - val_loss: 1.2106e-05
Epoch 419/512
512/512 - 0s - loss: 1.3010e-07 - val_loss: 1.2072e-05
Epoch 420/512
512/512 - 0s - loss: 1.2973e-07 - val_loss: 1.2039e-05
Epoch 421/512
512/512 - 0s - loss: 1.2935e-07 - val_loss: 1.2005e-05
Epoch 422/512
512/512 - 0s - loss: 1.2898e-07 - val_loss: 1.1973e-05
Epoch 423/512
512/512 - 0s - loss: 1.2862e-07 - val_loss: 1.1940e-05
Epoch 424/512
512/512 - 0s - loss: 1.2825e-07 - val_loss: 1.1907e-05
Epoch 425/512
512/512 - 0s - loss: 1.2789e-07 - val_loss: 1.1875e-05
Epoch 426/512
512/512 - 0s - loss: 1.2753e-07 - val_loss: 1.1843e-05
Epoch 427/512
512/512 - 0s - loss: 1.2717e-07 - val_loss: 1.1811e-05
Epoch 428/512
512/512 - 0s - loss: 1.2681e-07 - val_loss: 1.1780e-05
Epoch 429/512
512/512 - 0s - loss: 1.2646e-07 - val_loss: 1.1748e-05
Epoch 430/512
512/512 - 0s - loss: 1.2611e-07 - val_loss: 1.1717e-05
Epoch 431/512
512/512 - 0s - loss: 1.2576e-07 - val_loss: 1.1686e-05
Epoch 432/512
512/512 - 0s - loss: 1.2541e-07 - val_loss: 1.1655e-05
Epoch 433/512
512/512 - 0s - loss: 1.2507e-07 - val_loss: 1.1624e-05
Epoch 434/512
512/512 - 0s - loss: 1.2473e-07 - val_loss: 1.1594e-05
Epoch 435/512
512/512 - 0s - loss: 1.2439e-07 - val_loss: 1.1564e-05
Epoch 436/512
512/512 - 0s - loss: 1.2405e-07 - val_loss: 1.1534e-05
Epoch 437/512
512/512 - 0s - loss: 1.2371e-07 - val_loss: 1.1504e-05
Epoch 438/512
512/512 - 0s - loss: 1.2338e-07 - val_loss: 1.1474e-05
Epoch 439/512
512/512 - 0s - loss: 1.2305e-07 - val_loss: 1.1445e-05
Epoch 440/512
512/512 - 0s - loss: 1.2272e-07 - val_loss: 1.1415e-05
Epoch 441/512
512/512 - 0s - loss: 1.2239e-07 - val_loss: 1.1386e-05
Epoch 442/512
512/512 - 0s - loss: 1.2207e-07 - val_loss: 1.1357e-05
Epoch 443/512
512/512 - 0s - loss: 1.2175e-07 - val_loss: 1.1328e-05
Epoch 444/512
512/512 - 0s - loss: 1.2143e-07 - val_loss: 1.1300e-05
Epoch 445/512
512/512 - 0s - loss: 1.2111e-07 - val_loss: 1.1271e-05
Epoch 446/512
512/512 - 0s - loss: 1.2079e-07 - val_loss: 1.1243e-05
Epoch 447/512
512/512 - 0s - loss: 1.2048e-07 - val_loss: 1.1215e-05
Epoch 448/512
512/512 - 0s - loss: 1.2017e-07 - val_loss: 1.1187e-05
Epoch 449/512
512/512 - 0s - loss: 1.1986e-07 - val_loss: 1.1160e-05
Epoch 450/512
512/512 - 0s - loss: 1.1955e-07 - val_loss: 1.1132e-05
Epoch 451/512
512/512 - 0s - loss: 1.1924e-07 - val_loss: 1.1105e-05
Epoch 452/512
512/512 - 0s - loss: 1.1894e-07 - val_loss: 1.1077e-05
Epoch 453/512
512/512 - 0s - loss: 1.1863e-07 - val_loss: 1.1050e-05
Epoch 454/512
512/512 - 0s - loss: 1.1833e-07 - val_loss: 1.1023e-05
Epoch 455/512
512/512 - 0s - loss: 1.1803e-07 - val_loss: 1.0996e-05
Epoch 456/512
512/512 - 0s - loss: 1.1774e-07 - val_loss: 1.0970e-05
Epoch 457/512
512/512 - 0s - loss: 1.1744e-07 - val_loss: 1.0943e-05
Epoch 458/512
512/512 - 0s - loss: 1.1715e-07 - val_loss: 1.0917e-05
Epoch 459/512
512/512 - 0s - loss: 1.1686e-07 - val_loss: 1.0891e-05
Epoch 460/512
512/512 - 0s - loss: 1.1657e-07 - val_loss: 1.0865e-05
Epoch 461/512
512/512 - 0s - loss: 1.1628e-07 - val_loss: 1.0839e-05
Epoch 462/512
512/512 - 0s - loss: 1.1599e-07 - val_loss: 1.0813e-05
Epoch 463/512
512/512 - 0s - loss: 1.1571e-07 - val_loss: 1.0788e-05
Epoch 464/512
512/512 - 0s - loss: 1.1542e-07 - val_loss: 1.0762e-05
Epoch 465/512
512/512 - 0s - loss: 1.1514e-07 - val_loss: 1.0737e-05
Epoch 466/512
512/512 - 0s - loss: 1.1486e-07 - val_loss: 1.0712e-05
Epoch 467/512
512/512 - 0s - loss: 1.1458e-07 - val_loss: 1.0687e-05
Epoch 468/512
512/512 - 0s - loss: 1.1431e-07 - val_loss: 1.0662e-05
Epoch 469/512
512/512 - 0s - loss: 1.1403e-07 - val_loss: 1.0638e-05
Epoch 470/512
512/512 - 0s - loss: 1.1376e-07 - val_loss: 1.0613e-05
Epoch 471/512
512/512 - 0s - loss: 1.1348e-07 - val_loss: 1.0589e-05
Epoch 472/512
512/512 - 0s - loss: 1.1321e-07 - val_loss: 1.0564e-05
Epoch 473/512
512/512 - 0s - loss: 1.1295e-07 - val_loss: 1.0540e-05
Epoch 474/512
512/512 - 0s - loss: 1.1268e-07 - val_loss: 1.0516e-05
Epoch 475/512
512/512 - 0s - loss: 1.1241e-07 - val_loss: 1.0492e-05
Epoch 476/512
512/512 - 0s - loss: 1.1215e-07 - val_loss: 1.0469e-05
Epoch 477/512
512/512 - 0s - loss: 1.1189e-07 - val_loss: 1.0445e-05
Epoch 478/512
512/512 - 0s - loss: 1.1162e-07 - val_loss: 1.0422e-05
Epoch 479/512
512/512 - 0s - loss: 1.1137e-07 - val_loss: 1.0398e-05
Epoch 480/512
512/512 - 0s - loss: 1.1111e-07 - val_loss: 1.0375e-05
Epoch 481/512
512/512 - 0s - loss: 1.1085e-07 - val_loss: 1.0352e-05
Epoch 482/512
512/512 - 0s - loss: 1.1060e-07 - val_loss: 1.0329e-05
Epoch 483/512
512/512 - 0s - loss: 1.1034e-07 - val_loss: 1.0306e-05
Epoch 484/512
512/512 - 0s - loss: 1.1009e-07 - val_loss: 1.0283e-05
Epoch 485/512
512/512 - 0s - loss: 1.0984e-07 - val_loss: 1.0261e-05
Epoch 486/512
512/512 - 0s - loss: 1.0959e-07 - val_loss: 1.0238e-05
Epoch 487/512
512/512 - 0s - loss: 1.0934e-07 - val_loss: 1.0216e-05
Epoch 488/512
512/512 - 0s - loss: 1.0909e-07 - val_loss: 1.0194e-05
Epoch 489/512
512/512 - 0s - loss: 1.0885e-07 - val_loss: 1.0171e-05
Epoch 490/512
512/512 - 0s - loss: 1.0860e-07 - val_loss: 1.0149e-05
Epoch 491/512
512/512 - 0s - loss: 1.0836e-07 - val_loss: 1.0128e-05
Epoch 492/512
512/512 - 0s - loss: 1.0812e-07 - val_loss: 1.0106e-05
Epoch 493/512
512/512 - 0s - loss: 1.0788e-07 - val_loss: 1.0084e-05
Epoch 494/512
512/512 - 0s - loss: 1.0764e-07 - val_loss: 1.0062e-05
Epoch 495/512
512/512 - 0s - loss: 1.0740e-07 - val_loss: 1.0041e-05
Epoch 496/512
512/512 - 0s - loss: 1.0716e-07 - val_loss: 1.0020e-05
Epoch 497/512
512/512 - 0s - loss: 1.0693e-07 - val_loss: 9.9985e-06
Epoch 498/512
512/512 - 0s - loss: 1.0670e-07 - val_loss: 9.9773e-06
Epoch 499/512
512/512 - 0s - loss: 1.0646e-07 - val_loss: 9.9563e-06
Epoch 500/512
512/512 - 0s - loss: 1.0623e-07 - val_loss: 9.9354e-06
Epoch 501/512
512/512 - 0s - loss: 1.0600e-07 - val_loss: 9.9146e-06
Epoch 502/512
512/512 - 0s - loss: 1.0577e-07 - val_loss: 9.8939e-06
Epoch 503/512
512/512 - 0s - loss: 1.0554e-07 - val_loss: 9.8733e-06
Epoch 504/512
512/512 - 0s - loss: 1.0532e-07 - val_loss: 9.8529e-06
Epoch 505/512
512/512 - 0s - loss: 1.0509e-07 - val_loss: 9.8325e-06
Epoch 506/512
512/512 - 0s - loss: 1.0487e-07 - val_loss: 9.8122e-06
Epoch 507/512
512/512 - 0s - loss: 1.0464e-07 - val_loss: 9.7920e-06
Epoch 508/512
512/512 - 0s - loss: 1.0442e-07 - val_loss: 9.7720e-06
Epoch 509/512
512/512 - 0s - loss: 1.0420e-07 - val_loss: 9.7520e-06
Epoch 510/512
512/512 - 0s - loss: 1.0398e-07 - val_loss: 9.7321e-06
Epoch 511/512
512/512 - 0s - loss: 1.0376e-07 - val_loss: 9.7124e-06
Epoch 512/512
512/512 - 0s - loss: 1.0355e-07 - val_loss: 9.6927e-06
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3947e-10 - val_loss: 9.0003e-11
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.4250e-11 - val_loss: 4.6644e-11
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5605e-11 - val_loss: 4.4851e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4736e-11 - val_loss: 4.4622e-11
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4537e-11 - val_loss: 4.4463e-11
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4382e-11 - val_loss: 4.4317e-11
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4244e-11 - val_loss: 4.4173e-11
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4109e-11 - val_loss: 4.4047e-11
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3983e-11 - val_loss: 4.3924e-11
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3870e-11 - val_loss: 4.3818e-11
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3767e-11 - val_loss: 4.3722e-11
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3673e-11 - val_loss: 4.3634e-11
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3582e-11 - val_loss: 4.3536e-11
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3495e-11 - val_loss: 4.3464e-11
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3420e-11 - val_loss: 4.3379e-11
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3344e-11 - val_loss: 4.3311e-11
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3274e-11 - val_loss: 4.3240e-11
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3209e-11 - val_loss: 4.3182e-11
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3147e-11 - val_loss: 4.3122e-11
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3097e-11 - val_loss: 4.3071e-11
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3044e-11 - val_loss: 4.3024e-11
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3001e-11 - val_loss: 4.2978e-11
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2958e-11 - val_loss: 4.2938e-11
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2917e-11 - val_loss: 4.2900e-11
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2881e-11 - val_loss: 4.2864e-11
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2843e-11 - val_loss: 4.2828e-11
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2808e-11 - val_loss: 4.2791e-11
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2773e-11 - val_loss: 4.2755e-11
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2738e-11 - val_loss: 4.2724e-11
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2706e-11 - val_loss: 4.2689e-11
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2673e-11 - val_loss: 4.2656e-11
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2640e-11 - val_loss: 4.2621e-11
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2607e-11 - val_loss: 4.2589e-11
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2573e-11 - val_loss: 4.2558e-11
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2542e-11 - val_loss: 4.2524e-11
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2512e-11 - val_loss: 4.2493e-11
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2478e-11 - val_loss: 4.2460e-11
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2450e-11 - val_loss: 4.2438e-11
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2420e-11 - val_loss: 4.2399e-11
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2389e-11 - val_loss: 4.2371e-11
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2362e-11 - val_loss: 4.2344e-11
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2334e-11 - val_loss: 4.2318e-11
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2308e-11 - val_loss: 4.2291e-11
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2283e-11 - val_loss: 4.2266e-11
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2256e-11 - val_loss: 4.2237e-11
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2231e-11 - val_loss: 4.2211e-11
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2206e-11 - val_loss: 4.2185e-11
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2178e-11 - val_loss: 4.2164e-11
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2155e-11 - val_loss: 4.2141e-11
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2131e-11 - val_loss: 4.2113e-11
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2109e-11 - val_loss: 4.2092e-11
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2088e-11 - val_loss: 4.2071e-11
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2068e-11 - val_loss: 4.2053e-11
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2047e-11 - val_loss: 4.2030e-11
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2025e-11 - val_loss: 4.2011e-11
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2005e-11 - val_loss: 4.1985e-11
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1980e-11 - val_loss: 4.1962e-11
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1959e-11 - val_loss: 4.1942e-11
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1939e-11 - val_loss: 4.1920e-11
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1917e-11 - val_loss: 4.1900e-11
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1898e-11 - val_loss: 4.1880e-11
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1877e-11 - val_loss: 4.1860e-11
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1859e-11 - val_loss: 4.1845e-11
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1843e-11 - val_loss: 4.1831e-11
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1827e-11 - val_loss: 4.1812e-11
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1809e-11 - val_loss: 4.1796e-11
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1795e-11 - val_loss: 4.1781e-11
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1778e-11 - val_loss: 4.1766e-11
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1768e-11 - val_loss: 4.1750e-11
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1753e-11 - val_loss: 4.1739e-11
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1739e-11 - val_loss: 4.1725e-11
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1721e-11 - val_loss: 4.1708e-11
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1707e-11 - val_loss: 4.1688e-11
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1688e-11 - val_loss: 4.1671e-11
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1669e-11 - val_loss: 4.1651e-11
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1652e-11 - val_loss: 4.1635e-11
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1634e-11 - val_loss: 4.1623e-11
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1619e-11 - val_loss: 4.1604e-11
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1600e-11 - val_loss: 4.1585e-11
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1582e-11 - val_loss: 4.1564e-11
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1562e-11 - val_loss: 4.1548e-11
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1547e-11 - val_loss: 4.1532e-11
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1529e-11 - val_loss: 4.1511e-11
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1512e-11 - val_loss: 4.1491e-11
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1491e-11 - val_loss: 4.1472e-11
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1472e-11 - val_loss: 4.1454e-11
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1455e-11 - val_loss: 4.1435e-11
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1438e-11 - val_loss: 4.1420e-11
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1420e-11 - val_loss: 4.1404e-11
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1405e-11 - val_loss: 4.1393e-11
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1391e-11 - val_loss: 4.1374e-11
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1372e-11 - val_loss: 4.1355e-11
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1358e-11 - val_loss: 4.1337e-11
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1337e-11 - val_loss: 4.1319e-11
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1317e-11 - val_loss: 4.1296e-11
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1299e-11 - val_loss: 4.1281e-11
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1281e-11 - val_loss: 4.1264e-11
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1262e-11 - val_loss: 4.1243e-11
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1243e-11 - val_loss: 4.1226e-11
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1221e-11 - val_loss: 4.1204e-11
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1204e-11 - val_loss: 4.1187e-11
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1190e-11 - val_loss: 4.1172e-11
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1175e-11 - val_loss: 4.1161e-11
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1162e-11 - val_loss: 4.1145e-11
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1146e-11 - val_loss: 4.1135e-11
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1134e-11 - val_loss: 4.1118e-11
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1118e-11 - val_loss: 4.1107e-11
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1109e-11 - val_loss: 4.1091e-11
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1093e-11 - val_loss: 4.1080e-11
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1081e-11 - val_loss: 4.1068e-11
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1069e-11 - val_loss: 4.1053e-11
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1054e-11 - val_loss: 4.1037e-11
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1039e-11 - val_loss: 4.1021e-11
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1023e-11 - val_loss: 4.1005e-11
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1010e-11 - val_loss: 4.0990e-11
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0993e-11 - val_loss: 4.0974e-11
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0975e-11 - val_loss: 4.0955e-11
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0960e-11 - val_loss: 4.0945e-11
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0944e-11 - val_loss: 4.0929e-11
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0930e-11 - val_loss: 4.0912e-11
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0913e-11 - val_loss: 4.0899e-11
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0900e-11 - val_loss: 4.0883e-11
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0887e-11 - val_loss: 4.0870e-11
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0871e-11 - val_loss: 4.0857e-11
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0858e-11 - val_loss: 4.0843e-11
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0845e-11 - val_loss: 4.0824e-11
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0830e-11 - val_loss: 4.0816e-11
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0818e-11 - val_loss: 4.0799e-11
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0799e-11 - val_loss: 4.0783e-11
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0786e-11 - val_loss: 4.0768e-11
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0768e-11 - val_loss: 4.0751e-11
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0753e-11 - val_loss: 4.0732e-11
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0735e-11 - val_loss: 4.0717e-11
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0718e-11 - val_loss: 4.0699e-11
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0703e-11 - val_loss: 4.0682e-11
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0685e-11 - val_loss: 4.0667e-11
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0670e-11 - val_loss: 4.0653e-11
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0653e-11 - val_loss: 4.0633e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0635e-11 - val_loss: 4.0617e-11
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0620e-11 - val_loss: 4.0604e-11
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0604e-11 - val_loss: 4.0588e-11
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0591e-11 - val_loss: 4.0572e-11
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0577e-11 - val_loss: 4.0558e-11
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0563e-11 - val_loss: 4.0544e-11
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0551e-11 - val_loss: 4.0535e-11
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0539e-11 - val_loss: 4.0518e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0523e-11 - val_loss: 4.0510e-11
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0511e-11 - val_loss: 4.0495e-11
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0497e-11 - val_loss: 4.0481e-11
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0483e-11 - val_loss: 4.0465e-11
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0468e-11 - val_loss: 4.0454e-11
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0454e-11 - val_loss: 4.0438e-11
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0439e-11 - val_loss: 4.0419e-11
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0423e-11 - val_loss: 4.0403e-11
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0409e-11 - val_loss: 4.0391e-11
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0395e-11 - val_loss: 4.0376e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0380e-11 - val_loss: 4.0361e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0367e-11 - val_loss: 4.0350e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0353e-11 - val_loss: 4.0338e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0340e-11 - val_loss: 4.0322e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0324e-11 - val_loss: 4.0307e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0309e-11 - val_loss: 4.0292e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0297e-11 - val_loss: 4.0279e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0281e-11 - val_loss: 4.0265e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0268e-11 - val_loss: 4.0254e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0254e-11 - val_loss: 4.0237e-11
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0238e-11 - val_loss: 4.0226e-11
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0226e-11 - val_loss: 4.0213e-11
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0213e-11 - val_loss: 4.0195e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0197e-11 - val_loss: 4.0178e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0184e-11 - val_loss: 4.0165e-11
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0167e-11 - val_loss: 4.0151e-11
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0154e-11 - val_loss: 4.0133e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0137e-11 - val_loss: 4.0118e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0120e-11 - val_loss: 4.0101e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0104e-11 - val_loss: 4.0086e-11
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0088e-11 - val_loss: 4.0072e-11
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0075e-11 - val_loss: 4.0057e-11
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0059e-11 - val_loss: 4.0037e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0042e-11 - val_loss: 4.0024e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0029e-11 - val_loss: 4.0014e-11
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0017e-11 - val_loss: 4.0001e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0004e-11 - val_loss: 3.9986e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9990e-11 - val_loss: 3.9976e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9976e-11 - val_loss: 3.9959e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9963e-11 - val_loss: 3.9947e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9951e-11 - val_loss: 3.9936e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9941e-11 - val_loss: 3.9923e-11
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9926e-11 - val_loss: 3.9908e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9910e-11 - val_loss: 3.9890e-11
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9896e-11 - val_loss: 3.9880e-11
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9882e-11 - val_loss: 3.9865e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9868e-11 - val_loss: 3.9849e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9853e-11 - val_loss: 3.9837e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9840e-11 - val_loss: 3.9826e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9828e-11 - val_loss: 3.9807e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9813e-11 - val_loss: 3.9796e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9800e-11 - val_loss: 3.9782e-11
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9786e-11 - val_loss: 3.9773e-11
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9774e-11 - val_loss: 3.9755e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9760e-11 - val_loss: 3.9747e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9749e-11 - val_loss: 3.9730e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9735e-11 - val_loss: 3.9716e-11
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9720e-11 - val_loss: 3.9700e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9705e-11 - val_loss: 3.9687e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9691e-11 - val_loss: 3.9674e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9677e-11 - val_loss: 3.9658e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9663e-11 - val_loss: 3.9644e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9649e-11 - val_loss: 3.9632e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9634e-11 - val_loss: 3.9618e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9621e-11 - val_loss: 3.9603e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9606e-11 - val_loss: 3.9588e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9592e-11 - val_loss: 3.9573e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9576e-11 - val_loss: 3.9558e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9564e-11 - val_loss: 3.9543e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9547e-11 - val_loss: 3.9532e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9532e-11 - val_loss: 3.9514e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9516e-11 - val_loss: 3.9499e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9502e-11 - val_loss: 3.9486e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9488e-11 - val_loss: 3.9472e-11
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9474e-11 - val_loss: 3.9458e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9457e-11 - val_loss: 3.9442e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9446e-11 - val_loss: 3.9433e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9433e-11 - val_loss: 3.9419e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9420e-11 - val_loss: 3.9406e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9408e-11 - val_loss: 3.9390e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9396e-11 - val_loss: 3.9381e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9382e-11 - val_loss: 3.9368e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9371e-11 - val_loss: 3.9355e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9359e-11 - val_loss: 3.9344e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9345e-11 - val_loss: 3.9329e-11
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9331e-11 - val_loss: 3.9314e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9316e-11 - val_loss: 3.9302e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9304e-11 - val_loss: 3.9288e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9293e-11 - val_loss: 3.9273e-11
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9278e-11 - val_loss: 3.9262e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9265e-11 - val_loss: 3.9249e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9251e-11 - val_loss: 3.9236e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9239e-11 - val_loss: 3.9224e-11
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9223e-11 - val_loss: 3.9210e-11
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9212e-11 - val_loss: 3.9199e-11
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9200e-11 - val_loss: 3.9181e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9186e-11 - val_loss: 3.9170e-11
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9173e-11 - val_loss: 3.9157e-11
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9158e-11 - val_loss: 3.9144e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9146e-11 - val_loss: 3.9128e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9131e-11 - val_loss: 3.9116e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9118e-11 - val_loss: 3.9104e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9106e-11 - val_loss: 3.9089e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9090e-11 - val_loss: 3.9076e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9076e-11 - val_loss: 3.9062e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9063e-11 - val_loss: 3.9047e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9050e-11 - val_loss: 3.9029e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9035e-11 - val_loss: 3.9020e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9021e-11 - val_loss: 3.9003e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9006e-11 - val_loss: 3.8990e-11
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8991e-11 - val_loss: 3.8975e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8978e-11 - val_loss: 3.8961e-11
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8963e-11 - val_loss: 3.8943e-11
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8947e-11 - val_loss: 3.8930e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8935e-11 - val_loss: 3.8918e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8919e-11 - val_loss: 3.8906e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8907e-11 - val_loss: 3.8893e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8893e-11 - val_loss: 3.8879e-11
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8880e-11 - val_loss: 3.8860e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8865e-11 - val_loss: 3.8854e-11
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8855e-11 - val_loss: 3.8839e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8843e-11 - val_loss: 3.8829e-11
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8832e-11 - val_loss: 3.8816e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8818e-11 - val_loss: 3.8806e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8809e-11 - val_loss: 3.8798e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8796e-11 - val_loss: 3.8780e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8783e-11 - val_loss: 3.8767e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8769e-11 - val_loss: 3.8758e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8758e-11 - val_loss: 3.8742e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8743e-11 - val_loss: 3.8730e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8731e-11 - val_loss: 3.8715e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8719e-11 - val_loss: 3.8704e-11
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8702e-11 - val_loss: 3.8687e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8692e-11 - val_loss: 3.8676e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8678e-11 - val_loss: 3.8665e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8667e-11 - val_loss: 3.8652e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8653e-11 - val_loss: 3.8638e-11
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8639e-11 - val_loss: 3.8624e-11
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8626e-11 - val_loss: 3.8612e-11
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8613e-11 - val_loss: 3.8600e-11
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8598e-11 - val_loss: 3.8581e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8586e-11 - val_loss: 3.8571e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8573e-11 - val_loss: 3.8557e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8558e-11 - val_loss: 3.8550e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8549e-11 - val_loss: 3.8532e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8535e-11 - val_loss: 3.8520e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8524e-11 - val_loss: 3.8508e-11
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8508e-11 - val_loss: 3.8492e-11
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8493e-11 - val_loss: 3.8476e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8478e-11 - val_loss: 3.8463e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8467e-11 - val_loss: 3.8451e-11
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8453e-11 - val_loss: 3.8435e-11
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8438e-11 - val_loss: 3.8422e-11
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8424e-11 - val_loss: 3.8406e-11
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8409e-11 - val_loss: 3.8396e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8396e-11 - val_loss: 3.8381e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8382e-11 - val_loss: 3.8370e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8370e-11 - val_loss: 3.8356e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8356e-11 - val_loss: 3.8340e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8345e-11 - val_loss: 3.8332e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8333e-11 - val_loss: 3.8318e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8321e-11 - val_loss: 3.8307e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8309e-11 - val_loss: 3.8293e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8297e-11 - val_loss: 3.8283e-11
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8284e-11 - val_loss: 3.8270e-11
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8273e-11 - val_loss: 3.8258e-11
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8261e-11 - val_loss: 3.8246e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8248e-11 - val_loss: 3.8233e-11
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8235e-11 - val_loss: 3.8220e-11
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8222e-11 - val_loss: 3.8210e-11
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8212e-11 - val_loss: 3.8195e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8198e-11 - val_loss: 3.8183e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8186e-11 - val_loss: 3.8171e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8172e-11 - val_loss: 3.8157e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8159e-11 - val_loss: 3.8144e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8146e-11 - val_loss: 3.8128e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8134e-11 - val_loss: 3.8121e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8121e-11 - val_loss: 3.8105e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8108e-11 - val_loss: 3.8094e-11
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8096e-11 - val_loss: 3.8081e-11
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8082e-11 - val_loss: 3.8067e-11
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8068e-11 - val_loss: 3.8055e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8057e-11 - val_loss: 3.8041e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8043e-11 - val_loss: 3.8030e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8030e-11 - val_loss: 3.8018e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8021e-11 - val_loss: 3.8002e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8006e-11 - val_loss: 3.7995e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7993e-11 - val_loss: 3.7978e-11
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7981e-11 - val_loss: 3.7967e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7969e-11 - val_loss: 3.7955e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7956e-11 - val_loss: 3.7941e-11
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7943e-11 - val_loss: 3.7928e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7929e-11 - val_loss: 3.7915e-11
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7915e-11 - val_loss: 3.7901e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7900e-11 - val_loss: 3.7884e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7885e-11 - val_loss: 3.7874e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7872e-11 - val_loss: 3.7862e-11
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7862e-11 - val_loss: 3.7849e-11
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7847e-11 - val_loss: 3.7834e-11
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7836e-11 - val_loss: 3.7825e-11
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7824e-11 - val_loss: 3.7809e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7810e-11 - val_loss: 3.7799e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7800e-11 - val_loss: 3.7788e-11
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7788e-11 - val_loss: 3.7773e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7774e-11 - val_loss: 3.7763e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7765e-11 - val_loss: 3.7752e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7752e-11 - val_loss: 3.7742e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7740e-11 - val_loss: 3.7728e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7725e-11 - val_loss: 3.7711e-11
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7713e-11 - val_loss: 3.7698e-11
Epoch 357/512

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7701e-11 - val_loss: 3.7688e-11
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7691e-11 - val_loss: 3.7679e-11
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7678e-11 - val_loss: 3.7666e-11
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7665e-11 - val_loss: 3.7653e-11
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7654e-11 - val_loss: 3.7638e-11
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7641e-11 - val_loss: 3.7627e-11
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7630e-11 - val_loss: 3.7618e-11
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7619e-11 - val_loss: 3.7607e-11
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7606e-11 - val_loss: 3.7594e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7592e-11 - val_loss: 3.7580e-11
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7582e-11 - val_loss: 3.7569e-11
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7569e-11 - val_loss: 3.7555e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7557e-11 - val_loss: 3.7540e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7541e-11 - val_loss: 3.7524e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7526e-11 - val_loss: 3.7514e-11
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7517e-11 - val_loss: 3.7500e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7502e-11 - val_loss: 3.7488e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7491e-11 - val_loss: 3.7477e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7479e-11 - val_loss: 3.7466e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7466e-11 - val_loss: 3.7454e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7455e-11 - val_loss: 3.7440e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7442e-11 - val_loss: 3.7430e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7430e-11 - val_loss: 3.7415e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7419e-11 - val_loss: 3.7406e-11
Epoch 381/512

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7406e-11 - val_loss: 3.7393e-11
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7393e-11 - val_loss: 3.7375e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7377e-11 - val_loss: 3.7365e-11
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7366e-11 - val_loss: 3.7354e-11
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7353e-11 - val_loss: 3.7341e-11
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7339e-11 - val_loss: 3.7322e-11
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7324e-11 - val_loss: 3.7312e-11
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7313e-11 - val_loss: 3.7301e-11
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7302e-11 - val_loss: 3.7292e-11
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7291e-11 - val_loss: 3.7276e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7277e-11 - val_loss: 3.7266e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7266e-11 - val_loss: 3.7253e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7254e-11 - val_loss: 3.7245e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7244e-11 - val_loss: 3.7232e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7232e-11 - val_loss: 3.7218e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7219e-11 - val_loss: 3.7207e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7208e-11 - val_loss: 3.7193e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7195e-11 - val_loss: 3.7185e-11
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7185e-11 - val_loss: 3.7172e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7173e-11 - val_loss: 3.7160e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7160e-11 - val_loss: 3.7148e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7148e-11 - val_loss: 3.7135e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7135e-11 - val_loss: 3.7124e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7123e-11 - val_loss: 3.7111e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7111e-11 - val_loss: 3.7100e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7101e-11 - val_loss: 3.7090e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7089e-11 - val_loss: 3.7075e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7076e-11 - val_loss: 3.7066e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7066e-11 - val_loss: 3.7050e-11
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7053e-11 - val_loss: 3.7040e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7039e-11 - val_loss: 3.7026e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7027e-11 - val_loss: 3.7015e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7013e-11 - val_loss: 3.7002e-11
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7004e-11 - val_loss: 3.6992e-11
Epoch 415/512

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6992e-11 - val_loss: 3.6979e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6980e-11 - val_loss: 3.6965e-11
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6966e-11 - val_loss: 3.6953e-11
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6953e-11 - val_loss: 3.6940e-11
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6943e-11 - val_loss: 3.6929e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6931e-11 - val_loss: 3.6917e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6919e-11 - val_loss: 3.6908e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6908e-11 - val_loss: 3.6895e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6896e-11 - val_loss: 3.6885e-11
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6882e-11 - val_loss: 3.6869e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6871e-11 - val_loss: 3.6855e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6856e-11 - val_loss: 3.6846e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6847e-11 - val_loss: 3.6832e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6834e-11 - val_loss: 3.6823e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6823e-11 - val_loss: 3.6809e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6810e-11 - val_loss: 3.6793e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6795e-11 - val_loss: 3.6783e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6785e-11 - val_loss: 3.6769e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6771e-11 - val_loss: 3.6758e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6760e-11 - val_loss: 3.6744e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6747e-11 - val_loss: 3.6735e-11
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6735e-11 - val_loss: 3.6724e-11
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6724e-11 - val_loss: 3.6714e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6714e-11 - val_loss: 3.6700e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6700e-11 - val_loss: 3.6689e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6689e-11 - val_loss: 3.6677e-11
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6678e-11 - val_loss: 3.6665e-11
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6666e-11 - val_loss: 3.6655e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6656e-11 - val_loss: 3.6643e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6643e-11 - val_loss: 3.6631e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6632e-11 - val_loss: 3.6622e-11
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6622e-11 - val_loss: 3.6610e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6611e-11 - val_loss: 3.6599e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6600e-11 - val_loss: 3.6586e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6586e-11 - val_loss: 3.6572e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6574e-11 - val_loss: 3.6563e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6560e-11 - val_loss: 3.6549e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6551e-11 - val_loss: 3.6543e-11
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6538e-11 - val_loss: 3.6523e-11
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6524e-11 - val_loss: 3.6514e-11
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6513e-11 - val_loss: 3.6505e-11
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6501e-11 - val_loss: 3.6490e-11
Epoch 457/512

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6491e-11 - val_loss: 3.6479e-11
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6478e-11 - val_loss: 3.6470e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6468e-11 - val_loss: 3.6459e-11
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6457e-11 - val_loss: 3.6444e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6444e-11 - val_loss: 3.6434e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6432e-11 - val_loss: 3.6423e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6422e-11 - val_loss: 3.6413e-11
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6413e-11 - val_loss: 3.6400e-11
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6400e-11 - val_loss: 3.6386e-11
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6385e-11 - val_loss: 3.6374e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6375e-11 - val_loss: 3.6362e-11
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6362e-11 - val_loss: 3.6355e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6350e-11 - val_loss: 3.6339e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6340e-11 - val_loss: 3.6327e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6327e-11 - val_loss: 3.6316e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6315e-11 - val_loss: 3.6303e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6303e-11 - val_loss: 3.6289e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6290e-11 - val_loss: 3.6279e-11
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6281e-11 - val_loss: 3.6270e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6269e-11 - val_loss: 3.6259e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6257e-11 - val_loss: 3.6245e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6245e-11 - val_loss: 3.6234e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6234e-11 - val_loss: 3.6222e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6221e-11 - val_loss: 3.6209e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6208e-11 - val_loss: 3.6196e-11
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6195e-11 - val_loss: 3.6185e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6185e-11 - val_loss: 3.6174e-11
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6173e-11 - val_loss: 3.6164e-11
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6164e-11 - val_loss: 3.6152e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6150e-11 - val_loss: 3.6143e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6140e-11 - val_loss: 3.6129e-11
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6129e-11 - val_loss: 3.6119e-11
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6118e-11 - val_loss: 3.6111e-11
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6108e-11 - val_loss: 3.6099e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6097e-11 - val_loss: 3.6089e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6087e-11 - val_loss: 3.6076e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6074e-11 - val_loss: 3.6065e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6063e-11 - val_loss: 3.6051e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6053e-11 - val_loss: 3.6042e-11
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6039e-11 - val_loss: 3.6028e-11
Epoch 497/512

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6027e-11 - val_loss: 3.6014e-11
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6014e-11 - val_loss: 3.6005e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6005e-11 - val_loss: 3.5995e-11
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5992e-11 - val_loss: 3.5981e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5981e-11 - val_loss: 3.5973e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5970e-11 - val_loss: 3.5958e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5959e-11 - val_loss: 3.5949e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5949e-11 - val_loss: 3.5936e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5937e-11 - val_loss: 3.5928e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5926e-11 - val_loss: 3.5917e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5915e-11 - val_loss: 3.5907e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5904e-11 - val_loss: 3.5894e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5893e-11 - val_loss: 3.5886e-11
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5881e-11 - val_loss: 3.5871e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5870e-11 - val_loss: 3.5857e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5856e-11 - val_loss: 3.5850e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.089 | eve: 8.991 | bob: 9.030Epoch   0:   0% | abe: 9.107 | eve: 9.012 | bob: 9.048Epoch   0:   1% | abe: 9.097 | eve: 9.015 | bob: 9.038Epoch   0:   2% | abe: 9.093 | eve: 9.011 | bob: 9.034Epoch   0:   3% | abe: 9.094 | eve: 9.012 | bob: 9.036Epoch   0:   3% | abe: 9.091 | eve: 9.009 | bob: 9.033Epoch   0:   4% | abe: 9.092 | eve: 9.007 | bob: 9.034Epoch   0:   5% | abe: 9.089 | eve: 9.006 | bob: 9.030Epoch   0:   6% | abe: 9.085 | eve: 9.002 | bob: 9.026Epoch   0:   7% | abe: 9.086 | eve: 9.000 | bob: 9.028Epoch   0:   7% | abe: 9.086 | eve: 9.000 | bob: 9.027Epoch   0:   8% | abe: 9.087 | eve: 8.997 | bob: 9.029Epoch   0:   9% | abe: 9.087 | eve: 8.997 | bob: 9.029Epoch   0:  10% | abe: 9.087 | eve: 8.995 | bob: 9.029Epoch   0:  10% | abe: 9.085 | eve: 8.993 | bob: 9.027Epoch   0:  11% | abe: 9.085 | eve: 8.995 | bob: 9.027Epoch   0:  12% | abe: 9.085 | eve: 8.994 | bob: 9.026Epoch   0:  13% | abe: 9.084 | eve: 8.995 | bob: 9.025Epoch   0:  14% | abe: 9.083 | eve: 8.995 | bob: 9.024Epoch   0:  14% | abe: 9.085 | eve: 8.997 | bob: 9.026Epoch   0:  15% | abe: 9.084 | eve: 8.996 | bob: 9.025Epoch   0:  16% | abe: 9.083 | eve: 8.996 | bob: 9.024Epoch   0:  17% | abe: 9.082 | eve: 8.998 | bob: 9.023Epoch   0:  17% | abe: 9.083 | eve: 8.998 | bob: 9.024Epoch   0:  18% | abe: 9.082 | eve: 9.000 | bob: 9.023Epoch   0:  19% | abe: 9.081 | eve: 8.999 | bob: 9.022Epoch   0:  20% | abe: 9.081 | eve: 9.000 | bob: 9.022Epoch   0:  21% | abe: 9.082 | eve: 9.000 | bob: 9.023Epoch   0:  21% | abe: 9.082 | eve: 9.000 | bob: 9.023Epoch   0:  22% | abe: 9.082 | eve: 9.002 | bob: 9.024Epoch   0:  23% | abe: 9.081 | eve: 9.001 | bob: 9.024Epoch   0:  24% | abe: 9.081 | eve: 9.001 | bob: 9.023Epoch   0:  25% | abe: 9.080 | eve: 9.001 | bob: 9.023Epoch   0:  25% | abe: 9.080 | eve: 9.000 | bob: 9.023Epoch   0:  26% | abe: 9.080 | eve: 9.001 | bob: 9.024Epoch   0:  27% | abe: 9.079 | eve: 9.001 | bob: 9.024Epoch   0:  28% | abe: 9.079 | eve: 9.001 | bob: 9.024Epoch   0:  28% | abe: 9.079 | eve: 9.002 | bob: 9.025Epoch   0:  29% | abe: 9.080 | eve: 9.002 | bob: 9.025Epoch   0:  30% | abe: 9.080 | eve: 9.002 | bob: 9.026Epoch   0:  31% | abe: 9.080 | eve: 9.002 | bob: 9.026Epoch   0:  32% | abe: 9.080 | eve: 9.002 | bob: 9.027Epoch   0:  32% | abe: 9.079 | eve: 9.001 | bob: 9.026Epoch   0:  33% | abe: 9.079 | eve: 9.002 | bob: 9.026Epoch   0:  34% | abe: 9.079 | eve: 9.002 | bob: 9.027Epoch   0:  35% | abe: 9.079 | eve: 9.001 | bob: 9.027Epoch   0:  35% | abe: 9.080 | eve: 9.001 | bob: 9.028Epoch   0:  36% | abe: 9.080 | eve: 9.001 | bob: 9.029Epoch   0:  37% | abe: 9.080 | eve: 9.000 | bob: 9.029Epoch   0:  38% | abe: 9.080 | eve: 9.000 | bob: 9.029Epoch   0:  39% | abe: 9.080 | eve: 8.999 | bob: 9.029Epoch   0:  39% | abe: 9.080 | eve: 9.000 | bob: 9.030Epoch   0:  40% | abe: 9.080 | eve: 8.999 | bob: 9.030Epoch   0:  41% | abe: 9.079 | eve: 8.999 | bob: 9.029Epoch   0:  42% | abe: 9.079 | eve: 8.999 | bob: 9.030Epoch   0:  42% | abe: 9.079 | eve: 8.999 | bob: 9.030Epoch   0:  43% | abe: 9.079 | eve: 8.999 | bob: 9.030Epoch   0:  44% | abe: 9.079 | eve: 8.999 | bob: 9.031Epoch   0:  45% | abe: 9.079 | eve: 9.000 | bob: 9.031Epoch   0:  46% | abe: 9.079 | eve: 8.999 | bob: 9.031Epoch   0:  46% | abe: 9.079 | eve: 8.999 | bob: 9.031Epoch   0:  47% | abe: 9.078 | eve: 8.999 | bob: 9.032Epoch   0:  48% | abe: 9.078 | eve: 9.000 | bob: 9.032Epoch   0:  49% | abe: 9.078 | eve: 9.000 | bob: 9.033Epoch   0:  50% | abe: 9.078 | eve: 8.999 | bob: 9.033Epoch   0:  50% | abe: 9.078 | eve: 8.999 | bob: 9.033Epoch   0:  51% | abe: 9.079 | eve: 8.999 | bob: 9.034Epoch   0:  52% | abe: 9.078 | eve: 9.000 | bob: 9.034Epoch   0:  53% | abe: 9.078 | eve: 9.000 | bob: 9.034Epoch   0:  53% | abe: 9.078 | eve: 8.999 | bob: 9.034Epoch   0:  54% | abe: 9.078 | eve: 8.999 | bob: 9.034Epoch   0:  55% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  56% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  57% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  57% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  58% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  59% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  60% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  60% | abe: 9.078 | eve: 8.999 | bob: 9.035Epoch   0:  61% | abe: 9.078 | eve: 8.998 | bob: 9.036Epoch   0:  62% | abe: 9.078 | eve: 8.998 | bob: 9.036Epoch   0:  63% | abe: 9.078 | eve: 8.998 | bob: 9.035Epoch   0:  64% | abe: 9.078 | eve: 8.998 | bob: 9.035Epoch   0:  64% | abe: 9.077 | eve: 8.998 | bob: 9.035Epoch   0:  65% | abe: 9.077 | eve: 8.998 | bob: 9.034Epoch   0:  66% | abe: 9.077 | eve: 8.999 | bob: 9.034Epoch   0:  67% | abe: 9.077 | eve: 8.999 | bob: 9.034Epoch   0:  67% | abe: 9.076 | eve: 8.999 | bob: 9.033Epoch   0:  68% | abe: 9.076 | eve: 8.999 | bob: 9.033Epoch   0:  69% | abe: 9.075 | eve: 8.999 | bob: 9.032Epoch   0:  70% | abe: 9.075 | eve: 8.999 | bob: 9.032Epoch   0:  71% | abe: 9.075 | eve: 8.999 | bob: 9.032Epoch   0:  71% | abe: 9.075 | eve: 8.999 | bob: 9.032Epoch   0:  72% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  73% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  74% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  75% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  75% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  76% | abe: 9.074 | eve: 8.998 | bob: 9.031Epoch   0:  77% | abe: 9.073 | eve: 8.998 | bob: 9.031Epoch   0:  78% | abe: 9.073 | eve: 8.998 | bob: 9.030Epoch   0:  78% | abe: 9.073 | eve: 8.998 | bob: 9.030Epoch   0:  79% | abe: 9.073 | eve: 8.998 | bob: 9.031Epoch   0:  80% | abe: 9.073 | eve: 8.998 | bob: 9.031Epoch   0:  81% | abe: 9.073 | eve: 8.998 | bob: 9.031Epoch   0:  82% | abe: 9.072 | eve: 8.998 | bob: 9.030Epoch   0:  82% | abe: 9.072 | eve: 8.998 | bob: 9.030Epoch   0:  83% | abe: 9.072 | eve: 8.998 | bob: 9.030Epoch   0:  84% | abe: 9.071 | eve: 8.998 | bob: 9.030Epoch   0:  85% | abe: 9.071 | eve: 8.998 | bob: 9.029Epoch   0:  85% | abe: 9.070 | eve: 8.998 | bob: 9.029Epoch   0:  86% | abe: 9.070 | eve: 8.998 | bob: 9.029Epoch   0:  87% | abe: 9.070 | eve: 8.998 | bob: 9.029Epoch   0:  88% | abe: 9.070 | eve: 8.998 | bob: 9.029Epoch   0:  89% | abe: 9.069 | eve: 8.998 | bob: 9.029Epoch   0:  89% | abe: 9.069 | eve: 8.998 | bob: 9.029Epoch   0:  90% | abe: 9.069 | eve: 8.997 | bob: 9.029Epoch   0:  91% | abe: 9.069 | eve: 8.998 | bob: 9.029Epoch   0:  92% | abe: 9.068 | eve: 8.997 | bob: 9.028Epoch   0:  92% | abe: 9.068 | eve: 8.998 | bob: 9.028Epoch   0:  93% | abe: 9.068 | eve: 8.998 | bob: 9.028Epoch   0:  94% | abe: 9.067 | eve: 8.998 | bob: 9.028Epoch   0:  95% | abe: 9.067 | eve: 8.998 | bob: 9.027Epoch   0:  96% | abe: 9.066 | eve: 8.998 | bob: 9.027Epoch   0:  96% | abe: 9.066 | eve: 8.998 | bob: 9.027Epoch   0:  97% | abe: 9.066 | eve: 8.998 | bob: 9.027Epoch   0:  98% | abe: 9.066 | eve: 8.998 | bob: 9.027Epoch   0:  99% | abe: 9.066 | eve: 8.998 | bob: 9.027
New best Bob loss 9.026882886658996 at epoch 0
Epoch   1:   0% | abe: 9.069 | eve: 8.948 | bob: 9.030Epoch   1:   0% | abe: 9.046 | eve: 8.983 | bob: 9.005Epoch   1:   1% | abe: 9.031 | eve: 8.982 | bob: 8.989Epoch   1:   2% | abe: 9.025 | eve: 8.984 | bob: 8.983Epoch   1:   3% | abe: 9.023 | eve: 8.988 | bob: 8.983Epoch   1:   3% | abe: 9.023 | eve: 8.984 | bob: 8.985Epoch   1:   4% | abe: 9.019 | eve: 8.988 | bob: 8.982Epoch   1:   5% | abe: 9.022 | eve: 8.988 | bob: 8.986Epoch   1:   6% | abe: 9.022 | eve: 8.993 | bob: 8.986Epoch   1:   7% | abe: 9.025 | eve: 8.996 | bob: 8.989Epoch   1:   7% | abe: 9.027 | eve: 8.998 | bob: 8.991Epoch   1:   8% | abe: 9.027 | eve: 8.999 | bob: 8.990Epoch   1:   9% | abe: 9.027 | eve: 9.001 | bob: 8.990Epoch   1:  10% | abe: 9.024 | eve: 9.001 | bob: 8.987Epoch   1:  10% | abe: 9.020 | eve: 9.003 | bob: 8.983Epoch   1:  11% | abe: 9.018 | eve: 9.003 | bob: 8.981Epoch   1:  12% | abe: 9.017 | eve: 9.003 | bob: 8.980Epoch   1:  13% | abe: 9.018 | eve: 9.003 | bob: 8.982Epoch   1:  14% | abe: 9.019 | eve: 9.003 | bob: 8.983Epoch   1:  14% | abe: 9.020 | eve: 9.004 | bob: 8.985Epoch   1:  15% | abe: 9.019 | eve: 9.005 | bob: 8.984Epoch   1:  16% | abe: 9.020 | eve: 9.006 | bob: 8.985Epoch   1:  17% | abe: 9.020 | eve: 9.007 | bob: 8.986Epoch   1:  17% | abe: 9.019 | eve: 9.006 | bob: 8.985Epoch   1:  18% | abe: 9.019 | eve: 9.006 | bob: 8.986Epoch   1:  19% | abe: 9.020 | eve: 9.005 | bob: 8.987Epoch   1:  20% | abe: 9.019 | eve: 9.006 | bob: 8.987Epoch   1:  21% | abe: 9.020 | eve: 9.006 | bob: 8.987Epoch   1:  21% | abe: 9.019 | eve: 9.006 | bob: 8.986Epoch   1:  22% | abe: 9.019 | eve: 9.004 | bob: 8.986Epoch   1:  23% | abe: 9.018 | eve: 9.005 | bob: 8.986Epoch   1:  24% | abe: 9.016 | eve: 9.006 | bob: 8.985Epoch   1:  25% | abe: 9.017 | eve: 9.006 | bob: 8.985Epoch   1:  25% | abe: 9.016 | eve: 9.006 | bob: 8.985Epoch   1:  26% | abe: 9.015 | eve: 9.006 | bob: 8.984Epoch   1:  27% | abe: 9.014 | eve: 9.006 | bob: 8.983Epoch   1:  28% | abe: 9.013 | eve: 9.005 | bob: 8.982Epoch   1:  28% | abe: 9.013 | eve: 9.005 | bob: 8.981Epoch   1:  29% | abe: 9.013 | eve: 9.004 | bob: 8.981Epoch   1:  30% | abe: 9.012 | eve: 9.004 | bob: 8.981Epoch   1:  31% | abe: 9.011 | eve: 9.003 | bob: 8.980Epoch   1:  32% | abe: 9.010 | eve: 9.004 | bob: 8.979Epoch   1:  32% | abe: 9.009 | eve: 9.003 | bob: 8.977Epoch   1:  33% | abe: 9.007 | eve: 9.003 | bob: 8.976Epoch   1:  34% | abe: 9.007 | eve: 9.003 | bob: 8.975Epoch   1:  35% | abe: 9.007 | eve: 9.002 | bob: 8.975Epoch   1:  35% | abe: 9.006 | eve: 9.001 | bob: 8.974Epoch   1:  36% | abe: 9.004 | eve: 9.002 | bob: 8.974Epoch   1:  37% | abe: 9.003 | eve: 9.002 | bob: 8.972Epoch   1:  38% | abe: 9.002 | eve: 9.002 | bob: 8.972Epoch   1:  39% | abe: 9.001 | eve: 9.001 | bob: 8.971Epoch   1:  39% | abe: 9.000 | eve: 9.001 | bob: 8.970Epoch   1:  40% | abe: 8.999 | eve: 9.001 | bob: 8.970Epoch   1:  41% | abe: 8.999 | eve: 9.001 | bob: 8.970Epoch   1:  42% | abe: 8.999 | eve: 9.000 | bob: 8.970Epoch   1:  42% | abe: 8.998 | eve: 9.001 | bob: 8.969Epoch   1:  43% | abe: 8.997 | eve: 9.001 | bob: 8.968Epoch   1:  44% | abe: 8.996 | eve: 9.001 | bob: 8.967Epoch   1:  45% | abe: 8.995 | eve: 9.002 | bob: 8.965Epoch   1:  46% | abe: 8.994 | eve: 9.001 | bob: 8.964Epoch   1:  46% | abe: 8.993 | eve: 9.001 | bob: 8.963Epoch   1:  47% | abe: 8.992 | eve: 9.000 | bob: 8.962Epoch   1:  48% | abe: 8.991 | eve: 9.000 | bob: 8.961Epoch   1:  49% | abe: 8.990 | eve: 9.001 | bob: 8.960Epoch   1:  50% | abe: 8.990 | eve: 9.001 | bob: 8.959Epoch   1:  50% | abe: 8.989 | eve: 9.001 | bob: 8.959Epoch   1:  51% | abe: 8.988 | eve: 9.001 | bob: 8.958Epoch   1:  52% | abe: 8.987 | eve: 9.001 | bob: 8.956Epoch   1:  53% | abe: 8.985 | eve: 9.001 | bob: 8.955Epoch   1:  53% | abe: 8.984 | eve: 9.001 | bob: 8.954Epoch   1:  54% | abe: 8.983 | eve: 9.000 | bob: 8.952Epoch   1:  55% | abe: 8.982 | eve: 9.000 | bob: 8.951Epoch   1:  56% | abe: 8.981 | eve: 9.000 | bob: 8.950Epoch   1:  57% | abe: 8.981 | eve: 9.000 | bob: 8.949Epoch   1:  57% | abe: 8.979 | eve: 9.000 | bob: 8.948Epoch   1:  58% | abe: 8.979 | eve: 9.000 | bob: 8.947Epoch   1:  59% | abe: 8.977 | eve: 9.000 | bob: 8.946Epoch   1:  60% | abe: 8.976 | eve: 9.000 | bob: 8.945Epoch   1:  60% | abe: 8.975 | eve: 9.000 | bob: 8.944Epoch   1:  61% | abe: 8.975 | eve: 9.000 | bob: 8.943Epoch   1:  62% | abe: 8.974 | eve: 9.000 | bob: 8.942Epoch   1:  63% | abe: 8.973 | eve: 9.000 | bob: 8.941Epoch   1:  64% | abe: 8.972 | eve: 9.000 | bob: 8.940Epoch   1:  64% | abe: 8.971 | eve: 8.999 | bob: 8.938Epoch   1:  65% | abe: 8.970 | eve: 9.000 | bob: 8.937Epoch   1:  66% | abe: 8.969 | eve: 9.000 | bob: 8.936Epoch   1:  67% | abe: 8.968 | eve: 9.000 | bob: 8.935Epoch   1:  67% | abe: 8.967 | eve: 8.999 | bob: 8.934Epoch   1:  68% | abe: 8.966 | eve: 8.999 | bob: 8.933Epoch   1:  69% | abe: 8.965 | eve: 8.999 | bob: 8.932Epoch   1:  70% | abe: 8.964 | eve: 8.999 | bob: 8.931Epoch   1:  71% | abe: 8.964 | eve: 8.999 | bob: 8.930Epoch   1:  71% | abe: 8.962 | eve: 8.999 | bob: 8.929Epoch   1:  72% | abe: 8.961 | eve: 8.999 | bob: 8.927Epoch   1:  73% | abe: 8.960 | eve: 8.999 | bob: 8.926Epoch   1:  74% | abe: 8.959 | eve: 8.999 | bob: 8.924Epoch   1:  75% | abe: 8.958 | eve: 8.999 | bob: 8.924Epoch   1:  75% | abe: 8.957 | eve: 8.999 | bob: 8.922Epoch   1:  76% | abe: 8.956 | eve: 8.999 | bob: 8.921Epoch   1:  77% | abe: 8.955 | eve: 8.999 | bob: 8.920Epoch   1:  78% | abe: 8.954 | eve: 8.999 | bob: 8.919Epoch   1:  78% | abe: 8.953 | eve: 8.999 | bob: 8.918Epoch   1:  79% | abe: 8.952 | eve: 8.999 | bob: 8.917Epoch   1:  80% | abe: 8.950 | eve: 8.999 | bob: 8.915Epoch   1:  81% | abe: 8.949 | eve: 8.999 | bob: 8.914Epoch   1:  82% | abe: 8.948 | eve: 8.999 | bob: 8.913Epoch   1:  82% | abe: 8.947 | eve: 8.999 | bob: 8.912Epoch   1:  83% | abe: 8.946 | eve: 8.999 | bob: 8.910Epoch   1:  84% | abe: 8.945 | eve: 8.999 | bob: 8.909Epoch   1:  85% | abe: 8.944 | eve: 8.999 | bob: 8.908Epoch   1:  85% | abe: 8.943 | eve: 9.000 | bob: 8.907Epoch   1:  86% | abe: 8.942 | eve: 9.000 | bob: 8.906Epoch   1:  87% | abe: 8.941 | eve: 8.999 | bob: 8.904Epoch   1:  88% | abe: 8.940 | eve: 9.000 | bob: 8.903Epoch   1:  89% | abe: 8.939 | eve: 9.000 | bob: 8.902Epoch   1:  89% | abe: 8.937 | eve: 9.000 | bob: 8.901Epoch   1:  90% | abe: 8.936 | eve: 9.000 | bob: 8.899Epoch   1:  91% | abe: 8.935 | eve: 9.000 | bob: 8.898Epoch   1:  92% | abe: 8.933 | eve: 9.000 | bob: 8.896Epoch   1:  92% | abe: 8.932 | eve: 9.000 | bob: 8.895Epoch   1:  93% | abe: 8.931 | eve: 9.000 | bob: 8.894Epoch   1:  94% | abe: 8.930 | eve: 9.000 | bob: 8.892Epoch   1:  95% | abe: 8.928 | eve: 9.000 | bob: 8.891Epoch   1:  96% | abe: 8.927 | eve: 9.000 | bob: 8.890Epoch   1:  96% | abe: 8.926 | eve: 9.000 | bob: 8.888Epoch   1:  97% | abe: 8.925 | eve: 9.000 | bob: 8.887Epoch   1:  98% | abe: 8.923 | eve: 9.000 | bob: 8.885Epoch   1:  99% | abe: 8.922 | eve: 9.000 | bob: 8.884
New best Bob loss 8.883952386620535 at epoch 1
Epoch   2:   0% | abe: 8.757 | eve: 9.029 | bob: 8.712Epoch   2:   0% | abe: 8.747 | eve: 9.010 | bob: 8.703Epoch   2:   1% | abe: 8.739 | eve: 9.008 | bob: 8.692Epoch   2:   2% | abe: 8.741 | eve: 9.012 | bob: 8.692Epoch   2:   3% | abe: 8.741 | eve: 9.001 | bob: 8.691Epoch   2:   3% | abe: 8.740 | eve: 8.997 | bob: 8.690Epoch   2:   4% | abe: 8.742 | eve: 8.994 | bob: 8.692Epoch   2:   5% | abe: 8.739 | eve: 8.996 | bob: 8.689Epoch   2:   6% | abe: 8.738 | eve: 8.993 | bob: 8.688Epoch   2:   7% | abe: 8.738 | eve: 8.998 | bob: 8.689Epoch   2:   7% | abe: 8.735 | eve: 8.995 | bob: 8.687Epoch   2:   8% | abe: 8.732 | eve: 8.993 | bob: 8.683Epoch   2:   9% | abe: 8.732 | eve: 8.995 | bob: 8.683Epoch   2:  10% | abe: 8.730 | eve: 8.996 | bob: 8.681Epoch   2:  10% | abe: 8.727 | eve: 8.993 | bob: 8.678Epoch   2:  11% | abe: 8.724 | eve: 8.993 | bob: 8.676Epoch   2:  12% | abe: 8.725 | eve: 8.993 | bob: 8.678Epoch   2:  13% | abe: 8.723 | eve: 8.994 | bob: 8.675Epoch   2:  14% | abe: 8.720 | eve: 8.994 | bob: 8.672Epoch   2:  14% | abe: 8.718 | eve: 8.994 | bob: 8.669Epoch   2:  15% | abe: 8.717 | eve: 8.995 | bob: 8.669Epoch   2:  16% | abe: 8.716 | eve: 8.996 | bob: 8.667Epoch   2:  17% | abe: 8.714 | eve: 8.996 | bob: 8.664Epoch   2:  17% | abe: 8.711 | eve: 8.997 | bob: 8.662Epoch   2:  18% | abe: 8.709 | eve: 8.998 | bob: 8.660Epoch   2:  19% | abe: 8.707 | eve: 8.998 | bob: 8.659Epoch   2:  20% | abe: 8.705 | eve: 8.998 | bob: 8.658Epoch   2:  21% | abe: 8.702 | eve: 8.999 | bob: 8.655Epoch   2:  21% | abe: 8.701 | eve: 8.999 | bob: 8.654Epoch   2:  22% | abe: 8.700 | eve: 8.999 | bob: 8.653Epoch   2:  23% | abe: 8.697 | eve: 8.999 | bob: 8.650Epoch   2:  24% | abe: 8.693 | eve: 9.000 | bob: 8.647Epoch   2:  25% | abe: 8.691 | eve: 9.000 | bob: 8.645Epoch   2:  25% | abe: 8.688 | eve: 9.000 | bob: 8.642Epoch   2:  26% | abe: 8.685 | eve: 9.000 | bob: 8.640Epoch   2:  27% | abe: 8.684 | eve: 9.001 | bob: 8.639Epoch   2:  28% | abe: 8.682 | eve: 9.001 | bob: 8.637Epoch   2:  28% | abe: 8.680 | eve: 9.001 | bob: 8.635Epoch   2:  29% | abe: 8.678 | eve: 9.000 | bob: 8.632Epoch   2:  30% | abe: 8.675 | eve: 9.001 | bob: 8.630Epoch   2:  31% | abe: 8.674 | eve: 9.001 | bob: 8.629Epoch   2:  32% | abe: 8.672 | eve: 9.000 | bob: 8.627Epoch   2:  32% | abe: 8.669 | eve: 9.000 | bob: 8.625Epoch   2:  33% | abe: 8.666 | eve: 9.000 | bob: 8.623Epoch   2:  34% | abe: 8.664 | eve: 9.000 | bob: 8.620Epoch   2:  35% | abe: 8.661 | eve: 8.999 | bob: 8.618Epoch   2:  35% | abe: 8.659 | eve: 8.999 | bob: 8.616Epoch   2:  36% | abe: 8.655 | eve: 8.999 | bob: 8.613Epoch   2:  37% | abe: 8.653 | eve: 9.000 | bob: 8.611Epoch   2:  38% | abe: 8.651 | eve: 8.999 | bob: 8.609Epoch   2:  39% | abe: 8.648 | eve: 8.999 | bob: 8.606Epoch   2:  39% | abe: 8.646 | eve: 8.999 | bob: 8.604Epoch   2:  40% | abe: 8.643 | eve: 8.998 | bob: 8.601Epoch   2:  41% | abe: 8.640 | eve: 8.998 | bob: 8.598Epoch   2:  42% | abe: 8.637 | eve: 8.999 | bob: 8.596Epoch   2:  42% | abe: 8.635 | eve: 8.999 | bob: 8.594Epoch   2:  43% | abe: 8.632 | eve: 8.999 | bob: 8.592Epoch   2:  44% | abe: 8.630 | eve: 8.999 | bob: 8.590Epoch   2:  45% | abe: 8.627 | eve: 8.999 | bob: 8.588Epoch   2:  46% | abe: 8.625 | eve: 9.000 | bob: 8.585Epoch   2:  46% | abe: 8.622 | eve: 8.999 | bob: 8.583Epoch   2:  47% | abe: 8.620 | eve: 8.999 | bob: 8.581Epoch   2:  48% | abe: 8.618 | eve: 9.000 | bob: 8.579Epoch   2:  49% | abe: 8.615 | eve: 8.999 | bob: 8.577Epoch   2:  50% | abe: 8.613 | eve: 8.999 | bob: 8.575Epoch   2:  50% | abe: 8.610 | eve: 9.000 | bob: 8.572Epoch   2:  51% | abe: 8.607 | eve: 9.000 | bob: 8.569Epoch   2:  52% | abe: 8.605 | eve: 9.000 | bob: 8.567Epoch   2:  53% | abe: 8.602 | eve: 8.999 | bob: 8.564Epoch   2:  53% | abe: 8.599 | eve: 8.999 | bob: 8.562Epoch   2:  54% | abe: 8.596 | eve: 8.999 | bob: 8.559Epoch   2:  55% | abe: 8.593 | eve: 8.999 | bob: 8.557Epoch   2:  56% | abe: 8.591 | eve: 8.999 | bob: 8.554Epoch   2:  57% | abe: 8.588 | eve: 8.999 | bob: 8.551Epoch   2:  57% | abe: 8.585 | eve: 8.998 | bob: 8.549Epoch   2:  58% | abe: 8.582 | eve: 8.998 | bob: 8.546Epoch   2:  59% | abe: 8.579 | eve: 8.998 | bob: 8.544Epoch   2:  60% | abe: 8.576 | eve: 8.998 | bob: 8.541Epoch   2:  60% | abe: 8.572 | eve: 8.998 | bob: 8.538Epoch   2:  61% | abe: 8.570 | eve: 8.999 | bob: 8.535Epoch   2:  62% | abe: 8.567 | eve: 8.998 | bob: 8.532Epoch   2:  63% | abe: 8.564 | eve: 8.998 | bob: 8.529Epoch   2:  64% | abe: 8.561 | eve: 8.998 | bob: 8.526Epoch   2:  64% | abe: 8.558 | eve: 8.998 | bob: 8.524Epoch   2:  65% | abe: 8.555 | eve: 8.999 | bob: 8.521Epoch   2:  66% | abe: 8.552 | eve: 8.999 | bob: 8.519Epoch   2:  67% | abe: 8.549 | eve: 8.999 | bob: 8.517Epoch   2:  67% | abe: 8.546 | eve: 8.999 | bob: 8.514Epoch   2:  68% | abe: 8.543 | eve: 8.999 | bob: 8.511Epoch   2:  69% | abe: 8.540 | eve: 8.999 | bob: 8.508Epoch   2:  70% | abe: 8.537 | eve: 8.999 | bob: 8.505Epoch   2:  71% | abe: 8.534 | eve: 8.999 | bob: 8.502Epoch   2:  71% | abe: 8.531 | eve: 8.999 | bob: 8.500Epoch   2:  72% | abe: 8.528 | eve: 8.999 | bob: 8.497Epoch   2:  73% | abe: 8.525 | eve: 8.999 | bob: 8.494Epoch   2:  74% | abe: 8.522 | eve: 8.999 | bob: 8.491Epoch   2:  75% | abe: 8.518 | eve: 8.999 | bob: 8.488Epoch   2:  75% | abe: 8.516 | eve: 8.999 | bob: 8.486Epoch   2:  76% | abe: 8.512 | eve: 8.999 | bob: 8.483Epoch   2:  77% | abe: 8.509 | eve: 8.999 | bob: 8.480Epoch   2:  78% | abe: 8.506 | eve: 8.999 | bob: 8.477Epoch   2:  78% | abe: 8.502 | eve: 8.999 | bob: 8.473Epoch   2:  79% | abe: 8.499 | eve: 8.999 | bob: 8.471Epoch   2:  80% | abe: 8.496 | eve: 9.000 | bob: 8.468Epoch   2:  81% | abe: 8.493 | eve: 9.000 | bob: 8.465Epoch   2:  82% | abe: 8.490 | eve: 9.000 | bob: 8.462Epoch   2:  82% | abe: 8.487 | eve: 9.000 | bob: 8.459Epoch   2:  83% | abe: 8.483 | eve: 9.000 | bob: 8.456Epoch   2:  84% | abe: 8.480 | eve: 9.000 | bob: 8.453Epoch   2:  85% | abe: 8.477 | eve: 9.000 | bob: 8.450Epoch   2:  85% | abe: 8.474 | eve: 8.999 | bob: 8.447Epoch   2:  86% | abe: 8.471 | eve: 9.000 | bob: 8.444Epoch   2:  87% | abe: 8.468 | eve: 9.000 | bob: 8.441Epoch   2:  88% | abe: 8.464 | eve: 9.000 | bob: 8.438Epoch   2:  89% | abe: 8.461 | eve: 9.000 | bob: 8.435Epoch   2:  89% | abe: 8.458 | eve: 9.000 | bob: 8.432Epoch   2:  90% | abe: 8.454 | eve: 9.000 | bob: 8.429Epoch   2:  91% | abe: 8.451 | eve: 9.000 | bob: 8.426Epoch   2:  92% | abe: 8.447 | eve: 9.000 | bob: 8.423Epoch   2:  92% | abe: 8.444 | eve: 9.000 | bob: 8.420Epoch   2:  93% | abe: 8.441 | eve: 9.000 | bob: 8.417Epoch   2:  94% | abe: 8.437 | eve: 9.000 | bob: 8.414Epoch   2:  95% | abe: 8.434 | eve: 9.000 | bob: 8.411Epoch   2:  96% | abe: 8.430 | eve: 9.000 | bob: 8.408Epoch   2:  96% | abe: 8.427 | eve: 9.000 | bob: 8.405Epoch   2:  97% | abe: 8.424 | eve: 9.000 | bob: 8.402Epoch   2:  98% | abe: 8.420 | eve: 9.000 | bob: 8.398Epoch   2:  99% | abe: 8.417 | eve: 9.000 | bob: 8.395
New best Bob loss 8.395118414341937 at epoch 2
Epoch   3:   0% | abe: 7.991 | eve: 9.029 | bob: 8.023Epoch   3:   0% | abe: 7.967 | eve: 9.021 | bob: 7.998Epoch   3:   1% | abe: 7.972 | eve: 9.015 | bob: 7.994Epoch   3:   2% | abe: 7.970 | eve: 9.005 | bob: 7.988Epoch   3:   3% | abe: 7.962 | eve: 9.005 | bob: 7.980Epoch   3:   3% | abe: 7.960 | eve: 9.004 | bob: 7.978Epoch   3:   4% | abe: 7.954 | eve: 9.005 | bob: 7.976Epoch   3:   5% | abe: 7.955 | eve: 9.009 | bob: 7.976Epoch   3:   6% | abe: 7.950 | eve: 9.007 | bob: 7.970Epoch   3:   7% | abe: 7.948 | eve: 9.007 | bob: 7.967Epoch   3:   7% | abe: 7.944 | eve: 9.007 | bob: 7.962Epoch   3:   8% | abe: 7.943 | eve: 9.005 | bob: 7.961Epoch   3:   9% | abe: 7.937 | eve: 9.005 | bob: 7.956Epoch   3:  10% | abe: 7.932 | eve: 9.006 | bob: 7.951Epoch   3:  10% | abe: 7.928 | eve: 9.007 | bob: 7.947Epoch   3:  11% | abe: 7.926 | eve: 9.006 | bob: 7.945Epoch   3:  12% | abe: 7.921 | eve: 9.008 | bob: 7.940Epoch   3:  13% | abe: 7.917 | eve: 9.008 | bob: 7.936Epoch   3:  14% | abe: 7.913 | eve: 9.010 | bob: 7.933Epoch   3:  14% | abe: 7.909 | eve: 9.011 | bob: 7.929Epoch   3:  15% | abe: 7.904 | eve: 9.012 | bob: 7.924Epoch   3:  16% | abe: 7.900 | eve: 9.012 | bob: 7.921Epoch   3:  17% | abe: 7.895 | eve: 9.012 | bob: 7.916Epoch   3:  17% | abe: 7.889 | eve: 9.012 | bob: 7.911Epoch   3:  18% | abe: 7.886 | eve: 9.013 | bob: 7.908Epoch   3:  19% | abe: 7.883 | eve: 9.013 | bob: 7.904Epoch   3:  20% | abe: 7.878 | eve: 9.012 | bob: 7.899Epoch   3:  21% | abe: 7.874 | eve: 9.010 | bob: 7.896Epoch   3:  21% | abe: 7.870 | eve: 9.009 | bob: 7.893Epoch   3:  22% | abe: 7.865 | eve: 9.009 | bob: 7.888Epoch   3:  23% | abe: 7.862 | eve: 9.009 | bob: 7.884Epoch   3:  24% | abe: 7.858 | eve: 9.009 | bob: 7.880Epoch   3:  25% | abe: 7.854 | eve: 9.009 | bob: 7.877Epoch   3:  25% | abe: 7.851 | eve: 9.008 | bob: 7.874Epoch   3:  26% | abe: 7.847 | eve: 9.009 | bob: 7.870Epoch   3:  27% | abe: 7.843 | eve: 9.009 | bob: 7.866Epoch   3:  28% | abe: 7.838 | eve: 9.009 | bob: 7.862Epoch   3:  28% | abe: 7.835 | eve: 9.009 | bob: 7.858Epoch   3:  29% | abe: 7.830 | eve: 9.009 | bob: 7.854Epoch   3:  30% | abe: 7.827 | eve: 9.009 | bob: 7.851Epoch   3:  31% | abe: 7.822 | eve: 9.009 | bob: 7.847Epoch   3:  32% | abe: 7.818 | eve: 9.009 | bob: 7.843Epoch   3:  32% | abe: 7.815 | eve: 9.009 | bob: 7.839Epoch   3:  33% | abe: 7.811 | eve: 9.010 | bob: 7.835Epoch   3:  34% | abe: 7.806 | eve: 9.009 | bob: 7.830Epoch   3:  35% | abe: 7.802 | eve: 9.009 | bob: 7.827Epoch   3:  35% | abe: 7.798 | eve: 9.009 | bob: 7.823Epoch   3:  36% | abe: 7.794 | eve: 9.009 | bob: 7.819Epoch   3:  37% | abe: 7.789 | eve: 9.009 | bob: 7.815Epoch   3:  38% | abe: 7.784 | eve: 9.009 | bob: 7.810Epoch   3:  39% | abe: 7.780 | eve: 9.009 | bob: 7.806Epoch   3:  39% | abe: 7.776 | eve: 9.009 | bob: 7.802Epoch   3:  40% | abe: 7.771 | eve: 9.009 | bob: 7.797Epoch   3:  41% | abe: 7.767 | eve: 9.008 | bob: 7.794Epoch   3:  42% | abe: 7.763 | eve: 9.008 | bob: 7.790Epoch   3:  42% | abe: 7.759 | eve: 9.009 | bob: 7.786Epoch   3:  43% | abe: 7.755 | eve: 9.008 | bob: 7.782Epoch   3:  44% | abe: 7.750 | eve: 9.008 | bob: 7.778Epoch   3:  45% | abe: 7.746 | eve: 9.007 | bob: 7.773Epoch   3:  46% | abe: 7.741 | eve: 9.007 | bob: 7.769Epoch   3:  46% | abe: 7.737 | eve: 9.006 | bob: 7.765Epoch   3:  47% | abe: 7.732 | eve: 9.007 | bob: 7.760Epoch   3:  48% | abe: 7.727 | eve: 9.007 | bob: 7.756Epoch   3:  49% | abe: 7.723 | eve: 9.006 | bob: 7.752Epoch   3:  50% | abe: 7.719 | eve: 9.005 | bob: 7.747Epoch   3:  50% | abe: 7.714 | eve: 9.005 | bob: 7.743Epoch   3:  51% | abe: 7.709 | eve: 9.005 | bob: 7.739Epoch   3:  52% | abe: 7.706 | eve: 9.005 | bob: 7.735Epoch   3:  53% | abe: 7.701 | eve: 9.005 | bob: 7.730Epoch   3:  53% | abe: 7.697 | eve: 9.005 | bob: 7.727Epoch   3:  54% | abe: 7.693 | eve: 9.005 | bob: 7.723Epoch   3:  55% | abe: 7.688 | eve: 9.005 | bob: 7.719Epoch   3:  56% | abe: 7.684 | eve: 9.005 | bob: 7.714Epoch   3:  57% | abe: 7.680 | eve: 9.006 | bob: 7.710Epoch   3:  57% | abe: 7.675 | eve: 9.006 | bob: 7.706Epoch   3:  58% | abe: 7.671 | eve: 9.006 | bob: 7.702Epoch   3:  59% | abe: 7.666 | eve: 9.006 | bob: 7.697Epoch   3:  60% | abe: 7.662 | eve: 9.005 | bob: 7.693Epoch   3:  60% | abe: 7.658 | eve: 9.005 | bob: 7.689Epoch   3:  61% | abe: 7.653 | eve: 9.006 | bob: 7.685Epoch   3:  62% | abe: 7.649 | eve: 9.005 | bob: 7.681Epoch   3:  63% | abe: 7.644 | eve: 9.005 | bob: 7.677Epoch   3:  64% | abe: 7.640 | eve: 9.005 | bob: 7.672Epoch   3:  64% | abe: 7.635 | eve: 9.005 | bob: 7.668Epoch   3:  65% | abe: 7.631 | eve: 9.005 | bob: 7.663Epoch   3:  66% | abe: 7.626 | eve: 9.005 | bob: 7.659Epoch   3:  67% | abe: 7.622 | eve: 9.005 | bob: 7.655Epoch   3:  67% | abe: 7.618 | eve: 9.005 | bob: 7.651Epoch   3:  68% | abe: 7.614 | eve: 9.005 | bob: 7.647Epoch   3:  69% | abe: 7.609 | eve: 9.005 | bob: 7.643Epoch   3:  70% | abe: 7.605 | eve: 9.006 | bob: 7.639Epoch   3:  71% | abe: 7.601 | eve: 9.005 | bob: 7.635Epoch   3:  71% | abe: 7.597 | eve: 9.005 | bob: 7.631Epoch   3:  72% | abe: 7.592 | eve: 9.005 | bob: 7.626Epoch   3:  73% | abe: 7.588 | eve: 9.005 | bob: 7.622Epoch   3:  74% | abe: 7.583 | eve: 9.005 | bob: 7.617Epoch   3:  75% | abe: 7.579 | eve: 9.005 | bob: 7.613Epoch   3:  75% | abe: 7.574 | eve: 9.005 | bob: 7.609Epoch   3:  76% | abe: 7.570 | eve: 9.005 | bob: 7.605Epoch   3:  77% | abe: 7.566 | eve: 9.005 | bob: 7.601Epoch   3:  78% | abe: 7.561 | eve: 9.005 | bob: 7.596Epoch   3:  78% | abe: 7.557 | eve: 9.005 | bob: 7.592Epoch   3:  79% | abe: 7.552 | eve: 9.005 | bob: 7.588Epoch   3:  80% | abe: 7.548 | eve: 9.004 | bob: 7.583Epoch   3:  81% | abe: 7.544 | eve: 9.004 | bob: 7.579Epoch   3:  82% | abe: 7.539 | eve: 9.004 | bob: 7.575Epoch   3:  82% | abe: 7.535 | eve: 9.004 | bob: 7.571Epoch   3:  83% | abe: 7.531 | eve: 9.004 | bob: 7.567Epoch   3:  84% | abe: 7.526 | eve: 9.004 | bob: 7.562Epoch   3:  85% | abe: 7.521 | eve: 9.004 | bob: 7.558Epoch   3:  85% | abe: 7.517 | eve: 9.004 | bob: 7.554Epoch   3:  86% | abe: 7.513 | eve: 9.004 | bob: 7.549Epoch   3:  87% | abe: 7.508 | eve: 9.004 | bob: 7.545Epoch   3:  88% | abe: 7.504 | eve: 9.004 | bob: 7.541Epoch   3:  89% | abe: 7.500 | eve: 9.004 | bob: 7.537Epoch   3:  89% | abe: 7.496 | eve: 9.004 | bob: 7.533Epoch   3:  90% | abe: 7.492 | eve: 9.003 | bob: 7.529Epoch   3:  91% | abe: 7.487 | eve: 9.003 | bob: 7.524Epoch   3:  92% | abe: 7.483 | eve: 9.003 | bob: 7.520Epoch   3:  92% | abe: 7.479 | eve: 9.003 | bob: 7.516Epoch   3:  93% | abe: 7.474 | eve: 9.003 | bob: 7.512Epoch   3:  94% | abe: 7.470 | eve: 9.003 | bob: 7.508Epoch   3:  95% | abe: 7.466 | eve: 9.003 | bob: 7.503Epoch   3:  96% | abe: 7.461 | eve: 9.003 | bob: 7.499Epoch   3:  96% | abe: 7.457 | eve: 9.003 | bob: 7.495Epoch   3:  97% | abe: 7.453 | eve: 9.003 | bob: 7.490Epoch   3:  98% | abe: 7.448 | eve: 9.003 | bob: 7.486Epoch   3:  99% | abe: 7.444 | eve: 9.003 | bob: 7.482
New best Bob loss 7.481902563067024 at epoch 3
Epoch   4:   0% | abe: 6.901 | eve: 8.981 | bob: 6.950Epoch   4:   0% | abe: 6.902 | eve: 8.981 | bob: 6.938Epoch   4:   1% | abe: 6.899 | eve: 8.988 | bob: 6.937Epoch   4:   2% | abe: 6.891 | eve: 8.996 | bob: 6.935Epoch   4:   3% | abe: 6.886 | eve: 9.001 | bob: 6.934Epoch   4:   3% | abe: 6.882 | eve: 8.999 | bob: 6.931Epoch   4:   4% | abe: 6.881 | eve: 9.000 | bob: 6.927Epoch   4:   5% | abe: 6.873 | eve: 9.005 | bob: 6.919Epoch   4:   6% | abe: 6.866 | eve: 9.006 | bob: 6.915Epoch   4:   7% | abe: 6.860 | eve: 9.004 | bob: 6.909Epoch   4:   7% | abe: 6.854 | eve: 9.005 | bob: 6.903Epoch   4:   8% | abe: 6.850 | eve: 9.003 | bob: 6.899Epoch   4:   9% | abe: 6.850 | eve: 9.003 | bob: 6.898Epoch   4:  10% | abe: 6.846 | eve: 9.003 | bob: 6.895Epoch   4:  10% | abe: 6.841 | eve: 9.002 | bob: 6.891Epoch   4:  11% | abe: 6.837 | eve: 9.001 | bob: 6.887Epoch   4:  12% | abe: 6.832 | eve: 9.002 | bob: 6.882Epoch   4:  13% | abe: 6.829 | eve: 9.001 | bob: 6.878Epoch   4:  14% | abe: 6.824 | eve: 8.999 | bob: 6.874Epoch   4:  14% | abe: 6.820 | eve: 8.998 | bob: 6.870Epoch   4:  15% | abe: 6.817 | eve: 8.998 | bob: 6.867Epoch   4:  16% | abe: 6.811 | eve: 8.998 | bob: 6.861Epoch   4:  17% | abe: 6.807 | eve: 8.998 | bob: 6.856Epoch   4:  17% | abe: 6.803 | eve: 8.997 | bob: 6.853Epoch   4:  18% | abe: 6.799 | eve: 8.996 | bob: 6.850Epoch   4:  19% | abe: 6.796 | eve: 8.996 | bob: 6.847Epoch   4:  20% | abe: 6.792 | eve: 8.996 | bob: 6.842Epoch   4:  21% | abe: 6.787 | eve: 8.996 | bob: 6.837Epoch   4:  21% | abe: 6.784 | eve: 8.996 | bob: 6.834Epoch   4:  22% | abe: 6.780 | eve: 8.996 | bob: 6.829Epoch   4:  23% | abe: 6.776 | eve: 8.996 | bob: 6.825Epoch   4:  24% | abe: 6.772 | eve: 8.995 | bob: 6.821Epoch   4:  25% | abe: 6.768 | eve: 8.994 | bob: 6.817Epoch   4:  25% | abe: 6.764 | eve: 8.994 | bob: 6.813Epoch   4:  26% | abe: 6.760 | eve: 8.995 | bob: 6.809Epoch   4:  27% | abe: 6.755 | eve: 8.994 | bob: 6.804Epoch   4:  28% | abe: 6.752 | eve: 8.995 | bob: 6.801Epoch   4:  28% | abe: 6.748 | eve: 8.995 | bob: 6.797Epoch   4:  29% | abe: 6.744 | eve: 8.996 | bob: 6.792Epoch   4:  30% | abe: 6.740 | eve: 8.996 | bob: 6.788Epoch   4:  31% | abe: 6.736 | eve: 8.996 | bob: 6.784Epoch   4:  32% | abe: 6.732 | eve: 8.996 | bob: 6.780Epoch   4:  32% | abe: 6.728 | eve: 8.996 | bob: 6.776Epoch   4:  33% | abe: 6.724 | eve: 8.996 | bob: 6.772Epoch   4:  34% | abe: 6.721 | eve: 8.996 | bob: 6.769Epoch   4:  35% | abe: 6.717 | eve: 8.996 | bob: 6.764Epoch   4:  35% | abe: 6.713 | eve: 8.995 | bob: 6.760Epoch   4:  36% | abe: 6.709 | eve: 8.995 | bob: 6.756Epoch   4:  37% | abe: 6.705 | eve: 8.995 | bob: 6.752Epoch   4:  38% | abe: 6.701 | eve: 8.995 | bob: 6.749Epoch   4:  39% | abe: 6.697 | eve: 8.995 | bob: 6.744Epoch   4:  39% | abe: 6.694 | eve: 8.995 | bob: 6.740Epoch   4:  40% | abe: 6.689 | eve: 8.994 | bob: 6.736Epoch   4:  41% | abe: 6.685 | eve: 8.995 | bob: 6.732Epoch   4:  42% | abe: 6.681 | eve: 8.995 | bob: 6.728Epoch   4:  42% | abe: 6.676 | eve: 8.995 | bob: 6.723Epoch   4:  43% | abe: 6.672 | eve: 8.996 | bob: 6.719Epoch   4:  44% | abe: 6.668 | eve: 8.996 | bob: 6.715Epoch   4:  45% | abe: 6.665 | eve: 8.996 | bob: 6.712Epoch   4:  46% | abe: 6.661 | eve: 8.996 | bob: 6.707Epoch   4:  46% | abe: 6.656 | eve: 8.996 | bob: 6.703Epoch   4:  47% | abe: 6.652 | eve: 8.996 | bob: 6.699Epoch   4:  48% | abe: 6.648 | eve: 8.997 | bob: 6.695Epoch   4:  49% | abe: 6.644 | eve: 8.997 | bob: 6.691Epoch   4:  50% | abe: 6.640 | eve: 8.997 | bob: 6.687Epoch   4:  50% | abe: 6.636 | eve: 8.997 | bob: 6.682Epoch   4:  51% | abe: 6.632 | eve: 8.996 | bob: 6.678Epoch   4:  52% | abe: 6.628 | eve: 8.996 | bob: 6.674Epoch   4:  53% | abe: 6.624 | eve: 8.997 | bob: 6.671Epoch   4:  53% | abe: 6.620 | eve: 8.997 | bob: 6.666Epoch   4:  54% | abe: 6.616 | eve: 8.998 | bob: 6.662Epoch   4:  55% | abe: 6.612 | eve: 8.997 | bob: 6.658Epoch   4:  56% | abe: 6.608 | eve: 8.998 | bob: 6.654Epoch   4:  57% | abe: 6.604 | eve: 8.998 | bob: 6.650Epoch   4:  57% | abe: 6.600 | eve: 8.997 | bob: 6.646Epoch   4:  58% | abe: 6.596 | eve: 8.997 | bob: 6.642Epoch   4:  59% | abe: 6.592 | eve: 8.998 | bob: 6.638Epoch   4:  60% | abe: 6.589 | eve: 8.998 | bob: 6.634Epoch   4:  60% | abe: 6.585 | eve: 8.998 | bob: 6.630Epoch   4:  61% | abe: 6.581 | eve: 8.998 | bob: 6.626Epoch   4:  62% | abe: 6.577 | eve: 8.998 | bob: 6.622Epoch   4:  63% | abe: 6.573 | eve: 8.998 | bob: 6.618Epoch   4:  64% | abe: 6.569 | eve: 8.998 | bob: 6.614Epoch   4:  64% | abe: 6.565 | eve: 8.998 | bob: 6.610Epoch   4:  65% | abe: 6.561 | eve: 8.998 | bob: 6.606Epoch   4:  66% | abe: 6.557 | eve: 8.999 | bob: 6.602Epoch   4:  67% | abe: 6.553 | eve: 8.999 | bob: 6.598Epoch   4:  67% | abe: 6.549 | eve: 8.999 | bob: 6.594Epoch   4:  68% | abe: 6.545 | eve: 9.000 | bob: 6.590Epoch   4:  69% | abe: 6.541 | eve: 9.000 | bob: 6.586Epoch   4:  70% | abe: 6.538 | eve: 9.000 | bob: 6.582Epoch   4:  71% | abe: 6.534 | eve: 9.000 | bob: 6.578Epoch   4:  71% | abe: 6.530 | eve: 9.000 | bob: 6.574Epoch   4:  72% | abe: 6.526 | eve: 9.000 | bob: 6.570Epoch   4:  73% | abe: 6.522 | eve: 9.000 | bob: 6.566Epoch   4:  74% | abe: 6.519 | eve: 9.000 | bob: 6.562Epoch   4:  75% | abe: 6.515 | eve: 9.000 | bob: 6.558Epoch   4:  75% | abe: 6.511 | eve: 9.000 | bob: 6.555Epoch   4:  76% | abe: 6.507 | eve: 9.000 | bob: 6.551Epoch   4:  77% | abe: 6.504 | eve: 9.000 | bob: 6.547Epoch   4:  78% | abe: 6.500 | eve: 9.000 | bob: 6.543Epoch   4:  78% | abe: 6.496 | eve: 9.000 | bob: 6.539Epoch   4:  79% | abe: 6.492 | eve: 9.000 | bob: 6.535Epoch   4:  80% | abe: 6.488 | eve: 9.000 | bob: 6.531Epoch   4:  81% | abe: 6.484 | eve: 9.000 | bob: 6.527Epoch   4:  82% | abe: 6.480 | eve: 9.000 | bob: 6.523Epoch   4:  82% | abe: 6.476 | eve: 9.000 | bob: 6.519Epoch   4:  83% | abe: 6.473 | eve: 9.000 | bob: 6.516Epoch   4:  84% | abe: 6.469 | eve: 9.000 | bob: 6.512Epoch   4:  85% | abe: 6.465 | eve: 9.000 | bob: 6.508Epoch   4:  85% | abe: 6.461 | eve: 9.000 | bob: 6.504Epoch   4:  86% | abe: 6.458 | eve: 9.000 | bob: 6.500Epoch   4:  87% | abe: 6.454 | eve: 9.000 | bob: 6.496Epoch   4:  88% | abe: 6.450 | eve: 9.000 | bob: 6.492Epoch   4:  89% | abe: 6.447 | eve: 9.000 | bob: 6.489Epoch   4:  89% | abe: 6.443 | eve: 9.000 | bob: 6.485Epoch   4:  90% | abe: 6.439 | eve: 9.000 | bob: 6.481Epoch   4:  91% | abe: 6.436 | eve: 9.000 | bob: 6.477Epoch   4:  92% | abe: 6.432 | eve: 9.000 | bob: 6.474Epoch   4:  92% | abe: 6.428 | eve: 9.000 | bob: 6.470Epoch   4:  93% | abe: 6.425 | eve: 8.999 | bob: 6.466Epoch   4:  94% | abe: 6.421 | eve: 8.999 | bob: 6.462Epoch   4:  95% | abe: 6.417 | eve: 8.999 | bob: 6.459Epoch   4:  96% | abe: 6.414 | eve: 8.999 | bob: 6.455Epoch   4:  96% | abe: 6.410 | eve: 8.999 | bob: 6.451Epoch   4:  97% | abe: 6.406 | eve: 8.999 | bob: 6.447Epoch   4:  98% | abe: 6.402 | eve: 8.999 | bob: 6.443Epoch   4:  99% | abe: 6.399 | eve: 8.999 | bob: 6.439
New best Bob loss 6.439471336784436 at epoch 4
Epoch   5:   0% | abe: 5.935 | eve: 8.993 | bob: 5.968Epoch   5:   0% | abe: 5.925 | eve: 8.991 | bob: 5.956Epoch   5:   1% | abe: 5.923 | eve: 8.991 | bob: 5.952Epoch   5:   2% | abe: 5.918 | eve: 8.998 | bob: 5.946Epoch   5:   3% | abe: 5.914 | eve: 8.996 | bob: 5.942Epoch   5:   3% | abe: 5.907 | eve: 8.995 | bob: 5.936Epoch   5:   4% | abe: 5.902 | eve: 8.993 | bob: 5.931Epoch   5:   5% | abe: 5.904 | eve: 8.996 | bob: 5.932Epoch   5:   6% | abe: 5.900 | eve: 8.994 | bob: 5.928Epoch   5:   7% | abe: 5.898 | eve: 8.996 | bob: 5.925Epoch   5:   7% | abe: 5.896 | eve: 8.992 | bob: 5.924Epoch   5:   8% | abe: 5.893 | eve: 8.993 | bob: 5.922Epoch   5:   9% | abe: 5.889 | eve: 8.993 | bob: 5.918Epoch   5:  10% | abe: 5.887 | eve: 8.993 | bob: 5.915Epoch   5:  10% | abe: 5.883 | eve: 8.992 | bob: 5.911Epoch   5:  11% | abe: 5.877 | eve: 8.993 | bob: 5.905Epoch   5:  12% | abe: 5.875 | eve: 8.993 | bob: 5.904Epoch   5:  13% | abe: 5.872 | eve: 8.993 | bob: 5.900Epoch   5:  14% | abe: 5.870 | eve: 8.992 | bob: 5.898Epoch   5:  14% | abe: 5.865 | eve: 8.991 | bob: 5.892Epoch   5:  15% | abe: 5.861 | eve: 8.992 | bob: 5.888Epoch   5:  16% | abe: 5.857 | eve: 8.992 | bob: 5.885Epoch   5:  17% | abe: 5.852 | eve: 8.992 | bob: 5.879Epoch   5:  17% | abe: 5.849 | eve: 8.991 | bob: 5.876Epoch   5:  18% | abe: 5.845 | eve: 8.989 | bob: 5.872Epoch   5:  19% | abe: 5.843 | eve: 8.990 | bob: 5.870Epoch   5:  20% | abe: 5.841 | eve: 8.991 | bob: 5.868Epoch   5:  21% | abe: 5.838 | eve: 8.991 | bob: 5.864Epoch   5:  21% | abe: 5.835 | eve: 8.991 | bob: 5.861Epoch   5:  22% | abe: 5.831 | eve: 8.992 | bob: 5.857Epoch   5:  23% | abe: 5.828 | eve: 8.992 | bob: 5.854Epoch   5:  24% | abe: 5.825 | eve: 8.993 | bob: 5.851Epoch   5:  25% | abe: 5.822 | eve: 8.992 | bob: 5.847Epoch   5:  25% | abe: 5.819 | eve: 8.993 | bob: 5.844Epoch   5:  26% | abe: 5.817 | eve: 8.993 | bob: 5.841Epoch   5:  27% | abe: 5.812 | eve: 8.993 | bob: 5.837Epoch   5:  28% | abe: 5.809 | eve: 8.993 | bob: 5.833Epoch   5:  28% | abe: 5.805 | eve: 8.992 | bob: 5.829Epoch   5:  29% | abe: 5.803 | eve: 8.992 | bob: 5.826Epoch   5:  30% | abe: 5.800 | eve: 8.992 | bob: 5.823Epoch   5:  31% | abe: 5.796 | eve: 8.993 | bob: 5.820Epoch   5:  32% | abe: 5.793 | eve: 8.993 | bob: 5.816Epoch   5:  32% | abe: 5.790 | eve: 8.992 | bob: 5.813Epoch   5:  33% | abe: 5.787 | eve: 8.992 | bob: 5.810Epoch   5:  34% | abe: 5.784 | eve: 8.993 | bob: 5.807Epoch   5:  35% | abe: 5.781 | eve: 8.993 | bob: 5.803Epoch   5:  35% | abe: 5.777 | eve: 8.993 | bob: 5.800Epoch   5:  36% | abe: 5.774 | eve: 8.994 | bob: 5.796Epoch   5:  37% | abe: 5.771 | eve: 8.994 | bob: 5.793Epoch   5:  38% | abe: 5.769 | eve: 8.994 | bob: 5.791Epoch   5:  39% | abe: 5.765 | eve: 8.994 | bob: 5.787Epoch   5:  39% | abe: 5.762 | eve: 8.994 | bob: 5.784Epoch   5:  40% | abe: 5.759 | eve: 8.994 | bob: 5.781Epoch   5:  41% | abe: 5.756 | eve: 8.993 | bob: 5.777Epoch   5:  42% | abe: 5.753 | eve: 8.993 | bob: 5.774Epoch   5:  42% | abe: 5.750 | eve: 8.993 | bob: 5.771Epoch   5:  43% | abe: 5.747 | eve: 8.993 | bob: 5.768Epoch   5:  44% | abe: 5.743 | eve: 8.993 | bob: 5.764Epoch   5:  45% | abe: 5.740 | eve: 8.993 | bob: 5.761Epoch   5:  46% | abe: 5.736 | eve: 8.993 | bob: 5.757Epoch   5:  46% | abe: 5.733 | eve: 8.992 | bob: 5.754Epoch   5:  47% | abe: 5.730 | eve: 8.993 | bob: 5.751Epoch   5:  48% | abe: 5.727 | eve: 8.992 | bob: 5.748Epoch   5:  49% | abe: 5.724 | eve: 8.992 | bob: 5.744Epoch   5:  50% | abe: 5.721 | eve: 8.993 | bob: 5.741Epoch   5:  50% | abe: 5.717 | eve: 8.994 | bob: 5.738Epoch   5:  51% | abe: 5.715 | eve: 8.994 | bob: 5.735Epoch   5:  52% | abe: 5.712 | eve: 8.994 | bob: 5.732Epoch   5:  53% | abe: 5.708 | eve: 8.994 | bob: 5.728Epoch   5:  53% | abe: 5.705 | eve: 8.994 | bob: 5.725Epoch   5:  54% | abe: 5.703 | eve: 8.994 | bob: 5.722Epoch   5:  55% | abe: 5.700 | eve: 8.994 | bob: 5.719Epoch   5:  56% | abe: 5.697 | eve: 8.994 | bob: 5.716Epoch   5:  57% | abe: 5.694 | eve: 8.993 | bob: 5.713Epoch   5:  57% | abe: 5.691 | eve: 8.993 | bob: 5.710Epoch   5:  58% | abe: 5.688 | eve: 8.993 | bob: 5.707Epoch   5:  59% | abe: 5.685 | eve: 8.993 | bob: 5.703Epoch   5:  60% | abe: 5.682 | eve: 8.993 | bob: 5.700Epoch   5:  60% | abe: 5.679 | eve: 8.994 | bob: 5.697Epoch   5:  61% | abe: 5.676 | eve: 8.994 | bob: 5.694Epoch   5:  62% | abe: 5.673 | eve: 8.994 | bob: 5.691Epoch   5:  63% | abe: 5.670 | eve: 8.994 | bob: 5.688Epoch   5:  64% | abe: 5.667 | eve: 8.994 | bob: 5.685Epoch   5:  64% | abe: 5.664 | eve: 8.994 | bob: 5.682Epoch   5:  65% | abe: 5.661 | eve: 8.995 | bob: 5.679Epoch   5:  66% | abe: 5.658 | eve: 8.994 | bob: 5.675Epoch   5:  67% | abe: 5.655 | eve: 8.995 | bob: 5.672Epoch   5:  67% | abe: 5.652 | eve: 8.995 | bob: 5.669Epoch   5:  68% | abe: 5.649 | eve: 8.995 | bob: 5.666Epoch   5:  69% | abe: 5.646 | eve: 8.995 | bob: 5.663Epoch   5:  70% | abe: 5.643 | eve: 8.995 | bob: 5.660Epoch   5:  71% | abe: 5.640 | eve: 8.995 | bob: 5.657Epoch   5:  71% | abe: 5.637 | eve: 8.995 | bob: 5.653Epoch   5:  72% | abe: 5.634 | eve: 8.996 | bob: 5.651Epoch   5:  73% | abe: 5.631 | eve: 8.996 | bob: 5.647Epoch   5:  74% | abe: 5.628 | eve: 8.996 | bob: 5.644Epoch   5:  75% | abe: 5.626 | eve: 8.996 | bob: 5.641Epoch   5:  75% | abe: 5.623 | eve: 8.996 | bob: 5.639Epoch   5:  76% | abe: 5.620 | eve: 8.995 | bob: 5.636Epoch   5:  77% | abe: 5.617 | eve: 8.995 | bob: 5.633Epoch   5:  78% | abe: 5.614 | eve: 8.996 | bob: 5.630Epoch   5:  78% | abe: 5.611 | eve: 8.995 | bob: 5.626Epoch   5:  79% | abe: 5.608 | eve: 8.995 | bob: 5.623Epoch   5:  80% | abe: 5.605 | eve: 8.995 | bob: 5.620Epoch   5:  81% | abe: 5.602 | eve: 8.995 | bob: 5.617Epoch   5:  82% | abe: 5.599 | eve: 8.995 | bob: 5.614Epoch   5:  82% | abe: 5.596 | eve: 8.995 | bob: 5.611Epoch   5:  83% | abe: 5.593 | eve: 8.995 | bob: 5.607Epoch   5:  84% | abe: 5.590 | eve: 8.996 | bob: 5.604Epoch   5:  85% | abe: 5.587 | eve: 8.995 | bob: 5.602Epoch   5:  85% | abe: 5.584 | eve: 8.995 | bob: 5.598Epoch   5:  86% | abe: 5.581 | eve: 8.995 | bob: 5.595Epoch   5:  87% | abe: 5.578 | eve: 8.995 | bob: 5.592Epoch   5:  88% | abe: 5.576 | eve: 8.995 | bob: 5.589Epoch   5:  89% | abe: 5.572 | eve: 8.995 | bob: 5.586Epoch   5:  89% | abe: 5.569 | eve: 8.995 | bob: 5.583Epoch   5:  90% | abe: 5.566 | eve: 8.995 | bob: 5.580Epoch   5:  91% | abe: 5.564 | eve: 8.995 | bob: 5.577Epoch   5:  92% | abe: 5.561 | eve: 8.995 | bob: 5.574Epoch   5:  92% | abe: 5.558 | eve: 8.995 | bob: 5.571Epoch   5:  93% | abe: 5.555 | eve: 8.995 | bob: 5.568Epoch   5:  94% | abe: 5.553 | eve: 8.995 | bob: 5.565Epoch   5:  95% | abe: 5.549 | eve: 8.995 | bob: 5.562Epoch   5:  96% | abe: 5.547 | eve: 8.995 | bob: 5.559Epoch   5:  96% | abe: 5.544 | eve: 8.995 | bob: 5.556Epoch   5:  97% | abe: 5.541 | eve: 8.995 | bob: 5.553Epoch   5:  98% | abe: 5.538 | eve: 8.995 | bob: 5.550Epoch   5:  99% | abe: 5.535 | eve: 8.995 | bob: 5.547
New best Bob loss 5.546790425786526 at epoch 5
Epoch   6:   0% | abe: 5.162 | eve: 9.014 | bob: 5.164Epoch   6:   0% | abe: 5.168 | eve: 9.019 | bob: 5.165Epoch   6:   1% | abe: 5.165 | eve: 9.007 | bob: 5.160Epoch   6:   2% | abe: 5.164 | eve: 9.015 | bob: 5.159Epoch   6:   3% | abe: 5.160 | eve: 9.019 | bob: 5.156Epoch   6:   3% | abe: 5.160 | eve: 9.018 | bob: 5.155Epoch   6:   4% | abe: 5.154 | eve: 9.016 | bob: 5.147Epoch   6:   5% | abe: 5.150 | eve: 9.011 | bob: 5.144Epoch   6:   6% | abe: 5.150 | eve: 9.011 | bob: 5.143Epoch   6:   7% | abe: 5.148 | eve: 9.009 | bob: 5.141Epoch   6:   7% | abe: 5.145 | eve: 9.008 | bob: 5.137Epoch   6:   8% | abe: 5.142 | eve: 9.006 | bob: 5.134Epoch   6:   9% | abe: 5.141 | eve: 9.005 | bob: 5.133Epoch   6:  10% | abe: 5.137 | eve: 9.007 | bob: 5.130Epoch   6:  10% | abe: 5.134 | eve: 9.006 | bob: 5.127Epoch   6:  11% | abe: 5.131 | eve: 9.006 | bob: 5.123Epoch   6:  12% | abe: 5.128 | eve: 9.005 | bob: 5.120Epoch   6:  13% | abe: 5.125 | eve: 9.004 | bob: 5.117Epoch   6:  14% | abe: 5.123 | eve: 9.007 | bob: 5.114Epoch   6:  14% | abe: 5.122 | eve: 9.006 | bob: 5.114Epoch   6:  15% | abe: 5.119 | eve: 9.005 | bob: 5.111Epoch   6:  16% | abe: 5.116 | eve: 9.003 | bob: 5.109Epoch   6:  17% | abe: 5.113 | eve: 9.002 | bob: 5.105Epoch   6:  17% | abe: 5.111 | eve: 9.005 | bob: 5.103Epoch   6:  18% | abe: 5.107 | eve: 9.004 | bob: 5.099Epoch   6:  19% | abe: 5.106 | eve: 9.003 | bob: 5.098Epoch   6:  20% | abe: 5.102 | eve: 9.001 | bob: 5.094Epoch   6:  21% | abe: 5.101 | eve: 9.000 | bob: 5.092Epoch   6:  21% | abe: 5.099 | eve: 9.001 | bob: 5.090Epoch   6:  22% | abe: 5.096 | eve: 9.002 | bob: 5.088Epoch   6:  23% | abe: 5.094 | eve: 9.001 | bob: 5.085Epoch   6:  24% | abe: 5.092 | eve: 9.001 | bob: 5.083Epoch   6:  25% | abe: 5.090 | eve: 9.000 | bob: 5.081Epoch   6:  25% | abe: 5.088 | eve: 9.000 | bob: 5.078Epoch   6:  26% | abe: 5.086 | eve: 8.999 | bob: 5.077Epoch   6:  27% | abe: 5.083 | eve: 8.999 | bob: 5.074Epoch   6:  28% | abe: 5.080 | eve: 8.999 | bob: 5.071Epoch   6:  28% | abe: 5.077 | eve: 8.998 | bob: 5.068Epoch   6:  29% | abe: 5.074 | eve: 8.999 | bob: 5.065Epoch   6:  30% | abe: 5.072 | eve: 8.999 | bob: 5.063Epoch   6:  31% | abe: 5.070 | eve: 8.998 | bob: 5.060Epoch   6:  32% | abe: 5.067 | eve: 8.998 | bob: 5.057Epoch   6:  32% | abe: 5.065 | eve: 8.998 | bob: 5.055Epoch   6:  33% | abe: 5.063 | eve: 8.998 | bob: 5.053Epoch   6:  34% | abe: 5.061 | eve: 8.998 | bob: 5.051Epoch   6:  35% | abe: 5.058 | eve: 8.997 | bob: 5.048Epoch   6:  35% | abe: 5.055 | eve: 8.997 | bob: 5.045Epoch   6:  36% | abe: 5.053 | eve: 8.997 | bob: 5.042Epoch   6:  37% | abe: 5.049 | eve: 8.997 | bob: 5.039Epoch   6:  38% | abe: 5.047 | eve: 8.997 | bob: 5.036Epoch   6:  39% | abe: 5.044 | eve: 8.997 | bob: 5.034Epoch   6:  39% | abe: 5.042 | eve: 8.997 | bob: 5.031Epoch   6:  40% | abe: 5.040 | eve: 8.996 | bob: 5.028Epoch   6:  41% | abe: 5.037 | eve: 8.996 | bob: 5.026Epoch   6:  42% | abe: 5.034 | eve: 8.996 | bob: 5.023Epoch   6:  42% | abe: 5.032 | eve: 8.996 | bob: 5.020Epoch   6:  43% | abe: 5.029 | eve: 8.996 | bob: 5.017Epoch   6:  44% | abe: 5.027 | eve: 8.996 | bob: 5.015Epoch   6:  45% | abe: 5.024 | eve: 8.996 | bob: 5.012Epoch   6:  46% | abe: 5.021 | eve: 8.996 | bob: 5.009Epoch   6:  46% | abe: 5.019 | eve: 8.995 | bob: 5.007Epoch   6:  47% | abe: 5.016 | eve: 8.995 | bob: 5.004Epoch   6:  48% | abe: 5.014 | eve: 8.995 | bob: 5.002Epoch   6:  49% | abe: 5.011 | eve: 8.996 | bob: 4.999Epoch   6:  50% | abe: 5.009 | eve: 8.995 | bob: 4.996Epoch   6:  50% | abe: 5.006 | eve: 8.995 | bob: 4.993Epoch   6:  51% | abe: 5.003 | eve: 8.995 | bob: 4.991Epoch   6:  52% | abe: 5.001 | eve: 8.995 | bob: 4.988Epoch   6:  53% | abe: 4.998 | eve: 8.995 | bob: 4.985Epoch   6:  53% | abe: 4.995 | eve: 8.995 | bob: 4.982Epoch   6:  54% | abe: 4.993 | eve: 8.995 | bob: 4.980Epoch   6:  55% | abe: 4.991 | eve: 8.995 | bob: 4.977Epoch   6:  56% | abe: 4.989 | eve: 8.996 | bob: 4.975Epoch   6:  57% | abe: 4.986 | eve: 8.996 | bob: 4.972Epoch   6:  57% | abe: 4.984 | eve: 8.995 | bob: 4.970Epoch   6:  58% | abe: 4.981 | eve: 8.996 | bob: 4.967Epoch   6:  59% | abe: 4.979 | eve: 8.996 | bob: 4.965Epoch   6:  60% | abe: 4.977 | eve: 8.996 | bob: 4.963Epoch   6:  60% | abe: 4.974 | eve: 8.996 | bob: 4.960Epoch   6:  61% | abe: 4.972 | eve: 8.996 | bob: 4.958Epoch   6:  62% | abe: 4.969 | eve: 8.996 | bob: 4.955Epoch   6:  63% | abe: 4.967 | eve: 8.996 | bob: 4.953Epoch   6:  64% | abe: 4.965 | eve: 8.997 | bob: 4.950Epoch   6:  64% | abe: 4.962 | eve: 8.997 | bob: 4.948Epoch   6:  65% | abe: 4.960 | eve: 8.998 | bob: 4.945Epoch   6:  66% | abe: 4.957 | eve: 8.997 | bob: 4.943Epoch   6:  67% | abe: 4.955 | eve: 8.997 | bob: 4.940Epoch   6:  67% | abe: 4.952 | eve: 8.998 | bob: 4.938Epoch   6:  68% | abe: 4.950 | eve: 8.997 | bob: 4.936Epoch   6:  69% | abe: 4.948 | eve: 8.997 | bob: 4.933Epoch   6:  70% | abe: 4.945 | eve: 8.997 | bob: 4.931Epoch   6:  71% | abe: 4.943 | eve: 8.997 | bob: 4.928Epoch   6:  71% | abe: 4.941 | eve: 8.997 | bob: 4.926Epoch   6:  72% | abe: 4.938 | eve: 8.997 | bob: 4.923Epoch   6:  73% | abe: 4.936 | eve: 8.998 | bob: 4.921Epoch   6:  74% | abe: 4.934 | eve: 8.997 | bob: 4.919Epoch   6:  75% | abe: 4.932 | eve: 8.998 | bob: 4.917Epoch   6:  75% | abe: 4.930 | eve: 8.998 | bob: 4.914Epoch   6:  76% | abe: 4.928 | eve: 8.998 | bob: 4.912Epoch   6:  77% | abe: 4.925 | eve: 8.998 | bob: 4.910Epoch   6:  78% | abe: 4.923 | eve: 8.998 | bob: 4.907Epoch   6:  78% | abe: 4.921 | eve: 8.998 | bob: 4.905Epoch   6:  79% | abe: 4.918 | eve: 8.998 | bob: 4.902Epoch   6:  80% | abe: 4.916 | eve: 8.998 | bob: 4.900Epoch   6:  81% | abe: 4.914 | eve: 8.998 | bob: 4.898Epoch   6:  82% | abe: 4.912 | eve: 8.998 | bob: 4.895Epoch   6:  82% | abe: 4.909 | eve: 8.998 | bob: 4.893Epoch   6:  83% | abe: 4.907 | eve: 8.998 | bob: 4.891Epoch   6:  84% | abe: 4.905 | eve: 8.998 | bob: 4.889Epoch   6:  85% | abe: 4.902 | eve: 8.998 | bob: 4.886Epoch   6:  85% | abe: 4.900 | eve: 8.998 | bob: 4.883Epoch   6:  86% | abe: 4.898 | eve: 8.999 | bob: 4.881Epoch   6:  87% | abe: 4.895 | eve: 8.999 | bob: 4.879Epoch   6:  88% | abe: 4.893 | eve: 8.999 | bob: 4.877Epoch   6:  89% | abe: 4.891 | eve: 8.999 | bob: 4.874Epoch   6:  89% | abe: 4.889 | eve: 8.999 | bob: 4.872Epoch   6:  90% | abe: 4.887 | eve: 8.999 | bob: 4.870Epoch   6:  91% | abe: 4.885 | eve: 8.999 | bob: 4.868Epoch   6:  92% | abe: 4.883 | eve: 8.999 | bob: 4.866Epoch   6:  92% | abe: 4.881 | eve: 8.999 | bob: 4.863Epoch   6:  93% | abe: 4.878 | eve: 8.999 | bob: 4.861Epoch   6:  94% | abe: 4.876 | eve: 8.999 | bob: 4.859Epoch   6:  95% | abe: 4.874 | eve: 8.999 | bob: 4.856Epoch   6:  96% | abe: 4.871 | eve: 8.999 | bob: 4.854Epoch   6:  96% | abe: 4.869 | eve: 8.999 | bob: 4.852Epoch   6:  97% | abe: 4.867 | eve: 8.999 | bob: 4.850Epoch   6:  98% | abe: 4.865 | eve: 8.999 | bob: 4.847Epoch   6:  99% | abe: 4.862 | eve: 8.999 | bob: 4.845
New best Bob loss 4.844698292639919 at epoch 6
Epoch   7:   0% | abe: 4.556 | eve: 8.997 | bob: 4.528Epoch   7:   0% | abe: 4.576 | eve: 8.997 | bob: 4.552Epoch   7:   1% | abe: 4.576 | eve: 9.001 | bob: 4.554Epoch   7:   2% | abe: 4.571 | eve: 9.012 | bob: 4.545Epoch   7:   3% | abe: 4.570 | eve: 9.022 | bob: 4.542Epoch   7:   3% | abe: 4.571 | eve: 9.016 | bob: 4.542Epoch   7:   4% | abe: 4.564 | eve: 9.011 | bob: 4.535Epoch   7:   5% | abe: 4.566 | eve: 9.014 | bob: 4.537Epoch   7:   6% | abe: 4.559 | eve: 9.010 | bob: 4.530Epoch   7:   7% | abe: 4.556 | eve: 9.006 | bob: 4.527Epoch   7:   7% | abe: 4.552 | eve: 9.004 | bob: 4.523Epoch   7:   8% | abe: 4.550 | eve: 9.006 | bob: 4.522Epoch   7:   9% | abe: 4.548 | eve: 9.005 | bob: 4.521Epoch   7:  10% | abe: 4.546 | eve: 9.005 | bob: 4.518Epoch   7:  10% | abe: 4.544 | eve: 9.004 | bob: 4.515Epoch   7:  11% | abe: 4.544 | eve: 9.003 | bob: 4.515Epoch   7:  12% | abe: 4.541 | eve: 9.004 | bob: 4.512Epoch   7:  13% | abe: 4.538 | eve: 9.002 | bob: 4.509Epoch   7:  14% | abe: 4.538 | eve: 9.000 | bob: 4.509Epoch   7:  14% | abe: 4.537 | eve: 9.000 | bob: 4.508Epoch   7:  15% | abe: 4.536 | eve: 9.000 | bob: 4.507Epoch   7:  16% | abe: 4.534 | eve: 9.001 | bob: 4.505Epoch   7:  17% | abe: 4.533 | eve: 9.002 | bob: 4.503Epoch   7:  17% | abe: 4.530 | eve: 9.001 | bob: 4.500Epoch   7:  18% | abe: 4.529 | eve: 9.000 | bob: 4.499Epoch   7:  19% | abe: 4.528 | eve: 8.999 | bob: 4.498Epoch   7:  20% | abe: 4.528 | eve: 9.000 | bob: 4.497Epoch   7:  21% | abe: 4.526 | eve: 9.000 | bob: 4.495Epoch   7:  21% | abe: 4.525 | eve: 8.999 | bob: 4.494Epoch   7:  22% | abe: 4.523 | eve: 8.999 | bob: 4.492Epoch   7:  23% | abe: 4.521 | eve: 8.999 | bob: 4.490Epoch   7:  24% | abe: 4.519 | eve: 8.999 | bob: 4.488Epoch   7:  25% | abe: 4.517 | eve: 8.999 | bob: 4.486Epoch   7:  25% | abe: 4.515 | eve: 8.998 | bob: 4.484Epoch   7:  26% | abe: 4.513 | eve: 8.998 | bob: 4.482Epoch   7:  27% | abe: 4.510 | eve: 8.998 | bob: 4.479Epoch   7:  28% | abe: 4.509 | eve: 8.999 | bob: 4.478Epoch   7:  28% | abe: 4.507 | eve: 8.999 | bob: 4.476Epoch   7:  29% | abe: 4.505 | eve: 8.998 | bob: 4.474Epoch   7:  30% | abe: 4.504 | eve: 8.999 | bob: 4.473Epoch   7:  31% | abe: 4.502 | eve: 8.999 | bob: 4.471Epoch   7:  32% | abe: 4.501 | eve: 8.999 | bob: 4.469Epoch   7:  32% | abe: 4.499 | eve: 8.998 | bob: 4.467Epoch   7:  33% | abe: 4.497 | eve: 8.999 | bob: 4.465Epoch   7:  34% | abe: 4.495 | eve: 8.999 | bob: 4.463Epoch   7:  35% | abe: 4.493 | eve: 9.000 | bob: 4.461Epoch   7:  35% | abe: 4.491 | eve: 9.000 | bob: 4.459Epoch   7:  36% | abe: 4.489 | eve: 9.000 | bob: 4.457Epoch   7:  37% | abe: 4.488 | eve: 8.999 | bob: 4.455Epoch   7:  38% | abe: 4.485 | eve: 9.000 | bob: 4.453Epoch   7:  39% | abe: 4.483 | eve: 9.000 | bob: 4.451Epoch   7:  39% | abe: 4.481 | eve: 9.001 | bob: 4.449Epoch   7:  40% | abe: 4.478 | eve: 9.001 | bob: 4.446Epoch   7:  41% | abe: 4.476 | eve: 9.002 | bob: 4.444Epoch   7:  42% | abe: 4.475 | eve: 9.002 | bob: 4.442Epoch   7:  42% | abe: 4.473 | eve: 9.002 | bob: 4.440Epoch   7:  43% | abe: 4.471 | eve: 9.002 | bob: 4.438Epoch   7:  44% | abe: 4.469 | eve: 9.002 | bob: 4.436Epoch   7:  45% | abe: 4.467 | eve: 9.002 | bob: 4.434Epoch   7:  46% | abe: 4.465 | eve: 9.001 | bob: 4.432Epoch   7:  46% | abe: 4.464 | eve: 9.002 | bob: 4.430Epoch   7:  47% | abe: 4.462 | eve: 9.002 | bob: 4.428Epoch   7:  48% | abe: 4.459 | eve: 9.001 | bob: 4.426Epoch   7:  49% | abe: 4.458 | eve: 9.002 | bob: 4.424Epoch   7:  50% | abe: 4.456 | eve: 9.001 | bob: 4.422Epoch   7:  50% | abe: 4.454 | eve: 9.001 | bob: 4.420Epoch   7:  51% | abe: 4.452 | eve: 9.001 | bob: 4.418Epoch   7:  52% | abe: 4.450 | eve: 9.001 | bob: 4.416Epoch   7:  53% | abe: 4.448 | eve: 9.001 | bob: 4.414Epoch   7:  53% | abe: 4.447 | eve: 9.001 | bob: 4.412Epoch   7:  54% | abe: 4.445 | eve: 9.001 | bob: 4.410Epoch   7:  55% | abe: 4.442 | eve: 9.000 | bob: 4.408Epoch   7:  56% | abe: 4.440 | eve: 9.000 | bob: 4.406Epoch   7:  57% | abe: 4.438 | eve: 9.000 | bob: 4.404Epoch   7:  57% | abe: 4.436 | eve: 9.000 | bob: 4.401Epoch   7:  58% | abe: 4.434 | eve: 8.999 | bob: 4.399Epoch   7:  59% | abe: 4.432 | eve: 8.999 | bob: 4.398Epoch   7:  60% | abe: 4.430 | eve: 8.999 | bob: 4.395Epoch   7:  60% | abe: 4.429 | eve: 9.000 | bob: 4.394Epoch   7:  61% | abe: 4.427 | eve: 9.000 | bob: 4.392Epoch   7:  62% | abe: 4.425 | eve: 8.999 | bob: 4.390Epoch   7:  63% | abe: 4.424 | eve: 8.999 | bob: 4.388Epoch   7:  64% | abe: 4.422 | eve: 8.999 | bob: 4.387Epoch   7:  64% | abe: 4.420 | eve: 8.999 | bob: 4.385Epoch   7:  65% | abe: 4.418 | eve: 8.999 | bob: 4.383Epoch   7:  66% | abe: 4.417 | eve: 8.999 | bob: 4.381Epoch   7:  67% | abe: 4.415 | eve: 8.999 | bob: 4.379Epoch   7:  67% | abe: 4.413 | eve: 8.999 | bob: 4.377Epoch   7:  68% | abe: 4.411 | eve: 8.999 | bob: 4.375Epoch   7:  69% | abe: 4.409 | eve: 8.999 | bob: 4.373Epoch   7:  70% | abe: 4.408 | eve: 8.999 | bob: 4.372Epoch   7:  71% | abe: 4.406 | eve: 8.999 | bob: 4.370Epoch   7:  71% | abe: 4.404 | eve: 8.999 | bob: 4.368Epoch   7:  72% | abe: 4.402 | eve: 8.999 | bob: 4.366Epoch   7:  73% | abe: 4.400 | eve: 8.999 | bob: 4.364Epoch   7:  74% | abe: 4.398 | eve: 8.999 | bob: 4.362Epoch   7:  75% | abe: 4.397 | eve: 8.999 | bob: 4.361Epoch   7:  75% | abe: 4.395 | eve: 8.999 | bob: 4.359Epoch   7:  76% | abe: 4.394 | eve: 8.999 | bob: 4.357Epoch   7:  77% | abe: 4.392 | eve: 8.999 | bob: 4.356Epoch   7:  78% | abe: 4.390 | eve: 9.000 | bob: 4.353Epoch   7:  78% | abe: 4.388 | eve: 8.999 | bob: 4.351Epoch   7:  79% | abe: 4.386 | eve: 8.999 | bob: 4.350Epoch   7:  80% | abe: 4.384 | eve: 8.999 | bob: 4.348Epoch   7:  81% | abe: 4.382 | eve: 8.999 | bob: 4.346Epoch   7:  82% | abe: 4.380 | eve: 8.999 | bob: 4.344Epoch   7:  82% | abe: 4.378 | eve: 8.998 | bob: 4.342Epoch   7:  83% | abe: 4.377 | eve: 8.999 | bob: 4.340Epoch   7:  84% | abe: 4.375 | eve: 8.999 | bob: 4.338Epoch   7:  85% | abe: 4.373 | eve: 8.999 | bob: 4.336Epoch   7:  85% | abe: 4.371 | eve: 8.999 | bob: 4.334Epoch   7:  86% | abe: 4.369 | eve: 8.999 | bob: 4.333Epoch   7:  87% | abe: 4.368 | eve: 8.999 | bob: 4.331Epoch   7:  88% | abe: 4.366 | eve: 8.998 | bob: 4.329Epoch   7:  89% | abe: 4.364 | eve: 8.998 | bob: 4.327Epoch   7:  89% | abe: 4.362 | eve: 8.998 | bob: 4.325Epoch   7:  90% | abe: 4.361 | eve: 8.998 | bob: 4.324Epoch   7:  91% | abe: 4.359 | eve: 8.998 | bob: 4.322Epoch   7:  92% | abe: 4.357 | eve: 8.998 | bob: 4.320Epoch   7:  92% | abe: 4.355 | eve: 8.998 | bob: 4.318Epoch   7:  93% | abe: 4.353 | eve: 8.998 | bob: 4.316Epoch   7:  94% | abe: 4.352 | eve: 8.998 | bob: 4.314Epoch   7:  95% | abe: 4.350 | eve: 8.998 | bob: 4.312Epoch   7:  96% | abe: 4.348 | eve: 8.998 | bob: 4.310Epoch   7:  96% | abe: 4.346 | eve: 8.998 | bob: 4.309Epoch   7:  97% | abe: 4.344 | eve: 8.998 | bob: 4.307Epoch   7:  98% | abe: 4.342 | eve: 8.998 | bob: 4.305Epoch   7:  99% | abe: 4.340 | eve: 8.997 | bob: 4.303
New best Bob loss 4.302751797412384 at epoch 7
Epoch   8:   0% | abe: 4.077 | eve: 8.977 | bob: 4.031Epoch   8:   0% | abe: 4.099 | eve: 8.974 | bob: 4.048Epoch   8:   1% | abe: 4.104 | eve: 8.983 | bob: 4.052Epoch   8:   2% | abe: 4.106 | eve: 8.996 | bob: 4.055Epoch   8:   3% | abe: 4.102 | eve: 8.994 | bob: 4.051Epoch   8:   3% | abe: 4.099 | eve: 8.996 | bob: 4.050Epoch   8:   4% | abe: 4.101 | eve: 8.991 | bob: 4.052Epoch   8:   5% | abe: 4.098 | eve: 8.989 | bob: 4.048Epoch   8:   6% | abe: 4.097 | eve: 8.989 | bob: 4.048Epoch   8:   7% | abe: 4.095 | eve: 8.986 | bob: 4.046Epoch   8:   7% | abe: 4.094 | eve: 8.989 | bob: 4.046Epoch   8:   8% | abe: 4.093 | eve: 8.989 | bob: 4.044Epoch   8:   9% | abe: 4.092 | eve: 8.987 | bob: 4.044Epoch   8:  10% | abe: 4.089 | eve: 8.988 | bob: 4.041Epoch   8:  10% | abe: 4.087 | eve: 8.988 | bob: 4.039Epoch   8:  11% | abe: 4.085 | eve: 8.987 | bob: 4.037Epoch   8:  12% | abe: 4.083 | eve: 8.985 | bob: 4.035Epoch   8:  13% | abe: 4.081 | eve: 8.986 | bob: 4.033Epoch   8:  14% | abe: 4.079 | eve: 8.987 | bob: 4.031Epoch   8:  14% | abe: 4.078 | eve: 8.989 | bob: 4.031Epoch   8:  15% | abe: 4.076 | eve: 8.991 | bob: 4.028Epoch   8:  16% | abe: 4.074 | eve: 8.990 | bob: 4.026Epoch   8:  17% | abe: 4.073 | eve: 8.990 | bob: 4.025Epoch   8:  17% | abe: 4.072 | eve: 8.990 | bob: 4.024Epoch   8:  18% | abe: 4.071 | eve: 8.991 | bob: 4.023Epoch   8:  19% | abe: 4.070 | eve: 8.992 | bob: 4.022Epoch   8:  20% | abe: 4.069 | eve: 8.992 | bob: 4.021Epoch   8:  21% | abe: 4.069 | eve: 8.993 | bob: 4.020Epoch   8:  21% | abe: 4.066 | eve: 8.994 | bob: 4.017Epoch   8:  22% | abe: 4.064 | eve: 8.993 | bob: 4.016Epoch   8:  23% | abe: 4.063 | eve: 8.994 | bob: 4.014Epoch   8:  24% | abe: 4.062 | eve: 8.995 | bob: 4.014Epoch   8:  25% | abe: 4.062 | eve: 8.994 | bob: 4.013Epoch   8:  25% | abe: 4.061 | eve: 8.995 | bob: 4.012Epoch   8:  26% | abe: 4.059 | eve: 8.995 | bob: 4.010Epoch   8:  27% | abe: 4.057 | eve: 8.996 | bob: 4.008Epoch   8:  28% | abe: 4.055 | eve: 8.996 | bob: 4.006Epoch   8:  28% | abe: 4.054 | eve: 8.995 | bob: 4.005Epoch   8:  29% | abe: 4.052 | eve: 8.995 | bob: 4.003Epoch   8:  30% | abe: 4.051 | eve: 8.995 | bob: 4.002Epoch   8:  31% | abe: 4.049 | eve: 8.996 | bob: 4.000Epoch   8:  32% | abe: 4.049 | eve: 8.997 | bob: 3.999Epoch   8:  32% | abe: 4.047 | eve: 8.997 | bob: 3.997Epoch   8:  33% | abe: 4.045 | eve: 8.996 | bob: 3.996Epoch   8:  34% | abe: 4.043 | eve: 8.996 | bob: 3.994Epoch   8:  35% | abe: 4.042 | eve: 8.997 | bob: 3.992Epoch   8:  35% | abe: 4.040 | eve: 8.997 | bob: 3.990Epoch   8:  36% | abe: 4.039 | eve: 8.996 | bob: 3.989Epoch   8:  37% | abe: 4.037 | eve: 8.996 | bob: 3.987Epoch   8:  38% | abe: 4.035 | eve: 8.996 | bob: 3.985Epoch   8:  39% | abe: 4.033 | eve: 8.996 | bob: 3.983Epoch   8:  39% | abe: 4.031 | eve: 8.995 | bob: 3.981Epoch   8:  40% | abe: 4.029 | eve: 8.995 | bob: 3.979Epoch   8:  41% | abe: 4.027 | eve: 8.995 | bob: 3.977Epoch   8:  42% | abe: 4.025 | eve: 8.995 | bob: 3.975Epoch   8:  42% | abe: 4.023 | eve: 8.995 | bob: 3.973Epoch   8:  43% | abe: 4.022 | eve: 8.996 | bob: 3.971Epoch   8:  44% | abe: 4.020 | eve: 8.996 | bob: 3.970Epoch   8:  45% | abe: 4.019 | eve: 8.996 | bob: 3.968Epoch   8:  46% | abe: 4.017 | eve: 8.996 | bob: 3.967Epoch   8:  46% | abe: 4.015 | eve: 8.996 | bob: 3.965Epoch   8:  47% | abe: 4.013 | eve: 8.996 | bob: 3.963Epoch   8:  48% | abe: 4.011 | eve: 8.996 | bob: 3.961Epoch   8:  49% | abe: 4.010 | eve: 8.996 | bob: 3.959Epoch   8:  50% | abe: 4.008 | eve: 8.996 | bob: 3.958Epoch   8:  50% | abe: 4.007 | eve: 8.997 | bob: 3.956Epoch   8:  51% | abe: 4.005 | eve: 8.996 | bob: 3.955Epoch   8:  52% | abe: 4.004 | eve: 8.996 | bob: 3.953Epoch   8:  53% | abe: 4.002 | eve: 8.997 | bob: 3.951Epoch   8:  53% | abe: 4.001 | eve: 8.996 | bob: 3.950Epoch   8:  54% | abe: 3.999 | eve: 8.997 | bob: 3.949Epoch   8:  55% | abe: 3.997 | eve: 8.996 | bob: 3.947Epoch   8:  56% | abe: 3.996 | eve: 8.996 | bob: 3.945Epoch   8:  57% | abe: 3.994 | eve: 8.996 | bob: 3.943Epoch   8:  57% | abe: 3.993 | eve: 8.996 | bob: 3.942Epoch   8:  58% | abe: 3.991 | eve: 8.997 | bob: 3.940Epoch   8:  59% | abe: 3.990 | eve: 8.997 | bob: 3.939Epoch   8:  60% | abe: 3.988 | eve: 8.997 | bob: 3.937Epoch   8:  60% | abe: 3.986 | eve: 8.996 | bob: 3.935Epoch   8:  61% | abe: 3.985 | eve: 8.997 | bob: 3.934Epoch   8:  62% | abe: 3.983 | eve: 8.998 | bob: 3.932Epoch   8:  63% | abe: 3.982 | eve: 8.998 | bob: 3.931Epoch   8:  64% | abe: 3.980 | eve: 8.998 | bob: 3.929Epoch   8:  64% | abe: 3.978 | eve: 8.998 | bob: 3.927Epoch   8:  65% | abe: 3.977 | eve: 8.998 | bob: 3.925Epoch   8:  66% | abe: 3.976 | eve: 8.998 | bob: 3.924Epoch   8:  67% | abe: 3.974 | eve: 8.998 | bob: 3.922Epoch   8:  67% | abe: 3.972 | eve: 8.997 | bob: 3.921Epoch   8:  68% | abe: 3.971 | eve: 8.997 | bob: 3.919Epoch   8:  69% | abe: 3.970 | eve: 8.997 | bob: 3.918Epoch   8:  70% | abe: 3.968 | eve: 8.997 | bob: 3.916Epoch   8:  71% | abe: 3.966 | eve: 8.998 | bob: 3.914Epoch   8:  71% | abe: 3.965 | eve: 8.997 | bob: 3.913Epoch   8:  72% | abe: 3.963 | eve: 8.997 | bob: 3.912Epoch   8:  73% | abe: 3.962 | eve: 8.997 | bob: 3.910Epoch   8:  74% | abe: 3.960 | eve: 8.997 | bob: 3.908Epoch   8:  75% | abe: 3.959 | eve: 8.997 | bob: 3.907Epoch   8:  75% | abe: 3.957 | eve: 8.998 | bob: 3.905Epoch   8:  76% | abe: 3.956 | eve: 8.998 | bob: 3.904Epoch   8:  77% | abe: 3.954 | eve: 8.998 | bob: 3.902Epoch   8:  78% | abe: 3.952 | eve: 8.998 | bob: 3.900Epoch   8:  78% | abe: 3.951 | eve: 8.998 | bob: 3.899Epoch   8:  79% | abe: 3.949 | eve: 8.998 | bob: 3.897Epoch   8:  80% | abe: 3.948 | eve: 8.998 | bob: 3.896Epoch   8:  81% | abe: 3.946 | eve: 8.998 | bob: 3.894Epoch   8:  82% | abe: 3.945 | eve: 8.998 | bob: 3.893Epoch   8:  82% | abe: 3.943 | eve: 8.998 | bob: 3.891Epoch   8:  83% | abe: 3.941 | eve: 8.998 | bob: 3.889Epoch   8:  84% | abe: 3.940 | eve: 8.998 | bob: 3.888Epoch   8:  85% | abe: 3.938 | eve: 8.998 | bob: 3.886Epoch   8:  85% | abe: 3.937 | eve: 8.998 | bob: 3.885Epoch   8:  86% | abe: 3.935 | eve: 8.998 | bob: 3.883Epoch   8:  87% | abe: 3.933 | eve: 8.998 | bob: 3.881Epoch   8:  88% | abe: 3.932 | eve: 8.998 | bob: 3.880Epoch   8:  89% | abe: 3.930 | eve: 8.998 | bob: 3.878Epoch   8:  89% | abe: 3.929 | eve: 8.998 | bob: 3.876Epoch   8:  90% | abe: 3.927 | eve: 8.998 | bob: 3.875Epoch   8:  91% | abe: 3.926 | eve: 8.999 | bob: 3.873Epoch   8:  92% | abe: 3.924 | eve: 8.999 | bob: 3.872Epoch   8:  92% | abe: 3.923 | eve: 8.999 | bob: 3.870Epoch   8:  93% | abe: 3.921 | eve: 8.999 | bob: 3.869Epoch   8:  94% | abe: 3.920 | eve: 8.999 | bob: 3.867Epoch   8:  95% | abe: 3.918 | eve: 8.999 | bob: 3.866Epoch   8:  96% | abe: 3.917 | eve: 8.999 | bob: 3.864Epoch   8:  96% | abe: 3.915 | eve: 8.999 | bob: 3.863Epoch   8:  97% | abe: 3.914 | eve: 8.999 | bob: 3.861Epoch   8:  98% | abe: 3.912 | eve: 8.999 | bob: 3.860Epoch   8:  99% | abe: 3.911 | eve: 8.999 | bob: 3.858
New best Bob loss 3.858378418329835 at epoch 8
Epoch   9:   0% | abe: 3.710 | eve: 9.010 | bob: 3.651Epoch   9:   0% | abe: 3.715 | eve: 9.003 | bob: 3.656Epoch   9:   1% | abe: 3.714 | eve: 9.001 | bob: 3.655Epoch   9:   2% | abe: 3.714 | eve: 8.991 | bob: 3.656Epoch   9:   3% | abe: 3.716 | eve: 8.993 | bob: 3.658Epoch   9:   3% | abe: 3.711 | eve: 8.988 | bob: 3.654Epoch   9:   4% | abe: 3.704 | eve: 8.988 | bob: 3.647Epoch   9:   5% | abe: 3.703 | eve: 8.990 | bob: 3.646Epoch   9:   6% | abe: 3.703 | eve: 8.991 | bob: 3.645Epoch   9:   7% | abe: 3.699 | eve: 8.996 | bob: 3.642Epoch   9:   7% | abe: 3.697 | eve: 8.997 | bob: 3.639Epoch   9:   8% | abe: 3.700 | eve: 8.995 | bob: 3.642Epoch   9:   9% | abe: 3.700 | eve: 8.996 | bob: 3.642Epoch   9:  10% | abe: 3.697 | eve: 8.997 | bob: 3.640Epoch   9:  10% | abe: 3.696 | eve: 8.999 | bob: 3.639Epoch   9:  11% | abe: 3.693 | eve: 8.999 | bob: 3.636Epoch   9:  12% | abe: 3.693 | eve: 8.999 | bob: 3.636Epoch   9:  13% | abe: 3.690 | eve: 9.000 | bob: 3.633Epoch   9:  14% | abe: 3.690 | eve: 9.000 | bob: 3.632Epoch   9:  14% | abe: 3.689 | eve: 9.000 | bob: 3.631Epoch   9:  15% | abe: 3.688 | eve: 9.000 | bob: 3.630Epoch   9:  16% | abe: 3.687 | eve: 9.002 | bob: 3.629Epoch   9:  17% | abe: 3.686 | eve: 9.001 | bob: 3.628Epoch   9:  17% | abe: 3.685 | eve: 9.001 | bob: 3.627Epoch   9:  18% | abe: 3.681 | eve: 9.003 | bob: 3.623Epoch   9:  19% | abe: 3.681 | eve: 9.004 | bob: 3.623Epoch   9:  20% | abe: 3.678 | eve: 9.004 | bob: 3.621Epoch   9:  21% | abe: 3.678 | eve: 9.004 | bob: 3.620Epoch   9:  21% | abe: 3.676 | eve: 9.003 | bob: 3.618Epoch   9:  22% | abe: 3.674 | eve: 9.003 | bob: 3.617Epoch   9:  23% | abe: 3.673 | eve: 9.004 | bob: 3.616Epoch   9:  24% | abe: 3.672 | eve: 9.004 | bob: 3.615Epoch   9:  25% | abe: 3.670 | eve: 9.004 | bob: 3.613Epoch   9:  25% | abe: 3.668 | eve: 9.004 | bob: 3.611Epoch   9:  26% | abe: 3.667 | eve: 9.004 | bob: 3.610Epoch   9:  27% | abe: 3.665 | eve: 9.003 | bob: 3.608Epoch   9:  28% | abe: 3.663 | eve: 9.004 | bob: 3.606Epoch   9:  28% | abe: 3.662 | eve: 9.005 | bob: 3.604Epoch   9:  29% | abe: 3.661 | eve: 9.005 | bob: 3.604Epoch   9:  30% | abe: 3.659 | eve: 9.004 | bob: 3.601Epoch   9:  31% | abe: 3.657 | eve: 9.004 | bob: 3.600Epoch   9:  32% | abe: 3.657 | eve: 9.003 | bob: 3.599Epoch   9:  32% | abe: 3.655 | eve: 9.004 | bob: 3.597Epoch   9:  33% | abe: 3.654 | eve: 9.003 | bob: 3.596Epoch   9:  34% | abe: 3.653 | eve: 9.002 | bob: 3.595Epoch   9:  35% | abe: 3.651 | eve: 9.002 | bob: 3.594Epoch   9:  35% | abe: 3.650 | eve: 9.002 | bob: 3.592Epoch   9:  36% | abe: 3.648 | eve: 9.001 | bob: 3.590Epoch   9:  37% | abe: 3.646 | eve: 9.001 | bob: 3.589Epoch   9:  38% | abe: 3.645 | eve: 9.001 | bob: 3.587Epoch   9:  39% | abe: 3.644 | eve: 9.001 | bob: 3.586Epoch   9:  39% | abe: 3.642 | eve: 9.000 | bob: 3.585Epoch   9:  40% | abe: 3.641 | eve: 9.001 | bob: 3.584Epoch   9:  41% | abe: 3.640 | eve: 9.000 | bob: 3.582Epoch   9:  42% | abe: 3.638 | eve: 9.001 | bob: 3.581Epoch   9:  42% | abe: 3.637 | eve: 9.000 | bob: 3.579Epoch   9:  43% | abe: 3.636 | eve: 9.000 | bob: 3.579Epoch   9:  44% | abe: 3.635 | eve: 9.000 | bob: 3.578Epoch   9:  45% | abe: 3.634 | eve: 9.000 | bob: 3.576Epoch   9:  46% | abe: 3.633 | eve: 9.001 | bob: 3.575Epoch   9:  46% | abe: 3.631 | eve: 9.001 | bob: 3.573Epoch   9:  47% | abe: 3.630 | eve: 9.001 | bob: 3.573Epoch   9:  48% | abe: 3.629 | eve: 9.000 | bob: 3.571Epoch   9:  49% | abe: 3.627 | eve: 9.001 | bob: 3.570Epoch   9:  50% | abe: 3.626 | eve: 9.000 | bob: 3.568Epoch   9:  50% | abe: 3.624 | eve: 9.000 | bob: 3.567Epoch   9:  51% | abe: 3.623 | eve: 9.000 | bob: 3.565Epoch   9:  52% | abe: 3.621 | eve: 9.000 | bob: 3.564Epoch   9:  53% | abe: 3.620 | eve: 9.000 | bob: 3.563Epoch   9:  53% | abe: 3.619 | eve: 9.000 | bob: 3.561Epoch   9:  54% | abe: 3.618 | eve: 9.000 | bob: 3.560Epoch   9:  55% | abe: 3.617 | eve: 9.000 | bob: 3.559Epoch   9:  56% | abe: 3.615 | eve: 9.000 | bob: 3.558Epoch   9:  57% | abe: 3.614 | eve: 9.000 | bob: 3.556Epoch   9:  57% | abe: 3.613 | eve: 9.000 | bob: 3.555Epoch   9:  58% | abe: 3.611 | eve: 9.000 | bob: 3.554Epoch   9:  59% | abe: 3.610 | eve: 9.000 | bob: 3.552Epoch   9:  60% | abe: 3.609 | eve: 9.000 | bob: 3.551Epoch   9:  60% | abe: 3.607 | eve: 9.000 | bob: 3.549Epoch   9:  61% | abe: 3.606 | eve: 9.000 | bob: 3.548Epoch   9:  62% | abe: 3.604 | eve: 9.000 | bob: 3.547Epoch   9:  63% | abe: 3.603 | eve: 9.000 | bob: 3.545Epoch   9:  64% | abe: 3.602 | eve: 9.001 | bob: 3.544Epoch   9:  64% | abe: 3.601 | eve: 9.001 | bob: 3.543Epoch   9:  65% | abe: 3.599 | eve: 9.001 | bob: 3.541Epoch   9:  66% | abe: 3.598 | eve: 9.002 | bob: 3.540Epoch   9:  67% | abe: 3.597 | eve: 9.002 | bob: 3.539Epoch   9:  67% | abe: 3.596 | eve: 9.002 | bob: 3.537Epoch   9:  68% | abe: 3.595 | eve: 9.002 | bob: 3.536Epoch   9:  69% | abe: 3.593 | eve: 9.002 | bob: 3.535Epoch   9:  70% | abe: 3.592 | eve: 9.002 | bob: 3.533Epoch   9:  71% | abe: 3.591 | eve: 9.002 | bob: 3.532Epoch   9:  71% | abe: 3.589 | eve: 9.002 | bob: 3.531Epoch   9:  72% | abe: 3.588 | eve: 9.002 | bob: 3.529Epoch   9:  73% | abe: 3.587 | eve: 9.002 | bob: 3.528Epoch   9:  74% | abe: 3.585 | eve: 9.003 | bob: 3.527Epoch   9:  75% | abe: 3.584 | eve: 9.003 | bob: 3.526Epoch   9:  75% | abe: 3.583 | eve: 9.002 | bob: 3.525Epoch   9:  76% | abe: 3.582 | eve: 9.003 | bob: 3.523Epoch   9:  77% | abe: 3.581 | eve: 9.003 | bob: 3.522Epoch   9:  78% | abe: 3.580 | eve: 9.003 | bob: 3.521Epoch   9:  78% | abe: 3.579 | eve: 9.003 | bob: 3.520Epoch   9:  79% | abe: 3.577 | eve: 9.003 | bob: 3.519Epoch   9:  80% | abe: 3.576 | eve: 9.003 | bob: 3.517Epoch   9:  81% | abe: 3.575 | eve: 9.002 | bob: 3.516Epoch   9:  82% | abe: 3.574 | eve: 9.002 | bob: 3.515Epoch   9:  82% | abe: 3.572 | eve: 9.002 | bob: 3.513Epoch   9:  83% | abe: 3.571 | eve: 9.002 | bob: 3.512Epoch   9:  84% | abe: 3.570 | eve: 9.002 | bob: 3.511Epoch   9:  85% | abe: 3.569 | eve: 9.002 | bob: 3.510Epoch   9:  85% | abe: 3.568 | eve: 9.002 | bob: 3.509Epoch   9:  86% | abe: 3.567 | eve: 9.002 | bob: 3.508Epoch   9:  87% | abe: 3.566 | eve: 9.002 | bob: 3.506Epoch   9:  88% | abe: 3.565 | eve: 9.002 | bob: 3.506Epoch   9:  89% | abe: 3.564 | eve: 9.001 | bob: 3.504Epoch   9:  89% | abe: 3.563 | eve: 9.002 | bob: 3.503Epoch   9:  90% | abe: 3.562 | eve: 9.002 | bob: 3.502Epoch   9:  91% | abe: 3.560 | eve: 9.002 | bob: 3.501Epoch   9:  92% | abe: 3.559 | eve: 9.001 | bob: 3.499Epoch   9:  92% | abe: 3.558 | eve: 9.002 | bob: 3.498Epoch   9:  93% | abe: 3.557 | eve: 9.002 | bob: 3.497Epoch   9:  94% | abe: 3.555 | eve: 9.001 | bob: 3.496Epoch   9:  95% | abe: 3.555 | eve: 9.001 | bob: 3.495Epoch   9:  96% | abe: 3.554 | eve: 9.001 | bob: 3.493Epoch   9:  96% | abe: 3.553 | eve: 9.001 | bob: 3.492Epoch   9:  97% | abe: 3.552 | eve: 9.001 | bob: 3.491Epoch   9:  98% | abe: 3.551 | eve: 9.001 | bob: 3.491Epoch   9:  99% | abe: 3.550 | eve: 9.001 | bob: 3.490
New best Bob loss 3.489611822530378 at epoch 9
Epoch  10:   0% | abe: 3.405 | eve: 9.014 | bob: 3.345Epoch  10:   0% | abe: 3.422 | eve: 9.012 | bob: 3.360Epoch  10:   1% | abe: 3.421 | eve: 9.012 | bob: 3.356Epoch  10:   2% | abe: 3.417 | eve: 9.008 | bob: 3.353Epoch  10:   3% | abe: 3.421 | eve: 9.006 | bob: 3.357Epoch  10:   3% | abe: 3.422 | eve: 9.003 | bob: 3.356Epoch  10:   4% | abe: 3.419 | eve: 9.001 | bob: 3.353Epoch  10:   5% | abe: 3.420 | eve: 9.006 | bob: 3.354Epoch  10:   6% | abe: 3.417 | eve: 9.005 | bob: 3.351Epoch  10:   7% | abe: 3.413 | eve: 9.008 | bob: 3.348Epoch  10:   7% | abe: 3.413 | eve: 9.006 | bob: 3.347Epoch  10:   8% | abe: 3.411 | eve: 9.005 | bob: 3.345Epoch  10:   9% | abe: 3.410 | eve: 9.005 | bob: 3.344Epoch  10:  10% | abe: 3.410 | eve: 9.005 | bob: 3.344Epoch  10:  10% | abe: 3.408 | eve: 9.004 | bob: 3.343Epoch  10:  11% | abe: 3.406 | eve: 9.003 | bob: 3.342Epoch  10:  12% | abe: 3.405 | eve: 9.003 | bob: 3.341Epoch  10:  13% | abe: 3.404 | eve: 9.004 | bob: 3.340Epoch  10:  14% | abe: 3.404 | eve: 9.001 | bob: 3.340Epoch  10:  14% | abe: 3.403 | eve: 9.002 | bob: 3.338Epoch  10:  15% | abe: 3.401 | eve: 9.002 | bob: 3.336Epoch  10:  16% | abe: 3.400 | eve: 9.004 | bob: 3.335Epoch  10:  17% | abe: 3.398 | eve: 9.002 | bob: 3.333Epoch  10:  17% | abe: 3.396 | eve: 9.002 | bob: 3.331Epoch  10:  18% | abe: 3.395 | eve: 9.002 | bob: 3.330Epoch  10:  19% | abe: 3.395 | eve: 9.001 | bob: 3.329Epoch  10:  20% | abe: 3.395 | eve: 9.001 | bob: 3.330Epoch  10:  21% | abe: 3.394 | eve: 9.000 | bob: 3.328Epoch  10:  21% | abe: 3.393 | eve: 9.000 | bob: 3.328Epoch  10:  22% | abe: 3.391 | eve: 9.000 | bob: 3.326Epoch  10:  23% | abe: 3.390 | eve: 9.001 | bob: 3.324Epoch  10:  24% | abe: 3.389 | eve: 9.000 | bob: 3.323Epoch  10:  25% | abe: 3.389 | eve: 8.999 | bob: 3.323Epoch  10:  25% | abe: 3.388 | eve: 8.999 | bob: 3.322Epoch  10:  26% | abe: 3.387 | eve: 8.999 | bob: 3.321Epoch  10:  27% | abe: 3.387 | eve: 9.000 | bob: 3.321Epoch  10:  28% | abe: 3.386 | eve: 9.000 | bob: 3.320Epoch  10:  28% | abe: 3.386 | eve: 9.001 | bob: 3.320Epoch  10:  29% | abe: 3.386 | eve: 9.000 | bob: 3.320Epoch  10:  30% | abe: 3.386 | eve: 9.002 | bob: 3.319Epoch  10:  31% | abe: 3.385 | eve: 9.001 | bob: 3.319Epoch  10:  32% | abe: 3.385 | eve: 9.002 | bob: 3.318Epoch  10:  32% | abe: 3.385 | eve: 9.002 | bob: 3.317Epoch  10:  33% | abe: 3.384 | eve: 9.002 | bob: 3.317Epoch  10:  34% | abe: 3.383 | eve: 9.003 | bob: 3.316Epoch  10:  35% | abe: 3.382 | eve: 9.002 | bob: 3.316Epoch  10:  35% | abe: 3.382 | eve: 9.002 | bob: 3.315Epoch  10:  36% | abe: 3.381 | eve: 9.003 | bob: 3.314Epoch  10:  37% | abe: 3.380 | eve: 9.003 | bob: 3.313Epoch  10:  38% | abe: 3.379 | eve: 9.003 | bob: 3.312Epoch  10:  39% | abe: 3.379 | eve: 9.002 | bob: 3.312Epoch  10:  39% | abe: 3.378 | eve: 9.001 | bob: 3.311Epoch  10:  40% | abe: 3.377 | eve: 9.001 | bob: 3.310Epoch  10:  41% | abe: 3.377 | eve: 9.001 | bob: 3.309Epoch  10:  42% | abe: 3.377 | eve: 9.001 | bob: 3.309Epoch  10:  42% | abe: 3.376 | eve: 9.001 | bob: 3.308Epoch  10:  43% | abe: 3.375 | eve: 9.000 | bob: 3.308Epoch  10:  44% | abe: 3.374 | eve: 9.001 | bob: 3.307Epoch  10:  45% | abe: 3.374 | eve: 9.000 | bob: 3.306Epoch  10:  46% | abe: 3.373 | eve: 9.000 | bob: 3.305Epoch  10:  46% | abe: 3.373 | eve: 9.000 | bob: 3.304Epoch  10:  47% | abe: 3.372 | eve: 9.000 | bob: 3.304Epoch  10:  48% | abe: 3.371 | eve: 9.001 | bob: 3.303Epoch  10:  49% | abe: 3.371 | eve: 9.001 | bob: 3.302Epoch  10:  50% | abe: 3.371 | eve: 9.000 | bob: 3.302Epoch  10:  50% | abe: 3.370 | eve: 9.001 | bob: 3.301Epoch  10:  51% | abe: 3.370 | eve: 9.001 | bob: 3.301Epoch  10:  52% | abe: 3.369 | eve: 9.001 | bob: 3.300Epoch  10:  53% | abe: 3.369 | eve: 9.001 | bob: 3.300Epoch  10:  53% | abe: 3.368 | eve: 9.001 | bob: 3.299Epoch  10:  54% | abe: 3.368 | eve: 9.001 | bob: 3.299Epoch  10:  55% | abe: 3.368 | eve: 9.000 | bob: 3.299Epoch  10:  56% | abe: 3.367 | eve: 8.999 | bob: 3.298Epoch  10:  57% | abe: 3.366 | eve: 9.000 | bob: 3.297Epoch  10:  57% | abe: 3.366 | eve: 9.000 | bob: 3.296Epoch  10:  58% | abe: 3.365 | eve: 8.999 | bob: 3.296Epoch  10:  59% | abe: 3.364 | eve: 8.999 | bob: 3.295Epoch  10:  60% | abe: 3.364 | eve: 8.999 | bob: 3.294Epoch  10:  60% | abe: 3.363 | eve: 8.999 | bob: 3.294Epoch  10:  61% | abe: 3.363 | eve: 8.998 | bob: 3.293Epoch  10:  62% | abe: 3.362 | eve: 8.998 | bob: 3.293Epoch  10:  63% | abe: 3.362 | eve: 8.998 | bob: 3.292Epoch  10:  64% | abe: 3.361 | eve: 8.998 | bob: 3.291Epoch  10:  64% | abe: 3.360 | eve: 8.998 | bob: 3.291Epoch  10:  65% | abe: 3.360 | eve: 8.998 | bob: 3.290Epoch  10:  66% | abe: 3.360 | eve: 8.998 | bob: 3.290Epoch  10:  67% | abe: 3.359 | eve: 8.998 | bob: 3.289Epoch  10:  67% | abe: 3.359 | eve: 8.998 | bob: 3.289Epoch  10:  68% | abe: 3.358 | eve: 8.999 | bob: 3.288Epoch  10:  69% | abe: 3.358 | eve: 8.999 | bob: 3.288Epoch  10:  70% | abe: 3.357 | eve: 8.999 | bob: 3.287Epoch  10:  71% | abe: 3.357 | eve: 8.999 | bob: 3.286Epoch  10:  71% | abe: 3.356 | eve: 8.999 | bob: 3.285Epoch  10:  72% | abe: 3.356 | eve: 9.000 | bob: 3.285Epoch  10:  73% | abe: 3.355 | eve: 9.000 | bob: 3.285Epoch  10:  74% | abe: 3.355 | eve: 8.999 | bob: 3.285Epoch  10:  75% | abe: 3.355 | eve: 8.999 | bob: 3.284Epoch  10:  75% | abe: 3.354 | eve: 8.999 | bob: 3.283Epoch  10:  76% | abe: 3.354 | eve: 8.999 | bob: 3.283Epoch  10:  77% | abe: 3.353 | eve: 8.999 | bob: 3.282Epoch  10:  78% | abe: 3.353 | eve: 8.999 | bob: 3.282Epoch  10:  78% | abe: 3.352 | eve: 9.000 | bob: 3.281Epoch  10:  79% | abe: 3.352 | eve: 9.000 | bob: 3.281Epoch  10:  80% | abe: 3.352 | eve: 9.000 | bob: 3.281Epoch  10:  81% | abe: 3.351 | eve: 9.000 | bob: 3.280Epoch  10:  82% | abe: 3.350 | eve: 8.999 | bob: 3.279Epoch  10:  82% | abe: 3.350 | eve: 8.999 | bob: 3.279Epoch  10:  83% | abe: 3.349 | eve: 8.999 | bob: 3.278Epoch  10:  84% | abe: 3.349 | eve: 9.000 | bob: 3.278Epoch  10:  85% | abe: 3.348 | eve: 9.000 | bob: 3.277Epoch  10:  85% | abe: 3.348 | eve: 8.999 | bob: 3.277Epoch  10:  86% | abe: 3.347 | eve: 8.999 | bob: 3.276Epoch  10:  87% | abe: 3.347 | eve: 8.999 | bob: 3.276Epoch  10:  88% | abe: 3.346 | eve: 8.999 | bob: 3.275Epoch  10:  89% | abe: 3.346 | eve: 8.999 | bob: 3.275Epoch  10:  89% | abe: 3.345 | eve: 8.999 | bob: 3.274Epoch  10:  90% | abe: 3.345 | eve: 8.999 | bob: 3.274Epoch  10:  91% | abe: 3.344 | eve: 8.999 | bob: 3.273Epoch  10:  92% | abe: 3.344 | eve: 8.999 | bob: 3.273Epoch  10:  92% | abe: 3.344 | eve: 8.999 | bob: 3.272Epoch  10:  93% | abe: 3.343 | eve: 8.999 | bob: 3.272Epoch  10:  94% | abe: 3.343 | eve: 8.999 | bob: 3.272Epoch  10:  95% | abe: 3.342 | eve: 8.999 | bob: 3.271Epoch  10:  96% | abe: 3.342 | eve: 8.999 | bob: 3.270Epoch  10:  96% | abe: 3.342 | eve: 8.999 | bob: 3.270Epoch  10:  97% | abe: 3.341 | eve: 8.999 | bob: 3.270Epoch  10:  98% | abe: 3.341 | eve: 8.999 | bob: 3.269Epoch  10:  99% | abe: 3.340 | eve: 8.999 | bob: 3.269
New best Bob loss 3.2688485240449836 at epoch 10
Epoch  11:   0% | abe: 3.287 | eve: 9.020 | bob: 3.210Epoch  11:   0% | abe: 3.291 | eve: 9.010 | bob: 3.214Epoch  11:   1% | abe: 3.284 | eve: 9.001 | bob: 3.206Epoch  11:   2% | abe: 3.282 | eve: 9.007 | bob: 3.207Epoch  11:   3% | abe: 3.280 | eve: 9.010 | bob: 3.207Epoch  11:   3% | abe: 3.281 | eve: 9.017 | bob: 3.208Epoch  11:   4% | abe: 3.281 | eve: 9.012 | bob: 3.208Epoch  11:   5% | abe: 3.279 | eve: 9.009 | bob: 3.206Epoch  11:   6% | abe: 3.278 | eve: 9.012 | bob: 3.207Epoch  11:   7% | abe: 3.280 | eve: 9.010 | bob: 3.209Epoch  11:   7% | abe: 3.279 | eve: 9.009 | bob: 3.208Epoch  11:   8% | abe: 3.278 | eve: 9.009 | bob: 3.206Epoch  11:   9% | abe: 3.274 | eve: 9.010 | bob: 3.203Epoch  11:  10% | abe: 3.275 | eve: 9.009 | bob: 3.203Epoch  11:  10% | abe: 3.274 | eve: 9.008 | bob: 3.203Epoch  11:  11% | abe: 3.274 | eve: 9.006 | bob: 3.203Epoch  11:  12% | abe: 3.275 | eve: 9.007 | bob: 3.203Epoch  11:  13% | abe: 3.274 | eve: 9.007 | bob: 3.202Epoch  11:  14% | abe: 3.275 | eve: 9.008 | bob: 3.203Epoch  11:  14% | abe: 3.275 | eve: 9.007 | bob: 3.203Epoch  11:  15% | abe: 3.273 | eve: 9.007 | bob: 3.201Epoch  11:  16% | abe: 3.272 | eve: 9.006 | bob: 3.199Epoch  11:  17% | abe: 3.273 | eve: 9.005 | bob: 3.200Epoch  11:  17% | abe: 3.273 | eve: 9.005 | bob: 3.200Epoch  11:  18% | abe: 3.273 | eve: 9.004 | bob: 3.200Epoch  11:  19% | abe: 3.272 | eve: 9.004 | bob: 3.200Epoch  11:  20% | abe: 3.274 | eve: 9.003 | bob: 3.201Epoch  11:  21% | abe: 3.274 | eve: 9.003 | bob: 3.201Epoch  11:  21% | abe: 3.274 | eve: 9.001 | bob: 3.201Epoch  11:  22% | abe: 3.273 | eve: 9.001 | bob: 3.200Epoch  11:  23% | abe: 3.273 | eve: 9.002 | bob: 3.201Epoch  11:  24% | abe: 3.273 | eve: 9.002 | bob: 3.200Epoch  11:  25% | abe: 3.272 | eve: 9.002 | bob: 3.200Epoch  11:  25% | abe: 3.273 | eve: 9.001 | bob: 3.200Epoch  11:  26% | abe: 3.273 | eve: 9.002 | bob: 3.200Epoch  11:  27% | abe: 3.273 | eve: 9.002 | bob: 3.200Epoch  11:  28% | abe: 3.272 | eve: 9.002 | bob: 3.200Epoch  11:  28% | abe: 3.271 | eve: 9.002 | bob: 3.199Epoch  11:  29% | abe: 3.270 | eve: 9.002 | bob: 3.198Epoch  11:  30% | abe: 3.271 | eve: 9.002 | bob: 3.198Epoch  11:  31% | abe: 3.271 | eve: 9.002 | bob: 3.199Epoch  11:  32% | abe: 3.271 | eve: 9.001 | bob: 3.199Epoch  11:  32% | abe: 3.271 | eve: 9.001 | bob: 3.199Epoch  11:  33% | abe: 3.271 | eve: 9.000 | bob: 3.199Epoch  11:  34% | abe: 3.270 | eve: 9.000 | bob: 3.198Epoch  11:  35% | abe: 3.269 | eve: 9.001 | bob: 3.197Epoch  11:  35% | abe: 3.269 | eve: 9.000 | bob: 3.197Epoch  11:  36% | abe: 3.269 | eve: 9.001 | bob: 3.197Epoch  11:  37% | abe: 3.269 | eve: 9.001 | bob: 3.197Epoch  11:  38% | abe: 3.268 | eve: 9.001 | bob: 3.196Epoch  11:  39% | abe: 3.268 | eve: 9.002 | bob: 3.196Epoch  11:  39% | abe: 3.267 | eve: 9.001 | bob: 3.195Epoch  11:  40% | abe: 3.267 | eve: 9.002 | bob: 3.194Epoch  11:  41% | abe: 3.266 | eve: 9.002 | bob: 3.194Epoch  11:  42% | abe: 3.266 | eve: 9.002 | bob: 3.194Epoch  11:  42% | abe: 3.266 | eve: 9.002 | bob: 3.194Epoch  11:  43% | abe: 3.266 | eve: 9.002 | bob: 3.193Epoch  11:  44% | abe: 3.266 | eve: 9.002 | bob: 3.193Epoch  11:  45% | abe: 3.266 | eve: 9.002 | bob: 3.193Epoch  11:  46% | abe: 3.266 | eve: 9.002 | bob: 3.193Epoch  11:  46% | abe: 3.265 | eve: 9.002 | bob: 3.192Epoch  11:  47% | abe: 3.264 | eve: 9.001 | bob: 3.192Epoch  11:  48% | abe: 3.264 | eve: 9.001 | bob: 3.191Epoch  11:  49% | abe: 3.263 | eve: 9.001 | bob: 3.190Epoch  11:  50% | abe: 3.263 | eve: 9.002 | bob: 3.190Epoch  11:  50% | abe: 3.263 | eve: 9.002 | bob: 3.190Epoch  11:  51% | abe: 3.263 | eve: 9.002 | bob: 3.190Epoch  11:  52% | abe: 3.262 | eve: 9.002 | bob: 3.189Epoch  11:  53% | abe: 3.262 | eve: 9.002 | bob: 3.189Epoch  11:  53% | abe: 3.262 | eve: 9.002 | bob: 3.189Epoch  11:  54% | abe: 3.262 | eve: 9.002 | bob: 3.189Epoch  11:  55% | abe: 3.262 | eve: 9.002 | bob: 3.188Epoch  11:  56% | abe: 3.262 | eve: 9.002 | bob: 3.189Epoch  11:  57% | abe: 3.262 | eve: 9.001 | bob: 3.189Epoch  11:  57% | abe: 3.261 | eve: 9.001 | bob: 3.188Epoch  11:  58% | abe: 3.262 | eve: 9.001 | bob: 3.188Epoch  11:  59% | abe: 3.262 | eve: 9.001 | bob: 3.189Epoch  11:  60% | abe: 3.261 | eve: 9.001 | bob: 3.189Epoch  11:  60% | abe: 3.261 | eve: 9.002 | bob: 3.188Epoch  11:  61% | abe: 3.261 | eve: 9.002 | bob: 3.188Epoch  11:  62% | abe: 3.261 | eve: 9.001 | bob: 3.187Epoch  11:  63% | abe: 3.260 | eve: 9.001 | bob: 3.187Epoch  11:  64% | abe: 3.259 | eve: 9.001 | bob: 3.186Epoch  11:  64% | abe: 3.259 | eve: 9.001 | bob: 3.186Epoch  11:  65% | abe: 3.259 | eve: 9.002 | bob: 3.186Epoch  11:  66% | abe: 3.259 | eve: 9.001 | bob: 3.185Epoch  11:  67% | abe: 3.259 | eve: 9.001 | bob: 3.185Epoch  11:  67% | abe: 3.259 | eve: 9.001 | bob: 3.185Epoch  11:  68% | abe: 3.258 | eve: 9.001 | bob: 3.185Epoch  11:  69% | abe: 3.258 | eve: 9.001 | bob: 3.184Epoch  11:  70% | abe: 3.258 | eve: 9.001 | bob: 3.184Epoch  11:  71% | abe: 3.257 | eve: 9.001 | bob: 3.184Epoch  11:  71% | abe: 3.257 | eve: 9.001 | bob: 3.183Epoch  11:  72% | abe: 3.257 | eve: 9.001 | bob: 3.183Epoch  11:  73% | abe: 3.257 | eve: 9.002 | bob: 3.183Epoch  11:  74% | abe: 3.256 | eve: 9.002 | bob: 3.182Epoch  11:  75% | abe: 3.256 | eve: 9.002 | bob: 3.182Epoch  11:  75% | abe: 3.255 | eve: 9.002 | bob: 3.181Epoch  11:  76% | abe: 3.255 | eve: 9.001 | bob: 3.181Epoch  11:  77% | abe: 3.255 | eve: 9.001 | bob: 3.181Epoch  11:  78% | abe: 3.255 | eve: 9.001 | bob: 3.180Epoch  11:  78% | abe: 3.254 | eve: 9.001 | bob: 3.180Epoch  11:  79% | abe: 3.254 | eve: 9.001 | bob: 3.180Epoch  11:  80% | abe: 3.254 | eve: 9.001 | bob: 3.179Epoch  11:  81% | abe: 3.253 | eve: 9.001 | bob: 3.179Epoch  11:  82% | abe: 3.253 | eve: 9.001 | bob: 3.179Epoch  11:  82% | abe: 3.253 | eve: 9.001 | bob: 3.178Epoch  11:  83% | abe: 3.252 | eve: 9.001 | bob: 3.178Epoch  11:  84% | abe: 3.252 | eve: 9.001 | bob: 3.178Epoch  11:  85% | abe: 3.252 | eve: 9.000 | bob: 3.177Epoch  11:  85% | abe: 3.252 | eve: 9.001 | bob: 3.177Epoch  11:  86% | abe: 3.251 | eve: 9.001 | bob: 3.176Epoch  11:  87% | abe: 3.251 | eve: 9.001 | bob: 3.176Epoch  11:  88% | abe: 3.251 | eve: 9.001 | bob: 3.176Epoch  11:  89% | abe: 3.250 | eve: 9.001 | bob: 3.176Epoch  11:  89% | abe: 3.250 | eve: 9.001 | bob: 3.175Epoch  11:  90% | abe: 3.249 | eve: 9.001 | bob: 3.174Epoch  11:  91% | abe: 3.249 | eve: 9.001 | bob: 3.174Epoch  11:  92% | abe: 3.249 | eve: 9.001 | bob: 3.174Epoch  11:  92% | abe: 3.249 | eve: 9.001 | bob: 3.174Epoch  11:  93% | abe: 3.249 | eve: 9.001 | bob: 3.174Epoch  11:  94% | abe: 3.248 | eve: 9.001 | bob: 3.173Epoch  11:  95% | abe: 3.248 | eve: 9.001 | bob: 3.173Epoch  11:  96% | abe: 3.248 | eve: 9.001 | bob: 3.173Epoch  11:  96% | abe: 3.248 | eve: 9.001 | bob: 3.173Epoch  11:  97% | abe: 3.248 | eve: 9.001 | bob: 3.172Epoch  11:  98% | abe: 3.247 | eve: 9.000 | bob: 3.172Epoch  11:  99% | abe: 3.247 | eve: 9.001 | bob: 3.172
New best Bob loss 3.172003941762682 at epoch 11
Epoch  12:   0% | abe: 3.235 | eve: 9.010 | bob: 3.158Epoch  12:   0% | abe: 3.220 | eve: 9.012 | bob: 3.140Epoch  12:   1% | abe: 3.222 | eve: 9.013 | bob: 3.140Epoch  12:   2% | abe: 3.225 | eve: 9.004 | bob: 3.145Epoch  12:   3% | abe: 3.218 | eve: 8.998 | bob: 3.137Epoch  12:   3% | abe: 3.218 | eve: 8.994 | bob: 3.136Epoch  12:   4% | abe: 3.218 | eve: 8.996 | bob: 3.136Epoch  12:   5% | abe: 3.220 | eve: 8.995 | bob: 3.138Epoch  12:   6% | abe: 3.221 | eve: 9.000 | bob: 3.140Epoch  12:   7% | abe: 3.220 | eve: 9.000 | bob: 3.140Epoch  12:   7% | abe: 3.221 | eve: 9.000 | bob: 3.141Epoch  12:   8% | abe: 3.219 | eve: 9.002 | bob: 3.140Epoch  12:   9% | abe: 3.219 | eve: 9.003 | bob: 3.139Epoch  12:  10% | abe: 3.216 | eve: 9.004 | bob: 3.135Epoch  12:  10% | abe: 3.218 | eve: 9.002 | bob: 3.137Epoch  12:  11% | abe: 3.218 | eve: 9.000 | bob: 3.136Epoch  12:  12% | abe: 3.219 | eve: 9.000 | bob: 3.137Epoch  12:  13% | abe: 3.218 | eve: 9.002 | bob: 3.136Epoch  12:  14% | abe: 3.216 | eve: 9.002 | bob: 3.135Epoch  12:  14% | abe: 3.216 | eve: 9.000 | bob: 3.135Epoch  12:  15% | abe: 3.216 | eve: 8.999 | bob: 3.135Epoch  12:  16% | abe: 3.217 | eve: 8.999 | bob: 3.136Epoch  12:  17% | abe: 3.217 | eve: 9.001 | bob: 3.137Epoch  12:  17% | abe: 3.216 | eve: 8.999 | bob: 3.136Epoch  12:  18% | abe: 3.215 | eve: 9.000 | bob: 3.135Epoch  12:  19% | abe: 3.215 | eve: 9.000 | bob: 3.134Epoch  12:  20% | abe: 3.215 | eve: 9.000 | bob: 3.134Epoch  12:  21% | abe: 3.215 | eve: 9.000 | bob: 3.135Epoch  12:  21% | abe: 3.214 | eve: 9.000 | bob: 3.135Epoch  12:  22% | abe: 3.213 | eve: 9.000 | bob: 3.133Epoch  12:  23% | abe: 3.213 | eve: 9.000 | bob: 3.133Epoch  12:  24% | abe: 3.213 | eve: 9.000 | bob: 3.133Epoch  12:  25% | abe: 3.213 | eve: 8.998 | bob: 3.133Epoch  12:  25% | abe: 3.213 | eve: 8.998 | bob: 3.134Epoch  12:  26% | abe: 3.213 | eve: 8.998 | bob: 3.133Epoch  12:  27% | abe: 3.211 | eve: 8.998 | bob: 3.132Epoch  12:  28% | abe: 3.212 | eve: 8.998 | bob: 3.132Epoch  12:  28% | abe: 3.212 | eve: 8.998 | bob: 3.132Epoch  12:  29% | abe: 3.211 | eve: 8.998 | bob: 3.132Epoch  12:  30% | abe: 3.211 | eve: 8.998 | bob: 3.131Epoch  12:  31% | abe: 3.211 | eve: 8.997 | bob: 3.131Epoch  12:  32% | abe: 3.210 | eve: 8.997 | bob: 3.131Epoch  12:  32% | abe: 3.210 | eve: 8.997 | bob: 3.130Epoch  12:  33% | abe: 3.210 | eve: 8.997 | bob: 3.130Epoch  12:  34% | abe: 3.210 | eve: 8.997 | bob: 3.130Epoch  12:  35% | abe: 3.209 | eve: 8.997 | bob: 3.130Epoch  12:  35% | abe: 3.209 | eve: 8.997 | bob: 3.130Epoch  12:  36% | abe: 3.209 | eve: 8.998 | bob: 3.130Epoch  12:  37% | abe: 3.209 | eve: 8.998 | bob: 3.129Epoch  12:  38% | abe: 3.208 | eve: 8.998 | bob: 3.129Epoch  12:  39% | abe: 3.208 | eve: 8.998 | bob: 3.128Epoch  12:  39% | abe: 3.207 | eve: 8.998 | bob: 3.128Epoch  12:  40% | abe: 3.207 | eve: 8.998 | bob: 3.128Epoch  12:  41% | abe: 3.206 | eve: 8.998 | bob: 3.127Epoch  12:  42% | abe: 3.205 | eve: 8.998 | bob: 3.127Epoch  12:  42% | abe: 3.205 | eve: 8.998 | bob: 3.127Epoch  12:  43% | abe: 3.205 | eve: 8.998 | bob: 3.127Epoch  12:  44% | abe: 3.205 | eve: 8.997 | bob: 3.127Epoch  12:  45% | abe: 3.205 | eve: 8.997 | bob: 3.127Epoch  12:  46% | abe: 3.204 | eve: 8.997 | bob: 3.126Epoch  12:  46% | abe: 3.204 | eve: 8.997 | bob: 3.126Epoch  12:  47% | abe: 3.203 | eve: 8.998 | bob: 3.126Epoch  12:  48% | abe: 3.203 | eve: 8.998 | bob: 3.125Epoch  12:  49% | abe: 3.203 | eve: 8.998 | bob: 3.125Epoch  12:  50% | abe: 3.202 | eve: 8.999 | bob: 3.124Epoch  12:  50% | abe: 3.202 | eve: 8.998 | bob: 3.124Epoch  12:  51% | abe: 3.202 | eve: 8.998 | bob: 3.124Epoch  12:  52% | abe: 3.202 | eve: 8.998 | bob: 3.124Epoch  12:  53% | abe: 3.202 | eve: 8.998 | bob: 3.124Epoch  12:  53% | abe: 3.202 | eve: 8.998 | bob: 3.124Epoch  12:  54% | abe: 3.201 | eve: 8.998 | bob: 3.124Epoch  12:  55% | abe: 3.201 | eve: 8.997 | bob: 3.124Epoch  12:  56% | abe: 3.201 | eve: 8.997 | bob: 3.123Epoch  12:  57% | abe: 3.200 | eve: 8.997 | bob: 3.123Epoch  12:  57% | abe: 3.200 | eve: 8.997 | bob: 3.123Epoch  12:  58% | abe: 3.201 | eve: 8.997 | bob: 3.123Epoch  12:  59% | abe: 3.200 | eve: 8.997 | bob: 3.123Epoch  12:  60% | abe: 3.200 | eve: 8.997 | bob: 3.123Epoch  12:  60% | abe: 3.200 | eve: 8.997 | bob: 3.123Epoch  12:  61% | abe: 3.200 | eve: 8.997 | bob: 3.122Epoch  12:  62% | abe: 3.200 | eve: 8.997 | bob: 3.122Epoch  12:  63% | abe: 3.199 | eve: 8.997 | bob: 3.122Epoch  12:  64% | abe: 3.199 | eve: 8.997 | bob: 3.122Epoch  12:  64% | abe: 3.199 | eve: 8.997 | bob: 3.122Epoch  12:  65% | abe: 3.199 | eve: 8.997 | bob: 3.122Epoch  12:  66% | abe: 3.198 | eve: 8.997 | bob: 3.121Epoch  12:  67% | abe: 3.198 | eve: 8.997 | bob: 3.121Epoch  12:  67% | abe: 3.198 | eve: 8.997 | bob: 3.121Epoch  12:  68% | abe: 3.197 | eve: 8.997 | bob: 3.120Epoch  12:  69% | abe: 3.197 | eve: 8.997 | bob: 3.121Epoch  12:  70% | abe: 3.197 | eve: 8.997 | bob: 3.121Epoch  12:  71% | abe: 3.197 | eve: 8.997 | bob: 3.121Epoch  12:  71% | abe: 3.197 | eve: 8.997 | bob: 3.120Epoch  12:  72% | abe: 3.196 | eve: 8.997 | bob: 3.120Epoch  12:  73% | abe: 3.196 | eve: 8.997 | bob: 3.120Epoch  12:  74% | abe: 3.196 | eve: 8.997 | bob: 3.120Epoch  12:  75% | abe: 3.196 | eve: 8.997 | bob: 3.119Epoch  12:  75% | abe: 3.196 | eve: 8.997 | bob: 3.119Epoch  12:  76% | abe: 3.196 | eve: 8.997 | bob: 3.119Epoch  12:  77% | abe: 3.195 | eve: 8.997 | bob: 3.119Epoch  12:  78% | abe: 3.196 | eve: 8.997 | bob: 3.119Epoch  12:  78% | abe: 3.196 | eve: 8.997 | bob: 3.119Epoch  12:  79% | abe: 3.195 | eve: 8.996 | bob: 3.119Epoch  12:  80% | abe: 3.195 | eve: 8.996 | bob: 3.119Epoch  12:  81% | abe: 3.195 | eve: 8.996 | bob: 3.119Epoch  12:  82% | abe: 3.195 | eve: 8.996 | bob: 3.118Epoch  12:  82% | abe: 3.195 | eve: 8.995 | bob: 3.118Epoch  12:  83% | abe: 3.194 | eve: 8.996 | bob: 3.118Epoch  12:  84% | abe: 3.194 | eve: 8.996 | bob: 3.117Epoch  12:  85% | abe: 3.194 | eve: 8.996 | bob: 3.117Epoch  12:  85% | abe: 3.193 | eve: 8.997 | bob: 3.117Epoch  12:  86% | abe: 3.193 | eve: 8.997 | bob: 3.117Epoch  12:  87% | abe: 3.193 | eve: 8.997 | bob: 3.117Epoch  12:  88% | abe: 3.193 | eve: 8.997 | bob: 3.116Epoch  12:  89% | abe: 3.193 | eve: 8.997 | bob: 3.116Epoch  12:  89% | abe: 3.193 | eve: 8.998 | bob: 3.116Epoch  12:  90% | abe: 3.193 | eve: 8.998 | bob: 3.116Epoch  12:  91% | abe: 3.192 | eve: 8.998 | bob: 3.116Epoch  12:  92% | abe: 3.192 | eve: 8.998 | bob: 3.115Epoch  12:  92% | abe: 3.192 | eve: 8.999 | bob: 3.115Epoch  12:  93% | abe: 3.191 | eve: 8.998 | bob: 3.115Epoch  12:  94% | abe: 3.191 | eve: 8.998 | bob: 3.115Epoch  12:  95% | abe: 3.191 | eve: 8.998 | bob: 3.115Epoch  12:  96% | abe: 3.191 | eve: 8.998 | bob: 3.114Epoch  12:  96% | abe: 3.191 | eve: 8.998 | bob: 3.114Epoch  12:  97% | abe: 3.191 | eve: 8.998 | bob: 3.114Epoch  12:  98% | abe: 3.191 | eve: 8.998 | bob: 3.114Epoch  12:  99% | abe: 3.191 | eve: 8.998 | bob: 3.114
New best Bob loss 3.113940047890196 at epoch 12
Epoch  13:   0% | abe: 3.178 | eve: 8.966 | bob: 3.108Epoch  13:   0% | abe: 3.176 | eve: 8.979 | bob: 3.103Epoch  13:   1% | abe: 3.181 | eve: 8.980 | bob: 3.105Epoch  13:   2% | abe: 3.177 | eve: 8.978 | bob: 3.097Epoch  13:   3% | abe: 3.176 | eve: 8.977 | bob: 3.093Epoch  13:   3% | abe: 3.174 | eve: 8.985 | bob: 3.092Epoch  13:   4% | abe: 3.177 | eve: 8.991 | bob: 3.097Epoch  13:   5% | abe: 3.174 | eve: 8.992 | bob: 3.097Epoch  13:   6% | abe: 3.175 | eve: 8.992 | bob: 3.100Epoch  13:   7% | abe: 3.171 | eve: 8.991 | bob: 3.097Epoch  13:   7% | abe: 3.173 | eve: 8.992 | bob: 3.098Epoch  13:   8% | abe: 3.174 | eve: 8.995 | bob: 3.099Epoch  13:   9% | abe: 3.173 | eve: 8.997 | bob: 3.098Epoch  13:  10% | abe: 3.171 | eve: 8.996 | bob: 3.097Epoch  13:  10% | abe: 3.170 | eve: 8.996 | bob: 3.096Epoch  13:  11% | abe: 3.170 | eve: 8.998 | bob: 3.097Epoch  13:  12% | abe: 3.169 | eve: 8.999 | bob: 3.094Epoch  13:  13% | abe: 3.168 | eve: 9.001 | bob: 3.094Epoch  13:  14% | abe: 3.167 | eve: 8.999 | bob: 3.093Epoch  13:  14% | abe: 3.166 | eve: 8.999 | bob: 3.091Epoch  13:  15% | abe: 3.167 | eve: 8.999 | bob: 3.093Epoch  13:  16% | abe: 3.166 | eve: 8.999 | bob: 3.092Epoch  13:  17% | abe: 3.165 | eve: 9.000 | bob: 3.091Epoch  13:  17% | abe: 3.165 | eve: 9.000 | bob: 3.091Epoch  13:  18% | abe: 3.166 | eve: 9.001 | bob: 3.091Epoch  13:  19% | abe: 3.166 | eve: 9.001 | bob: 3.092Epoch  13:  20% | abe: 3.167 | eve: 9.002 | bob: 3.093Epoch  13:  21% | abe: 3.167 | eve: 9.003 | bob: 3.092Epoch  13:  21% | abe: 3.167 | eve: 9.003 | bob: 3.093Epoch  13:  22% | abe: 3.165 | eve: 9.003 | bob: 3.091Epoch  13:  23% | abe: 3.165 | eve: 9.002 | bob: 3.091Epoch  13:  24% | abe: 3.165 | eve: 9.001 | bob: 3.090Epoch  13:  25% | abe: 3.165 | eve: 9.000 | bob: 3.090Epoch  13:  25% | abe: 3.165 | eve: 9.000 | bob: 3.091Epoch  13:  26% | abe: 3.166 | eve: 9.000 | bob: 3.092Epoch  13:  27% | abe: 3.166 | eve: 9.000 | bob: 3.091Epoch  13:  28% | abe: 3.166 | eve: 9.000 | bob: 3.091Epoch  13:  28% | abe: 3.166 | eve: 9.001 | bob: 3.091Epoch  13:  29% | abe: 3.166 | eve: 9.000 | bob: 3.091Epoch  13:  30% | abe: 3.166 | eve: 9.001 | bob: 3.091Epoch  13:  31% | abe: 3.167 | eve: 9.000 | bob: 3.092Epoch  13:  32% | abe: 3.168 | eve: 9.000 | bob: 3.093Epoch  13:  32% | abe: 3.167 | eve: 9.001 | bob: 3.092Epoch  13:  33% | abe: 3.167 | eve: 9.001 | bob: 3.092Epoch  13:  34% | abe: 3.166 | eve: 9.001 | bob: 3.092Epoch  13:  35% | abe: 3.166 | eve: 9.001 | bob: 3.092Epoch  13:  35% | abe: 3.166 | eve: 9.001 | bob: 3.092Epoch  13:  36% | abe: 3.166 | eve: 9.001 | bob: 3.092Epoch  13:  37% | abe: 3.165 | eve: 9.001 | bob: 3.091Epoch  13:  38% | abe: 3.166 | eve: 9.002 | bob: 3.092Epoch  13:  39% | abe: 3.165 | eve: 9.002 | bob: 3.091Epoch  13:  39% | abe: 3.164 | eve: 9.002 | bob: 3.091Epoch  13:  40% | abe: 3.164 | eve: 9.002 | bob: 3.090Epoch  13:  41% | abe: 3.164 | eve: 9.002 | bob: 3.090Epoch  13:  42% | abe: 3.164 | eve: 9.003 | bob: 3.090Epoch  13:  42% | abe: 3.164 | eve: 9.003 | bob: 3.090Epoch  13:  43% | abe: 3.164 | eve: 9.003 | bob: 3.090Epoch  13:  44% | abe: 3.164 | eve: 9.003 | bob: 3.090Epoch  13:  45% | abe: 3.165 | eve: 9.003 | bob: 3.090Epoch  13:  46% | abe: 3.165 | eve: 9.003 | bob: 3.090Epoch  13:  46% | abe: 3.165 | eve: 9.003 | bob: 3.091Epoch  13:  47% | abe: 3.165 | eve: 9.003 | bob: 3.091Epoch  13:  48% | abe: 3.166 | eve: 9.003 | bob: 3.091Epoch  13:  49% | abe: 3.166 | eve: 9.003 | bob: 3.091Epoch  13:  50% | abe: 3.165 | eve: 9.003 | bob: 3.091Epoch  13:  50% | abe: 3.166 | eve: 9.003 | bob: 3.091Epoch  13:  51% | abe: 3.166 | eve: 9.002 | bob: 3.091Epoch  13:  52% | abe: 3.166 | eve: 9.002 | bob: 3.091Epoch  13:  53% | abe: 3.166 | eve: 9.002 | bob: 3.091Epoch  13:  53% | abe: 3.165 | eve: 9.002 | bob: 3.091Epoch  13:  54% | abe: 3.165 | eve: 9.002 | bob: 3.090Epoch  13:  55% | abe: 3.165 | eve: 9.002 | bob: 3.091Epoch  13:  56% | abe: 3.165 | eve: 9.002 | bob: 3.090Epoch  13:  57% | abe: 3.165 | eve: 9.002 | bob: 3.090Epoch  13:  57% | abe: 3.165 | eve: 9.002 | bob: 3.090Epoch  13:  58% | abe: 3.165 | eve: 9.001 | bob: 3.090Epoch  13:  59% | abe: 3.164 | eve: 9.001 | bob: 3.090Epoch  13:  60% | abe: 3.164 | eve: 9.001 | bob: 3.090Epoch  13:  60% | abe: 3.164 | eve: 9.001 | bob: 3.090Epoch  13:  61% | abe: 3.164 | eve: 9.001 | bob: 3.090Epoch  13:  62% | abe: 3.164 | eve: 9.001 | bob: 3.090Epoch  13:  63% | abe: 3.164 | eve: 9.001 | bob: 3.089Epoch  13:  64% | abe: 3.164 | eve: 9.000 | bob: 3.089Epoch  13:  64% | abe: 3.164 | eve: 9.001 | bob: 3.089Epoch  13:  65% | abe: 3.163 | eve: 9.001 | bob: 3.089Epoch  13:  66% | abe: 3.163 | eve: 9.000 | bob: 3.088Epoch  13:  67% | abe: 3.163 | eve: 9.000 | bob: 3.088Epoch  13:  67% | abe: 3.163 | eve: 9.000 | bob: 3.088Epoch  13:  68% | abe: 3.163 | eve: 9.000 | bob: 3.088Epoch  13:  69% | abe: 3.162 | eve: 9.000 | bob: 3.087Epoch  13:  70% | abe: 3.163 | eve: 9.000 | bob: 3.088Epoch  13:  71% | abe: 3.162 | eve: 9.000 | bob: 3.088Epoch  13:  71% | abe: 3.162 | eve: 9.000 | bob: 3.088Epoch  13:  72% | abe: 3.162 | eve: 9.000 | bob: 3.088Epoch  13:  73% | abe: 3.162 | eve: 9.000 | bob: 3.087Epoch  13:  74% | abe: 3.162 | eve: 9.000 | bob: 3.087Epoch  13:  75% | abe: 3.162 | eve: 9.000 | bob: 3.087Epoch  13:  75% | abe: 3.161 | eve: 9.000 | bob: 3.087Epoch  13:  76% | abe: 3.162 | eve: 9.000 | bob: 3.087Epoch  13:  77% | abe: 3.161 | eve: 9.000 | bob: 3.086Epoch  13:  78% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  78% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  79% | abe: 3.161 | eve: 9.000 | bob: 3.086Epoch  13:  80% | abe: 3.161 | eve: 9.000 | bob: 3.086Epoch  13:  81% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  82% | abe: 3.161 | eve: 9.001 | bob: 3.087Epoch  13:  82% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  83% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  84% | abe: 3.160 | eve: 9.001 | bob: 3.086Epoch  13:  85% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  85% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  86% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  87% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  88% | abe: 3.161 | eve: 9.001 | bob: 3.087Epoch  13:  89% | abe: 3.161 | eve: 9.001 | bob: 3.086Epoch  13:  89% | abe: 3.160 | eve: 9.000 | bob: 3.086Epoch  13:  90% | abe: 3.160 | eve: 9.001 | bob: 3.086Epoch  13:  91% | abe: 3.160 | eve: 9.001 | bob: 3.085Epoch  13:  92% | abe: 3.160 | eve: 9.001 | bob: 3.085Epoch  13:  92% | abe: 3.159 | eve: 9.001 | bob: 3.085Epoch  13:  93% | abe: 3.159 | eve: 9.001 | bob: 3.085Epoch  13:  94% | abe: 3.159 | eve: 9.000 | bob: 3.085Epoch  13:  95% | abe: 3.159 | eve: 9.000 | bob: 3.085Epoch  13:  96% | abe: 3.159 | eve: 9.000 | bob: 3.084Epoch  13:  96% | abe: 3.159 | eve: 9.000 | bob: 3.084Epoch  13:  97% | abe: 3.159 | eve: 9.000 | bob: 3.084Epoch  13:  98% | abe: 3.159 | eve: 9.000 | bob: 3.084Epoch  13:  99% | abe: 3.159 | eve: 9.000 | bob: 3.084
New best Bob loss 3.0841879975421307 at epoch 13
Epoch  14:   0% | abe: 3.127 | eve: 8.968 | bob: 3.052Epoch  14:   0% | abe: 3.138 | eve: 8.991 | bob: 3.066Epoch  14:   1% | abe: 3.142 | eve: 8.995 | bob: 3.068Epoch  14:   2% | abe: 3.138 | eve: 8.995 | bob: 3.063Epoch  14:   3% | abe: 3.145 | eve: 8.989 | bob: 3.070Epoch  14:   3% | abe: 3.148 | eve: 8.992 | bob: 3.074Epoch  14:   4% | abe: 3.148 | eve: 8.990 | bob: 3.075Epoch  14:   5% | abe: 3.150 | eve: 8.991 | bob: 3.077Epoch  14:   6% | abe: 3.148 | eve: 8.992 | bob: 3.077Epoch  14:   7% | abe: 3.147 | eve: 8.993 | bob: 3.075Epoch  14:   7% | abe: 3.148 | eve: 8.992 | bob: 3.076Epoch  14:   8% | abe: 3.148 | eve: 8.991 | bob: 3.075Epoch  14:   9% | abe: 3.145 | eve: 8.995 | bob: 3.072Epoch  14:  10% | abe: 3.144 | eve: 8.991 | bob: 3.071Epoch  14:  10% | abe: 3.144 | eve: 8.995 | bob: 3.071Epoch  14:  11% | abe: 3.146 | eve: 8.992 | bob: 3.074Epoch  14:  12% | abe: 3.146 | eve: 8.991 | bob: 3.073Epoch  14:  13% | abe: 3.147 | eve: 8.992 | bob: 3.074Epoch  14:  14% | abe: 3.147 | eve: 8.992 | bob: 3.073Epoch  14:  14% | abe: 3.146 | eve: 8.991 | bob: 3.072Epoch  14:  15% | abe: 3.146 | eve: 8.993 | bob: 3.072Epoch  14:  16% | abe: 3.147 | eve: 8.995 | bob: 3.073Epoch  14:  17% | abe: 3.147 | eve: 8.995 | bob: 3.072Epoch  14:  17% | abe: 3.146 | eve: 8.996 | bob: 3.072Epoch  14:  18% | abe: 3.146 | eve: 8.995 | bob: 3.072Epoch  14:  19% | abe: 3.147 | eve: 8.993 | bob: 3.072Epoch  14:  20% | abe: 3.147 | eve: 8.995 | bob: 3.073Epoch  14:  21% | abe: 3.145 | eve: 8.995 | bob: 3.070Epoch  14:  21% | abe: 3.145 | eve: 8.995 | bob: 3.070Epoch  14:  22% | abe: 3.145 | eve: 8.995 | bob: 3.070Epoch  14:  23% | abe: 3.145 | eve: 8.995 | bob: 3.071Epoch  14:  24% | abe: 3.145 | eve: 8.995 | bob: 3.071Epoch  14:  25% | abe: 3.145 | eve: 8.994 | bob: 3.071Epoch  14:  25% | abe: 3.145 | eve: 8.995 | bob: 3.071Epoch  14:  26% | abe: 3.145 | eve: 8.995 | bob: 3.071Epoch  14:  27% | abe: 3.145 | eve: 8.995 | bob: 3.071Epoch  14:  28% | abe: 3.146 | eve: 8.995 | bob: 3.071Epoch  14:  28% | abe: 3.146 | eve: 8.994 | bob: 3.072Epoch  14:  29% | abe: 3.146 | eve: 8.995 | bob: 3.072Epoch  14:  30% | abe: 3.146 | eve: 8.995 | bob: 3.071Epoch  14:  31% | abe: 3.146 | eve: 8.995 | bob: 3.072Epoch  14:  32% | abe: 3.147 | eve: 8.996 | bob: 3.072Epoch  14:  32% | abe: 3.147 | eve: 8.996 | bob: 3.072Epoch  14:  33% | abe: 3.147 | eve: 8.996 | bob: 3.072Epoch  14:  34% | abe: 3.147 | eve: 8.996 | bob: 3.072Epoch  14:  35% | abe: 3.146 | eve: 8.997 | bob: 3.071Epoch  14:  35% | abe: 3.146 | eve: 8.996 | bob: 3.071Epoch  14:  36% | abe: 3.146 | eve: 8.996 | bob: 3.072Epoch  14:  37% | abe: 3.146 | eve: 8.995 | bob: 3.072Epoch  14:  38% | abe: 3.145 | eve: 8.996 | bob: 3.071Epoch  14:  39% | abe: 3.145 | eve: 8.996 | bob: 3.070Epoch  14:  39% | abe: 3.144 | eve: 8.995 | bob: 3.070Epoch  14:  40% | abe: 3.143 | eve: 8.995 | bob: 3.069Epoch  14:  41% | abe: 3.143 | eve: 8.995 | bob: 3.069Epoch  14:  42% | abe: 3.143 | eve: 8.996 | bob: 3.069Epoch  14:  42% | abe: 3.143 | eve: 8.996 | bob: 3.069Epoch  14:  43% | abe: 3.144 | eve: 8.996 | bob: 3.070Epoch  14:  44% | abe: 3.144 | eve: 8.997 | bob: 3.069Epoch  14:  45% | abe: 3.144 | eve: 8.997 | bob: 3.070Epoch  14:  46% | abe: 3.144 | eve: 8.996 | bob: 3.070Epoch  14:  46% | abe: 3.144 | eve: 8.997 | bob: 3.070Epoch  14:  47% | abe: 3.144 | eve: 8.997 | bob: 3.070Epoch  14:  48% | abe: 3.143 | eve: 8.997 | bob: 3.070Epoch  14:  49% | abe: 3.144 | eve: 8.996 | bob: 3.070Epoch  14:  50% | abe: 3.143 | eve: 8.996 | bob: 3.069Epoch  14:  50% | abe: 3.143 | eve: 8.996 | bob: 3.069Epoch  14:  51% | abe: 3.143 | eve: 8.996 | bob: 3.069Epoch  14:  52% | abe: 3.143 | eve: 8.996 | bob: 3.068Epoch  14:  53% | abe: 3.143 | eve: 8.997 | bob: 3.069Epoch  14:  53% | abe: 3.143 | eve: 8.997 | bob: 3.069Epoch  14:  54% | abe: 3.143 | eve: 8.997 | bob: 3.069Epoch  14:  55% | abe: 3.143 | eve: 8.997 | bob: 3.069Epoch  14:  56% | abe: 3.143 | eve: 8.997 | bob: 3.069Epoch  14:  57% | abe: 3.142 | eve: 8.998 | bob: 3.069Epoch  14:  57% | abe: 3.142 | eve: 8.998 | bob: 3.068Epoch  14:  58% | abe: 3.142 | eve: 8.998 | bob: 3.068Epoch  14:  59% | abe: 3.142 | eve: 8.997 | bob: 3.068Epoch  14:  60% | abe: 3.142 | eve: 8.997 | bob: 3.068Epoch  14:  60% | abe: 3.142 | eve: 8.997 | bob: 3.068Epoch  14:  61% | abe: 3.142 | eve: 8.997 | bob: 3.068Epoch  14:  62% | abe: 3.142 | eve: 8.998 | bob: 3.068Epoch  14:  63% | abe: 3.142 | eve: 8.998 | bob: 3.068Epoch  14:  64% | abe: 3.141 | eve: 8.997 | bob: 3.068Epoch  14:  64% | abe: 3.141 | eve: 8.998 | bob: 3.068Epoch  14:  65% | abe: 3.142 | eve: 8.998 | bob: 3.068Epoch  14:  66% | abe: 3.141 | eve: 8.998 | bob: 3.067Epoch  14:  67% | abe: 3.141 | eve: 8.998 | bob: 3.067Epoch  14:  67% | abe: 3.141 | eve: 8.997 | bob: 3.067Epoch  14:  68% | abe: 3.141 | eve: 8.997 | bob: 3.067Epoch  14:  69% | abe: 3.141 | eve: 8.997 | bob: 3.067Epoch  14:  70% | abe: 3.141 | eve: 8.997 | bob: 3.067Epoch  14:  71% | abe: 3.141 | eve: 8.997 | bob: 3.067Epoch  14:  71% | abe: 3.140 | eve: 8.997 | bob: 3.066Epoch  14:  72% | abe: 3.140 | eve: 8.998 | bob: 3.066Epoch  14:  73% | abe: 3.140 | eve: 8.998 | bob: 3.066Epoch  14:  74% | abe: 3.140 | eve: 8.997 | bob: 3.066Epoch  14:  75% | abe: 3.139 | eve: 8.997 | bob: 3.066Epoch  14:  75% | abe: 3.140 | eve: 8.997 | bob: 3.066Epoch  14:  76% | abe: 3.140 | eve: 8.997 | bob: 3.066Epoch  14:  77% | abe: 3.140 | eve: 8.997 | bob: 3.066Epoch  14:  78% | abe: 3.140 | eve: 8.997 | bob: 3.065Epoch  14:  78% | abe: 3.140 | eve: 8.997 | bob: 3.065Epoch  14:  79% | abe: 3.139 | eve: 8.997 | bob: 3.065Epoch  14:  80% | abe: 3.139 | eve: 8.997 | bob: 3.065Epoch  14:  81% | abe: 3.139 | eve: 8.997 | bob: 3.065Epoch  14:  82% | abe: 3.139 | eve: 8.996 | bob: 3.065Epoch  14:  82% | abe: 3.139 | eve: 8.997 | bob: 3.065Epoch  14:  83% | abe: 3.139 | eve: 8.997 | bob: 3.064Epoch  14:  84% | abe: 3.139 | eve: 8.996 | bob: 3.064Epoch  14:  85% | abe: 3.138 | eve: 8.996 | bob: 3.064Epoch  14:  85% | abe: 3.138 | eve: 8.996 | bob: 3.064Epoch  14:  86% | abe: 3.138 | eve: 8.997 | bob: 3.064Epoch  14:  87% | abe: 3.138 | eve: 8.997 | bob: 3.064Epoch  14:  88% | abe: 3.139 | eve: 8.997 | bob: 3.064Epoch  14:  89% | abe: 3.139 | eve: 8.997 | bob: 3.064Epoch  14:  89% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  90% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  91% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  92% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  92% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  93% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  94% | abe: 3.138 | eve: 8.997 | bob: 3.063Epoch  14:  95% | abe: 3.137 | eve: 8.997 | bob: 3.062Epoch  14:  96% | abe: 3.137 | eve: 8.997 | bob: 3.062Epoch  14:  96% | abe: 3.137 | eve: 8.998 | bob: 3.062Epoch  14:  97% | abe: 3.137 | eve: 8.998 | bob: 3.062Epoch  14:  98% | abe: 3.137 | eve: 8.998 | bob: 3.062Epoch  14:  99% | abe: 3.137 | eve: 8.998 | bob: 3.062
New best Bob loss 3.062025829629988 at epoch 14
Epoch  15:   0% | abe: 3.102 | eve: 8.996 | bob: 3.033Epoch  15:   0% | abe: 3.112 | eve: 8.997 | bob: 3.042Epoch  15:   1% | abe: 3.127 | eve: 8.997 | bob: 3.057Epoch  15:   2% | abe: 3.114 | eve: 8.989 | bob: 3.044Epoch  15:   3% | abe: 3.118 | eve: 8.988 | bob: 3.048Epoch  15:   3% | abe: 3.115 | eve: 8.989 | bob: 3.045Epoch  15:   4% | abe: 3.115 | eve: 8.988 | bob: 3.046Epoch  15:   5% | abe: 3.119 | eve: 8.988 | bob: 3.048Epoch  15:   6% | abe: 3.117 | eve: 8.987 | bob: 3.046Epoch  15:   7% | abe: 3.116 | eve: 8.988 | bob: 3.045Epoch  15:   7% | abe: 3.115 | eve: 8.995 | bob: 3.043Epoch  15:   8% | abe: 3.116 | eve: 8.997 | bob: 3.045Epoch  15:   9% | abe: 3.118 | eve: 8.996 | bob: 3.047Epoch  15:  10% | abe: 3.119 | eve: 8.994 | bob: 3.048Epoch  15:  10% | abe: 3.117 | eve: 8.995 | bob: 3.045Epoch  15:  11% | abe: 3.118 | eve: 8.994 | bob: 3.046Epoch  15:  12% | abe: 3.118 | eve: 8.997 | bob: 3.046Epoch  15:  13% | abe: 3.116 | eve: 8.996 | bob: 3.043Epoch  15:  14% | abe: 3.117 | eve: 8.996 | bob: 3.043Epoch  15:  14% | abe: 3.117 | eve: 8.997 | bob: 3.044Epoch  15:  15% | abe: 3.118 | eve: 8.995 | bob: 3.045Epoch  15:  16% | abe: 3.118 | eve: 8.994 | bob: 3.046Epoch  15:  17% | abe: 3.118 | eve: 8.996 | bob: 3.046Epoch  15:  17% | abe: 3.118 | eve: 8.997 | bob: 3.045Epoch  15:  18% | abe: 3.119 | eve: 8.997 | bob: 3.046Epoch  15:  19% | abe: 3.121 | eve: 8.998 | bob: 3.048Epoch  15:  20% | abe: 3.120 | eve: 8.999 | bob: 3.047Epoch  15:  21% | abe: 3.120 | eve: 8.998 | bob: 3.046Epoch  15:  21% | abe: 3.120 | eve: 8.997 | bob: 3.046Epoch  15:  22% | abe: 3.119 | eve: 8.999 | bob: 3.046Epoch  15:  23% | abe: 3.120 | eve: 8.999 | bob: 3.046Epoch  15:  24% | abe: 3.120 | eve: 8.999 | bob: 3.046Epoch  15:  25% | abe: 3.120 | eve: 8.998 | bob: 3.046Epoch  15:  25% | abe: 3.121 | eve: 8.997 | bob: 3.047Epoch  15:  26% | abe: 3.120 | eve: 8.997 | bob: 3.047Epoch  15:  27% | abe: 3.119 | eve: 8.998 | bob: 3.047Epoch  15:  28% | abe: 3.119 | eve: 8.999 | bob: 3.047Epoch  15:  28% | abe: 3.119 | eve: 8.999 | bob: 3.047Epoch  15:  29% | abe: 3.120 | eve: 8.999 | bob: 3.047Epoch  15:  30% | abe: 3.120 | eve: 9.000 | bob: 3.047Epoch  15:  31% | abe: 3.120 | eve: 9.000 | bob: 3.046Epoch  15:  32% | abe: 3.119 | eve: 9.000 | bob: 3.045Epoch  15:  32% | abe: 3.119 | eve: 8.999 | bob: 3.045Epoch  15:  33% | abe: 3.119 | eve: 8.999 | bob: 3.045Epoch  15:  34% | abe: 3.120 | eve: 9.000 | bob: 3.046Epoch  15:  35% | abe: 3.119 | eve: 9.000 | bob: 3.045Epoch  15:  35% | abe: 3.119 | eve: 8.999 | bob: 3.045Epoch  15:  36% | abe: 3.119 | eve: 9.000 | bob: 3.045Epoch  15:  37% | abe: 3.120 | eve: 9.000 | bob: 3.046Epoch  15:  38% | abe: 3.120 | eve: 9.000 | bob: 3.046Epoch  15:  39% | abe: 3.120 | eve: 9.000 | bob: 3.046Epoch  15:  39% | abe: 3.121 | eve: 9.000 | bob: 3.046Epoch  15:  40% | abe: 3.121 | eve: 9.001 | bob: 3.046Epoch  15:  41% | abe: 3.121 | eve: 9.001 | bob: 3.046Epoch  15:  42% | abe: 3.121 | eve: 9.001 | bob: 3.046Epoch  15:  42% | abe: 3.120 | eve: 9.001 | bob: 3.045Epoch  15:  43% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  44% | abe: 3.121 | eve: 9.000 | bob: 3.046Epoch  15:  45% | abe: 3.121 | eve: 9.000 | bob: 3.046Epoch  15:  46% | abe: 3.121 | eve: 8.999 | bob: 3.046Epoch  15:  46% | abe: 3.121 | eve: 9.000 | bob: 3.046Epoch  15:  47% | abe: 3.121 | eve: 8.999 | bob: 3.046Epoch  15:  48% | abe: 3.120 | eve: 9.000 | bob: 3.045Epoch  15:  49% | abe: 3.120 | eve: 9.000 | bob: 3.045Epoch  15:  50% | abe: 3.120 | eve: 8.999 | bob: 3.045Epoch  15:  50% | abe: 3.120 | eve: 8.999 | bob: 3.045Epoch  15:  51% | abe: 3.120 | eve: 8.999 | bob: 3.044Epoch  15:  52% | abe: 3.120 | eve: 8.999 | bob: 3.044Epoch  15:  53% | abe: 3.120 | eve: 8.998 | bob: 3.044Epoch  15:  53% | abe: 3.120 | eve: 8.999 | bob: 3.045Epoch  15:  54% | abe: 3.120 | eve: 8.999 | bob: 3.044Epoch  15:  55% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  56% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  57% | abe: 3.121 | eve: 8.999 | bob: 3.045Epoch  15:  57% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  58% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  59% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  60% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  60% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  61% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  62% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  63% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  64% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  64% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  65% | abe: 3.122 | eve: 9.001 | bob: 3.045Epoch  15:  66% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  67% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  67% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  68% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  69% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  70% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  71% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  71% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  72% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  73% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  74% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  75% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  75% | abe: 3.121 | eve: 9.002 | bob: 3.045Epoch  15:  76% | abe: 3.120 | eve: 9.001 | bob: 3.045Epoch  15:  77% | abe: 3.120 | eve: 9.001 | bob: 3.045Epoch  15:  78% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  78% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  79% | abe: 3.120 | eve: 9.001 | bob: 3.045Epoch  15:  80% | abe: 3.121 | eve: 9.000 | bob: 3.045Epoch  15:  81% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  82% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  82% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  83% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  84% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  85% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  85% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  86% | abe: 3.120 | eve: 9.000 | bob: 3.044Epoch  15:  87% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  88% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  89% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  89% | abe: 3.121 | eve: 9.001 | bob: 3.045Epoch  15:  90% | abe: 3.121 | eve: 9.001 | bob: 3.044Epoch  15:  91% | abe: 3.121 | eve: 9.002 | bob: 3.044Epoch  15:  92% | abe: 3.120 | eve: 9.002 | bob: 3.044Epoch  15:  92% | abe: 3.120 | eve: 9.002 | bob: 3.044Epoch  15:  93% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  94% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  95% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  96% | abe: 3.120 | eve: 9.002 | bob: 3.044Epoch  15:  96% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  97% | abe: 3.120 | eve: 9.002 | bob: 3.044Epoch  15:  98% | abe: 3.120 | eve: 9.001 | bob: 3.044Epoch  15:  99% | abe: 3.120 | eve: 9.001 | bob: 3.044
New best Bob loss 3.043775841064871 at epoch 15
Epoch  16:   0% | abe: 3.104 | eve: 9.022 | bob: 3.029Epoch  16:   0% | abe: 3.108 | eve: 9.024 | bob: 3.030Epoch  16:   1% | abe: 3.114 | eve: 9.019 | bob: 3.038Epoch  16:   2% | abe: 3.114 | eve: 9.010 | bob: 3.035Epoch  16:   3% | abe: 3.116 | eve: 9.005 | bob: 3.039Epoch  16:   3% | abe: 3.116 | eve: 9.005 | bob: 3.037Epoch  16:   4% | abe: 3.117 | eve: 9.002 | bob: 3.038Epoch  16:   5% | abe: 3.118 | eve: 9.007 | bob: 3.038Epoch  16:   6% | abe: 3.115 | eve: 9.005 | bob: 3.036Epoch  16:   7% | abe: 3.113 | eve: 9.005 | bob: 3.035Epoch  16:   7% | abe: 3.114 | eve: 9.005 | bob: 3.036Epoch  16:   8% | abe: 3.114 | eve: 9.006 | bob: 3.036Epoch  16:   9% | abe: 3.114 | eve: 9.005 | bob: 3.037Epoch  16:  10% | abe: 3.115 | eve: 9.003 | bob: 3.038Epoch  16:  10% | abe: 3.115 | eve: 9.002 | bob: 3.038Epoch  16:  11% | abe: 3.113 | eve: 8.999 | bob: 3.036Epoch  16:  12% | abe: 3.114 | eve: 8.999 | bob: 3.037Epoch  16:  13% | abe: 3.115 | eve: 8.999 | bob: 3.038Epoch  16:  14% | abe: 3.116 | eve: 8.998 | bob: 3.039Epoch  16:  14% | abe: 3.116 | eve: 9.000 | bob: 3.039Epoch  16:  15% | abe: 3.116 | eve: 9.000 | bob: 3.038Epoch  16:  16% | abe: 3.115 | eve: 9.000 | bob: 3.037Epoch  16:  17% | abe: 3.116 | eve: 9.000 | bob: 3.039Epoch  16:  17% | abe: 3.116 | eve: 9.000 | bob: 3.038Epoch  16:  18% | abe: 3.116 | eve: 8.999 | bob: 3.039Epoch  16:  19% | abe: 3.116 | eve: 8.999 | bob: 3.039Epoch  16:  20% | abe: 3.116 | eve: 8.999 | bob: 3.040Epoch  16:  21% | abe: 3.116 | eve: 9.000 | bob: 3.040Epoch  16:  21% | abe: 3.117 | eve: 8.999 | bob: 3.040Epoch  16:  22% | abe: 3.117 | eve: 8.998 | bob: 3.040Epoch  16:  23% | abe: 3.117 | eve: 8.998 | bob: 3.039Epoch  16:  24% | abe: 3.118 | eve: 8.998 | bob: 3.039Epoch  16:  25% | abe: 3.117 | eve: 8.998 | bob: 3.039Epoch  16:  25% | abe: 3.118 | eve: 8.998 | bob: 3.040Epoch  16:  26% | abe: 3.118 | eve: 8.998 | bob: 3.040Epoch  16:  27% | abe: 3.119 | eve: 8.999 | bob: 3.041Epoch  16:  28% | abe: 3.118 | eve: 8.999 | bob: 3.040Epoch  16:  28% | abe: 3.118 | eve: 8.999 | bob: 3.041Epoch  16:  29% | abe: 3.119 | eve: 9.001 | bob: 3.041Epoch  16:  30% | abe: 3.120 | eve: 9.001 | bob: 3.042Epoch  16:  31% | abe: 3.120 | eve: 9.001 | bob: 3.042Epoch  16:  32% | abe: 3.119 | eve: 9.001 | bob: 3.042Epoch  16:  32% | abe: 3.119 | eve: 9.001 | bob: 3.042Epoch  16:  33% | abe: 3.119 | eve: 9.002 | bob: 3.041Epoch  16:  34% | abe: 3.119 | eve: 9.002 | bob: 3.041Epoch  16:  35% | abe: 3.119 | eve: 9.002 | bob: 3.041Epoch  16:  35% | abe: 3.120 | eve: 9.002 | bob: 3.042Epoch  16:  36% | abe: 3.119 | eve: 9.002 | bob: 3.041Epoch  16:  37% | abe: 3.119 | eve: 9.003 | bob: 3.042Epoch  16:  38% | abe: 3.118 | eve: 9.003 | bob: 3.042Epoch  16:  39% | abe: 3.119 | eve: 9.003 | bob: 3.042Epoch  16:  39% | abe: 3.119 | eve: 9.002 | bob: 3.043Epoch  16:  40% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  41% | abe: 3.119 | eve: 9.002 | bob: 3.043Epoch  16:  42% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  42% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  43% | abe: 3.118 | eve: 9.003 | bob: 3.042Epoch  16:  44% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  45% | abe: 3.118 | eve: 9.002 | bob: 3.041Epoch  16:  46% | abe: 3.117 | eve: 9.002 | bob: 3.041Epoch  16:  46% | abe: 3.118 | eve: 9.002 | bob: 3.041Epoch  16:  47% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  48% | abe: 3.119 | eve: 9.002 | bob: 3.042Epoch  16:  49% | abe: 3.119 | eve: 9.002 | bob: 3.042Epoch  16:  50% | abe: 3.118 | eve: 9.003 | bob: 3.042Epoch  16:  50% | abe: 3.118 | eve: 9.002 | bob: 3.042Epoch  16:  51% | abe: 3.117 | eve: 9.002 | bob: 3.041Epoch  16:  52% | abe: 3.117 | eve: 9.002 | bob: 3.041Epoch  16:  53% | abe: 3.117 | eve: 9.003 | bob: 3.041Epoch  16:  53% | abe: 3.117 | eve: 9.003 | bob: 3.041Epoch  16:  54% | abe: 3.117 | eve: 9.003 | bob: 3.041Epoch  16:  55% | abe: 3.117 | eve: 9.003 | bob: 3.041Epoch  16:  56% | abe: 3.116 | eve: 9.003 | bob: 3.041Epoch  16:  57% | abe: 3.116 | eve: 9.003 | bob: 3.041Epoch  16:  57% | abe: 3.117 | eve: 9.002 | bob: 3.041Epoch  16:  58% | abe: 3.117 | eve: 9.002 | bob: 3.041Epoch  16:  59% | abe: 3.116 | eve: 9.003 | bob: 3.040Epoch  16:  60% | abe: 3.116 | eve: 9.002 | bob: 3.040Epoch  16:  60% | abe: 3.116 | eve: 9.002 | bob: 3.040Epoch  16:  61% | abe: 3.116 | eve: 9.002 | bob: 3.040Epoch  16:  62% | abe: 3.116 | eve: 9.002 | bob: 3.040Epoch  16:  63% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  64% | abe: 3.116 | eve: 9.002 | bob: 3.040Epoch  16:  64% | abe: 3.115 | eve: 9.002 | bob: 3.040Epoch  16:  65% | abe: 3.115 | eve: 9.002 | bob: 3.040Epoch  16:  66% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  67% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  67% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  68% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  69% | abe: 3.114 | eve: 9.002 | bob: 3.039Epoch  16:  70% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  71% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  71% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  72% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  73% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  74% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  75% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  75% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  76% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  77% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  78% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  78% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  79% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  80% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  81% | abe: 3.115 | eve: 9.002 | bob: 3.039Epoch  16:  82% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  82% | abe: 3.114 | eve: 9.001 | bob: 3.038Epoch  16:  83% | abe: 3.115 | eve: 9.001 | bob: 3.039Epoch  16:  84% | abe: 3.115 | eve: 9.002 | bob: 3.038Epoch  16:  85% | abe: 3.115 | eve: 9.002 | bob: 3.038Epoch  16:  85% | abe: 3.115 | eve: 9.002 | bob: 3.038Epoch  16:  86% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  87% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  88% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  89% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  89% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  90% | abe: 3.114 | eve: 9.002 | bob: 3.038Epoch  16:  91% | abe: 3.114 | eve: 9.001 | bob: 3.038Epoch  16:  92% | abe: 3.114 | eve: 9.001 | bob: 3.038Epoch  16:  92% | abe: 3.114 | eve: 9.001 | bob: 3.038Epoch  16:  93% | abe: 3.114 | eve: 9.001 | bob: 3.038Epoch  16:  94% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  95% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  96% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  96% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  97% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  98% | abe: 3.113 | eve: 9.001 | bob: 3.037Epoch  16:  99% | abe: 3.113 | eve: 9.001 | bob: 3.037
New best Bob loss 3.037100318805301 at epoch 16
Epoch  17:   0% | abe: 3.086 | eve: 9.004 | bob: 3.005Epoch  17:   0% | abe: 3.110 | eve: 8.993 | bob: 3.030Epoch  17:   1% | abe: 3.119 | eve: 9.009 | bob: 3.040Epoch  17:   2% | abe: 3.122 | eve: 9.004 | bob: 3.046Epoch  17:   3% | abe: 3.119 | eve: 8.998 | bob: 3.043Epoch  17:   3% | abe: 3.120 | eve: 8.997 | bob: 3.045Epoch  17:   4% | abe: 3.115 | eve: 8.999 | bob: 3.040Epoch  17:   5% | abe: 3.115 | eve: 8.997 | bob: 3.039Epoch  17:   6% | abe: 3.112 | eve: 8.997 | bob: 3.037Epoch  17:   7% | abe: 3.111 | eve: 8.999 | bob: 3.035Epoch  17:   7% | abe: 3.110 | eve: 8.998 | bob: 3.034Epoch  17:   8% | abe: 3.109 | eve: 8.999 | bob: 3.033Epoch  17:   9% | abe: 3.111 | eve: 8.997 | bob: 3.035Epoch  17:  10% | abe: 3.110 | eve: 8.998 | bob: 3.034Epoch  17:  10% | abe: 3.106 | eve: 8.998 | bob: 3.031Epoch  17:  11% | abe: 3.107 | eve: 8.998 | bob: 3.031Epoch  17:  12% | abe: 3.109 | eve: 8.997 | bob: 3.033Epoch  17:  13% | abe: 3.109 | eve: 8.998 | bob: 3.033Epoch  17:  14% | abe: 3.109 | eve: 8.997 | bob: 3.033Epoch  17:  14% | abe: 3.109 | eve: 8.999 | bob: 3.032Epoch  17:  15% | abe: 3.108 | eve: 9.000 | bob: 3.031Epoch  17:  16% | abe: 3.108 | eve: 8.999 | bob: 3.031Epoch  17:  17% | abe: 3.107 | eve: 8.999 | bob: 3.030Epoch  17:  17% | abe: 3.106 | eve: 8.999 | bob: 3.029Epoch  17:  18% | abe: 3.105 | eve: 9.000 | bob: 3.028Epoch  17:  19% | abe: 3.104 | eve: 9.000 | bob: 3.027Epoch  17:  20% | abe: 3.104 | eve: 9.000 | bob: 3.028Epoch  17:  21% | abe: 3.104 | eve: 9.001 | bob: 3.028Epoch  17:  21% | abe: 3.104 | eve: 9.001 | bob: 3.028Epoch  17:  22% | abe: 3.103 | eve: 9.001 | bob: 3.028Epoch  17:  23% | abe: 3.103 | eve: 9.000 | bob: 3.028Epoch  17:  24% | abe: 3.104 | eve: 9.000 | bob: 3.028Epoch  17:  25% | abe: 3.104 | eve: 8.999 | bob: 3.028Epoch  17:  25% | abe: 3.104 | eve: 8.999 | bob: 3.029Epoch  17:  26% | abe: 3.105 | eve: 8.998 | bob: 3.029Epoch  17:  27% | abe: 3.105 | eve: 8.998 | bob: 3.028Epoch  17:  28% | abe: 3.105 | eve: 8.998 | bob: 3.029Epoch  17:  28% | abe: 3.105 | eve: 8.999 | bob: 3.029Epoch  17:  29% | abe: 3.106 | eve: 9.000 | bob: 3.029Epoch  17:  30% | abe: 3.106 | eve: 9.000 | bob: 3.029Epoch  17:  31% | abe: 3.106 | eve: 9.001 | bob: 3.029Epoch  17:  32% | abe: 3.105 | eve: 9.001 | bob: 3.028Epoch  17:  32% | abe: 3.105 | eve: 9.001 | bob: 3.028Epoch  17:  33% | abe: 3.104 | eve: 9.002 | bob: 3.028Epoch  17:  34% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  17:  35% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  17:  35% | abe: 3.103 | eve: 9.000 | bob: 3.027Epoch  17:  36% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  37% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  38% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  39% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  39% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  40% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  17:  41% | abe: 3.103 | eve: 8.999 | bob: 3.026Epoch  17:  42% | abe: 3.104 | eve: 8.999 | bob: 3.026Epoch  17:  42% | abe: 3.104 | eve: 8.999 | bob: 3.027Epoch  17:  43% | abe: 3.104 | eve: 8.998 | bob: 3.027Epoch  17:  44% | abe: 3.104 | eve: 8.998 | bob: 3.027Epoch  17:  45% | abe: 3.104 | eve: 8.999 | bob: 3.027Epoch  17:  46% | abe: 3.104 | eve: 8.998 | bob: 3.026Epoch  17:  46% | abe: 3.104 | eve: 8.998 | bob: 3.026Epoch  17:  47% | abe: 3.104 | eve: 8.999 | bob: 3.027Epoch  17:  48% | abe: 3.104 | eve: 8.998 | bob: 3.027Epoch  17:  49% | abe: 3.104 | eve: 8.998 | bob: 3.027Epoch  17:  50% | abe: 3.104 | eve: 8.998 | bob: 3.027Epoch  17:  50% | abe: 3.104 | eve: 8.999 | bob: 3.028Epoch  17:  51% | abe: 3.105 | eve: 8.999 | bob: 3.028Epoch  17:  52% | abe: 3.105 | eve: 8.999 | bob: 3.028Epoch  17:  53% | abe: 3.104 | eve: 8.999 | bob: 3.027Epoch  17:  53% | abe: 3.104 | eve: 9.000 | bob: 3.027Epoch  17:  54% | abe: 3.104 | eve: 9.000 | bob: 3.027Epoch  17:  55% | abe: 3.104 | eve: 9.000 | bob: 3.027Epoch  17:  56% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  17:  57% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  57% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  58% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  59% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  60% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  60% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  61% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  62% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  63% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  64% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  64% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  65% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  66% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  67% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  67% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  68% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  69% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  70% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  71% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  71% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  17:  72% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  73% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  74% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  75% | abe: 3.102 | eve: 9.001 | bob: 3.025Epoch  17:  75% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  76% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  77% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  78% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  78% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  79% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  80% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  81% | abe: 3.102 | eve: 9.000 | bob: 3.025Epoch  17:  82% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  82% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  83% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  84% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  85% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  85% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  86% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  87% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  88% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  89% | abe: 3.102 | eve: 9.001 | bob: 3.026Epoch  17:  89% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  90% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  91% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  92% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  92% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  93% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  94% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  95% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  17:  96% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  96% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  97% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  98% | abe: 3.102 | eve: 9.000 | bob: 3.026Epoch  17:  99% | abe: 3.102 | eve: 9.000 | bob: 3.026
New best Bob loss 3.0257929637265306 at epoch 17
Epoch  18:   0% | abe: 3.094 | eve: 8.981 | bob: 3.032Epoch  18:   0% | abe: 3.081 | eve: 8.992 | bob: 3.015Epoch  18:   1% | abe: 3.082 | eve: 8.990 | bob: 3.015Epoch  18:   2% | abe: 3.087 | eve: 8.992 | bob: 3.017Epoch  18:   3% | abe: 3.087 | eve: 8.994 | bob: 3.016Epoch  18:   3% | abe: 3.088 | eve: 8.996 | bob: 3.016Epoch  18:   4% | abe: 3.089 | eve: 8.997 | bob: 3.016Epoch  18:   5% | abe: 3.093 | eve: 8.993 | bob: 3.019Epoch  18:   6% | abe: 3.090 | eve: 8.995 | bob: 3.014Epoch  18:   7% | abe: 3.092 | eve: 8.998 | bob: 3.016Epoch  18:   7% | abe: 3.094 | eve: 8.999 | bob: 3.018Epoch  18:   8% | abe: 3.095 | eve: 8.996 | bob: 3.019Epoch  18:   9% | abe: 3.097 | eve: 8.996 | bob: 3.021Epoch  18:  10% | abe: 3.098 | eve: 8.996 | bob: 3.021Epoch  18:  10% | abe: 3.097 | eve: 8.993 | bob: 3.020Epoch  18:  11% | abe: 3.096 | eve: 8.993 | bob: 3.019Epoch  18:  12% | abe: 3.098 | eve: 8.994 | bob: 3.021Epoch  18:  13% | abe: 3.097 | eve: 8.994 | bob: 3.020Epoch  18:  14% | abe: 3.097 | eve: 8.995 | bob: 3.020Epoch  18:  14% | abe: 3.098 | eve: 8.995 | bob: 3.021Epoch  18:  15% | abe: 3.099 | eve: 8.995 | bob: 3.021Epoch  18:  16% | abe: 3.100 | eve: 8.995 | bob: 3.021Epoch  18:  17% | abe: 3.101 | eve: 8.996 | bob: 3.023Epoch  18:  17% | abe: 3.101 | eve: 8.998 | bob: 3.023Epoch  18:  18% | abe: 3.102 | eve: 8.997 | bob: 3.024Epoch  18:  19% | abe: 3.101 | eve: 8.997 | bob: 3.023Epoch  18:  20% | abe: 3.102 | eve: 8.998 | bob: 3.024Epoch  18:  21% | abe: 3.103 | eve: 8.997 | bob: 3.025Epoch  18:  21% | abe: 3.103 | eve: 8.999 | bob: 3.025Epoch  18:  22% | abe: 3.103 | eve: 9.000 | bob: 3.025Epoch  18:  23% | abe: 3.103 | eve: 9.002 | bob: 3.024Epoch  18:  24% | abe: 3.102 | eve: 9.001 | bob: 3.024Epoch  18:  25% | abe: 3.103 | eve: 9.002 | bob: 3.025Epoch  18:  25% | abe: 3.103 | eve: 9.003 | bob: 3.024Epoch  18:  26% | abe: 3.103 | eve: 9.002 | bob: 3.024Epoch  18:  27% | abe: 3.103 | eve: 9.001 | bob: 3.024Epoch  18:  28% | abe: 3.104 | eve: 9.001 | bob: 3.025Epoch  18:  28% | abe: 3.104 | eve: 9.001 | bob: 3.025Epoch  18:  29% | abe: 3.104 | eve: 9.001 | bob: 3.025Epoch  18:  30% | abe: 3.104 | eve: 9.002 | bob: 3.026Epoch  18:  31% | abe: 3.104 | eve: 9.003 | bob: 3.026Epoch  18:  32% | abe: 3.104 | eve: 9.002 | bob: 3.026Epoch  18:  32% | abe: 3.103 | eve: 9.002 | bob: 3.026Epoch  18:  33% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  34% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  35% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  35% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  36% | abe: 3.104 | eve: 9.002 | bob: 3.027Epoch  18:  37% | abe: 3.104 | eve: 9.002 | bob: 3.027Epoch  18:  38% | abe: 3.105 | eve: 9.001 | bob: 3.028Epoch  18:  39% | abe: 3.105 | eve: 9.000 | bob: 3.028Epoch  18:  39% | abe: 3.105 | eve: 9.000 | bob: 3.028Epoch  18:  40% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  18:  41% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  18:  42% | abe: 3.105 | eve: 9.001 | bob: 3.028Epoch  18:  42% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  18:  43% | abe: 3.104 | eve: 9.001 | bob: 3.027Epoch  18:  44% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  45% | abe: 3.103 | eve: 9.001 | bob: 3.026Epoch  18:  46% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  46% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  47% | abe: 3.104 | eve: 8.999 | bob: 3.027Epoch  18:  48% | abe: 3.104 | eve: 9.000 | bob: 3.026Epoch  18:  49% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  50% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  50% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  51% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  52% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  53% | abe: 3.103 | eve: 9.000 | bob: 3.026Epoch  18:  53% | abe: 3.103 | eve: 9.000 | bob: 3.025Epoch  18:  54% | abe: 3.102 | eve: 8.999 | bob: 3.025Epoch  18:  55% | abe: 3.102 | eve: 8.999 | bob: 3.024Epoch  18:  56% | abe: 3.101 | eve: 8.999 | bob: 3.024Epoch  18:  57% | abe: 3.101 | eve: 8.999 | bob: 3.024Epoch  18:  57% | abe: 3.101 | eve: 9.000 | bob: 3.023Epoch  18:  58% | abe: 3.101 | eve: 9.000 | bob: 3.023Epoch  18:  59% | abe: 3.100 | eve: 9.000 | bob: 3.023Epoch  18:  60% | abe: 3.100 | eve: 9.001 | bob: 3.023Epoch  18:  60% | abe: 3.100 | eve: 9.001 | bob: 3.022Epoch  18:  61% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  62% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  63% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  64% | abe: 3.101 | eve: 9.000 | bob: 3.023Epoch  18:  64% | abe: 3.100 | eve: 9.000 | bob: 3.023Epoch  18:  65% | abe: 3.100 | eve: 9.000 | bob: 3.023Epoch  18:  66% | abe: 3.100 | eve: 9.000 | bob: 3.023Epoch  18:  67% | abe: 3.100 | eve: 9.001 | bob: 3.023Epoch  18:  67% | abe: 3.100 | eve: 9.001 | bob: 3.023Epoch  18:  68% | abe: 3.100 | eve: 9.001 | bob: 3.022Epoch  18:  69% | abe: 3.100 | eve: 9.000 | bob: 3.023Epoch  18:  70% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  71% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  71% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  72% | abe: 3.100 | eve: 9.000 | bob: 3.022Epoch  18:  73% | abe: 3.099 | eve: 9.000 | bob: 3.022Epoch  18:  74% | abe: 3.099 | eve: 9.000 | bob: 3.022Epoch  18:  75% | abe: 3.099 | eve: 9.000 | bob: 3.021Epoch  18:  75% | abe: 3.099 | eve: 9.000 | bob: 3.021Epoch  18:  76% | abe: 3.099 | eve: 9.000 | bob: 3.021Epoch  18:  77% | abe: 3.099 | eve: 9.000 | bob: 3.021Epoch  18:  78% | abe: 3.099 | eve: 9.001 | bob: 3.021Epoch  18:  78% | abe: 3.099 | eve: 9.001 | bob: 3.021Epoch  18:  79% | abe: 3.099 | eve: 9.001 | bob: 3.021Epoch  18:  80% | abe: 3.099 | eve: 9.001 | bob: 3.021Epoch  18:  81% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  82% | abe: 3.099 | eve: 9.001 | bob: 3.021Epoch  18:  82% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  83% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  84% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  85% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  85% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  86% | abe: 3.099 | eve: 9.000 | bob: 3.020Epoch  18:  87% | abe: 3.099 | eve: 9.000 | bob: 3.020Epoch  18:  88% | abe: 3.099 | eve: 9.000 | bob: 3.020Epoch  18:  89% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  89% | abe: 3.099 | eve: 9.000 | bob: 3.020Epoch  18:  90% | abe: 3.099 | eve: 9.001 | bob: 3.020Epoch  18:  91% | abe: 3.098 | eve: 9.000 | bob: 3.020Epoch  18:  92% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  92% | abe: 3.098 | eve: 9.000 | bob: 3.020Epoch  18:  93% | abe: 3.098 | eve: 9.000 | bob: 3.020Epoch  18:  94% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  95% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  96% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  96% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  97% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  98% | abe: 3.098 | eve: 9.000 | bob: 3.019Epoch  18:  99% | abe: 3.098 | eve: 9.000 | bob: 3.019
New best Bob loss 3.0191877027061764 at epoch 18
Epoch  19:   0% | abe: 3.117 | eve: 9.019 | bob: 3.028Epoch  19:   0% | abe: 3.102 | eve: 9.007 | bob: 3.015Epoch  19:   1% | abe: 3.095 | eve: 9.002 | bob: 3.010Epoch  19:   2% | abe: 3.084 | eve: 8.998 | bob: 2.998Epoch  19:   3% | abe: 3.086 | eve: 8.999 | bob: 3.000Epoch  19:   3% | abe: 3.087 | eve: 8.995 | bob: 3.000Epoch  19:   4% | abe: 3.090 | eve: 8.998 | bob: 3.004Epoch  19:   5% | abe: 3.094 | eve: 8.999 | bob: 3.008Epoch  19:   6% | abe: 3.093 | eve: 8.999 | bob: 3.007Epoch  19:   7% | abe: 3.094 | eve: 9.001 | bob: 3.007Epoch  19:   7% | abe: 3.098 | eve: 9.002 | bob: 3.010Epoch  19:   8% | abe: 3.097 | eve: 9.002 | bob: 3.009Epoch  19:   9% | abe: 3.096 | eve: 9.001 | bob: 3.008Epoch  19:  10% | abe: 3.097 | eve: 9.003 | bob: 3.010Epoch  19:  10% | abe: 3.096 | eve: 9.004 | bob: 3.011Epoch  19:  11% | abe: 3.095 | eve: 9.002 | bob: 3.010Epoch  19:  12% | abe: 3.095 | eve: 9.002 | bob: 3.011Epoch  19:  13% | abe: 3.092 | eve: 9.000 | bob: 3.009Epoch  19:  14% | abe: 3.093 | eve: 9.001 | bob: 3.010Epoch  19:  14% | abe: 3.094 | eve: 9.003 | bob: 3.012Epoch  19:  15% | abe: 3.094 | eve: 9.003 | bob: 3.012Epoch  19:  16% | abe: 3.094 | eve: 9.001 | bob: 3.013Epoch  19:  17% | abe: 3.094 | eve: 9.000 | bob: 3.014Epoch  19:  17% | abe: 3.095 | eve: 9.000 | bob: 3.015Epoch  19:  18% | abe: 3.095 | eve: 8.998 | bob: 3.015Epoch  19:  19% | abe: 3.095 | eve: 8.998 | bob: 3.015Epoch  19:  20% | abe: 3.094 | eve: 8.998 | bob: 3.014Epoch  19:  21% | abe: 3.095 | eve: 8.999 | bob: 3.015Epoch  19:  21% | abe: 3.096 | eve: 8.998 | bob: 3.016Epoch  19:  22% | abe: 3.096 | eve: 8.997 | bob: 3.016Epoch  19:  23% | abe: 3.096 | eve: 8.997 | bob: 3.016Epoch  19:  24% | abe: 3.095 | eve: 8.997 | bob: 3.016Epoch  19:  25% | abe: 3.095 | eve: 8.997 | bob: 3.016Epoch  19:  25% | abe: 3.095 | eve: 8.997 | bob: 3.016Epoch  19:  26% | abe: 3.096 | eve: 8.997 | bob: 3.017Epoch  19:  27% | abe: 3.095 | eve: 8.997 | bob: 3.016Epoch  19:  28% | abe: 3.095 | eve: 8.997 | bob: 3.016Epoch  19:  28% | abe: 3.095 | eve: 8.996 | bob: 3.016Epoch  19:  29% | abe: 3.095 | eve: 8.996 | bob: 3.016Epoch  19:  30% | abe: 3.095 | eve: 8.996 | bob: 3.016Epoch  19:  31% | abe: 3.094 | eve: 8.996 | bob: 3.015Epoch  19:  32% | abe: 3.094 | eve: 8.996 | bob: 3.015Epoch  19:  32% | abe: 3.094 | eve: 8.996 | bob: 3.016Epoch  19:  33% | abe: 3.094 | eve: 8.996 | bob: 3.016Epoch  19:  34% | abe: 3.094 | eve: 8.995 | bob: 3.016Epoch  19:  35% | abe: 3.095 | eve: 8.996 | bob: 3.017Epoch  19:  35% | abe: 3.095 | eve: 8.996 | bob: 3.017Epoch  19:  36% | abe: 3.095 | eve: 8.996 | bob: 3.017Epoch  19:  37% | abe: 3.095 | eve: 8.997 | bob: 3.017Epoch  19:  38% | abe: 3.096 | eve: 8.997 | bob: 3.018Epoch  19:  39% | abe: 3.096 | eve: 8.997 | bob: 3.018Epoch  19:  39% | abe: 3.095 | eve: 8.997 | bob: 3.018Epoch  19:  40% | abe: 3.096 | eve: 8.997 | bob: 3.018Epoch  19:  41% | abe: 3.096 | eve: 8.997 | bob: 3.018Epoch  19:  42% | abe: 3.096 | eve: 8.997 | bob: 3.018Epoch  19:  42% | abe: 3.097 | eve: 8.997 | bob: 3.019Epoch  19:  43% | abe: 3.097 | eve: 8.997 | bob: 3.019Epoch  19:  44% | abe: 3.097 | eve: 8.997 | bob: 3.018Epoch  19:  45% | abe: 3.097 | eve: 8.997 | bob: 3.018Epoch  19:  46% | abe: 3.097 | eve: 8.998 | bob: 3.018Epoch  19:  46% | abe: 3.096 | eve: 8.999 | bob: 3.018Epoch  19:  47% | abe: 3.096 | eve: 8.999 | bob: 3.018Epoch  19:  48% | abe: 3.096 | eve: 8.999 | bob: 3.018Epoch  19:  49% | abe: 3.096 | eve: 8.999 | bob: 3.018Epoch  19:  50% | abe: 3.096 | eve: 9.000 | bob: 3.018Epoch  19:  50% | abe: 3.097 | eve: 9.000 | bob: 3.018Epoch  19:  51% | abe: 3.096 | eve: 9.001 | bob: 3.018Epoch  19:  52% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  53% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  53% | abe: 3.096 | eve: 9.000 | bob: 3.018Epoch  19:  54% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  55% | abe: 3.097 | eve: 9.000 | bob: 3.018Epoch  19:  56% | abe: 3.097 | eve: 9.001 | bob: 3.018Epoch  19:  57% | abe: 3.097 | eve: 9.001 | bob: 3.018Epoch  19:  57% | abe: 3.097 | eve: 9.000 | bob: 3.018Epoch  19:  58% | abe: 3.097 | eve: 9.000 | bob: 3.018Epoch  19:  59% | abe: 3.097 | eve: 9.000 | bob: 3.018Epoch  19:  60% | abe: 3.096 | eve: 9.001 | bob: 3.018Epoch  19:  60% | abe: 3.096 | eve: 9.001 | bob: 3.018Epoch  19:  61% | abe: 3.096 | eve: 9.001 | bob: 3.018Epoch  19:  62% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  63% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  64% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  64% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  65% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  66% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  67% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  67% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  68% | abe: 3.096 | eve: 9.000 | bob: 3.016Epoch  19:  69% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  70% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  71% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  71% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  72% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  73% | abe: 3.096 | eve: 9.001 | bob: 3.017Epoch  19:  74% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  75% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  75% | abe: 3.096 | eve: 9.000 | bob: 3.017Epoch  19:  76% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  77% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  78% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  78% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  79% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  80% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  81% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  82% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  82% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  83% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  84% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  85% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  85% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  86% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  87% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  88% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  89% | abe: 3.095 | eve: 8.999 | bob: 3.017Epoch  19:  89% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  90% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  91% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  92% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  92% | abe: 3.095 | eve: 9.000 | bob: 3.017Epoch  19:  93% | abe: 3.094 | eve: 8.999 | bob: 3.017Epoch  19:  94% | abe: 3.095 | eve: 9.000 | bob: 3.018Epoch  19:  95% | abe: 3.095 | eve: 9.000 | bob: 3.018Epoch  19:  96% | abe: 3.094 | eve: 9.000 | bob: 3.017Epoch  19:  96% | abe: 3.094 | eve: 9.000 | bob: 3.017Epoch  19:  97% | abe: 3.094 | eve: 9.000 | bob: 3.017Epoch  19:  98% | abe: 3.094 | eve: 9.000 | bob: 3.017Epoch  19:  99% | abe: 3.094 | eve: 9.000 | bob: 3.017
New best Bob loss 3.0170794154753366 at epoch 19
Epoch  20:   0% | abe: 3.089 | eve: 8.999 | bob: 3.020Epoch  20:   0% | abe: 3.097 | eve: 8.989 | bob: 3.029Epoch  20:   1% | abe: 3.096 | eve: 8.984 | bob: 3.020Epoch  20:   2% | abe: 3.085 | eve: 8.987 | bob: 3.007Epoch  20:   3% | abe: 3.089 | eve: 8.995 | bob: 3.011Epoch  20:   3% | abe: 3.089 | eve: 8.996 | bob: 3.011Epoch  20:   4% | abe: 3.086 | eve: 8.997 | bob: 3.008Epoch  20:   5% | abe: 3.087 | eve: 9.000 | bob: 3.008Epoch  20:   6% | abe: 3.090 | eve: 8.998 | bob: 3.009Epoch  20:   7% | abe: 3.086 | eve: 8.999 | bob: 3.005Epoch  20:   7% | abe: 3.088 | eve: 8.997 | bob: 3.006Epoch  20:   8% | abe: 3.086 | eve: 8.999 | bob: 3.004Epoch  20:   9% | abe: 3.085 | eve: 8.999 | bob: 3.003Epoch  20:  10% | abe: 3.086 | eve: 9.000 | bob: 3.004Epoch  20:  10% | abe: 3.086 | eve: 9.001 | bob: 3.005Epoch  20:  11% | abe: 3.086 | eve: 9.001 | bob: 3.005Epoch  20:  12% | abe: 3.087 | eve: 9.001 | bob: 3.007Epoch  20:  13% | abe: 3.088 | eve: 9.001 | bob: 3.007Epoch  20:  14% | abe: 3.088 | eve: 8.999 | bob: 3.008Epoch  20:  14% | abe: 3.087 | eve: 8.999 | bob: 3.007Epoch  20:  15% | abe: 3.088 | eve: 8.999 | bob: 3.008Epoch  20:  16% | abe: 3.089 | eve: 8.999 | bob: 3.009Epoch  20:  17% | abe: 3.088 | eve: 8.999 | bob: 3.009Epoch  20:  17% | abe: 3.088 | eve: 9.000 | bob: 3.009Epoch  20:  18% | abe: 3.089 | eve: 8.999 | bob: 3.009Epoch  20:  19% | abe: 3.090 | eve: 8.998 | bob: 3.009Epoch  20:  20% | abe: 3.090 | eve: 8.999 | bob: 3.009Epoch  20:  21% | abe: 3.089 | eve: 8.999 | bob: 3.009Epoch  20:  21% | abe: 3.091 | eve: 8.998 | bob: 3.010Epoch  20:  22% | abe: 3.091 | eve: 8.998 | bob: 3.011Epoch  20:  23% | abe: 3.091 | eve: 8.998 | bob: 3.011Epoch  20:  24% | abe: 3.091 | eve: 8.998 | bob: 3.011Epoch  20:  25% | abe: 3.091 | eve: 8.999 | bob: 3.011Epoch  20:  25% | abe: 3.091 | eve: 8.999 | bob: 3.011Epoch  20:  26% | abe: 3.092 | eve: 8.999 | bob: 3.013Epoch  20:  27% | abe: 3.092 | eve: 9.000 | bob: 3.012Epoch  20:  28% | abe: 3.091 | eve: 8.998 | bob: 3.012Epoch  20:  28% | abe: 3.092 | eve: 8.999 | bob: 3.012Epoch  20:  29% | abe: 3.092 | eve: 8.999 | bob: 3.013Epoch  20:  30% | abe: 3.092 | eve: 8.999 | bob: 3.012Epoch  20:  31% | abe: 3.092 | eve: 8.999 | bob: 3.012Epoch  20:  32% | abe: 3.092 | eve: 8.998 | bob: 3.012Epoch  20:  32% | abe: 3.092 | eve: 8.998 | bob: 3.012Epoch  20:  33% | abe: 3.092 | eve: 8.997 | bob: 3.011Epoch  20:  34% | abe: 3.091 | eve: 8.997 | bob: 3.010Epoch  20:  35% | abe: 3.090 | eve: 8.997 | bob: 3.010Epoch  20:  35% | abe: 3.090 | eve: 8.997 | bob: 3.009Epoch  20:  36% | abe: 3.090 | eve: 8.997 | bob: 3.009Epoch  20:  37% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  38% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  39% | abe: 3.089 | eve: 8.998 | bob: 3.009Epoch  20:  39% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  40% | abe: 3.089 | eve: 8.998 | bob: 3.009Epoch  20:  41% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  42% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  42% | abe: 3.089 | eve: 8.997 | bob: 3.009Epoch  20:  43% | abe: 3.090 | eve: 8.997 | bob: 3.010Epoch  20:  44% | abe: 3.089 | eve: 8.996 | bob: 3.009Epoch  20:  45% | abe: 3.089 | eve: 8.996 | bob: 3.010Epoch  20:  46% | abe: 3.090 | eve: 8.996 | bob: 3.010Epoch  20:  46% | abe: 3.090 | eve: 8.996 | bob: 3.010Epoch  20:  47% | abe: 3.090 | eve: 8.997 | bob: 3.010Epoch  20:  48% | abe: 3.090 | eve: 8.997 | bob: 3.010Epoch  20:  49% | abe: 3.090 | eve: 8.997 | bob: 3.010Epoch  20:  50% | abe: 3.090 | eve: 8.997 | bob: 3.011Epoch  20:  50% | abe: 3.090 | eve: 8.997 | bob: 3.011Epoch  20:  51% | abe: 3.090 | eve: 8.996 | bob: 3.011Epoch  20:  52% | abe: 3.091 | eve: 8.997 | bob: 3.012Epoch  20:  53% | abe: 3.090 | eve: 8.996 | bob: 3.011Epoch  20:  53% | abe: 3.090 | eve: 8.997 | bob: 3.012Epoch  20:  54% | abe: 3.091 | eve: 8.997 | bob: 3.012Epoch  20:  55% | abe: 3.091 | eve: 8.997 | bob: 3.012Epoch  20:  56% | abe: 3.090 | eve: 8.997 | bob: 3.012Epoch  20:  57% | abe: 3.090 | eve: 8.998 | bob: 3.012Epoch  20:  57% | abe: 3.090 | eve: 8.997 | bob: 3.012Epoch  20:  58% | abe: 3.090 | eve: 8.998 | bob: 3.012Epoch  20:  59% | abe: 3.090 | eve: 8.998 | bob: 3.012Epoch  20:  60% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  60% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  61% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  62% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  63% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  64% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  64% | abe: 3.089 | eve: 8.997 | bob: 3.011Epoch  20:  65% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  66% | abe: 3.089 | eve: 8.998 | bob: 3.011Epoch  20:  67% | abe: 3.089 | eve: 8.997 | bob: 3.011Epoch  20:  67% | abe: 3.089 | eve: 8.997 | bob: 3.011Epoch  20:  68% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  69% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  70% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  71% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  71% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  72% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  73% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  74% | abe: 3.089 | eve: 8.998 | bob: 3.010Epoch  20:  75% | abe: 3.089 | eve: 8.998 | bob: 3.010Epoch  20:  75% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  76% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  77% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  78% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  78% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  79% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  80% | abe: 3.089 | eve: 8.998 | bob: 3.010Epoch  20:  81% | abe: 3.089 | eve: 8.998 | bob: 3.010Epoch  20:  82% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  82% | abe: 3.089 | eve: 8.996 | bob: 3.010Epoch  20:  83% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  84% | abe: 3.089 | eve: 8.996 | bob: 3.010Epoch  20:  85% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  85% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  86% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  87% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  88% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  89% | abe: 3.089 | eve: 8.997 | bob: 3.010Epoch  20:  89% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  90% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  91% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  92% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  92% | abe: 3.088 | eve: 8.996 | bob: 3.010Epoch  20:  93% | abe: 3.088 | eve: 8.996 | bob: 3.010Epoch  20:  94% | abe: 3.088 | eve: 8.996 | bob: 3.010Epoch  20:  95% | abe: 3.088 | eve: 8.996 | bob: 3.010Epoch  20:  96% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  96% | abe: 3.088 | eve: 8.996 | bob: 3.010Epoch  20:  97% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  98% | abe: 3.088 | eve: 8.997 | bob: 3.010Epoch  20:  99% | abe: 3.088 | eve: 8.997 | bob: 3.010
New best Bob loss 3.010155713933841 at epoch 20
Epoch  21:   0% | abe: 3.118 | eve: 8.990 | bob: 3.045Epoch  21:   0% | abe: 3.108 | eve: 9.026 | bob: 3.037Epoch  21:   1% | abe: 3.103 | eve: 9.007 | bob: 3.029Epoch  21:   2% | abe: 3.096 | eve: 9.005 | bob: 3.024Epoch  21:   3% | abe: 3.096 | eve: 9.003 | bob: 3.025Epoch  21:   3% | abe: 3.094 | eve: 8.997 | bob: 3.022Epoch  21:   4% | abe: 3.092 | eve: 9.000 | bob: 3.020Epoch  21:   5% | abe: 3.088 | eve: 9.000 | bob: 3.015Epoch  21:   6% | abe: 3.089 | eve: 9.001 | bob: 3.015Epoch  21:   7% | abe: 3.089 | eve: 9.001 | bob: 3.016Epoch  21:   7% | abe: 3.091 | eve: 9.001 | bob: 3.019Epoch  21:   8% | abe: 3.091 | eve: 8.999 | bob: 3.018Epoch  21:   9% | abe: 3.092 | eve: 8.998 | bob: 3.018Epoch  21:  10% | abe: 3.093 | eve: 8.998 | bob: 3.019Epoch  21:  10% | abe: 3.093 | eve: 8.997 | bob: 3.019Epoch  21:  11% | abe: 3.094 | eve: 8.997 | bob: 3.020Epoch  21:  12% | abe: 3.093 | eve: 8.997 | bob: 3.020Epoch  21:  13% | abe: 3.094 | eve: 9.000 | bob: 3.020Epoch  21:  14% | abe: 3.096 | eve: 9.001 | bob: 3.022Epoch  21:  14% | abe: 3.096 | eve: 9.002 | bob: 3.022Epoch  21:  15% | abe: 3.096 | eve: 9.001 | bob: 3.022Epoch  21:  16% | abe: 3.096 | eve: 9.002 | bob: 3.022Epoch  21:  17% | abe: 3.097 | eve: 9.002 | bob: 3.023Epoch  21:  17% | abe: 3.096 | eve: 9.003 | bob: 3.022Epoch  21:  18% | abe: 3.095 | eve: 9.002 | bob: 3.022Epoch  21:  19% | abe: 3.096 | eve: 9.002 | bob: 3.023Epoch  21:  20% | abe: 3.096 | eve: 9.001 | bob: 3.022Epoch  21:  21% | abe: 3.096 | eve: 9.000 | bob: 3.023Epoch  21:  21% | abe: 3.096 | eve: 9.000 | bob: 3.022Epoch  21:  22% | abe: 3.095 | eve: 8.999 | bob: 3.022Epoch  21:  23% | abe: 3.095 | eve: 8.999 | bob: 3.021Epoch  21:  24% | abe: 3.095 | eve: 8.999 | bob: 3.022Epoch  21:  25% | abe: 3.095 | eve: 8.998 | bob: 3.022Epoch  21:  25% | abe: 3.094 | eve: 8.998 | bob: 3.020Epoch  21:  26% | abe: 3.093 | eve: 8.997 | bob: 3.020Epoch  21:  27% | abe: 3.094 | eve: 8.998 | bob: 3.020Epoch  21:  28% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  28% | abe: 3.093 | eve: 8.999 | bob: 3.020Epoch  21:  29% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  30% | abe: 3.094 | eve: 8.999 | bob: 3.020Epoch  21:  31% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  32% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  32% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  33% | abe: 3.093 | eve: 8.999 | bob: 3.018Epoch  21:  34% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  35% | abe: 3.092 | eve: 8.999 | bob: 3.018Epoch  21:  35% | abe: 3.092 | eve: 9.000 | bob: 3.018Epoch  21:  36% | abe: 3.093 | eve: 8.999 | bob: 3.018Epoch  21:  37% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  38% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  39% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  39% | abe: 3.093 | eve: 9.001 | bob: 3.018Epoch  21:  40% | abe: 3.093 | eve: 9.001 | bob: 3.018Epoch  21:  41% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  42% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  42% | abe: 3.093 | eve: 9.001 | bob: 3.018Epoch  21:  43% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  44% | abe: 3.093 | eve: 9.000 | bob: 3.017Epoch  21:  45% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  46% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  46% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  47% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  48% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  49% | abe: 3.093 | eve: 8.999 | bob: 3.019Epoch  21:  50% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  50% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  51% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  52% | abe: 3.093 | eve: 8.999 | bob: 3.020Epoch  21:  53% | abe: 3.094 | eve: 8.999 | bob: 3.020Epoch  21:  53% | abe: 3.094 | eve: 8.999 | bob: 3.020Epoch  21:  54% | abe: 3.094 | eve: 8.999 | bob: 3.020Epoch  21:  55% | abe: 3.094 | eve: 9.000 | bob: 3.020Epoch  21:  56% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  57% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  57% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  58% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  59% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  60% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  60% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  61% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  62% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  63% | abe: 3.093 | eve: 9.001 | bob: 3.019Epoch  21:  64% | abe: 3.094 | eve: 9.001 | bob: 3.019Epoch  21:  64% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  65% | abe: 3.094 | eve: 9.000 | bob: 3.020Epoch  21:  66% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  67% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  67% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  68% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  69% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  70% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  71% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  71% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  72% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  73% | abe: 3.094 | eve: 9.000 | bob: 3.019Epoch  21:  74% | abe: 3.093 | eve: 9.000 | bob: 3.019Epoch  21:  75% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  75% | abe: 3.093 | eve: 9.001 | bob: 3.018Epoch  21:  76% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  77% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  78% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  78% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  79% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  80% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  81% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  82% | abe: 3.092 | eve: 9.000 | bob: 3.017Epoch  21:  82% | abe: 3.092 | eve: 9.000 | bob: 3.018Epoch  21:  83% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  84% | abe: 3.093 | eve: 9.000 | bob: 3.018Epoch  21:  85% | abe: 3.092 | eve: 8.999 | bob: 3.017Epoch  21:  85% | abe: 3.093 | eve: 8.999 | bob: 3.017Epoch  21:  86% | abe: 3.092 | eve: 8.999 | bob: 3.017Epoch  21:  87% | abe: 3.092 | eve: 8.999 | bob: 3.017Epoch  21:  88% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  89% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  89% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  90% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  91% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  92% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  92% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  93% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  94% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  95% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  96% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  96% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  97% | abe: 3.092 | eve: 8.999 | bob: 3.016Epoch  21:  98% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  21:  99% | abe: 3.091 | eve: 8.999 | bob: 3.016Epoch  22:   0% | abe: 3.089 | eve: 9.009 | bob: 3.024Epoch  22:   0% | abe: 3.086 | eve: 8.991 | bob: 3.015Epoch  22:   1% | abe: 3.078 | eve: 8.986 | bob: 3.006Epoch  22:   2% | abe: 3.078 | eve: 8.994 | bob: 3.008Epoch  22:   3% | abe: 3.083 | eve: 8.993 | bob: 3.012Epoch  22:   3% | abe: 3.085 | eve: 8.994 | bob: 3.013Epoch  22:   4% | abe: 3.080 | eve: 8.996 | bob: 3.008Epoch  22:   5% | abe: 3.086 | eve: 9.002 | bob: 3.013Epoch  22:   6% | abe: 3.086 | eve: 8.999 | bob: 3.013Epoch  22:   7% | abe: 3.088 | eve: 8.998 | bob: 3.014Epoch  22:   7% | abe: 3.089 | eve: 9.000 | bob: 3.015Epoch  22:   8% | abe: 3.087 | eve: 9.001 | bob: 3.014Epoch  22:   9% | abe: 3.088 | eve: 9.001 | bob: 3.016Epoch  22:  10% | abe: 3.091 | eve: 9.002 | bob: 3.018Epoch  22:  10% | abe: 3.092 | eve: 9.001 | bob: 3.019Epoch  22:  11% | abe: 3.092 | eve: 9.002 | bob: 3.019Epoch  22:  12% | abe: 3.091 | eve: 9.002 | bob: 3.019Epoch  22:  13% | abe: 3.090 | eve: 9.002 | bob: 3.018Epoch  22:  14% | abe: 3.090 | eve: 9.001 | bob: 3.018Epoch  22:  14% | abe: 3.090 | eve: 9.001 | bob: 3.018Epoch  22:  15% | abe: 3.091 | eve: 9.001 | bob: 3.019Epoch  22:  16% | abe: 3.092 | eve: 9.000 | bob: 3.020Epoch  22:  17% | abe: 3.092 | eve: 9.000 | bob: 3.021Epoch  22:  17% | abe: 3.091 | eve: 9.000 | bob: 3.020Epoch  22:  18% | abe: 3.091 | eve: 8.999 | bob: 3.020Epoch  22:  19% | abe: 3.093 | eve: 9.000 | bob: 3.021Epoch  22:  20% | abe: 3.092 | eve: 8.999 | bob: 3.021Epoch  22:  21% | abe: 3.092 | eve: 8.998 | bob: 3.021Epoch  22:  21% | abe: 3.092 | eve: 8.997 | bob: 3.021Epoch  22:  22% | abe: 3.092 | eve: 8.997 | bob: 3.022Epoch  22:  23% | abe: 3.090 | eve: 8.997 | bob: 3.021Epoch  22:  24% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  25% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  25% | abe: 3.089 | eve: 8.998 | bob: 3.020Epoch  22:  26% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  27% | abe: 3.091 | eve: 8.998 | bob: 3.020Epoch  22:  28% | abe: 3.092 | eve: 8.998 | bob: 3.021Epoch  22:  28% | abe: 3.092 | eve: 8.998 | bob: 3.021Epoch  22:  29% | abe: 3.092 | eve: 8.997 | bob: 3.021Epoch  22:  30% | abe: 3.091 | eve: 8.998 | bob: 3.021Epoch  22:  31% | abe: 3.092 | eve: 8.998 | bob: 3.021Epoch  22:  32% | abe: 3.091 | eve: 8.998 | bob: 3.020Epoch  22:  32% | abe: 3.091 | eve: 8.999 | bob: 3.019Epoch  22:  33% | abe: 3.091 | eve: 8.999 | bob: 3.019Epoch  22:  34% | abe: 3.091 | eve: 8.999 | bob: 3.019Epoch  22:  35% | abe: 3.091 | eve: 8.998 | bob: 3.018Epoch  22:  35% | abe: 3.091 | eve: 8.998 | bob: 3.018Epoch  22:  36% | abe: 3.090 | eve: 8.999 | bob: 3.018Epoch  22:  37% | abe: 3.090 | eve: 8.999 | bob: 3.018Epoch  22:  38% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  22:  39% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  22:  39% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  22:  40% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  22:  41% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  22:  42% | abe: 3.090 | eve: 8.997 | bob: 3.018Epoch  22:  42% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  22:  43% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  22:  44% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  22:  45% | abe: 3.088 | eve: 8.996 | bob: 3.017Epoch  22:  46% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  46% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  47% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  48% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  49% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  50% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  22:  50% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  22:  51% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  22:  52% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  53% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  53% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  54% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  55% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  56% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  57% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  57% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  58% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  59% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  60% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  60% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  61% | abe: 3.090 | eve: 8.997 | bob: 3.019Epoch  22:  62% | abe: 3.090 | eve: 8.997 | bob: 3.019Epoch  22:  63% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  64% | abe: 3.090 | eve: 8.998 | bob: 3.019Epoch  22:  64% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  65% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  66% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  67% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  67% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  68% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  69% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  70% | abe: 3.090 | eve: 8.998 | bob: 3.021Epoch  22:  71% | abe: 3.090 | eve: 8.997 | bob: 3.020Epoch  22:  71% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  72% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  73% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  74% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  75% | abe: 3.090 | eve: 8.998 | bob: 3.020Epoch  22:  75% | abe: 3.089 | eve: 8.998 | bob: 3.019Epoch  22:  76% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  77% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  78% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  78% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  79% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  80% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  81% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  82% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  22:  82% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  22:  83% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  22:  84% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  85% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  85% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  22:  86% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  22:  87% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  22:  88% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  89% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  89% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  90% | abe: 3.089 | eve: 8.997 | bob: 3.019Epoch  22:  91% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  92% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  92% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  93% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  22:  94% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  95% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  22:  96% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  96% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  97% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  98% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  22:  99% | abe: 3.089 | eve: 8.996 | bob: 3.018Epoch  23:   0% | abe: 3.107 | eve: 8.971 | bob: 3.031Epoch  23:   0% | abe: 3.095 | eve: 8.999 | bob: 3.018Epoch  23:   1% | abe: 3.086 | eve: 8.998 | bob: 3.008Epoch  23:   2% | abe: 3.076 | eve: 8.998 | bob: 3.000Epoch  23:   3% | abe: 3.079 | eve: 8.999 | bob: 3.004Epoch  23:   3% | abe: 3.080 | eve: 8.999 | bob: 3.004Epoch  23:   4% | abe: 3.077 | eve: 8.997 | bob: 3.003Epoch  23:   5% | abe: 3.077 | eve: 8.997 | bob: 3.004Epoch  23:   6% | abe: 3.077 | eve: 8.998 | bob: 3.005Epoch  23:   7% | abe: 3.078 | eve: 8.997 | bob: 3.006Epoch  23:   7% | abe: 3.079 | eve: 8.998 | bob: 3.007Epoch  23:   8% | abe: 3.082 | eve: 8.999 | bob: 3.010Epoch  23:   9% | abe: 3.081 | eve: 8.998 | bob: 3.009Epoch  23:  10% | abe: 3.082 | eve: 8.996 | bob: 3.010Epoch  23:  10% | abe: 3.083 | eve: 8.996 | bob: 3.010Epoch  23:  11% | abe: 3.082 | eve: 8.995 | bob: 3.010Epoch  23:  12% | abe: 3.083 | eve: 8.994 | bob: 3.009Epoch  23:  13% | abe: 3.083 | eve: 8.994 | bob: 3.010Epoch  23:  14% | abe: 3.083 | eve: 8.994 | bob: 3.009Epoch  23:  14% | abe: 3.083 | eve: 8.992 | bob: 3.009Epoch  23:  15% | abe: 3.085 | eve: 8.990 | bob: 3.011Epoch  23:  16% | abe: 3.086 | eve: 8.990 | bob: 3.011Epoch  23:  17% | abe: 3.085 | eve: 8.991 | bob: 3.011Epoch  23:  17% | abe: 3.087 | eve: 8.990 | bob: 3.013Epoch  23:  18% | abe: 3.087 | eve: 8.991 | bob: 3.013Epoch  23:  19% | abe: 3.087 | eve: 8.990 | bob: 3.013Epoch  23:  20% | abe: 3.086 | eve: 8.991 | bob: 3.013Epoch  23:  21% | abe: 3.087 | eve: 8.991 | bob: 3.014Epoch  23:  21% | abe: 3.087 | eve: 8.993 | bob: 3.015Epoch  23:  22% | abe: 3.087 | eve: 8.993 | bob: 3.015Epoch  23:  23% | abe: 3.086 | eve: 8.994 | bob: 3.015Epoch  23:  24% | abe: 3.086 | eve: 8.994 | bob: 3.015Epoch  23:  25% | abe: 3.085 | eve: 8.994 | bob: 3.015Epoch  23:  25% | abe: 3.085 | eve: 8.992 | bob: 3.015Epoch  23:  26% | abe: 3.086 | eve: 8.993 | bob: 3.016Epoch  23:  27% | abe: 3.086 | eve: 8.992 | bob: 3.016Epoch  23:  28% | abe: 3.087 | eve: 8.991 | bob: 3.017Epoch  23:  28% | abe: 3.087 | eve: 8.992 | bob: 3.017Epoch  23:  29% | abe: 3.086 | eve: 8.992 | bob: 3.017Epoch  23:  30% | abe: 3.085 | eve: 8.993 | bob: 3.017Epoch  23:  31% | abe: 3.084 | eve: 8.992 | bob: 3.016Epoch  23:  32% | abe: 3.084 | eve: 8.992 | bob: 3.016Epoch  23:  32% | abe: 3.084 | eve: 8.992 | bob: 3.016Epoch  23:  33% | abe: 3.084 | eve: 8.993 | bob: 3.016Epoch  23:  34% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  35% | abe: 3.084 | eve: 8.993 | bob: 3.017Epoch  23:  35% | abe: 3.084 | eve: 8.993 | bob: 3.017Epoch  23:  36% | abe: 3.084 | eve: 8.994 | bob: 3.017Epoch  23:  37% | abe: 3.084 | eve: 8.994 | bob: 3.018Epoch  23:  38% | abe: 3.084 | eve: 8.993 | bob: 3.018Epoch  23:  39% | abe: 3.085 | eve: 8.993 | bob: 3.019Epoch  23:  39% | abe: 3.085 | eve: 8.992 | bob: 3.019Epoch  23:  40% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  41% | abe: 3.084 | eve: 8.992 | bob: 3.019Epoch  23:  42% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  42% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  43% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  44% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  45% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  46% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  46% | abe: 3.085 | eve: 8.992 | bob: 3.021Epoch  23:  47% | abe: 3.085 | eve: 8.992 | bob: 3.020Epoch  23:  48% | abe: 3.084 | eve: 8.993 | bob: 3.020Epoch  23:  49% | abe: 3.084 | eve: 8.993 | bob: 3.020Epoch  23:  50% | abe: 3.085 | eve: 8.993 | bob: 3.020Epoch  23:  50% | abe: 3.085 | eve: 8.993 | bob: 3.020Epoch  23:  51% | abe: 3.084 | eve: 8.994 | bob: 3.019Epoch  23:  52% | abe: 3.085 | eve: 8.994 | bob: 3.019Epoch  23:  53% | abe: 3.084 | eve: 8.994 | bob: 3.019Epoch  23:  53% | abe: 3.084 | eve: 8.994 | bob: 3.018Epoch  23:  54% | abe: 3.084 | eve: 8.994 | bob: 3.018Epoch  23:  55% | abe: 3.084 | eve: 8.994 | bob: 3.017Epoch  23:  56% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  57% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  57% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  58% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  59% | abe: 3.086 | eve: 8.994 | bob: 3.017Epoch  23:  60% | abe: 3.085 | eve: 8.994 | bob: 3.017Epoch  23:  60% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  61% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  62% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  63% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  64% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  64% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  65% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  66% | abe: 3.085 | eve: 8.993 | bob: 3.016Epoch  23:  67% | abe: 3.085 | eve: 8.993 | bob: 3.016Epoch  23:  67% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  68% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  69% | abe: 3.085 | eve: 8.994 | bob: 3.016Epoch  23:  70% | abe: 3.085 | eve: 8.994 | bob: 3.015Epoch  23:  71% | abe: 3.084 | eve: 8.994 | bob: 3.015Epoch  23:  71% | abe: 3.085 | eve: 8.994 | bob: 3.015Epoch  23:  72% | abe: 3.085 | eve: 8.994 | bob: 3.015Epoch  23:  73% | abe: 3.084 | eve: 8.994 | bob: 3.015Epoch  23:  74% | abe: 3.084 | eve: 8.994 | bob: 3.015Epoch  23:  75% | abe: 3.084 | eve: 8.993 | bob: 3.015Epoch  23:  75% | abe: 3.084 | eve: 8.993 | bob: 3.014Epoch  23:  76% | abe: 3.084 | eve: 8.993 | bob: 3.015Epoch  23:  77% | abe: 3.084 | eve: 8.993 | bob: 3.015Epoch  23:  78% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  78% | abe: 3.084 | eve: 8.993 | bob: 3.015Epoch  23:  79% | abe: 3.085 | eve: 8.994 | bob: 3.015Epoch  23:  80% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  81% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  82% | abe: 3.084 | eve: 8.993 | bob: 3.014Epoch  23:  82% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  83% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  84% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  85% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  85% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  86% | abe: 3.085 | eve: 8.993 | bob: 3.015Epoch  23:  87% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  88% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  89% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  89% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  90% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  91% | abe: 3.084 | eve: 8.993 | bob: 3.014Epoch  23:  92% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  92% | abe: 3.084 | eve: 8.993 | bob: 3.013Epoch  23:  93% | abe: 3.085 | eve: 8.993 | bob: 3.014Epoch  23:  94% | abe: 3.085 | eve: 8.993 | bob: 3.013Epoch  23:  95% | abe: 3.084 | eve: 8.993 | bob: 3.013Epoch  23:  96% | abe: 3.084 | eve: 8.993 | bob: 3.013Epoch  23:  96% | abe: 3.084 | eve: 8.993 | bob: 3.013Epoch  23:  97% | abe: 3.084 | eve: 8.993 | bob: 3.013Epoch  23:  98% | abe: 3.084 | eve: 8.993 | bob: 3.012Epoch  23:  99% | abe: 3.084 | eve: 8.994 | bob: 3.013Epoch  24:   0% | abe: 3.102 | eve: 9.013 | bob: 3.041Epoch  24:   0% | abe: 3.093 | eve: 9.009 | bob: 3.026Epoch  24:   1% | abe: 3.085 | eve: 9.006 | bob: 3.024Epoch  24:   2% | abe: 3.088 | eve: 9.012 | bob: 3.023Epoch  24:   3% | abe: 3.086 | eve: 9.003 | bob: 3.020Epoch  24:   3% | abe: 3.087 | eve: 8.994 | bob: 3.023Epoch  24:   4% | abe: 3.091 | eve: 8.989 | bob: 3.026Epoch  24:   5% | abe: 3.089 | eve: 8.990 | bob: 3.024Epoch  24:   6% | abe: 3.091 | eve: 8.992 | bob: 3.029Epoch  24:   7% | abe: 3.091 | eve: 8.994 | bob: 3.028Epoch  24:   7% | abe: 3.093 | eve: 8.996 | bob: 3.031Epoch  24:   8% | abe: 3.091 | eve: 8.994 | bob: 3.029Epoch  24:   9% | abe: 3.090 | eve: 8.994 | bob: 3.028Epoch  24:  10% | abe: 3.090 | eve: 8.993 | bob: 3.029Epoch  24:  10% | abe: 3.091 | eve: 8.995 | bob: 3.030Epoch  24:  11% | abe: 3.090 | eve: 8.994 | bob: 3.029Epoch  24:  12% | abe: 3.091 | eve: 8.997 | bob: 3.030Epoch  24:  13% | abe: 3.090 | eve: 8.996 | bob: 3.028Epoch  24:  14% | abe: 3.088 | eve: 8.996 | bob: 3.026Epoch  24:  14% | abe: 3.089 | eve: 8.996 | bob: 3.026Epoch  24:  15% | abe: 3.087 | eve: 8.995 | bob: 3.025Epoch  24:  16% | abe: 3.087 | eve: 8.994 | bob: 3.025Epoch  24:  17% | abe: 3.085 | eve: 8.992 | bob: 3.023Epoch  24:  17% | abe: 3.086 | eve: 8.993 | bob: 3.023Epoch  24:  18% | abe: 3.086 | eve: 8.993 | bob: 3.023Epoch  24:  19% | abe: 3.085 | eve: 8.993 | bob: 3.022Epoch  24:  20% | abe: 3.085 | eve: 8.992 | bob: 3.023Epoch  24:  21% | abe: 3.085 | eve: 8.994 | bob: 3.022Epoch  24:  21% | abe: 3.085 | eve: 8.994 | bob: 3.023Epoch  24:  22% | abe: 3.086 | eve: 8.993 | bob: 3.023Epoch  24:  23% | abe: 3.086 | eve: 8.994 | bob: 3.023Epoch  24:  24% | abe: 3.086 | eve: 8.995 | bob: 3.023Epoch  24:  25% | abe: 3.086 | eve: 8.995 | bob: 3.024Epoch  24:  25% | abe: 3.086 | eve: 8.995 | bob: 3.023Epoch  24:  26% | abe: 3.086 | eve: 8.996 | bob: 3.023Epoch  24:  27% | abe: 3.087 | eve: 8.995 | bob: 3.023Epoch  24:  28% | abe: 3.086 | eve: 8.994 | bob: 3.022Epoch  24:  28% | abe: 3.087 | eve: 8.994 | bob: 3.023Epoch  24:  29% | abe: 3.088 | eve: 8.994 | bob: 3.023Epoch  24:  30% | abe: 3.087 | eve: 8.993 | bob: 3.022Epoch  24:  31% | abe: 3.087 | eve: 8.994 | bob: 3.022Epoch  24:  32% | abe: 3.088 | eve: 8.993 | bob: 3.022Epoch  24:  32% | abe: 3.088 | eve: 8.994 | bob: 3.022Epoch  24:  33% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  34% | abe: 3.088 | eve: 8.994 | bob: 3.022Epoch  24:  35% | abe: 3.089 | eve: 8.994 | bob: 3.022Epoch  24:  35% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  36% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  37% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  38% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  39% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  39% | abe: 3.089 | eve: 8.994 | bob: 3.023Epoch  24:  40% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  41% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  42% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  42% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  43% | abe: 3.089 | eve: 8.995 | bob: 3.023Epoch  24:  44% | abe: 3.089 | eve: 8.996 | bob: 3.023Epoch  24:  45% | abe: 3.089 | eve: 8.996 | bob: 3.022Epoch  24:  46% | abe: 3.089 | eve: 8.996 | bob: 3.022Epoch  24:  46% | abe: 3.089 | eve: 8.996 | bob: 3.022Epoch  24:  47% | abe: 3.089 | eve: 8.996 | bob: 3.021Epoch  24:  48% | abe: 3.089 | eve: 8.996 | bob: 3.021Epoch  24:  49% | abe: 3.089 | eve: 8.997 | bob: 3.021Epoch  24:  50% | abe: 3.089 | eve: 8.997 | bob: 3.021Epoch  24:  50% | abe: 3.089 | eve: 8.997 | bob: 3.020Epoch  24:  51% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  52% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  53% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  53% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  54% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  55% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  24:  56% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  24:  57% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  57% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  58% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  59% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  60% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  60% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  61% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  62% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  24:  63% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  24:  64% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  24:  64% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  65% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  24:  66% | abe: 3.088 | eve: 8.997 | bob: 3.019Epoch  24:  67% | abe: 3.088 | eve: 8.997 | bob: 3.019Epoch  24:  67% | abe: 3.088 | eve: 8.997 | bob: 3.019Epoch  24:  68% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  69% | abe: 3.088 | eve: 8.997 | bob: 3.019Epoch  24:  70% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  71% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  71% | abe: 3.088 | eve: 8.996 | bob: 3.019Epoch  24:  72% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  73% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  74% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  75% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  75% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  76% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  77% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  78% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  78% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  79% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  80% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  81% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  82% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  82% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  83% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  84% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  85% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  85% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  86% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  87% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  88% | abe: 3.086 | eve: 8.997 | bob: 3.017Epoch  24:  89% | abe: 3.086 | eve: 8.997 | bob: 3.018Epoch  24:  89% | abe: 3.087 | eve: 8.997 | bob: 3.018Epoch  24:  90% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  91% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  24:  92% | abe: 3.087 | eve: 8.997 | bob: 3.019Epoch  24:  92% | abe: 3.087 | eve: 8.996 | bob: 3.019Epoch  24:  93% | abe: 3.087 | eve: 8.996 | bob: 3.019Epoch  24:  94% | abe: 3.087 | eve: 8.996 | bob: 3.019Epoch  24:  95% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  24:  96% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  24:  96% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  24:  97% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  24:  98% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  24:  99% | abe: 3.087 | eve: 8.996 | bob: 3.020Epoch  25:   0% | abe: 3.078 | eve: 8.996 | bob: 3.011Epoch  25:   0% | abe: 3.083 | eve: 8.985 | bob: 3.017Epoch  25:   1% | abe: 3.085 | eve: 8.989 | bob: 3.017Epoch  25:   2% | abe: 3.078 | eve: 8.998 | bob: 3.011Epoch  25:   3% | abe: 3.083 | eve: 8.997 | bob: 3.015Epoch  25:   3% | abe: 3.079 | eve: 8.993 | bob: 3.010Epoch  25:   4% | abe: 3.080 | eve: 9.000 | bob: 3.011Epoch  25:   5% | abe: 3.081 | eve: 8.999 | bob: 3.010Epoch  25:   6% | abe: 3.081 | eve: 9.003 | bob: 3.010Epoch  25:   7% | abe: 3.080 | eve: 9.003 | bob: 3.009Epoch  25:   7% | abe: 3.080 | eve: 9.002 | bob: 3.010Epoch  25:   8% | abe: 3.079 | eve: 9.003 | bob: 3.010Epoch  25:   9% | abe: 3.079 | eve: 9.003 | bob: 3.009Epoch  25:  10% | abe: 3.079 | eve: 9.003 | bob: 3.009Epoch  25:  10% | abe: 3.078 | eve: 9.003 | bob: 3.008Epoch  25:  11% | abe: 3.077 | eve: 9.004 | bob: 3.006Epoch  25:  12% | abe: 3.078 | eve: 9.005 | bob: 3.008Epoch  25:  13% | abe: 3.078 | eve: 9.004 | bob: 3.009Epoch  25:  14% | abe: 3.078 | eve: 9.003 | bob: 3.008Epoch  25:  14% | abe: 3.077 | eve: 9.000 | bob: 3.008Epoch  25:  15% | abe: 3.076 | eve: 8.998 | bob: 3.008Epoch  25:  16% | abe: 3.077 | eve: 8.998 | bob: 3.010Epoch  25:  17% | abe: 3.077 | eve: 8.997 | bob: 3.010Epoch  25:  17% | abe: 3.077 | eve: 8.996 | bob: 3.010Epoch  25:  18% | abe: 3.079 | eve: 8.996 | bob: 3.012Epoch  25:  19% | abe: 3.078 | eve: 8.995 | bob: 3.011Epoch  25:  20% | abe: 3.078 | eve: 8.996 | bob: 3.011Epoch  25:  21% | abe: 3.079 | eve: 8.996 | bob: 3.013Epoch  25:  21% | abe: 3.080 | eve: 8.995 | bob: 3.013Epoch  25:  22% | abe: 3.079 | eve: 8.995 | bob: 3.013Epoch  25:  23% | abe: 3.081 | eve: 8.996 | bob: 3.014Epoch  25:  24% | abe: 3.081 | eve: 8.996 | bob: 3.015Epoch  25:  25% | abe: 3.082 | eve: 8.996 | bob: 3.016Epoch  25:  25% | abe: 3.083 | eve: 8.995 | bob: 3.017Epoch  25:  26% | abe: 3.083 | eve: 8.996 | bob: 3.017Epoch  25:  27% | abe: 3.083 | eve: 8.996 | bob: 3.018Epoch  25:  28% | abe: 3.084 | eve: 8.997 | bob: 3.018Epoch  25:  28% | abe: 3.084 | eve: 8.996 | bob: 3.019Epoch  25:  29% | abe: 3.084 | eve: 8.996 | bob: 3.018Epoch  25:  30% | abe: 3.085 | eve: 8.997 | bob: 3.019Epoch  25:  31% | abe: 3.085 | eve: 8.996 | bob: 3.020Epoch  25:  32% | abe: 3.085 | eve: 8.995 | bob: 3.020Epoch  25:  32% | abe: 3.085 | eve: 8.995 | bob: 3.021Epoch  25:  33% | abe: 3.086 | eve: 8.994 | bob: 3.021Epoch  25:  34% | abe: 3.085 | eve: 8.994 | bob: 3.020Epoch  25:  35% | abe: 3.085 | eve: 8.995 | bob: 3.021Epoch  25:  35% | abe: 3.086 | eve: 8.995 | bob: 3.021Epoch  25:  36% | abe: 3.086 | eve: 8.996 | bob: 3.021Epoch  25:  37% | abe: 3.086 | eve: 8.996 | bob: 3.021Epoch  25:  38% | abe: 3.086 | eve: 8.994 | bob: 3.021Epoch  25:  39% | abe: 3.085 | eve: 8.994 | bob: 3.020Epoch  25:  39% | abe: 3.086 | eve: 8.994 | bob: 3.020Epoch  25:  40% | abe: 3.086 | eve: 8.994 | bob: 3.020Epoch  25:  41% | abe: 3.086 | eve: 8.995 | bob: 3.021Epoch  25:  42% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  42% | abe: 3.085 | eve: 8.995 | bob: 3.019Epoch  25:  43% | abe: 3.085 | eve: 8.995 | bob: 3.020Epoch  25:  44% | abe: 3.085 | eve: 8.995 | bob: 3.020Epoch  25:  45% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  46% | abe: 3.085 | eve: 8.996 | bob: 3.020Epoch  25:  46% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  47% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  48% | abe: 3.085 | eve: 8.995 | bob: 3.020Epoch  25:  49% | abe: 3.086 | eve: 8.994 | bob: 3.020Epoch  25:  50% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  50% | abe: 3.086 | eve: 8.995 | bob: 3.020Epoch  25:  51% | abe: 3.086 | eve: 8.995 | bob: 3.019Epoch  25:  52% | abe: 3.086 | eve: 8.995 | bob: 3.019Epoch  25:  53% | abe: 3.086 | eve: 8.995 | bob: 3.019Epoch  25:  53% | abe: 3.086 | eve: 8.995 | bob: 3.019Epoch  25:  54% | abe: 3.086 | eve: 8.995 | bob: 3.018Epoch  25:  55% | abe: 3.086 | eve: 8.995 | bob: 3.018Epoch  25:  56% | abe: 3.085 | eve: 8.995 | bob: 3.017Epoch  25:  57% | abe: 3.086 | eve: 8.996 | bob: 3.017Epoch  25:  57% | abe: 3.086 | eve: 8.996 | bob: 3.017Epoch  25:  58% | abe: 3.086 | eve: 8.995 | bob: 3.017Epoch  25:  59% | abe: 3.086 | eve: 8.995 | bob: 3.017Epoch  25:  60% | abe: 3.086 | eve: 8.995 | bob: 3.017Epoch  25:  60% | abe: 3.086 | eve: 8.995 | bob: 3.017Epoch  25:  61% | abe: 3.087 | eve: 8.995 | bob: 3.018Epoch  25:  62% | abe: 3.086 | eve: 8.995 | bob: 3.018Epoch  25:  63% | abe: 3.087 | eve: 8.995 | bob: 3.018Epoch  25:  64% | abe: 3.087 | eve: 8.995 | bob: 3.019Epoch  25:  64% | abe: 3.087 | eve: 8.995 | bob: 3.019Epoch  25:  65% | abe: 3.087 | eve: 8.995 | bob: 3.018Epoch  25:  66% | abe: 3.087 | eve: 8.995 | bob: 3.018Epoch  25:  67% | abe: 3.088 | eve: 8.995 | bob: 3.019Epoch  25:  67% | abe: 3.088 | eve: 8.995 | bob: 3.018Epoch  25:  68% | abe: 3.088 | eve: 8.995 | bob: 3.018Epoch  25:  69% | abe: 3.087 | eve: 8.995 | bob: 3.018Epoch  25:  70% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  25:  71% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  25:  71% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  25:  72% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  25:  73% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  25:  74% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  25:  75% | abe: 3.088 | eve: 8.996 | bob: 3.018Epoch  25:  75% | abe: 3.087 | eve: 8.996 | bob: 3.018Epoch  25:  76% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  77% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  78% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  25:  78% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  79% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  80% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  81% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  82% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  82% | abe: 3.087 | eve: 8.996 | bob: 3.017Epoch  25:  83% | abe: 3.087 | eve: 8.995 | bob: 3.016Epoch  25:  84% | abe: 3.087 | eve: 8.995 | bob: 3.016Epoch  25:  85% | abe: 3.087 | eve: 8.996 | bob: 3.016Epoch  25:  85% | abe: 3.087 | eve: 8.996 | bob: 3.016Epoch  25:  86% | abe: 3.086 | eve: 8.996 | bob: 3.016Epoch  25:  87% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  88% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  89% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  89% | abe: 3.086 | eve: 8.994 | bob: 3.016Epoch  25:  90% | abe: 3.086 | eve: 8.994 | bob: 3.016Epoch  25:  91% | abe: 3.086 | eve: 8.994 | bob: 3.016Epoch  25:  92% | abe: 3.086 | eve: 8.994 | bob: 3.016Epoch  25:  92% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  93% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  94% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  95% | abe: 3.086 | eve: 8.995 | bob: 3.016Epoch  25:  96% | abe: 3.085 | eve: 8.995 | bob: 3.016Epoch  25:  96% | abe: 3.085 | eve: 8.995 | bob: 3.015Epoch  25:  97% | abe: 3.085 | eve: 8.996 | bob: 3.016Epoch  25:  98% | abe: 3.085 | eve: 8.996 | bob: 3.016Epoch  25:  99% | abe: 3.085 | eve: 8.996 | bob: 3.016Epoch  26:   0% | abe: 3.051 | eve: 9.026 | bob: 2.999Epoch  26:   0% | abe: 3.052 | eve: 9.020 | bob: 2.995Epoch  26:   1% | abe: 3.065 | eve: 9.009 | bob: 3.007Epoch  26:   2% | abe: 3.080 | eve: 9.013 | bob: 3.022Epoch  26:   3% | abe: 3.080 | eve: 9.002 | bob: 3.021Epoch  26:   3% | abe: 3.081 | eve: 9.002 | bob: 3.021Epoch  26:   4% | abe: 3.080 | eve: 9.003 | bob: 3.017Epoch  26:   5% | abe: 3.088 | eve: 9.001 | bob: 3.024Epoch  26:   6% | abe: 3.088 | eve: 8.999 | bob: 3.023Epoch  26:   7% | abe: 3.086 | eve: 9.002 | bob: 3.020Epoch  26:   7% | abe: 3.086 | eve: 9.003 | bob: 3.019Epoch  26:   8% | abe: 3.089 | eve: 9.000 | bob: 3.021Epoch  26:   9% | abe: 3.090 | eve: 8.997 | bob: 3.022Epoch  26:  10% | abe: 3.088 | eve: 8.995 | bob: 3.020Epoch  26:  10% | abe: 3.090 | eve: 8.995 | bob: 3.021Epoch  26:  11% | abe: 3.089 | eve: 8.995 | bob: 3.019Epoch  26:  12% | abe: 3.089 | eve: 8.994 | bob: 3.020Epoch  26:  13% | abe: 3.089 | eve: 8.993 | bob: 3.020Epoch  26:  14% | abe: 3.091 | eve: 8.995 | bob: 3.022Epoch  26:  14% | abe: 3.089 | eve: 8.995 | bob: 3.020Epoch  26:  15% | abe: 3.092 | eve: 8.997 | bob: 3.022Epoch  26:  16% | abe: 3.092 | eve: 8.996 | bob: 3.022Epoch  26:  17% | abe: 3.093 | eve: 8.996 | bob: 3.023Epoch  26:  17% | abe: 3.091 | eve: 8.995 | bob: 3.021Epoch  26:  18% | abe: 3.090 | eve: 8.995 | bob: 3.019Epoch  26:  19% | abe: 3.089 | eve: 8.996 | bob: 3.019Epoch  26:  20% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  26:  21% | abe: 3.088 | eve: 8.996 | bob: 3.017Epoch  26:  21% | abe: 3.089 | eve: 8.996 | bob: 3.017Epoch  26:  22% | abe: 3.090 | eve: 8.996 | bob: 3.018Epoch  26:  23% | abe: 3.090 | eve: 8.997 | bob: 3.017Epoch  26:  24% | abe: 3.090 | eve: 8.997 | bob: 3.018Epoch  26:  25% | abe: 3.090 | eve: 8.997 | bob: 3.017Epoch  26:  25% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  26:  26% | abe: 3.090 | eve: 8.997 | bob: 3.018Epoch  26:  27% | abe: 3.090 | eve: 8.997 | bob: 3.018Epoch  26:  28% | abe: 3.090 | eve: 8.998 | bob: 3.018Epoch  26:  28% | abe: 3.091 | eve: 8.998 | bob: 3.019Epoch  26:  29% | abe: 3.090 | eve: 8.997 | bob: 3.019Epoch  26:  30% | abe: 3.090 | eve: 8.998 | bob: 3.019Epoch  26:  31% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  26:  32% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  26:  32% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  26:  33% | abe: 3.088 | eve: 8.997 | bob: 3.018Epoch  26:  34% | abe: 3.087 | eve: 8.997 | bob: 3.017Epoch  26:  35% | abe: 3.088 | eve: 8.997 | bob: 3.017Epoch  26:  35% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  26:  36% | abe: 3.089 | eve: 8.997 | bob: 3.018Epoch  26:  37% | abe: 3.089 | eve: 8.998 | bob: 3.018Epoch  26:  38% | abe: 3.089 | eve: 8.998 | bob: 3.019Epoch  26:  39% | abe: 3.089 | eve: 8.998 | bob: 3.018Epoch  26:  39% | abe: 3.089 | eve: 8.999 | bob: 3.018Epoch  26:  40% | abe: 3.088 | eve: 8.999 | bob: 3.018Epoch  26:  41% | abe: 3.088 | eve: 8.999 | bob: 3.017Epoch  26:  42% | abe: 3.088 | eve: 8.999 | bob: 3.017Epoch  26:  42% | abe: 3.088 | eve: 8.999 | bob: 3.018Epoch  26:  43% | abe: 3.089 | eve: 8.999 | bob: 3.018Epoch  26:  44% | abe: 3.089 | eve: 8.999 | bob: 3.018Epoch  26:  45% | abe: 3.089 | eve: 8.999 | bob: 3.018Epoch  26:  46% | abe: 3.088 | eve: 8.999 | bob: 3.017Epoch  26:  46% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  47% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  48% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  49% | abe: 3.088 | eve: 8.998 | bob: 3.017Epoch  26:  50% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  50% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  51% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  52% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  53% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  53% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  54% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  55% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  56% | abe: 3.087 | eve: 8.998 | bob: 3.018Epoch  26:  57% | abe: 3.087 | eve: 8.999 | bob: 3.017Epoch  26:  57% | abe: 3.087 | eve: 8.998 | bob: 3.018Epoch  26:  58% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  59% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  60% | abe: 3.087 | eve: 8.999 | bob: 3.019Epoch  26:  60% | abe: 3.087 | eve: 9.000 | bob: 3.019Epoch  26:  61% | abe: 3.087 | eve: 8.999 | bob: 3.019Epoch  26:  62% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  63% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  64% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  64% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  65% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  66% | abe: 3.088 | eve: 8.999 | bob: 3.019Epoch  26:  67% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  67% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  68% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  69% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  70% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  71% | abe: 3.087 | eve: 8.999 | bob: 3.019Epoch  26:  71% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  72% | abe: 3.087 | eve: 8.998 | bob: 3.018Epoch  26:  73% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  74% | abe: 3.087 | eve: 8.999 | bob: 3.018Epoch  26:  75% | abe: 3.086 | eve: 8.998 | bob: 3.017Epoch  26:  75% | abe: 3.086 | eve: 8.999 | bob: 3.017Epoch  26:  76% | abe: 3.086 | eve: 8.999 | bob: 3.017Epoch  26:  77% | abe: 3.086 | eve: 8.999 | bob: 3.017Epoch  26:  78% | abe: 3.086 | eve: 8.999 | bob: 3.017Epoch  26:  78% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  79% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  80% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  81% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  82% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  82% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  83% | abe: 3.086 | eve: 8.999 | bob: 3.018Epoch  26:  84% | abe: 3.086 | eve: 8.998 | bob: 3.019Epoch  26:  85% | abe: 3.085 | eve: 8.999 | bob: 3.018Epoch  26:  85% | abe: 3.085 | eve: 8.999 | bob: 3.018Epoch  26:  86% | abe: 3.085 | eve: 8.999 | bob: 3.019Epoch  26:  87% | abe: 3.085 | eve: 8.999 | bob: 3.019Epoch  26:  88% | abe: 3.085 | eve: 8.999 | bob: 3.019Epoch  26:  89% | abe: 3.085 | eve: 8.999 | bob: 3.019Epoch  26:  89% | abe: 3.085 | eve: 9.000 | bob: 3.019Epoch  26:  90% | abe: 3.085 | eve: 9.000 | bob: 3.020Epoch  26:  91% | abe: 3.085 | eve: 8.999 | bob: 3.020Epoch  26:  92% | abe: 3.085 | eve: 8.999 | bob: 3.020Epoch  26:  92% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  93% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  94% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  95% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  96% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  96% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  97% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  98% | abe: 3.086 | eve: 8.999 | bob: 3.020Epoch  26:  99% | abe: 3.086 | eve: 8.999 | bob: 3.020
Early stopping: No improvement after 5 epochs since epoch 20. Best Bob loss: 3.010155713933841
Training complete.
cipher1 + cipher2
[[1.4091153  0.4674428  1.7213317  ... 0.21916401 0.72106016 0.88006085]
 [0.96502197 0.4678933  1.6475449  ... 0.6885531  0.8960125  0.7096047 ]
 [0.6792574  0.9836782  1.5254855  ... 0.940501   1.129554   0.6808847 ]
 ...
 [0.89836633 1.2746637  1.4720817  ... 1.1254585  0.6602361  1.0195246 ]
 [1.1370523  0.7174276  1.0867453  ... 0.46261832 0.40380114 1.3498149 ]
 [0.56982327 1.4595385  0.8896537  ... 1.375624   0.8388867  1.2852117 ]]
HO addition:
[[1.4095055  0.4675723  1.7218086  ... 0.2192247  0.72125983 0.8803046 ]
 [0.9652893  0.46802288 1.6480011  ... 0.68874377 0.8962606  0.70980126]
 [0.6794455  0.98395073 1.525908   ... 0.9407615  1.1298668  0.6810733 ]
 ...
 [0.89861506 1.2750167  1.4724895  ... 1.1257701  0.66041905 1.019807  ]
 [1.1373671  0.7176263  1.0870463  ... 0.46274644 0.40391296 1.3501887 ]
 [0.5699811  1.4599427  0.8899001  ... 1.376005   0.839119   1.2855676 ]]
cipher1 * cipher2
[[0.48916423 0.01693457 0.73131514 ... 0.01079172 0.04260811 0.16274142]
 [0.19760694 0.048621   0.6782664  ... 0.08202322 0.0605367  0.11841795]
 [0.11336448 0.16436768 0.578711   ... 0.19764644 0.29546222 0.10781603]
 ...
 [0.16066039 0.39589202 0.53272146 ... 0.31645858 0.10897516 0.25869158]
 [0.32217285 0.10800995 0.29391214 ... 0.03178371 0.02612296 0.44590488]
 [0.07612244 0.5006878  0.12220142 ... 0.46037287 0.12176477 0.34654796]]
HO multiplication
[[0.48921055 0.01695762 0.7313739  ... 0.0108557  0.0426533  0.16275738]
 [0.19762057 0.04865255 0.67833316 ... 0.08205433 0.06059453 0.11844184]
 [0.11338734 0.16439947 0.578779   ... 0.19765513 0.29546887 0.10784284]
 ...
 [0.16068706 0.39591882 0.5327815  ... 0.31645468 0.10899768 0.2586854 ]
 [0.3221714  0.10804094 0.29390737 ... 0.03181719 0.02615334 0.44594872]
 [0.07615477 0.50075114 0.12223496 ... 0.46041244 0.12180331 0.3466045 ]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 99.69%
Bob decrypted addition: [[0.         1.         1.         ... 1.         1.         0.65034664]
 [0.         0.24381924 0.8773516  ... 0.8793379  0.7008196  1.        ]
 [0.7226361  0.         0.         ... 0.9219781  0.         0.5856632 ]
 ...
 [0.40313894 1.         0.83217    ... 0.33458716 0.30744368 0.9320599 ]
 [0.         1.         0.         ... 0.19165024 0.7201308  1.        ]
 [1.         0.79023933 0.74332005 ... 0.99501085 0.         0.        ]]
Bob decrypted bits addition: [[0 1 1 ... 1 1 1]
 [0 0 1 ... 1 1 1]
 [1 0 0 ... 1 0 1]
 ...
 [0 1 1 ... 0 0 1]
 [0 1 0 ... 0 1 1]
 [1 1 1 ... 1 0 0]]
Number of correctly decrypted bits addition: 4160
Total number of bits addition: 8192
Decryption accuracy addition: 50.78125%
Bob decrypted multiplication: [[0.         1.         1.         ... 1.         1.         0.6767357 ]
 [0.         0.17979243 0.79187155 ... 0.89456487 0.69872975 1.        ]
 [0.41041628 0.         0.         ... 0.70159054 0.         0.5990464 ]
 ...
 [0.5447156  1.         0.69318205 ... 0.43300313 0.6175672  0.82520986]
 [0.         1.         0.         ... 0.14860359 0.63001496 1.        ]
 [1.         0.55065113 0.51683426 ... 0.90731776 0.         0.        ]]
Bob decrypted bits multiplication: [[0 1 1 ... 1 1 1]
 [0 0 1 ... 1 1 1]
 [0 0 0 ... 1 0 1]
 ...
 [1 1 1 ... 0 1 1]
 [0 1 0 ... 0 1 1]
 [1 1 1 ... 1 0 0]]
Number of correctly decrypted bits multiplication: 6079
Total number of bits multiplication: 8192
Decryption accuracy multiplication: 74.20654296875%
Eve decrypted addition: [[0.48337236 0.48810384 0.5452875  ... 0.53182006 0.44921026 0.44651008]
 [0.48899946 0.49643373 0.5583378  ... 0.5393744  0.44806007 0.4465968 ]
 [0.48618844 0.49142018 0.5495355  ... 0.53461665 0.44866237 0.44685546]
 ...
 [0.48198637 0.4883885  0.55038697 ... 0.5325722  0.44958612 0.44542888]
 [0.4885066  0.49467832 0.55349416 ... 0.5369262  0.4482575  0.44711268]
 [0.48679185 0.49571308 0.5608244  ... 0.54067755 0.44815007 0.4459076 ]]
Eve decrypted bits addition: [[0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 ...
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]]
Number of correctly decrypted bits by Eve addition: 3073
Total number of bits addition: 8192
Decryption accuracy by Eve addition: 37.51220703125%
Eve decrypted mulitplication: [[0.48749822 0.49297357 0.56566167 ... 0.54739136 0.44723317 0.44612387]
 [0.48381883 0.49861366 0.56954163 ... 0.5499292  0.44849265 0.44485238]
 [0.4864322  0.49444845 0.56777394 ... 0.5524585  0.44871795 0.44580984]
 ...
 [0.48194665 0.49439675 0.56841373 ... 0.54820156 0.44904476 0.44429156]
 [0.4866933  0.49826482 0.56616896 ... 0.5479925  0.44835693 0.4459968 ]
 [0.4802911  0.4964774  0.5701713  ... 0.5511376  0.44910273 0.44438744]]
Eve decrypted bits mulitplication: [[0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 ...
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]
 [0 0 1 ... 1 0 0]]
Number of correctly decrypted bits by Eve mulitplication: 4070
Total number of bits mulitplication: 8192
Decryption accuracy by Eve mulitplication: 49.6826171875%
Bob decrypted P1: [[0. 1. 1. ... 1. 1. 0.]
 [0. 1. 1. ... 1. 1. 1.]
 [1. 0. 0. ... 0. 0. 0.]
 ...
 [1. 1. 0. ... 0. 1. 1.]
 [0. 1. 0. ... 1. 1. 1.]
 [1. 0. 0. ... 1. 0. 0.]]
Bob decrypted bits P1: [[0 1 1 ... 1 1 0]
 [0 1 1 ... 1 1 1]
 [1 0 0 ... 0 0 0]
 ...
 [1 1 0 ... 0 1 1]
 [0 1 0 ... 1 1 1]
 [1 0 0 ... 1 0 0]]
Number of correctly decrypted bits P1: 8192
Total number of bits P1: 8192
Decryption accuracy P1: 100.0%
Bob decrypted P2: [[0. 1. 1. ... 1. 1. 1.]
 [0. 0. 0. ... 0. 0. 1.]
 [0. 0. 0. ... 1. 0. 1.]
 ...
 [0. 1. 1. ... 1. 0. 0.]
 [0. 1. 0. ... 0. 0. 1.]
 [1. 1. 1. ... 0. 0. 0.]]
Bob decrypted bits P2: [[0 1 1 ... 1 1 1]
 [0 0 0 ... 0 0 1]
 [0 0 0 ... 1 0 1]
 ...
 [0 1 1 ... 1 0 0]
 [0 1 0 ... 0 0 1]
 [1 1 1 ... 0 0 0]]
Number of correctly decrypted bits P2: 8192
Total number of bits P2: 8192
Decryption accuracy P2: 100.0%
