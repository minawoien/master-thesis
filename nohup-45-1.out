WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-13 10:22:03.617931: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-13 10:22:03.749324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-13 10:22:03.750121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-13 10:22:03.752154: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-13 10:22:03.753850: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-13 10:22:03.754457: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-13 10:22:03.756700: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-13 10:22:03.758465: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-13 10:22:03.763380: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-13 10:22:03.771922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-13 10:22:03.772357: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-13 10:22:03.790801: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-13 10:22:03.793967: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4deafd0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-13 10:22:03.794030: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-13 10:22:04.107381: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x47b4490 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-13 10:22:04.107531: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-13 10:22:04.115766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-13 10:22:04.115911: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-13 10:22:04.115935: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-13 10:22:04.115953: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-13 10:22:04.115970: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-13 10:22:04.115987: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-13 10:22:04.116004: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-13 10:22:04.116021: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-13 10:22:04.124079: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-13 10:22:04.124349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-13 10:22:04.132521: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-13 10:22:04.132625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-13 10:22:04.132644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-13 10:22:04.142467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-13 10:22:08.792402: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.4400 - val_loss: 0.0013
Epoch 2/512
512/512 - 0s - loss: 0.0696 - val_loss: 1.7130e-04
Epoch 3/512
512/512 - 0s - loss: 0.0104 - val_loss: 4.6900e-05
Epoch 4/512
512/512 - 0s - loss: 0.0039 - val_loss: 3.0215e-05
Epoch 5/512
512/512 - 0s - loss: 0.0027 - val_loss: 2.0948e-05
Epoch 6/512
512/512 - 0s - loss: 0.0018 - val_loss: 1.3399e-05
Epoch 7/512
512/512 - 0s - loss: 0.0011 - val_loss: 7.7178e-06
Epoch 8/512
512/512 - 0s - loss: 6.1480e-04 - val_loss: 3.9008e-06
Epoch 9/512
512/512 - 0s - loss: 2.9574e-04 - val_loss: 1.6732e-06
Epoch 10/512
512/512 - 0s - loss: 1.1947e-04 - val_loss: 5.8251e-07
Epoch 11/512
512/512 - 0s - loss: 3.8693e-05 - val_loss: 1.5534e-07
Epoch 12/512
512/512 - 0s - loss: 9.4725e-06 - val_loss: 2.9695e-08
Epoch 13/512
512/512 - 0s - loss: 1.8338e-06 - val_loss: 2.6799e-08
Epoch 14/512
512/512 - 0s - loss: 1.0172e-04 - val_loss: 1.9968e-05
Epoch 15/512
512/512 - 0s - loss: 0.0045 - val_loss: 1.0642e-05
Epoch 16/512
512/512 - 0s - loss: 4.8414e-04 - val_loss: 1.1351e-06
Epoch 17/512
512/512 - 0s - loss: 1.2681e-04 - val_loss: 2.5970e-06
Epoch 18/512
512/512 - 0s - loss: 8.7890e-04 - val_loss: 2.7319e-05
Epoch 19/512
512/512 - 0s - loss: 0.0025 - val_loss: 9.7693e-06
Epoch 20/512
512/512 - 0s - loss: 6.5889e-04 - val_loss: 4.3024e-06
Epoch 21/512
512/512 - 0s - loss: 5.9296e-04 - val_loss: 1.1129e-05
Epoch 22/512
512/512 - 0s - loss: 0.0016 - val_loss: 1.6652e-05
Epoch 23/512
512/512 - 0s - loss: 0.0013 - val_loss: 7.3353e-06
Epoch 24/512
512/512 - 0s - loss: 7.1882e-04 - val_loss: 8.0684e-06
Epoch 25/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.3939e-05
Epoch 26/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.0191e-05
Epoch 27/512
512/512 - 0s - loss: 9.2067e-04 - val_loss: 8.0960e-06
Epoch 28/512
512/512 - 0s - loss: 9.1995e-04 - val_loss: 1.0923e-05
Epoch 29/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.0937e-05
Epoch 30/512
512/512 - 0s - loss: 0.0010 - val_loss: 8.7706e-06
Epoch 31/512
512/512 - 0s - loss: 9.0759e-04 - val_loss: 9.5441e-06
Epoch 32/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0471e-05
Epoch 33/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.1874e-06
Epoch 34/512
512/512 - 0s - loss: 9.1959e-04 - val_loss: 8.8819e-06
Epoch 35/512
512/512 - 0s - loss: 9.4850e-04 - val_loss: 9.6151e-06
Epoch 36/512
512/512 - 0s - loss: 9.9057e-04 - val_loss: 9.2233e-06
Epoch 37/512
512/512 - 0s - loss: 9.2540e-04 - val_loss: 8.6880e-06
Epoch 38/512
512/512 - 0s - loss: 8.9508e-04 - val_loss: 9.1746e-06
Epoch 39/512
512/512 - 0s - loss: 9.5786e-04 - val_loss: 8.8728e-06
Epoch 40/512
512/512 - 0s - loss: 8.8852e-04 - val_loss: 8.4912e-06
Epoch 41/512
512/512 - 0s - loss: 8.8165e-04 - val_loss: 8.6836e-06
Epoch 42/512
512/512 - 0s - loss: 8.9309e-04 - val_loss: 8.7492e-06
Epoch 43/512
512/512 - 0s - loss: 8.8430e-04 - val_loss: 8.3883e-06
Epoch 44/512
512/512 - 0s - loss: 8.4993e-04 - val_loss: 8.2679e-06
Epoch 45/512
512/512 - 0s - loss: 8.5010e-04 - val_loss: 8.3304e-06
Epoch 46/512
512/512 - 0s - loss: 8.5667e-04 - val_loss: 8.1530e-06
Epoch 47/512
512/512 - 0s - loss: 8.2504e-04 - val_loss: 8.0072e-06
Epoch 48/512
512/512 - 0s - loss: 8.2185e-04 - val_loss: 8.0824e-06
Epoch 49/512
512/512 - 0s - loss: 8.2608e-04 - val_loss: 7.8890e-06
Epoch 50/512
512/512 - 0s - loss: 7.9482e-04 - val_loss: 7.7617e-06
Epoch 51/512
512/512 - 0s - loss: 7.9261e-04 - val_loss: 7.9421e-06
Epoch 52/512
512/512 - 0s - loss: 8.0341e-04 - val_loss: 7.7116e-06
Epoch 53/512
512/512 - 0s - loss: 7.7428e-04 - val_loss: 7.4879e-06
Epoch 54/512
512/512 - 0s - loss: 7.6331e-04 - val_loss: 7.5159e-06
Epoch 55/512
512/512 - 0s - loss: 7.5996e-04 - val_loss: 7.6393e-06
Epoch 56/512
512/512 - 0s - loss: 7.6590e-04 - val_loss: 7.4578e-06
Epoch 57/512
512/512 - 0s - loss: 7.4579e-04 - val_loss: 7.1442e-06
Epoch 58/512
512/512 - 0s - loss: 7.2451e-04 - val_loss: 7.2347e-06
Epoch 59/512
512/512 - 0s - loss: 7.3930e-04 - val_loss: 7.1969e-06
Epoch 60/512
512/512 - 0s - loss: 7.2009e-04 - val_loss: 7.0480e-06
Epoch 61/512
512/512 - 0s - loss: 7.1141e-04 - val_loss: 7.0126e-06
Epoch 62/512
512/512 - 0s - loss: 7.0790e-04 - val_loss: 6.8895e-06
Epoch 63/512
512/512 - 0s - loss: 6.9786e-04 - val_loss: 6.8202e-06
Epoch 64/512
512/512 - 0s - loss: 6.9098e-04 - val_loss: 6.7322e-06
Epoch 65/512
512/512 - 0s - loss: 6.8075e-04 - val_loss: 6.7053e-06
Epoch 66/512
512/512 - 0s - loss: 6.7479e-04 - val_loss: 6.7033e-06
Epoch 67/512
512/512 - 0s - loss: 6.7410e-04 - val_loss: 6.5634e-06
Epoch 68/512
512/512 - 0s - loss: 6.5997e-04 - val_loss: 6.4270e-06
Epoch 69/512
512/512 - 0s - loss: 6.5077e-04 - val_loss: 6.4225e-06
Epoch 70/512
512/512 - 0s - loss: 6.4852e-04 - val_loss: 6.4153e-06
Epoch 71/512
512/512 - 0s - loss: 6.4631e-04 - val_loss: 6.2006e-06
Epoch 72/512
512/512 - 0s - loss: 6.2413e-04 - val_loss: 6.1831e-06
Epoch 73/512
512/512 - 0s - loss: 6.2333e-04 - val_loss: 6.2953e-06
Epoch 74/512
512/512 - 0s - loss: 6.3701e-04 - val_loss: 5.9687e-06
Epoch 75/512
512/512 - 0s - loss: 5.9796e-04 - val_loss: 5.8997e-06
Epoch 76/512
512/512 - 0s - loss: 6.0141e-04 - val_loss: 6.0895e-06
Epoch 77/512
512/512 - 0s - loss: 6.1144e-04 - val_loss: 5.9532e-06
Epoch 78/512
512/512 - 0s - loss: 5.9496e-04 - val_loss: 5.6423e-06
Epoch 79/512
512/512 - 0s - loss: 5.7248e-04 - val_loss: 5.6928e-06
Epoch 80/512
512/512 - 0s - loss: 5.7882e-04 - val_loss: 5.9316e-06
Epoch 81/512
512/512 - 0s - loss: 5.8861e-04 - val_loss: 5.6805e-06
Epoch 82/512
512/512 - 0s - loss: 5.5988e-04 - val_loss: 5.4518e-06
Epoch 83/512
512/512 - 0s - loss: 5.5361e-04 - val_loss: 5.5310e-06
Epoch 84/512
512/512 - 0s - loss: 5.5640e-04 - val_loss: 5.5835e-06
Epoch 85/512
512/512 - 0s - loss: 5.5654e-04 - val_loss: 5.3625e-06
Epoch 86/512
512/512 - 0s - loss: 5.3383e-04 - val_loss: 5.2563e-06
Epoch 87/512
512/512 - 0s - loss: 5.3271e-04 - val_loss: 5.3445e-06
Epoch 88/512
512/512 - 0s - loss: 5.3579e-04 - val_loss: 5.2408e-06
Epoch 89/512
512/512 - 0s - loss: 5.1758e-04 - val_loss: 5.1784e-06
Epoch 90/512
512/512 - 0s - loss: 5.1649e-04 - val_loss: 5.1839e-06
Epoch 91/512
512/512 - 0s - loss: 5.1514e-04 - val_loss: 5.0746e-06
Epoch 92/512
512/512 - 0s - loss: 5.0425e-04 - val_loss: 4.9216e-06
Epoch 93/512
512/512 - 0s - loss: 4.9692e-04 - val_loss: 4.8747e-06
Epoch 94/512
512/512 - 0s - loss: 4.8834e-04 - val_loss: 4.8994e-06
Epoch 95/512
512/512 - 0s - loss: 4.9283e-04 - val_loss: 4.8087e-06
Epoch 96/512
512/512 - 0s - loss: 4.8068e-04 - val_loss: 4.6440e-06
Epoch 97/512
512/512 - 0s - loss: 4.6323e-04 - val_loss: 4.7743e-06
Epoch 98/512
512/512 - 0s - loss: 4.7968e-04 - val_loss: 4.7321e-06
Epoch 99/512
512/512 - 0s - loss: 4.6520e-04 - val_loss: 4.4697e-06
Epoch 100/512
512/512 - 0s - loss: 4.4764e-04 - val_loss: 4.4651e-06
Epoch 101/512
512/512 - 0s - loss: 4.5289e-04 - val_loss: 4.4899e-06
Epoch 102/512
512/512 - 0s - loss: 4.4584e-04 - val_loss: 4.4310e-06
Epoch 103/512
512/512 - 0s - loss: 4.3632e-04 - val_loss: 4.4234e-06
Epoch 104/512
512/512 - 0s - loss: 4.3749e-04 - val_loss: 4.3363e-06
Epoch 105/512
512/512 - 0s - loss: 4.3024e-04 - val_loss: 4.1890e-06
Epoch 106/512
512/512 - 0s - loss: 4.1433e-04 - val_loss: 4.2108e-06
Epoch 107/512
512/512 - 0s - loss: 4.2008e-04 - val_loss: 4.1981e-06
Epoch 108/512
512/512 - 0s - loss: 4.1791e-04 - val_loss: 3.9486e-06
Epoch 109/512
512/512 - 0s - loss: 3.9024e-04 - val_loss: 4.0201e-06
Epoch 110/512
512/512 - 0s - loss: 4.0594e-04 - val_loss: 4.0915e-06
Epoch 111/512
512/512 - 0s - loss: 4.0217e-04 - val_loss: 3.8173e-06
Epoch 112/512
512/512 - 0s - loss: 3.8060e-04 - val_loss: 3.6819e-06
Epoch 113/512
512/512 - 0s - loss: 3.7641e-04 - val_loss: 3.8066e-06
Epoch 114/512
512/512 - 0s - loss: 3.8568e-04 - val_loss: 3.7518e-06
Epoch 115/512
512/512 - 0s - loss: 3.6921e-04 - val_loss: 3.6031e-06
Epoch 116/512
512/512 - 0s - loss: 3.6044e-04 - val_loss: 3.6391e-06
Epoch 117/512
512/512 - 0s - loss: 3.6322e-04 - val_loss: 3.6298e-06
Epoch 118/512
512/512 - 0s - loss: 3.5819e-04 - val_loss: 3.5131e-06
Epoch 119/512
512/512 - 0s - loss: 3.4559e-04 - val_loss: 3.4415e-06
Epoch 120/512
512/512 - 0s - loss: 3.4389e-04 - val_loss: 3.4295e-06
Epoch 121/512
512/512 - 0s - loss: 3.4043e-04 - val_loss: 3.3551e-06
Epoch 122/512
512/512 - 0s - loss: 3.3433e-04 - val_loss: 3.2564e-06
Epoch 123/512
512/512 - 0s - loss: 3.2321e-04 - val_loss: 3.2534e-06
Epoch 124/512
512/512 - 0s - loss: 3.2437e-04 - val_loss: 3.2407e-06
Epoch 125/512
512/512 - 0s - loss: 3.2117e-04 - val_loss: 3.0918e-06
Epoch 126/512
512/512 - 0s - loss: 3.0662e-04 - val_loss: 3.0580e-06
Epoch 127/512
512/512 - 0s - loss: 3.0492e-04 - val_loss: 3.1310e-06
Epoch 128/512
512/512 - 0s - loss: 3.0988e-04 - val_loss: 3.0076e-06
Epoch 129/512
512/512 - 0s - loss: 2.9295e-04 - val_loss: 2.8693e-06
Epoch 130/512
512/512 - 0s - loss: 2.8736e-04 - val_loss: 2.9121e-06
Epoch 131/512
512/512 - 0s - loss: 2.9051e-04 - val_loss: 2.8595e-06
Epoch 132/512
512/512 - 0s - loss: 2.8003e-04 - val_loss: 2.7744e-06
Epoch 133/512
512/512 - 0s - loss: 2.7572e-04 - val_loss: 2.7515e-06
Epoch 134/512
512/512 - 0s - loss: 2.7157e-04 - val_loss: 2.7105e-06
Epoch 135/512
512/512 - 0s - loss: 2.6702e-04 - val_loss: 2.6491e-06
Epoch 136/512
512/512 - 0s - loss: 2.6123e-04 - val_loss: 2.5887e-06
Epoch 137/512
512/512 - 0s - loss: 2.5685e-04 - val_loss: 2.5233e-06
Epoch 138/512
512/512 - 0s - loss: 2.5038e-04 - val_loss: 2.5014e-06
Epoch 139/512
512/512 - 0s - loss: 2.4783e-04 - val_loss: 2.4785e-06
Epoch 140/512
512/512 - 0s - loss: 2.4221e-04 - val_loss: 2.4272e-06
Epoch 141/512
512/512 - 0s - loss: 2.3850e-04 - val_loss: 2.3584e-06
Epoch 142/512
512/512 - 0s - loss: 2.3334e-04 - val_loss: 2.2766e-06
Epoch 143/512
512/512 - 0s - loss: 2.2568e-04 - val_loss: 2.2726e-06
Epoch 144/512
512/512 - 0s - loss: 2.2585e-04 - val_loss: 2.2523e-06
Epoch 145/512
512/512 - 0s - loss: 2.1999e-04 - val_loss: 2.1712e-06
Epoch 146/512
512/512 - 0s - loss: 2.1407e-04 - val_loss: 2.1104e-06
Epoch 147/512
512/512 - 0s - loss: 2.0878e-04 - val_loss: 2.1118e-06
Epoch 148/512
512/512 - 0s - loss: 2.0758e-04 - val_loss: 2.0985e-06
Epoch 149/512
512/512 - 0s - loss: 2.0510e-04 - val_loss: 1.9859e-06
Epoch 150/512
512/512 - 0s - loss: 1.9343e-04 - val_loss: 1.9453e-06
Epoch 151/512
512/512 - 0s - loss: 1.9452e-04 - val_loss: 1.9396e-06
Epoch 152/512
512/512 - 0s - loss: 1.9055e-04 - val_loss: 1.8755e-06
Epoch 153/512
512/512 - 0s - loss: 1.8391e-04 - val_loss: 1.8273e-06
Epoch 154/512
512/512 - 0s - loss: 1.8079e-04 - val_loss: 1.8051e-06
Epoch 155/512
512/512 - 0s - loss: 1.7721e-04 - val_loss: 1.7891e-06
Epoch 156/512
512/512 - 0s - loss: 1.7540e-04 - val_loss: 1.7085e-06
Epoch 157/512
512/512 - 0s - loss: 1.6718e-04 - val_loss: 1.6496e-06
Epoch 158/512
512/512 - 0s - loss: 1.6390e-04 - val_loss: 1.6661e-06
Epoch 159/512
512/512 - 0s - loss: 1.6401e-04 - val_loss: 1.6386e-06
Epoch 160/512
512/512 - 0s - loss: 1.5959e-04 - val_loss: 1.5290e-06
Epoch 161/512
512/512 - 0s - loss: 1.5028e-04 - val_loss: 1.5161e-06
Epoch 162/512
512/512 - 0s - loss: 1.5211e-04 - val_loss: 1.5160e-06
Epoch 163/512
512/512 - 0s - loss: 1.4801e-04 - val_loss: 1.4605e-06
Epoch 164/512
512/512 - 0s - loss: 1.4159e-04 - val_loss: 1.4374e-06
Epoch 165/512
512/512 - 0s - loss: 1.4160e-04 - val_loss: 1.3970e-06
Epoch 166/512
512/512 - 0s - loss: 1.3690e-04 - val_loss: 1.3404e-06
Epoch 167/512
512/512 - 0s - loss: 1.3164e-04 - val_loss: 1.3268e-06
Epoch 168/512
512/512 - 0s - loss: 1.3018e-04 - val_loss: 1.3277e-06
Epoch 169/512
512/512 - 0s - loss: 1.2737e-04 - val_loss: 1.2899e-06
Epoch 170/512
512/512 - 0s - loss: 1.2490e-04 - val_loss: 1.2204e-06
Epoch 171/512
512/512 - 0s - loss: 1.1871e-04 - val_loss: 1.1882e-06
Epoch 172/512
512/512 - 0s - loss: 1.1745e-04 - val_loss: 1.1770e-06
Epoch 173/512
512/512 - 0s - loss: 1.1515e-04 - val_loss: 1.1403e-06
Epoch 174/512
512/512 - 0s - loss: 1.1119e-04 - val_loss: 1.1022e-06
Epoch 175/512
512/512 - 0s - loss: 1.0831e-04 - val_loss: 1.0747e-06
Epoch 176/512
512/512 - 0s - loss: 1.0573e-04 - val_loss: 1.0474e-06
Epoch 177/512
512/512 - 0s - loss: 1.0173e-04 - val_loss: 1.0361e-06
Epoch 178/512
512/512 - 0s - loss: 1.0207e-04 - val_loss: 9.9228e-07
Epoch 179/512
512/512 - 0s - loss: 9.6012e-05 - val_loss: 9.5246e-07
Epoch 180/512
512/512 - 0s - loss: 9.3944e-05 - val_loss: 9.5830e-07
Epoch 181/512
512/512 - 0s - loss: 9.3131e-05 - val_loss: 9.3821e-07
Epoch 182/512
512/512 - 0s - loss: 8.9705e-05 - val_loss: 9.0150e-07
Epoch 183/512
512/512 - 0s - loss: 8.7074e-05 - val_loss: 8.6932e-07
Epoch 184/512
512/512 - 0s - loss: 8.4672e-05 - val_loss: 8.4398e-07
Epoch 185/512
512/512 - 0s - loss: 8.2173e-05 - val_loss: 8.2331e-07
Epoch 186/512
512/512 - 0s - loss: 7.9778e-05 - val_loss: 8.0541e-07
Epoch 187/512
512/512 - 0s - loss: 7.8454e-05 - val_loss: 7.7534e-07
Epoch 188/512
512/512 - 0s - loss: 7.4894e-05 - val_loss: 7.4735e-07
Epoch 189/512
512/512 - 0s - loss: 7.2962e-05 - val_loss: 7.3812e-07
Epoch 190/512
512/512 - 0s - loss: 7.1938e-05 - val_loss: 7.1185e-07
Epoch 191/512
512/512 - 0s - loss: 6.8883e-05 - val_loss: 6.8463e-07
Epoch 192/512
512/512 - 0s - loss: 6.6158e-05 - val_loss: 6.8070e-07
Epoch 193/512
512/512 - 0s - loss: 6.5955e-05 - val_loss: 6.6271e-07
Epoch 194/512
512/512 - 0s - loss: 6.3604e-05 - val_loss: 6.2084e-07
Epoch 195/512
512/512 - 0s - loss: 6.0359e-05 - val_loss: 6.0222e-07
Epoch 196/512
512/512 - 0s - loss: 5.9204e-05 - val_loss: 6.0272e-07
Epoch 197/512
512/512 - 0s - loss: 5.7924e-05 - val_loss: 5.8578e-07
Epoch 198/512
512/512 - 0s - loss: 5.6388e-05 - val_loss: 5.5223e-07
Epoch 199/512
512/512 - 0s - loss: 5.3772e-05 - val_loss: 5.2765e-07
Epoch 200/512
512/512 - 0s - loss: 5.2031e-05 - val_loss: 5.1796e-07
Epoch 201/512
512/512 - 0s - loss: 5.0455e-05 - val_loss: 5.1833e-07
Epoch 202/512
512/512 - 0s - loss: 5.0092e-05 - val_loss: 4.9708e-07
Epoch 203/512
512/512 - 0s - loss: 4.7638e-05 - val_loss: 4.6677e-07
Epoch 204/512
512/512 - 0s - loss: 4.5286e-05 - val_loss: 4.6426e-07
Epoch 205/512
512/512 - 0s - loss: 4.5459e-05 - val_loss: 4.5176e-07
Epoch 206/512
512/512 - 0s - loss: 4.3164e-05 - val_loss: 4.2632e-07
Epoch 207/512
512/512 - 0s - loss: 4.1257e-05 - val_loss: 4.2187e-07
Epoch 208/512
512/512 - 0s - loss: 4.1040e-05 - val_loss: 4.1009e-07
Epoch 209/512
512/512 - 0s - loss: 3.8949e-05 - val_loss: 3.9591e-07
Epoch 210/512
512/512 - 0s - loss: 3.8217e-05 - val_loss: 3.7857e-07
Epoch 211/512
512/512 - 0s - loss: 3.6476e-05 - val_loss: 3.6336e-07
Epoch 212/512
512/512 - 0s - loss: 3.5121e-05 - val_loss: 3.5976e-07
Epoch 213/512
512/512 - 0s - loss: 3.4803e-05 - val_loss: 3.4728e-07
Epoch 214/512
512/512 - 0s - loss: 3.3258e-05 - val_loss: 3.2585e-07
Epoch 215/512
512/512 - 0s - loss: 3.1530e-05 - val_loss: 3.1780e-07
Epoch 216/512
512/512 - 0s - loss: 3.0857e-05 - val_loss: 3.1832e-07
Epoch 217/512
512/512 - 0s - loss: 3.0455e-05 - val_loss: 3.0230e-07
Epoch 218/512
512/512 - 0s - loss: 2.8577e-05 - val_loss: 2.8562e-07
Epoch 219/512
512/512 - 0s - loss: 2.7788e-05 - val_loss: 2.7850e-07
Epoch 220/512
512/512 - 0s - loss: 2.6717e-05 - val_loss: 2.7776e-07
Epoch 221/512
512/512 - 0s - loss: 2.6436e-05 - val_loss: 2.6628e-07
Epoch 222/512
512/512 - 0s - loss: 2.5068e-05 - val_loss: 2.4987e-07
Epoch 223/512
512/512 - 0s - loss: 2.3895e-05 - val_loss: 2.4503e-07
Epoch 224/512
512/512 - 0s - loss: 2.3660e-05 - val_loss: 2.3752e-07
Epoch 225/512
512/512 - 0s - loss: 2.2719e-05 - val_loss: 2.2164e-07
Epoch 226/512
512/512 - 0s - loss: 2.1280e-05 - val_loss: 2.1821e-07
Epoch 227/512
512/512 - 0s - loss: 2.1200e-05 - val_loss: 2.1309e-07
Epoch 228/512
512/512 - 0s - loss: 2.0383e-05 - val_loss: 2.0076e-07
Epoch 229/512
512/512 - 0s - loss: 1.9220e-05 - val_loss: 1.9723e-07
Epoch 230/512
512/512 - 0s - loss: 1.8869e-05 - val_loss: 1.9404e-07
Epoch 231/512
512/512 - 0s - loss: 1.8492e-05 - val_loss: 1.8085e-07
Epoch 232/512
512/512 - 0s - loss: 1.7151e-05 - val_loss: 1.7233e-07
Epoch 233/512
512/512 - 0s - loss: 1.6820e-05 - val_loss: 1.7043e-07
Epoch 234/512
512/512 - 0s - loss: 1.6289e-05 - val_loss: 1.6529e-07
Epoch 235/512
512/512 - 0s - loss: 1.5674e-05 - val_loss: 1.5732e-07
Epoch 236/512
512/512 - 0s - loss: 1.5041e-05 - val_loss: 1.4990e-07
Epoch 237/512
512/512 - 0s - loss: 1.4389e-05 - val_loss: 1.4688e-07
Epoch 238/512
512/512 - 0s - loss: 1.4057e-05 - val_loss: 1.4218e-07
Epoch 239/512
512/512 - 0s - loss: 1.3499e-05 - val_loss: 1.3450e-07
Epoch 240/512
512/512 - 0s - loss: 1.2809e-05 - val_loss: 1.2984e-07
Epoch 241/512
512/512 - 0s - loss: 1.2442e-05 - val_loss: 1.2720e-07
Epoch 242/512
512/512 - 0s - loss: 1.2065e-05 - val_loss: 1.2137e-07
Epoch 243/512
512/512 - 0s - loss: 1.1516e-05 - val_loss: 1.1505e-07
Epoch 244/512
512/512 - 0s - loss: 1.0997e-05 - val_loss: 1.1204e-07
Epoch 245/512
512/512 - 0s - loss: 1.0702e-05 - val_loss: 1.0891e-07
Epoch 246/512
512/512 - 0s - loss: 1.0325e-05 - val_loss: 1.0347e-07
Epoch 247/512
512/512 - 0s - loss: 9.7720e-06 - val_loss: 9.9728e-08
Epoch 248/512
512/512 - 0s - loss: 9.4284e-06 - val_loss: 9.7928e-08
Epoch 249/512
512/512 - 0s - loss: 9.3057e-06 - val_loss: 9.2377e-08
Epoch 250/512
512/512 - 0s - loss: 8.6419e-06 - val_loss: 8.7614e-08
Epoch 251/512
512/512 - 0s - loss: 8.4432e-06 - val_loss: 8.4622e-08
Epoch 252/512
512/512 - 0s - loss: 8.0366e-06 - val_loss: 8.2215e-08
Epoch 253/512
512/512 - 0s - loss: 7.8822e-06 - val_loss: 7.8325e-08
Epoch 254/512
512/512 - 0s - loss: 7.3264e-06 - val_loss: 7.5730e-08
Epoch 255/512
512/512 - 0s - loss: 7.2424e-06 - val_loss: 7.4115e-08
Epoch 256/512
512/512 - 0s - loss: 6.9747e-06 - val_loss: 6.8732e-08
Epoch 257/512
512/512 - 0s - loss: 6.5255e-06 - val_loss: 6.5330e-08
Epoch 258/512
512/512 - 0s - loss: 6.2274e-06 - val_loss: 6.6220e-08
Epoch 259/512
512/512 - 0s - loss: 6.3023e-06 - val_loss: 6.3508e-08
Epoch 260/512
512/512 - 0s - loss: 5.8147e-06 - val_loss: 5.8316e-08
Epoch 261/512
512/512 - 0s - loss: 5.5326e-06 - val_loss: 5.6868e-08
Epoch 262/512
512/512 - 0s - loss: 5.4621e-06 - val_loss: 5.5193e-08
Epoch 263/512
512/512 - 0s - loss: 5.1730e-06 - val_loss: 5.2050e-08
Epoch 264/512
512/512 - 0s - loss: 4.9818e-06 - val_loss: 4.8653e-08
Epoch 265/512
512/512 - 0s - loss: 4.6317e-06 - val_loss: 4.8524e-08
Epoch 266/512
512/512 - 0s - loss: 4.6179e-06 - val_loss: 4.8639e-08
Epoch 267/512
512/512 - 0s - loss: 4.5315e-06 - val_loss: 4.3519e-08
Epoch 268/512
512/512 - 0s - loss: 4.0417e-06 - val_loss: 4.0813e-08
Epoch 269/512
512/512 - 0s - loss: 3.9717e-06 - val_loss: 4.2450e-08
Epoch 270/512
512/512 - 0s - loss: 3.9937e-06 - val_loss: 4.0201e-08
Epoch 271/512
512/512 - 0s - loss: 3.6822e-06 - val_loss: 3.6616e-08
Epoch 272/512
512/512 - 0s - loss: 3.4760e-06 - val_loss: 3.5922e-08
Epoch 273/512
512/512 - 0s - loss: 3.4219e-06 - val_loss: 3.5211e-08
Epoch 274/512
512/512 - 0s - loss: 3.2981e-06 - val_loss: 3.3015e-08
Epoch 275/512
512/512 - 0s - loss: 3.0976e-06 - val_loss: 3.0917e-08
Epoch 276/512
512/512 - 0s - loss: 2.9204e-06 - val_loss: 3.0977e-08
Epoch 277/512
512/512 - 0s - loss: 2.9314e-06 - val_loss: 2.9882e-08
Epoch 278/512
512/512 - 0s - loss: 2.7449e-06 - val_loss: 2.7768e-08
Epoch 279/512
512/512 - 0s - loss: 2.6131e-06 - val_loss: 2.6387e-08
Epoch 280/512
512/512 - 0s - loss: 2.4891e-06 - val_loss: 2.5705e-08
Epoch 281/512
512/512 - 0s - loss: 2.4147e-06 - val_loss: 2.5089e-08
Epoch 282/512
512/512 - 0s - loss: 2.3463e-06 - val_loss: 2.3086e-08
Epoch 283/512
512/512 - 0s - loss: 2.1569e-06 - val_loss: 2.2133e-08
Epoch 284/512
512/512 - 0s - loss: 2.1030e-06 - val_loss: 2.2013e-08
Epoch 285/512
512/512 - 0s - loss: 2.0449e-06 - val_loss: 2.0990e-08
Epoch 286/512
512/512 - 0s - loss: 1.9381e-06 - val_loss: 1.9701e-08
Epoch 287/512
512/512 - 0s - loss: 1.8244e-06 - val_loss: 1.8924e-08
Epoch 288/512
512/512 - 0s - loss: 1.7861e-06 - val_loss: 1.8009e-08
Epoch 289/512
512/512 - 0s - loss: 1.6926e-06 - val_loss: 1.6981e-08
Epoch 290/512
512/512 - 0s - loss: 1.6065e-06 - val_loss: 1.6308e-08
Epoch 291/512
512/512 - 0s - loss: 1.5441e-06 - val_loss: 1.5902e-08
Epoch 292/512
512/512 - 0s - loss: 1.4917e-06 - val_loss: 1.5210e-08
Epoch 293/512
512/512 - 0s - loss: 1.4193e-06 - val_loss: 1.4145e-08
Epoch 294/512
512/512 - 0s - loss: 1.3371e-06 - val_loss: 1.3664e-08
Epoch 295/512
512/512 - 0s - loss: 1.2956e-06 - val_loss: 1.3244e-08
Epoch 296/512
512/512 - 0s - loss: 1.2412e-06 - val_loss: 1.2664e-08
Epoch 297/512
512/512 - 0s - loss: 1.1819e-06 - val_loss: 1.1975e-08
Epoch 298/512
512/512 - 0s - loss: 1.1184e-06 - val_loss: 1.1597e-08
Epoch 299/512
512/512 - 0s - loss: 1.0785e-06 - val_loss: 1.1264e-08
Epoch 300/512
512/512 - 0s - loss: 1.0405e-06 - val_loss: 1.0688e-08
Epoch 301/512
512/512 - 0s - loss: 9.8345e-07 - val_loss: 1.0041e-08
Epoch 302/512
512/512 - 0s - loss: 9.3541e-07 - val_loss: 9.6649e-09
Epoch 303/512
512/512 - 0s - loss: 8.9772e-07 - val_loss: 9.2915e-09
Epoch 304/512
512/512 - 0s - loss: 8.6431e-07 - val_loss: 8.8525e-09
Epoch 305/512
512/512 - 0s - loss: 8.1470e-07 - val_loss: 8.3965e-09
Epoch 306/512
512/512 - 0s - loss: 7.7949e-07 - val_loss: 8.1881e-09
Epoch 307/512
512/512 - 0s - loss: 7.6037e-07 - val_loss: 7.6540e-09
Epoch 308/512
512/512 - 0s - loss: 7.0935e-07 - val_loss: 7.1045e-09
Epoch 309/512
512/512 - 0s - loss: 6.6414e-07 - val_loss: 7.1022e-09
Epoch 310/512
512/512 - 0s - loss: 6.6734e-07 - val_loss: 6.8155e-09
Epoch 311/512
512/512 - 0s - loss: 6.2903e-07 - val_loss: 6.0561e-09
Epoch 312/512
512/512 - 0s - loss: 5.6864e-07 - val_loss: 5.9587e-09
Epoch 313/512
512/512 - 0s - loss: 5.6897e-07 - val_loss: 6.0274e-09
Epoch 314/512
512/512 - 0s - loss: 5.6196e-07 - val_loss: 5.4010e-09
Epoch 315/512
512/512 - 0s - loss: 4.9489e-07 - val_loss: 5.0721e-09
Epoch 316/512
512/512 - 0s - loss: 4.8305e-07 - val_loss: 5.2243e-09
Epoch 317/512
512/512 - 0s - loss: 4.8909e-07 - val_loss: 4.9205e-09
Epoch 318/512
512/512 - 0s - loss: 4.4729e-07 - val_loss: 4.3349e-09
Epoch 319/512
512/512 - 0s - loss: 4.0501e-07 - val_loss: 4.3733e-09
Epoch 320/512
512/512 - 0s - loss: 4.1739e-07 - val_loss: 4.3982e-09
Epoch 321/512
512/512 - 0s - loss: 3.9724e-07 - val_loss: 3.9408e-09
Epoch 322/512
512/512 - 0s - loss: 3.5722e-07 - val_loss: 3.7460e-09
Epoch 323/512
512/512 - 0s - loss: 3.5327e-07 - val_loss: 3.7322e-09
Epoch 324/512
512/512 - 0s - loss: 3.4507e-07 - val_loss: 3.4707e-09
Epoch 325/512
512/512 - 0s - loss: 3.1752e-07 - val_loss: 3.1847e-09
Epoch 326/512
512/512 - 0s - loss: 3.0109e-07 - val_loss: 3.1329e-09
Epoch 327/512
512/512 - 0s - loss: 2.9193e-07 - val_loss: 3.0833e-09
Epoch 328/512
512/512 - 0s - loss: 2.8287e-07 - val_loss: 2.8746e-09
Epoch 329/512
512/512 - 0s - loss: 2.6382e-07 - val_loss: 2.6569e-09
Epoch 330/512
512/512 - 0s - loss: 2.4914e-07 - val_loss: 2.5916e-09
Epoch 331/512
512/512 - 0s - loss: 2.4187e-07 - val_loss: 2.5135e-09
Epoch 332/512
512/512 - 0s - loss: 2.3104e-07 - val_loss: 2.3617e-09
Epoch 333/512
512/512 - 0s - loss: 2.1766e-07 - val_loss: 2.2222e-09
Epoch 334/512
512/512 - 0s - loss: 2.0709e-07 - val_loss: 2.1405e-09
Epoch 335/512
512/512 - 0s - loss: 2.0041e-07 - val_loss: 2.0263e-09
Epoch 336/512
512/512 - 0s - loss: 1.8675e-07 - val_loss: 1.9520e-09
Epoch 337/512
512/512 - 0s - loss: 1.8227e-07 - val_loss: 1.8603e-09
Epoch 338/512
512/512 - 0s - loss: 1.6989e-07 - val_loss: 1.7784e-09
Epoch 339/512
512/512 - 0s - loss: 1.6472e-07 - val_loss: 1.7021e-09
Epoch 340/512
512/512 - 0s - loss: 1.5738e-07 - val_loss: 1.5812e-09
Epoch 341/512
512/512 - 0s - loss: 1.4604e-07 - val_loss: 1.5124e-09
Epoch 342/512
512/512 - 0s - loss: 1.4107e-07 - val_loss: 1.4857e-09
Epoch 343/512
512/512 - 0s - loss: 1.3689e-07 - val_loss: 1.4041e-09
Epoch 344/512
512/512 - 0s - loss: 1.2737e-07 - val_loss: 1.3241e-09
Epoch 345/512
512/512 - 0s - loss: 1.2257e-07 - val_loss: 1.2713e-09
Epoch 346/512
512/512 - 0s - loss: 1.1703e-07 - val_loss: 1.1928e-09
Epoch 347/512
512/512 - 0s - loss: 1.0961e-07 - val_loss: 1.1409e-09
Epoch 348/512
512/512 - 0s - loss: 1.0647e-07 - val_loss: 1.1003e-09
Epoch 349/512
512/512 - 0s - loss: 1.0152e-07 - val_loss: 1.0285e-09
Epoch 350/512
512/512 - 0s - loss: 9.4192e-08 - val_loss: 9.8714e-10
Epoch 351/512
512/512 - 0s - loss: 9.1408e-08 - val_loss: 9.6994e-10
Epoch 352/512
512/512 - 0s - loss: 8.9083e-08 - val_loss: 9.0011e-10
Epoch 353/512
512/512 - 0s - loss: 8.1676e-08 - val_loss: 8.4394e-10
Epoch 354/512
512/512 - 0s - loss: 7.8145e-08 - val_loss: 8.3124e-10
Epoch 355/512
512/512 - 0s - loss: 7.6466e-08 - val_loss: 7.8542e-10
Epoch 356/512
512/512 - 0s - loss: 7.1934e-08 - val_loss: 7.2936e-10
Epoch 357/512
512/512 - 0s - loss: 6.7071e-08 - val_loss: 7.0530e-10
Epoch 358/512
512/512 - 0s - loss: 6.5389e-08 - val_loss: 6.8757e-10
Epoch 359/512
512/512 - 0s - loss: 6.2854e-08 - val_loss: 6.4450e-10
Epoch 360/512
512/512 - 0s - loss: 5.8686e-08 - val_loss: 6.0029e-10
Epoch 361/512
512/512 - 0s - loss: 5.5756e-08 - val_loss: 5.8181e-10
Epoch 362/512
512/512 - 0s - loss: 5.3799e-08 - val_loss: 5.6149e-10
Epoch 363/512
512/512 - 0s - loss: 5.1144e-08 - val_loss: 5.3786e-10
Epoch 364/512
512/512 - 0s - loss: 4.9201e-08 - val_loss: 5.0439e-10
Epoch 365/512
512/512 - 0s - loss: 4.6134e-08 - val_loss: 4.7419e-10
Epoch 366/512
512/512 - 0s - loss: 4.4245e-08 - val_loss: 4.5229e-10
Epoch 367/512
512/512 - 0s - loss: 4.1759e-08 - val_loss: 4.3864e-10
Epoch 368/512
512/512 - 0s - loss: 4.0555e-08 - val_loss: 4.2349e-10
Epoch 369/512
512/512 - 0s - loss: 3.8474e-08 - val_loss: 3.9836e-10
Epoch 370/512
512/512 - 0s - loss: 3.6547e-08 - val_loss: 3.7807e-10
Epoch 371/512
512/512 - 0s - loss: 3.5175e-08 - val_loss: 3.5362e-10
Epoch 372/512
512/512 - 0s - loss: 3.2397e-08 - val_loss: 3.4417e-10
Epoch 373/512
512/512 - 0s - loss: 3.1776e-08 - val_loss: 3.4099e-10
Epoch 374/512
512/512 - 0s - loss: 3.1196e-08 - val_loss: 3.1834e-10
Epoch 375/512
512/512 - 0s - loss: 2.8920e-08 - val_loss: 2.9088e-10
Epoch 376/512
512/512 - 0s - loss: 2.6911e-08 - val_loss: 2.7988e-10
Epoch 377/512
512/512 - 0s - loss: 2.6247e-08 - val_loss: 2.7841e-10
Epoch 378/512
512/512 - 0s - loss: 2.5496e-08 - val_loss: 2.6577e-10
Epoch 379/512
512/512 - 0s - loss: 2.4202e-08 - val_loss: 2.4201e-10
Epoch 380/512
512/512 - 0s - loss: 2.2334e-08 - val_loss: 2.3391e-10
Epoch 381/512
512/512 - 0s - loss: 2.1889e-08 - val_loss: 2.2852e-10
Epoch 382/512
512/512 - 0s - loss: 2.1183e-08 - val_loss: 2.1736e-10
Epoch 383/512
512/512 - 0s - loss: 1.9989e-08 - val_loss: 2.0497e-10
Epoch 384/512
512/512 - 0s - loss: 1.9156e-08 - val_loss: 1.9416e-10
Epoch 385/512
512/512 - 0s - loss: 1.7966e-08 - val_loss: 1.8638e-10
Epoch 386/512
512/512 - 0s - loss: 1.7383e-08 - val_loss: 1.8180e-10
Epoch 387/512
512/512 - 0s - loss: 1.6855e-08 - val_loss: 1.7532e-10
Epoch 388/512
512/512 - 0s - loss: 1.6055e-08 - val_loss: 1.6477e-10
Epoch 389/512
512/512 - 0s - loss: 1.5247e-08 - val_loss: 1.5618e-10
Epoch 390/512
512/512 - 0s - loss: 1.4470e-08 - val_loss: 1.5092e-10
Epoch 391/512
512/512 - 0s - loss: 1.4060e-08 - val_loss: 1.4514e-10
Epoch 392/512
512/512 - 0s - loss: 1.3399e-08 - val_loss: 1.3944e-10
Epoch 393/512
512/512 - 0s - loss: 1.2838e-08 - val_loss: 1.3213e-10
Epoch 394/512
512/512 - 0s - loss: 1.2289e-08 - val_loss: 1.2713e-10
Epoch 395/512
512/512 - 0s - loss: 1.1798e-08 - val_loss: 1.2184e-10
Epoch 396/512
512/512 - 0s - loss: 1.1398e-08 - val_loss: 1.1612e-10
Epoch 397/512
512/512 - 0s - loss: 1.0682e-08 - val_loss: 1.1073e-10
Epoch 398/512
512/512 - 0s - loss: 1.0378e-08 - val_loss: 1.0777e-10
Epoch 399/512
512/512 - 0s - loss: 1.0037e-08 - val_loss: 1.0278e-10
Epoch 400/512
512/512 - 0s - loss: 9.5484e-09 - val_loss: 9.9392e-11
Epoch 401/512
512/512 - 0s - loss: 9.2114e-09 - val_loss: 9.5395e-11
Epoch 402/512
512/512 - 0s - loss: 8.8728e-09 - val_loss: 9.1187e-11
Epoch 403/512
512/512 - 0s - loss: 8.4157e-09 - val_loss: 8.6910e-11
Epoch 404/512
512/512 - 0s - loss: 8.0994e-09 - val_loss: 8.4894e-11
Epoch 405/512
512/512 - 0s - loss: 7.9648e-09 - val_loss: 8.0627e-11
Epoch 406/512
512/512 - 0s - loss: 7.4750e-09 - val_loss: 7.5914e-11
Epoch 407/512
512/512 - 0s - loss: 7.0716e-09 - val_loss: 7.4873e-11
Epoch 408/512
512/512 - 0s - loss: 7.0281e-09 - val_loss: 7.3691e-11
Epoch 409/512
512/512 - 0s - loss: 6.8651e-09 - val_loss: 6.9656e-11
Epoch 410/512
512/512 - 0s - loss: 6.4209e-09 - val_loss: 6.5484e-11
Epoch 411/512
512/512 - 0s - loss: 6.0883e-09 - val_loss: 6.4006e-11
Epoch 412/512
512/512 - 0s - loss: 5.9690e-09 - val_loss: 6.3074e-11
Epoch 413/512
512/512 - 0s - loss: 5.8918e-09 - val_loss: 6.0294e-11
Epoch 414/512
512/512 - 0s - loss: 5.5794e-09 - val_loss: 5.6367e-11
Epoch 415/512
512/512 - 0s - loss: 5.2881e-09 - val_loss: 5.4117e-11
Epoch 416/512
512/512 - 0s - loss: 5.1078e-09 - val_loss: 5.3074e-11
Epoch 417/512
512/512 - 0s - loss: 4.9415e-09 - val_loss: 5.2483e-11
Epoch 418/512
512/512 - 0s - loss: 4.9009e-09 - val_loss: 5.0699e-11
Epoch 419/512
512/512 - 0s - loss: 4.7448e-09 - val_loss: 4.7904e-11
Epoch 420/512
512/512 - 0s - loss: 4.4604e-09 - val_loss: 4.5601e-11
Epoch 421/512
512/512 - 0s - loss: 4.2824e-09 - val_loss: 4.4838e-11
Epoch 422/512
512/512 - 0s - loss: 4.1972e-09 - val_loss: 4.4118e-11
Epoch 423/512
512/512 - 0s - loss: 4.0982e-09 - val_loss: 4.2562e-11
Epoch 424/512
512/512 - 0s - loss: 3.9806e-09 - val_loss: 4.0652e-11
Epoch 425/512
512/512 - 0s - loss: 3.7878e-09 - val_loss: 3.8849e-11
Epoch 426/512
512/512 - 0s - loss: 3.6416e-09 - val_loss: 3.7452e-11
Epoch 427/512
512/512 - 0s - loss: 3.5685e-09 - val_loss: 3.6445e-11
Epoch 428/512
512/512 - 0s - loss: 3.4042e-09 - val_loss: 3.5724e-11
Epoch 429/512
512/512 - 0s - loss: 3.4223e-09 - val_loss: 3.4565e-11
Epoch 430/512
512/512 - 0s - loss: 3.2504e-09 - val_loss: 3.2638e-11
Epoch 431/512
512/512 - 0s - loss: 3.0702e-09 - val_loss: 3.1293e-11
Epoch 432/512
512/512 - 0s - loss: 2.9627e-09 - val_loss: 3.1169e-11
Epoch 433/512
512/512 - 0s - loss: 2.9869e-09 - val_loss: 3.0567e-11
Epoch 434/512
512/512 - 0s - loss: 2.9109e-09 - val_loss: 2.9650e-11
Epoch 435/512
512/512 - 0s - loss: 2.7512e-09 - val_loss: 2.8158e-11
Epoch 436/512
512/512 - 0s - loss: 2.6514e-09 - val_loss: 2.7361e-11
Epoch 437/512
512/512 - 0s - loss: 2.5870e-09 - val_loss: 2.6989e-11
Epoch 438/512
512/512 - 0s - loss: 2.5245e-09 - val_loss: 2.6402e-11
Epoch 439/512
512/512 - 0s - loss: 2.4884e-09 - val_loss: 2.5765e-11
Epoch 440/512
512/512 - 0s - loss: 2.4315e-09 - val_loss: 2.4540e-11
Epoch 441/512
512/512 - 0s - loss: 2.3236e-09 - val_loss: 2.3587e-11
Epoch 442/512
512/512 - 0s - loss: 2.2331e-09 - val_loss: 2.3080e-11
Epoch 443/512
512/512 - 0s - loss: 2.1935e-09 - val_loss: 2.2531e-11
Epoch 444/512
512/512 - 0s - loss: 2.1452e-09 - val_loss: 2.1910e-11
Epoch 445/512
512/512 - 0s - loss: 2.0925e-09 - val_loss: 2.1174e-11
Epoch 446/512
512/512 - 0s - loss: 2.0254e-09 - val_loss: 2.0583e-11
Epoch 447/512
512/512 - 0s - loss: 1.9502e-09 - val_loss: 2.0169e-11
Epoch 448/512
512/512 - 0s - loss: 1.9155e-09 - val_loss: 1.9705e-11
Epoch 449/512
512/512 - 0s - loss: 1.8620e-09 - val_loss: 1.9099e-11
Epoch 450/512
512/512 - 0s - loss: 1.8250e-09 - val_loss: 1.8552e-11
Epoch 451/512
512/512 - 0s - loss: 1.7798e-09 - val_loss: 1.8081e-11
Epoch 452/512
512/512 - 0s - loss: 1.7128e-09 - val_loss: 1.7741e-11
Epoch 453/512
512/512 - 0s - loss: 1.6958e-09 - val_loss: 1.7390e-11
Epoch 454/512
512/512 - 0s - loss: 1.6383e-09 - val_loss: 1.6949e-11
Epoch 455/512
512/512 - 0s - loss: 1.6164e-09 - val_loss: 1.6681e-11
Epoch 456/512
512/512 - 0s - loss: 1.5865e-09 - val_loss: 1.5931e-11
Epoch 457/512
512/512 - 0s - loss: 1.5231e-09 - val_loss: 1.5215e-11
Epoch 458/512
512/512 - 0s - loss: 1.4596e-09 - val_loss: 1.4846e-11
Epoch 459/512
512/512 - 0s - loss: 1.4298e-09 - val_loss: 1.4765e-11
Epoch 460/512
512/512 - 0s - loss: 1.4166e-09 - val_loss: 1.4504e-11
Epoch 461/512
512/512 - 0s - loss: 1.3840e-09 - val_loss: 1.4459e-11
Epoch 462/512
512/512 - 0s - loss: 1.3841e-09 - val_loss: 1.4022e-11
Epoch 463/512
512/512 - 0s - loss: 1.3219e-09 - val_loss: 1.3787e-11
Epoch 464/512
512/512 - 0s - loss: 1.3132e-09 - val_loss: 1.3101e-11
Epoch 465/512
512/512 - 0s - loss: 1.2403e-09 - val_loss: 1.2554e-11
Epoch 466/512
512/512 - 0s - loss: 1.2142e-09 - val_loss: 1.2205e-11
Epoch 467/512
512/512 - 0s - loss: 1.1790e-09 - val_loss: 1.2161e-11
Epoch 468/512
512/512 - 0s - loss: 1.1754e-09 - val_loss: 1.2170e-11
Epoch 469/512
512/512 - 0s - loss: 1.1628e-09 - val_loss: 1.1904e-11
Epoch 470/512
512/512 - 0s - loss: 1.1384e-09 - val_loss: 1.1742e-11
Epoch 471/512
512/512 - 0s - loss: 1.1427e-09 - val_loss: 1.1425e-11
Epoch 472/512
512/512 - 0s - loss: 1.0754e-09 - val_loss: 1.0907e-11
Epoch 473/512
512/512 - 0s - loss: 1.0459e-09 - val_loss: 1.0676e-11
Epoch 474/512
512/512 - 0s - loss: 1.0199e-09 - val_loss: 1.0545e-11
Epoch 475/512
512/512 - 0s - loss: 1.0194e-09 - val_loss: 1.0389e-11
Epoch 476/512
512/512 - 0s - loss: 9.9031e-10 - val_loss: 1.0033e-11
Epoch 477/512
512/512 - 0s - loss: 9.7137e-10 - val_loss: 9.8970e-12
Epoch 478/512
512/512 - 0s - loss: 9.5192e-10 - val_loss: 9.6443e-12
Epoch 479/512
512/512 - 0s - loss: 9.1827e-10 - val_loss: 9.3608e-12
Epoch 480/512
512/512 - 0s - loss: 9.0852e-10 - val_loss: 9.3400e-12
Epoch 481/512
512/512 - 0s - loss: 8.9107e-10 - val_loss: 9.1184e-12
Epoch 482/512
512/512 - 0s - loss: 8.8617e-10 - val_loss: 9.1228e-12
Epoch 483/512
512/512 - 0s - loss: 8.6356e-10 - val_loss: 8.8868e-12
Epoch 484/512
512/512 - 0s - loss: 8.4426e-10 - val_loss: 8.5059e-12
Epoch 485/512
512/512 - 0s - loss: 8.2036e-10 - val_loss: 8.3419e-12
Epoch 486/512
512/512 - 0s - loss: 8.0085e-10 - val_loss: 8.3087e-12
Epoch 487/512
512/512 - 0s - loss: 8.0176e-10 - val_loss: 8.1767e-12
Epoch 488/512
512/512 - 0s - loss: 7.9422e-10 - val_loss: 8.0009e-12
Epoch 489/512
512/512 - 0s - loss: 7.6077e-10 - val_loss: 7.6282e-12
Epoch 490/512
512/512 - 0s - loss: 7.3295e-10 - val_loss: 7.4452e-12
Epoch 491/512
512/512 - 0s - loss: 7.2954e-10 - val_loss: 7.4028e-12
Epoch 492/512
512/512 - 0s - loss: 7.3063e-10 - val_loss: 7.4632e-12
Epoch 493/512
512/512 - 0s - loss: 7.1950e-10 - val_loss: 7.3354e-12
Epoch 494/512
512/512 - 0s - loss: 7.0882e-10 - val_loss: 7.0550e-12
Epoch 495/512
512/512 - 0s - loss: 6.6887e-10 - val_loss: 6.7779e-12
Epoch 496/512
512/512 - 0s - loss: 6.5739e-10 - val_loss: 6.7855e-12
Epoch 497/512
512/512 - 0s - loss: 6.6264e-10 - val_loss: 6.8829e-12
Epoch 498/512
512/512 - 0s - loss: 6.6590e-10 - val_loss: 6.8456e-12
Epoch 499/512
512/512 - 0s - loss: 6.6549e-10 - val_loss: 6.6640e-12
Epoch 500/512
512/512 - 0s - loss: 6.3895e-10 - val_loss: 6.3530e-12
Epoch 501/512
512/512 - 0s - loss: 6.1154e-10 - val_loss: 6.0858e-12
Epoch 502/512
512/512 - 0s - loss: 5.8803e-10 - val_loss: 5.8704e-12
Epoch 503/512
512/512 - 0s - loss: 5.7067e-10 - val_loss: 5.8892e-12
Epoch 504/512
512/512 - 0s - loss: 5.8119e-10 - val_loss: 5.9843e-12
Epoch 505/512
512/512 - 0s - loss: 5.8689e-10 - val_loss: 6.0638e-12
Epoch 506/512
512/512 - 0s - loss: 5.7969e-10 - val_loss: 5.9024e-12
Epoch 507/512
512/512 - 0s - loss: 5.6726e-10 - val_loss: 5.6958e-12
Epoch 508/512
512/512 - 0s - loss: 5.4166e-10 - val_loss: 5.3722e-12
Epoch 509/512
512/512 - 0s - loss: 5.1592e-10 - val_loss: 5.2185e-12
Epoch 510/512
512/512 - 0s - loss: 5.0660e-10 - val_loss: 5.2255e-12
Epoch 511/512
512/512 - 0s - loss: 5.1436e-10 - val_loss: 5.3480e-12
Epoch 512/512
512/512 - 0s - loss: 5.2829e-10 - val_loss: 5.3480e-12
2024-04-13 10:22:27.684047: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.0664e-10 - val_loss: 1.6310e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7811e-09 - val_loss: 1.8743e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5138e-09 - val_loss: 1.0889e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.6616e-10 - val_loss: 6.4349e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.6730e-10 - val_loss: 5.2754e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2511e-10 - val_loss: 5.8731e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2654e-10 - val_loss: 7.6625e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2846e-10 - val_loss: 9.6751e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6055e-10 - val_loss: 9.9354e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1456e-10 - val_loss: 8.5285e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7944e-10 - val_loss: 7.3289e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8502e-10 - val_loss: 6.7735e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5895e-10 - val_loss: 6.8744e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8039e-10 - val_loss: 7.3022e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2313e-10 - val_loss: 7.6924e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4287e-10 - val_loss: 7.4795e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0513e-10 - val_loss: 6.8494e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5256e-10 - val_loss: 6.5237e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3033e-10 - val_loss: 6.4863e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3156e-10 - val_loss: 6.5417e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4223e-10 - val_loss: 6.5875e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3542e-10 - val_loss: 6.4710e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2675e-10 - val_loss: 6.2398e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9929e-10 - val_loss: 6.1244e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8249e-10 - val_loss: 5.8740e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7049e-10 - val_loss: 5.8399e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7093e-10 - val_loss: 5.9561e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8017e-10 - val_loss: 5.9746e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8013e-10 - val_loss: 5.9006e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6015e-10 - val_loss: 5.5129e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.2363e-10 - val_loss: 5.2261e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.0088e-10 - val_loss: 5.0976e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9747e-10 - val_loss: 5.1354e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0517e-10 - val_loss: 5.2607e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1449e-10 - val_loss: 5.2319e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.0269e-10 - val_loss: 5.0118e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.8451e-10 - val_loss: 4.8964e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.7222e-10 - val_loss: 4.7427e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6480e-10 - val_loss: 4.7772e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6779e-10 - val_loss: 4.8091e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.6116e-10 - val_loss: 4.5603e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.3371e-10 - val_loss: 4.3984e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.2559e-10 - val_loss: 4.3715e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3127e-10 - val_loss: 4.4357e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3309e-10 - val_loss: 4.4829e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3230e-10 - val_loss: 4.4059e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.2701e-10 - val_loss: 4.2738e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.1787e-10 - val_loss: 4.2660e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.0671e-10 - val_loss: 4.0267e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.9090e-10 - val_loss: 3.9202e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.8518e-10 - val_loss: 3.9023e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8560e-10 - val_loss: 3.9679e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8624e-10 - val_loss: 3.9279e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.8499e-10 - val_loss: 3.8733e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.7230e-10 - val_loss: 3.7087e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.5804e-10 - val_loss: 3.6582e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.5664e-10 - val_loss: 3.6553e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5617e-10 - val_loss: 3.6620e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6355e-10 - val_loss: 3.6897e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.5806e-10 - val_loss: 3.6297e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.4968e-10 - val_loss: 3.4655e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.3581e-10 - val_loss: 3.3704e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.2161e-10 - val_loss: 3.2295e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1781e-10 - val_loss: 3.2844e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2744e-10 - val_loss: 3.4074e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3530e-10 - val_loss: 3.4429e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3336e-10 - val_loss: 3.2791e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.1583e-10 - val_loss: 3.1454e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.0448e-10 - val_loss: 3.0894e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.0185e-10 - val_loss: 3.0785e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.0270e-10 - val_loss: 3.0268e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.9813e-10 - val_loss: 3.0171e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9847e-10 - val_loss: 3.0429e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9819e-10 - val_loss: 3.0475e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9695e-10 - val_loss: 3.0340e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.9565e-10 - val_loss: 2.9382e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.8383e-10 - val_loss: 2.7935e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.7055e-10 - val_loss: 2.7401e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.6823e-10 - val_loss: 2.7364e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6719e-10 - val_loss: 2.7474e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.6579e-10 - val_loss: 2.6683e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.5868e-10 - val_loss: 2.6086e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5801e-10 - val_loss: 2.6561e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6307e-10 - val_loss: 2.6957e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6445e-10 - val_loss: 2.6340e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.5568e-10 - val_loss: 2.5546e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.4817e-10 - val_loss: 2.4845e-10
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.4004e-10 - val_loss: 2.4382e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4245e-10 - val_loss: 2.5319e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4899e-10 - val_loss: 2.4799e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.4068e-10 - val_loss: 2.4206e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.3463e-10 - val_loss: 2.3470e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3165e-10 - val_loss: 2.3875e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.3406e-10 - val_loss: 2.3412e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2767e-10 - val_loss: 2.3056e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2971e-10 - val_loss: 2.3055e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2205e-10 - val_loss: 2.2450e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.1470e-10 - val_loss: 2.1658e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1736e-10 - val_loss: 2.2211e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1680e-10 - val_loss: 2.2330e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.1954e-10 - val_loss: 2.1638e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0907e-10 - val_loss: 2.1286e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0920e-10 - val_loss: 2.1228e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0726e-10 - val_loss: 2.0810e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0429e-10 - val_loss: 2.1026e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0215e-10 - val_loss: 2.0379e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0500e-10 - val_loss: 2.0774e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.9862e-10 - val_loss: 2.0134e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.9651e-10 - val_loss: 1.9709e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.9235e-10 - val_loss: 1.9629e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.9089e-10 - val_loss: 1.8869e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8802e-10 - val_loss: 1.9271e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8898e-10 - val_loss: 1.9257e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8936e-10 - val_loss: 1.9176e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9079e-10 - val_loss: 1.9253e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8915e-10 - val_loss: 1.9259e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.8623e-10 - val_loss: 1.8706e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.8656e-10 - val_loss: 1.8609e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.8131e-10 - val_loss: 1.8098e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.7621e-10 - val_loss: 1.7179e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.6867e-10 - val_loss: 1.7110e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7049e-10 - val_loss: 1.7830e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7459e-10 - val_loss: 1.7860e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7230e-10 - val_loss: 1.7119e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.6862e-10 - val_loss: 1.6709e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5963e-10 - val_loss: 1.6326e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5952e-10 - val_loss: 1.6176e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5997e-10 - val_loss: 1.6262e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5868e-10 - val_loss: 1.6198e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6239e-10 - val_loss: 1.6407e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6107e-10 - val_loss: 1.6446e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.6201e-10 - val_loss: 1.5959e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5659e-10 - val_loss: 1.5779e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5601e-10 - val_loss: 1.5539e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.5187e-10 - val_loss: 1.5170e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4962e-10 - val_loss: 1.5181e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.4909e-10 - val_loss: 1.4873e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.4626e-10 - val_loss: 1.4606e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4254e-10 - val_loss: 1.4675e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4377e-10 - val_loss: 1.4651e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4297e-10 - val_loss: 1.4942e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4817e-10 - val_loss: 1.4997e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4811e-10 - val_loss: 1.4851e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4701e-10 - val_loss: 1.4639e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.4262e-10 - val_loss: 1.4190e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.3732e-10 - val_loss: 1.3759e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.3300e-10 - val_loss: 1.3256e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.3065e-10 - val_loss: 1.3234e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3151e-10 - val_loss: 1.3422e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3270e-10 - val_loss: 1.3647e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3579e-10 - val_loss: 1.3950e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3719e-10 - val_loss: 1.3761e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3267e-10 - val_loss: 1.3236e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2940e-10 - val_loss: 1.3246e-10
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.2864e-10 - val_loss: 1.2638e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2461e-10 - val_loss: 1.2677e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.2546e-10 - val_loss: 1.2602e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.2018e-10 - val_loss: 1.2160e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2255e-10 - val_loss: 1.2858e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2593e-10 - val_loss: 1.3162e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2939e-10 - val_loss: 1.3295e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.2707e-10 - val_loss: 1.1954e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1508e-10 - val_loss: 1.2133e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1851e-10 - val_loss: 1.2041e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.1596e-10 - val_loss: 1.1583e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1526e-10 - val_loss: 1.1610e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.1166e-10 - val_loss: 1.1238e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.1019e-10 - val_loss: 1.1023e-10
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1014e-10 - val_loss: 1.1323e-10
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1298e-10 - val_loss: 1.1354e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1128e-10 - val_loss: 1.1109e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1063e-10 - val_loss: 1.1527e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1289e-10 - val_loss: 1.1069e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.0627e-10 - val_loss: 1.0979e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0812e-10 - val_loss: 1.1002e-10
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0933e-10 - val_loss: 1.1023e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.1014e-10 - val_loss: 1.0889e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.0551e-10 - val_loss: 1.0752e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0732e-10 - val_loss: 1.1111e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.0981e-10 - val_loss: 1.0731e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0623e-10 - val_loss: 1.0792e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0520e-10 - val_loss: 1.0971e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.0663e-10 - val_loss: 1.0259e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0137e-10 - val_loss: 1.0290e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0125e-10 - val_loss: 1.0261e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.9339e-11 - val_loss: 9.7913e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.5242e-11 - val_loss: 9.7848e-11
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.5404e-11 - val_loss: 9.4518e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6578e-11 - val_loss: 9.9136e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6958e-11 - val_loss: 1.0024e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7773e-11 - val_loss: 9.6026e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5778e-11 - val_loss: 9.6646e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.2616e-11 - val_loss: 9.3544e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.2326e-11 - val_loss: 9.2664e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.0182e-11 - val_loss: 9.0214e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9015e-11 - val_loss: 9.2681e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5710e-11 - val_loss: 9.7961e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7301e-11 - val_loss: 9.9661e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7123e-11 - val_loss: 9.4658e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3406e-11 - val_loss: 9.2543e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.9344e-11 - val_loss: 8.8243e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7685e-11 - val_loss: 8.9990e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8789e-11 - val_loss: 9.0528e-11
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 9.0422e-11 - val_loss: 8.8125e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.4243e-11 - val_loss: 8.6208e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.5392e-11 - val_loss: 8.5151e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.4796e-11 - val_loss: 8.4561e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 8.1937e-11 - val_loss: 8.1770e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3341e-11 - val_loss: 8.6484e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5272e-11 - val_loss: 8.9534e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7894e-11 - val_loss: 8.6343e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4063e-11 - val_loss: 8.7470e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5751e-11 - val_loss: 8.4129e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 7.9826e-11 - val_loss: 7.5411e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5447e-11 - val_loss: 7.6921e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 7.4262e-11 - val_loss: 7.2522e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4433e-11 - val_loss: 7.6692e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5643e-11 - val_loss: 7.5214e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6489e-11 - val_loss: 7.7747e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4685e-11 - val_loss: 7.5576e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7679e-11 - val_loss: 7.8715e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6339e-11 - val_loss: 7.8726e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9196e-11 - val_loss: 7.9550e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7113e-11 - val_loss: 7.8193e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7596e-11 - val_loss: 7.6872e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4679e-11 - val_loss: 7.6486e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4602e-11 - val_loss: 7.3253e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3005e-11 - val_loss: 7.7844e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6430e-11 - val_loss: 7.4732e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 7.2629e-11 - val_loss: 7.1503e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0189e-11 - val_loss: 7.1843e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1838e-11 - val_loss: 7.6894e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5486e-11 - val_loss: 7.3927e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1925e-11 - val_loss: 7.1709e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0121e-11 - val_loss: 7.2395e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2998e-11 - val_loss: 7.7231e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6193e-11 - val_loss: 7.5103e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 7.1844e-11 - val_loss: 6.7021e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4945e-11 - val_loss: 6.7121e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8549e-11 - val_loss: 6.8527e-11
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 6.5475e-11 - val_loss: 6.4591e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4663e-11 - val_loss: 6.4714e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 6.3258e-11 - val_loss: 6.3942e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6935e-11 - val_loss: 7.2151e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9632e-11 - val_loss: 6.6588e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6985e-11 - val_loss: 6.7232e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5324e-11 - val_loss: 6.4819e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4752e-11 - val_loss: 6.7807e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6609e-11 - val_loss: 6.5150e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4348e-11 - val_loss: 6.7598e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6317e-11 - val_loss: 6.5383e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 6.4114e-11 - val_loss: 6.3559e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5412e-11 - val_loss: 6.6885e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4171e-11 - val_loss: 6.4249e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5300e-11 - val_loss: 6.6053e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 6.3551e-11 - val_loss: 6.3423e-11
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 6.0969e-11 - val_loss: 5.9028e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9286e-11 - val_loss: 6.0642e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0693e-11 - val_loss: 6.0548e-11
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.7227e-11 - val_loss: 5.5168e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5880e-11 - val_loss: 5.7095e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6184e-11 - val_loss: 5.6849e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7673e-11 - val_loss: 6.2949e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2506e-11 - val_loss: 6.0796e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7394e-11 - val_loss: 5.5580e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6950e-11 - val_loss: 6.0775e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0264e-11 - val_loss: 5.9726e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0494e-11 - val_loss: 6.2529e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1790e-11 - val_loss: 6.0112e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7332e-11 - val_loss: 5.5314e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6713e-11 - val_loss: 5.8194e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6599e-11 - val_loss: 5.6661e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.4805e-11 - val_loss: 5.2384e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0936e-11 - val_loss: 5.2922e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2576e-11 - val_loss: 5.3177e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.1742e-11 - val_loss: 5.0435e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.9081e-11 - val_loss: 5.0218e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1209e-11 - val_loss: 5.3831e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2873e-11 - val_loss: 5.1244e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2180e-11 - val_loss: 5.4102e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3565e-11 - val_loss: 5.5194e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7599e-11 - val_loss: 6.1647e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0133e-11 - val_loss: 5.6745e-11
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.3040e-11 - val_loss: 4.9881e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8990e-11 - val_loss: 5.1992e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2453e-11 - val_loss: 5.4043e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5224e-11 - val_loss: 5.9127e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8472e-11 - val_loss: 5.6819e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5286e-11 - val_loss: 5.2564e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 5.0001e-11 - val_loss: 4.8598e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9765e-11 - val_loss: 5.2168e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2744e-11 - val_loss: 5.3210e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0306e-11 - val_loss: 4.8788e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9694e-11 - val_loss: 5.1808e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0960e-11 - val_loss: 5.0403e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9204e-11 - val_loss: 4.8759e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0040e-11 - val_loss: 5.3522e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4867e-11 - val_loss: 6.0147e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8053e-11 - val_loss: 5.5027e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2776e-11 - val_loss: 5.0115e-11
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.8105e-11 - val_loss: 4.5715e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.3395e-11 - val_loss: 4.1078e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.9886e-11 - val_loss: 3.9885e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0754e-11 - val_loss: 4.4089e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5343e-11 - val_loss: 4.8121e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0023e-11 - val_loss: 5.3764e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3356e-11 - val_loss: 5.1850e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9046e-11 - val_loss: 4.4106e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1781e-11 - val_loss: 4.0484e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0566e-11 - val_loss: 4.3756e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5031e-11 - val_loss: 4.8118e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7651e-11 - val_loss: 5.0981e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2046e-11 - val_loss: 5.1311e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8630e-11 - val_loss: 4.5766e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3500e-11 - val_loss: 4.0857e-11
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.9671e-11 - val_loss: 3.9786e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1684e-11 - val_loss: 4.5949e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6428e-11 - val_loss: 4.8119e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8003e-11 - val_loss: 4.7519e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5952e-11 - val_loss: 4.5635e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4912e-11 - val_loss: 4.3359e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 4.1680e-11 - val_loss: 3.9718e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.8900e-11 - val_loss: 3.6836e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.4817e-11 - val_loss: 3.3834e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3533e-11 - val_loss: 3.5434e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6810e-11 - val_loss: 3.9475e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9773e-11 - val_loss: 4.3875e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4124e-11 - val_loss: 4.5516e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4793e-11 - val_loss: 4.4273e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3615e-11 - val_loss: 4.5189e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5027e-11 - val_loss: 4.5678e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3860e-11 - val_loss: 4.1074e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8927e-11 - val_loss: 3.9338e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9879e-11 - val_loss: 4.3386e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3790e-11 - val_loss: 4.4211e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2504e-11 - val_loss: 4.1403e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2041e-11 - val_loss: 4.5013e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5304e-11 - val_loss: 4.5791e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3451e-11 - val_loss: 4.1283e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9982e-11 - val_loss: 3.8998e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7612e-11 - val_loss: 3.5478e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.4239e-11 - val_loss: 3.2607e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 3.1263e-11 - val_loss: 2.9639e-11
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.8828e-11 - val_loss: 2.9120e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0375e-11 - val_loss: 3.1858e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2312e-11 - val_loss: 3.4083e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4635e-11 - val_loss: 3.8655e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9066e-11 - val_loss: 4.1581e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1388e-11 - val_loss: 4.3759e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4931e-11 - val_loss: 4.6426e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4727e-11 - val_loss: 4.2102e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0622e-11 - val_loss: 3.9528e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8191e-11 - val_loss: 3.5454e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4766e-11 - val_loss: 3.5888e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7146e-11 - val_loss: 3.9302e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8659e-11 - val_loss: 3.7563e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5021e-11 - val_loss: 3.2948e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2076e-11 - val_loss: 3.2836e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1795e-11 - val_loss: 2.9807e-11
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.8953e-11 - val_loss: 2.8613e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0018e-11 - val_loss: 3.3108e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3881e-11 - val_loss: 3.5467e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5478e-11 - val_loss: 3.7370e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8291e-11 - val_loss: 3.9685e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9025e-11 - val_loss: 3.7032e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4628e-11 - val_loss: 3.2550e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1719e-11 - val_loss: 2.9983e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8748e-11 - val_loss: 2.8754e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8406e-11 - val_loss: 2.9061e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1285e-11 - val_loss: 3.4598e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4693e-11 - val_loss: 3.5365e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6526e-11 - val_loss: 3.8579e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8629e-11 - val_loss: 3.8810e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7221e-11 - val_loss: 3.4234e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3143e-11 - val_loss: 3.2117e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0595e-11 - val_loss: 2.8917e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.8379e-11 - val_loss: 2.7582e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.6568e-11 - val_loss: 2.6268e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.5674e-11 - val_loss: 2.3798e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2848e-11 - val_loss: 2.2399e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2697e-11 - val_loss: 2.5880e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6883e-11 - val_loss: 2.8200e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8147e-11 - val_loss: 2.9377e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0711e-11 - val_loss: 3.2698e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2596e-11 - val_loss: 3.3476e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4357e-11 - val_loss: 3.7459e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7949e-11 - val_loss: 3.9586e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8833e-11 - val_loss: 3.7338e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4820e-11 - val_loss: 3.3014e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1622e-11 - val_loss: 2.9875e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8791e-11 - val_loss: 2.7750e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7200e-11 - val_loss: 2.7120e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8032e-11 - val_loss: 3.0643e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1166e-11 - val_loss: 3.3410e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3471e-11 - val_loss: 3.4562e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6152e-11 - val_loss: 3.8196e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7997e-11 - val_loss: 3.7126e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5005e-11 - val_loss: 3.2684e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1576e-11 - val_loss: 2.9871e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8624e-11 - val_loss: 2.7580e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6918e-11 - val_loss: 2.7038e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7225e-11 - val_loss: 2.9728e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0500e-11 - val_loss: 3.2741e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2695e-11 - val_loss: 3.4216e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5493e-11 - val_loss: 3.7415e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7820e-11 - val_loss: 3.7090e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4872e-11 - val_loss: 3.2489e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1282e-11 - val_loss: 2.9654e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8441e-11 - val_loss: 2.8140e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7647e-11 - val_loss: 2.6109e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5552e-11 - val_loss: 2.3938e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2804e-11 - val_loss: 2.2282e-11
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.1916e-11 - val_loss: 2.2189e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2356e-11 - val_loss: 2.4031e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4630e-11 - val_loss: 2.6854e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7012e-11 - val_loss: 2.7724e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8113e-11 - val_loss: 3.0895e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1130e-11 - val_loss: 3.2268e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2273e-11 - val_loss: 3.2705e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1166e-11 - val_loss: 2.9184e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8382e-11 - val_loss: 2.7748e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6661e-11 - val_loss: 2.4613e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.2656e-11 - val_loss: 2.1236e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0886e-11 - val_loss: 2.1641e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1990e-11 - val_loss: 2.3874e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5261e-11 - val_loss: 2.7243e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7316e-11 - val_loss: 2.8620e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8812e-11 - val_loss: 3.0849e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0940e-11 - val_loss: 3.1345e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1398e-11 - val_loss: 3.0178e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8764e-11 - val_loss: 2.8196e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7243e-11 - val_loss: 2.5478e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4016e-11 - val_loss: 2.2171e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.1492e-11 - val_loss: 2.0982e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0984e-11 - val_loss: 2.1697e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1387e-11 - val_loss: 2.1503e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3618e-11 - val_loss: 2.5691e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6373e-11 - val_loss: 2.7285e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7143e-11 - val_loss: 2.6831e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7885e-11 - val_loss: 3.0438e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0808e-11 - val_loss: 3.1874e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1239e-11 - val_loss: 3.0239e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8381e-11 - val_loss: 2.6567e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4668e-11 - val_loss: 2.2305e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1556e-11 - val_loss: 2.1568e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1321e-11 - val_loss: 2.1098e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0632e-11 - val_loss: 2.0662e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1549e-11 - val_loss: 2.4931e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5678e-11 - val_loss: 2.6065e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6613e-11 - val_loss: 2.7504e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8660e-11 - val_loss: 3.0649e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1013e-11 - val_loss: 3.2315e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2310e-11 - val_loss: 3.0958e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9377e-11 - val_loss: 2.7295e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6501e-11 - val_loss: 2.6933e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5479e-11 - val_loss: 2.2947e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2456e-11 - val_loss: 2.2993e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3247e-11 - val_loss: 2.5234e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5807e-11 - val_loss: 2.6485e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5966e-11 - val_loss: 2.6362e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6109e-11 - val_loss: 2.4329e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2896e-11 - val_loss: 2.1850e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1683e-11 - val_loss: 2.1789e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1536e-11 - val_loss: 2.1634e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1809e-11 - val_loss: 2.3559e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4050e-11 - val_loss: 2.6313e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6400e-11 - val_loss: 2.6939e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5902e-11 - val_loss: 2.3868e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2783e-11 - val_loss: 2.2750e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2150e-11 - val_loss: 2.1521e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 2.0868e-11 - val_loss: 1.9446e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.8601e-11 - val_loss: 1.8176e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.7985e-11 - val_loss: 1.8077e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.7573e-11 - val_loss: 1.7415e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9286e-11 - val_loss: 2.1394e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1253e-11 - val_loss: 2.1441e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1063e-11 - val_loss: 2.0832e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0975e-11 - val_loss: 2.1733e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3037e-11 - val_loss: 2.5451e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6080e-11 - val_loss: 2.7060e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6704e-11 - val_loss: 2.5588e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4291e-11 - val_loss: 2.3200e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2313e-11 - val_loss: 2.1618e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1275e-11 - val_loss: 2.1127e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0131e-11 - val_loss: 1.8604e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8121e-11 - val_loss: 1.8029e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8018e-11 - val_loss: 1.8578e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9472e-11 - val_loss: 2.0875e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1238e-11 - val_loss: 2.1724e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1233e-11 - val_loss: 2.1002e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1084e-11 - val_loss: 2.1121e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0130e-11 - val_loss: 1.8377e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.7804e-11 - val_loss: 1.6792e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/addition_weights.h5
512/512 - 0s - loss: 1.6437e-11 - val_loss: 1.6710e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6730e-11 - val_loss: 1.6792e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6781e-11 - val_loss: 1.8414e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9456e-11 - val_loss: 2.0640e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1176e-11 - val_loss: 2.1770e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1623e-11 - val_loss: 2.1849e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3078e-11 - val_loss: 2.4243e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4402e-11 - val_loss: 2.5144e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4114e-11 - val_loss: 2.2024e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1176e-11 - val_loss: 2.0485e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0568e-11 - val_loss: 2.0540e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0356e-11 - val_loss: 2.1089e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1732e-11 - val_loss: 2.3753e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4902e-11 - val_loss: 2.6385e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6023e-11 - val_loss: 2.4388e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3021e-11 - val_loss: 2.0964e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0296e-11 - val_loss: 1.9916e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9805e-11 - val_loss: 2.0454e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9417e-11 - val_loss: 1.7933e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.0629 - val_loss: 0.0039
Epoch 2/512
512/512 - 0s - loss: 0.0185 - val_loss: 0.0072
Epoch 3/512
512/512 - 0s - loss: 0.0208 - val_loss: 0.0089
Epoch 4/512
512/512 - 0s - loss: 0.0137 - val_loss: 0.0117
Epoch 5/512
512/512 - 0s - loss: 0.0090 - val_loss: 0.0071
Epoch 6/512
512/512 - 0s - loss: 0.0057 - val_loss: 0.0036
Epoch 7/512
512/512 - 0s - loss: 0.0044 - val_loss: 0.0017
Epoch 8/512
512/512 - 0s - loss: 0.0054 - val_loss: 0.0016
Epoch 9/512
512/512 - 0s - loss: 0.0031 - val_loss: 0.0016
Epoch 10/512
512/512 - 0s - loss: 0.0028 - val_loss: 0.0023
Epoch 11/512
512/512 - 0s - loss: 0.0039 - val_loss: 0.0030
Epoch 12/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0033
Epoch 13/512
512/512 - 0s - loss: 0.0023 - val_loss: 0.0040
Epoch 14/512
512/512 - 0s - loss: 0.0027 - val_loss: 0.0048
Epoch 15/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0049
Epoch 16/512
512/512 - 0s - loss: 0.0021 - val_loss: 0.0049
Epoch 17/512
512/512 - 0s - loss: 0.0022 - val_loss: 0.0053
Epoch 18/512
512/512 - 0s - loss: 0.0025 - val_loss: 0.0052
Epoch 19/512
512/512 - 0s - loss: 0.0019 - val_loss: 0.0049
Epoch 20/512
512/512 - 0s - loss: 0.0019 - val_loss: 0.0047
Epoch 21/512
512/512 - 0s - loss: 0.0020 - val_loss: 0.0043
Epoch 22/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0036
Epoch 23/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0030
Epoch 24/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0024
Epoch 25/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0018
Epoch 26/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0015
Epoch 27/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0012
Epoch 28/512
512/512 - 0s - loss: 9.3823e-04 - val_loss: 0.0011
Epoch 29/512
512/512 - 0s - loss: 9.7740e-04 - val_loss: 9.4543e-04
Epoch 30/512
512/512 - 0s - loss: 8.8860e-04 - val_loss: 8.5288e-04
Epoch 31/512
512/512 - 0s - loss: 9.0608e-04 - val_loss: 8.0587e-04
Epoch 32/512
512/512 - 0s - loss: 8.2580e-04 - val_loss: 7.3549e-04
Epoch 33/512
512/512 - 0s - loss: 7.8410e-04 - val_loss: 6.7543e-04
Epoch 34/512
512/512 - 0s - loss: 7.6753e-04 - val_loss: 6.1603e-04
Epoch 35/512
512/512 - 0s - loss: 7.2373e-04 - val_loss: 5.5926e-04
Epoch 36/512
512/512 - 0s - loss: 6.7963e-04 - val_loss: 5.0366e-04
Epoch 37/512
512/512 - 0s - loss: 6.5117e-04 - val_loss: 4.6828e-04
Epoch 38/512
512/512 - 0s - loss: 6.1107e-04 - val_loss: 4.3150e-04
Epoch 39/512
512/512 - 0s - loss: 5.6998e-04 - val_loss: 4.0768e-04
Epoch 40/512
512/512 - 0s - loss: 5.7612e-04 - val_loss: 3.8225e-04
Epoch 41/512
512/512 - 0s - loss: 5.1029e-04 - val_loss: 3.3904e-04
Epoch 42/512
512/512 - 0s - loss: 4.7112e-04 - val_loss: 3.0151e-04
Epoch 43/512
512/512 - 0s - loss: 4.5649e-04 - val_loss: 2.6002e-04
Epoch 44/512
512/512 - 0s - loss: 4.0638e-04 - val_loss: 2.2696e-04
Epoch 45/512
512/512 - 0s - loss: 3.6477e-04 - val_loss: 1.9082e-04
Epoch 46/512
512/512 - 0s - loss: 3.3786e-04 - val_loss: 1.6293e-04
Epoch 47/512
512/512 - 0s - loss: 3.1005e-04 - val_loss: 1.4088e-04
Epoch 48/512
512/512 - 0s - loss: 2.6868e-04 - val_loss: 1.2073e-04
Epoch 49/512
512/512 - 0s - loss: 2.5328e-04 - val_loss: 9.9570e-05
Epoch 50/512
512/512 - 0s - loss: 2.2686e-04 - val_loss: 8.6074e-05
Epoch 51/512
512/512 - 0s - loss: 2.0881e-04 - val_loss: 7.1702e-05
Epoch 52/512
512/512 - 0s - loss: 1.9562e-04 - val_loss: 6.0190e-05
Epoch 53/512
512/512 - 0s - loss: 1.7416e-04 - val_loss: 5.1446e-05
Epoch 54/512
512/512 - 0s - loss: 1.6527e-04 - val_loss: 4.3365e-05
Epoch 55/512
512/512 - 0s - loss: 1.5218e-04 - val_loss: 3.6680e-05
Epoch 56/512
512/512 - 0s - loss: 1.3809e-04 - val_loss: 3.1069e-05
Epoch 57/512
512/512 - 0s - loss: 1.3028e-04 - val_loss: 2.6660e-05
Epoch 58/512
512/512 - 0s - loss: 1.2094e-04 - val_loss: 2.2833e-05
Epoch 59/512
512/512 - 0s - loss: 1.1364e-04 - val_loss: 1.9968e-05
Epoch 60/512
512/512 - 0s - loss: 1.0479e-04 - val_loss: 1.7559e-05
Epoch 61/512
512/512 - 0s - loss: 9.6206e-05 - val_loss: 1.5612e-05
Epoch 62/512
512/512 - 0s - loss: 9.1631e-05 - val_loss: 1.3894e-05
Epoch 63/512
512/512 - 0s - loss: 8.7321e-05 - val_loss: 1.2547e-05
Epoch 64/512
512/512 - 0s - loss: 7.6840e-05 - val_loss: 1.1293e-05
Epoch 65/512
512/512 - 0s - loss: 7.3904e-05 - val_loss: 1.0162e-05
Epoch 66/512
512/512 - 0s - loss: 7.1833e-05 - val_loss: 9.2516e-06
Epoch 67/512
512/512 - 0s - loss: 6.2591e-05 - val_loss: 8.3861e-06
Epoch 68/512
512/512 - 0s - loss: 6.1140e-05 - val_loss: 7.5611e-06
Epoch 69/512
512/512 - 0s - loss: 5.7015e-05 - val_loss: 6.8410e-06
Epoch 70/512
512/512 - 0s - loss: 5.2549e-05 - val_loss: 6.1759e-06
Epoch 71/512
512/512 - 0s - loss: 4.8375e-05 - val_loss: 5.5705e-06
Epoch 72/512
512/512 - 0s - loss: 4.7017e-05 - val_loss: 5.0000e-06
Epoch 73/512
512/512 - 0s - loss: 4.3349e-05 - val_loss: 4.4871e-06
Epoch 74/512
512/512 - 0s - loss: 3.8633e-05 - val_loss: 4.0050e-06
Epoch 75/512
512/512 - 0s - loss: 3.8212e-05 - val_loss: 3.5705e-06
Epoch 76/512
512/512 - 0s - loss: 3.5103e-05 - val_loss: 3.1709e-06
Epoch 77/512
512/512 - 0s - loss: 3.1888e-05 - val_loss: 2.8075e-06
Epoch 78/512
512/512 - 0s - loss: 2.9800e-05 - val_loss: 2.4785e-06
Epoch 79/512
512/512 - 0s - loss: 2.8888e-05 - val_loss: 2.1830e-06
Epoch 80/512
512/512 - 0s - loss: 2.5774e-05 - val_loss: 1.9137e-06
Epoch 81/512
512/512 - 0s - loss: 2.3833e-05 - val_loss: 1.6779e-06
Epoch 82/512
512/512 - 0s - loss: 2.2637e-05 - val_loss: 1.4656e-06
Epoch 83/512
512/512 - 0s - loss: 2.0745e-05 - val_loss: 1.2757e-06
Epoch 84/512
512/512 - 0s - loss: 1.9207e-05 - val_loss: 1.1098e-06
Epoch 85/512
512/512 - 0s - loss: 1.8259e-05 - val_loss: 9.6032e-07
Epoch 86/512
512/512 - 0s - loss: 1.6179e-05 - val_loss: 8.3262e-07
Epoch 87/512
512/512 - 0s - loss: 1.5625e-05 - val_loss: 7.1660e-07
Epoch 88/512
512/512 - 0s - loss: 1.3859e-05 - val_loss: 6.2170e-07
Epoch 89/512
512/512 - 0s - loss: 1.3192e-05 - val_loss: 5.3922e-07
Epoch 90/512
512/512 - 0s - loss: 1.2202e-05 - val_loss: 4.6549e-07
Epoch 91/512
512/512 - 0s - loss: 1.1104e-05 - val_loss: 4.0194e-07
Epoch 92/512
512/512 - 0s - loss: 1.0382e-05 - val_loss: 3.4947e-07
Epoch 93/512
512/512 - 0s - loss: 9.5753e-06 - val_loss: 3.0104e-07
Epoch 94/512
512/512 - 0s - loss: 8.4871e-06 - val_loss: 2.6293e-07
Epoch 95/512
512/512 - 0s - loss: 8.2689e-06 - val_loss: 2.2833e-07
Epoch 96/512
512/512 - 0s - loss: 7.4064e-06 - val_loss: 1.9886e-07
Epoch 97/512
512/512 - 0s - loss: 6.8963e-06 - val_loss: 1.7282e-07
Epoch 98/512
512/512 - 0s - loss: 6.1881e-06 - val_loss: 1.5167e-07
Epoch 99/512
512/512 - 0s - loss: 5.8265e-06 - val_loss: 1.3380e-07
Epoch 100/512
512/512 - 0s - loss: 5.3256e-06 - val_loss: 1.1758e-07
Epoch 101/512
512/512 - 0s - loss: 4.9431e-06 - val_loss: 1.0315e-07
Epoch 102/512
512/512 - 0s - loss: 4.4243e-06 - val_loss: 9.1514e-08
Epoch 103/512
512/512 - 0s - loss: 4.1603e-06 - val_loss: 8.1624e-08
Epoch 104/512
512/512 - 0s - loss: 3.8356e-06 - val_loss: 7.2305e-08
Epoch 105/512
512/512 - 0s - loss: 3.4551e-06 - val_loss: 6.4686e-08
Epoch 106/512
512/512 - 0s - loss: 3.2153e-06 - val_loss: 5.7528e-08
Epoch 107/512
512/512 - 0s - loss: 2.9141e-06 - val_loss: 5.1701e-08
Epoch 108/512
512/512 - 0s - loss: 2.6760e-06 - val_loss: 4.6561e-08
Epoch 109/512
512/512 - 0s - loss: 2.4900e-06 - val_loss: 4.2081e-08
Epoch 110/512
512/512 - 0s - loss: 2.2583e-06 - val_loss: 3.7927e-08
Epoch 111/512
512/512 - 0s - loss: 2.0499e-06 - val_loss: 3.4560e-08
Epoch 112/512
512/512 - 0s - loss: 1.9136e-06 - val_loss: 3.1501e-08
Epoch 113/512
512/512 - 0s - loss: 1.7466e-06 - val_loss: 2.8584e-08
Epoch 114/512
512/512 - 0s - loss: 1.5740e-06 - val_loss: 2.6260e-08
Epoch 115/512
512/512 - 0s - loss: 1.4608e-06 - val_loss: 2.4311e-08
Epoch 116/512
512/512 - 0s - loss: 1.3677e-06 - val_loss: 2.2079e-08
Epoch 117/512
512/512 - 0s - loss: 1.1893e-06 - val_loss: 2.0477e-08
Epoch 118/512
512/512 - 0s - loss: 1.1256e-06 - val_loss: 1.9006e-08
Epoch 119/512
512/512 - 0s - loss: 1.0377e-06 - val_loss: 1.7724e-08
Epoch 120/512
512/512 - 0s - loss: 9.4615e-07 - val_loss: 1.6286e-08
Epoch 121/512
512/512 - 0s - loss: 8.4296e-07 - val_loss: 1.5246e-08
Epoch 122/512
512/512 - 0s - loss: 7.8697e-07 - val_loss: 1.4304e-08
Epoch 123/512
512/512 - 0s - loss: 7.3379e-07 - val_loss: 1.3331e-08
Epoch 124/512
512/512 - 0s - loss: 6.5692e-07 - val_loss: 1.2515e-08
Epoch 125/512
512/512 - 0s - loss: 5.9727e-07 - val_loss: 1.1784e-08
Epoch 126/512
512/512 - 0s - loss: 5.5302e-07 - val_loss: 1.1140e-08
Epoch 127/512
512/512 - 0s - loss: 5.0717e-07 - val_loss: 1.0569e-08
Epoch 128/512
512/512 - 0s - loss: 4.7474e-07 - val_loss: 9.9196e-09
Epoch 129/512
512/512 - 0s - loss: 4.1346e-07 - val_loss: 9.4103e-09
Epoch 130/512
512/512 - 0s - loss: 3.8788e-07 - val_loss: 9.0126e-09
Epoch 131/512
512/512 - 0s - loss: 3.6518e-07 - val_loss: 8.5321e-09
Epoch 132/512
512/512 - 0s - loss: 3.2348e-07 - val_loss: 8.0973e-09
Epoch 133/512
512/512 - 0s - loss: 2.9522e-07 - val_loss: 7.7464e-09
Epoch 134/512
512/512 - 0s - loss: 2.7590e-07 - val_loss: 7.4382e-09
Epoch 135/512
512/512 - 0s - loss: 2.5580e-07 - val_loss: 7.0779e-09
Epoch 136/512
512/512 - 0s - loss: 2.2535e-07 - val_loss: 6.7971e-09
Epoch 137/512
512/512 - 0s - loss: 2.1538e-07 - val_loss: 6.5188e-09
Epoch 138/512
512/512 - 0s - loss: 1.9361e-07 - val_loss: 6.2510e-09
Epoch 139/512
512/512 - 0s - loss: 1.7783e-07 - val_loss: 5.9939e-09
Epoch 140/512
512/512 - 0s - loss: 1.6178e-07 - val_loss: 5.7732e-09
Epoch 141/512
512/512 - 0s - loss: 1.5088e-07 - val_loss: 5.5691e-09
Epoch 142/512
512/512 - 0s - loss: 1.4024e-07 - val_loss: 5.3557e-09
Epoch 143/512
512/512 - 0s - loss: 1.2520e-07 - val_loss: 5.1489e-09
Epoch 144/512
512/512 - 0s - loss: 1.1570e-07 - val_loss: 4.9812e-09
Epoch 145/512
512/512 - 0s - loss: 1.0806e-07 - val_loss: 4.8249e-09
Epoch 146/512
512/512 - 0s - loss: 9.9630e-08 - val_loss: 4.6573e-09
Epoch 147/512
512/512 - 0s - loss: 9.0652e-08 - val_loss: 4.4929e-09
Epoch 148/512
512/512 - 0s - loss: 8.3517e-08 - val_loss: 4.3426e-09
Epoch 149/512
512/512 - 0s - loss: 7.6016e-08 - val_loss: 4.2163e-09
Epoch 150/512
512/512 - 0s - loss: 7.2815e-08 - val_loss: 4.0817e-09
Epoch 151/512
512/512 - 0s - loss: 6.5533e-08 - val_loss: 3.9537e-09
Epoch 152/512
512/512 - 0s - loss: 6.0516e-08 - val_loss: 3.8297e-09
Epoch 153/512
512/512 - 0s - loss: 5.6195e-08 - val_loss: 3.7086e-09
Epoch 154/512
512/512 - 0s - loss: 5.1678e-08 - val_loss: 3.5993e-09
Epoch 155/512
512/512 - 0s - loss: 4.8032e-08 - val_loss: 3.4967e-09
Epoch 156/512
512/512 - 0s - loss: 4.4616e-08 - val_loss: 3.3864e-09
Epoch 157/512
512/512 - 0s - loss: 4.1231e-08 - val_loss: 3.2891e-09
Epoch 158/512
512/512 - 0s - loss: 3.7883e-08 - val_loss: 3.1959e-09
Epoch 159/512
512/512 - 0s - loss: 3.5982e-08 - val_loss: 3.1045e-09
Epoch 160/512
512/512 - 0s - loss: 3.3056e-08 - val_loss: 3.0128e-09
Epoch 161/512
512/512 - 0s - loss: 3.0443e-08 - val_loss: 2.9261e-09
Epoch 162/512
512/512 - 0s - loss: 2.8331e-08 - val_loss: 2.8443e-09
Epoch 163/512
512/512 - 0s - loss: 2.6990e-08 - val_loss: 2.7645e-09
Epoch 164/512
512/512 - 0s - loss: 2.4530e-08 - val_loss: 2.6830e-09
Epoch 165/512
512/512 - 0s - loss: 2.3015e-08 - val_loss: 2.6060e-09
Epoch 166/512
512/512 - 0s - loss: 2.1583e-08 - val_loss: 2.5354e-09
Epoch 167/512
512/512 - 0s - loss: 1.9915e-08 - val_loss: 2.4635e-09
Epoch 168/512
512/512 - 0s - loss: 1.8980e-08 - val_loss: 2.3984e-09
Epoch 169/512
512/512 - 0s - loss: 1.7685e-08 - val_loss: 2.3276e-09
Epoch 170/512
512/512 - 0s - loss: 1.6414e-08 - val_loss: 2.2617e-09
Epoch 171/512
512/512 - 0s - loss: 1.5452e-08 - val_loss: 2.1982e-09
Epoch 172/512
512/512 - 0s - loss: 1.4442e-08 - val_loss: 2.1367e-09
Epoch 173/512
512/512 - 0s - loss: 1.3566e-08 - val_loss: 2.0782e-09
Epoch 174/512
512/512 - 0s - loss: 1.2941e-08 - val_loss: 2.0188e-09
Epoch 175/512
512/512 - 0s - loss: 1.1860e-08 - val_loss: 1.9622e-09
Epoch 176/512
512/512 - 0s - loss: 1.1245e-08 - val_loss: 1.9096e-09
Epoch 177/512
512/512 - 0s - loss: 1.0726e-08 - val_loss: 1.8566e-09
Epoch 178/512
512/512 - 0s - loss: 1.0133e-08 - val_loss: 1.8028e-09
Epoch 179/512
512/512 - 0s - loss: 9.4696e-09 - val_loss: 1.7505e-09
Epoch 180/512
512/512 - 0s - loss: 8.7936e-09 - val_loss: 1.7022e-09
Epoch 181/512
512/512 - 0s - loss: 8.3993e-09 - val_loss: 1.6552e-09
Epoch 182/512
512/512 - 0s - loss: 7.9624e-09 - val_loss: 1.6102e-09
Epoch 183/512
512/512 - 0s - loss: 7.5556e-09 - val_loss: 1.5659e-09
Epoch 184/512
512/512 - 0s - loss: 7.2435e-09 - val_loss: 1.5223e-09
Epoch 185/512
512/512 - 0s - loss: 6.7412e-09 - val_loss: 1.4783e-09
Epoch 186/512
512/512 - 0s - loss: 6.3338e-09 - val_loss: 1.4389e-09
Epoch 187/512
512/512 - 0s - loss: 6.2252e-09 - val_loss: 1.3985e-09
Epoch 188/512
512/512 - 0s - loss: 5.7696e-09 - val_loss: 1.3587e-09
Epoch 189/512
512/512 - 0s - loss: 5.4031e-09 - val_loss: 1.3221e-09
Epoch 190/512
512/512 - 0s - loss: 5.2803e-09 - val_loss: 1.2862e-09
Epoch 191/512
512/512 - 0s - loss: 4.9186e-09 - val_loss: 1.2510e-09
Epoch 192/512
512/512 - 0s - loss: 4.7627e-09 - val_loss: 1.2176e-09
Epoch 193/512
512/512 - 0s - loss: 4.5675e-09 - val_loss: 1.1842e-09
Epoch 194/512
512/512 - 0s - loss: 4.2786e-09 - val_loss: 1.1514e-09
Epoch 195/512
512/512 - 0s - loss: 4.1245e-09 - val_loss: 1.1200e-09
Epoch 196/512
512/512 - 0s - loss: 3.9280e-09 - val_loss: 1.0891e-09
Epoch 197/512
512/512 - 0s - loss: 3.7241e-09 - val_loss: 1.0603e-09
Epoch 198/512
512/512 - 0s - loss: 3.5691e-09 - val_loss: 1.0320e-09
Epoch 199/512
512/512 - 0s - loss: 3.4412e-09 - val_loss: 1.0056e-09
Epoch 200/512
512/512 - 0s - loss: 3.3564e-09 - val_loss: 9.7852e-10
Epoch 201/512
512/512 - 0s - loss: 3.1615e-09 - val_loss: 9.5231e-10
Epoch 202/512
512/512 - 0s - loss: 3.0293e-09 - val_loss: 9.2686e-10
Epoch 203/512
512/512 - 0s - loss: 2.8979e-09 - val_loss: 9.0240e-10
Epoch 204/512
512/512 - 0s - loss: 2.7886e-09 - val_loss: 8.7888e-10
Epoch 205/512
512/512 - 0s - loss: 2.6752e-09 - val_loss: 8.5679e-10
Epoch 206/512
512/512 - 0s - loss: 2.6141e-09 - val_loss: 8.3395e-10
Epoch 207/512
512/512 - 0s - loss: 2.4844e-09 - val_loss: 8.1233e-10
Epoch 208/512
512/512 - 0s - loss: 2.3773e-09 - val_loss: 7.9132e-10
Epoch 209/512
512/512 - 0s - loss: 2.3150e-09 - val_loss: 7.7126e-10
Epoch 210/512
512/512 - 0s - loss: 2.2119e-09 - val_loss: 7.5142e-10
Epoch 211/512
512/512 - 0s - loss: 2.1348e-09 - val_loss: 7.3219e-10
Epoch 212/512
512/512 - 0s - loss: 2.0485e-09 - val_loss: 7.1429e-10
Epoch 213/512
512/512 - 0s - loss: 2.0100e-09 - val_loss: 6.9678e-10
Epoch 214/512
512/512 - 0s - loss: 1.9492e-09 - val_loss: 6.7898e-10
Epoch 215/512
512/512 - 0s - loss: 1.8759e-09 - val_loss: 6.6126e-10
Epoch 216/512
512/512 - 0s - loss: 1.7796e-09 - val_loss: 6.4492e-10
Epoch 217/512
512/512 - 0s - loss: 1.7382e-09 - val_loss: 6.2951e-10
Epoch 218/512
512/512 - 0s - loss: 1.6760e-09 - val_loss: 6.1353e-10
Epoch 219/512
512/512 - 0s - loss: 1.6278e-09 - val_loss: 5.9902e-10
Epoch 220/512
512/512 - 0s - loss: 1.5884e-09 - val_loss: 5.8417e-10
Epoch 221/512
512/512 - 0s - loss: 1.5287e-09 - val_loss: 5.6950e-10
Epoch 222/512
512/512 - 0s - loss: 1.4620e-09 - val_loss: 5.5599e-10
Epoch 223/512
512/512 - 0s - loss: 1.4371e-09 - val_loss: 5.4275e-10
Epoch 224/512
512/512 - 0s - loss: 1.4044e-09 - val_loss: 5.2972e-10
Epoch 225/512
512/512 - 0s - loss: 1.3549e-09 - val_loss: 5.1735e-10
Epoch 226/512
512/512 - 0s - loss: 1.3298e-09 - val_loss: 5.0501e-10
Epoch 227/512
512/512 - 0s - loss: 1.2841e-09 - val_loss: 4.9296e-10
Epoch 228/512
512/512 - 0s - loss: 1.2366e-09 - val_loss: 4.8116e-10
Epoch 229/512
512/512 - 0s - loss: 1.2062e-09 - val_loss: 4.7005e-10
Epoch 230/512
512/512 - 0s - loss: 1.1757e-09 - val_loss: 4.5916e-10
Epoch 231/512
512/512 - 0s - loss: 1.1479e-09 - val_loss: 4.4827e-10
Epoch 232/512
512/512 - 0s - loss: 1.1070e-09 - val_loss: 4.3795e-10
Epoch 233/512
512/512 - 0s - loss: 1.0820e-09 - val_loss: 4.2790e-10
Epoch 234/512
512/512 - 0s - loss: 1.0586e-09 - val_loss: 4.1826e-10
Epoch 235/512
512/512 - 0s - loss: 1.0349e-09 - val_loss: 4.0868e-10
Epoch 236/512
512/512 - 0s - loss: 1.0071e-09 - val_loss: 3.9927e-10
Epoch 237/512
512/512 - 0s - loss: 9.7770e-10 - val_loss: 3.9044e-10
Epoch 238/512
512/512 - 0s - loss: 9.6848e-10 - val_loss: 3.8159e-10
Epoch 239/512
512/512 - 0s - loss: 9.3666e-10 - val_loss: 3.7298e-10
Epoch 240/512
512/512 - 0s - loss: 9.1071e-10 - val_loss: 3.6463e-10
Epoch 241/512
512/512 - 0s - loss: 8.9920e-10 - val_loss: 3.5659e-10
Epoch 242/512
512/512 - 0s - loss: 8.7689e-10 - val_loss: 3.4869e-10
Epoch 243/512
512/512 - 0s - loss: 8.5402e-10 - val_loss: 3.4088e-10
Epoch 244/512
512/512 - 0s - loss: 8.2250e-10 - val_loss: 3.3328e-10
Epoch 245/512
512/512 - 0s - loss: 8.0473e-10 - val_loss: 3.2609e-10
Epoch 246/512
512/512 - 0s - loss: 7.9131e-10 - val_loss: 3.1916e-10
Epoch 247/512
512/512 - 0s - loss: 7.9244e-10 - val_loss: 3.1254e-10
Epoch 248/512
512/512 - 0s - loss: 7.8364e-10 - val_loss: 3.0571e-10
Epoch 249/512
512/512 - 0s - loss: 7.5461e-10 - val_loss: 2.9867e-10
Epoch 250/512
512/512 - 0s - loss: 7.1643e-10 - val_loss: 2.9187e-10
Epoch 251/512
512/512 - 0s - loss: 6.8979e-10 - val_loss: 2.8561e-10
Epoch 252/512
512/512 - 0s - loss: 6.7628e-10 - val_loss: 2.7985e-10
Epoch 253/512
512/512 - 0s - loss: 6.7194e-10 - val_loss: 2.7405e-10
Epoch 254/512
512/512 - 0s - loss: 6.6886e-10 - val_loss: 2.6837e-10
Epoch 255/512
512/512 - 0s - loss: 6.6483e-10 - val_loss: 2.6300e-10
Epoch 256/512
512/512 - 0s - loss: 6.5965e-10 - val_loss: 2.5737e-10
Epoch 257/512
512/512 - 0s - loss: 6.4089e-10 - val_loss: 2.5205e-10
Epoch 258/512
512/512 - 0s - loss: 6.2584e-10 - val_loss: 2.4683e-10
Epoch 259/512
512/512 - 0s - loss: 6.1275e-10 - val_loss: 2.4157e-10
Epoch 260/512
512/512 - 0s - loss: 5.9520e-10 - val_loss: 2.3648e-10
Epoch 261/512
512/512 - 0s - loss: 5.8654e-10 - val_loss: 2.3173e-10
Epoch 262/512
512/512 - 0s - loss: 5.7384e-10 - val_loss: 2.2701e-10
Epoch 263/512
512/512 - 0s - loss: 5.6268e-10 - val_loss: 2.2243e-10
Epoch 264/512
512/512 - 0s - loss: 5.5499e-10 - val_loss: 2.1800e-10
Epoch 265/512
512/512 - 0s - loss: 5.4538e-10 - val_loss: 2.1358e-10
Epoch 266/512
512/512 - 0s - loss: 5.3995e-10 - val_loss: 2.0937e-10
Epoch 267/512
512/512 - 0s - loss: 5.2677e-10 - val_loss: 2.0521e-10
Epoch 268/512
512/512 - 0s - loss: 5.1646e-10 - val_loss: 2.0112e-10
Epoch 269/512
512/512 - 0s - loss: 5.0651e-10 - val_loss: 1.9721e-10
Epoch 270/512
512/512 - 0s - loss: 5.0160e-10 - val_loss: 1.9335e-10
Epoch 271/512
512/512 - 0s - loss: 4.8831e-10 - val_loss: 1.8955e-10
Epoch 272/512
512/512 - 0s - loss: 4.8161e-10 - val_loss: 1.8582e-10
Epoch 273/512
512/512 - 0s - loss: 4.7457e-10 - val_loss: 1.8224e-10
Epoch 274/512
512/512 - 0s - loss: 4.6528e-10 - val_loss: 1.7868e-10
Epoch 275/512
512/512 - 0s - loss: 4.5101e-10 - val_loss: 1.7521e-10
Epoch 276/512
512/512 - 0s - loss: 4.4754e-10 - val_loss: 1.7201e-10
Epoch 277/512
512/512 - 0s - loss: 4.3745e-10 - val_loss: 1.6878e-10
Epoch 278/512
512/512 - 0s - loss: 4.3772e-10 - val_loss: 1.6558e-10
Epoch 279/512
512/512 - 0s - loss: 4.2773e-10 - val_loss: 1.6252e-10
Epoch 280/512
512/512 - 0s - loss: 4.2185e-10 - val_loss: 1.5949e-10
Epoch 281/512
512/512 - 0s - loss: 4.1328e-10 - val_loss: 1.5663e-10
Epoch 282/512
512/512 - 0s - loss: 4.1486e-10 - val_loss: 1.5375e-10
Epoch 283/512
512/512 - 0s - loss: 4.0967e-10 - val_loss: 1.5093e-10
Epoch 284/512
512/512 - 0s - loss: 4.0416e-10 - val_loss: 1.4818e-10
Epoch 285/512
512/512 - 0s - loss: 3.9372e-10 - val_loss: 1.4550e-10
Epoch 286/512
512/512 - 0s - loss: 3.7907e-10 - val_loss: 1.4290e-10
Epoch 287/512
512/512 - 0s - loss: 3.7593e-10 - val_loss: 1.4036e-10
Epoch 288/512
512/512 - 0s - loss: 3.7118e-10 - val_loss: 1.3791e-10
Epoch 289/512
512/512 - 0s - loss: 3.6855e-10 - val_loss: 1.3554e-10
Epoch 290/512
512/512 - 0s - loss: 3.7092e-10 - val_loss: 1.3318e-10
Epoch 291/512
512/512 - 0s - loss: 3.6338e-10 - val_loss: 1.3088e-10
Epoch 292/512
512/512 - 0s - loss: 3.5195e-10 - val_loss: 1.2867e-10
Epoch 293/512
512/512 - 0s - loss: 3.5166e-10 - val_loss: 1.2649e-10
Epoch 294/512
512/512 - 0s - loss: 3.4678e-10 - val_loss: 1.2437e-10
Epoch 295/512
512/512 - 0s - loss: 3.3934e-10 - val_loss: 1.2230e-10
Epoch 296/512
512/512 - 0s - loss: 3.3736e-10 - val_loss: 1.2028e-10
Epoch 297/512
512/512 - 0s - loss: 3.3798e-10 - val_loss: 1.1835e-10
Epoch 298/512
512/512 - 0s - loss: 3.3305e-10 - val_loss: 1.1636e-10
Epoch 299/512
512/512 - 0s - loss: 3.2637e-10 - val_loss: 1.1456e-10
Epoch 300/512
512/512 - 0s - loss: 3.1751e-10 - val_loss: 1.1269e-10
Epoch 301/512
512/512 - 0s - loss: 3.1159e-10 - val_loss: 1.1087e-10
Epoch 302/512
512/512 - 0s - loss: 3.0246e-10 - val_loss: 1.0911e-10
Epoch 303/512
512/512 - 0s - loss: 2.9830e-10 - val_loss: 1.0745e-10
Epoch 304/512
512/512 - 0s - loss: 2.9932e-10 - val_loss: 1.0580e-10
Epoch 305/512
512/512 - 0s - loss: 2.9907e-10 - val_loss: 1.0420e-10
Epoch 306/512
512/512 - 0s - loss: 2.9512e-10 - val_loss: 1.0263e-10
Epoch 307/512
512/512 - 0s - loss: 2.9206e-10 - val_loss: 1.0112e-10
Epoch 308/512
512/512 - 0s - loss: 2.9239e-10 - val_loss: 9.9655e-11
Epoch 309/512
512/512 - 0s - loss: 2.9094e-10 - val_loss: 9.8179e-11
Epoch 310/512
512/512 - 0s - loss: 2.8664e-10 - val_loss: 9.6748e-11
Epoch 311/512
512/512 - 0s - loss: 2.8177e-10 - val_loss: 9.5374e-11
Epoch 312/512
512/512 - 0s - loss: 2.7740e-10 - val_loss: 9.4054e-11
Epoch 313/512
512/512 - 0s - loss: 2.6807e-10 - val_loss: 9.2802e-11
Epoch 314/512
512/512 - 0s - loss: 2.6257e-10 - val_loss: 9.1504e-11
Epoch 315/512
512/512 - 0s - loss: 2.5977e-10 - val_loss: 9.0298e-11
Epoch 316/512
512/512 - 0s - loss: 2.5723e-10 - val_loss: 8.9047e-11
Epoch 317/512
512/512 - 0s - loss: 2.5654e-10 - val_loss: 8.7843e-11
Epoch 318/512
512/512 - 0s - loss: 2.5709e-10 - val_loss: 8.6758e-11
Epoch 319/512
512/512 - 0s - loss: 2.5444e-10 - val_loss: 8.5640e-11
Epoch 320/512
512/512 - 0s - loss: 2.4889e-10 - val_loss: 8.4583e-11
Epoch 321/512
512/512 - 0s - loss: 2.4674e-10 - val_loss: 8.3511e-11
Epoch 322/512
512/512 - 0s - loss: 2.4823e-10 - val_loss: 8.2469e-11
Epoch 323/512
512/512 - 0s - loss: 2.4625e-10 - val_loss: 8.1508e-11
Epoch 324/512
512/512 - 0s - loss: 2.4331e-10 - val_loss: 8.0558e-11
Epoch 325/512
512/512 - 0s - loss: 2.3830e-10 - val_loss: 7.9610e-11
Epoch 326/512
512/512 - 0s - loss: 2.3704e-10 - val_loss: 7.8662e-11
Epoch 327/512
512/512 - 0s - loss: 2.3542e-10 - val_loss: 7.7805e-11
Epoch 328/512
512/512 - 0s - loss: 2.3196e-10 - val_loss: 7.6957e-11
Epoch 329/512
512/512 - 0s - loss: 2.2665e-10 - val_loss: 7.6130e-11
Epoch 330/512
512/512 - 0s - loss: 2.2275e-10 - val_loss: 7.5336e-11
Epoch 331/512
512/512 - 0s - loss: 2.2182e-10 - val_loss: 7.4541e-11
Epoch 332/512
512/512 - 0s - loss: 2.2119e-10 - val_loss: 7.3750e-11
Epoch 333/512
512/512 - 0s - loss: 2.2191e-10 - val_loss: 7.2973e-11
Epoch 334/512
512/512 - 0s - loss: 2.2354e-10 - val_loss: 7.2246e-11
Epoch 335/512
512/512 - 0s - loss: 2.2261e-10 - val_loss: 7.1514e-11
Epoch 336/512
512/512 - 0s - loss: 2.2143e-10 - val_loss: 7.0885e-11
Epoch 337/512
512/512 - 0s - loss: 2.1447e-10 - val_loss: 7.0247e-11
Epoch 338/512
512/512 - 0s - loss: 2.1123e-10 - val_loss: 6.9594e-11
Epoch 339/512
512/512 - 0s - loss: 2.0939e-10 - val_loss: 6.9036e-11
Epoch 340/512
512/512 - 0s - loss: 2.0546e-10 - val_loss: 6.8436e-11
Epoch 341/512
512/512 - 0s - loss: 2.0108e-10 - val_loss: 6.7860e-11
Epoch 342/512
512/512 - 0s - loss: 2.0088e-10 - val_loss: 6.7293e-11
Epoch 343/512
512/512 - 0s - loss: 1.9659e-10 - val_loss: 6.6818e-11
Epoch 344/512
512/512 - 0s - loss: 1.9375e-10 - val_loss: 6.6252e-11
Epoch 345/512
512/512 - 0s - loss: 1.9260e-10 - val_loss: 6.5778e-11
Epoch 346/512
512/512 - 0s - loss: 1.9228e-10 - val_loss: 6.5265e-11
Epoch 347/512
512/512 - 0s - loss: 1.8958e-10 - val_loss: 6.4819e-11
Epoch 348/512
512/512 - 0s - loss: 1.8846e-10 - val_loss: 6.4339e-11
Epoch 349/512
512/512 - 0s - loss: 1.8529e-10 - val_loss: 6.3931e-11
Epoch 350/512
512/512 - 0s - loss: 1.8397e-10 - val_loss: 6.3517e-11
Epoch 351/512
512/512 - 0s - loss: 1.8255e-10 - val_loss: 6.3108e-11
Epoch 352/512
512/512 - 0s - loss: 1.8122e-10 - val_loss: 6.2666e-11
Epoch 353/512
512/512 - 0s - loss: 1.7996e-10 - val_loss: 6.2307e-11
Epoch 354/512
512/512 - 0s - loss: 1.7776e-10 - val_loss: 6.1945e-11
Epoch 355/512
512/512 - 0s - loss: 1.7822e-10 - val_loss: 6.1557e-11
Epoch 356/512
512/512 - 0s - loss: 1.7962e-10 - val_loss: 6.1202e-11
Epoch 357/512
512/512 - 0s - loss: 1.7995e-10 - val_loss: 6.0799e-11
Epoch 358/512
512/512 - 0s - loss: 1.7941e-10 - val_loss: 6.0534e-11
Epoch 359/512
512/512 - 0s - loss: 1.7480e-10 - val_loss: 6.0288e-11
Epoch 360/512
512/512 - 0s - loss: 1.7314e-10 - val_loss: 5.9983e-11
Epoch 361/512
512/512 - 0s - loss: 1.7053e-10 - val_loss: 5.9754e-11
Epoch 362/512
512/512 - 0s - loss: 1.6870e-10 - val_loss: 5.9444e-11
Epoch 363/512
512/512 - 0s - loss: 1.6700e-10 - val_loss: 5.9297e-11
Epoch 364/512
512/512 - 0s - loss: 1.6244e-10 - val_loss: 5.9089e-11
Epoch 365/512
512/512 - 0s - loss: 1.6120e-10 - val_loss: 5.8793e-11
Epoch 366/512
512/512 - 0s - loss: 1.6394e-10 - val_loss: 5.8550e-11
Epoch 367/512
512/512 - 0s - loss: 1.6385e-10 - val_loss: 5.8349e-11
Epoch 368/512
512/512 - 0s - loss: 1.6105e-10 - val_loss: 5.8170e-11
Epoch 369/512
512/512 - 0s - loss: 1.5933e-10 - val_loss: 5.8054e-11
Epoch 370/512
512/512 - 0s - loss: 1.5583e-10 - val_loss: 5.7892e-11
Epoch 371/512
512/512 - 0s - loss: 1.5329e-10 - val_loss: 5.7777e-11
Epoch 372/512
512/512 - 0s - loss: 1.5338e-10 - val_loss: 5.7545e-11
Epoch 373/512
512/512 - 0s - loss: 1.5587e-10 - val_loss: 5.7329e-11
Epoch 374/512
512/512 - 0s - loss: 1.5547e-10 - val_loss: 5.7167e-11
Epoch 375/512
512/512 - 0s - loss: 1.5433e-10 - val_loss: 5.7088e-11
Epoch 376/512
512/512 - 0s - loss: 1.5167e-10 - val_loss: 5.7002e-11
Epoch 377/512
512/512 - 0s - loss: 1.4973e-10 - val_loss: 5.6946e-11
Epoch 378/512
512/512 - 0s - loss: 1.4794e-10 - val_loss: 5.6848e-11
Epoch 379/512
512/512 - 0s - loss: 1.4707e-10 - val_loss: 5.6700e-11
Epoch 380/512
512/512 - 0s - loss: 1.4605e-10 - val_loss: 5.6645e-11
Epoch 381/512
512/512 - 0s - loss: 1.4457e-10 - val_loss: 5.6537e-11
Epoch 382/512
512/512 - 0s - loss: 1.4561e-10 - val_loss: 5.6423e-11
Epoch 383/512
512/512 - 0s - loss: 1.4420e-10 - val_loss: 5.6398e-11
Epoch 384/512
512/512 - 0s - loss: 1.4243e-10 - val_loss: 5.6420e-11
Epoch 385/512
512/512 - 0s - loss: 1.4176e-10 - val_loss: 5.6310e-11
Epoch 386/512
512/512 - 0s - loss: 1.4228e-10 - val_loss: 5.6187e-11
Epoch 387/512
512/512 - 0s - loss: 1.4330e-10 - val_loss: 5.6187e-11
Epoch 388/512
512/512 - 0s - loss: 1.4128e-10 - val_loss: 5.6158e-11
Epoch 389/512
512/512 - 0s - loss: 1.4178e-10 - val_loss: 5.6114e-11
Epoch 390/512
512/512 - 0s - loss: 1.3987e-10 - val_loss: 5.6113e-11
Epoch 391/512
512/512 - 0s - loss: 1.3944e-10 - val_loss: 5.6057e-11
Epoch 392/512
512/512 - 0s - loss: 1.3993e-10 - val_loss: 5.6053e-11
Epoch 393/512
512/512 - 0s - loss: 1.3979e-10 - val_loss: 5.6020e-11
Epoch 394/512
512/512 - 0s - loss: 1.3819e-10 - val_loss: 5.6076e-11
Epoch 395/512
512/512 - 0s - loss: 1.3606e-10 - val_loss: 5.6135e-11
Epoch 396/512
512/512 - 0s - loss: 1.3435e-10 - val_loss: 5.6195e-11
Epoch 397/512
512/512 - 0s - loss: 1.3285e-10 - val_loss: 5.6260e-11
Epoch 398/512
512/512 - 0s - loss: 1.3013e-10 - val_loss: 5.6332e-11
Epoch 399/512
512/512 - 0s - loss: 1.2782e-10 - val_loss: 5.6434e-11
Epoch 400/512
512/512 - 0s - loss: 1.2604e-10 - val_loss: 5.6554e-11
Epoch 401/512
512/512 - 0s - loss: 1.2520e-10 - val_loss: 5.6596e-11
Epoch 402/512
512/512 - 0s - loss: 1.2576e-10 - val_loss: 5.6499e-11
Epoch 403/512
512/512 - 0s - loss: 1.2852e-10 - val_loss: 5.6482e-11
Epoch 404/512
512/512 - 0s - loss: 1.2851e-10 - val_loss: 5.6527e-11
Epoch 405/512
512/512 - 0s - loss: 1.2803e-10 - val_loss: 5.6587e-11
Epoch 406/512
512/512 - 0s - loss: 1.2671e-10 - val_loss: 5.6696e-11
Epoch 407/512
512/512 - 0s - loss: 1.2396e-10 - val_loss: 5.6875e-11
Epoch 408/512
512/512 - 0s - loss: 1.2293e-10 - val_loss: 5.6903e-11
Epoch 409/512
512/512 - 0s - loss: 1.2392e-10 - val_loss: 5.6862e-11
Epoch 410/512
512/512 - 0s - loss: 1.2569e-10 - val_loss: 5.6892e-11
Epoch 411/512
512/512 - 0s - loss: 1.2556e-10 - val_loss: 5.7015e-11
Epoch 412/512
512/512 - 0s - loss: 1.2260e-10 - val_loss: 5.7229e-11
Epoch 413/512
512/512 - 0s - loss: 1.2118e-10 - val_loss: 5.7304e-11
Epoch 414/512
512/512 - 0s - loss: 1.1907e-10 - val_loss: 5.7531e-11
Epoch 415/512
512/512 - 0s - loss: 1.1632e-10 - val_loss: 5.7711e-11
Epoch 416/512
512/512 - 0s - loss: 1.1463e-10 - val_loss: 5.7911e-11
Epoch 417/512
512/512 - 0s - loss: 1.1214e-10 - val_loss: 5.8012e-11
Epoch 418/512
512/512 - 0s - loss: 1.1269e-10 - val_loss: 5.8099e-11
Epoch 419/512
512/512 - 0s - loss: 1.1261e-10 - val_loss: 5.8246e-11
Epoch 420/512
512/512 - 0s - loss: 1.1048e-10 - val_loss: 5.8376e-11
Epoch 421/512
512/512 - 0s - loss: 1.1016e-10 - val_loss: 5.8511e-11
Epoch 422/512
512/512 - 0s - loss: 1.0955e-10 - val_loss: 5.8596e-11
Epoch 423/512
512/512 - 0s - loss: 1.1021e-10 - val_loss: 5.8631e-11
Epoch 424/512
512/512 - 0s - loss: 1.1198e-10 - val_loss: 5.8651e-11
Epoch 425/512
512/512 - 0s - loss: 1.1313e-10 - val_loss: 5.8707e-11
Epoch 426/512
512/512 - 0s - loss: 1.1356e-10 - val_loss: 5.8920e-11
Epoch 427/512
512/512 - 0s - loss: 1.1265e-10 - val_loss: 5.9051e-11
Epoch 428/512
512/512 - 0s - loss: 1.1181e-10 - val_loss: 5.9160e-11
Epoch 429/512
512/512 - 0s - loss: 1.1329e-10 - val_loss: 5.9248e-11
Epoch 430/512
512/512 - 0s - loss: 1.1210e-10 - val_loss: 5.9401e-11
Epoch 431/512
512/512 - 0s - loss: 1.1112e-10 - val_loss: 5.9545e-11
Epoch 432/512
512/512 - 0s - loss: 1.1070e-10 - val_loss: 5.9742e-11
Epoch 433/512
512/512 - 0s - loss: 1.0884e-10 - val_loss: 5.9999e-11
Epoch 434/512
512/512 - 0s - loss: 1.0699e-10 - val_loss: 6.0188e-11
Epoch 435/512
512/512 - 0s - loss: 1.0632e-10 - val_loss: 6.0315e-11
Epoch 436/512
512/512 - 0s - loss: 1.0665e-10 - val_loss: 6.0394e-11
Epoch 437/512
512/512 - 0s - loss: 1.0717e-10 - val_loss: 6.0524e-11
Epoch 438/512
512/512 - 0s - loss: 1.0748e-10 - val_loss: 6.0682e-11
Epoch 439/512
512/512 - 0s - loss: 1.0666e-10 - val_loss: 6.0801e-11
Epoch 440/512
512/512 - 0s - loss: 1.0682e-10 - val_loss: 6.1009e-11
Epoch 441/512
512/512 - 0s - loss: 1.0527e-10 - val_loss: 6.1186e-11
Epoch 442/512
512/512 - 0s - loss: 1.0430e-10 - val_loss: 6.1517e-11
Epoch 443/512
512/512 - 0s - loss: 1.0158e-10 - val_loss: 6.1751e-11
Epoch 444/512
512/512 - 0s - loss: 1.0118e-10 - val_loss: 6.1816e-11
Epoch 445/512
512/512 - 0s - loss: 1.0100e-10 - val_loss: 6.2053e-11
Epoch 446/512
512/512 - 0s - loss: 9.9945e-11 - val_loss: 6.2294e-11
Epoch 447/512
512/512 - 0s - loss: 9.9648e-11 - val_loss: 6.2349e-11
Epoch 448/512
512/512 - 0s - loss: 9.9838e-11 - val_loss: 6.2619e-11
Epoch 449/512
512/512 - 0s - loss: 9.8160e-11 - val_loss: 6.2842e-11
Epoch 450/512
512/512 - 0s - loss: 9.7257e-11 - val_loss: 6.2925e-11
Epoch 451/512
512/512 - 0s - loss: 9.8151e-11 - val_loss: 6.3141e-11
Epoch 452/512
512/512 - 0s - loss: 9.6541e-11 - val_loss: 6.3466e-11
Epoch 453/512
512/512 - 0s - loss: 9.4214e-11 - val_loss: 6.3749e-11
Epoch 454/512
512/512 - 0s - loss: 9.2255e-11 - val_loss: 6.3975e-11
Epoch 455/512
512/512 - 0s - loss: 9.2398e-11 - val_loss: 6.4083e-11
Epoch 456/512
512/512 - 0s - loss: 9.3312e-11 - val_loss: 6.4262e-11
Epoch 457/512
512/512 - 0s - loss: 9.3033e-11 - val_loss: 6.4348e-11
Epoch 458/512
512/512 - 0s - loss: 9.4142e-11 - val_loss: 6.4396e-11
Epoch 459/512
512/512 - 0s - loss: 9.5137e-11 - val_loss: 6.4581e-11
Epoch 460/512
512/512 - 0s - loss: 9.4350e-11 - val_loss: 6.4791e-11
Epoch 461/512
512/512 - 0s - loss: 9.4635e-11 - val_loss: 6.4835e-11
Epoch 462/512
512/512 - 0s - loss: 9.6459e-11 - val_loss: 6.4965e-11
Epoch 463/512
512/512 - 0s - loss: 9.5696e-11 - val_loss: 6.5213e-11
Epoch 464/512
512/512 - 0s - loss: 9.4123e-11 - val_loss: 6.5581e-11
Epoch 465/512
512/512 - 0s - loss: 9.1363e-11 - val_loss: 6.5884e-11
Epoch 466/512
512/512 - 0s - loss: 8.9489e-11 - val_loss: 6.6113e-11
Epoch 467/512
512/512 - 0s - loss: 8.9623e-11 - val_loss: 6.6305e-11
Epoch 468/512
512/512 - 0s - loss: 8.9085e-11 - val_loss: 6.6409e-11
Epoch 469/512
512/512 - 0s - loss: 9.0190e-11 - val_loss: 6.6488e-11
Epoch 470/512
512/512 - 0s - loss: 9.0911e-11 - val_loss: 6.6633e-11
Epoch 471/512
512/512 - 0s - loss: 9.0994e-11 - val_loss: 6.6839e-11
Epoch 472/512
512/512 - 0s - loss: 9.0740e-11 - val_loss: 6.6956e-11
Epoch 473/512
512/512 - 0s - loss: 9.2090e-11 - val_loss: 6.7102e-11
Epoch 474/512
512/512 - 0s - loss: 9.0984e-11 - val_loss: 6.7375e-11
Epoch 475/512
512/512 - 0s - loss: 8.9458e-11 - val_loss: 6.7647e-11
Epoch 476/512
512/512 - 0s - loss: 8.7985e-11 - val_loss: 6.7859e-11
Epoch 477/512
512/512 - 0s - loss: 8.8549e-11 - val_loss: 6.8027e-11
Epoch 478/512
512/512 - 0s - loss: 8.8256e-11 - val_loss: 6.8228e-11
Epoch 479/512
512/512 - 0s - loss: 8.7544e-11 - val_loss: 6.8455e-11
Epoch 480/512
512/512 - 0s - loss: 8.7252e-11 - val_loss: 6.8621e-11
Epoch 481/512
512/512 - 0s - loss: 8.7595e-11 - val_loss: 6.8727e-11
Epoch 482/512
512/512 - 0s - loss: 8.8254e-11 - val_loss: 6.8899e-11
Epoch 483/512
512/512 - 0s - loss: 8.8216e-11 - val_loss: 6.9028e-11
Epoch 484/512
512/512 - 0s - loss: 8.7636e-11 - val_loss: 6.9301e-11
Epoch 485/512
512/512 - 0s - loss: 8.6848e-11 - val_loss: 6.9491e-11
Epoch 486/512
512/512 - 0s - loss: 8.6983e-11 - val_loss: 6.9635e-11
Epoch 487/512
512/512 - 0s - loss: 8.7290e-11 - val_loss: 6.9759e-11
Epoch 488/512
512/512 - 0s - loss: 8.7422e-11 - val_loss: 7.0009e-11
Epoch 489/512
512/512 - 0s - loss: 8.6269e-11 - val_loss: 7.0296e-11
Epoch 490/512
512/512 - 0s - loss: 8.4374e-11 - val_loss: 7.0597e-11
Epoch 491/512
512/512 - 0s - loss: 8.3040e-11 - val_loss: 7.0908e-11
Epoch 492/512
512/512 - 0s - loss: 8.2199e-11 - val_loss: 7.1057e-11
Epoch 493/512
512/512 - 0s - loss: 8.1824e-11 - val_loss: 7.1275e-11
Epoch 494/512
512/512 - 0s - loss: 8.2588e-11 - val_loss: 7.1354e-11
Epoch 495/512
512/512 - 0s - loss: 8.3117e-11 - val_loss: 7.1400e-11
Epoch 496/512
512/512 - 0s - loss: 8.3851e-11 - val_loss: 7.1608e-11
Epoch 497/512
512/512 - 0s - loss: 8.3438e-11 - val_loss: 7.1747e-11
Epoch 498/512
512/512 - 0s - loss: 8.4215e-11 - val_loss: 7.1888e-11
Epoch 499/512
512/512 - 0s - loss: 8.4469e-11 - val_loss: 7.1986e-11
Epoch 500/512
512/512 - 0s - loss: 8.5447e-11 - val_loss: 7.2046e-11
Epoch 501/512
512/512 - 0s - loss: 8.6020e-11 - val_loss: 7.2263e-11
Epoch 502/512
512/512 - 0s - loss: 8.4521e-11 - val_loss: 7.2562e-11
Epoch 503/512
512/512 - 0s - loss: 8.3568e-11 - val_loss: 7.2875e-11
Epoch 504/512
512/512 - 0s - loss: 8.2736e-11 - val_loss: 7.2916e-11
Epoch 505/512
512/512 - 0s - loss: 8.3649e-11 - val_loss: 7.3093e-11
Epoch 506/512
512/512 - 0s - loss: 8.2797e-11 - val_loss: 7.3304e-11
Epoch 507/512
512/512 - 0s - loss: 8.2966e-11 - val_loss: 7.3474e-11
Epoch 508/512
512/512 - 0s - loss: 8.3336e-11 - val_loss: 7.3516e-11
Epoch 509/512
512/512 - 0s - loss: 8.4182e-11 - val_loss: 7.3635e-11
Epoch 510/512
512/512 - 0s - loss: 8.4665e-11 - val_loss: 7.3779e-11
Epoch 511/512
512/512 - 0s - loss: 8.4766e-11 - val_loss: 7.3900e-11
Epoch 512/512
512/512 - 0s - loss: 8.4979e-11 - val_loss: 7.4091e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.8540e-10 - val_loss: 7.5319e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7477e-08 - val_loss: 7.7492e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.0911e-09 - val_loss: 7.1772e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.5628e-10 - val_loss: 2.1841e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0542e-10 - val_loss: 2.2190e-10
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1707e-10 - val_loss: 6.0303e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2087e-09 - val_loss: 2.7763e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4812e-09 - val_loss: 5.2424e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6351e-09 - val_loss: 2.4976e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9566e-09 - val_loss: 1.1623e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0643e-09 - val_loss: 9.4059e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0623e-09 - val_loss: 1.2655e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5975e-09 - val_loss: 2.0182e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3662e-09 - val_loss: 2.3933e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3784e-09 - val_loss: 1.8911e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7642e-09 - val_loss: 1.3721e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3294e-09 - val_loss: 1.1789e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2344e-09 - val_loss: 1.2442e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3666e-09 - val_loss: 1.4143e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5274e-09 - val_loss: 1.4863e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5363e-09 - val_loss: 1.3932e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3926e-09 - val_loss: 1.2257e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2287e-09 - val_loss: 1.1020e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1247e-09 - val_loss: 1.0684e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1208e-09 - val_loss: 1.0863e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1361e-09 - val_loss: 1.0958e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1358e-09 - val_loss: 1.0676e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0901e-09 - val_loss: 1.0113e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0208e-09 - val_loss: 9.4038e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5900e-10 - val_loss: 8.9725e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2261e-10 - val_loss: 8.7291e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0207e-10 - val_loss: 8.6161e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8917e-10 - val_loss: 8.4231e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6388e-10 - val_loss: 8.2569e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4352e-10 - val_loss: 7.9664e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1400e-10 - val_loss: 7.6852e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8586e-10 - val_loss: 7.3473e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4674e-10 - val_loss: 6.9976e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1581e-10 - val_loss: 6.8020e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9876e-10 - val_loss: 6.6807e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8689e-10 - val_loss: 6.5490e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6748e-10 - val_loss: 6.3842e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5290e-10 - val_loss: 6.2006e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3396e-10 - val_loss: 6.0086e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1239e-10 - val_loss: 5.8147e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9243e-10 - val_loss: 5.6155e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7289e-10 - val_loss: 5.4909e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6208e-10 - val_loss: 5.3902e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4761e-10 - val_loss: 5.2019e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2912e-10 - val_loss: 5.0268e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1170e-10 - val_loss: 4.9177e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0047e-10 - val_loss: 4.8013e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8924e-10 - val_loss: 4.7012e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7768e-10 - val_loss: 4.5868e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6819e-10 - val_loss: 4.4603e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5616e-10 - val_loss: 4.3670e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4320e-10 - val_loss: 4.2440e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3177e-10 - val_loss: 4.1202e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1865e-10 - val_loss: 4.0060e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0793e-10 - val_loss: 3.9272e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9979e-10 - val_loss: 3.8501e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9018e-10 - val_loss: 3.7618e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8369e-10 - val_loss: 3.6905e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7446e-10 - val_loss: 3.5933e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6560e-10 - val_loss: 3.5084e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5437e-10 - val_loss: 3.3540e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3827e-10 - val_loss: 3.2357e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2837e-10 - val_loss: 3.1872e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2751e-10 - val_loss: 3.2152e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2978e-10 - val_loss: 3.2075e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2621e-10 - val_loss: 3.1439e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1842e-10 - val_loss: 3.0523e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0704e-10 - val_loss: 2.9451e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9758e-10 - val_loss: 2.8565e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8778e-10 - val_loss: 2.7652e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8029e-10 - val_loss: 2.7497e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7860e-10 - val_loss: 2.7168e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7625e-10 - val_loss: 2.6862e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7142e-10 - val_loss: 2.6249e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6672e-10 - val_loss: 2.5680e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5866e-10 - val_loss: 2.5009e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5405e-10 - val_loss: 2.4593e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4782e-10 - val_loss: 2.3888e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4166e-10 - val_loss: 2.3254e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3634e-10 - val_loss: 2.2906e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3220e-10 - val_loss: 2.2487e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2779e-10 - val_loss: 2.2135e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2424e-10 - val_loss: 2.2030e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2444e-10 - val_loss: 2.1858e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.2153e-10 - val_loss: 2.1368e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.1657e-10 - val_loss: 2.1105e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.1175e-10 - val_loss: 2.0302e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.0294e-10 - val_loss: 1.9420e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9502e-10 - val_loss: 1.8975e-10
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9165e-10 - val_loss: 1.8555e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8845e-10 - val_loss: 1.8403e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8718e-10 - val_loss: 1.8249e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8491e-10 - val_loss: 1.8052e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8269e-10 - val_loss: 1.7822e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8079e-10 - val_loss: 1.7628e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7856e-10 - val_loss: 1.7202e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7257e-10 - val_loss: 1.6668e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6781e-10 - val_loss: 1.6398e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6590e-10 - val_loss: 1.6192e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6269e-10 - val_loss: 1.5878e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6031e-10 - val_loss: 1.5705e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5945e-10 - val_loss: 1.5598e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5883e-10 - val_loss: 1.5568e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5898e-10 - val_loss: 1.5749e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6029e-10 - val_loss: 1.5779e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5858e-10 - val_loss: 1.5337e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5566e-10 - val_loss: 1.5164e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5318e-10 - val_loss: 1.4783e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4776e-10 - val_loss: 1.4266e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4362e-10 - val_loss: 1.4004e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4116e-10 - val_loss: 1.3683e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3828e-10 - val_loss: 1.3526e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3581e-10 - val_loss: 1.3318e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3509e-10 - val_loss: 1.3270e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3369e-10 - val_loss: 1.3113e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3238e-10 - val_loss: 1.2853e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2962e-10 - val_loss: 1.2627e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2735e-10 - val_loss: 1.2378e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2596e-10 - val_loss: 1.2357e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2501e-10 - val_loss: 1.2284e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2417e-10 - val_loss: 1.2105e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2237e-10 - val_loss: 1.1881e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1925e-10 - val_loss: 1.1641e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1749e-10 - val_loss: 1.1517e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1652e-10 - val_loss: 1.1417e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1507e-10 - val_loss: 1.1215e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1275e-10 - val_loss: 1.0973e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1077e-10 - val_loss: 1.0910e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1077e-10 - val_loss: 1.0996e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1110e-10 - val_loss: 1.0935e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1024e-10 - val_loss: 1.0817e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0881e-10 - val_loss: 1.0583e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0702e-10 - val_loss: 1.0524e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0629e-10 - val_loss: 1.0417e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0438e-10 - val_loss: 1.0073e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0101e-10 - val_loss: 9.6957e-11
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.6769e-11 - val_loss: 9.3239e-11
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.3496e-11 - val_loss: 9.0801e-11
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.1845e-11 - val_loss: 9.0221e-11
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.1280e-11 - val_loss: 8.9262e-11
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.9582e-11 - val_loss: 8.8234e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.9348e-11 - val_loss: 8.7543e-11
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.8208e-11 - val_loss: 8.7483e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8551e-11 - val_loss: 8.7642e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9160e-11 - val_loss: 8.9120e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0928e-11 - val_loss: 8.9664e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0521e-11 - val_loss: 8.8984e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0053e-11 - val_loss: 8.8731e-11
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.9453e-11 - val_loss: 8.7431e-11
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.7957e-11 - val_loss: 8.5493e-11
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.5445e-11 - val_loss: 8.3104e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.3803e-11 - val_loss: 8.1283e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.1681e-11 - val_loss: 8.0526e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.1004e-11 - val_loss: 7.9237e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.9239e-11 - val_loss: 7.7737e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.8614e-11 - val_loss: 7.7477e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.7761e-11 - val_loss: 7.5328e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.5128e-11 - val_loss: 7.2826e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.2763e-11 - val_loss: 7.1670e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.2373e-11 - val_loss: 7.1360e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.1426e-11 - val_loss: 7.0338e-11
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.0552e-11 - val_loss: 6.9583e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0721e-11 - val_loss: 6.9887e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0654e-11 - val_loss: 7.0488e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 7.0666e-11 - val_loss: 6.9332e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.9377e-11 - val_loss: 6.7470e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8437e-11 - val_loss: 6.7676e-11
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.7741e-11 - val_loss: 6.6762e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.6951e-11 - val_loss: 6.6384e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.6520e-11 - val_loss: 6.5211e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.5145e-11 - val_loss: 6.3791e-11
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.4444e-11 - val_loss: 6.2828e-11
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.3253e-11 - val_loss: 6.2122e-11
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.2357e-11 - val_loss: 6.1582e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2205e-11 - val_loss: 6.2334e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.3002e-11 - val_loss: 6.1463e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1929e-11 - val_loss: 6.1901e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.2309e-11 - val_loss: 6.0853e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 6.0653e-11 - val_loss: 5.8898e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.8929e-11 - val_loss: 5.7441e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.7527e-11 - val_loss: 5.6330e-11
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.6487e-11 - val_loss: 5.5451e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6113e-11 - val_loss: 5.5588e-11
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.5894e-11 - val_loss: 5.5008e-11
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.5105e-11 - val_loss: 5.4611e-11
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.5511e-11 - val_loss: 5.4478e-11
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.4855e-11 - val_loss: 5.4292e-11
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.4327e-11 - val_loss: 5.3423e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.3938e-11 - val_loss: 5.3355e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.3354e-11 - val_loss: 5.2328e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.2526e-11 - val_loss: 5.1058e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.1381e-11 - val_loss: 5.0562e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.1339e-11 - val_loss: 5.0419e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1013e-11 - val_loss: 5.0474e-11
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.1117e-11 - val_loss: 5.0326e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 5.0391e-11 - val_loss: 4.9766e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.9955e-11 - val_loss: 4.9165e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9870e-11 - val_loss: 4.9729e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0430e-11 - val_loss: 4.9799e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.9692e-11 - val_loss: 4.8494e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.8691e-11 - val_loss: 4.8198e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.8379e-11 - val_loss: 4.7511e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.8012e-11 - val_loss: 4.7493e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.7399e-11 - val_loss: 4.6320e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6703e-11 - val_loss: 4.6552e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6858e-11 - val_loss: 4.6743e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7524e-11 - val_loss: 4.8157e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9150e-11 - val_loss: 4.9088e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0041e-11 - val_loss: 4.9773e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0031e-11 - val_loss: 4.9172e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9737e-11 - val_loss: 4.9051e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8766e-11 - val_loss: 4.7183e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.7202e-11 - val_loss: 4.6316e-11
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.6583e-11 - val_loss: 4.5355e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.5229e-11 - val_loss: 4.4365e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4782e-11 - val_loss: 4.4723e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5940e-11 - val_loss: 4.5939e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6065e-11 - val_loss: 4.4946e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.4892e-11 - val_loss: 4.3877e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.3952e-11 - val_loss: 4.3093e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.3312e-11 - val_loss: 4.2264e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.2317e-11 - val_loss: 4.1469e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1582e-11 - val_loss: 4.1545e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2179e-11 - val_loss: 4.2391e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2758e-11 - val_loss: 4.2580e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2943e-11 - val_loss: 4.2554e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2845e-11 - val_loss: 4.2072e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.2114e-11 - val_loss: 4.1114e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.1211e-11 - val_loss: 4.0360e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 4.0564e-11 - val_loss: 3.9959e-11
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.9858e-11 - val_loss: 3.9058e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.9129e-11 - val_loss: 3.8445e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8737e-11 - val_loss: 3.8613e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.8844e-11 - val_loss: 3.8357e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9016e-11 - val_loss: 3.9127e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9676e-11 - val_loss: 3.9847e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0355e-11 - val_loss: 4.0206e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1148e-11 - val_loss: 4.1398e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1553e-11 - val_loss: 4.0603e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0903e-11 - val_loss: 4.0469e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0103e-11 - val_loss: 3.9279e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.9111e-11 - val_loss: 3.7913e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.7727e-11 - val_loss: 3.6781e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.7038e-11 - val_loss: 3.6534e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6889e-11 - val_loss: 3.6691e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.6644e-11 - val_loss: 3.6072e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.6161e-11 - val_loss: 3.5759e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.5879e-11 - val_loss: 3.5427e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5637e-11 - val_loss: 3.5449e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.5742e-11 - val_loss: 3.5325e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.5584e-11 - val_loss: 3.5236e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5627e-11 - val_loss: 3.5315e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.5307e-11 - val_loss: 3.5103e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5657e-11 - val_loss: 3.5461e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5621e-11 - val_loss: 3.5887e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.5431e-11 - val_loss: 3.4150e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.3806e-11 - val_loss: 3.2571e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.2518e-11 - val_loss: 3.1803e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.1606e-11 - val_loss: 3.0432e-11
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.0446e-11 - val_loss: 3.0236e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 3.0225e-11 - val_loss: 2.9699e-11
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.9654e-11 - val_loss: 2.9141e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.9303e-11 - val_loss: 2.8791e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8993e-11 - val_loss: 2.8853e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9094e-11 - val_loss: 2.8919e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9116e-11 - val_loss: 2.8972e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9060e-11 - val_loss: 2.9293e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.9329e-11 - val_loss: 2.8681e-11
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.8884e-11 - val_loss: 2.8553e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.8509e-11 - val_loss: 2.8384e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.8654e-11 - val_loss: 2.8201e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.8252e-11 - val_loss: 2.8093e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.8269e-11 - val_loss: 2.7884e-11
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.7882e-11 - val_loss: 2.7798e-11
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.7608e-11 - val_loss: 2.7048e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7442e-11 - val_loss: 2.7112e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7766e-11 - val_loss: 2.7820e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7918e-11 - val_loss: 2.7532e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7771e-11 - val_loss: 2.7378e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7260e-11 - val_loss: 2.7135e-11
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.7110e-11 - val_loss: 2.6804e-11
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.6514e-11 - val_loss: 2.5794e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.5792e-11 - val_loss: 2.4908e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.4679e-11 - val_loss: 2.4047e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.3969e-11 - val_loss: 2.3545e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.3700e-11 - val_loss: 2.3230e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.3385e-11 - val_loss: 2.3011e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.3117e-11 - val_loss: 2.2931e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3201e-11 - val_loss: 2.3417e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3661e-11 - val_loss: 2.3586e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3867e-11 - val_loss: 2.4035e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4017e-11 - val_loss: 2.3983e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4123e-11 - val_loss: 2.4028e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4141e-11 - val_loss: 2.3845e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4001e-11 - val_loss: 2.3777e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3667e-11 - val_loss: 2.3589e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3739e-11 - val_loss: 2.3507e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3398e-11 - val_loss: 2.2950e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.2992e-11 - val_loss: 2.2519e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.2634e-11 - val_loss: 2.2284e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.2237e-11 - val_loss: 2.1984e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2236e-11 - val_loss: 2.2097e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.2050e-11 - val_loss: 2.1553e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1728e-11 - val_loss: 2.1695e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2240e-11 - val_loss: 2.2235e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2250e-11 - val_loss: 2.2332e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2340e-11 - val_loss: 2.2140e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2205e-11 - val_loss: 2.2315e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2417e-11 - val_loss: 2.2345e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2249e-11 - val_loss: 2.1987e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2043e-11 - val_loss: 2.1907e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2022e-11 - val_loss: 2.1708e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.1421e-11 - val_loss: 2.0846e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.1013e-11 - val_loss: 2.0800e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.0990e-11 - val_loss: 2.0630e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 2.0583e-11 - val_loss: 1.9852e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9852e-11 - val_loss: 1.9672e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9944e-11 - val_loss: 1.9764e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9780e-11 - val_loss: 1.9555e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9614e-11 - val_loss: 1.9400e-11
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9393e-11 - val_loss: 1.9223e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9407e-11 - val_loss: 1.9494e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9528e-11 - val_loss: 1.9455e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9388e-11 - val_loss: 1.9058e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.9043e-11 - val_loss: 1.8868e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8860e-11 - val_loss: 1.8606e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8896e-11 - val_loss: 1.8887e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8795e-11 - val_loss: 1.8243e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8415e-11 - val_loss: 1.8148e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8402e-11 - val_loss: 1.8566e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8636e-11 - val_loss: 1.8313e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.8211e-11 - val_loss: 1.7918e-11
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7862e-11 - val_loss: 1.7600e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7523e-11 - val_loss: 1.7303e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7426e-11 - val_loss: 1.7324e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7612e-11 - val_loss: 1.7838e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8122e-11 - val_loss: 1.7861e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7924e-11 - val_loss: 1.7668e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7927e-11 - val_loss: 1.8196e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7898e-11 - val_loss: 1.7596e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7713e-11 - val_loss: 1.7408e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7392e-11 - val_loss: 1.7479e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7387e-11 - val_loss: 1.7227e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7368e-11 - val_loss: 1.7124e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7207e-11 - val_loss: 1.7226e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.7061e-11 - val_loss: 1.6784e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6756e-11 - val_loss: 1.6593e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6559e-11 - val_loss: 1.6390e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6463e-11 - val_loss: 1.6223e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6413e-11 - val_loss: 1.6429e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6535e-11 - val_loss: 1.6615e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6608e-11 - val_loss: 1.6593e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6633e-11 - val_loss: 1.6622e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6735e-11 - val_loss: 1.6563e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6556e-11 - val_loss: 1.6401e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6469e-11 - val_loss: 1.6298e-11
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.6207e-11 - val_loss: 1.5612e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5845e-11 - val_loss: 1.6119e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6020e-11 - val_loss: 1.6019e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6169e-11 - val_loss: 1.6122e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6450e-11 - val_loss: 1.6579e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6846e-11 - val_loss: 1.7029e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7098e-11 - val_loss: 1.6979e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7245e-11 - val_loss: 1.7053e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6976e-11 - val_loss: 1.6501e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6632e-11 - val_loss: 1.6600e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6700e-11 - val_loss: 1.6577e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6445e-11 - val_loss: 1.6109e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6007e-11 - val_loss: 1.5719e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5792e-11 - val_loss: 1.5815e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5829e-11 - val_loss: 1.5858e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5830e-11 - val_loss: 1.5780e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5982e-11 - val_loss: 1.6291e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6483e-11 - val_loss: 1.6780e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6981e-11 - val_loss: 1.6870e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6959e-11 - val_loss: 1.6825e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6961e-11 - val_loss: 1.7005e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7282e-11 - val_loss: 1.7472e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7309e-11 - val_loss: 1.7012e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7015e-11 - val_loss: 1.6888e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6915e-11 - val_loss: 1.6785e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7005e-11 - val_loss: 1.6856e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6967e-11 - val_loss: 1.6896e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6793e-11 - val_loss: 1.6623e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6569e-11 - val_loss: 1.6389e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6433e-11 - val_loss: 1.6392e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6524e-11 - val_loss: 1.6451e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6488e-11 - val_loss: 1.6334e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6435e-11 - val_loss: 1.6386e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6587e-11 - val_loss: 1.6453e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6517e-11 - val_loss: 1.6586e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6636e-11 - val_loss: 1.6504e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6532e-11 - val_loss: 1.6260e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6236e-11 - val_loss: 1.5993e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5927e-11 - val_loss: 1.5671e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5721e-11 - val_loss: 1.5704e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5791e-11 - val_loss: 1.5562e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5485e-11 - val_loss: 1.5348e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5477e-11 - val_loss: 1.5198e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.5039e-11 - val_loss: 1.4873e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4778e-11 - val_loss: 1.4640e-11
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4699e-11 - val_loss: 1.4599e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4580e-11 - val_loss: 1.4281e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4498e-11 - val_loss: 1.4590e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4662e-11 - val_loss: 1.4913e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4993e-11 - val_loss: 1.5114e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5074e-11 - val_loss: 1.4918e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4728e-11 - val_loss: 1.4571e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4804e-11 - val_loss: 1.4786e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4852e-11 - val_loss: 1.4719e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4796e-11 - val_loss: 1.4747e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4932e-11 - val_loss: 1.4858e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5013e-11 - val_loss: 1.4892e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4940e-11 - val_loss: 1.4802e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4865e-11 - val_loss: 1.4733e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4577e-11 - val_loss: 1.4477e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4630e-11 - val_loss: 1.4539e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4616e-11 - val_loss: 1.4390e-11
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4460e-11 - val_loss: 1.4188e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.4038e-11 - val_loss: 1.4076e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4149e-11 - val_loss: 1.4133e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3988e-11 - val_loss: 1.3849e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3701e-11 - val_loss: 1.3430e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3326e-11 - val_loss: 1.3055e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.3011e-11 - val_loss: 1.2770e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2754e-11 - val_loss: 1.2485e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2496e-11 - val_loss: 1.2394e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2471e-11 - val_loss: 1.2424e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2449e-11 - val_loss: 1.2337e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2384e-11 - val_loss: 1.2315e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2413e-11 - val_loss: 1.2586e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2534e-11 - val_loss: 1.2349e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2410e-11 - val_loss: 1.2010e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2087e-11 - val_loss: 1.2178e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2265e-11 - val_loss: 1.2042e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2126e-11 - val_loss: 1.2162e-11
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.2087e-11 - val_loss: 1.1811e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1739e-11 - val_loss: 1.1642e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1871e-11 - val_loss: 1.1995e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1847e-11 - val_loss: 1.1581e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1626e-11 - val_loss: 1.1702e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1589e-11 - val_loss: 1.1350e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1349e-11 - val_loss: 1.1131e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1283e-11 - val_loss: 1.1283e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1371e-11 - val_loss: 1.1381e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1489e-11 - val_loss: 1.1479e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1575e-11 - val_loss: 1.1690e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2074e-11 - val_loss: 1.2315e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2297e-11 - val_loss: 1.2013e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2025e-11 - val_loss: 1.1858e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1912e-11 - val_loss: 1.1914e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1913e-11 - val_loss: 1.1731e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1679e-11 - val_loss: 1.1417e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1386e-11 - val_loss: 1.1355e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1408e-11 - val_loss: 1.1186e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1131e-11 - val_loss: 1.0949e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0992e-11 - val_loss: 1.0965e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1068e-11 - val_loss: 1.1195e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1391e-11 - val_loss: 1.1165e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1399e-11 - val_loss: 1.1411e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1510e-11 - val_loss: 1.1288e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1316e-11 - val_loss: 1.1344e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1463e-11 - val_loss: 1.1393e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1414e-11 - val_loss: 1.1375e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1433e-11 - val_loss: 1.1312e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1288e-11 - val_loss: 1.1274e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.1125e-11 - val_loss: 1.0792e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0774e-11 - val_loss: 1.0474e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0446e-11 - val_loss: 1.0362e-11
Epoch 475/512

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0384e-11 - val_loss: 1.0342e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 1.0187e-11 - val_loss: 9.8779e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9078e-12 - val_loss: 9.9982e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0032e-11 - val_loss: 1.0016e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.9128e-12 - val_loss: 9.7432e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9909e-12 - val_loss: 1.0235e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0159e-11 - val_loss: 9.9376e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9887e-12 - val_loss: 1.0028e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0097e-11 - val_loss: 1.0052e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0014e-11 - val_loss: 9.9154e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9715e-12 - val_loss: 9.8551e-12
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.7945e-12 - val_loss: 9.6599e-12
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.5826e-12 - val_loss: 9.4278e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5658e-12 - val_loss: 9.7873e-12
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0051e-11 - val_loss: 1.0159e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0195e-11 - val_loss: 9.9280e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9823e-12 - val_loss: 1.0163e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0208e-11 - val_loss: 1.0019e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9129e-12 - val_loss: 1.0006e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0036e-11 - val_loss: 9.9716e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9950e-12 - val_loss: 9.9918e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0076e-11 - val_loss: 1.0205e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0162e-11 - val_loss: 1.0166e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0282e-11 - val_loss: 1.0191e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0222e-11 - val_loss: 1.0028e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8633e-12 - val_loss: 9.4669e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4424e-12 - val_loss: 9.4799e-12
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.2898e-12 - val_loss: 9.0725e-12
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.1325e-12 - val_loss: 9.0459e-12
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 9.0143e-12 - val_loss: 8.9196e-12
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.7590e-12 - val_loss: 8.4342e-12
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-0.00005/multiplication_weights.h5
512/512 - 0s - loss: 8.4407e-12 - val_loss: 8.3494e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3076e-12 - val_loss: 8.3525e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3343e-12 - val_loss: 8.4015e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5668e-12 - val_loss: 8.8906e-12
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9170e-12 - val_loss: 9.0115e-12
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1693e-12 - val_loss: 9.4552e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5818e-12 - val_loss: 9.7603e-12
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.765 | eve: 9.426 | bob: 9.621Epoch   0:   0% | abe: 9.707 | eve: 9.419 | bob: 9.567Epoch   0:   1% | abe: 9.661 | eve: 9.408 | bob: 9.529Epoch   0:   2% | abe: 9.638 | eve: 9.416 | bob: 9.513Epoch   0:   3% | abe: 9.597 | eve: 9.410 | bob: 9.477Epoch   0:   3% | abe: 9.576 | eve: 9.410 | bob: 9.461Epoch   0:   4% | abe: 9.538 | eve: 9.413 | bob: 9.427Epoch   0:   5% | abe: 9.520 | eve: 9.409 | bob: 9.413Epoch   0:   6% | abe: 9.505 | eve: 9.414 | bob: 9.403Epoch   0:   7% | abe: 9.485 | eve: 9.424 | bob: 9.386Epoch   0:   7% | abe: 9.472 | eve: 9.426 | bob: 9.377Epoch   0:   8% | abe: 9.454 | eve: 9.427 | bob: 9.362Epoch   0:   9% | abe: 9.432 | eve: 9.435 | bob: 9.343Epoch   0:  10% | abe: 9.418 | eve: 9.436 | bob: 9.332Epoch   0:  10% | abe: 9.405 | eve: 9.444 | bob: 9.321Epoch   0:  11% | abe: 9.392 | eve: 9.439 | bob: 9.309Epoch   0:  12% | abe: 9.377 | eve: 9.438 | bob: 9.297