WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-10 08:50:03.728536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-10 08:50:03.862685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-10 08:50:03.865303: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-10 08:50:03.869081: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-10 08:50:03.873143: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-10 08:50:03.874861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-10 08:50:03.879430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-10 08:50:03.882574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-10 08:50:03.892398: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-10 08:50:03.901906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-10 08:50:03.903137: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-10 08:50:03.926693: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-10 08:50:03.929252: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5806940 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-10 08:50:03.930120: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-10 08:50:04.586349: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f7ecf0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-10 08:50:04.586419: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-10 08:50:04.597938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-10 08:50:04.598117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-10 08:50:04.598153: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-10 08:50:04.598180: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-10 08:50:04.598207: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-10 08:50:04.598232: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-10 08:50:04.598258: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-10 08:50:04.598285: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-10 08:50:04.607741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-10 08:50:04.608342: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-10 08:50:04.617087: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-10 08:50:04.617570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-10 08:50:04.617656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-10 08:50:04.624032: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-10 08:50:10.906054: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6842 - val_loss: 0.0038
Epoch 2/512
512/512 - 0s - loss: 0.2255 - val_loss: 5.0974e-04
Epoch 3/512
512/512 - 0s - loss: 0.0246 - val_loss: 4.8977e-05
Epoch 4/512
512/512 - 0s - loss: 0.0037 - val_loss: 2.6425e-05
Epoch 5/512
512/512 - 0s - loss: 0.0024 - val_loss: 1.9651e-05
Epoch 6/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.3855e-05
Epoch 7/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.0901e-06
Epoch 8/512
512/512 - 0s - loss: 7.6366e-04 - val_loss: 5.4501e-06
Epoch 9/512
512/512 - 0s - loss: 4.4154e-04 - val_loss: 2.9104e-06
Epoch 10/512
512/512 - 0s - loss: 2.2519e-04 - val_loss: 1.3351e-06
Epoch 11/512
512/512 - 0s - loss: 9.7484e-05 - val_loss: 5.0170e-07
Epoch 12/512
512/512 - 0s - loss: 3.4135e-05 - val_loss: 1.4866e-07
Epoch 13/512
512/512 - 0s - loss: 1.4038e-05 - val_loss: 7.9301e-07
Epoch 14/512
512/512 - 0s - loss: 0.0024 - val_loss: 5.7186e-05
Epoch 15/512
512/512 - 0s - loss: 0.0028 - val_loss: 2.9568e-06
Epoch 16/512
512/512 - 0s - loss: 2.0163e-04 - val_loss: 1.7536e-06
Epoch 17/512
512/512 - 0s - loss: 3.7704e-04 - val_loss: 1.2876e-05
Epoch 18/512
512/512 - 0s - loss: 0.0026 - val_loss: 2.2280e-05
Epoch 19/512
512/512 - 0s - loss: 0.0014 - val_loss: 5.7773e-06
Epoch 20/512
512/512 - 0s - loss: 5.8253e-04 - val_loss: 7.9485e-06
Epoch 21/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.9910e-05
Epoch 22/512
512/512 - 0s - loss: 0.0018 - val_loss: 1.1331e-05
Epoch 23/512
512/512 - 0s - loss: 9.8143e-04 - val_loss: 8.2556e-06
Epoch 24/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.3305e-05
Epoch 25/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.3855e-05
Epoch 26/512
512/512 - 0s - loss: 0.0013 - val_loss: 9.7455e-06
Epoch 27/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0916e-05
Epoch 28/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.3159e-05
Epoch 29/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.0856e-05
Epoch 30/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0291e-05
Epoch 31/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1876e-05
Epoch 32/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1352e-05
Epoch 33/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0244e-05
Epoch 34/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0773e-05
Epoch 35/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1075e-05
Epoch 36/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0317e-05
Epoch 37/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0252e-05
Epoch 38/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0847e-05
Epoch 39/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0148e-05
Epoch 40/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.9113e-06
Epoch 41/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0323e-05
Epoch 42/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0234e-05
Epoch 43/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8212e-06
Epoch 44/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.8459e-06
Epoch 45/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.9032e-06
Epoch 46/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.6528e-06
Epoch 47/512
512/512 - 0s - loss: 9.8599e-04 - val_loss: 9.5801e-06
Epoch 48/512
512/512 - 0s - loss: 9.9200e-04 - val_loss: 9.7105e-06
Epoch 49/512
512/512 - 0s - loss: 9.9768e-04 - val_loss: 9.4586e-06
Epoch 50/512
512/512 - 0s - loss: 9.6022e-04 - val_loss: 9.3642e-06
Epoch 51/512
512/512 - 0s - loss: 9.6478e-04 - val_loss: 9.6369e-06
Epoch 52/512
512/512 - 0s - loss: 9.8115e-04 - val_loss: 9.3646e-06
Epoch 53/512
512/512 - 0s - loss: 9.4719e-04 - val_loss: 9.1291e-06
Epoch 54/512
512/512 - 0s - loss: 9.3841e-04 - val_loss: 9.2165e-06
Epoch 55/512
512/512 - 0s - loss: 9.3871e-04 - val_loss: 9.3911e-06
Epoch 56/512
512/512 - 0s - loss: 9.4858e-04 - val_loss: 9.2032e-06
Epoch 57/512
512/512 - 0s - loss: 9.2789e-04 - val_loss: 8.8641e-06
Epoch 58/512
512/512 - 0s - loss: 9.0627e-04 - val_loss: 9.0132e-06
Epoch 59/512
512/512 - 0s - loss: 9.2760e-04 - val_loss: 8.9908e-06
Epoch 60/512
512/512 - 0s - loss: 9.0723e-04 - val_loss: 8.8614e-06
Epoch 61/512
512/512 - 0s - loss: 9.0173e-04 - val_loss: 8.8537e-06
Epoch 62/512
512/512 - 0s - loss: 9.0038e-04 - val_loss: 8.7320e-06
Epoch 63/512
512/512 - 0s - loss: 8.9197e-04 - val_loss: 8.6923e-06
Epoch 64/512
512/512 - 0s - loss: 8.8783e-04 - val_loss: 8.6216e-06
Epoch 65/512
512/512 - 0s - loss: 8.7888e-04 - val_loss: 8.6254e-06
Epoch 66/512
512/512 - 0s - loss: 8.7532e-04 - val_loss: 8.6719e-06
Epoch 67/512
512/512 - 0s - loss: 8.7934e-04 - val_loss: 8.5327e-06
Epoch 68/512
512/512 - 0s - loss: 8.6533e-04 - val_loss: 8.3996e-06
Epoch 69/512
512/512 - 0s - loss: 8.5800e-04 - val_loss: 8.4391e-06
Epoch 70/512
512/512 - 0s - loss: 8.5940e-04 - val_loss: 8.4735e-06
Epoch 71/512
512/512 - 0s - loss: 8.6113e-04 - val_loss: 8.2449e-06
Epoch 72/512
512/512 - 0s - loss: 8.3749e-04 - val_loss: 8.2669e-06
Epoch 73/512
512/512 - 0s - loss: 8.4064e-04 - val_loss: 8.4523e-06
Epoch 74/512
512/512 - 0s - loss: 8.6300e-04 - val_loss: 8.0712e-06
Epoch 75/512
512/512 - 0s - loss: 8.1702e-04 - val_loss: 8.0469e-06
Epoch 76/512
512/512 - 0s - loss: 8.2688e-04 - val_loss: 8.3326e-06
Epoch 77/512
512/512 - 0s - loss: 8.4345e-04 - val_loss: 8.1899e-06
Epoch 78/512
512/512 - 0s - loss: 8.2710e-04 - val_loss: 7.8406e-06
Epoch 79/512
512/512 - 0s - loss: 8.0310e-04 - val_loss: 7.9605e-06
Epoch 80/512
512/512 - 0s - loss: 8.1615e-04 - val_loss: 8.3101e-06
Epoch 81/512
512/512 - 0s - loss: 8.3291e-04 - val_loss: 8.0290e-06
Epoch 82/512
512/512 - 0s - loss: 8.0077e-04 - val_loss: 7.7939e-06
Epoch 83/512
512/512 - 0s - loss: 7.9832e-04 - val_loss: 7.9357e-06
Epoch 84/512
512/512 - 0s - loss: 8.0534e-04 - val_loss: 8.0504e-06
Epoch 85/512
512/512 - 0s - loss: 8.1163e-04 - val_loss: 7.7981e-06
Epoch 86/512
512/512 - 0s - loss: 7.8503e-04 - val_loss: 7.7309e-06
Epoch 87/512
512/512 - 0s - loss: 7.9078e-04 - val_loss: 7.9014e-06
Epoch 88/512
512/512 - 0s - loss: 7.9891e-04 - val_loss: 7.7920e-06
Epoch 89/512
512/512 - 0s - loss: 7.7868e-04 - val_loss: 7.7788e-06
Epoch 90/512
512/512 - 0s - loss: 7.8421e-04 - val_loss: 7.8343e-06
Epoch 91/512
512/512 - 0s - loss: 7.8689e-04 - val_loss: 7.7307e-06
Epoch 92/512
512/512 - 0s - loss: 7.7663e-04 - val_loss: 7.5772e-06
Epoch 93/512
512/512 - 0s - loss: 7.7358e-04 - val_loss: 7.5620e-06
Epoch 94/512
512/512 - 0s - loss: 7.6530e-04 - val_loss: 7.6482e-06
Epoch 95/512
512/512 - 0s - loss: 7.7824e-04 - val_loss: 7.5811e-06
Epoch 96/512
512/512 - 0s - loss: 7.6710e-04 - val_loss: 7.3949e-06
Epoch 97/512
512/512 - 0s - loss: 7.4599e-04 - val_loss: 7.6576e-06
Epoch 98/512
512/512 - 0s - loss: 7.7796e-04 - val_loss: 7.6388e-06
Epoch 99/512
512/512 - 0s - loss: 7.6064e-04 - val_loss: 7.3047e-06
Epoch 100/512
512/512 - 0s - loss: 7.4160e-04 - val_loss: 7.3723e-06
Epoch 101/512
512/512 - 0s - loss: 7.5516e-04 - val_loss: 7.4581e-06
Epoch 102/512
512/512 - 0s - loss: 7.5059e-04 - val_loss: 7.4416e-06
Epoch 103/512
512/512 - 0s - loss: 7.4192e-04 - val_loss: 7.5260e-06
Epoch 104/512
512/512 - 0s - loss: 7.5328e-04 - val_loss: 7.4141e-06
Epoch 105/512
512/512 - 0s - loss: 7.4416e-04 - val_loss: 7.2319e-06
Epoch 106/512
512/512 - 0s - loss: 7.2578e-04 - val_loss: 7.3731e-06
Epoch 107/512
512/512 - 0s - loss: 7.4473e-04 - val_loss: 7.4163e-06
Epoch 108/512
512/512 - 0s - loss: 7.4652e-04 - val_loss: 7.0330e-06
Epoch 109/512
512/512 - 0s - loss: 7.0491e-04 - val_loss: 7.2591e-06
Epoch 110/512
512/512 - 0s - loss: 7.4186e-04 - val_loss: 7.4376e-06
Epoch 111/512
512/512 - 0s - loss: 7.4100e-04 - val_loss: 7.0250e-06
Epoch 112/512
512/512 - 0s - loss: 7.1111e-04 - val_loss: 6.8727e-06
Epoch 113/512
512/512 - 0s - loss: 7.1132e-04 - val_loss: 7.1500e-06
Epoch 114/512
512/512 - 0s - loss: 7.3353e-04 - val_loss: 7.1216e-06
Epoch 115/512
512/512 - 0s - loss: 7.1190e-04 - val_loss: 6.9625e-06
Epoch 116/512
512/512 - 0s - loss: 7.0584e-04 - val_loss: 7.0862e-06
Epoch 117/512
512/512 - 0s - loss: 7.1580e-04 - val_loss: 7.1115e-06
Epoch 118/512
512/512 - 0s - loss: 7.1252e-04 - val_loss: 7.0192e-06
Epoch 119/512
512/512 - 0s - loss: 7.0165e-04 - val_loss: 6.9664e-06
Epoch 120/512
512/512 - 0s - loss: 7.0427e-04 - val_loss: 6.9645e-06
Epoch 121/512
512/512 - 0s - loss: 7.0144e-04 - val_loss: 6.9196e-06
Epoch 122/512
512/512 - 0s - loss: 7.0232e-04 - val_loss: 6.8257e-06
Epoch 123/512
512/512 - 0s - loss: 6.8700e-04 - val_loss: 6.8730e-06
Epoch 124/512
512/512 - 0s - loss: 6.9563e-04 - val_loss: 6.9206e-06
Epoch 125/512
512/512 - 0s - loss: 6.9683e-04 - val_loss: 6.7415e-06
Epoch 126/512
512/512 - 0s - loss: 6.7996e-04 - val_loss: 6.7322e-06
Epoch 127/512
512/512 - 0s - loss: 6.7972e-04 - val_loss: 6.9380e-06
Epoch 128/512
512/512 - 0s - loss: 6.9828e-04 - val_loss: 6.7766e-06
Epoch 129/512
512/512 - 0s - loss: 6.7271e-04 - val_loss: 6.6083e-06
Epoch 130/512
512/512 - 0s - loss: 6.7122e-04 - val_loss: 6.7357e-06
Epoch 131/512
512/512 - 0s - loss: 6.8046e-04 - val_loss: 6.6941e-06
Epoch 132/512
512/512 - 0s - loss: 6.6866e-04 - val_loss: 6.6342e-06
Epoch 133/512
512/512 - 0s - loss: 6.6974e-04 - val_loss: 6.6368e-06
Epoch 134/512
512/512 - 0s - loss: 6.6476e-04 - val_loss: 6.6080e-06
Epoch 135/512
512/512 - 0s - loss: 6.6344e-04 - val_loss: 6.5876e-06
Epoch 136/512
512/512 - 0s - loss: 6.6174e-04 - val_loss: 6.5447e-06
Epoch 137/512
512/512 - 0s - loss: 6.5831e-04 - val_loss: 6.4271e-06
Epoch 138/512
512/512 - 0s - loss: 6.4872e-04 - val_loss: 6.4844e-06
Epoch 139/512
512/512 - 0s - loss: 6.5439e-04 - val_loss: 6.5250e-06
Epoch 140/512
512/512 - 0s - loss: 6.4899e-04 - val_loss: 6.4692e-06
Epoch 141/512
512/512 - 0s - loss: 6.4769e-04 - val_loss: 6.3525e-06
Epoch 142/512
512/512 - 0s - loss: 6.4150e-04 - val_loss: 6.2464e-06
Epoch 143/512
512/512 - 0s - loss: 6.3300e-04 - val_loss: 6.3564e-06
Epoch 144/512
512/512 - 0s - loss: 6.4202e-04 - val_loss: 6.3763e-06
Epoch 145/512
512/512 - 0s - loss: 6.3518e-04 - val_loss: 6.2414e-06
Epoch 146/512
512/512 - 0s - loss: 6.2688e-04 - val_loss: 6.1711e-06
Epoch 147/512
512/512 - 0s - loss: 6.2319e-04 - val_loss: 6.2576e-06
Epoch 148/512
512/512 - 0s - loss: 6.2708e-04 - val_loss: 6.2938e-06
Epoch 149/512
512/512 - 0s - loss: 6.2774e-04 - val_loss: 6.1056e-06
Epoch 150/512
512/512 - 0s - loss: 6.0882e-04 - val_loss: 6.0781e-06
Epoch 151/512
512/512 - 0s - loss: 6.1725e-04 - val_loss: 6.0799e-06
Epoch 152/512
512/512 - 0s - loss: 6.1076e-04 - val_loss: 6.0182e-06
Epoch 153/512
512/512 - 0s - loss: 6.0438e-04 - val_loss: 5.9999e-06
Epoch 154/512
512/512 - 0s - loss: 6.0474e-04 - val_loss: 6.0009e-06
Epoch 155/512
512/512 - 0s - loss: 6.0076e-04 - val_loss: 6.0142e-06
Epoch 156/512
512/512 - 0s - loss: 6.0287e-04 - val_loss: 5.8551e-06
Epoch 157/512
512/512 - 0s - loss: 5.8682e-04 - val_loss: 5.7849e-06
Epoch 158/512
512/512 - 0s - loss: 5.8618e-04 - val_loss: 5.9028e-06
Epoch 159/512
512/512 - 0s - loss: 5.9332e-04 - val_loss: 5.8840e-06
Epoch 160/512
512/512 - 0s - loss: 5.8745e-04 - val_loss: 5.6359e-06
Epoch 161/512
512/512 - 0s - loss: 5.6668e-04 - val_loss: 5.6926e-06
Epoch 162/512
512/512 - 0s - loss: 5.8100e-04 - val_loss: 5.7436e-06
Epoch 163/512
512/512 - 0s - loss: 5.7433e-04 - val_loss: 5.6455e-06
Epoch 164/512
512/512 - 0s - loss: 5.6051e-04 - val_loss: 5.6665e-06
Epoch 165/512
512/512 - 0s - loss: 5.7067e-04 - val_loss: 5.5680e-06
Epoch 166/512
512/512 - 0s - loss: 5.5981e-04 - val_loss: 5.4741e-06
Epoch 167/512
512/512 - 0s - loss: 5.5067e-04 - val_loss: 5.5148e-06
Epoch 168/512
512/512 - 0s - loss: 5.5285e-04 - val_loss: 5.6009e-06
Epoch 169/512
512/512 - 0s - loss: 5.5031e-04 - val_loss: 5.5528e-06
Epoch 170/512
512/512 - 0s - loss: 5.5105e-04 - val_loss: 5.3384e-06
Epoch 171/512
512/512 - 0s - loss: 5.3172e-04 - val_loss: 5.3232e-06
Epoch 172/512
512/512 - 0s - loss: 5.3807e-04 - val_loss: 5.3597e-06
Epoch 173/512
512/512 - 0s - loss: 5.3632e-04 - val_loss: 5.2545e-06
Epoch 174/512
512/512 - 0s - loss: 5.2591e-04 - val_loss: 5.1833e-06
Epoch 175/512
512/512 - 0s - loss: 5.2264e-04 - val_loss: 5.1779e-06
Epoch 176/512
512/512 - 0s - loss: 5.2237e-04 - val_loss: 5.1101e-06
Epoch 177/512
512/512 - 0s - loss: 5.0891e-04 - val_loss: 5.1443e-06
Epoch 178/512
512/512 - 0s - loss: 5.1980e-04 - val_loss: 5.0377e-06
Epoch 179/512
512/512 - 0s - loss: 4.9985e-04 - val_loss: 4.9773e-06
Epoch 180/512
512/512 - 0s - loss: 5.0215e-04 - val_loss: 5.0539e-06
Epoch 181/512
512/512 - 0s - loss: 5.0109e-04 - val_loss: 5.0095e-06
Epoch 182/512
512/512 - 0s - loss: 4.9309e-04 - val_loss: 4.9441e-06
Epoch 183/512
512/512 - 0s - loss: 4.9088e-04 - val_loss: 4.8372e-06
Epoch 184/512
512/512 - 0s - loss: 4.8227e-04 - val_loss: 4.7983e-06
Epoch 185/512
512/512 - 0s - loss: 4.8068e-04 - val_loss: 4.7842e-06
Epoch 186/512
512/512 - 0s - loss: 4.7501e-04 - val_loss: 4.7507e-06
Epoch 187/512
512/512 - 0s - loss: 4.7425e-04 - val_loss: 4.6685e-06
Epoch 188/512
512/512 - 0s - loss: 4.6374e-04 - val_loss: 4.6064e-06
Epoch 189/512
512/512 - 0s - loss: 4.6033e-04 - val_loss: 4.6103e-06
Epoch 190/512
512/512 - 0s - loss: 4.6061e-04 - val_loss: 4.5374e-06
Epoch 191/512
512/512 - 0s - loss: 4.5155e-04 - val_loss: 4.4762e-06
Epoch 192/512
512/512 - 0s - loss: 4.4428e-04 - val_loss: 4.4883e-06
Epoch 193/512
512/512 - 0s - loss: 4.4552e-04 - val_loss: 4.4637e-06
Epoch 194/512
512/512 - 0s - loss: 4.4138e-04 - val_loss: 4.3242e-06
Epoch 195/512
512/512 - 0s - loss: 4.3078e-04 - val_loss: 4.2451e-06
Epoch 196/512
512/512 - 0s - loss: 4.2637e-04 - val_loss: 4.2873e-06
Epoch 197/512
512/512 - 0s - loss: 4.2454e-04 - val_loss: 4.2801e-06
Epoch 198/512
512/512 - 0s - loss: 4.2505e-04 - val_loss: 4.1088e-06
Epoch 199/512
512/512 - 0s - loss: 4.0972e-04 - val_loss: 4.0165e-06
Epoch 200/512
512/512 - 0s - loss: 4.0747e-04 - val_loss: 4.0446e-06
Epoch 201/512
512/512 - 0s - loss: 4.0334e-04 - val_loss: 4.0732e-06
Epoch 202/512
512/512 - 0s - loss: 4.0403e-04 - val_loss: 3.9760e-06
Epoch 203/512
512/512 - 0s - loss: 3.9340e-04 - val_loss: 3.8597e-06
Epoch 204/512
512/512 - 0s - loss: 3.8517e-04 - val_loss: 3.9094e-06
Epoch 205/512
512/512 - 0s - loss: 3.9105e-04 - val_loss: 3.8190e-06
Epoch 206/512
512/512 - 0s - loss: 3.7654e-04 - val_loss: 3.7030e-06
Epoch 207/512
512/512 - 0s - loss: 3.7000e-04 - val_loss: 3.7677e-06
Epoch 208/512
512/512 - 0s - loss: 3.7621e-04 - val_loss: 3.7110e-06
Epoch 209/512
512/512 - 0s - loss: 3.6156e-04 - val_loss: 3.6382e-06
Epoch 210/512
512/512 - 0s - loss: 3.6153e-04 - val_loss: 3.5632e-06
Epoch 211/512
512/512 - 0s - loss: 3.5374e-04 - val_loss: 3.5025e-06
Epoch 212/512
512/512 - 0s - loss: 3.4699e-04 - val_loss: 3.5192e-06
Epoch 213/512
512/512 - 0s - loss: 3.4945e-04 - val_loss: 3.4556e-06
Epoch 214/512
512/512 - 0s - loss: 3.4091e-04 - val_loss: 3.3277e-06
Epoch 215/512
512/512 - 0s - loss: 3.3122e-04 - val_loss: 3.2894e-06
Epoch 216/512
512/512 - 0s - loss: 3.2724e-04 - val_loss: 3.3559e-06
Epoch 217/512
512/512 - 0s - loss: 3.3185e-04 - val_loss: 3.2471e-06
Epoch 218/512
512/512 - 0s - loss: 3.1526e-04 - val_loss: 3.1495e-06
Epoch 219/512
512/512 - 0s - loss: 3.1503e-04 - val_loss: 3.1369e-06
Epoch 220/512
512/512 - 0s - loss: 3.0858e-04 - val_loss: 3.1499e-06
Epoch 221/512
512/512 - 0s - loss: 3.0945e-04 - val_loss: 3.0526e-06
Epoch 222/512
512/512 - 0s - loss: 2.9725e-04 - val_loss: 2.9680e-06
Epoch 223/512
512/512 - 0s - loss: 2.9376e-04 - val_loss: 2.9778e-06
Epoch 224/512
512/512 - 0s - loss: 2.9436e-04 - val_loss: 2.8864e-06
Epoch 225/512
512/512 - 0s - loss: 2.8343e-04 - val_loss: 2.7969e-06
Epoch 226/512
512/512 - 0s - loss: 2.7819e-04 - val_loss: 2.8270e-06
Epoch 227/512
512/512 - 0s - loss: 2.7973e-04 - val_loss: 2.7342e-06
Epoch 228/512
512/512 - 0s - loss: 2.6939e-04 - val_loss: 2.6412e-06
Epoch 229/512
512/512 - 0s - loss: 2.6255e-04 - val_loss: 2.6976e-06
Epoch 230/512
512/512 - 0s - loss: 2.6458e-04 - val_loss: 2.6757e-06
Epoch 231/512
512/512 - 0s - loss: 2.6050e-04 - val_loss: 2.5169e-06
Epoch 232/512
512/512 - 0s - loss: 2.4674e-04 - val_loss: 2.4630e-06
Epoch 233/512
512/512 - 0s - loss: 2.4690e-04 - val_loss: 2.5023e-06
Epoch 234/512
512/512 - 0s - loss: 2.4544e-04 - val_loss: 2.4501e-06
Epoch 235/512
512/512 - 0s - loss: 2.3823e-04 - val_loss: 2.3413e-06
Epoch 236/512
512/512 - 0s - loss: 2.3100e-04 - val_loss: 2.3119e-06
Epoch 237/512
512/512 - 0s - loss: 2.2945e-04 - val_loss: 2.3101e-06
Epoch 238/512
512/512 - 0s - loss: 2.2634e-04 - val_loss: 2.2357e-06
Epoch 239/512
512/512 - 0s - loss: 2.1855e-04 - val_loss: 2.1754e-06
Epoch 240/512
512/512 - 0s - loss: 2.1443e-04 - val_loss: 2.1684e-06
Epoch 241/512
512/512 - 0s - loss: 2.1222e-04 - val_loss: 2.1352e-06
Epoch 242/512
512/512 - 0s - loss: 2.0756e-04 - val_loss: 2.0632e-06
Epoch 243/512
512/512 - 0s - loss: 2.0222e-04 - val_loss: 1.9940e-06
Epoch 244/512
512/512 - 0s - loss: 1.9646e-04 - val_loss: 1.9815e-06
Epoch 245/512
512/512 - 0s - loss: 1.9469e-04 - val_loss: 1.9635e-06
Epoch 246/512
512/512 - 0s - loss: 1.9169e-04 - val_loss: 1.8852e-06
Epoch 247/512
512/512 - 0s - loss: 1.8279e-04 - val_loss: 1.8562e-06
Epoch 248/512
512/512 - 0s - loss: 1.8124e-04 - val_loss: 1.8635e-06
Epoch 249/512
512/512 - 0s - loss: 1.8189e-04 - val_loss: 1.7665e-06
Epoch 250/512
512/512 - 0s - loss: 1.6960e-04 - val_loss: 1.7147e-06
Epoch 251/512
512/512 - 0s - loss: 1.7086e-04 - val_loss: 1.6938e-06
Epoch 252/512
512/512 - 0s - loss: 1.6501e-04 - val_loss: 1.6605e-06
Epoch 253/512
512/512 - 0s - loss: 1.6331e-04 - val_loss: 1.6105e-06
Epoch 254/512
512/512 - 0s - loss: 1.5565e-04 - val_loss: 1.5936e-06
Epoch 255/512
512/512 - 0s - loss: 1.5603e-04 - val_loss: 1.5664e-06
Epoch 256/512
512/512 - 0s - loss: 1.5160e-04 - val_loss: 1.4848e-06
Epoch 257/512
512/512 - 0s - loss: 1.4524e-04 - val_loss: 1.4540e-06
Epoch 258/512
512/512 - 0s - loss: 1.4235e-04 - val_loss: 1.4672e-06
Epoch 259/512
512/512 - 0s - loss: 1.4254e-04 - val_loss: 1.4276e-06
Epoch 260/512
512/512 - 0s - loss: 1.3550e-04 - val_loss: 1.3646e-06
Epoch 261/512
512/512 - 0s - loss: 1.3272e-04 - val_loss: 1.3346e-06
Epoch 262/512
512/512 - 0s - loss: 1.3053e-04 - val_loss: 1.2965e-06
Epoch 263/512
512/512 - 0s - loss: 1.2570e-04 - val_loss: 1.2612e-06
Epoch 264/512
512/512 - 0s - loss: 1.2440e-04 - val_loss: 1.2018e-06
Epoch 265/512
512/512 - 0s - loss: 1.1706e-04 - val_loss: 1.2011e-06
Epoch 266/512
512/512 - 0s - loss: 1.1736e-04 - val_loss: 1.2204e-06
Epoch 267/512
512/512 - 0s - loss: 1.1728e-04 - val_loss: 1.1267e-06
Epoch 268/512
512/512 - 0s - loss: 1.0775e-04 - val_loss: 1.0709e-06
Epoch 269/512
512/512 - 0s - loss: 1.0657e-04 - val_loss: 1.0961e-06
Epoch 270/512
512/512 - 0s - loss: 1.0606e-04 - val_loss: 1.0786e-06
Epoch 271/512
512/512 - 0s - loss: 1.0274e-04 - val_loss: 1.0170e-06
Epoch 272/512
512/512 - 0s - loss: 9.8166e-05 - val_loss: 9.8394e-07
Epoch 273/512
512/512 - 0s - loss: 9.5802e-05 - val_loss: 9.7289e-07
Epoch 274/512
512/512 - 0s - loss: 9.4370e-05 - val_loss: 9.5236e-07
Epoch 275/512
512/512 - 0s - loss: 9.1749e-05 - val_loss: 8.9697e-07
Epoch 276/512
512/512 - 0s - loss: 8.6352e-05 - val_loss: 8.8747e-07
Epoch 277/512
512/512 - 0s - loss: 8.6446e-05 - val_loss: 8.8662e-07
Epoch 278/512
512/512 - 0s - loss: 8.4512e-05 - val_loss: 8.4788e-07
Epoch 279/512
512/512 - 0s - loss: 8.0913e-05 - val_loss: 7.9951e-07
Epoch 280/512
512/512 - 0s - loss: 7.7236e-05 - val_loss: 7.8803e-07
Epoch 281/512
512/512 - 0s - loss: 7.6531e-05 - val_loss: 7.8630e-07
Epoch 282/512
512/512 - 0s - loss: 7.5692e-05 - val_loss: 7.2796e-07
Epoch 283/512
512/512 - 0s - loss: 6.9645e-05 - val_loss: 7.1175e-07
Epoch 284/512
512/512 - 0s - loss: 6.9859e-05 - val_loss: 7.1844e-07
Epoch 285/512
512/512 - 0s - loss: 6.8021e-05 - val_loss: 6.9218e-07
Epoch 286/512
512/512 - 0s - loss: 6.5693e-05 - val_loss: 6.5931e-07
Epoch 287/512
512/512 - 0s - loss: 6.2748e-05 - val_loss: 6.3693e-07
Epoch 288/512
512/512 - 0s - loss: 6.1752e-05 - val_loss: 6.1428e-07
Epoch 289/512
512/512 - 0s - loss: 5.9315e-05 - val_loss: 5.9365e-07
Epoch 290/512
512/512 - 0s - loss: 5.7795e-05 - val_loss: 5.7194e-07
Epoch 291/512
512/512 - 0s - loss: 5.5100e-05 - val_loss: 5.6386e-07
Epoch 292/512
512/512 - 0s - loss: 5.4602e-05 - val_loss: 5.5073e-07
Epoch 293/512
512/512 - 0s - loss: 5.2631e-05 - val_loss: 5.1139e-07
Epoch 294/512
512/512 - 0s - loss: 4.9600e-05 - val_loss: 5.0222e-07
Epoch 295/512
512/512 - 0s - loss: 4.9057e-05 - val_loss: 4.9760e-07
Epoch 296/512
512/512 - 0s - loss: 4.7729e-05 - val_loss: 4.7825e-07
Epoch 297/512
512/512 - 0s - loss: 4.5731e-05 - val_loss: 4.5400e-07
Epoch 298/512
512/512 - 0s - loss: 4.3635e-05 - val_loss: 4.4812e-07
Epoch 299/512
512/512 - 0s - loss: 4.2879e-05 - val_loss: 4.4243e-07
Epoch 300/512
512/512 - 0s - loss: 4.1772e-05 - val_loss: 4.2197e-07
Epoch 301/512
512/512 - 0s - loss: 3.9794e-05 - val_loss: 4.0170e-07
Epoch 302/512
512/512 - 0s - loss: 3.8378e-05 - val_loss: 3.9101e-07
Epoch 303/512
512/512 - 0s - loss: 3.7222e-05 - val_loss: 3.7986e-07
Epoch 304/512
512/512 - 0s - loss: 3.6275e-05 - val_loss: 3.6229e-07
Epoch 305/512
512/512 - 0s - loss: 3.4141e-05 - val_loss: 3.5548e-07
Epoch 306/512
512/512 - 0s - loss: 3.3918e-05 - val_loss: 3.4793e-07
Epoch 307/512
512/512 - 0s - loss: 3.2762e-05 - val_loss: 3.2202e-07
Epoch 308/512
512/512 - 0s - loss: 3.0704e-05 - val_loss: 3.0724e-07
Epoch 309/512
512/512 - 0s - loss: 2.9594e-05 - val_loss: 3.1326e-07
Epoch 310/512
512/512 - 0s - loss: 2.9948e-05 - val_loss: 2.9826e-07
Epoch 311/512
512/512 - 0s - loss: 2.8034e-05 - val_loss: 2.6945e-07
Epoch 312/512
512/512 - 0s - loss: 2.6093e-05 - val_loss: 2.6992e-07
Epoch 313/512
512/512 - 0s - loss: 2.6314e-05 - val_loss: 2.7138e-07
Epoch 314/512
512/512 - 0s - loss: 2.5725e-05 - val_loss: 2.4908e-07
Epoch 315/512
512/512 - 0s - loss: 2.3573e-05 - val_loss: 2.3708e-07
Epoch 316/512
512/512 - 0s - loss: 2.2912e-05 - val_loss: 2.4235e-07
Epoch 317/512
512/512 - 0s - loss: 2.3148e-05 - val_loss: 2.3157e-07
Epoch 318/512
512/512 - 0s - loss: 2.1763e-05 - val_loss: 2.0748e-07
Epoch 319/512
512/512 - 0s - loss: 1.9760e-05 - val_loss: 2.0910e-07
Epoch 320/512
512/512 - 0s - loss: 2.0374e-05 - val_loss: 2.1472e-07
Epoch 321/512
512/512 - 0s - loss: 1.9875e-05 - val_loss: 1.9453e-07
Epoch 322/512
512/512 - 0s - loss: 1.7898e-05 - val_loss: 1.8418e-07
Epoch 323/512
512/512 - 0s - loss: 1.7828e-05 - val_loss: 1.8544e-07
Epoch 324/512
512/512 - 0s - loss: 1.7513e-05 - val_loss: 1.7586e-07
Epoch 325/512
512/512 - 0s - loss: 1.6427e-05 - val_loss: 1.6291e-07
Epoch 326/512
512/512 - 0s - loss: 1.5582e-05 - val_loss: 1.6129e-07
Epoch 327/512
512/512 - 0s - loss: 1.5351e-05 - val_loss: 1.5884e-07
Epoch 328/512
512/512 - 0s - loss: 1.4864e-05 - val_loss: 1.4897e-07
Epoch 329/512
512/512 - 0s - loss: 1.3961e-05 - val_loss: 1.4023e-07
Epoch 330/512
512/512 - 0s - loss: 1.3477e-05 - val_loss: 1.3837e-07
Epoch 331/512
512/512 - 0s - loss: 1.3076e-05 - val_loss: 1.3369e-07
Epoch 332/512
512/512 - 0s - loss: 1.2584e-05 - val_loss: 1.2567e-07
Epoch 333/512
512/512 - 0s - loss: 1.1848e-05 - val_loss: 1.2111e-07
Epoch 334/512
512/512 - 0s - loss: 1.1532e-05 - val_loss: 1.1853e-07
Epoch 335/512
512/512 - 0s - loss: 1.1258e-05 - val_loss: 1.1072e-07
Epoch 336/512
512/512 - 0s - loss: 1.0371e-05 - val_loss: 1.0761e-07
Epoch 337/512
512/512 - 0s - loss: 1.0322e-05 - val_loss: 1.0535e-07
Epoch 338/512
512/512 - 0s - loss: 9.7816e-06 - val_loss: 1.0091e-07
Epoch 339/512
512/512 - 0s - loss: 9.4434e-06 - val_loss: 9.5504e-08
Epoch 340/512
512/512 - 0s - loss: 9.0204e-06 - val_loss: 8.9954e-08
Epoch 341/512
512/512 - 0s - loss: 8.5120e-06 - val_loss: 8.8097e-08
Epoch 342/512
512/512 - 0s - loss: 8.3725e-06 - val_loss: 8.5720e-08
Epoch 343/512
512/512 - 0s - loss: 8.0171e-06 - val_loss: 8.0217e-08
Epoch 344/512
512/512 - 0s - loss: 7.4524e-06 - val_loss: 7.8433e-08
Epoch 345/512
512/512 - 0s - loss: 7.4590e-06 - val_loss: 7.6192e-08
Epoch 346/512
512/512 - 0s - loss: 7.0693e-06 - val_loss: 6.9393e-08
Epoch 347/512
512/512 - 0s - loss: 6.4707e-06 - val_loss: 6.8529e-08
Epoch 348/512
512/512 - 0s - loss: 6.5962e-06 - val_loss: 6.7388e-08
Epoch 349/512
512/512 - 0s - loss: 6.2440e-06 - val_loss: 6.1243e-08
Epoch 350/512
512/512 - 0s - loss: 5.6971e-06 - val_loss: 5.9759e-08
Epoch 351/512
512/512 - 0s - loss: 5.7002e-06 - val_loss: 6.0321e-08
Epoch 352/512
512/512 - 0s - loss: 5.6073e-06 - val_loss: 5.4641e-08
Epoch 353/512
512/512 - 0s - loss: 5.0207e-06 - val_loss: 5.1630e-08
Epoch 354/512
512/512 - 0s - loss: 4.9074e-06 - val_loss: 5.2881e-08
Epoch 355/512
512/512 - 0s - loss: 4.9169e-06 - val_loss: 4.9626e-08
Epoch 356/512
512/512 - 0s - loss: 4.5491e-06 - val_loss: 4.4606e-08
Epoch 357/512
512/512 - 0s - loss: 4.2005e-06 - val_loss: 4.4618e-08
Epoch 358/512
512/512 - 0s - loss: 4.2440e-06 - val_loss: 4.4570e-08
Epoch 359/512
512/512 - 0s - loss: 4.0837e-06 - val_loss: 4.0305e-08
Epoch 360/512
512/512 - 0s - loss: 3.7186e-06 - val_loss: 3.7702e-08
Epoch 361/512
512/512 - 0s - loss: 3.5996e-06 - val_loss: 3.8577e-08
Epoch 362/512
512/512 - 0s - loss: 3.5960e-06 - val_loss: 3.6504e-08
Epoch 363/512
512/512 - 0s - loss: 3.3155e-06 - val_loss: 3.3566e-08
Epoch 364/512
512/512 - 0s - loss: 3.1470e-06 - val_loss: 3.2834e-08
Epoch 365/512
512/512 - 0s - loss: 3.0850e-06 - val_loss: 3.1688e-08
Epoch 366/512
512/512 - 0s - loss: 2.9392e-06 - val_loss: 2.9413e-08
Epoch 367/512
512/512 - 0s - loss: 2.7214e-06 - val_loss: 2.8687e-08
Epoch 368/512
512/512 - 0s - loss: 2.7147e-06 - val_loss: 2.7855e-08
Epoch 369/512
512/512 - 0s - loss: 2.5509e-06 - val_loss: 2.5872e-08
Epoch 370/512
512/512 - 0s - loss: 2.4015e-06 - val_loss: 2.4882e-08
Epoch 371/512
512/512 - 0s - loss: 2.3534e-06 - val_loss: 2.3781e-08
Epoch 372/512
512/512 - 0s - loss: 2.1924e-06 - val_loss: 2.2828e-08
Epoch 373/512
512/512 - 0s - loss: 2.1171e-06 - val_loss: 2.2379e-08
Epoch 374/512
512/512 - 0s - loss: 2.0672e-06 - val_loss: 2.0791e-08
Epoch 375/512
512/512 - 0s - loss: 1.9150e-06 - val_loss: 1.9398e-08
Epoch 376/512
512/512 - 0s - loss: 1.8292e-06 - val_loss: 1.9061e-08
Epoch 377/512
512/512 - 0s - loss: 1.7861e-06 - val_loss: 1.8277e-08
Epoch 378/512
512/512 - 0s - loss: 1.6724e-06 - val_loss: 1.7240e-08
Epoch 379/512
512/512 - 0s - loss: 1.6107e-06 - val_loss: 1.6350e-08
Epoch 380/512
512/512 - 0s - loss: 1.5193e-06 - val_loss: 1.5846e-08
Epoch 381/512
512/512 - 0s - loss: 1.4767e-06 - val_loss: 1.5120e-08
Epoch 382/512
512/512 - 0s - loss: 1.3979e-06 - val_loss: 1.4239e-08
Epoch 383/512
512/512 - 0s - loss: 1.3214e-06 - val_loss: 1.3702e-08
Epoch 384/512
512/512 - 0s - loss: 1.2980e-06 - val_loss: 1.2812e-08
Epoch 385/512
512/512 - 0s - loss: 1.1846e-06 - val_loss: 1.2161e-08
Epoch 386/512
512/512 - 0s - loss: 1.1467e-06 - val_loss: 1.2150e-08
Epoch 387/512
512/512 - 0s - loss: 1.1318e-06 - val_loss: 1.1500e-08
Epoch 388/512
512/512 - 0s - loss: 1.0437e-06 - val_loss: 1.0597e-08
Epoch 389/512
512/512 - 0s - loss: 9.9026e-07 - val_loss: 1.0359e-08
Epoch 390/512
512/512 - 0s - loss: 9.6137e-07 - val_loss: 1.0071e-08
Epoch 391/512
512/512 - 0s - loss: 9.2341e-07 - val_loss: 9.3516e-09
Epoch 392/512
512/512 - 0s - loss: 8.6053e-07 - val_loss: 8.8428e-09
Epoch 393/512
512/512 - 0s - loss: 8.2235e-07 - val_loss: 8.5973e-09
Epoch 394/512
512/512 - 0s - loss: 7.9902e-07 - val_loss: 8.1374e-09
Epoch 395/512
512/512 - 0s - loss: 7.4741e-07 - val_loss: 7.6694e-09
Epoch 396/512
512/512 - 0s - loss: 7.1986e-07 - val_loss: 7.3261e-09
Epoch 397/512
512/512 - 0s - loss: 6.7829e-07 - val_loss: 7.0217e-09
Epoch 398/512
512/512 - 0s - loss: 6.5584e-07 - val_loss: 6.7065e-09
Epoch 399/512
512/512 - 0s - loss: 6.2164e-07 - val_loss: 6.2258e-09
Epoch 400/512
512/512 - 0s - loss: 5.7903e-07 - val_loss: 6.1110e-09
Epoch 401/512
512/512 - 0s - loss: 5.6827e-07 - val_loss: 5.9430e-09
Epoch 402/512
512/512 - 0s - loss: 5.4273e-07 - val_loss: 5.5339e-09
Epoch 403/512
512/512 - 0s - loss: 5.0513e-07 - val_loss: 5.1739e-09
Epoch 404/512
512/512 - 0s - loss: 4.8136e-07 - val_loss: 5.0846e-09
Epoch 405/512
512/512 - 0s - loss: 4.7302e-07 - val_loss: 4.7590e-09
Epoch 406/512
512/512 - 0s - loss: 4.3680e-07 - val_loss: 4.4107e-09
Epoch 407/512
512/512 - 0s - loss: 4.0836e-07 - val_loss: 4.4154e-09
Epoch 408/512
512/512 - 0s - loss: 4.1359e-07 - val_loss: 4.2093e-09
Epoch 409/512
512/512 - 0s - loss: 3.8135e-07 - val_loss: 3.7780e-09
Epoch 410/512
512/512 - 0s - loss: 3.4983e-07 - val_loss: 3.6859e-09
Epoch 411/512
512/512 - 0s - loss: 3.4705e-07 - val_loss: 3.7097e-09
Epoch 412/512
512/512 - 0s - loss: 3.3491e-07 - val_loss: 3.4246e-09
Epoch 413/512
512/512 - 0s - loss: 3.1003e-07 - val_loss: 3.1352e-09
Epoch 414/512
512/512 - 0s - loss: 2.9280e-07 - val_loss: 3.0293e-09
Epoch 415/512
512/512 - 0s - loss: 2.8222e-07 - val_loss: 2.9949e-09
Epoch 416/512
512/512 - 0s - loss: 2.7488e-07 - val_loss: 2.7886e-09
Epoch 417/512
512/512 - 0s - loss: 2.5104e-07 - val_loss: 2.6152e-09
Epoch 418/512
512/512 - 0s - loss: 2.4209e-07 - val_loss: 2.5733e-09
Epoch 419/512
512/512 - 0s - loss: 2.3781e-07 - val_loss: 2.3822e-09
Epoch 420/512
512/512 - 0s - loss: 2.1624e-07 - val_loss: 2.2206e-09
Epoch 421/512
512/512 - 0s - loss: 2.0778e-07 - val_loss: 2.1961e-09
Epoch 422/512
512/512 - 0s - loss: 2.0077e-07 - val_loss: 2.1183e-09
Epoch 423/512
512/512 - 0s - loss: 1.9135e-07 - val_loss: 1.9702e-09
Epoch 424/512
512/512 - 0s - loss: 1.8133e-07 - val_loss: 1.8265e-09
Epoch 425/512
512/512 - 0s - loss: 1.6753e-07 - val_loss: 1.7741e-09
Epoch 426/512
512/512 - 0s - loss: 1.6472e-07 - val_loss: 1.7367e-09
Epoch 427/512
512/512 - 0s - loss: 1.5822e-07 - val_loss: 1.5794e-09
Epoch 428/512
512/512 - 0s - loss: 1.4370e-07 - val_loss: 1.5028e-09
Epoch 429/512
512/512 - 0s - loss: 1.4210e-07 - val_loss: 1.4610e-09
Epoch 430/512
512/512 - 0s - loss: 1.3346e-07 - val_loss: 1.3692e-09
Epoch 431/512
512/512 - 0s - loss: 1.2648e-07 - val_loss: 1.3089e-09
Epoch 432/512
512/512 - 0s - loss: 1.2043e-07 - val_loss: 1.2557e-09
Epoch 433/512
512/512 - 0s - loss: 1.1674e-07 - val_loss: 1.1683e-09
Epoch 434/512
512/512 - 0s - loss: 1.0720e-07 - val_loss: 1.1005e-09
Epoch 435/512
512/512 - 0s - loss: 1.0150e-07 - val_loss: 1.1146e-09
Epoch 436/512
512/512 - 0s - loss: 1.0236e-07 - val_loss: 1.0484e-09
Epoch 437/512
512/512 - 0s - loss: 9.3712e-08 - val_loss: 9.4404e-10
Epoch 438/512
512/512 - 0s - loss: 8.6392e-08 - val_loss: 9.2151e-10
Epoch 439/512
512/512 - 0s - loss: 8.5608e-08 - val_loss: 9.1325e-10
Epoch 440/512
512/512 - 0s - loss: 8.3116e-08 - val_loss: 8.3433e-10
Epoch 441/512
512/512 - 0s - loss: 7.5554e-08 - val_loss: 7.6896e-10
Epoch 442/512
512/512 - 0s - loss: 7.1736e-08 - val_loss: 7.5750e-10
Epoch 443/512
512/512 - 0s - loss: 6.9978e-08 - val_loss: 7.3431e-10
Epoch 444/512
512/512 - 0s - loss: 6.6858e-08 - val_loss: 6.8443e-10
Epoch 445/512
512/512 - 0s - loss: 6.2630e-08 - val_loss: 6.3042e-10
Epoch 446/512
512/512 - 0s - loss: 5.8244e-08 - val_loss: 6.1930e-10
Epoch 447/512
512/512 - 0s - loss: 5.7393e-08 - val_loss: 6.0296e-10
Epoch 448/512
512/512 - 0s - loss: 5.4680e-08 - val_loss: 5.6112e-10
Epoch 449/512
512/512 - 0s - loss: 5.0783e-08 - val_loss: 5.2665e-10
Epoch 450/512
512/512 - 0s - loss: 4.8820e-08 - val_loss: 5.0663e-10
Epoch 451/512
512/512 - 0s - loss: 4.6695e-08 - val_loss: 4.7731e-10
Epoch 452/512
512/512 - 0s - loss: 4.3392e-08 - val_loss: 4.6041e-10
Epoch 453/512
512/512 - 0s - loss: 4.2403e-08 - val_loss: 4.4482e-10
Epoch 454/512
512/512 - 0s - loss: 4.0276e-08 - val_loss: 4.2028e-10
Epoch 455/512
512/512 - 0s - loss: 3.8127e-08 - val_loss: 3.9514e-10
Epoch 456/512
512/512 - 0s - loss: 3.6325e-08 - val_loss: 3.6830e-10
Epoch 457/512
512/512 - 0s - loss: 3.4110e-08 - val_loss: 3.5152e-10
Epoch 458/512
512/512 - 0s - loss: 3.2714e-08 - val_loss: 3.4332e-10
Epoch 459/512
512/512 - 0s - loss: 3.1583e-08 - val_loss: 3.2463e-10
Epoch 460/512
512/512 - 0s - loss: 2.9586e-08 - val_loss: 3.0759e-10
Epoch 461/512
512/512 - 0s - loss: 2.8343e-08 - val_loss: 2.9622e-10
Epoch 462/512
512/512 - 0s - loss: 2.7262e-08 - val_loss: 2.7748e-10
Epoch 463/512
512/512 - 0s - loss: 2.5235e-08 - val_loss: 2.6546e-10
Epoch 464/512
512/512 - 0s - loss: 2.4715e-08 - val_loss: 2.5719e-10
Epoch 465/512
512/512 - 0s - loss: 2.3483e-08 - val_loss: 2.4094e-10
Epoch 466/512
512/512 - 0s - loss: 2.2139e-08 - val_loss: 2.2638e-10
Epoch 467/512
512/512 - 0s - loss: 2.0922e-08 - val_loss: 2.1904e-10
Epoch 468/512
512/512 - 0s - loss: 2.0268e-08 - val_loss: 2.1239e-10
Epoch 469/512
512/512 - 0s - loss: 1.9507e-08 - val_loss: 2.0015e-10
Epoch 470/512
512/512 - 0s - loss: 1.8114e-08 - val_loss: 1.9059e-10
Epoch 471/512
512/512 - 0s - loss: 1.7871e-08 - val_loss: 1.8058e-10
Epoch 472/512
512/512 - 0s - loss: 1.6586e-08 - val_loss: 1.6934e-10
Epoch 473/512
512/512 - 0s - loss: 1.5645e-08 - val_loss: 1.6540e-10
Epoch 474/512
512/512 - 0s - loss: 1.5364e-08 - val_loss: 1.6105e-10
Epoch 475/512
512/512 - 0s - loss: 1.4865e-08 - val_loss: 1.5108e-10
Epoch 476/512
512/512 - 0s - loss: 1.3769e-08 - val_loss: 1.4131e-10
Epoch 477/512
512/512 - 0s - loss: 1.3187e-08 - val_loss: 1.3543e-10
Epoch 478/512
512/512 - 0s - loss: 1.2572e-08 - val_loss: 1.3114e-10
Epoch 479/512
512/512 - 0s - loss: 1.2145e-08 - val_loss: 1.2690e-10
Epoch 480/512
512/512 - 0s - loss: 1.1775e-08 - val_loss: 1.1955e-10
Epoch 481/512
512/512 - 0s - loss: 1.0936e-08 - val_loss: 1.1425e-10
Epoch 482/512
512/512 - 0s - loss: 1.0615e-08 - val_loss: 1.1060e-10
Epoch 483/512
512/512 - 0s - loss: 1.0229e-08 - val_loss: 1.0518e-10
Epoch 484/512
512/512 - 0s - loss: 9.7263e-09 - val_loss: 9.9110e-11
Epoch 485/512
512/512 - 0s - loss: 9.2276e-09 - val_loss: 9.4596e-11
Epoch 486/512
512/512 - 0s - loss: 8.7663e-09 - val_loss: 9.2628e-11
Epoch 487/512
512/512 - 0s - loss: 8.6179e-09 - val_loss: 9.0239e-11
Epoch 488/512
512/512 - 0s - loss: 8.3263e-09 - val_loss: 8.4961e-11
Epoch 489/512
512/512 - 0s - loss: 7.8078e-09 - val_loss: 8.0152e-11
Epoch 490/512
512/512 - 0s - loss: 7.3982e-09 - val_loss: 7.7613e-11
Epoch 491/512
512/512 - 0s - loss: 7.3073e-09 - val_loss: 7.4597e-11
Epoch 492/512
512/512 - 0s - loss: 6.9238e-09 - val_loss: 7.1056e-11
Epoch 493/512
512/512 - 0s - loss: 6.6094e-09 - val_loss: 6.8767e-11
Epoch 494/512
512/512 - 0s - loss: 6.4354e-09 - val_loss: 6.5238e-11
Epoch 495/512
512/512 - 0s - loss: 6.0836e-09 - val_loss: 6.3039e-11
Epoch 496/512
512/512 - 0s - loss: 5.8857e-09 - val_loss: 6.0494e-11
Epoch 497/512
512/512 - 0s - loss: 5.6761e-09 - val_loss: 5.8438e-11
Epoch 498/512
512/512 - 0s - loss: 5.4488e-09 - val_loss: 5.5570e-11
Epoch 499/512
512/512 - 0s - loss: 5.2065e-09 - val_loss: 5.3803e-11
Epoch 500/512
512/512 - 0s - loss: 5.0360e-09 - val_loss: 5.2489e-11
Epoch 501/512
512/512 - 0s - loss: 4.9551e-09 - val_loss: 5.0535e-11
Epoch 502/512
512/512 - 0s - loss: 4.7331e-09 - val_loss: 4.7418e-11
Epoch 503/512
512/512 - 0s - loss: 4.4111e-09 - val_loss: 4.5293e-11
Epoch 504/512
512/512 - 0s - loss: 4.2787e-09 - val_loss: 4.4044e-11
Epoch 505/512
512/512 - 0s - loss: 4.1835e-09 - val_loss: 4.3488e-11
Epoch 506/512
512/512 - 0s - loss: 4.0643e-09 - val_loss: 4.2106e-11
Epoch 507/512
512/512 - 0s - loss: 3.9569e-09 - val_loss: 4.0432e-11
Epoch 508/512
512/512 - 0s - loss: 3.7863e-09 - val_loss: 3.8431e-11
Epoch 509/512
512/512 - 0s - loss: 3.5883e-09 - val_loss: 3.6909e-11
Epoch 510/512
512/512 - 0s - loss: 3.4701e-09 - val_loss: 3.5619e-11
Epoch 511/512
512/512 - 0s - loss: 3.3624e-09 - val_loss: 3.4245e-11
Epoch 512/512
512/512 - 0s - loss: 3.2440e-09 - val_loss: 3.3496e-11
2024-04-10 08:50:32.980612: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4620e-09 - val_loss: 6.2436e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5393e-09 - val_loss: 4.5798e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5675e-09 - val_loss: 2.7232e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4075e-09 - val_loss: 2.4096e-09
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4975e-09 - val_loss: 3.0916e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2919e-09 - val_loss: 3.9411e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7056e-09 - val_loss: 3.6065e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1360e-09 - val_loss: 2.8343e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5468e-09 - val_loss: 2.4904e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4148e-09 - val_loss: 2.6386e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6309e-09 - val_loss: 2.9306e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8001e-09 - val_loss: 2.8763e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6397e-09 - val_loss: 2.5732e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3513e-09 - val_loss: 2.3311e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2194e-09 - val_loss: 2.3036e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2103e-09 - val_loss: 2.3388e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2359e-09 - val_loss: 2.3243e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2033e-09 - val_loss: 2.2503e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1178e-09 - val_loss: 2.1341e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9855e-09 - val_loss: 1.9986e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8945e-09 - val_loss: 1.9638e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8840e-09 - val_loss: 1.9758e-09
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8734e-09 - val_loss: 1.9169e-09
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8124e-09 - val_loss: 1.8514e-09
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7402e-09 - val_loss: 1.7644e-09
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6759e-09 - val_loss: 1.7259e-09
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6399e-09 - val_loss: 1.6753e-09
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5900e-09 - val_loss: 1.6346e-09
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5650e-09 - val_loss: 1.6105e-09
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5222e-09 - val_loss: 1.5520e-09
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4849e-09 - val_loss: 1.5257e-09
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4448e-09 - val_loss: 1.4660e-09
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3916e-09 - val_loss: 1.4123e-09
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3484e-09 - val_loss: 1.3855e-09
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3190e-09 - val_loss: 1.3546e-09
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2975e-09 - val_loss: 1.3259e-09
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2696e-09 - val_loss: 1.3024e-09
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2457e-09 - val_loss: 1.2660e-09
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2062e-09 - val_loss: 1.2283e-09
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1683e-09 - val_loss: 1.1940e-09
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1379e-09 - val_loss: 1.1607e-09
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1109e-09 - val_loss: 1.1302e-09
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0873e-09 - val_loss: 1.1162e-09
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0755e-09 - val_loss: 1.1027e-09
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0558e-09 - val_loss: 1.0726e-09
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0251e-09 - val_loss: 1.0262e-09
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7301e-10 - val_loss: 9.8838e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4610e-10 - val_loss: 9.5905e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2159e-10 - val_loss: 9.6045e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2979e-10 - val_loss: 9.5305e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0469e-10 - val_loss: 9.1782e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8739e-10 - val_loss: 9.1444e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7383e-10 - val_loss: 8.8166e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4971e-10 - val_loss: 8.7438e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4286e-10 - val_loss: 8.5332e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1332e-10 - val_loss: 8.3053e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0126e-10 - val_loss: 8.1174e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7942e-10 - val_loss: 7.9898e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6554e-10 - val_loss: 7.7708e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4242e-10 - val_loss: 7.5464e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2913e-10 - val_loss: 7.4867e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1822e-10 - val_loss: 7.4166e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1738e-10 - val_loss: 7.3053e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8982e-10 - val_loss: 6.8068e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5546e-10 - val_loss: 6.6643e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4824e-10 - val_loss: 6.7020e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4905e-10 - val_loss: 6.6964e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5008e-10 - val_loss: 6.6621e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3905e-10 - val_loss: 6.4851e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2297e-10 - val_loss: 6.3738e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1471e-10 - val_loss: 6.2363e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9449e-10 - val_loss: 5.9873e-10
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7691e-10 - val_loss: 5.8479e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6928e-10 - val_loss: 5.8883e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6831e-10 - val_loss: 5.7487e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5472e-10 - val_loss: 5.5802e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3939e-10 - val_loss: 5.5101e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3376e-10 - val_loss: 5.4772e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2864e-10 - val_loss: 5.3392e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1806e-10 - val_loss: 5.2574e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1117e-10 - val_loss: 5.2391e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0267e-10 - val_loss: 5.0832e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9012e-10 - val_loss: 4.8965e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7219e-10 - val_loss: 4.7627e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5846e-10 - val_loss: 4.5538e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4156e-10 - val_loss: 4.5409e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4648e-10 - val_loss: 4.6485e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4989e-10 - val_loss: 4.6130e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4994e-10 - val_loss: 4.5230e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3112e-10 - val_loss: 4.3428e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2002e-10 - val_loss: 4.3363e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2236e-10 - val_loss: 4.2877e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1401e-10 - val_loss: 4.2191e-10
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0351e-10 - val_loss: 4.0180e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9497e-10 - val_loss: 4.0621e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9164e-10 - val_loss: 3.9858e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8439e-10 - val_loss: 3.8480e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7643e-10 - val_loss: 3.8529e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7639e-10 - val_loss: 3.8765e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7633e-10 - val_loss: 3.7817e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6736e-10 - val_loss: 3.7066e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5545e-10 - val_loss: 3.5110e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4035e-10 - val_loss: 3.4484e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4346e-10 - val_loss: 3.5400e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4285e-10 - val_loss: 3.4899e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4042e-10 - val_loss: 3.4509e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3665e-10 - val_loss: 3.4618e-10
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3243e-10 - val_loss: 3.3277e-10
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1819e-10 - val_loss: 3.1633e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0745e-10 - val_loss: 3.1099e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0878e-10 - val_loss: 3.2226e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1284e-10 - val_loss: 3.1275e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0590e-10 - val_loss: 3.1375e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0469e-10 - val_loss: 3.0574e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9480e-10 - val_loss: 2.9152e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8452e-10 - val_loss: 2.9062e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8309e-10 - val_loss: 2.8772e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7918e-10 - val_loss: 2.7977e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7345e-10 - val_loss: 2.8044e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7467e-10 - val_loss: 2.7625e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6950e-10 - val_loss: 2.7382e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6964e-10 - val_loss: 2.7476e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6930e-10 - val_loss: 2.7651e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7076e-10 - val_loss: 2.7736e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6784e-10 - val_loss: 2.6789e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5791e-10 - val_loss: 2.5539e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4902e-10 - val_loss: 2.5326e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4724e-10 - val_loss: 2.5444e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5066e-10 - val_loss: 2.5657e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4733e-10 - val_loss: 2.4564e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3878e-10 - val_loss: 2.4470e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3752e-10 - val_loss: 2.3084e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2406e-10 - val_loss: 2.2862e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2313e-10 - val_loss: 2.1918e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1685e-10 - val_loss: 2.2387e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2088e-10 - val_loss: 2.2688e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1866e-10 - val_loss: 2.2307e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2129e-10 - val_loss: 2.2550e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2137e-10 - val_loss: 2.2091e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1317e-10 - val_loss: 2.1418e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0448e-10 - val_loss: 1.9964e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9431e-10 - val_loss: 2.0122e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9905e-10 - val_loss: 2.0682e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0507e-10 - val_loss: 2.0581e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0032e-10 - val_loss: 2.0446e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0246e-10 - val_loss: 2.0503e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9820e-10 - val_loss: 2.0342e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9806e-10 - val_loss: 1.9656e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9105e-10 - val_loss: 1.9046e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8699e-10 - val_loss: 1.9198e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8406e-10 - val_loss: 1.8172e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7462e-10 - val_loss: 1.7557e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7384e-10 - val_loss: 1.8098e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7969e-10 - val_loss: 1.8515e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8043e-10 - val_loss: 1.8359e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7866e-10 - val_loss: 1.7648e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7477e-10 - val_loss: 1.7684e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7054e-10 - val_loss: 1.6768e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6631e-10 - val_loss: 1.6985e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6620e-10 - val_loss: 1.6876e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6409e-10 - val_loss: 1.6726e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6506e-10 - val_loss: 1.6623e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6334e-10 - val_loss: 1.6201e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6131e-10 - val_loss: 1.6776e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6162e-10 - val_loss: 1.6494e-10
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6018e-10 - val_loss: 1.6084e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5682e-10 - val_loss: 1.5637e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5223e-10 - val_loss: 1.5304e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4847e-10 - val_loss: 1.5129e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4736e-10 - val_loss: 1.5084e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5104e-10 - val_loss: 1.5534e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5376e-10 - val_loss: 1.5604e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5325e-10 - val_loss: 1.5135e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4779e-10 - val_loss: 1.4771e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4394e-10 - val_loss: 1.4450e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4155e-10 - val_loss: 1.4255e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3953e-10 - val_loss: 1.4085e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3892e-10 - val_loss: 1.3670e-10
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3111e-10 - val_loss: 1.3071e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2733e-10 - val_loss: 1.3000e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2892e-10 - val_loss: 1.3230e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3123e-10 - val_loss: 1.3435e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3189e-10 - val_loss: 1.3512e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3206e-10 - val_loss: 1.3316e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2970e-10 - val_loss: 1.3157e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2848e-10 - val_loss: 1.3354e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3046e-10 - val_loss: 1.3593e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3347e-10 - val_loss: 1.3547e-10
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3283e-10 - val_loss: 1.3538e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3206e-10 - val_loss: 1.3703e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3384e-10 - val_loss: 1.3835e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3417e-10 - val_loss: 1.3683e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3066e-10 - val_loss: 1.2317e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1914e-10 - val_loss: 1.1773e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1305e-10 - val_loss: 1.1019e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1108e-10 - val_loss: 1.1315e-10
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1196e-10 - val_loss: 1.1172e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1150e-10 - val_loss: 1.1184e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1007e-10 - val_loss: 1.1575e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1458e-10 - val_loss: 1.1736e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1312e-10 - val_loss: 1.1020e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0793e-10 - val_loss: 1.0692e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0365e-10 - val_loss: 1.0395e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0522e-10 - val_loss: 1.1293e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1186e-10 - val_loss: 1.1615e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1410e-10 - val_loss: 1.1818e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1614e-10 - val_loss: 1.1834e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1582e-10 - val_loss: 1.1471e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1244e-10 - val_loss: 1.1152e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0798e-10 - val_loss: 1.0816e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0537e-10 - val_loss: 1.0295e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0215e-10 - val_loss: 1.0556e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0393e-10 - val_loss: 1.0278e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8991e-11 - val_loss: 9.5336e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.2986e-11 - val_loss: 9.2447e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.0283e-11 - val_loss: 8.8929e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0583e-11 - val_loss: 9.6802e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7291e-11 - val_loss: 1.0023e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0141e-10 - val_loss: 1.0219e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9040e-11 - val_loss: 1.0206e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9264e-11 - val_loss: 9.7059e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5530e-11 - val_loss: 9.6563e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3797e-11 - val_loss: 9.3686e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1478e-11 - val_loss: 9.0999e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9335e-11 - val_loss: 9.0285e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8394e-11 - val_loss: 8.8385e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8359e-11 - val_loss: 8.8789e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6472e-11 - val_loss: 8.7730e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6615e-11 - val_loss: 8.7752e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8369e-11 - val_loss: 9.0742e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8289e-11 - val_loss: 9.2047e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9331e-11 - val_loss: 8.8029e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9019e-11 - val_loss: 9.0454e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7552e-11 - val_loss: 8.7649e-11
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5250e-11 - val_loss: 8.5345e-11
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3909e-11 - val_loss: 8.4815e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3849e-11 - val_loss: 8.1978e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8273e-11 - val_loss: 8.0135e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9641e-11 - val_loss: 7.8877e-11
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7299e-11 - val_loss: 7.8600e-11
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6617e-11 - val_loss: 7.7900e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8891e-11 - val_loss: 7.8955e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6714e-11 - val_loss: 7.5311e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4624e-11 - val_loss: 7.6359e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5954e-11 - val_loss: 7.8438e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6540e-11 - val_loss: 7.6325e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4401e-11 - val_loss: 7.6120e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6231e-11 - val_loss: 7.9898e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1294e-11 - val_loss: 8.4231e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1585e-11 - val_loss: 8.3617e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9770e-11 - val_loss: 7.6425e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3910e-11 - val_loss: 7.0621e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8650e-11 - val_loss: 6.9289e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7637e-11 - val_loss: 6.7906e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7913e-11 - val_loss: 6.9468e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8583e-11 - val_loss: 6.9980e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1109e-11 - val_loss: 7.5473e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4211e-11 - val_loss: 7.4224e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4061e-11 - val_loss: 7.2901e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9662e-11 - val_loss: 6.9233e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8398e-11 - val_loss: 6.8877e-11
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7382e-11 - val_loss: 6.7781e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7593e-11 - val_loss: 6.8484e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7794e-11 - val_loss: 6.7788e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8498e-11 - val_loss: 7.0167e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8240e-11 - val_loss: 6.5670e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4224e-11 - val_loss: 6.7428e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7729e-11 - val_loss: 6.8778e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8741e-11 - val_loss: 6.9150e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7284e-11 - val_loss: 6.4942e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2930e-11 - val_loss: 6.4273e-11
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4296e-11 - val_loss: 6.3885e-11
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0724e-11 - val_loss: 5.9380e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1112e-11 - val_loss: 6.3875e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1821e-11 - val_loss: 6.0730e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2133e-11 - val_loss: 6.1876e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8670e-11 - val_loss: 5.7619e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5749e-11 - val_loss: 5.4039e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4030e-11 - val_loss: 5.7723e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8949e-11 - val_loss: 6.5190e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5210e-11 - val_loss: 6.4219e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1491e-11 - val_loss: 6.1302e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1359e-11 - val_loss: 6.1741e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9336e-11 - val_loss: 5.8221e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7170e-11 - val_loss: 5.7279e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6966e-11 - val_loss: 5.8865e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9251e-11 - val_loss: 6.3317e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2196e-11 - val_loss: 6.0624e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9740e-11 - val_loss: 6.2992e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2399e-11 - val_loss: 6.0381e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7696e-11 - val_loss: 5.5236e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.3157e-11 - val_loss: 5.2081e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2835e-11 - val_loss: 5.4021e-11
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1756e-11 - val_loss: 5.0826e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1250e-11 - val_loss: 5.5351e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5908e-11 - val_loss: 5.5197e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2126e-11 - val_loss: 5.0611e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2938e-11 - val_loss: 5.5408e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3848e-11 - val_loss: 5.2790e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3496e-11 - val_loss: 5.6130e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6884e-11 - val_loss: 5.8237e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7308e-11 - val_loss: 5.6686e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6540e-11 - val_loss: 5.6257e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3974e-11 - val_loss: 5.2785e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3783e-11 - val_loss: 5.4016e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2271e-11 - val_loss: 5.0935e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0049e-11 - val_loss: 4.7832e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5347e-11 - val_loss: 4.3748e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1756e-11 - val_loss: 4.1063e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1980e-11 - val_loss: 4.4008e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4646e-11 - val_loss: 4.4982e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5241e-11 - val_loss: 4.7654e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6956e-11 - val_loss: 4.6429e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5819e-11 - val_loss: 4.6576e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7522e-11 - val_loss: 5.1434e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1302e-11 - val_loss: 5.3344e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2757e-11 - val_loss: 5.1942e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1318e-11 - val_loss: 5.1218e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9677e-11 - val_loss: 4.7333e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8217e-11 - val_loss: 4.9287e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8816e-11 - val_loss: 4.8047e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6305e-11 - val_loss: 4.5329e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4362e-11 - val_loss: 4.7059e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6552e-11 - val_loss: 4.5789e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5546e-11 - val_loss: 4.6346e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8045e-11 - val_loss: 5.1062e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0471e-11 - val_loss: 4.8529e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5583e-11 - val_loss: 4.3930e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5662e-11 - val_loss: 4.7024e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6026e-11 - val_loss: 4.4432e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2972e-11 - val_loss: 4.1317e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9888e-11 - val_loss: 3.9046e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9350e-11 - val_loss: 4.1556e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1514e-11 - val_loss: 4.2619e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1670e-11 - val_loss: 4.0569e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9262e-11 - val_loss: 4.1148e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2260e-11 - val_loss: 4.4535e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4511e-11 - val_loss: 4.7818e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6472e-11 - val_loss: 4.4574e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3200e-11 - val_loss: 4.3425e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2954e-11 - val_loss: 4.1634e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9598e-11 - val_loss: 3.8112e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9159e-11 - val_loss: 4.2423e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2604e-11 - val_loss: 4.1948e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0678e-11 - val_loss: 3.9741e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9930e-11 - val_loss: 4.2145e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2448e-11 - val_loss: 4.3753e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5689e-11 - val_loss: 4.9442e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7491e-11 - val_loss: 4.4930e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3600e-11 - val_loss: 4.0968e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8477e-11 - val_loss: 3.7340e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6514e-11 - val_loss: 3.5265e-11
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3890e-11 - val_loss: 3.2632e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2311e-11 - val_loss: 3.5104e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6138e-11 - val_loss: 3.7836e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8212e-11 - val_loss: 4.0285e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1039e-11 - val_loss: 4.0418e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8935e-11 - val_loss: 3.8881e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7972e-11 - val_loss: 3.8713e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9760e-11 - val_loss: 4.2448e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0695e-11 - val_loss: 3.8240e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7227e-11 - val_loss: 3.6315e-11
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4898e-11 - val_loss: 3.2621e-11
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1589e-11 - val_loss: 3.0746e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2702e-11 - val_loss: 3.4691e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4866e-11 - val_loss: 3.4381e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3262e-11 - val_loss: 3.3041e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1643e-11 - val_loss: 3.0778e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9292e-11 - val_loss: 2.7567e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6473e-11 - val_loss: 2.6530e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8340e-11 - val_loss: 3.0275e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1482e-11 - val_loss: 3.2368e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2777e-11 - val_loss: 3.4898e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5267e-11 - val_loss: 3.6659e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6037e-11 - val_loss: 3.4228e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3853e-11 - val_loss: 3.4333e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4752e-11 - val_loss: 3.6631e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5884e-11 - val_loss: 3.6941e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5377e-11 - val_loss: 3.2894e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1626e-11 - val_loss: 3.1048e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1569e-11 - val_loss: 3.3499e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4745e-11 - val_loss: 3.7274e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5669e-11 - val_loss: 3.3475e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2391e-11 - val_loss: 3.1501e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3087e-11 - val_loss: 3.6139e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6274e-11 - val_loss: 3.6831e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5700e-11 - val_loss: 3.4228e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2598e-11 - val_loss: 3.1076e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9294e-11 - val_loss: 2.7495e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6870e-11 - val_loss: 2.7364e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8761e-11 - val_loss: 3.1009e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1507e-11 - val_loss: 3.2027e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0419e-11 - val_loss: 2.8492e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7111e-11 - val_loss: 2.5689e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5111e-11 - val_loss: 2.5502e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7426e-11 - val_loss: 2.9619e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9482e-11 - val_loss: 3.0486e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0604e-11 - val_loss: 3.2754e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3513e-11 - val_loss: 3.3693e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3289e-11 - val_loss: 3.4049e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2295e-11 - val_loss: 3.0732e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9117e-11 - val_loss: 2.8085e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7156e-11 - val_loss: 2.6293e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5481e-11 - val_loss: 2.5188e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7056e-11 - val_loss: 2.9428e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9343e-11 - val_loss: 2.9454e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8612e-11 - val_loss: 2.7001e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6319e-11 - val_loss: 2.5983e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5343e-11 - val_loss: 2.5462e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7013e-11 - val_loss: 3.0312e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9812e-11 - val_loss: 3.0814e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1829e-11 - val_loss: 3.4629e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4737e-11 - val_loss: 3.4138e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2528e-11 - val_loss: 3.0163e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9468e-11 - val_loss: 2.9596e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8673e-11 - val_loss: 2.7142e-11
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5883e-11 - val_loss: 2.4867e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5643e-11 - val_loss: 2.7493e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8312e-11 - val_loss: 2.9207e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9368e-11 - val_loss: 2.9464e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8631e-11 - val_loss: 2.7598e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6979e-11 - val_loss: 2.6915e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8013e-11 - val_loss: 3.0851e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0719e-11 - val_loss: 3.0262e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0178e-11 - val_loss: 2.9385e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8003e-11 - val_loss: 2.6887e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5817e-11 - val_loss: 2.5263e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6812e-11 - val_loss: 2.9362e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9720e-11 - val_loss: 2.9955e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0598e-11 - val_loss: 3.3294e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4066e-11 - val_loss: 3.3847e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2131e-11 - val_loss: 3.0768e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9877e-11 - val_loss: 2.9945e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8770e-11 - val_loss: 2.6508e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5577e-11 - val_loss: 2.4960e-11
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4493e-11 - val_loss: 2.4097e-11
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3088e-11 - val_loss: 2.0975e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0655e-11 - val_loss: 2.0213e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9936e-11 - val_loss: 2.0107e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9608e-11 - val_loss: 1.8788e-11
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8397e-11 - val_loss: 1.8062e-11
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7500e-11 - val_loss: 1.7674e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8659e-11 - val_loss: 2.0134e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0501e-11 - val_loss: 2.1489e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1294e-11 - val_loss: 2.2101e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3670e-11 - val_loss: 2.5147e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5645e-11 - val_loss: 2.6532e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5945e-11 - val_loss: 2.5964e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5821e-11 - val_loss: 2.7129e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7605e-11 - val_loss: 2.8478e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8979e-11 - val_loss: 2.8174e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6467e-11 - val_loss: 2.5197e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4655e-11 - val_loss: 2.5147e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4486e-11 - val_loss: 2.4993e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6099e-11 - val_loss: 2.7763e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8225e-11 - val_loss: 2.7981e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6500e-11 - val_loss: 2.5431e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4619e-11 - val_loss: 2.3798e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3783e-11 - val_loss: 2.3054e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1939e-11 - val_loss: 2.0717e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0314e-11 - val_loss: 2.0626e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0304e-11 - val_loss: 2.0593e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1785e-11 - val_loss: 2.4233e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3992e-11 - val_loss: 2.4034e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4560e-11 - val_loss: 2.5253e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6438e-11 - val_loss: 2.7632e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8592e-11 - val_loss: 2.9769e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8272e-11 - val_loss: 2.6763e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5787e-11 - val_loss: 2.5186e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4911e-11 - val_loss: 2.5383e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5880e-11 - val_loss: 2.7848e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6903e-11 - val_loss: 2.4560e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3608e-11 - val_loss: 2.3192e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3129e-11 - val_loss: 2.3313e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2154e-11 - val_loss: 2.0571e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9310e-11 - val_loss: 1.8501e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8792e-11 - val_loss: 1.9387e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9167e-11 - val_loss: 1.8769e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7735e-11 - val_loss: 1.6536e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6338e-11 - val_loss: 1.6353e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5983e-11 - val_loss: 1.5824e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6265e-11 - val_loss: 1.6478e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7381e-11 - val_loss: 1.9215e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9334e-11 - val_loss: 1.9921e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9540e-11 - val_loss: 2.0309e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0918e-11 - val_loss: 2.3296e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4026e-11 - val_loss: 2.4570e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4691e-11 - val_loss: 2.4969e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3167e-11 - val_loss: 2.0786e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0467e-11 - val_loss: 2.0291e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0021e-11 - val_loss: 2.0233e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0075e-11 - val_loss: 1.7843e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7074e-11 - val_loss: 1.6969e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6776e-11 - val_loss: 1.6928e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6788e-11 - val_loss: 1.7091e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8237e-11 - val_loss: 1.9355e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9801e-11 - val_loss: 2.1112e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0865e-11 - val_loss: 2.0975e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0566e-11 - val_loss: 2.0620e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2119e-11 - val_loss: 2.4060e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4058e-11 - val_loss: 2.5091e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4998e-11 - val_loss: 2.3372e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1398e-11 - val_loss: 1.9788e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9938e-11 - val_loss: 1.9483e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9518e-11 - val_loss: 1.9864e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8783e-11 - val_loss: 1.6542e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5961e-11 - val_loss: 1.5675e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5341e-11 - val_loss: 1.5309e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5483e-11 - val_loss: 1.5892e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7162e-11 - val_loss: 1.8516e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8819e-11 - val_loss: 1.9894e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9808e-11 - val_loss: 1.9795e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.0280 - val_loss: 0.0048
Epoch 2/512
512/512 - 0s - loss: 0.0156 - val_loss: 0.0027
Epoch 3/512
512/512 - 0s - loss: 0.0118 - val_loss: 0.0021
Epoch 4/512
512/512 - 0s - loss: 0.0091 - val_loss: 0.0017
Epoch 5/512
512/512 - 0s - loss: 0.0069 - val_loss: 0.0015
Epoch 6/512
512/512 - 0s - loss: 0.0052 - val_loss: 0.0014
Epoch 7/512
512/512 - 0s - loss: 0.0039 - val_loss: 0.0013
Epoch 8/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0012
Epoch 9/512
512/512 - 0s - loss: 0.0023 - val_loss: 0.0012
Epoch 10/512
512/512 - 0s - loss: 0.0018 - val_loss: 0.0012
Epoch 11/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0011
Epoch 12/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 13/512
512/512 - 0s - loss: 8.6530e-04 - val_loss: 0.0011
Epoch 14/512
512/512 - 0s - loss: 7.2313e-04 - val_loss: 0.0010
Epoch 15/512
512/512 - 0s - loss: 5.9722e-04 - val_loss: 0.0010
Epoch 16/512
512/512 - 0s - loss: 4.8553e-04 - val_loss: 9.8689e-04
Epoch 17/512
512/512 - 0s - loss: 4.1299e-04 - val_loss: 9.6161e-04
Epoch 18/512
512/512 - 0s - loss: 3.5674e-04 - val_loss: 9.3345e-04
Epoch 19/512
512/512 - 0s - loss: 2.9828e-04 - val_loss: 9.0517e-04
Epoch 20/512
512/512 - 0s - loss: 2.5091e-04 - val_loss: 8.7810e-04
Epoch 21/512
512/512 - 0s - loss: 2.1964e-04 - val_loss: 8.5186e-04
Epoch 22/512
512/512 - 0s - loss: 1.9131e-04 - val_loss: 8.2512e-04
Epoch 23/512
512/512 - 0s - loss: 1.6064e-04 - val_loss: 7.9855e-04
Epoch 24/512
512/512 - 0s - loss: 1.3863e-04 - val_loss: 7.7357e-04
Epoch 25/512
512/512 - 0s - loss: 1.2389e-04 - val_loss: 7.5046e-04
Epoch 26/512
512/512 - 0s - loss: 1.0713e-04 - val_loss: 7.2763e-04
Epoch 27/512
512/512 - 0s - loss: 9.2105e-05 - val_loss: 7.0529e-04
Epoch 28/512
512/512 - 0s - loss: 8.1895e-05 - val_loss: 6.8426e-04
Epoch 29/512
512/512 - 0s - loss: 7.4222e-05 - val_loss: 6.6440e-04
Epoch 30/512
512/512 - 0s - loss: 6.4855e-05 - val_loss: 6.4390e-04
Epoch 31/512
512/512 - 0s - loss: 5.6848e-05 - val_loss: 6.2352e-04
Epoch 32/512
512/512 - 0s - loss: 5.1102e-05 - val_loss: 6.0355e-04
Epoch 33/512
512/512 - 0s - loss: 4.6686e-05 - val_loss: 5.8422e-04
Epoch 34/512
512/512 - 0s - loss: 4.1251e-05 - val_loss: 5.6455e-04
Epoch 35/512
512/512 - 0s - loss: 3.6554e-05 - val_loss: 5.4508e-04
Epoch 36/512
512/512 - 0s - loss: 3.3111e-05 - val_loss: 5.2600e-04
Epoch 37/512
512/512 - 0s - loss: 3.0351e-05 - val_loss: 5.0729e-04
Epoch 38/512
512/512 - 0s - loss: 2.7086e-05 - val_loss: 4.8880e-04
Epoch 39/512
512/512 - 0s - loss: 2.4199e-05 - val_loss: 4.7066e-04
Epoch 40/512
512/512 - 0s - loss: 2.2081e-05 - val_loss: 4.5301e-04
Epoch 41/512
512/512 - 0s - loss: 2.0332e-05 - val_loss: 4.3569e-04
Epoch 42/512
512/512 - 0s - loss: 1.8184e-05 - val_loss: 4.1854e-04
Epoch 43/512
512/512 - 0s - loss: 1.6399e-05 - val_loss: 4.0194e-04
Epoch 44/512
512/512 - 0s - loss: 1.5107e-05 - val_loss: 3.8596e-04
Epoch 45/512
512/512 - 0s - loss: 1.3956e-05 - val_loss: 3.7114e-04
Epoch 46/512
512/512 - 0s - loss: 1.2546e-05 - val_loss: 3.5619e-04
Epoch 47/512
512/512 - 0s - loss: 1.2033e-05 - val_loss: 3.4190e-04
Epoch 48/512
512/512 - 0s - loss: 1.0539e-05 - val_loss: 3.2685e-04
Epoch 49/512
512/512 - 0s - loss: 9.4581e-06 - val_loss: 3.1318e-04
Epoch 50/512
512/512 - 0s - loss: 8.6969e-06 - val_loss: 3.0014e-04
Epoch 51/512
512/512 - 0s - loss: 8.1119e-06 - val_loss: 2.8817e-04
Epoch 52/512
512/512 - 0s - loss: 7.9418e-06 - val_loss: 2.7539e-04
Epoch 53/512
512/512 - 0s - loss: 6.8767e-06 - val_loss: 2.6372e-04
Epoch 54/512
512/512 - 0s - loss: 6.4927e-06 - val_loss: 2.5263e-04
Epoch 55/512
512/512 - 0s - loss: 5.9755e-06 - val_loss: 2.4196e-04
Epoch 56/512
512/512 - 0s - loss: 5.4460e-06 - val_loss: 2.3177e-04
Epoch 57/512
512/512 - 0s - loss: 5.0736e-06 - val_loss: 2.2205e-04
Epoch 58/512
512/512 - 0s - loss: 4.8029e-06 - val_loss: 2.1271e-04
Epoch 59/512
512/512 - 0s - loss: 4.4633e-06 - val_loss: 2.0384e-04
Epoch 60/512
512/512 - 0s - loss: 4.0732e-06 - val_loss: 1.9544e-04
Epoch 61/512
512/512 - 0s - loss: 3.8101e-06 - val_loss: 1.8744e-04
Epoch 62/512
512/512 - 0s - loss: 3.8035e-06 - val_loss: 1.7979e-04
Epoch 63/512
512/512 - 0s - loss: 3.3414e-06 - val_loss: 1.7251e-04
Epoch 64/512
512/512 - 0s - loss: 3.1219e-06 - val_loss: 1.6561e-04
Epoch 65/512
512/512 - 0s - loss: 2.9345e-06 - val_loss: 1.5904e-04
Epoch 66/512
512/512 - 0s - loss: 2.7800e-06 - val_loss: 1.5279e-04
Epoch 67/512
512/512 - 0s - loss: 2.7353e-06 - val_loss: 1.4691e-04
Epoch 68/512
512/512 - 0s - loss: 2.4829e-06 - val_loss: 1.4131e-04
Epoch 69/512
512/512 - 0s - loss: 2.3211e-06 - val_loss: 1.3599e-04
Epoch 70/512
512/512 - 0s - loss: 2.1982e-06 - val_loss: 1.3098e-04
Epoch 71/512
512/512 - 0s - loss: 2.1594e-06 - val_loss: 1.2625e-04
Epoch 72/512
512/512 - 0s - loss: 2.0176e-06 - val_loss: 1.2165e-04
Epoch 73/512
512/512 - 0s - loss: 1.8792e-06 - val_loss: 1.1736e-04
Epoch 74/512
512/512 - 0s - loss: 1.7885e-06 - val_loss: 1.1330e-04
Epoch 75/512
512/512 - 0s - loss: 1.7153e-06 - val_loss: 1.0945e-04
Epoch 76/512
512/512 - 0s - loss: 1.6891e-06 - val_loss: 1.0580e-04
Epoch 77/512
512/512 - 0s - loss: 1.5607e-06 - val_loss: 1.0234e-04
Epoch 78/512
512/512 - 0s - loss: 1.4866e-06 - val_loss: 9.9065e-05
Epoch 79/512
512/512 - 0s - loss: 1.4274e-06 - val_loss: 9.5988e-05
Epoch 80/512
512/512 - 0s - loss: 1.4129e-06 - val_loss: 9.3039e-05
Epoch 81/512
512/512 - 0s - loss: 1.3195e-06 - val_loss: 9.0187e-05
Epoch 82/512
512/512 - 0s - loss: 1.2595e-06 - val_loss: 8.7522e-05
Epoch 83/512
512/512 - 0s - loss: 1.2126e-06 - val_loss: 8.4999e-05
Epoch 84/512
512/512 - 0s - loss: 1.1815e-06 - val_loss: 8.2608e-05
Epoch 85/512
512/512 - 0s - loss: 1.1415e-06 - val_loss: 8.0277e-05
Epoch 86/512
512/512 - 0s - loss: 1.0953e-06 - val_loss: 7.8109e-05
Epoch 87/512
512/512 - 0s - loss: 1.0499e-06 - val_loss: 7.6052e-05
Epoch 88/512
512/512 - 0s - loss: 1.0177e-06 - val_loss: 7.4107e-05
Epoch 89/512
512/512 - 0s - loss: 9.9976e-07 - val_loss: 7.2240e-05
Epoch 90/512
512/512 - 0s - loss: 9.5978e-07 - val_loss: 7.0428e-05
Epoch 91/512
512/512 - 0s - loss: 9.2306e-07 - val_loss: 6.8708e-05
Epoch 92/512
512/512 - 0s - loss: 8.9538e-07 - val_loss: 6.7081e-05
Epoch 93/512
512/512 - 0s - loss: 8.7527e-07 - val_loss: 6.5536e-05
Epoch 94/512
512/512 - 0s - loss: 8.5205e-07 - val_loss: 6.4037e-05
Epoch 95/512
512/512 - 0s - loss: 8.2332e-07 - val_loss: 6.2600e-05
Epoch 96/512
512/512 - 0s - loss: 7.9959e-07 - val_loss: 6.1233e-05
Epoch 97/512
512/512 - 0s - loss: 7.8028e-07 - val_loss: 5.9932e-05
Epoch 98/512
512/512 - 0s - loss: 7.6303e-07 - val_loss: 5.8678e-05
Epoch 99/512
512/512 - 0s - loss: 7.4189e-07 - val_loss: 5.7470e-05
Epoch 100/512
512/512 - 0s - loss: 7.2202e-07 - val_loss: 5.6313e-05
Epoch 101/512
512/512 - 0s - loss: 7.0461e-07 - val_loss: 5.5204e-05
Epoch 102/512
512/512 - 0s - loss: 6.8962e-07 - val_loss: 5.4142e-05
Epoch 103/512
512/512 - 0s - loss: 6.7401e-07 - val_loss: 5.3116e-05
Epoch 104/512
512/512 - 0s - loss: 6.5788e-07 - val_loss: 5.2133e-05
Epoch 105/512
512/512 - 0s - loss: 6.4325e-07 - val_loss: 5.1184e-05
Epoch 106/512
512/512 - 0s - loss: 6.2934e-07 - val_loss: 5.0270e-05
Epoch 107/512
512/512 - 0s - loss: 6.1712e-07 - val_loss: 4.9391e-05
Epoch 108/512
512/512 - 0s - loss: 6.0424e-07 - val_loss: 4.8539e-05
Epoch 109/512
512/512 - 0s - loss: 5.9097e-07 - val_loss: 4.7723e-05
Epoch 110/512
512/512 - 0s - loss: 5.7954e-07 - val_loss: 4.6941e-05
Epoch 111/512
512/512 - 0s - loss: 5.6949e-07 - val_loss: 4.6178e-05
Epoch 112/512
512/512 - 0s - loss: 5.5840e-07 - val_loss: 4.5437e-05
Epoch 113/512
512/512 - 0s - loss: 5.4818e-07 - val_loss: 4.4721e-05
Epoch 114/512
512/512 - 0s - loss: 5.3745e-07 - val_loss: 4.4028e-05
Epoch 115/512
512/512 - 0s - loss: 5.2762e-07 - val_loss: 4.3355e-05
Epoch 116/512
512/512 - 0s - loss: 5.2002e-07 - val_loss: 4.2705e-05
Epoch 117/512
512/512 - 0s - loss: 5.1128e-07 - val_loss: 4.2089e-05
Epoch 118/512
512/512 - 0s - loss: 5.0148e-07 - val_loss: 4.1487e-05
Epoch 119/512
512/512 - 0s - loss: 4.9269e-07 - val_loss: 4.0901e-05
Epoch 120/512
512/512 - 0s - loss: 4.8475e-07 - val_loss: 4.0332e-05
Epoch 121/512
512/512 - 0s - loss: 4.7725e-07 - val_loss: 3.9778e-05
Epoch 122/512
512/512 - 0s - loss: 4.7012e-07 - val_loss: 3.9241e-05
Epoch 123/512
512/512 - 0s - loss: 4.6313e-07 - val_loss: 3.8722e-05
Epoch 124/512
512/512 - 0s - loss: 4.5575e-07 - val_loss: 3.8220e-05
Epoch 125/512
512/512 - 0s - loss: 4.4896e-07 - val_loss: 3.7731e-05
Epoch 126/512
512/512 - 0s - loss: 4.4234e-07 - val_loss: 3.7256e-05
Epoch 127/512
512/512 - 0s - loss: 4.3587e-07 - val_loss: 3.6792e-05
Epoch 128/512
512/512 - 0s - loss: 4.2962e-07 - val_loss: 3.6339e-05
Epoch 129/512
512/512 - 0s - loss: 4.2382e-07 - val_loss: 3.5897e-05
Epoch 130/512
512/512 - 0s - loss: 4.1848e-07 - val_loss: 3.5468e-05
Epoch 131/512
512/512 - 0s - loss: 4.1258e-07 - val_loss: 3.5055e-05
Epoch 132/512
512/512 - 0s - loss: 4.0687e-07 - val_loss: 3.4655e-05
Epoch 133/512
512/512 - 0s - loss: 4.0205e-07 - val_loss: 3.4266e-05
Epoch 134/512
512/512 - 0s - loss: 3.9722e-07 - val_loss: 3.3888e-05
Epoch 135/512
512/512 - 0s - loss: 3.9340e-07 - val_loss: 3.3515e-05
Epoch 136/512
512/512 - 0s - loss: 3.8774e-07 - val_loss: 3.3142e-05
Epoch 137/512
512/512 - 0s - loss: 3.8202e-07 - val_loss: 3.2782e-05
Epoch 138/512
512/512 - 0s - loss: 3.7723e-07 - val_loss: 3.2435e-05
Epoch 139/512
512/512 - 0s - loss: 3.7301e-07 - val_loss: 3.2096e-05
Epoch 140/512
512/512 - 0s - loss: 3.6916e-07 - val_loss: 3.1765e-05
Epoch 141/512
512/512 - 0s - loss: 3.6502e-07 - val_loss: 3.1440e-05
Epoch 142/512
512/512 - 0s - loss: 3.6052e-07 - val_loss: 3.1121e-05
Epoch 143/512
512/512 - 0s - loss: 3.5626e-07 - val_loss: 3.0810e-05
Epoch 144/512
512/512 - 0s - loss: 3.5230e-07 - val_loss: 3.0507e-05
Epoch 145/512
512/512 - 0s - loss: 3.4852e-07 - val_loss: 3.0211e-05
Epoch 146/512
512/512 - 0s - loss: 3.4489e-07 - val_loss: 2.9922e-05
Epoch 147/512
512/512 - 0s - loss: 3.4130e-07 - val_loss: 2.9638e-05
Epoch 148/512
512/512 - 0s - loss: 3.3766e-07 - val_loss: 2.9359e-05
Epoch 149/512
512/512 - 0s - loss: 3.3401e-07 - val_loss: 2.9087e-05
Epoch 150/512
512/512 - 0s - loss: 3.3053e-07 - val_loss: 2.8821e-05
Epoch 151/512
512/512 - 0s - loss: 3.2724e-07 - val_loss: 2.8561e-05
Epoch 152/512
512/512 - 0s - loss: 3.2408e-07 - val_loss: 2.8306e-05
Epoch 153/512
512/512 - 0s - loss: 3.2092e-07 - val_loss: 2.8056e-05
Epoch 154/512
512/512 - 0s - loss: 3.1771e-07 - val_loss: 2.7810e-05
Epoch 155/512
512/512 - 0s - loss: 3.1459e-07 - val_loss: 2.7570e-05
Epoch 156/512
512/512 - 0s - loss: 3.1163e-07 - val_loss: 2.7335e-05
Epoch 157/512
512/512 - 0s - loss: 3.0876e-07 - val_loss: 2.7104e-05
Epoch 158/512
512/512 - 0s - loss: 3.0588e-07 - val_loss: 2.6878e-05
Epoch 159/512
512/512 - 0s - loss: 3.0308e-07 - val_loss: 2.6656e-05
Epoch 160/512
512/512 - 0s - loss: 3.0031e-07 - val_loss: 2.6438e-05
Epoch 161/512
512/512 - 0s - loss: 2.9763e-07 - val_loss: 2.6224e-05
Epoch 162/512
512/512 - 0s - loss: 2.9498e-07 - val_loss: 2.6014e-05
Epoch 163/512
512/512 - 0s - loss: 2.9238e-07 - val_loss: 2.5808e-05
Epoch 164/512
512/512 - 0s - loss: 2.8985e-07 - val_loss: 2.5606e-05
Epoch 165/512
512/512 - 0s - loss: 2.8740e-07 - val_loss: 2.5407e-05
Epoch 166/512
512/512 - 0s - loss: 2.8496e-07 - val_loss: 2.5212e-05
Epoch 167/512
512/512 - 0s - loss: 2.8258e-07 - val_loss: 2.5020e-05
Epoch 168/512
512/512 - 0s - loss: 2.8022e-07 - val_loss: 2.4832e-05
Epoch 169/512
512/512 - 0s - loss: 2.7791e-07 - val_loss: 2.4647e-05
Epoch 170/512
512/512 - 0s - loss: 2.7565e-07 - val_loss: 2.4465e-05
Epoch 171/512
512/512 - 0s - loss: 2.7344e-07 - val_loss: 2.4286e-05
Epoch 172/512
512/512 - 0s - loss: 2.7127e-07 - val_loss: 2.4111e-05
Epoch 173/512
512/512 - 0s - loss: 2.6914e-07 - val_loss: 2.3938e-05
Epoch 174/512
512/512 - 0s - loss: 2.6706e-07 - val_loss: 2.3768e-05
Epoch 175/512
512/512 - 0s - loss: 2.6498e-07 - val_loss: 2.3601e-05
Epoch 176/512
512/512 - 0s - loss: 2.6295e-07 - val_loss: 2.3436e-05
Epoch 177/512
512/512 - 0s - loss: 2.6099e-07 - val_loss: 2.3275e-05
Epoch 178/512
512/512 - 0s - loss: 2.5905e-07 - val_loss: 2.3115e-05
Epoch 179/512
512/512 - 0s - loss: 2.5712e-07 - val_loss: 2.2959e-05
Epoch 180/512
512/512 - 0s - loss: 2.5521e-07 - val_loss: 2.2804e-05
Epoch 181/512
512/512 - 0s - loss: 2.5335e-07 - val_loss: 2.2653e-05
Epoch 182/512
512/512 - 0s - loss: 2.5156e-07 - val_loss: 2.2504e-05
Epoch 183/512
512/512 - 0s - loss: 2.4979e-07 - val_loss: 2.2356e-05
Epoch 184/512
512/512 - 0s - loss: 2.4799e-07 - val_loss: 2.2211e-05
Epoch 185/512
512/512 - 0s - loss: 2.4624e-07 - val_loss: 2.2068e-05
Epoch 186/512
512/512 - 0s - loss: 2.4451e-07 - val_loss: 2.1928e-05
Epoch 187/512
512/512 - 0s - loss: 2.4283e-07 - val_loss: 2.1789e-05
Epoch 188/512
512/512 - 0s - loss: 2.4121e-07 - val_loss: 2.1653e-05
Epoch 189/512
512/512 - 0s - loss: 2.3960e-07 - val_loss: 2.1519e-05
Epoch 190/512
512/512 - 0s - loss: 2.3798e-07 - val_loss: 2.1386e-05
Epoch 191/512
512/512 - 0s - loss: 2.3638e-07 - val_loss: 2.1255e-05
Epoch 192/512
512/512 - 0s - loss: 2.3483e-07 - val_loss: 2.1127e-05
Epoch 193/512
512/512 - 0s - loss: 2.3333e-07 - val_loss: 2.1000e-05
Epoch 194/512
512/512 - 0s - loss: 2.3183e-07 - val_loss: 2.0875e-05
Epoch 195/512
512/512 - 0s - loss: 2.3033e-07 - val_loss: 2.0751e-05
Epoch 196/512
512/512 - 0s - loss: 2.2886e-07 - val_loss: 2.0630e-05
Epoch 197/512
512/512 - 0s - loss: 2.2744e-07 - val_loss: 2.0510e-05
Epoch 198/512
512/512 - 0s - loss: 2.2603e-07 - val_loss: 2.0391e-05
Epoch 199/512
512/512 - 0s - loss: 2.2461e-07 - val_loss: 2.0274e-05
Epoch 200/512
512/512 - 0s - loss: 2.2323e-07 - val_loss: 2.0159e-05
Epoch 201/512
512/512 - 0s - loss: 2.2185e-07 - val_loss: 2.0045e-05
Epoch 202/512
512/512 - 0s - loss: 2.2051e-07 - val_loss: 1.9933e-05
Epoch 203/512
512/512 - 0s - loss: 2.1919e-07 - val_loss: 1.9823e-05
Epoch 204/512
512/512 - 0s - loss: 2.1789e-07 - val_loss: 1.9713e-05
Epoch 205/512
512/512 - 0s - loss: 2.1661e-07 - val_loss: 1.9606e-05
Epoch 206/512
512/512 - 0s - loss: 2.1539e-07 - val_loss: 1.9499e-05
Epoch 207/512
512/512 - 0s - loss: 2.1414e-07 - val_loss: 1.9394e-05
Epoch 208/512
512/512 - 0s - loss: 2.1287e-07 - val_loss: 1.9290e-05
Epoch 209/512
512/512 - 0s - loss: 2.1164e-07 - val_loss: 1.9188e-05
Epoch 210/512
512/512 - 0s - loss: 2.1043e-07 - val_loss: 1.9086e-05
Epoch 211/512
512/512 - 0s - loss: 2.0924e-07 - val_loss: 1.8987e-05
Epoch 212/512
512/512 - 0s - loss: 2.0810e-07 - val_loss: 1.8888e-05
Epoch 213/512
512/512 - 0s - loss: 2.0697e-07 - val_loss: 1.8791e-05
Epoch 214/512
512/512 - 0s - loss: 2.0582e-07 - val_loss: 1.8694e-05
Epoch 215/512
512/512 - 0s - loss: 2.0469e-07 - val_loss: 1.8599e-05
Epoch 216/512
512/512 - 0s - loss: 2.0357e-07 - val_loss: 1.8505e-05
Epoch 217/512
512/512 - 0s - loss: 2.0245e-07 - val_loss: 1.8412e-05
Epoch 218/512
512/512 - 0s - loss: 2.0137e-07 - val_loss: 1.8320e-05
Epoch 219/512
512/512 - 0s - loss: 2.0033e-07 - val_loss: 1.8230e-05
Epoch 220/512
512/512 - 0s - loss: 1.9929e-07 - val_loss: 1.8140e-05
Epoch 221/512
512/512 - 0s - loss: 1.9826e-07 - val_loss: 1.8052e-05
Epoch 222/512
512/512 - 0s - loss: 1.9720e-07 - val_loss: 1.7964e-05
Epoch 223/512
512/512 - 0s - loss: 1.9615e-07 - val_loss: 1.7877e-05
Epoch 224/512
512/512 - 0s - loss: 1.9515e-07 - val_loss: 1.7792e-05
Epoch 225/512
512/512 - 0s - loss: 1.9418e-07 - val_loss: 1.7708e-05
Epoch 226/512
512/512 - 0s - loss: 1.9320e-07 - val_loss: 1.7624e-05
Epoch 227/512
512/512 - 0s - loss: 1.9225e-07 - val_loss: 1.7541e-05
Epoch 228/512
512/512 - 0s - loss: 1.9129e-07 - val_loss: 1.7459e-05
Epoch 229/512
512/512 - 0s - loss: 1.9035e-07 - val_loss: 1.7379e-05
Epoch 230/512
512/512 - 0s - loss: 1.8940e-07 - val_loss: 1.7298e-05
Epoch 231/512
512/512 - 0s - loss: 1.8847e-07 - val_loss: 1.7219e-05
Epoch 232/512
512/512 - 0s - loss: 1.8755e-07 - val_loss: 1.7141e-05
Epoch 233/512
512/512 - 0s - loss: 1.8665e-07 - val_loss: 1.7064e-05
Epoch 234/512
512/512 - 0s - loss: 1.8576e-07 - val_loss: 1.6987e-05
Epoch 235/512
512/512 - 0s - loss: 1.8490e-07 - val_loss: 1.6912e-05
Epoch 236/512
512/512 - 0s - loss: 1.8403e-07 - val_loss: 1.6837e-05
Epoch 237/512
512/512 - 0s - loss: 1.8316e-07 - val_loss: 1.6762e-05
Epoch 238/512
512/512 - 0s - loss: 1.8229e-07 - val_loss: 1.6689e-05
Epoch 239/512
512/512 - 0s - loss: 1.8145e-07 - val_loss: 1.6616e-05
Epoch 240/512
512/512 - 0s - loss: 1.8060e-07 - val_loss: 1.6544e-05
Epoch 241/512
512/512 - 0s - loss: 1.7979e-07 - val_loss: 1.6473e-05
Epoch 242/512
512/512 - 0s - loss: 1.7897e-07 - val_loss: 1.6403e-05
Epoch 243/512
512/512 - 0s - loss: 1.7815e-07 - val_loss: 1.6333e-05
Epoch 244/512
512/512 - 0s - loss: 1.7736e-07 - val_loss: 1.6264e-05
Epoch 245/512
512/512 - 0s - loss: 1.7658e-07 - val_loss: 1.6196e-05
Epoch 246/512
512/512 - 0s - loss: 1.7579e-07 - val_loss: 1.6128e-05
Epoch 247/512
512/512 - 0s - loss: 1.7498e-07 - val_loss: 1.6061e-05
Epoch 248/512
512/512 - 0s - loss: 1.7421e-07 - val_loss: 1.5995e-05
Epoch 249/512
512/512 - 0s - loss: 1.7348e-07 - val_loss: 1.5929e-05
Epoch 250/512
512/512 - 0s - loss: 1.7275e-07 - val_loss: 1.5864e-05
Epoch 251/512
512/512 - 0s - loss: 1.7201e-07 - val_loss: 1.5800e-05
Epoch 252/512
512/512 - 0s - loss: 1.7126e-07 - val_loss: 1.5736e-05
Epoch 253/512
512/512 - 0s - loss: 1.7054e-07 - val_loss: 1.5673e-05
Epoch 254/512
512/512 - 0s - loss: 1.6983e-07 - val_loss: 1.5611e-05
Epoch 255/512
512/512 - 0s - loss: 1.6910e-07 - val_loss: 1.5548e-05
Epoch 256/512
512/512 - 0s - loss: 1.6838e-07 - val_loss: 1.5487e-05
Epoch 257/512
512/512 - 0s - loss: 1.6767e-07 - val_loss: 1.5426e-05
Epoch 258/512
512/512 - 0s - loss: 1.6698e-07 - val_loss: 1.5366e-05
Epoch 259/512
512/512 - 0s - loss: 1.6631e-07 - val_loss: 1.5307e-05
Epoch 260/512
512/512 - 0s - loss: 1.6564e-07 - val_loss: 1.5248e-05
Epoch 261/512
512/512 - 0s - loss: 1.6497e-07 - val_loss: 1.5189e-05
Epoch 262/512
512/512 - 0s - loss: 1.6429e-07 - val_loss: 1.5131e-05
Epoch 263/512
512/512 - 0s - loss: 1.6362e-07 - val_loss: 1.5073e-05
Epoch 264/512
512/512 - 0s - loss: 1.6298e-07 - val_loss: 1.5017e-05
Epoch 265/512
512/512 - 0s - loss: 1.6233e-07 - val_loss: 1.4960e-05
Epoch 266/512
512/512 - 0s - loss: 1.6170e-07 - val_loss: 1.4904e-05
Epoch 267/512
512/512 - 0s - loss: 1.6107e-07 - val_loss: 1.4849e-05
Epoch 268/512
512/512 - 0s - loss: 1.6043e-07 - val_loss: 1.4794e-05
Epoch 269/512
512/512 - 0s - loss: 1.5982e-07 - val_loss: 1.4739e-05
Epoch 270/512
512/512 - 0s - loss: 1.5919e-07 - val_loss: 1.4685e-05
Epoch 271/512
512/512 - 0s - loss: 1.5856e-07 - val_loss: 1.4632e-05
Epoch 272/512
512/512 - 0s - loss: 1.5795e-07 - val_loss: 1.4579e-05
Epoch 273/512
512/512 - 0s - loss: 1.5734e-07 - val_loss: 1.4526e-05
Epoch 274/512
512/512 - 0s - loss: 1.5676e-07 - val_loss: 1.4474e-05
Epoch 275/512
512/512 - 0s - loss: 1.5619e-07 - val_loss: 1.4423e-05
Epoch 276/512
512/512 - 0s - loss: 1.5561e-07 - val_loss: 1.4371e-05
Epoch 277/512
512/512 - 0s - loss: 1.5502e-07 - val_loss: 1.4320e-05
Epoch 278/512
512/512 - 0s - loss: 1.5445e-07 - val_loss: 1.4270e-05
Epoch 279/512
512/512 - 0s - loss: 1.5388e-07 - val_loss: 1.4220e-05
Epoch 280/512
512/512 - 0s - loss: 1.5331e-07 - val_loss: 1.4171e-05
Epoch 281/512
512/512 - 0s - loss: 1.5273e-07 - val_loss: 1.4121e-05
Epoch 282/512
512/512 - 0s - loss: 1.5219e-07 - val_loss: 1.4073e-05
Epoch 283/512
512/512 - 0s - loss: 1.5165e-07 - val_loss: 1.4025e-05
Epoch 284/512
512/512 - 0s - loss: 1.5111e-07 - val_loss: 1.3977e-05
Epoch 285/512
512/512 - 0s - loss: 1.5057e-07 - val_loss: 1.3929e-05
Epoch 286/512
512/512 - 0s - loss: 1.5003e-07 - val_loss: 1.3882e-05
Epoch 287/512
512/512 - 0s - loss: 1.4948e-07 - val_loss: 1.3835e-05
Epoch 288/512
512/512 - 0s - loss: 1.4895e-07 - val_loss: 1.3789e-05
Epoch 289/512
512/512 - 0s - loss: 1.4844e-07 - val_loss: 1.3743e-05
Epoch 290/512
512/512 - 0s - loss: 1.4794e-07 - val_loss: 1.3697e-05
Epoch 291/512
512/512 - 0s - loss: 1.4744e-07 - val_loss: 1.3652e-05
Epoch 292/512
512/512 - 0s - loss: 1.4694e-07 - val_loss: 1.3607e-05
Epoch 293/512
512/512 - 0s - loss: 1.4642e-07 - val_loss: 1.3563e-05
Epoch 294/512
512/512 - 0s - loss: 1.4590e-07 - val_loss: 1.3518e-05
Epoch 295/512
512/512 - 0s - loss: 1.4539e-07 - val_loss: 1.3474e-05
Epoch 296/512
512/512 - 0s - loss: 1.4488e-07 - val_loss: 1.3431e-05
Epoch 297/512
512/512 - 0s - loss: 1.4440e-07 - val_loss: 1.3388e-05
Epoch 298/512
512/512 - 0s - loss: 1.4393e-07 - val_loss: 1.3345e-05
Epoch 299/512
512/512 - 0s - loss: 1.4347e-07 - val_loss: 1.3303e-05
Epoch 300/512
512/512 - 0s - loss: 1.4299e-07 - val_loss: 1.3260e-05
Epoch 301/512
512/512 - 0s - loss: 1.4250e-07 - val_loss: 1.3218e-05
Epoch 302/512
512/512 - 0s - loss: 1.4203e-07 - val_loss: 1.3177e-05
Epoch 303/512
512/512 - 0s - loss: 1.4156e-07 - val_loss: 1.3136e-05
Epoch 304/512
512/512 - 0s - loss: 1.4111e-07 - val_loss: 1.3095e-05
Epoch 305/512
512/512 - 0s - loss: 1.4066e-07 - val_loss: 1.3054e-05
Epoch 306/512
512/512 - 0s - loss: 1.4019e-07 - val_loss: 1.3014e-05
Epoch 307/512
512/512 - 0s - loss: 1.3973e-07 - val_loss: 1.2974e-05
Epoch 308/512
512/512 - 0s - loss: 1.3928e-07 - val_loss: 1.2934e-05
Epoch 309/512
512/512 - 0s - loss: 1.3882e-07 - val_loss: 1.2895e-05
Epoch 310/512
512/512 - 0s - loss: 1.3840e-07 - val_loss: 1.2856e-05
Epoch 311/512
512/512 - 0s - loss: 1.3797e-07 - val_loss: 1.2817e-05
Epoch 312/512
512/512 - 0s - loss: 1.3755e-07 - val_loss: 1.2778e-05
Epoch 313/512
512/512 - 0s - loss: 1.3711e-07 - val_loss: 1.2740e-05
Epoch 314/512
512/512 - 0s - loss: 1.3670e-07 - val_loss: 1.2702e-05
Epoch 315/512
512/512 - 0s - loss: 1.3626e-07 - val_loss: 1.2664e-05
Epoch 316/512
512/512 - 0s - loss: 1.3583e-07 - val_loss: 1.2627e-05
Epoch 317/512
512/512 - 0s - loss: 1.3540e-07 - val_loss: 1.2589e-05
Epoch 318/512
512/512 - 0s - loss: 1.3499e-07 - val_loss: 1.2552e-05
Epoch 319/512
512/512 - 0s - loss: 1.3456e-07 - val_loss: 1.2516e-05
Epoch 320/512
512/512 - 0s - loss: 1.3417e-07 - val_loss: 1.2479e-05
Epoch 321/512
512/512 - 0s - loss: 1.3377e-07 - val_loss: 1.2443e-05
Epoch 322/512
512/512 - 0s - loss: 1.3337e-07 - val_loss: 1.2407e-05
Epoch 323/512
512/512 - 0s - loss: 1.3297e-07 - val_loss: 1.2372e-05
Epoch 324/512
512/512 - 0s - loss: 1.3258e-07 - val_loss: 1.2336e-05
Epoch 325/512
512/512 - 0s - loss: 1.3218e-07 - val_loss: 1.2301e-05
Epoch 326/512
512/512 - 0s - loss: 1.3178e-07 - val_loss: 1.2266e-05
Epoch 327/512
512/512 - 0s - loss: 1.3138e-07 - val_loss: 1.2232e-05
Epoch 328/512
512/512 - 0s - loss: 1.3101e-07 - val_loss: 1.2197e-05
Epoch 329/512
512/512 - 0s - loss: 1.3063e-07 - val_loss: 1.2163e-05
Epoch 330/512
512/512 - 0s - loss: 1.3025e-07 - val_loss: 1.2129e-05
Epoch 331/512
512/512 - 0s - loss: 1.2986e-07 - val_loss: 1.2095e-05
Epoch 332/512
512/512 - 0s - loss: 1.2948e-07 - val_loss: 1.2062e-05
Epoch 333/512
512/512 - 0s - loss: 1.2911e-07 - val_loss: 1.2028e-05
Epoch 334/512
512/512 - 0s - loss: 1.2873e-07 - val_loss: 1.1995e-05
Epoch 335/512
512/512 - 0s - loss: 1.2837e-07 - val_loss: 1.1962e-05
Epoch 336/512
512/512 - 0s - loss: 1.2800e-07 - val_loss: 1.1930e-05
Epoch 337/512
512/512 - 0s - loss: 1.2765e-07 - val_loss: 1.1897e-05
Epoch 338/512
512/512 - 0s - loss: 1.2729e-07 - val_loss: 1.1865e-05
Epoch 339/512
512/512 - 0s - loss: 1.2693e-07 - val_loss: 1.1833e-05
Epoch 340/512
512/512 - 0s - loss: 1.2657e-07 - val_loss: 1.1801e-05
Epoch 341/512
512/512 - 0s - loss: 1.2622e-07 - val_loss: 1.1770e-05
Epoch 342/512
512/512 - 0s - loss: 1.2587e-07 - val_loss: 1.1738e-05
Epoch 343/512
512/512 - 0s - loss: 1.2553e-07 - val_loss: 1.1707e-05
Epoch 344/512
512/512 - 0s - loss: 1.2518e-07 - val_loss: 1.1676e-05
Epoch 345/512
512/512 - 0s - loss: 1.2484e-07 - val_loss: 1.1645e-05
Epoch 346/512
512/512 - 0s - loss: 1.2449e-07 - val_loss: 1.1615e-05
Epoch 347/512
512/512 - 0s - loss: 1.2414e-07 - val_loss: 1.1584e-05
Epoch 348/512
512/512 - 0s - loss: 1.2381e-07 - val_loss: 1.1554e-05
Epoch 349/512
512/512 - 0s - loss: 1.2349e-07 - val_loss: 1.1524e-05
Epoch 350/512
512/512 - 0s - loss: 1.2315e-07 - val_loss: 1.1494e-05
Epoch 351/512
512/512 - 0s - loss: 1.2282e-07 - val_loss: 1.1465e-05
Epoch 352/512
512/512 - 0s - loss: 1.2249e-07 - val_loss: 1.1435e-05
Epoch 353/512
512/512 - 0s - loss: 1.2216e-07 - val_loss: 1.1406e-05
Epoch 354/512
512/512 - 0s - loss: 1.2184e-07 - val_loss: 1.1377e-05
Epoch 355/512
512/512 - 0s - loss: 1.2151e-07 - val_loss: 1.1348e-05
Epoch 356/512
512/512 - 0s - loss: 1.2119e-07 - val_loss: 1.1319e-05
Epoch 357/512
512/512 - 0s - loss: 1.2087e-07 - val_loss: 1.1291e-05
Epoch 358/512
512/512 - 0s - loss: 1.2056e-07 - val_loss: 1.1262e-05
Epoch 359/512
512/512 - 0s - loss: 1.2025e-07 - val_loss: 1.1234e-05
Epoch 360/512
512/512 - 0s - loss: 1.1994e-07 - val_loss: 1.1206e-05
Epoch 361/512
512/512 - 0s - loss: 1.1963e-07 - val_loss: 1.1178e-05
Epoch 362/512
512/512 - 0s - loss: 1.1932e-07 - val_loss: 1.1150e-05
Epoch 363/512
512/512 - 0s - loss: 1.1901e-07 - val_loss: 1.1123e-05
Epoch 364/512
512/512 - 0s - loss: 1.1872e-07 - val_loss: 1.1096e-05
Epoch 365/512
512/512 - 0s - loss: 1.1841e-07 - val_loss: 1.1068e-05
Epoch 366/512
512/512 - 0s - loss: 1.1809e-07 - val_loss: 1.1041e-05
Epoch 367/512
512/512 - 0s - loss: 1.1780e-07 - val_loss: 1.1014e-05
Epoch 368/512
512/512 - 0s - loss: 1.1751e-07 - val_loss: 1.0988e-05
Epoch 369/512
512/512 - 0s - loss: 1.1722e-07 - val_loss: 1.0961e-05
Epoch 370/512
512/512 - 0s - loss: 1.1694e-07 - val_loss: 1.0935e-05
Epoch 371/512
512/512 - 0s - loss: 1.1664e-07 - val_loss: 1.0908e-05
Epoch 372/512
512/512 - 0s - loss: 1.1635e-07 - val_loss: 1.0882e-05
Epoch 373/512
512/512 - 0s - loss: 1.1604e-07 - val_loss: 1.0856e-05
Epoch 374/512
512/512 - 0s - loss: 1.1575e-07 - val_loss: 1.0830e-05
Epoch 375/512
512/512 - 0s - loss: 1.1547e-07 - val_loss: 1.0805e-05
Epoch 376/512
512/512 - 0s - loss: 1.1520e-07 - val_loss: 1.0779e-05
Epoch 377/512
512/512 - 0s - loss: 1.1492e-07 - val_loss: 1.0754e-05
Epoch 378/512
512/512 - 0s - loss: 1.1464e-07 - val_loss: 1.0729e-05
Epoch 379/512
512/512 - 0s - loss: 1.1436e-07 - val_loss: 1.0704e-05
Epoch 380/512
512/512 - 0s - loss: 1.1408e-07 - val_loss: 1.0679e-05
Epoch 381/512
512/512 - 0s - loss: 1.1381e-07 - val_loss: 1.0654e-05
Epoch 382/512
512/512 - 0s - loss: 1.1353e-07 - val_loss: 1.0629e-05
Epoch 383/512
512/512 - 0s - loss: 1.1326e-07 - val_loss: 1.0605e-05
Epoch 384/512
512/512 - 0s - loss: 1.1300e-07 - val_loss: 1.0581e-05
Epoch 385/512
512/512 - 0s - loss: 1.1273e-07 - val_loss: 1.0556e-05
Epoch 386/512
512/512 - 0s - loss: 1.1245e-07 - val_loss: 1.0532e-05
Epoch 387/512
512/512 - 0s - loss: 1.1219e-07 - val_loss: 1.0508e-05
Epoch 388/512
512/512 - 0s - loss: 1.1193e-07 - val_loss: 1.0484e-05
Epoch 389/512
512/512 - 0s - loss: 1.1167e-07 - val_loss: 1.0461e-05
Epoch 390/512
512/512 - 0s - loss: 1.1141e-07 - val_loss: 1.0437e-05
Epoch 391/512
512/512 - 0s - loss: 1.1115e-07 - val_loss: 1.0414e-05
Epoch 392/512
512/512 - 0s - loss: 1.1090e-07 - val_loss: 1.0390e-05
Epoch 393/512
512/512 - 0s - loss: 1.1064e-07 - val_loss: 1.0367e-05
Epoch 394/512
512/512 - 0s - loss: 1.1039e-07 - val_loss: 1.0344e-05
Epoch 395/512
512/512 - 0s - loss: 1.1013e-07 - val_loss: 1.0321e-05
Epoch 396/512
512/512 - 0s - loss: 1.0987e-07 - val_loss: 1.0298e-05
Epoch 397/512
512/512 - 0s - loss: 1.0962e-07 - val_loss: 1.0275e-05
Epoch 398/512
512/512 - 0s - loss: 1.0937e-07 - val_loss: 1.0253e-05
Epoch 399/512
512/512 - 0s - loss: 1.0913e-07 - val_loss: 1.0230e-05
Epoch 400/512
512/512 - 0s - loss: 1.0889e-07 - val_loss: 1.0208e-05
Epoch 401/512
512/512 - 0s - loss: 1.0863e-07 - val_loss: 1.0186e-05
Epoch 402/512
512/512 - 0s - loss: 1.0839e-07 - val_loss: 1.0164e-05
Epoch 403/512
512/512 - 0s - loss: 1.0813e-07 - val_loss: 1.0142e-05
Epoch 404/512
512/512 - 0s - loss: 1.0789e-07 - val_loss: 1.0120e-05
Epoch 405/512
512/512 - 0s - loss: 1.0766e-07 - val_loss: 1.0098e-05
Epoch 406/512
512/512 - 0s - loss: 1.0743e-07 - val_loss: 1.0077e-05
Epoch 407/512
512/512 - 0s - loss: 1.0721e-07 - val_loss: 1.0055e-05
Epoch 408/512
512/512 - 0s - loss: 1.0697e-07 - val_loss: 1.0034e-05
Epoch 409/512
512/512 - 0s - loss: 1.0672e-07 - val_loss: 1.0012e-05
Epoch 410/512
512/512 - 0s - loss: 1.0648e-07 - val_loss: 9.9909e-06
Epoch 411/512
512/512 - 0s - loss: 1.0624e-07 - val_loss: 9.9698e-06
Epoch 412/512
512/512 - 0s - loss: 1.0602e-07 - val_loss: 9.9489e-06
Epoch 413/512
512/512 - 0s - loss: 1.0579e-07 - val_loss: 9.9280e-06
Epoch 414/512
512/512 - 0s - loss: 1.0556e-07 - val_loss: 9.9072e-06
Epoch 415/512
512/512 - 0s - loss: 1.0533e-07 - val_loss: 9.8866e-06
Epoch 416/512
512/512 - 0s - loss: 1.0511e-07 - val_loss: 9.8660e-06
Epoch 417/512
512/512 - 0s - loss: 1.0488e-07 - val_loss: 9.8456e-06
Epoch 418/512
512/512 - 0s - loss: 1.0466e-07 - val_loss: 9.8253e-06
Epoch 419/512
512/512 - 0s - loss: 1.0445e-07 - val_loss: 9.8051e-06
Epoch 420/512
512/512 - 0s - loss: 1.0423e-07 - val_loss: 9.7849e-06
Epoch 421/512
512/512 - 0s - loss: 1.0401e-07 - val_loss: 9.7649e-06
Epoch 422/512
512/512 - 0s - loss: 1.0378e-07 - val_loss: 9.7448e-06
Epoch 423/512
512/512 - 0s - loss: 1.0356e-07 - val_loss: 9.7250e-06
Epoch 424/512
512/512 - 0s - loss: 1.0333e-07 - val_loss: 9.7052e-06
Epoch 425/512
512/512 - 0s - loss: 1.0312e-07 - val_loss: 9.6855e-06
Epoch 426/512
512/512 - 0s - loss: 1.0290e-07 - val_loss: 9.6660e-06
Epoch 427/512
512/512 - 0s - loss: 1.0270e-07 - val_loss: 9.6466e-06
Epoch 428/512
512/512 - 0s - loss: 1.0249e-07 - val_loss: 9.6272e-06
Epoch 429/512
512/512 - 0s - loss: 1.0228e-07 - val_loss: 9.6080e-06
Epoch 430/512
512/512 - 0s - loss: 1.0206e-07 - val_loss: 9.5888e-06
Epoch 431/512
512/512 - 0s - loss: 1.0185e-07 - val_loss: 9.5696e-06
Epoch 432/512
512/512 - 0s - loss: 1.0163e-07 - val_loss: 9.5507e-06
Epoch 433/512
512/512 - 0s - loss: 1.0144e-07 - val_loss: 9.5318e-06
Epoch 434/512
512/512 - 0s - loss: 1.0123e-07 - val_loss: 9.5130e-06
Epoch 435/512
512/512 - 0s - loss: 1.0103e-07 - val_loss: 9.4943e-06
Epoch 436/512
512/512 - 0s - loss: 1.0082e-07 - val_loss: 9.4757e-06
Epoch 437/512
512/512 - 0s - loss: 1.0062e-07 - val_loss: 9.4571e-06
Epoch 438/512
512/512 - 0s - loss: 1.0040e-07 - val_loss: 9.4385e-06
Epoch 439/512
512/512 - 0s - loss: 1.0020e-07 - val_loss: 9.4202e-06
Epoch 440/512
512/512 - 0s - loss: 1.0000e-07 - val_loss: 9.4019e-06
Epoch 441/512
512/512 - 0s - loss: 9.9805e-08 - val_loss: 9.3837e-06
Epoch 442/512
512/512 - 0s - loss: 9.9608e-08 - val_loss: 9.3657e-06
Epoch 443/512
512/512 - 0s - loss: 9.9422e-08 - val_loss: 9.3477e-06
Epoch 444/512
512/512 - 0s - loss: 9.9229e-08 - val_loss: 9.3298e-06
Epoch 445/512
512/512 - 0s - loss: 9.9034e-08 - val_loss: 9.3119e-06
Epoch 446/512
512/512 - 0s - loss: 9.8842e-08 - val_loss: 9.2942e-06
Epoch 447/512
512/512 - 0s - loss: 9.8639e-08 - val_loss: 9.2764e-06
Epoch 448/512
512/512 - 0s - loss: 9.8436e-08 - val_loss: 9.2588e-06
Epoch 449/512
512/512 - 0s - loss: 9.8246e-08 - val_loss: 9.2413e-06
Epoch 450/512
512/512 - 0s - loss: 9.8054e-08 - val_loss: 9.2238e-06
Epoch 451/512
512/512 - 0s - loss: 9.7860e-08 - val_loss: 9.2064e-06
Epoch 452/512
512/512 - 0s - loss: 9.7671e-08 - val_loss: 9.1892e-06
Epoch 453/512
512/512 - 0s - loss: 9.7501e-08 - val_loss: 9.1721e-06
Epoch 454/512
512/512 - 0s - loss: 9.7324e-08 - val_loss: 9.1550e-06
Epoch 455/512
512/512 - 0s - loss: 9.7133e-08 - val_loss: 9.1379e-06
Epoch 456/512
512/512 - 0s - loss: 9.6939e-08 - val_loss: 9.1209e-06
Epoch 457/512
512/512 - 0s - loss: 9.6744e-08 - val_loss: 9.1039e-06
Epoch 458/512
512/512 - 0s - loss: 9.6555e-08 - val_loss: 9.0871e-06
Epoch 459/512
512/512 - 0s - loss: 9.6375e-08 - val_loss: 9.0704e-06
Epoch 460/512
512/512 - 0s - loss: 9.6206e-08 - val_loss: 9.0538e-06
Epoch 461/512
512/512 - 0s - loss: 9.6018e-08 - val_loss: 9.0371e-06
Epoch 462/512
512/512 - 0s - loss: 9.5826e-08 - val_loss: 9.0205e-06
Epoch 463/512
512/512 - 0s - loss: 9.5642e-08 - val_loss: 9.0041e-06
Epoch 464/512
512/512 - 0s - loss: 9.5467e-08 - val_loss: 8.9878e-06
Epoch 465/512
512/512 - 0s - loss: 9.5304e-08 - val_loss: 8.9716e-06
Epoch 466/512
512/512 - 0s - loss: 9.5136e-08 - val_loss: 8.9553e-06
Epoch 467/512
512/512 - 0s - loss: 9.4952e-08 - val_loss: 8.9391e-06
Epoch 468/512
512/512 - 0s - loss: 9.4769e-08 - val_loss: 8.9230e-06
Epoch 469/512
512/512 - 0s - loss: 9.4589e-08 - val_loss: 8.9070e-06
Epoch 470/512
512/512 - 0s - loss: 9.4416e-08 - val_loss: 8.8911e-06
Epoch 471/512
512/512 - 0s - loss: 9.4252e-08 - val_loss: 8.8752e-06
Epoch 472/512
512/512 - 0s - loss: 9.4081e-08 - val_loss: 8.8594e-06
Epoch 473/512
512/512 - 0s - loss: 9.3898e-08 - val_loss: 8.8437e-06
Epoch 474/512
512/512 - 0s - loss: 9.3731e-08 - val_loss: 8.8280e-06
Epoch 475/512
512/512 - 0s - loss: 9.3559e-08 - val_loss: 8.8124e-06
Epoch 476/512
512/512 - 0s - loss: 9.3398e-08 - val_loss: 8.7969e-06
Epoch 477/512
512/512 - 0s - loss: 9.3231e-08 - val_loss: 8.7814e-06
Epoch 478/512
512/512 - 0s - loss: 9.3057e-08 - val_loss: 8.7660e-06
Epoch 479/512
512/512 - 0s - loss: 9.2890e-08 - val_loss: 8.7507e-06
Epoch 480/512
512/512 - 0s - loss: 9.2723e-08 - val_loss: 8.7354e-06
Epoch 481/512
512/512 - 0s - loss: 9.2555e-08 - val_loss: 8.7201e-06
Epoch 482/512
512/512 - 0s - loss: 9.2387e-08 - val_loss: 8.7050e-06
Epoch 483/512
512/512 - 0s - loss: 9.2225e-08 - val_loss: 8.6900e-06
Epoch 484/512
512/512 - 0s - loss: 9.2070e-08 - val_loss: 8.6750e-06
Epoch 485/512
512/512 - 0s - loss: 9.1900e-08 - val_loss: 8.6600e-06
Epoch 486/512
512/512 - 0s - loss: 9.1735e-08 - val_loss: 8.6451e-06
Epoch 487/512
512/512 - 0s - loss: 9.1573e-08 - val_loss: 8.6302e-06
Epoch 488/512
512/512 - 0s - loss: 9.1412e-08 - val_loss: 8.6155e-06
Epoch 489/512
512/512 - 0s - loss: 9.1252e-08 - val_loss: 8.6008e-06
Epoch 490/512
512/512 - 0s - loss: 9.1092e-08 - val_loss: 8.5861e-06
Epoch 491/512
512/512 - 0s - loss: 9.0937e-08 - val_loss: 8.5716e-06
Epoch 492/512
512/512 - 0s - loss: 9.0778e-08 - val_loss: 8.5571e-06
Epoch 493/512
512/512 - 0s - loss: 9.0626e-08 - val_loss: 8.5427e-06
Epoch 494/512
512/512 - 0s - loss: 9.0473e-08 - val_loss: 8.5283e-06
Epoch 495/512
512/512 - 0s - loss: 9.0309e-08 - val_loss: 8.5138e-06
Epoch 496/512
512/512 - 0s - loss: 9.0145e-08 - val_loss: 8.4995e-06
Epoch 497/512
512/512 - 0s - loss: 8.9991e-08 - val_loss: 8.4853e-06
Epoch 498/512
512/512 - 0s - loss: 8.9835e-08 - val_loss: 8.4712e-06
Epoch 499/512
512/512 - 0s - loss: 8.9683e-08 - val_loss: 8.4570e-06
Epoch 500/512
512/512 - 0s - loss: 8.9530e-08 - val_loss: 8.4430e-06
Epoch 501/512
512/512 - 0s - loss: 8.9384e-08 - val_loss: 8.4291e-06
Epoch 502/512
512/512 - 0s - loss: 8.9237e-08 - val_loss: 8.4151e-06
Epoch 503/512
512/512 - 0s - loss: 8.9083e-08 - val_loss: 8.4012e-06
Epoch 504/512
512/512 - 0s - loss: 8.8929e-08 - val_loss: 8.3874e-06
Epoch 505/512
512/512 - 0s - loss: 8.8777e-08 - val_loss: 8.3736e-06
Epoch 506/512
512/512 - 0s - loss: 8.8631e-08 - val_loss: 8.3599e-06
Epoch 507/512
512/512 - 0s - loss: 8.8479e-08 - val_loss: 8.3462e-06
Epoch 508/512
512/512 - 0s - loss: 8.8329e-08 - val_loss: 8.3326e-06
Epoch 509/512
512/512 - 0s - loss: 8.8178e-08 - val_loss: 8.3191e-06
Epoch 510/512
512/512 - 0s - loss: 8.8031e-08 - val_loss: 8.3056e-06
Epoch 511/512
512/512 - 0s - loss: 8.7887e-08 - val_loss: 8.2921e-06
Epoch 512/512
512/512 - 0s - loss: 8.7748e-08 - val_loss: 8.2788e-06
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3684e-08 - val_loss: 1.8040e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1826e-09 - val_loss: 3.2825e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5691e-10 - val_loss: 4.1513e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3943e-11 - val_loss: 3.0677e-11
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8987e-11 - val_loss: 7.1396e-11
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7234e-10 - val_loss: 6.2780e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2309e-09 - val_loss: 6.8531e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8674e-09 - val_loss: 3.3025e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9251e-09 - val_loss: 6.6389e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1266e-10 - val_loss: 4.0796e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8250e-10 - val_loss: 7.4169e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1793e-09 - val_loss: 2.2244e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8766e-09 - val_loss: 3.1269e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5578e-09 - val_loss: 1.5644e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2384e-09 - val_loss: 8.7988e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4608e-10 - val_loss: 8.8163e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0233e-09 - val_loss: 1.3207e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5321e-09 - val_loss: 1.7837e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7650e-09 - val_loss: 1.5528e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3842e-09 - val_loss: 1.1164e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0338e-09 - val_loss: 9.3935e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4698e-10 - val_loss: 9.9113e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0542e-09 - val_loss: 1.1523e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1955e-09 - val_loss: 1.2033e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1732e-09 - val_loss: 1.0770e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0214e-09 - val_loss: 9.2845e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0331e-10 - val_loss: 8.6589e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6972e-10 - val_loss: 8.7968e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9760e-10 - val_loss: 9.1044e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1519e-10 - val_loss: 9.0035e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8401e-10 - val_loss: 8.4202e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2182e-10 - val_loss: 7.8449e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7251e-10 - val_loss: 7.5272e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5155e-10 - val_loss: 7.3870e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3830e-10 - val_loss: 7.2927e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2751e-10 - val_loss: 7.2001e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1483e-10 - val_loss: 6.9636e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8867e-10 - val_loss: 6.6864e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6144e-10 - val_loss: 6.4024e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3361e-10 - val_loss: 6.2184e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1946e-10 - val_loss: 6.1014e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0695e-10 - val_loss: 5.9675e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9004e-10 - val_loss: 5.7650e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7119e-10 - val_loss: 5.5914e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5387e-10 - val_loss: 5.4444e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4212e-10 - val_loss: 5.3528e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3104e-10 - val_loss: 5.2036e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1376e-10 - val_loss: 4.9931e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9526e-10 - val_loss: 4.8787e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8399e-10 - val_loss: 4.7769e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7497e-10 - val_loss: 4.6927e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6670e-10 - val_loss: 4.5905e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5666e-10 - val_loss: 4.4625e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4119e-10 - val_loss: 4.2974e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2635e-10 - val_loss: 4.1996e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1778e-10 - val_loss: 4.1085e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0845e-10 - val_loss: 4.0386e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0146e-10 - val_loss: 3.9529e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9306e-10 - val_loss: 3.8758e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8564e-10 - val_loss: 3.7804e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7402e-10 - val_loss: 3.6657e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6414e-10 - val_loss: 3.5661e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5592e-10 - val_loss: 3.5060e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4922e-10 - val_loss: 3.4373e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4079e-10 - val_loss: 3.3558e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3346e-10 - val_loss: 3.2630e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2186e-10 - val_loss: 3.1412e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1189e-10 - val_loss: 3.0655e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0603e-10 - val_loss: 3.0326e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0352e-10 - val_loss: 3.0296e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0347e-10 - val_loss: 2.9880e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9675e-10 - val_loss: 2.9148e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8829e-10 - val_loss: 2.8167e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7972e-10 - val_loss: 2.7518e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7317e-10 - val_loss: 2.6980e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6864e-10 - val_loss: 2.6645e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6512e-10 - val_loss: 2.6211e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6155e-10 - val_loss: 2.5869e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5662e-10 - val_loss: 2.5192e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5047e-10 - val_loss: 2.4792e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4822e-10 - val_loss: 2.4389e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4145e-10 - val_loss: 2.3581e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3461e-10 - val_loss: 2.3279e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3053e-10 - val_loss: 2.2690e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2440e-10 - val_loss: 2.2057e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1953e-10 - val_loss: 2.1874e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1891e-10 - val_loss: 2.1766e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1662e-10 - val_loss: 2.1387e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1289e-10 - val_loss: 2.1072e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0947e-10 - val_loss: 2.0620e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0569e-10 - val_loss: 2.0397e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0280e-10 - val_loss: 1.9966e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9879e-10 - val_loss: 1.9601e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9546e-10 - val_loss: 1.9373e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9294e-10 - val_loss: 1.9108e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9122e-10 - val_loss: 1.9078e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9000e-10 - val_loss: 1.8786e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8670e-10 - val_loss: 1.8290e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7965e-10 - val_loss: 1.7614e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7563e-10 - val_loss: 1.7441e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7342e-10 - val_loss: 1.7134e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7127e-10 - val_loss: 1.7144e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7142e-10 - val_loss: 1.7093e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7103e-10 - val_loss: 1.6908e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6748e-10 - val_loss: 1.6435e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6272e-10 - val_loss: 1.6001e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5856e-10 - val_loss: 1.5718e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5637e-10 - val_loss: 1.5472e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5365e-10 - val_loss: 1.5142e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4989e-10 - val_loss: 1.4738e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4658e-10 - val_loss: 1.4607e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4611e-10 - val_loss: 1.4572e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4582e-10 - val_loss: 1.4575e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4529e-10 - val_loss: 1.4389e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4314e-10 - val_loss: 1.4202e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4137e-10 - val_loss: 1.3967e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3766e-10 - val_loss: 1.3513e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3439e-10 - val_loss: 1.3250e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3165e-10 - val_loss: 1.3040e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3069e-10 - val_loss: 1.3016e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3019e-10 - val_loss: 1.2996e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3014e-10 - val_loss: 1.2962e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2912e-10 - val_loss: 1.2817e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2674e-10 - val_loss: 1.2471e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2421e-10 - val_loss: 1.2273e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2192e-10 - val_loss: 1.2272e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2241e-10 - val_loss: 1.2143e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2059e-10 - val_loss: 1.1917e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1876e-10 - val_loss: 1.1928e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1918e-10 - val_loss: 1.1908e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1888e-10 - val_loss: 1.1717e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1594e-10 - val_loss: 1.1419e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1367e-10 - val_loss: 1.1236e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1112e-10 - val_loss: 1.1108e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1026e-10 - val_loss: 1.0919e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0857e-10 - val_loss: 1.0690e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0642e-10 - val_loss: 1.0592e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0484e-10 - val_loss: 1.0322e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0321e-10 - val_loss: 1.0261e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0144e-10 - val_loss: 1.0007e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0005e-10 - val_loss: 9.9730e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0017e-10 - val_loss: 1.0000e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9852e-11 - val_loss: 9.9276e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8265e-11 - val_loss: 9.7319e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6903e-11 - val_loss: 9.5824e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4838e-11 - val_loss: 9.3816e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3474e-11 - val_loss: 9.1807e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1115e-11 - val_loss: 9.0589e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0584e-11 - val_loss: 9.1165e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1765e-11 - val_loss: 9.2587e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2866e-11 - val_loss: 9.2337e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2930e-11 - val_loss: 9.3221e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3143e-11 - val_loss: 9.1934e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0552e-11 - val_loss: 8.8651e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7450e-11 - val_loss: 8.6058e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5187e-11 - val_loss: 8.4222e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4111e-11 - val_loss: 8.3466e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3373e-11 - val_loss: 8.2714e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2879e-11 - val_loss: 8.3881e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4664e-11 - val_loss: 8.6060e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7404e-11 - val_loss: 8.8713e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8319e-11 - val_loss: 8.7145e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6134e-11 - val_loss: 8.4636e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3915e-11 - val_loss: 8.1891e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0595e-11 - val_loss: 7.8633e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7275e-11 - val_loss: 7.6169e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5770e-11 - val_loss: 7.5108e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5226e-11 - val_loss: 7.5723e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5528e-11 - val_loss: 7.5724e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5545e-11 - val_loss: 7.5326e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5241e-11 - val_loss: 7.4891e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5408e-11 - val_loss: 7.5257e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5334e-11 - val_loss: 7.5329e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5430e-11 - val_loss: 7.5417e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5349e-11 - val_loss: 7.5683e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5908e-11 - val_loss: 7.5611e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4567e-11 - val_loss: 7.3437e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2337e-11 - val_loss: 7.0512e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9994e-11 - val_loss: 6.9444e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9601e-11 - val_loss: 6.9206e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9590e-11 - val_loss: 6.9821e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9931e-11 - val_loss: 7.0169e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9848e-11 - val_loss: 6.9609e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9788e-11 - val_loss: 6.9933e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0081e-11 - val_loss: 6.9534e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9252e-11 - val_loss: 6.8408e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7567e-11 - val_loss: 6.5911e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5354e-11 - val_loss: 6.4597e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4584e-11 - val_loss: 6.4459e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4045e-11 - val_loss: 6.3913e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3788e-11 - val_loss: 6.4039e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4256e-11 - val_loss: 6.3947e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4004e-11 - val_loss: 6.3868e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4270e-11 - val_loss: 6.4672e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4279e-11 - val_loss: 6.3025e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1840e-11 - val_loss: 5.9782e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8921e-11 - val_loss: 5.7401e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7433e-11 - val_loss: 5.7091e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6872e-11 - val_loss: 5.6567e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6761e-11 - val_loss: 5.8051e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8377e-11 - val_loss: 5.8879e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9431e-11 - val_loss: 5.9927e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0229e-11 - val_loss: 5.9861e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8847e-11 - val_loss: 5.7395e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7010e-11 - val_loss: 5.6059e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5672e-11 - val_loss: 5.5067e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4793e-11 - val_loss: 5.4257e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3992e-11 - val_loss: 5.3594e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3286e-11 - val_loss: 5.2677e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2723e-11 - val_loss: 5.3320e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4321e-11 - val_loss: 5.5640e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6117e-11 - val_loss: 5.6930e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6770e-11 - val_loss: 5.6209e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5712e-11 - val_loss: 5.4684e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4259e-11 - val_loss: 5.3434e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3408e-11 - val_loss: 5.3189e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2909e-11 - val_loss: 5.2356e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2460e-11 - val_loss: 5.2640e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3017e-11 - val_loss: 5.3478e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3562e-11 - val_loss: 5.3967e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4154e-11 - val_loss: 5.4035e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3882e-11 - val_loss: 5.2948e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2866e-11 - val_loss: 5.2505e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1957e-11 - val_loss: 5.1065e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0487e-11 - val_loss: 4.9876e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9431e-11 - val_loss: 4.8896e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9147e-11 - val_loss: 4.9025e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8522e-11 - val_loss: 4.7690e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7515e-11 - val_loss: 4.7213e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7287e-11 - val_loss: 4.7690e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7661e-11 - val_loss: 4.7244e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7155e-11 - val_loss: 4.7047e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7021e-11 - val_loss: 4.6841e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6778e-11 - val_loss: 4.7258e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7944e-11 - val_loss: 4.8566e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8598e-11 - val_loss: 4.8745e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8735e-11 - val_loss: 4.8828e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8666e-11 - val_loss: 4.7837e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7518e-11 - val_loss: 4.7231e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7067e-11 - val_loss: 4.6466e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6195e-11 - val_loss: 4.5502e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5065e-11 - val_loss: 4.4427e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4257e-11 - val_loss: 4.3916e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4125e-11 - val_loss: 4.3841e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3622e-11 - val_loss: 4.3658e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4025e-11 - val_loss: 4.3989e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3533e-11 - val_loss: 4.3053e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3009e-11 - val_loss: 4.2914e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3020e-11 - val_loss: 4.2975e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2679e-11 - val_loss: 4.2224e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2151e-11 - val_loss: 4.1978e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1723e-11 - val_loss: 4.1452e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1822e-11 - val_loss: 4.2293e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2751e-11 - val_loss: 4.3062e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3162e-11 - val_loss: 4.3263e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3509e-11 - val_loss: 4.3491e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3493e-11 - val_loss: 4.3105e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3415e-11 - val_loss: 4.3679e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3534e-11 - val_loss: 4.2704e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2165e-11 - val_loss: 4.1420e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0412e-11 - val_loss: 3.8802e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8562e-11 - val_loss: 3.8234e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8263e-11 - val_loss: 3.8174e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7957e-11 - val_loss: 3.8227e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8317e-11 - val_loss: 3.8509e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8661e-11 - val_loss: 3.8999e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8950e-11 - val_loss: 3.8963e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8920e-11 - val_loss: 3.8147e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7723e-11 - val_loss: 3.7149e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6678e-11 - val_loss: 3.6258e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5987e-11 - val_loss: 3.5822e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5815e-11 - val_loss: 3.6363e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7226e-11 - val_loss: 3.8369e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8657e-11 - val_loss: 3.9294e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9805e-11 - val_loss: 4.0882e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1450e-11 - val_loss: 4.1539e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1587e-11 - val_loss: 4.1068e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0925e-11 - val_loss: 4.0648e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0016e-11 - val_loss: 3.8604e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8147e-11 - val_loss: 3.7331e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6971e-11 - val_loss: 3.6528e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6434e-11 - val_loss: 3.6381e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6283e-11 - val_loss: 3.6516e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6981e-11 - val_loss: 3.7634e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7807e-11 - val_loss: 3.8027e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7534e-11 - val_loss: 3.7237e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7261e-11 - val_loss: 3.7152e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7045e-11 - val_loss: 3.6776e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6402e-11 - val_loss: 3.5625e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5580e-11 - val_loss: 3.5238e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5241e-11 - val_loss: 3.5276e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5429e-11 - val_loss: 3.5461e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5541e-11 - val_loss: 3.5748e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6308e-11 - val_loss: 3.6899e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6831e-11 - val_loss: 3.6776e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6720e-11 - val_loss: 3.5990e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5786e-11 - val_loss: 3.5704e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5277e-11 - val_loss: 3.5024e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4551e-11 - val_loss: 3.3779e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3083e-11 - val_loss: 3.2612e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2289e-11 - val_loss: 3.1759e-11
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1145e-11 - val_loss: 3.0492e-11
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0464e-11 - val_loss: 3.0348e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0488e-11 - val_loss: 3.0678e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0791e-11 - val_loss: 3.1227e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1748e-11 - val_loss: 3.2145e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2609e-11 - val_loss: 3.3095e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2598e-11 - val_loss: 3.2339e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2540e-11 - val_loss: 3.2806e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2865e-11 - val_loss: 3.2970e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3022e-11 - val_loss: 3.3039e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3489e-11 - val_loss: 3.3397e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3080e-11 - val_loss: 3.2622e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2707e-11 - val_loss: 3.2455e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2417e-11 - val_loss: 3.2389e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2331e-11 - val_loss: 3.2603e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2858e-11 - val_loss: 3.3053e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2678e-11 - val_loss: 3.2244e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2043e-11 - val_loss: 3.1528e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1189e-11 - val_loss: 3.0684e-11
Epoch 321/512

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0304e-11 - val_loss: 3.0097e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9943e-11 - val_loss: 2.9591e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9745e-11 - val_loss: 2.9892e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9631e-11 - val_loss: 2.9555e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9549e-11 - val_loss: 2.9726e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9948e-11 - val_loss: 3.0403e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1050e-11 - val_loss: 3.1249e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1437e-11 - val_loss: 3.1318e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1394e-11 - val_loss: 3.1159e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0886e-11 - val_loss: 2.9999e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9706e-11 - val_loss: 2.9203e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8819e-11 - val_loss: 2.8447e-11
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8077e-11 - val_loss: 2.7838e-11
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7864e-11 - val_loss: 2.7767e-11
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7627e-11 - val_loss: 2.7493e-11
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7446e-11 - val_loss: 2.7430e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7493e-11 - val_loss: 2.7663e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7648e-11 - val_loss: 2.7634e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7785e-11 - val_loss: 2.7925e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8348e-11 - val_loss: 2.8908e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8992e-11 - val_loss: 2.9011e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9384e-11 - val_loss: 2.9770e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9986e-11 - val_loss: 3.0290e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0302e-11 - val_loss: 2.9775e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9949e-11 - val_loss: 3.0440e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0521e-11 - val_loss: 3.0885e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1394e-11 - val_loss: 3.1682e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1834e-11 - val_loss: 3.2111e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1828e-11 - val_loss: 3.1249e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1100e-11 - val_loss: 3.0805e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0616e-11 - val_loss: 3.0315e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0466e-11 - val_loss: 3.0650e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0474e-11 - val_loss: 3.0124e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9821e-11 - val_loss: 2.9353e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8988e-11 - val_loss: 2.8801e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8925e-11 - val_loss: 2.9073e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8966e-11 - val_loss: 2.8959e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9305e-11 - val_loss: 2.9380e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9327e-11 - val_loss: 2.9059e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8969e-11 - val_loss: 2.8616e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8204e-11 - val_loss: 2.7619e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7538e-11 - val_loss: 2.7469e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7490e-11 - val_loss: 2.7445e-11
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7106e-11 - val_loss: 2.6492e-11
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6382e-11 - val_loss: 2.6087e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6040e-11 - val_loss: 2.5796e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5835e-11 - val_loss: 2.5889e-11
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5923e-11 - val_loss: 2.5613e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5653e-11 - val_loss: 2.5730e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5793e-11 - val_loss: 2.5373e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5293e-11 - val_loss: 2.5209e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5407e-11 - val_loss: 2.5541e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5578e-11 - val_loss: 2.5556e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5952e-11 - val_loss: 2.6251e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6512e-11 - val_loss: 2.6723e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6699e-11 - val_loss: 2.6568e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6452e-11 - val_loss: 2.6192e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5822e-11 - val_loss: 2.5114e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5034e-11 - val_loss: 2.4953e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4886e-11 - val_loss: 2.4997e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5202e-11 - val_loss: 2.5167e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5422e-11 - val_loss: 2.5695e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5954e-11 - val_loss: 2.6369e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6887e-11 - val_loss: 2.7230e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7186e-11 - val_loss: 2.7146e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7174e-11 - val_loss: 2.7092e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7264e-11 - val_loss: 2.7238e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7146e-11 - val_loss: 2.7015e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7006e-11 - val_loss: 2.6873e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6602e-11 - val_loss: 2.6525e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6835e-11 - val_loss: 2.7296e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7284e-11 - val_loss: 2.7210e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7349e-11 - val_loss: 2.7440e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7453e-11 - val_loss: 2.7433e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7367e-11 - val_loss: 2.7145e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6888e-11 - val_loss: 2.6509e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6575e-11 - val_loss: 2.6415e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6402e-11 - val_loss: 2.6331e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6452e-11 - val_loss: 2.6410e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5946e-11 - val_loss: 2.5227e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4868e-11 - val_loss: 2.4382e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4365e-11 - val_loss: 2.4127e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3985e-11 - val_loss: 2.3786e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3730e-11 - val_loss: 2.3647e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3686e-11 - val_loss: 2.3744e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3952e-11 - val_loss: 2.4178e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4344e-11 - val_loss: 2.4460e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4667e-11 - val_loss: 2.4804e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4796e-11 - val_loss: 2.4368e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4247e-11 - val_loss: 2.3880e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3617e-11 - val_loss: 2.3365e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3191e-11 - val_loss: 2.3194e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3384e-11 - val_loss: 2.3427e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3321e-11 - val_loss: 2.3324e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3357e-11 - val_loss: 2.3232e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3307e-11 - val_loss: 2.3159e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3207e-11 - val_loss: 2.3391e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3401e-11 - val_loss: 2.3543e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4143e-11 - val_loss: 2.4634e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4479e-11 - val_loss: 2.4281e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4237e-11 - val_loss: 2.4055e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4175e-11 - val_loss: 2.4290e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4406e-11 - val_loss: 2.4328e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4157e-11 - val_loss: 2.4129e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4381e-11 - val_loss: 2.4511e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4504e-11 - val_loss: 2.4554e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4782e-11 - val_loss: 2.4919e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5077e-11 - val_loss: 2.4849e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4849e-11 - val_loss: 2.4833e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4720e-11 - val_loss: 2.4791e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4442e-11 - val_loss: 2.3928e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3506e-11 - val_loss: 2.2915e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3022e-11 - val_loss: 2.3242e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3516e-11 - val_loss: 2.3861e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4121e-11 - val_loss: 2.4129e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3848e-11 - val_loss: 2.3471e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3291e-11 - val_loss: 2.2975e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2912e-11 - val_loss: 2.2541e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2199e-11 - val_loss: 2.1685e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1604e-11 - val_loss: 2.1138e-11
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0768e-11 - val_loss: 2.0035e-11
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9948e-11 - val_loss: 1.9630e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9514e-11 - val_loss: 1.9475e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9385e-11 - val_loss: 1.9278e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9202e-11 - val_loss: 1.9245e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9448e-11 - val_loss: 1.9733e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0079e-11 - val_loss: 2.0410e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0692e-11 - val_loss: 2.0889e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1003e-11 - val_loss: 2.1142e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1172e-11 - val_loss: 2.0896e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0913e-11 - val_loss: 2.0673e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0633e-11 - val_loss: 2.0393e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0412e-11 - val_loss: 2.0542e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0596e-11 - val_loss: 2.0566e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0624e-11 - val_loss: 2.0680e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1042e-11 - val_loss: 2.1593e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1638e-11 - val_loss: 2.1947e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2171e-11 - val_loss: 2.2390e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2381e-11 - val_loss: 2.2489e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2767e-11 - val_loss: 2.3025e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3017e-11 - val_loss: 2.3001e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2891e-11 - val_loss: 2.2386e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2288e-11 - val_loss: 2.2140e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1932e-11 - val_loss: 2.1621e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1520e-11 - val_loss: 2.1328e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1139e-11 - val_loss: 2.1028e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0949e-11 - val_loss: 2.0958e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1190e-11 - val_loss: 2.1419e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1464e-11 - val_loss: 2.1772e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2052e-11 - val_loss: 2.1899e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1829e-11 - val_loss: 2.1647e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1625e-11 - val_loss: 2.1537e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1456e-11 - val_loss: 2.1301e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1013e-11 - val_loss: 2.0571e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0581e-11 - val_loss: 2.0536e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0402e-11 - val_loss: 2.0203e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0025e-11 - val_loss: 1.9895e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9855e-11 - val_loss: 1.9795e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0117e-11 - val_loss: 2.0393e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0506e-11 - val_loss: 2.0669e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0560e-11 - val_loss: 2.0479e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0549e-11 - val_loss: 2.0541e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0392e-11 - val_loss: 2.0215e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0162e-11 - val_loss: 1.9929e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9718e-11 - val_loss: 1.9367e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9315e-11 - val_loss: 1.9121e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-lr0.00005-RMS-20/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8959e-11 - val_loss: 1.8568e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8619e-11 - val_loss: 1.8678e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8833e-11 - val_loss: 1.8991e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9293e-11 - val_loss: 1.9488e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9737e-11 - val_loss: 1.9926e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0060e-11 - val_loss: 2.0350e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0463e-11 - val_loss: 2.0463e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0376e-11 - val_loss: 2.0257e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0324e-11 - val_loss: 2.0162e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0155e-11 - val_loss: 1.9879e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9672e-11 - val_loss: 1.9418e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9390e-11 - val_loss: 1.9577e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9844e-11 - val_loss: 2.0195e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0505e-11 - val_loss: 2.0815e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1020e-11 - val_loss: 2.1098e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1102e-11 - val_loss: 2.1193e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1025e-11 - val_loss: 2.0875e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0655e-11 - val_loss: 2.0262e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0189e-11 - val_loss: 1.9813e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9465e-11 - val_loss: 1.9016e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9104e-11 - val_loss: 1.9028e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9067e-11 - val_loss: 1.9077e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9200e-11 - val_loss: 1.9140e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9315e-11 - val_loss: 1.9718e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9819e-11 - val_loss: 2.0011e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0162e-11 - val_loss: 2.0385e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.787 | eve: 10.091 | bob: 9.386Epoch   0:   0% | abe: 9.628 | eve: 10.050 | bob: 9.317Epoch   0:   1% | abe: 9.523 | eve: 10.018 | bob: 9.267Epoch   0:   2% | abe: 9.456 | eve: 10.017 | bob: 9.240Epoch   0:   3% | abe: 9.389 | eve: 9.998 | bob: 9.203Epoch   0:   3% | abe: 9.351 | eve: 9.989 | bob: 9.189Epoch   0:   4% | abe: 9.309 | eve: 9.988 | bob: 9.164Epoch   0:   5% | abe: 9.285 | eve: 9.977 | bob: 9.154Epoch   0:   6% | abe: 9.269 | eve: 9.975 | bob: 9.148Epoch   0:   7% | abe: 9.252 | eve: 9.981 | bob: 9.140Epoch   0:   7% | abe: 9.242 | eve: 9.980 | bob: 9.137Epoch   0:   8% | abe: 9.229 | eve: 9.977 | bob: 9.131Epoch   0:   9% | abe: 9.214 | eve: 9.984 | bob: 9.121Epoch   0:  10% | abe: 9.203 | eve: 9.983 | bob: 9.115Epoch   0:  10% | abe: 9.196 | eve: 9.989 | bob: 9.112Epoch   0:  11% | abe: 9.186 | eve: 9.985 | bob: 9.105Epoch   0:  12% | abe: 9.178 | eve: 9.983 | bob: 9.100Epoch   0:  13% | abe: 9.175 | eve: 9.984 | bob: 9.100Epoch   0:  14% | abe: 9.172 | eve: 9.983 | bob: 9.099Epoch   0:  14% | abe: 9.165 | eve: 9.981 | bob: 9.095Epoch   0:  15% | abe: 9.163 | eve: 9.983 | bob: 9.095Epoch   0:  16% | abe: 9.156 | eve: 9.984 | bob: 9.090Epoch   0:  17% | abe: 9.150 | eve: 9.981 | bob: 9.086Epoch   0:  17% | abe: 9.146 | eve: 9.981 | bob: 9.084Epoch   0:  18% | abe: 9.144 | eve: 9.980 | bob: 9.083Epoch   0:  19% | abe: 9.142 | eve: 9.984 | bob: 9.083Epoch   0:  20% | abe: 9.140 | eve: 9.987 | bob: 9.083Epoch   0:  21% | abe: 9.137 | eve: 9.989 | bob: 9.080Epoch   0:  21% | abe: 9.135 | eve: 9.990 | bob: 9.080Epoch   0:  22% | abe: 9.133 | eve: 9.991 | bob: 9.079Epoch   0:  23% | abe: 9.130 | eve: 9.998 | bob: 9.077Epoch   0:  24% | abe: 9.129 | eve: 10.000 | bob: 9.076Epoch   0:  25% | abe: 9.128 | eve: 10.003 | bob: 9.077Epoch   0:  25% | abe: 9.127 | eve: 10.008 | bob: 9.077Epoch   0:  26% | abe: 9.124 | eve: 10.010 | bob: 9.074Epoch   0:  27% | abe: 9.122 | eve: 10.011 | bob: 9.073Epoch   0:  28% | abe: 9.120 | eve: 10.013 | bob: 9.072Epoch   0:  28% | abe: 9.119 | eve: 10.016 | bob: 9.072Epoch   0:  29% | abe: 9.117 | eve: 10.020 | bob: 9.070Epoch   0:  30% | abe: 9.115 | eve: 10.023 | bob: 9.069Epoch   0:  31% | abe: 9.114 | eve: 10.027 | bob: 9.068Epoch   0:  32% | abe: 9.112 | eve: 10.029 | bob: 9.067Epoch   0:  32% | abe: 9.112 | eve: 10.034 | bob: 9.068Epoch   0:  33% | abe: 9.111 | eve: 10.037 | bob: 9.067Epoch   0:  34% | abe: 9.109 | eve: 10.040 | bob: 9.066Epoch   0:  35% | abe: 9.107 | eve: 10.043 | bob: 9.064Epoch   0:  35% | abe: 9.107 | eve: 10.046 | bob: 9.064Epoch   0:  36% | abe: 9.105 | eve: 10.049 | bob: 9.062Epoch   0:  37% | abe: 9.104 | eve: 10.054 | bob: 9.062Epoch   0:  38% | abe: 9.104 | eve: 10.056 | bob: 9.062Epoch   0:  39% | abe: 9.102 | eve: 10.058 | bob: 9.061Epoch   0:  39% | abe: 9.102 | eve: 10.062 | bob: 9.061Epoch   0:  40% | abe: 9.102 | eve: 10.067 | bob: 9.061Epoch   0:  41% | abe: 9.102 | eve: 10.069 | bob: 9.061Epoch   0:  42% | abe: 9.102 | eve: 10.072 | bob: 9.061Epoch   0:  42% | abe: 9.102 | eve: 10.076 | bob: 9.061Epoch   0:  43% | abe: 9.100 | eve: 10.078 | bob: 9.060Epoch   0:  44% | abe: 9.099 | eve: 10.081 | bob: 9.060Epoch   0:  45% | abe: 9.099 | eve: 10.084 | bob: 9.059Epoch   0:  46% | abe: 9.097 | eve: 10.089 | bob: 9.058Epoch   0:  46% | abe: 9.097 | eve: 10.092 | bob: 9.058Epoch   0:  47% | abe: 9.096 | eve: 10.095 | bob: 9.057Epoch   0:  48% | abe: 9.096 | eve: 10.098 | bob: 9.057Epoch   0:  49% | abe: 9.096 | eve: 10.101 | bob: 9.057Epoch   0:  50% | abe: 9.094 | eve: 10.105 | bob: 9.056Epoch   0:  50% | abe: 9.094 | eve: 10.108 | bob: 9.055Epoch   0:  51% | abe: 9.093 | eve: 10.111 | bob: 9.055Epoch   0:  52% | abe: 9.092 | eve: 10.112 | bob: 9.054Epoch   0:  53% | abe: 9.092 | eve: 10.114 | bob: 9.054Epoch   0:  53% | abe: 9.091 | eve: 10.118 | bob: 9.054Epoch   0:  54% | abe: 9.092 | eve: 10.121 | bob: 9.055Epoch   0:  55% | abe: 9.091 | eve: 10.126 | bob: 9.054Epoch   0:  56% | abe: 9.090 | eve: 10.128 | bob: 9.053Epoch   0:  57% | abe: 9.089 | eve: 10.132 | bob: 9.052Epoch   0:  57% | abe: 9.088 | eve: 10.135 | bob: 9.052Epoch   0:  58% | abe: 9.088 | eve: 10.138 | bob: 9.051Epoch   0:  59% | abe: 9.087 | eve: 10.141 | bob: 9.051Epoch   0:  60% | abe: 9.087 | eve: 10.144 | bob: 9.050Epoch   0:  60% | abe: 9.087 | eve: 10.147 | bob: 9.051Epoch   0:  61% | abe: 9.086 | eve: 10.150 | bob: 9.050Epoch   0:  62% | abe: 9.086 | eve: 10.153 | bob: 9.050Epoch   0:  63% | abe: 9.086 | eve: 10.155 | bob: 9.050Epoch   0:  64% | abe: 9.085 | eve: 10.157 | bob: 9.049Epoch   0:  64% | abe: 9.085 | eve: 10.160 | bob: 9.049Epoch   0:  65% | abe: 9.085 | eve: 10.163 | bob: 9.049Epoch   0:  66% | abe: 9.084 | eve: 10.165 | bob: 9.048Epoch   0:  67% | abe: 9.084 | eve: 10.168 | bob: 9.048Epoch   0:  67% | abe: 9.083 | eve: 10.171 | bob: 9.047Epoch   0:  68% | abe: 9.083 | eve: 10.174 | bob: 9.048Epoch   0:  69% | abe: 9.083 | eve: 10.177 | bob: 9.047Epoch   0:  70% | abe: 9.082 | eve: 10.180 | bob: 9.047Epoch   0:  71% | abe: 9.082 | eve: 10.182 | bob: 9.047Epoch   0:  71% | abe: 9.082 | eve: 10.185 | bob: 9.047Epoch   0:  72% | abe: 9.081 | eve: 10.189 | bob: 9.046Epoch   0:  73% | abe: 9.080 | eve: 10.192 | bob: 9.045Epoch   0:  74% | abe: 9.080 | eve: 10.195 | bob: 9.045Epoch   0:  75% | abe: 9.080 | eve: 10.197 | bob: 9.045Epoch   0:  75% | abe: 9.079 | eve: 10.200 | bob: 9.044Epoch   0:  76% | abe: 9.079 | eve: 10.202 | bob: 9.044Epoch   0:  77% | abe: 9.078 | eve: 10.205 | bob: 9.043Epoch   0:  78% | abe: 9.078 | eve: 10.208 | bob: 9.043Epoch   0:  78% | abe: 9.078 | eve: 10.210 | bob: 9.043Epoch   0:  79% | abe: 9.078 | eve: 10.213 | bob: 9.043Epoch   0:  80% | abe: 9.077 | eve: 10.215 | bob: 9.042Epoch   0:  81% | abe: 9.077 | eve: 10.217 | bob: 9.042Epoch   0:  82% | abe: 9.077 | eve: 10.219 | bob: 9.042Epoch   0:  82% | abe: 9.076 | eve: 10.221 | bob: 9.042Epoch   0:  83% | abe: 9.076 | eve: 10.224 | bob: 9.042Epoch   0:  84% | abe: 9.076 | eve: 10.226 | bob: 9.042Epoch   0:  85% | abe: 9.076 | eve: 10.229 | bob: 9.042Epoch   0:  85% | abe: 9.076 | eve: 10.231 | bob: 9.041