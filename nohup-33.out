WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-03-29 17:00:29.948428: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-03-29 17:00:30.106583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-03-29 17:00:30.107507: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-29 17:00:30.109767: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-29 17:00:30.111939: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-29 17:00:30.113067: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-29 17:00:30.115665: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-29 17:00:30.117696: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-29 17:00:30.122545: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-29 17:00:30.129202: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-29 17:00:30.129782: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-03-29 17:00:30.148187: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-03-29 17:00:30.151522: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3be9a10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-03-29 17:00:30.151570: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-03-29 17:00:30.534086: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x35b3390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-29 17:00:30.534168: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-03-29 17:00:30.542445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-03-29 17:00:30.542551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-29 17:00:30.542592: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-29 17:00:30.542625: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-29 17:00:30.542657: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-29 17:00:30.542688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-29 17:00:30.542720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-29 17:00:30.542752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-29 17:00:30.555857: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-29 17:00:30.555918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-29 17:00:30.563656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-03-29 17:00:30.563688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-03-29 17:00:30.563701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-03-29 17:00:30.571143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
2024-03-29 17:00:34.343296: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 1024 samples, validate on 1024 samples
Epoch 1/2000
1024/1024 - 1s - loss: 0.0548 - val_loss: 0.0025
Epoch 2/2000
1024/1024 - 0s - loss: 0.0162 - val_loss: 0.0021
Epoch 3/2000
1024/1024 - 0s - loss: 0.0077 - val_loss: 0.0028
Epoch 4/2000
1024/1024 - 0s - loss: 0.0045 - val_loss: 9.0653e-04
Epoch 5/2000
1024/1024 - 0s - loss: 0.0026 - val_loss: 4.6557e-04
Epoch 6/2000
1024/1024 - 0s - loss: 0.0017 - val_loss: 3.3479e-04
Epoch 7/2000
1024/1024 - 0s - loss: 0.0013 - val_loss: 2.5360e-04
Epoch 8/2000
1024/1024 - 0s - loss: 0.0011 - val_loss: 2.0548e-04
Epoch 9/2000
1024/1024 - 0s - loss: 9.3195e-04 - val_loss: 1.8995e-04
Epoch 10/2000
1024/1024 - 0s - loss: 8.1090e-04 - val_loss: 1.8195e-04
Epoch 11/2000
1024/1024 - 0s - loss: 6.9128e-04 - val_loss: 2.3272e-04
Epoch 12/2000
1024/1024 - 0s - loss: 6.0007e-04 - val_loss: 2.5714e-04
Epoch 13/2000
1024/1024 - 0s - loss: 5.3332e-04 - val_loss: 2.3752e-04
Epoch 14/2000
1024/1024 - 0s - loss: 4.7422e-04 - val_loss: 2.2413e-04
Epoch 15/2000
1024/1024 - 0s - loss: 4.1305e-04 - val_loss: 2.3992e-04
Epoch 16/2000
1024/1024 - 0s - loss: 3.6514e-04 - val_loss: 2.4318e-04
Epoch 17/2000
1024/1024 - 0s - loss: 3.2234e-04 - val_loss: 2.3245e-04
Epoch 18/2000
1024/1024 - 0s - loss: 2.9036e-04 - val_loss: 2.1690e-04
Epoch 19/2000
1024/1024 - 0s - loss: 2.6804e-04 - val_loss: 2.2320e-04
Epoch 20/2000
1024/1024 - 0s - loss: 2.5014e-04 - val_loss: 2.0288e-04
Epoch 21/2000
1024/1024 - 0s - loss: 2.3746e-04 - val_loss: 2.1427e-04
Epoch 22/2000
1024/1024 - 0s - loss: 2.2488e-04 - val_loss: 2.0630e-04
Epoch 23/2000
1024/1024 - 0s - loss: 2.1390e-04 - val_loss: 1.9891e-04
Epoch 24/2000
1024/1024 - 0s - loss: 2.0445e-04 - val_loss: 1.9047e-04
Epoch 25/2000
1024/1024 - 0s - loss: 1.9966e-04 - val_loss: 2.1088e-04
Epoch 26/2000
1024/1024 - 0s - loss: 1.9339e-04 - val_loss: 2.1933e-04
Epoch 27/2000
1024/1024 - 0s - loss: 2.0096e-04 - val_loss: 1.7449e-04
Epoch 28/2000
1024/1024 - 0s - loss: 1.8326e-04 - val_loss: 1.9484e-04
Epoch 29/2000
1024/1024 - 0s - loss: 1.7813e-04 - val_loss: 1.9094e-04
Epoch 30/2000
1024/1024 - 0s - loss: 1.7267e-04 - val_loss: 1.8839e-04
Epoch 31/2000
1024/1024 - 0s - loss: 1.7550e-04 - val_loss: 2.0420e-04
Epoch 32/2000
1024/1024 - 0s - loss: 1.7367e-04 - val_loss: 1.7151e-04
Epoch 33/2000
1024/1024 - 0s - loss: 1.6750e-04 - val_loss: 1.8307e-04
Epoch 34/2000
1024/1024 - 0s - loss: 1.6043e-04 - val_loss: 1.8206e-04
Epoch 35/2000
1024/1024 - 0s - loss: 1.5789e-04 - val_loss: 1.8102e-04
Epoch 36/2000
1024/1024 - 0s - loss: 1.5401e-04 - val_loss: 1.7453e-04
Epoch 37/2000
1024/1024 - 0s - loss: 1.5741e-04 - val_loss: 1.5976e-04
Epoch 38/2000
1024/1024 - 0s - loss: 1.5804e-04 - val_loss: 1.9032e-04
Epoch 39/2000
1024/1024 - 0s - loss: 1.5382e-04 - val_loss: 1.7792e-04
Epoch 40/2000
1024/1024 - 0s - loss: 1.5750e-04 - val_loss: 1.6578e-04
Epoch 41/2000
1024/1024 - 0s - loss: 1.5202e-04 - val_loss: 1.6847e-04
Epoch 42/2000
1024/1024 - 0s - loss: 1.5453e-04 - val_loss: 1.6635e-04
Epoch 43/2000
1024/1024 - 0s - loss: 1.4843e-04 - val_loss: 1.5803e-04
Epoch 44/2000
1024/1024 - 0s - loss: 1.4489e-04 - val_loss: 1.6038e-04
Epoch 45/2000
1024/1024 - 0s - loss: 1.4588e-04 - val_loss: 1.7357e-04
Epoch 46/2000
1024/1024 - 0s - loss: 1.6472e-04 - val_loss: 1.4229e-04
Epoch 47/2000
1024/1024 - 0s - loss: 1.4794e-04 - val_loss: 1.5188e-04
Epoch 48/2000
1024/1024 - 0s - loss: 1.4332e-04 - val_loss: 1.5276e-04
Epoch 49/2000
1024/1024 - 0s - loss: 1.6768e-04 - val_loss: 1.6389e-04
Epoch 50/2000
1024/1024 - 0s - loss: 2.8287e-04 - val_loss: 1.3165e-04
Epoch 51/2000
1024/1024 - 0s - loss: 1.9579e-04 - val_loss: 1.4662e-04
Epoch 52/2000
1024/1024 - 0s - loss: 1.4128e-04 - val_loss: 1.2821e-04
Epoch 53/2000
1024/1024 - 0s - loss: 1.3961e-04 - val_loss: 1.6290e-04
Epoch 54/2000
1024/1024 - 0s - loss: 1.5436e-04 - val_loss: 1.3405e-04
Epoch 55/2000
1024/1024 - 0s - loss: 1.3796e-04 - val_loss: 1.2836e-04
Epoch 56/2000
1024/1024 - 0s - loss: 1.3828e-04 - val_loss: 1.4005e-04
Epoch 57/2000
1024/1024 - 0s - loss: 1.2870e-04 - val_loss: 1.4952e-04
Epoch 58/2000
1024/1024 - 0s - loss: 1.2759e-04 - val_loss: 1.4295e-04
Epoch 59/2000
1024/1024 - 0s - loss: 1.2639e-04 - val_loss: 1.4880e-04
Epoch 60/2000
1024/1024 - 0s - loss: 1.3085e-04 - val_loss: 1.5297e-04
Epoch 61/2000
1024/1024 - 0s - loss: 1.5963e-04 - val_loss: 1.2060e-04
Epoch 62/2000
1024/1024 - 0s - loss: 1.4737e-04 - val_loss: 1.2343e-04
Epoch 63/2000
1024/1024 - 0s - loss: 1.3929e-04 - val_loss: 1.1407e-04
Epoch 64/2000
1024/1024 - 0s - loss: 1.4119e-04 - val_loss: 1.4636e-04
Epoch 65/2000
1024/1024 - 0s - loss: 1.2507e-04 - val_loss: 1.3395e-04
Epoch 66/2000
1024/1024 - 0s - loss: 1.2120e-04 - val_loss: 1.2679e-04
Epoch 67/2000
1024/1024 - 0s - loss: 1.2465e-04 - val_loss: 1.5120e-04
Epoch 68/2000
1024/1024 - 0s - loss: 1.1762e-04 - val_loss: 1.3389e-04
Epoch 69/2000
1024/1024 - 0s - loss: 1.3137e-04 - val_loss: 1.5518e-04
Epoch 70/2000
1024/1024 - 0s - loss: 1.1874e-04 - val_loss: 1.4466e-04
Epoch 71/2000
1024/1024 - 0s - loss: 1.1822e-04 - val_loss: 1.4992e-04
Epoch 72/2000
1024/1024 - 0s - loss: 1.1652e-04 - val_loss: 1.5139e-04
Epoch 73/2000
1024/1024 - 0s - loss: 1.1460e-04 - val_loss: 1.5941e-04
Epoch 74/2000
1024/1024 - 0s - loss: 1.1362e-04 - val_loss: 1.3911e-04
Epoch 75/2000
1024/1024 - 0s - loss: 1.1849e-04 - val_loss: 1.5406e-04
Epoch 76/2000
1024/1024 - 0s - loss: 1.1553e-04 - val_loss: 1.4668e-04
Epoch 77/2000
1024/1024 - 0s - loss: 1.1233e-04 - val_loss: 1.3756e-04
Epoch 78/2000
1024/1024 - 0s - loss: 1.1506e-04 - val_loss: 1.3309e-04
Epoch 79/2000
1024/1024 - 0s - loss: 1.3068e-04 - val_loss: 1.1370e-04
Epoch 80/2000
1024/1024 - 0s - loss: 1.5559e-04 - val_loss: 1.5593e-04
Epoch 81/2000
1024/1024 - 0s - loss: 1.1800e-04 - val_loss: 1.5114e-04
Epoch 82/2000
1024/1024 - 0s - loss: 1.1423e-04 - val_loss: 1.3722e-04
Epoch 83/2000
1024/1024 - 0s - loss: 1.0816e-04 - val_loss: 1.4070e-04
Epoch 84/2000
1024/1024 - 0s - loss: 1.1161e-04 - val_loss: 1.4861e-04
Epoch 85/2000
1024/1024 - 0s - loss: 2.4924e-04 - val_loss: 1.1852e-04
Epoch 86/2000
1024/1024 - 0s - loss: 2.4161e-04 - val_loss: 1.0299e-04
Epoch 87/2000
1024/1024 - 0s - loss: 1.8357e-04 - val_loss: 1.1497e-04
Epoch 88/2000
1024/1024 - 0s - loss: 1.2031e-04 - val_loss: 1.2504e-04
Epoch 89/2000
1024/1024 - 0s - loss: 1.1328e-04 - val_loss: 1.0550e-04
Epoch 90/2000
1024/1024 - 0s - loss: 1.0463e-04 - val_loss: 1.0803e-04
Epoch 91/2000
1024/1024 - 0s - loss: 1.2779e-04 - val_loss: 1.1442e-04
Epoch 92/2000
1024/1024 - 0s - loss: 1.1426e-04 - val_loss: 1.0849e-04
Epoch 93/2000
1024/1024 - 0s - loss: 1.0435e-04 - val_loss: 1.2406e-04
Epoch 94/2000
1024/1024 - 0s - loss: 1.0220e-04 - val_loss: 1.1642e-04
Epoch 95/2000
1024/1024 - 0s - loss: 1.0169e-04 - val_loss: 1.2714e-04
Epoch 96/2000
1024/1024 - 0s - loss: 1.1301e-04 - val_loss: 1.0846e-04
Epoch 97/2000
1024/1024 - 0s - loss: 4.7413e-04 - val_loss: 1.1325e-04
Epoch 98/2000
1024/1024 - 0s - loss: 5.1606e-04 - val_loss: 6.9658e-05
Epoch 99/2000
1024/1024 - 0s - loss: 2.7845e-04 - val_loss: 4.3987e-05
Epoch 100/2000
1024/1024 - 0s - loss: 1.5287e-04 - val_loss: 5.9307e-05
Epoch 101/2000
1024/1024 - 0s - loss: 9.7342e-05 - val_loss: 6.2613e-05
Epoch 102/2000
1024/1024 - 0s - loss: 8.0506e-05 - val_loss: 6.2832e-05
Epoch 103/2000
1024/1024 - 0s - loss: 7.3259e-05 - val_loss: 6.4422e-05
Epoch 104/2000
1024/1024 - 0s - loss: 6.4708e-05 - val_loss: 6.6022e-05
Epoch 105/2000
1024/1024 - 0s - loss: 6.0526e-05 - val_loss: 6.8816e-05
Epoch 106/2000
1024/1024 - 0s - loss: 5.6708e-05 - val_loss: 7.0694e-05
Epoch 107/2000
1024/1024 - 0s - loss: 5.4164e-05 - val_loss: 6.8597e-05
Epoch 108/2000
1024/1024 - 0s - loss: 5.1723e-05 - val_loss: 7.1096e-05
Epoch 109/2000
1024/1024 - 0s - loss: 4.9463e-05 - val_loss: 7.1596e-05
Epoch 110/2000
1024/1024 - 0s - loss: 4.8698e-05 - val_loss: 7.0721e-05
Epoch 111/2000
1024/1024 - 0s - loss: 4.6297e-05 - val_loss: 6.9988e-05
Epoch 112/2000
1024/1024 - 0s - loss: 5.6806e-05 - val_loss: 6.8987e-05
Epoch 113/2000
1024/1024 - 0s - loss: 1.0868e-04 - val_loss: 7.6304e-05
Epoch 114/2000
1024/1024 - 0s - loss: 6.3215e-05 - val_loss: 6.5422e-05
Epoch 115/2000
1024/1024 - 0s - loss: 4.5973e-05 - val_loss: 6.8593e-05
Epoch 116/2000
1024/1024 - 0s - loss: 4.1169e-05 - val_loss: 6.8307e-05
Epoch 117/2000
1024/1024 - 0s - loss: 3.9373e-05 - val_loss: 6.6871e-05
Epoch 118/2000
1024/1024 - 0s - loss: 3.8261e-05 - val_loss: 6.6106e-05
Epoch 119/2000
1024/1024 - 0s - loss: 3.7493e-05 - val_loss: 6.5461e-05
Epoch 120/2000
1024/1024 - 0s - loss: 3.7795e-05 - val_loss: 6.4162e-05
Epoch 121/2000
1024/1024 - 0s - loss: 5.0367e-05 - val_loss: 6.0243e-05
Epoch 122/2000
1024/1024 - 0s - loss: 6.8871e-05 - val_loss: 6.2100e-05
Epoch 123/2000
1024/1024 - 0s - loss: 8.1166e-05 - val_loss: 5.7893e-05
Epoch 124/2000
1024/1024 - 0s - loss: 4.8841e-05 - val_loss: 6.2017e-05
Epoch 125/2000
1024/1024 - 0s - loss: 5.1063e-05 - val_loss: 6.5445e-05
Epoch 126/2000
1024/1024 - 0s - loss: 5.9218e-05 - val_loss: 5.8952e-05
Epoch 127/2000
1024/1024 - 0s - loss: 7.1668e-05 - val_loss: 5.2648e-05
Epoch 128/2000
1024/1024 - 0s - loss: 9.2475e-05 - val_loss: 6.1971e-05
Epoch 129/2000
1024/1024 - 0s - loss: 7.9717e-05 - val_loss: 5.0408e-05
Epoch 130/2000
1024/1024 - 0s - loss: 5.8628e-05 - val_loss: 5.8633e-05
Epoch 131/2000
1024/1024 - 0s - loss: 1.5751e-04 - val_loss: 4.5323e-05
Epoch 132/2000
1024/1024 - 0s - loss: 5.3058e-04 - val_loss: 6.0224e-05
Epoch 133/2000
1024/1024 - 0s - loss: 1.4105e-04 - val_loss: 3.9857e-05
Epoch 134/2000
1024/1024 - 0s - loss: 6.1967e-05 - val_loss: 4.6437e-05
Epoch 135/2000
1024/1024 - 0s - loss: 3.8494e-05 - val_loss: 4.5657e-05
Epoch 136/2000
1024/1024 - 0s - loss: 2.7076e-05 - val_loss: 4.5093e-05
Epoch 137/2000
1024/1024 - 0s - loss: 2.5494e-05 - val_loss: 4.6935e-05
Epoch 138/2000
1024/1024 - 0s - loss: 2.4654e-05 - val_loss: 4.6657e-05
Epoch 139/2000
1024/1024 - 0s - loss: 2.4029e-05 - val_loss: 4.7064e-05
Epoch 140/2000
1024/1024 - 0s - loss: 2.3541e-05 - val_loss: 4.6979e-05
Epoch 141/2000
1024/1024 - 0s - loss: 2.3134e-05 - val_loss: 4.6644e-05
Epoch 142/2000
1024/1024 - 0s - loss: 2.2764e-05 - val_loss: 4.6320e-05
Epoch 143/2000
1024/1024 - 0s - loss: 2.2448e-05 - val_loss: 4.5502e-05
Epoch 144/2000
1024/1024 - 0s - loss: 2.2121e-05 - val_loss: 4.4768e-05
Epoch 145/2000
1024/1024 - 0s - loss: 2.1811e-05 - val_loss: 4.4003e-05
Epoch 146/2000
1024/1024 - 0s - loss: 2.1498e-05 - val_loss: 4.2931e-05
Epoch 147/2000
1024/1024 - 0s - loss: 2.1189e-05 - val_loss: 4.1867e-05
Epoch 148/2000
1024/1024 - 0s - loss: 2.0830e-05 - val_loss: 4.0682e-05
Epoch 149/2000
1024/1024 - 0s - loss: 2.0432e-05 - val_loss: 3.9641e-05
Epoch 150/2000
1024/1024 - 0s - loss: 2.0046e-05 - val_loss: 3.8285e-05
Epoch 151/2000
1024/1024 - 0s - loss: 1.9594e-05 - val_loss: 3.7098e-05
Epoch 152/2000
1024/1024 - 0s - loss: 1.9180e-05 - val_loss: 3.5559e-05
Epoch 153/2000
1024/1024 - 0s - loss: 1.8675e-05 - val_loss: 3.4806e-05
Epoch 154/2000
1024/1024 - 0s - loss: 1.8339e-05 - val_loss: 3.4167e-05
Epoch 155/2000
1024/1024 - 0s - loss: 1.8078e-05 - val_loss: 3.3827e-05
Epoch 156/2000
1024/1024 - 0s - loss: 1.7916e-05 - val_loss: 3.4044e-05
Epoch 157/2000
1024/1024 - 0s - loss: 1.7717e-05 - val_loss: 3.4455e-05
Epoch 158/2000
1024/1024 - 0s - loss: 1.7578e-05 - val_loss: 3.4608e-05
Epoch 159/2000
1024/1024 - 0s - loss: 1.7402e-05 - val_loss: 3.4594e-05
Epoch 160/2000
1024/1024 - 0s - loss: 1.7251e-05 - val_loss: 3.4739e-05
Epoch 161/2000
1024/1024 - 0s - loss: 1.7132e-05 - val_loss: 3.4868e-05
Epoch 162/2000
1024/1024 - 0s - loss: 1.6977e-05 - val_loss: 3.4650e-05
Epoch 163/2000
1024/1024 - 0s - loss: 1.6824e-05 - val_loss: 3.5002e-05
Epoch 164/2000
1024/1024 - 0s - loss: 1.6707e-05 - val_loss: 3.4871e-05
Epoch 165/2000
1024/1024 - 0s - loss: 1.6614e-05 - val_loss: 3.5215e-05
Epoch 166/2000
1024/1024 - 0s - loss: 1.6411e-05 - val_loss: 3.5092e-05
Epoch 167/2000
1024/1024 - 0s - loss: 1.6242e-05 - val_loss: 3.5159e-05
Epoch 168/2000
1024/1024 - 0s - loss: 1.6108e-05 - val_loss: 3.5236e-05
Epoch 169/2000
1024/1024 - 0s - loss: 1.5967e-05 - val_loss: 3.5139e-05
Epoch 170/2000
1024/1024 - 0s - loss: 1.5845e-05 - val_loss: 3.5499e-05
Epoch 171/2000
1024/1024 - 0s - loss: 1.5795e-05 - val_loss: 3.5195e-05
Epoch 172/2000
1024/1024 - 0s - loss: 1.5599e-05 - val_loss: 3.5376e-05
Epoch 173/2000
1024/1024 - 0s - loss: 1.5453e-05 - val_loss: 3.5222e-05
Epoch 174/2000
1024/1024 - 0s - loss: 1.5384e-05 - val_loss: 3.5563e-05
Epoch 175/2000
1024/1024 - 0s - loss: 1.5231e-05 - val_loss: 3.5746e-05
Epoch 176/2000
1024/1024 - 0s - loss: 1.5253e-05 - val_loss: 3.5535e-05
Epoch 177/2000
1024/1024 - 0s - loss: 1.5238e-05 - val_loss: 3.5400e-05
Epoch 178/2000
1024/1024 - 0s - loss: 1.4965e-05 - val_loss: 3.6021e-05
Epoch 179/2000
1024/1024 - 0s - loss: 1.4913e-05 - val_loss: 3.5984e-05
Epoch 180/2000
1024/1024 - 0s - loss: 1.4687e-05 - val_loss: 3.5828e-05
Epoch 181/2000
1024/1024 - 0s - loss: 1.4436e-05 - val_loss: 3.5834e-05
Epoch 182/2000
1024/1024 - 0s - loss: 1.4315e-05 - val_loss: 3.5805e-05
Epoch 183/2000
1024/1024 - 0s - loss: 1.4208e-05 - val_loss: 3.5771e-05
Epoch 184/2000
1024/1024 - 0s - loss: 1.4084e-05 - val_loss: 3.6140e-05
Epoch 185/2000
1024/1024 - 0s - loss: 1.4261e-05 - val_loss: 3.6000e-05
Epoch 186/2000
1024/1024 - 0s - loss: 1.3962e-05 - val_loss: 3.6176e-05
Epoch 187/2000
1024/1024 - 0s - loss: 1.7056e-05 - val_loss: 3.6121e-05
Epoch 188/2000
1024/1024 - 0s - loss: 1.8470e-05 - val_loss: 3.5212e-05
Epoch 189/2000
1024/1024 - 0s - loss: 3.3650e-05 - val_loss: 3.7878e-05
Epoch 190/2000
1024/1024 - 0s - loss: 1.2721e-04 - val_loss: 4.7818e-05
Epoch 191/2000
1024/1024 - 0s - loss: 3.2227e-04 - val_loss: 3.3318e-05
Epoch 192/2000
1024/1024 - 0s - loss: 7.7222e-05 - val_loss: 3.0685e-05
Epoch 193/2000
1024/1024 - 0s - loss: 3.3315e-05 - val_loss: 3.5536e-05
Epoch 194/2000
1024/1024 - 0s - loss: 1.8314e-05 - val_loss: 3.4965e-05
Epoch 195/2000
1024/1024 - 0s - loss: 2.6532e-05 - val_loss: 3.4735e-05
Epoch 196/2000
1024/1024 - 0s - loss: 1.9069e-05 - val_loss: 3.5971e-05
Epoch 197/2000
1024/1024 - 0s - loss: 1.7416e-05 - val_loss: 3.5744e-05
Epoch 198/2000
1024/1024 - 0s - loss: 2.5102e-05 - val_loss: 3.8118e-05
Epoch 199/2000
1024/1024 - 0s - loss: 1.8043e-05 - val_loss: 3.6592e-05
Epoch 200/2000
1024/1024 - 0s - loss: 1.4205e-05 - val_loss: 3.7251e-05
Epoch 201/2000
1024/1024 - 0s - loss: 1.5872e-05 - val_loss: 3.6116e-05
Epoch 202/2000
1024/1024 - 0s - loss: 1.4387e-05 - val_loss: 3.6108e-05
Epoch 203/2000
1024/1024 - 0s - loss: 1.2619e-05 - val_loss: 3.6345e-05
Epoch 204/2000
1024/1024 - 0s - loss: 1.2422e-05 - val_loss: 3.6428e-05
Epoch 205/2000
1024/1024 - 0s - loss: 1.2254e-05 - val_loss: 3.6259e-05
Epoch 206/2000
1024/1024 - 0s - loss: 1.2139e-05 - val_loss: 3.6244e-05
Epoch 207/2000
1024/1024 - 0s - loss: 1.2033e-05 - val_loss: 3.6424e-05
Epoch 208/2000
1024/1024 - 0s - loss: 1.1913e-05 - val_loss: 3.6348e-05
Epoch 209/2000
1024/1024 - 0s - loss: 1.1812e-05 - val_loss: 3.6266e-05
Epoch 210/2000
1024/1024 - 0s - loss: 1.1715e-05 - val_loss: 3.6267e-05
Epoch 211/2000
1024/1024 - 0s - loss: 1.1634e-05 - val_loss: 3.6256e-05
Epoch 212/2000
1024/1024 - 0s - loss: 1.1624e-05 - val_loss: 3.6166e-05
Epoch 213/2000
1024/1024 - 0s - loss: 1.1589e-05 - val_loss: 3.6397e-05
Epoch 214/2000
1024/1024 - 0s - loss: 1.1495e-05 - val_loss: 3.6418e-05
Epoch 215/2000
1024/1024 - 0s - loss: 1.1327e-05 - val_loss: 3.6161e-05
Epoch 216/2000
1024/1024 - 0s - loss: 1.1175e-05 - val_loss: 3.6150e-05
Epoch 217/2000
1024/1024 - 0s - loss: 1.1047e-05 - val_loss: 3.6249e-05
Epoch 218/2000
1024/1024 - 0s - loss: 1.1064e-05 - val_loss: 3.6195e-05
Epoch 219/2000
1024/1024 - 0s - loss: 1.1032e-05 - val_loss: 3.5979e-05
Epoch 220/2000
1024/1024 - 0s - loss: 1.2907e-05 - val_loss: 3.6253e-05
Epoch 221/2000
1024/1024 - 0s - loss: 1.3589e-05 - val_loss: 3.6308e-05
Epoch 222/2000
1024/1024 - 0s - loss: 1.0685e-05 - val_loss: 3.6213e-05
Epoch 223/2000
1024/1024 - 0s - loss: 1.1240e-05 - val_loss: 3.5980e-05
Epoch 224/2000
1024/1024 - 0s - loss: 1.7366e-05 - val_loss: 3.3585e-05
Epoch 225/2000
1024/1024 - 0s - loss: 1.7036e-05 - val_loss: 3.6116e-05
Epoch 226/2000
1024/1024 - 0s - loss: 1.9594e-05 - val_loss: 3.5946e-05
Epoch 227/2000
1024/1024 - 0s - loss: 5.7167e-05 - val_loss: 3.0613e-05
Epoch 228/2000
1024/1024 - 0s - loss: 8.5248e-05 - val_loss: 3.2703e-05
Epoch 229/2000
1024/1024 - 0s - loss: 2.6269e-05 - val_loss: 3.5253e-05
Epoch 230/2000
1024/1024 - 0s - loss: 5.2904e-05 - val_loss: 3.3344e-05
Epoch 231/2000
1024/1024 - 0s - loss: 1.2119e-04 - val_loss: 3.6798e-05
Epoch 232/2000
1024/1024 - 0s - loss: 3.3936e-05 - val_loss: 3.4388e-05
Epoch 233/2000
1024/1024 - 0s - loss: 2.9561e-05 - val_loss: 3.7847e-05
Epoch 234/2000
1024/1024 - 0s - loss: 1.7543e-05 - val_loss: 3.5168e-05
Epoch 235/2000
1024/1024 - 0s - loss: 1.0624e-05 - val_loss: 3.6208e-05
Epoch 236/2000
1024/1024 - 0s - loss: 9.9872e-06 - val_loss: 3.5513e-05
Epoch 237/2000
1024/1024 - 0s - loss: 9.6278e-06 - val_loss: 3.5882e-05
Epoch 238/2000
1024/1024 - 0s - loss: 9.6073e-06 - val_loss: 3.5421e-05
Epoch 239/2000
1024/1024 - 0s - loss: 9.4481e-06 - val_loss: 3.5508e-05
Epoch 240/2000
1024/1024 - 0s - loss: 9.5982e-06 - val_loss: 3.5424e-05
Epoch 241/2000
1024/1024 - 0s - loss: 9.8670e-06 - val_loss: 3.4742e-05
Epoch 242/2000
1024/1024 - 0s - loss: 1.1150e-05 - val_loss: 3.4800e-05
Epoch 243/2000
1024/1024 - 0s - loss: 9.5140e-06 - val_loss: 3.5354e-05
Epoch 244/2000
1024/1024 - 0s - loss: 8.9816e-06 - val_loss: 3.4978e-05
Epoch 245/2000
1024/1024 - 0s - loss: 8.8598e-06 - val_loss: 3.4809e-05
Epoch 246/2000
1024/1024 - 0s - loss: 8.7917e-06 - val_loss: 3.4734e-05
Epoch 247/2000
1024/1024 - 0s - loss: 8.6963e-06 - val_loss: 3.4738e-05
Epoch 248/2000
1024/1024 - 0s - loss: 8.6100e-06 - val_loss: 3.4629e-05
Epoch 249/2000
1024/1024 - 0s - loss: 8.5405e-06 - val_loss: 3.4324e-05
Epoch 250/2000
1024/1024 - 0s - loss: 8.4410e-06 - val_loss: 3.4376e-05
Epoch 251/2000
1024/1024 - 0s - loss: 8.3639e-06 - val_loss: 3.4346e-05
Epoch 252/2000
1024/1024 - 0s - loss: 8.2832e-06 - val_loss: 3.3886e-05
Epoch 253/2000
1024/1024 - 0s - loss: 8.4023e-06 - val_loss: 3.3985e-05
Epoch 254/2000
1024/1024 - 0s - loss: 8.1241e-06 - val_loss: 3.3922e-05
Epoch 255/2000
1024/1024 - 0s - loss: 1.0202e-05 - val_loss: 3.4946e-05
Epoch 256/2000
1024/1024 - 0s - loss: 1.0948e-05 - val_loss: 3.1968e-05
Epoch 257/2000
1024/1024 - 0s - loss: 1.7116e-04 - val_loss: 3.0069e-05
Epoch 258/2000
1024/1024 - 0s - loss: 2.3343e-04 - val_loss: 3.8070e-05
Epoch 259/2000
1024/1024 - 0s - loss: 9.3399e-05 - val_loss: 3.3367e-05
Epoch 260/2000
1024/1024 - 0s - loss: 3.5962e-05 - val_loss: 2.0840e-05
Epoch 261/2000
1024/1024 - 0s - loss: 1.5650e-05 - val_loss: 2.6657e-05
Epoch 262/2000
1024/1024 - 0s - loss: 9.7794e-06 - val_loss: 3.0743e-05
Epoch 263/2000
1024/1024 - 0s - loss: 8.3350e-06 - val_loss: 3.2656e-05
Epoch 264/2000
1024/1024 - 0s - loss: 7.8692e-06 - val_loss: 3.3536e-05
Epoch 265/2000
1024/1024 - 0s - loss: 7.7307e-06 - val_loss: 3.3442e-05
Epoch 266/2000
1024/1024 - 0s - loss: 7.6449e-06 - val_loss: 3.3000e-05
Epoch 267/2000
1024/1024 - 0s - loss: 7.5313e-06 - val_loss: 3.3032e-05
Epoch 268/2000
1024/1024 - 0s - loss: 7.4303e-06 - val_loss: 3.2658e-05
Epoch 269/2000
1024/1024 - 0s - loss: 7.3743e-06 - val_loss: 3.2655e-05
Epoch 270/2000
1024/1024 - 0s - loss: 7.2620e-06 - val_loss: 3.2367e-05
Epoch 271/2000
1024/1024 - 0s - loss: 7.1704e-06 - val_loss: 3.2103e-05
Epoch 272/2000
1024/1024 - 0s - loss: 7.0999e-06 - val_loss: 3.2063e-05
Epoch 273/2000
1024/1024 - 0s - loss: 6.9992e-06 - val_loss: 3.1847e-05
Epoch 274/2000
1024/1024 - 0s - loss: 6.9643e-06 - val_loss: 3.1658e-05
Epoch 275/2000
1024/1024 - 0s - loss: 6.8513e-06 - val_loss: 3.1441e-05
Epoch 276/2000
1024/1024 - 0s - loss: 6.7510e-06 - val_loss: 3.1326e-05
Epoch 277/2000
1024/1024 - 0s - loss: 6.7032e-06 - val_loss: 3.1117e-05
Epoch 278/2000
1024/1024 - 0s - loss: 6.5772e-06 - val_loss: 3.0995e-05
Epoch 279/2000
1024/1024 - 0s - loss: 6.4895e-06 - val_loss: 3.0666e-05
Epoch 280/2000
1024/1024 - 0s - loss: 6.4029e-06 - val_loss: 3.0410e-05
Epoch 281/2000
1024/1024 - 0s - loss: 6.3077e-06 - val_loss: 3.0160e-05
Epoch 282/2000
1024/1024 - 0s - loss: 6.2266e-06 - val_loss: 2.9880e-05
Epoch 283/2000
1024/1024 - 0s - loss: 6.1406e-06 - val_loss: 2.9434e-05
Epoch 284/2000
1024/1024 - 0s - loss: 6.0586e-06 - val_loss: 3.0722e-05
Epoch 285/2000
1024/1024 - 0s - loss: 5.9676e-06 - val_loss: 3.0072e-05
Epoch 286/2000
1024/1024 - 0s - loss: 5.8927e-06 - val_loss: 2.9026e-05
Epoch 287/2000
1024/1024 - 0s - loss: 5.8354e-06 - val_loss: 2.9159e-05
Epoch 288/2000
1024/1024 - 0s - loss: 5.7321e-06 - val_loss: 2.9009e-05
Epoch 289/2000
1024/1024 - 0s - loss: 5.6350e-06 - val_loss: 2.8558e-05
Epoch 290/2000
1024/1024 - 0s - loss: 5.6043e-06 - val_loss: 2.8858e-05
Epoch 291/2000
1024/1024 - 0s - loss: 4.0439e-05 - val_loss: 2.7130e-05
Epoch 292/2000
1024/1024 - 0s - loss: 3.7262e-05 - val_loss: 2.6740e-05
Epoch 293/2000
1024/1024 - 0s - loss: 1.7677e-04 - val_loss: 3.1674e-05
Epoch 294/2000
1024/1024 - 0s - loss: 2.2924e-04 - val_loss: 2.1629e-05
Epoch 295/2000
1024/1024 - 0s - loss: 4.7888e-04 - val_loss: 2.3328e-05
Epoch 296/2000
1024/1024 - 0s - loss: 1.4863e-04 - val_loss: 2.2734e-05
Epoch 297/2000
1024/1024 - 0s - loss: 5.1624e-05 - val_loss: 2.8907e-05
Epoch 298/2000
1024/1024 - 0s - loss: 1.8134e-05 - val_loss: 2.1298e-05
Epoch 299/2000
1024/1024 - 0s - loss: 9.1365e-06 - val_loss: 2.4670e-05
Epoch 300/2000
1024/1024 - 0s - loss: 6.9797e-06 - val_loss: 2.7348e-05
Epoch 301/2000
1024/1024 - 0s - loss: 6.3502e-06 - val_loss: 2.8400e-05
Epoch 302/2000
1024/1024 - 0s - loss: 6.2443e-06 - val_loss: 2.5952e-05
Epoch 303/2000
1024/1024 - 0s - loss: 6.1271e-06 - val_loss: 2.5997e-05
Epoch 304/2000
1024/1024 - 0s - loss: 5.9331e-06 - val_loss: 2.6818e-05
Epoch 305/2000
1024/1024 - 0s - loss: 5.8890e-06 - val_loss: 2.6190e-05
Epoch 306/2000
1024/1024 - 0s - loss: 5.8734e-06 - val_loss: 2.5745e-05
Epoch 307/2000
1024/1024 - 0s - loss: 5.7002e-06 - val_loss: 2.5823e-05
Epoch 308/2000
1024/1024 - 0s - loss: 5.5616e-06 - val_loss: 2.5529e-05
Epoch 309/2000
1024/1024 - 0s - loss: 5.4526e-06 - val_loss: 2.5982e-05
Epoch 310/2000
1024/1024 - 0s - loss: 5.3740e-06 - val_loss: 2.5222e-05
Epoch 311/2000
1024/1024 - 0s - loss: 5.2793e-06 - val_loss: 2.5412e-05
Epoch 312/2000
1024/1024 - 0s - loss: 5.1835e-06 - val_loss: 2.5163e-05
Epoch 313/2000
1024/1024 - 0s - loss: 5.1538e-06 - val_loss: 2.7378e-05
Epoch 314/2000
1024/1024 - 0s - loss: 5.5668e-06 - val_loss: 2.0732e-05
Epoch 315/2000
1024/1024 - 0s - loss: 5.1375e-06 - val_loss: 2.2234e-05
Epoch 316/2000
1024/1024 - 0s - loss: 4.9229e-06 - val_loss: 2.3467e-05
Epoch 317/2000
1024/1024 - 0s - loss: 3985.6865 - val_loss: 3.3298e-04
Epoch 318/2000
1024/1024 - 0s - loss: 0.3620 - val_loss: 0.0517
Epoch 319/2000
1024/1024 - 0s - loss: 0.4507 - val_loss: 0.1538
Epoch 320/2000
1024/1024 - 0s - loss: 0.2039 - val_loss: 0.1813
Epoch 321/2000
1024/1024 - 0s - loss: 0.2045 - val_loss: 0.1713
Epoch 322/2000
1024/1024 - 0s - loss: 0.2031 - val_loss: 0.1645
Epoch 323/2000
1024/1024 - 0s - loss: 0.2026 - val_loss: 0.1626
Epoch 324/2000
1024/1024 - 0s - loss: 0.2024 - val_loss: 0.1618
Epoch 325/2000
1024/1024 - 0s - loss: 0.2023 - val_loss: 0.1613
Epoch 326/2000
1024/1024 - 0s - loss: 0.2022 - val_loss: 0.1610
Epoch 327/2000
1024/1024 - 0s - loss: 0.2022 - val_loss: 0.1607
Epoch 328/2000
1024/1024 - 0s - loss: 0.2021 - val_loss: 0.1604
Epoch 329/2000
1024/1024 - 0s - loss: 0.2020 - val_loss: 0.1602
Epoch 330/2000
1024/1024 - 0s - loss: 0.2019 - val_loss: 0.1599
Epoch 331/2000
1024/1024 - 0s - loss: 0.2019 - val_loss: 0.1597
Epoch 332/2000
1024/1024 - 0s - loss: 0.2018 - val_loss: 0.1595
Epoch 333/2000
1024/1024 - 0s - loss: 0.2017 - val_loss: 0.1593
Epoch 334/2000
1024/1024 - 0s - loss: 0.2017 - val_loss: 0.1592
Epoch 335/2000
1024/1024 - 0s - loss: 0.2016 - val_loss: 0.1590
Epoch 336/2000
1024/1024 - 0s - loss: 0.2016 - val_loss: 0.1588
Epoch 337/2000
1024/1024 - 0s - loss: 0.2015 - val_loss: 0.1587
Epoch 338/2000
1024/1024 - 0s - loss: 0.2014 - val_loss: 0.1585
Epoch 339/2000
1024/1024 - 0s - loss: 0.2014 - val_loss: 0.1583
Epoch 340/2000
1024/1024 - 0s - loss: 0.2013 - val_loss: 0.1582
Epoch 341/2000
1024/1024 - 0s - loss: 0.2012 - val_loss: 0.1581
Epoch 342/2000
1024/1024 - 0s - loss: 0.2012 - val_loss: 0.1579
Epoch 343/2000
1024/1024 - 0s - loss: 0.2011 - val_loss: 0.1578
Epoch 344/2000
1024/1024 - 0s - loss: 0.2011 - val_loss: 0.1577
Epoch 345/2000
1024/1024 - 0s - loss: 0.2010 - val_loss: 0.1575
Epoch 346/2000
1024/1024 - 0s - loss: 0.2009 - val_loss: 0.1574
Epoch 347/2000
1024/1024 - 0s - loss: 0.2009 - val_loss: 0.1573
Epoch 348/2000
1024/1024 - 0s - loss: 0.2008 - val_loss: 0.1572
Epoch 349/2000
1024/1024 - 0s - loss: 0.2008 - val_loss: 0.1570
Epoch 350/2000
1024/1024 - 0s - loss: 0.2007 - val_loss: 0.1569
Epoch 351/2000
1024/1024 - 0s - loss: 0.2007 - val_loss: 0.1568
Epoch 352/2000
1024/1024 - 0s - loss: 0.2006 - val_loss: 0.1567
Epoch 353/2000
1024/1024 - 0s - loss: 0.2005 - val_loss: 0.1566
Epoch 354/2000
1024/1024 - 0s - loss: 0.2005 - val_loss: 0.1565
Epoch 355/2000
1024/1024 - 0s - loss: 0.2004 - val_loss: 0.1564
Epoch 356/2000
1024/1024 - 0s - loss: 0.2004 - val_loss: 0.1563
Epoch 357/2000
1024/1024 - 0s - loss: 0.2003 - val_loss: 0.1562
Epoch 358/2000
1024/1024 - 0s - loss: 0.2003 - val_loss: 0.1561
Epoch 359/2000
1024/1024 - 0s - loss: 0.2002 - val_loss: 0.1560
Epoch 360/2000
1024/1024 - 0s - loss: 0.2002 - val_loss: 0.1560
Epoch 361/2000
1024/1024 - 0s - loss: 0.2001 - val_loss: 0.1559
Epoch 362/2000
1024/1024 - 0s - loss: 0.2001 - val_loss: 0.1558
Epoch 363/2000
1024/1024 - 0s - loss: 0.2000 - val_loss: 0.1557
Epoch 364/2000
1024/1024 - 0s - loss: 0.2000 - val_loss: 0.1556
Epoch 365/2000
1024/1024 - 0s - loss: 0.1999 - val_loss: 0.1556
Epoch 366/2000
1024/1024 - 0s - loss: 0.1999 - val_loss: 0.1555
Epoch 367/2000
1024/1024 - 0s - loss: 0.1998 - val_loss: 0.1554
Epoch 368/2000
1024/1024 - 0s - loss: 0.1998 - val_loss: 0.1554
Epoch 369/2000
1024/1024 - 0s - loss: 0.1997 - val_loss: 0.1553
Epoch 370/2000
1024/1024 - 0s - loss: 0.1997 - val_loss: 0.1552
Epoch 371/2000
1024/1024 - 0s - loss: 0.1996 - val_loss: 0.1552
Epoch 372/2000
1024/1024 - 0s - loss: 0.1996 - val_loss: 0.1551
Epoch 373/2000
1024/1024 - 0s - loss: 0.1995 - val_loss: 0.1551
Epoch 374/2000
1024/1024 - 0s - loss: 0.1995 - val_loss: 0.1550
Epoch 375/2000
1024/1024 - 0s - loss: 0.1994 - val_loss: 0.1550
Epoch 376/2000
1024/1024 - 0s - loss: 0.1994 - val_loss: 0.1549
Epoch 377/2000
1024/1024 - 0s - loss: 0.1994 - val_loss: 0.1549
Epoch 378/2000
1024/1024 - 0s - loss: 0.1993 - val_loss: 0.1548
Epoch 379/2000
1024/1024 - 0s - loss: 0.1993 - val_loss: 0.1548
Epoch 380/2000
1024/1024 - 0s - loss: 0.1992 - val_loss: 0.1547
Epoch 381/2000
1024/1024 - 0s - loss: 0.1992 - val_loss: 0.1547
Epoch 382/2000
1024/1024 - 0s - loss: 0.1991 - val_loss: 0.1546
Epoch 383/2000
1024/1024 - 0s - loss: 0.1991 - val_loss: 0.1546
Epoch 384/2000
1024/1024 - 0s - loss: 0.1991 - val_loss: 0.1545
Epoch 385/2000
1024/1024 - 0s - loss: 0.1990 - val_loss: 0.1545
Epoch 386/2000
1024/1024 - 0s - loss: 0.1990 - val_loss: 0.1545
Epoch 387/2000
1024/1024 - 0s - loss: 0.1989 - val_loss: 0.1544
Epoch 388/2000
1024/1024 - 0s - loss: 0.1989 - val_loss: 0.1544
Epoch 389/2000
1024/1024 - 0s - loss: 0.1989 - val_loss: 0.1543
Epoch 390/2000
1024/1024 - 0s - loss: 0.1988 - val_loss: 0.1543
Epoch 391/2000
1024/1024 - 0s - loss: 0.1988 - val_loss: 0.1543
Epoch 392/2000
1024/1024 - 0s - loss: 0.1987 - val_loss: 0.1542
Epoch 393/2000
1024/1024 - 0s - loss: 0.1987 - val_loss: 0.1542
Epoch 394/2000
1024/1024 - 0s - loss: 0.1987 - val_loss: 0.1542
Epoch 395/2000
1024/1024 - 0s - loss: 0.1986 - val_loss: 0.1541
Epoch 396/2000
1024/1024 - 0s - loss: 0.1986 - val_loss: 0.1541
Epoch 397/2000
1024/1024 - 0s - loss: 0.1985 - val_loss: 0.1541
Epoch 398/2000
1024/1024 - 0s - loss: 0.1985 - val_loss: 0.1540
Epoch 399/2000
1024/1024 - 0s - loss: 0.1985 - val_loss: 0.1540
Epoch 400/2000
1024/1024 - 0s - loss: 0.1984 - val_loss: 0.1539
Epoch 401/2000
1024/1024 - 0s - loss: 0.1984 - val_loss: 0.1539
Epoch 402/2000
1024/1024 - 0s - loss: 0.1983 - val_loss: 0.1539
Epoch 403/2000
1024/1024 - 0s - loss: 0.1983 - val_loss: 0.1539
Epoch 404/2000
1024/1024 - 0s - loss: 0.1983 - val_loss: 0.1538
Epoch 405/2000
1024/1024 - 0s - loss: 0.1982 - val_loss: 0.1538
Epoch 406/2000
1024/1024 - 0s - loss: 0.1982 - val_loss: 0.1538
Epoch 407/2000
1024/1024 - 0s - loss: 0.1981 - val_loss: 0.1537
Epoch 408/2000
1024/1024 - 0s - loss: 0.1981 - val_loss: 0.1537
Epoch 409/2000
1024/1024 - 0s - loss: 0.1981 - val_loss: 0.1537
Epoch 410/2000
1024/1024 - 0s - loss: 0.1980 - val_loss: 0.1536
Epoch 411/2000
1024/1024 - 0s - loss: 0.1980 - val_loss: 0.1536
Epoch 412/2000
1024/1024 - 0s - loss: 0.1979 - val_loss: 0.1536
Epoch 413/2000
1024/1024 - 0s - loss: 0.1979 - val_loss: 0.1535
Epoch 414/2000
1024/1024 - 0s - loss: 0.1979 - val_loss: 0.1535
Epoch 415/2000
1024/1024 - 0s - loss: 0.1978 - val_loss: 0.1535
Epoch 416/2000
1024/1024 - 0s - loss: 0.1978 - val_loss: 0.1534
Epoch 417/2000
1024/1024 - 0s - loss: 0.1977 - val_loss: 0.1534
Epoch 418/2000
1024/1024 - 0s - loss: 0.1977 - val_loss: 0.1534
Epoch 419/2000
1024/1024 - 0s - loss: 0.1976 - val_loss: 0.1533
Epoch 420/2000
1024/1024 - 0s - loss: 0.1976 - val_loss: 0.1533
Epoch 421/2000
1024/1024 - 0s - loss: 0.1976 - val_loss: 0.1533
Epoch 422/2000
1024/1024 - 0s - loss: 0.1975 - val_loss: 0.1532
Epoch 423/2000
1024/1024 - 0s - loss: 0.1975 - val_loss: 0.1532
Epoch 424/2000
1024/1024 - 0s - loss: 0.1974 - val_loss: 0.1532
Epoch 425/2000
1024/1024 - 0s - loss: 0.1974 - val_loss: 0.1531
Epoch 426/2000
1024/1024 - 0s - loss: 0.1973 - val_loss: 0.1531
Epoch 427/2000
1024/1024 - 0s - loss: 0.1973 - val_loss: 0.1531
Epoch 428/2000
1024/1024 - 0s - loss: 0.1973 - val_loss: 0.1530
Epoch 429/2000
1024/1024 - 0s - loss: 0.1972 - val_loss: 0.1530
Epoch 430/2000
1024/1024 - 0s - loss: 0.1972 - val_loss: 0.1530
Epoch 431/2000
1024/1024 - 0s - loss: 0.1971 - val_loss: 0.1529
Epoch 432/2000
1024/1024 - 0s - loss: 0.1971 - val_loss: 0.1529
Epoch 433/2000
1024/1024 - 0s - loss: 0.1970 - val_loss: 0.1529
Epoch 434/2000
1024/1024 - 0s - loss: 0.1970 - val_loss: 0.1529
Epoch 435/2000
1024/1024 - 0s - loss: 0.1969 - val_loss: 0.1528
Epoch 436/2000
1024/1024 - 0s - loss: 0.1969 - val_loss: 0.1528
Epoch 437/2000
1024/1024 - 0s - loss: 0.1969 - val_loss: 0.1528
Epoch 438/2000
1024/1024 - 0s - loss: 0.1968 - val_loss: 0.1527
Epoch 439/2000
1024/1024 - 0s - loss: 0.1968 - val_loss: 0.1527
Epoch 440/2000
1024/1024 - 0s - loss: 0.1967 - val_loss: 0.1526
Epoch 441/2000
1024/1024 - 0s - loss: 0.1967 - val_loss: 0.1526
Epoch 442/2000
1024/1024 - 0s - loss: 0.1966 - val_loss: 0.1526
Epoch 443/2000
1024/1024 - 0s - loss: 0.1966 - val_loss: 0.1525
Epoch 444/2000
1024/1024 - 0s - loss: 0.1965 - val_loss: 0.1525
Epoch 445/2000
1024/1024 - 0s - loss: 0.1965 - val_loss: 0.1525
Epoch 446/2000
1024/1024 - 0s - loss: 0.1964 - val_loss: 0.1524
Epoch 447/2000
1024/1024 - 0s - loss: 0.1964 - val_loss: 0.1524
Epoch 448/2000
1024/1024 - 0s - loss: 0.1963 - val_loss: 0.1524
Epoch 449/2000
1024/1024 - 0s - loss: 0.1963 - val_loss: 0.1523
Epoch 450/2000
1024/1024 - 0s - loss: 0.1962 - val_loss: 0.1523
Epoch 451/2000
1024/1024 - 0s - loss: 0.1962 - val_loss: 0.1523
Epoch 452/2000
1024/1024 - 0s - loss: 0.1961 - val_loss: 0.1522
Epoch 453/2000
1024/1024 - 0s - loss: 0.1961 - val_loss: 0.1522
Epoch 454/2000
1024/1024 - 0s - loss: 0.1960 - val_loss: 0.1522
Epoch 455/2000
1024/1024 - 0s - loss: 0.1960 - val_loss: 0.1521
Epoch 456/2000
1024/1024 - 0s - loss: 0.1959 - val_loss: 0.1521
Epoch 457/2000
1024/1024 - 0s - loss: 0.1959 - val_loss: 0.1520
Epoch 458/2000
1024/1024 - 0s - loss: 0.1958 - val_loss: 0.1520
Epoch 459/2000
1024/1024 - 0s - loss: 0.1958 - val_loss: 0.1520
Epoch 460/2000
1024/1024 - 0s - loss: 0.1957 - val_loss: 0.1519
Epoch 461/2000
1024/1024 - 0s - loss: 0.1957 - val_loss: 0.1519
Epoch 462/2000
1024/1024 - 0s - loss: 0.1956 - val_loss: 0.1519
Epoch 463/2000
1024/1024 - 0s - loss: 0.1956 - val_loss: 0.1518
Epoch 464/2000
1024/1024 - 0s - loss: 0.1955 - val_loss: 0.1518
Epoch 465/2000
1024/1024 - 0s - loss: 0.1955 - val_loss: 0.1517
Epoch 466/2000
1024/1024 - 0s - loss: 0.1954 - val_loss: 0.1517
Epoch 467/2000
1024/1024 - 0s - loss: 0.1954 - val_loss: 0.1517
Epoch 468/2000
1024/1024 - 0s - loss: 0.1953 - val_loss: 0.1516
Epoch 469/2000
1024/1024 - 0s - loss: 0.1952 - val_loss: 0.1516
Epoch 470/2000
1024/1024 - 0s - loss: 0.1952 - val_loss: 0.1515
Epoch 471/2000
1024/1024 - 0s - loss: 0.1951 - val_loss: 0.1515
Epoch 472/2000
1024/1024 - 0s - loss: 0.1951 - val_loss: 0.1514
Epoch 473/2000
1024/1024 - 0s - loss: 0.1950 - val_loss: 0.1514
Epoch 474/2000
1024/1024 - 0s - loss: 0.1950 - val_loss: 0.1514
Epoch 475/2000
1024/1024 - 0s - loss: 0.1949 - val_loss: 0.1513
Epoch 476/2000
1024/1024 - 0s - loss: 0.1948 - val_loss: 0.1513
Epoch 477/2000
1024/1024 - 0s - loss: 0.1948 - val_loss: 0.1512
Epoch 478/2000
1024/1024 - 0s - loss: 0.1947 - val_loss: 0.1512
Epoch 479/2000
1024/1024 - 0s - loss: 0.1947 - val_loss: 0.1511
Epoch 480/2000
1024/1024 - 0s - loss: 0.1946 - val_loss: 0.1511
Epoch 481/2000
1024/1024 - 0s - loss: 0.1945 - val_loss: 0.1510
Epoch 482/2000
1024/1024 - 0s - loss: 0.1945 - val_loss: 0.1510
Epoch 483/2000
1024/1024 - 0s - loss: 0.1944 - val_loss: 0.1509
Epoch 484/2000
1024/1024 - 0s - loss: 0.1944 - val_loss: 0.1509
Epoch 485/2000
1024/1024 - 0s - loss: 0.1943 - val_loss: 0.1508
Epoch 486/2000
1024/1024 - 0s - loss: 0.1942 - val_loss: 0.1508
Epoch 487/2000
1024/1024 - 0s - loss: 0.1942 - val_loss: 0.1507
Epoch 488/2000
1024/1024 - 0s - loss: 0.1941 - val_loss: 0.1507
Epoch 489/2000
1024/1024 - 0s - loss: 0.1940 - val_loss: 0.1506
Epoch 490/2000
1024/1024 - 0s - loss: 0.1940 - val_loss: 0.1506
Epoch 491/2000
1024/1024 - 0s - loss: 0.1939 - val_loss: 0.1505
Epoch 492/2000
1024/1024 - 0s - loss: 0.1938 - val_loss: 0.1505
Epoch 493/2000
1024/1024 - 0s - loss: 0.1938 - val_loss: 0.1504
Epoch 494/2000
1024/1024 - 0s - loss: 0.1937 - val_loss: 0.1504
Epoch 495/2000
1024/1024 - 0s - loss: 0.1936 - val_loss: 0.1503
Epoch 496/2000
1024/1024 - 0s - loss: 0.1935 - val_loss: 0.1503
Epoch 497/2000
1024/1024 - 0s - loss: 0.1935 - val_loss: 0.1502
Epoch 498/2000
1024/1024 - 0s - loss: 0.1934 - val_loss: 0.1502
Epoch 499/2000
1024/1024 - 0s - loss: 0.1933 - val_loss: 0.1501
Epoch 500/2000
1024/1024 - 0s - loss: 0.1933 - val_loss: 0.1500
Epoch 501/2000
1024/1024 - 0s - loss: 0.1932 - val_loss: 0.1500
Epoch 502/2000
1024/1024 - 0s - loss: 0.1931 - val_loss: 0.1499
Epoch 503/2000
1024/1024 - 0s - loss: 0.1930 - val_loss: 0.1499
Epoch 504/2000
1024/1024 - 0s - loss: 0.1929 - val_loss: 0.1498
Epoch 505/2000
1024/1024 - 0s - loss: 0.1929 - val_loss: 0.1497
Epoch 506/2000
1024/1024 - 0s - loss: 0.1928 - val_loss: 0.1497
Epoch 507/2000
1024/1024 - 0s - loss: 0.1927 - val_loss: 0.1496
Epoch 508/2000
1024/1024 - 0s - loss: 0.1926 - val_loss: 0.1496
Epoch 509/2000
1024/1024 - 0s - loss: 0.1925 - val_loss: 0.1495
Epoch 510/2000
1024/1024 - 0s - loss: 0.1925 - val_loss: 0.1494
Epoch 511/2000
1024/1024 - 0s - loss: 0.1924 - val_loss: 0.1494
Epoch 512/2000
1024/1024 - 0s - loss: 0.1923 - val_loss: 0.1493
Epoch 513/2000
1024/1024 - 0s - loss: 0.1922 - val_loss: 0.1492
Epoch 514/2000
1024/1024 - 0s - loss: 0.1921 - val_loss: 0.1492
Epoch 515/2000
1024/1024 - 0s - loss: 0.1920 - val_loss: 0.1491
Epoch 516/2000
1024/1024 - 0s - loss: 0.1919 - val_loss: 0.1490
Epoch 517/2000
1024/1024 - 0s - loss: 0.1918 - val_loss: 0.1489
Epoch 518/2000
1024/1024 - 0s - loss: 0.1918 - val_loss: 0.1489
Epoch 519/2000
1024/1024 - 0s - loss: 0.1917 - val_loss: 0.1488
Epoch 520/2000
1024/1024 - 0s - loss: 0.1916 - val_loss: 0.1487
Epoch 521/2000
1024/1024 - 0s - loss: 0.1915 - val_loss: 0.1486
Epoch 522/2000
1024/1024 - 0s - loss: 0.1914 - val_loss: 0.1486
Epoch 523/2000
1024/1024 - 0s - loss: 0.1913 - val_loss: 0.1485
Epoch 524/2000
1024/1024 - 0s - loss: 0.1912 - val_loss: 0.1484
Epoch 525/2000
1024/1024 - 0s - loss: 0.1911 - val_loss: 0.1483
Epoch 526/2000
1024/1024 - 0s - loss: 0.1910 - val_loss: 0.1482
Epoch 527/2000
1024/1024 - 0s - loss: 0.1909 - val_loss: 0.1482
Epoch 528/2000
1024/1024 - 0s - loss: 0.1908 - val_loss: 0.1481
Epoch 529/2000
1024/1024 - 0s - loss: 0.1907 - val_loss: 0.1480
Epoch 530/2000
1024/1024 - 0s - loss: 0.1906 - val_loss: 0.1479
Epoch 531/2000
1024/1024 - 0s - loss: 0.1904 - val_loss: 0.1478
Epoch 532/2000
1024/1024 - 0s - loss: 0.1903 - val_loss: 0.1477
Epoch 533/2000
1024/1024 - 0s - loss: 0.1902 - val_loss: 0.1476
Epoch 534/2000
1024/1024 - 0s - loss: 0.1901 - val_loss: 0.1475
Epoch 535/2000
1024/1024 - 0s - loss: 0.1900 - val_loss: 0.1474
Epoch 536/2000
1024/1024 - 0s - loss: 0.1899 - val_loss: 0.1473
Epoch 537/2000
1024/1024 - 0s - loss: 0.1898 - val_loss: 0.1473
Epoch 538/2000
1024/1024 - 0s - loss: 0.1896 - val_loss: 0.1472
Epoch 539/2000
1024/1024 - 0s - loss: 0.1895 - val_loss: 0.1471
Epoch 540/2000
1024/1024 - 0s - loss: 0.1894 - val_loss: 0.1470
Epoch 541/2000
1024/1024 - 0s - loss: 0.1893 - val_loss: 0.1469
Epoch 542/2000
1024/1024 - 0s - loss: 0.1891 - val_loss: 0.1467
Epoch 543/2000
1024/1024 - 0s - loss: 0.1890 - val_loss: 0.1466
Epoch 544/2000
1024/1024 - 0s - loss: 0.1889 - val_loss: 0.1465
Epoch 545/2000
1024/1024 - 0s - loss: 0.1887 - val_loss: 0.1464
Epoch 546/2000
1024/1024 - 0s - loss: 0.1886 - val_loss: 0.1463
Epoch 547/2000
1024/1024 - 0s - loss: 0.1885 - val_loss: 0.1462
Epoch 548/2000
1024/1024 - 0s - loss: 0.1883 - val_loss: 0.1461
Epoch 549/2000
1024/1024 - 0s - loss: 0.1882 - val_loss: 0.1460
Epoch 550/2000
1024/1024 - 0s - loss: 0.1880 - val_loss: 0.1459
Epoch 551/2000
1024/1024 - 0s - loss: 0.1879 - val_loss: 0.1457
Epoch 552/2000
1024/1024 - 0s - loss: 0.1877 - val_loss: 0.1456
Epoch 553/2000
1024/1024 - 0s - loss: 0.1876 - val_loss: 0.1455
Epoch 554/2000
1024/1024 - 0s - loss: 0.1874 - val_loss: 0.1454
Epoch 555/2000
1024/1024 - 0s - loss: 0.1873 - val_loss: 0.1452
Epoch 556/2000
1024/1024 - 0s - loss: 0.1871 - val_loss: 0.1451
Epoch 557/2000
1024/1024 - 0s - loss: 0.1869 - val_loss: 0.1450
Epoch 558/2000
1024/1024 - 0s - loss: 0.1868 - val_loss: 0.1448
Epoch 559/2000
1024/1024 - 0s - loss: 0.1866 - val_loss: 0.1447
Epoch 560/2000
1024/1024 - 0s - loss: 0.1864 - val_loss: 0.1446
Epoch 561/2000
1024/1024 - 0s - loss: 0.1863 - val_loss: 0.1444
Epoch 562/2000
1024/1024 - 0s - loss: 0.1861 - val_loss: 0.1443
Epoch 563/2000
1024/1024 - 0s - loss: 0.1859 - val_loss: 0.1441
Epoch 564/2000
1024/1024 - 0s - loss: 0.1857 - val_loss: 0.1440
Epoch 565/2000
1024/1024 - 0s - loss: 0.1855 - val_loss: 0.1438
Epoch 566/2000
1024/1024 - 0s - loss: 0.1853 - val_loss: 0.1437
Epoch 567/2000
1024/1024 - 0s - loss: 0.1851 - val_loss: 0.1435
Epoch 568/2000
1024/1024 - 0s - loss: 0.1849 - val_loss: 0.1434
Epoch 569/2000
1024/1024 - 0s - loss: 0.1847 - val_loss: 0.1432
Epoch 570/2000
1024/1024 - 0s - loss: 0.1845 - val_loss: 0.1430
Epoch 571/2000
1024/1024 - 0s - loss: 0.1843 - val_loss: 0.1429
Epoch 572/2000
1024/1024 - 0s - loss: 0.1841 - val_loss: 0.1427
Epoch 573/2000
1024/1024 - 0s - loss: 0.1839 - val_loss: 0.1425
Epoch 574/2000
1024/1024 - 0s - loss: 0.1837 - val_loss: 0.1423
Epoch 575/2000
1024/1024 - 0s - loss: 0.1835 - val_loss: 0.1422
Epoch 576/2000
1024/1024 - 0s - loss: 0.1832 - val_loss: 0.1420
Epoch 577/2000
1024/1024 - 0s - loss: 0.1830 - val_loss: 0.1418
Epoch 578/2000
1024/1024 - 0s - loss: 0.1827 - val_loss: 0.1416
Epoch 579/2000
1024/1024 - 0s - loss: 0.1825 - val_loss: 0.1414
Epoch 580/2000
1024/1024 - 0s - loss: 0.1823 - val_loss: 0.1412
Epoch 581/2000
1024/1024 - 0s - loss: 0.1820 - val_loss: 0.1410
Epoch 582/2000
1024/1024 - 0s - loss: 0.1817 - val_loss: 0.1408
Epoch 583/2000
1024/1024 - 0s - loss: 0.1815 - val_loss: 0.1406
Epoch 584/2000
1024/1024 - 0s - loss: 0.1812 - val_loss: 0.1404
Epoch 585/2000
1024/1024 - 0s - loss: 0.1809 - val_loss: 0.1402
Epoch 586/2000
1024/1024 - 0s - loss: 0.1807 - val_loss: 0.1400
Epoch 587/2000
1024/1024 - 0s - loss: 0.1804 - val_loss: 0.1397
Epoch 588/2000
1024/1024 - 0s - loss: 0.1801 - val_loss: 0.1395
Epoch 589/2000
1024/1024 - 0s - loss: 0.1798 - val_loss: 0.1393
Epoch 590/2000
1024/1024 - 0s - loss: 0.1795 - val_loss: 0.1391
Epoch 591/2000
1024/1024 - 0s - loss: 0.1792 - val_loss: 0.1388
Epoch 592/2000
1024/1024 - 0s - loss: 0.1789 - val_loss: 0.1386
Epoch 593/2000
1024/1024 - 0s - loss: 0.1785 - val_loss: 0.1383
Epoch 594/2000
1024/1024 - 0s - loss: 0.1782 - val_loss: 0.1381
Epoch 595/2000
1024/1024 - 0s - loss: 0.1779 - val_loss: 0.1378
Epoch 596/2000
1024/1024 - 0s - loss: 0.1775 - val_loss: 0.1375
Epoch 597/2000
1024/1024 - 0s - loss: 0.1772 - val_loss: 0.1373
Epoch 598/2000
1024/1024 - 0s - loss: 0.1768 - val_loss: 0.1370
Epoch 599/2000
1024/1024 - 0s - loss: 0.1764 - val_loss: 0.1367
Epoch 600/2000
1024/1024 - 0s - loss: 0.1760 - val_loss: 0.1364
Epoch 601/2000
1024/1024 - 0s - loss: 0.1756 - val_loss: 0.1362
Epoch 602/2000
1024/1024 - 0s - loss: 0.1752 - val_loss: 0.1359
Epoch 603/2000
1024/1024 - 0s - loss: 0.1748 - val_loss: 0.1356
Epoch 604/2000
1024/1024 - 0s - loss: 0.1744 - val_loss: 0.1353
Epoch 605/2000
1024/1024 - 0s - loss: 0.1740 - val_loss: 0.1350
Epoch 606/2000
1024/1024 - 0s - loss: 0.1736 - val_loss: 0.1346
Epoch 607/2000
1024/1024 - 0s - loss: 0.1731 - val_loss: 0.1343
Epoch 608/2000
1024/1024 - 0s - loss: 0.1727 - val_loss: 0.1340
Epoch 609/2000
1024/1024 - 0s - loss: 0.1722 - val_loss: 0.1337
Epoch 610/2000
1024/1024 - 0s - loss: 0.1717 - val_loss: 0.1333
Epoch 611/2000
1024/1024 - 0s - loss: 0.1712 - val_loss: 0.1330
Epoch 612/2000
1024/1024 - 0s - loss: 0.1707 - val_loss: 0.1326
Epoch 613/2000
1024/1024 - 0s - loss: 0.1702 - val_loss: 0.1323
Epoch 614/2000
1024/1024 - 0s - loss: 0.1696 - val_loss: 0.1319
Epoch 615/2000
1024/1024 - 0s - loss: 0.1691 - val_loss: 0.1315
Epoch 616/2000
1024/1024 - 0s - loss: 0.1685 - val_loss: 0.1311
Epoch 617/2000
1024/1024 - 0s - loss: 0.1679 - val_loss: 0.1308
Epoch 618/2000
1024/1024 - 0s - loss: 0.1674 - val_loss: 0.1304
Epoch 619/2000
1024/1024 - 0s - loss: 0.1667 - val_loss: 0.1300
Epoch 620/2000
1024/1024 - 0s - loss: 0.1661 - val_loss: 0.1296
Epoch 621/2000
1024/1024 - 0s - loss: 0.1655 - val_loss: 0.1291
Epoch 622/2000
1024/1024 - 0s - loss: 0.1648 - val_loss: 0.1287
Epoch 623/2000
1024/1024 - 0s - loss: 0.1641 - val_loss: 0.1283
Epoch 624/2000
1024/1024 - 0s - loss: 0.1634 - val_loss: 0.1278
Epoch 625/2000
1024/1024 - 0s - loss: 0.1627 - val_loss: 0.1274
Epoch 626/2000
1024/1024 - 0s - loss: 0.1619 - val_loss: 0.1269
Epoch 627/2000
1024/1024 - 0s - loss: 0.1612 - val_loss: 0.1265
Epoch 628/2000
1024/1024 - 0s - loss: 0.1604 - val_loss: 0.1260
Epoch 629/2000
1024/1024 - 0s - loss: 0.1596 - val_loss: 0.1255
Epoch 630/2000
1024/1024 - 0s - loss: 0.1587 - val_loss: 0.1251
Epoch 631/2000
1024/1024 - 0s - loss: 0.1579 - val_loss: 0.1246
Epoch 632/2000
1024/1024 - 0s - loss: 0.1570 - val_loss: 0.1241
Epoch 633/2000
1024/1024 - 0s - loss: 0.1560 - val_loss: 0.1235
Epoch 634/2000
1024/1024 - 0s - loss: 0.1551 - val_loss: 0.1230
Epoch 635/2000
1024/1024 - 0s - loss: 0.1541 - val_loss: 0.1225
Epoch 636/2000
1024/1024 - 0s - loss: 0.1531 - val_loss: 0.1220
Epoch 637/2000
1024/1024 - 0s - loss: 0.1521 - val_loss: 0.1215
Epoch 638/2000
1024/1024 - 0s - loss: 0.1510 - val_loss: 0.1209
Epoch 639/2000
1024/1024 - 0s - loss: 0.1499 - val_loss: 0.1204
Epoch 640/2000
1024/1024 - 0s - loss: 0.1487 - val_loss: 0.1198
Epoch 641/2000
1024/1024 - 0s - loss: 0.1475 - val_loss: 0.1193
Epoch 642/2000
1024/1024 - 0s - loss: 0.1463 - val_loss: 0.1187
Epoch 643/2000
1024/1024 - 0s - loss: 0.1451 - val_loss: 0.1181
Epoch 644/2000
1024/1024 - 0s - loss: 0.1438 - val_loss: 0.1176
Epoch 645/2000
1024/1024 - 0s - loss: 0.1425 - val_loss: 0.1170
Epoch 646/2000
1024/1024 - 0s - loss: 0.1412 - val_loss: 0.1164
Epoch 647/2000
1024/1024 - 0s - loss: 0.1398 - val_loss: 0.1159
Epoch 648/2000
1024/1024 - 0s - loss: 0.1384 - val_loss: 0.1153
Epoch 649/2000
1024/1024 - 0s - loss: 0.1369 - val_loss: 0.1147
Epoch 650/2000
1024/1024 - 0s - loss: 0.1355 - val_loss: 0.1142
Epoch 651/2000
1024/1024 - 0s - loss: 0.1340 - val_loss: 0.1136
Epoch 652/2000
1024/1024 - 0s - loss: 0.1325 - val_loss: 0.1130
Epoch 653/2000
1024/1024 - 0s - loss: 0.1309 - val_loss: 0.1124
Epoch 654/2000
1024/1024 - 0s - loss: 0.1293 - val_loss: 0.1118
Epoch 655/2000
1024/1024 - 0s - loss: 0.1276 - val_loss: 0.1112
Epoch 656/2000
1024/1024 - 0s - loss: 0.1259 - val_loss: 0.1104
Epoch 657/2000
1024/1024 - 0s - loss: 0.1240 - val_loss: 0.1095
Epoch 658/2000
1024/1024 - 0s - loss: 0.1216 - val_loss: 0.1078
Epoch 659/2000
1024/1024 - 0s - loss: 0.1177 - val_loss: 0.1021
Epoch 660/2000
1024/1024 - 0s - loss: 0.1016 - val_loss: 0.0848
Epoch 661/2000
1024/1024 - 0s - loss: 0.0927 - val_loss: 0.0832
Epoch 662/2000
1024/1024 - 0s - loss: 0.0912 - val_loss: 0.0828
Epoch 663/2000
1024/1024 - 0s - loss: 0.0898 - val_loss: 0.0824
Epoch 664/2000
1024/1024 - 0s - loss: 0.0884 - val_loss: 0.0821
Epoch 665/2000
1024/1024 - 0s - loss: 0.0871 - val_loss: 0.0817
Epoch 666/2000
1024/1024 - 0s - loss: 0.0858 - val_loss: 0.0814
Epoch 667/2000
1024/1024 - 0s - loss: 0.0846 - val_loss: 0.0812
Epoch 668/2000
1024/1024 - 0s - loss: 0.0835 - val_loss: 0.0809
Epoch 669/2000
1024/1024 - 0s - loss: 0.0825 - val_loss: 0.0807
Epoch 670/2000
1024/1024 - 0s - loss: 0.0815 - val_loss: 0.0804
Epoch 671/2000
1024/1024 - 0s - loss: 0.0805 - val_loss: 0.0802
Epoch 672/2000
1024/1024 - 0s - loss: 0.0796 - val_loss: 0.0801
Epoch 673/2000
1024/1024 - 0s - loss: 0.0788 - val_loss: 0.0799
Epoch 674/2000
1024/1024 - 0s - loss: 0.0780 - val_loss: 0.0797
Epoch 675/2000
1024/1024 - 0s - loss: 0.0772 - val_loss: 0.0796
Epoch 676/2000
1024/1024 - 0s - loss: 0.0765 - val_loss: 0.0795
Epoch 677/2000
1024/1024 - 0s - loss: 0.0758 - val_loss: 0.0794
Epoch 678/2000
1024/1024 - 0s - loss: 0.0752 - val_loss: 0.0792
Epoch 679/2000
1024/1024 - 0s - loss: 0.0746 - val_loss: 0.0791
Epoch 680/2000
1024/1024 - 0s - loss: 0.0740 - val_loss: 0.0791
Epoch 681/2000
1024/1024 - 0s - loss: 0.0735 - val_loss: 0.0790
Epoch 682/2000
1024/1024 - 0s - loss: 0.0730 - val_loss: 0.0789
Epoch 683/2000
1024/1024 - 0s - loss: 0.0725 - val_loss: 0.0788
Epoch 684/2000
1024/1024 - 0s - loss: 0.0720 - val_loss: 0.0788
Epoch 685/2000
1024/1024 - 0s - loss: 0.0715 - val_loss: 0.0787
Epoch 686/2000
1024/1024 - 0s - loss: 0.0711 - val_loss: 0.0787
Epoch 687/2000
1024/1024 - 0s - loss: 0.0707 - val_loss: 0.0786
Epoch 688/2000
1024/1024 - 0s - loss: 0.0703 - val_loss: 0.0786
Epoch 689/2000
1024/1024 - 0s - loss: 0.0699 - val_loss: 0.0785
Epoch 690/2000
1024/1024 - 0s - loss: 0.0696 - val_loss: 0.0785
Epoch 691/2000
1024/1024 - 0s - loss: 0.0692 - val_loss: 0.0785
Epoch 692/2000
1024/1024 - 0s - loss: 0.0689 - val_loss: 0.0784
Epoch 693/2000
1024/1024 - 0s - loss: 0.0685 - val_loss: 0.0784
Epoch 694/2000
1024/1024 - 0s - loss: 0.0682 - val_loss: 0.0784
Epoch 695/2000
1024/1024 - 0s - loss: 0.0679 - val_loss: 0.0784
Epoch 696/2000
1024/1024 - 0s - loss: 0.0676 - val_loss: 0.0783
Epoch 697/2000
1024/1024 - 0s - loss: 0.0673 - val_loss: 0.0783
Epoch 698/2000
1024/1024 - 0s - loss: 0.0671 - val_loss: 0.0783
Epoch 699/2000
1024/1024 - 0s - loss: 0.0668 - val_loss: 0.0783
Epoch 700/2000
1024/1024 - 0s - loss: 0.0665 - val_loss: 0.0783
Epoch 701/2000
1024/1024 - 0s - loss: 0.0663 - val_loss: 0.0783
Epoch 702/2000
1024/1024 - 0s - loss: 0.0660 - val_loss: 0.0782
Epoch 703/2000
1024/1024 - 0s - loss: 0.0658 - val_loss: 0.0782
Epoch 704/2000
1024/1024 - 0s - loss: 0.0656 - val_loss: 0.0782
Epoch 705/2000
1024/1024 - 0s - loss: 0.0653 - val_loss: 0.0782
Epoch 706/2000
1024/1024 - 0s - loss: 0.0651 - val_loss: 0.0782
Epoch 707/2000
1024/1024 - 0s - loss: 0.0649 - val_loss: 0.0782
Epoch 708/2000
1024/1024 - 0s - loss: 0.0647 - val_loss: 0.0782
Epoch 709/2000
1024/1024 - 0s - loss: 0.0645 - val_loss: 0.0782
Epoch 710/2000
1024/1024 - 0s - loss: 0.0643 - val_loss: 0.0782
Epoch 711/2000
1024/1024 - 0s - loss: 0.0641 - val_loss: 0.0782
Epoch 712/2000
1024/1024 - 0s - loss: 0.0639 - val_loss: 0.0781
Epoch 713/2000
1024/1024 - 0s - loss: 0.0637 - val_loss: 0.0781
Epoch 714/2000
1024/1024 - 0s - loss: 0.0636 - val_loss: 0.0781
Epoch 715/2000
1024/1024 - 0s - loss: 0.0634 - val_loss: 0.0781
Epoch 716/2000
1024/1024 - 0s - loss: 0.0632 - val_loss: 0.0781
Epoch 717/2000
1024/1024 - 0s - loss: 0.0631 - val_loss: 0.0781
Epoch 718/2000
1024/1024 - 0s - loss: 0.0629 - val_loss: 0.0781
Epoch 719/2000
1024/1024 - 0s - loss: 0.0628 - val_loss: 0.0781
Epoch 720/2000
1024/1024 - 0s - loss: 0.0626 - val_loss: 0.0781
Epoch 721/2000
1024/1024 - 0s - loss: 0.0625 - val_loss: 0.0780
Epoch 722/2000
1024/1024 - 0s - loss: 0.0623 - val_loss: 0.0780
Epoch 723/2000
1024/1024 - 0s - loss: 0.0622 - val_loss: 0.0780
Epoch 724/2000
1024/1024 - 0s - loss: 0.0620 - val_loss: 0.0780
Epoch 725/2000
1024/1024 - 0s - loss: 0.0619 - val_loss: 0.0780
Epoch 726/2000
1024/1024 - 0s - loss: 0.0618 - val_loss: 0.0779
Epoch 727/2000
1024/1024 - 0s - loss: 0.0616 - val_loss: 0.0779
Epoch 728/2000
1024/1024 - 0s - loss: 0.0615 - val_loss: 0.0778
Epoch 729/2000
1024/1024 - 0s - loss: 0.0614 - val_loss: 0.0778
Epoch 730/2000
1024/1024 - 0s - loss: 0.0613 - val_loss: 0.0777
Epoch 731/2000
1024/1024 - 0s - loss: 0.0611 - val_loss: 0.0776
Epoch 732/2000
1024/1024 - 0s - loss: 0.0610 - val_loss: 0.0775
Epoch 733/2000
1024/1024 - 0s - loss: 0.0609 - val_loss: 0.0773
Epoch 734/2000
1024/1024 - 0s - loss: 0.0608 - val_loss: 0.0771
Epoch 735/2000
1024/1024 - 0s - loss: 0.0607 - val_loss: 0.0768
Epoch 736/2000
1024/1024 - 0s - loss: 0.0606 - val_loss: 0.0764
Epoch 737/2000
1024/1024 - 0s - loss: 0.0605 - val_loss: 0.0758
Epoch 738/2000
1024/1024 - 0s - loss: 0.0603 - val_loss: 0.0747
Epoch 739/2000
1024/1024 - 0s - loss: 0.0602 - val_loss: 0.0725
Epoch 740/2000
1024/1024 - 0s - loss: 0.0600 - val_loss: 0.0670
Epoch 741/2000
1024/1024 - 0s - loss: 0.0597 - val_loss: 0.0488
Epoch 742/2000
1024/1024 - 0s - loss: 0.0590 - val_loss: 0.0129
Epoch 743/2000
1024/1024 - 0s - loss: 0.0588 - val_loss: 0.0211
Epoch 744/2000
1024/1024 - 0s - loss: 0.0586 - val_loss: 0.0177
Epoch 745/2000
1024/1024 - 0s - loss: 0.0585 - val_loss: 0.0175
Epoch 746/2000
1024/1024 - 0s - loss: 0.0584 - val_loss: 0.0174
Epoch 747/2000
1024/1024 - 0s - loss: 0.0583 - val_loss: 0.0170
Epoch 748/2000
1024/1024 - 0s - loss: 0.0582 - val_loss: 0.0170
Epoch 749/2000
1024/1024 - 0s - loss: 0.0581 - val_loss: 0.0163
Epoch 750/2000
1024/1024 - 0s - loss: 0.0579 - val_loss: 0.0166
Epoch 751/2000
1024/1024 - 0s - loss: 0.0578 - val_loss: 0.0159
Epoch 752/2000
1024/1024 - 0s - loss: 0.0577 - val_loss: 0.0158
Epoch 753/2000
1024/1024 - 0s - loss: 0.0577 - val_loss: 0.0158
Epoch 754/2000
1024/1024 - 0s - loss: 0.0576 - val_loss: 0.0150
Epoch 755/2000
1024/1024 - 0s - loss: 0.0575 - val_loss: 0.0152
Epoch 756/2000
1024/1024 - 0s - loss: 0.0574 - val_loss: 0.0148
Epoch 757/2000
1024/1024 - 0s - loss: 0.0573 - val_loss: 0.0145
Epoch 758/2000
1024/1024 - 0s - loss: 0.0572 - val_loss: 0.0149
Epoch 759/2000
1024/1024 - 0s - loss: 0.0571 - val_loss: 0.0139
Epoch 760/2000
1024/1024 - 0s - loss: 0.0570 - val_loss: 0.0142
Epoch 761/2000
1024/1024 - 0s - loss: 0.0570 - val_loss: 0.0141
Epoch 762/2000
1024/1024 - 0s - loss: 0.0569 - val_loss: 0.0133
Epoch 763/2000
1024/1024 - 0s - loss: 0.0568 - val_loss: 0.0135
Epoch 764/2000
1024/1024 - 0s - loss: 0.0567 - val_loss: 0.0133
Epoch 765/2000
1024/1024 - 0s - loss: 0.0566 - val_loss: 0.0129
Epoch 766/2000
1024/1024 - 0s - loss: 0.0566 - val_loss: 0.0134
Epoch 767/2000
1024/1024 - 0s - loss: 0.0565 - val_loss: 0.0128
Epoch 768/2000
1024/1024 - 0s - loss: 0.0564 - val_loss: 0.0127
Epoch 769/2000
1024/1024 - 0s - loss: 0.0564 - val_loss: 0.0120
Epoch 770/2000
1024/1024 - 0s - loss: 0.0563 - val_loss: 0.0130
Epoch 771/2000
1024/1024 - 0s - loss: 0.0562 - val_loss: 0.0117
Epoch 772/2000
1024/1024 - 0s - loss: 0.0561 - val_loss: 0.0126
Epoch 773/2000
1024/1024 - 0s - loss: 0.0561 - val_loss: 0.0115
Epoch 774/2000
1024/1024 - 0s - loss: 0.0560 - val_loss: 0.0120
Epoch 775/2000
1024/1024 - 0s - loss: 0.0559 - val_loss: 0.0110
Epoch 776/2000
1024/1024 - 0s - loss: 0.0559 - val_loss: 0.0113
Epoch 777/2000
1024/1024 - 0s - loss: 0.0558 - val_loss: 0.0113
Epoch 778/2000
1024/1024 - 0s - loss: 0.0558 - val_loss: 0.0111
Epoch 779/2000
1024/1024 - 0s - loss: 0.0557 - val_loss: 0.0112
Epoch 780/2000
1024/1024 - 0s - loss: 0.0556 - val_loss: 0.0115
Epoch 781/2000
1024/1024 - 0s - loss: 0.0556 - val_loss: 0.0103
Epoch 782/2000
1024/1024 - 0s - loss: 0.0555 - val_loss: 0.0108
Epoch 783/2000
1024/1024 - 0s - loss: 0.0555 - val_loss: 0.0107
Epoch 784/2000
1024/1024 - 0s - loss: 0.0554 - val_loss: 0.0102
Epoch 785/2000
1024/1024 - 0s - loss: 0.0553 - val_loss: 0.0108
Epoch 786/2000
1024/1024 - 0s - loss: 0.0553 - val_loss: 0.0098
Epoch 787/2000
1024/1024 - 0s - loss: 0.0552 - val_loss: 0.0104
Epoch 788/2000
1024/1024 - 0s - loss: 0.0552 - val_loss: 0.0102
Epoch 789/2000
1024/1024 - 0s - loss: 0.0551 - val_loss: 0.0097
Epoch 790/2000
1024/1024 - 0s - loss: 0.0551 - val_loss: 0.0103
Epoch 791/2000
1024/1024 - 0s - loss: 0.0550 - val_loss: 0.0103
Epoch 792/2000
1024/1024 - 0s - loss: 0.0550 - val_loss: 0.0095
Epoch 793/2000
1024/1024 - 0s - loss: 0.0549 - val_loss: 0.0097
Epoch 794/2000
1024/1024 - 0s - loss: 0.0549 - val_loss: 0.0098
Epoch 795/2000
1024/1024 - 0s - loss: 0.0548 - val_loss: 0.0096
Epoch 796/2000
1024/1024 - 0s - loss: 0.0548 - val_loss: 0.0098
Epoch 797/2000
1024/1024 - 0s - loss: 0.0547 - val_loss: 0.0086
Epoch 798/2000
1024/1024 - 0s - loss: 0.0547 - val_loss: 0.0091
Epoch 799/2000
1024/1024 - 0s - loss: 0.0546 - val_loss: 0.0093
Epoch 800/2000
1024/1024 - 0s - loss: 0.0546 - val_loss: 0.0096
Epoch 801/2000
1024/1024 - 0s - loss: 0.0545 - val_loss: 0.0084
Epoch 802/2000
1024/1024 - 0s - loss: 0.0545 - val_loss: 0.0089
Epoch 803/2000
1024/1024 - 0s - loss: 0.0544 - val_loss: 0.0094
Epoch 804/2000
1024/1024 - 0s - loss: 0.0544 - val_loss: 0.0095
Epoch 805/2000
1024/1024 - 0s - loss: 0.0544 - val_loss: 0.0083
Epoch 806/2000
1024/1024 - 0s - loss: 0.0543 - val_loss: 0.0081
Epoch 807/2000
1024/1024 - 0s - loss: 0.0543 - val_loss: 0.0088
Epoch 808/2000
1024/1024 - 0s - loss: 0.0542 - val_loss: 0.0092
Epoch 809/2000
1024/1024 - 0s - loss: 0.0542 - val_loss: 0.0077
Epoch 810/2000
1024/1024 - 0s - loss: 0.0541 - val_loss: 0.0085
Epoch 811/2000
1024/1024 - 0s - loss: 0.0541 - val_loss: 0.0080
Epoch 812/2000
1024/1024 - 0s - loss: 0.0541 - val_loss: 0.0083
Epoch 813/2000
1024/1024 - 0s - loss: 0.0540 - val_loss: 0.0079
Epoch 814/2000
1024/1024 - 0s - loss: 0.0540 - val_loss: 0.0083
Epoch 815/2000
1024/1024 - 0s - loss: 0.0539 - val_loss: 0.0087
Epoch 816/2000
1024/1024 - 0s - loss: 0.0539 - val_loss: 0.0071
Epoch 817/2000
1024/1024 - 0s - loss: 0.0539 - val_loss: 0.0074
Epoch 818/2000
1024/1024 - 0s - loss: 0.0538 - val_loss: 0.0078
Epoch 819/2000
1024/1024 - 0s - loss: 0.0538 - val_loss: 0.0066
Epoch 820/2000
1024/1024 - 0s - loss: 0.0537 - val_loss: 0.0076
Epoch 821/2000
1024/1024 - 0s - loss: 0.0537 - val_loss: 0.0079
Epoch 822/2000
1024/1024 - 0s - loss: 0.0537 - val_loss: 0.0071
Epoch 823/2000
1024/1024 - 0s - loss: 0.0536 - val_loss: 0.0076
Epoch 824/2000
1024/1024 - 0s - loss: 0.0536 - val_loss: 0.0073
Epoch 825/2000
1024/1024 - 0s - loss: 0.0536 - val_loss: 0.0072
Epoch 826/2000
1024/1024 - 0s - loss: 0.0535 - val_loss: 0.0081
Epoch 827/2000
1024/1024 - 0s - loss: 0.0535 - val_loss: 0.0062
Epoch 828/2000
1024/1024 - 0s - loss: 0.0516 - val_loss: 0.0075
Epoch 829/2000
1024/1024 - 0s - loss: 0.0503 - val_loss: 0.0062
Epoch 830/2000
1024/1024 - 0s - loss: 0.0500 - val_loss: 0.0069
Epoch 831/2000
1024/1024 - 0s - loss: 0.0498 - val_loss: 0.0073
Epoch 832/2000
1024/1024 - 0s - loss: 0.0495 - val_loss: 0.0061
Epoch 833/2000
1024/1024 - 0s - loss: 0.0492 - val_loss: 0.0060
Epoch 834/2000
1024/1024 - 0s - loss: 0.0490 - val_loss: 0.0054
Epoch 835/2000
1024/1024 - 0s - loss: 0.0488 - val_loss: 0.0053
Epoch 836/2000
1024/1024 - 0s - loss: 0.0486 - val_loss: 0.0052
Epoch 837/2000
1024/1024 - 0s - loss: 0.0484 - val_loss: 0.0042
Epoch 838/2000
1024/1024 - 0s - loss: 0.0482 - val_loss: 0.0047
Epoch 839/2000
1024/1024 - 0s - loss: 0.0481 - val_loss: 0.0045
Epoch 840/2000
1024/1024 - 0s - loss: 0.0479 - val_loss: 0.0043
Epoch 841/2000
1024/1024 - 0s - loss: 0.0478 - val_loss: 0.0045
Epoch 842/2000
1024/1024 - 0s - loss: 0.0476 - val_loss: 0.0045
Epoch 843/2000
1024/1024 - 0s - loss: 0.0475 - val_loss: 0.0041
Epoch 844/2000
1024/1024 - 0s - loss: 0.0473 - val_loss: 0.0039
Epoch 845/2000
1024/1024 - 0s - loss: 0.0472 - val_loss: 0.0039
Epoch 846/2000
1024/1024 - 0s - loss: 0.0471 - val_loss: 0.0039
Epoch 847/2000
1024/1024 - 0s - loss: 0.0470 - val_loss: 0.0040
Epoch 848/2000
1024/1024 - 0s - loss: 0.0468 - val_loss: 0.0035
Epoch 849/2000
1024/1024 - 0s - loss: 0.0466 - val_loss: 0.0036
Epoch 850/2000
1024/1024 - 0s - loss: 0.0465 - val_loss: 0.0036
Epoch 851/2000
1024/1024 - 0s - loss: 0.0464 - val_loss: 0.0036
Epoch 852/2000
1024/1024 - 0s - loss: 0.0463 - val_loss: 0.0032
Epoch 853/2000
1024/1024 - 0s - loss: 0.0462 - val_loss: 0.0040
Epoch 854/2000
1024/1024 - 0s - loss: 0.0461 - val_loss: 0.0030
Epoch 855/2000
1024/1024 - 0s - loss: 0.0460 - val_loss: 0.0035
Epoch 856/2000
1024/1024 - 0s - loss: 0.0459 - val_loss: 0.0031
Epoch 857/2000
1024/1024 - 0s - loss: 0.0458 - val_loss: 0.0030
Epoch 858/2000
1024/1024 - 0s - loss: 0.0457 - val_loss: 0.0029
Epoch 859/2000
1024/1024 - 0s - loss: 0.0457 - val_loss: 0.0032
Epoch 860/2000
1024/1024 - 0s - loss: 0.0456 - val_loss: 0.0031
Epoch 861/2000
1024/1024 - 0s - loss: 0.0455 - val_loss: 0.0027
Epoch 862/2000
1024/1024 - 0s - loss: 0.0454 - val_loss: 0.0025
Epoch 863/2000
1024/1024 - 0s - loss: 0.0453 - val_loss: 0.0029
Epoch 864/2000
1024/1024 - 0s - loss: 0.0452 - val_loss: 0.0028
Epoch 865/2000
1024/1024 - 0s - loss: 0.0451 - val_loss: 0.0030
Epoch 866/2000
1024/1024 - 0s - loss: 0.0451 - val_loss: 0.0026
Epoch 867/2000
1024/1024 - 0s - loss: 0.0450 - val_loss: 0.0027
Epoch 868/2000
1024/1024 - 0s - loss: 0.0449 - val_loss: 0.0027
Epoch 869/2000
1024/1024 - 0s - loss: 0.0448 - val_loss: 0.0028
Epoch 870/2000
1024/1024 - 0s - loss: 0.0447 - val_loss: 0.0029
Epoch 871/2000
1024/1024 - 0s - loss: 0.0446 - val_loss: 0.0032
Epoch 872/2000
1024/1024 - 0s - loss: 0.0445 - val_loss: 0.0033
Epoch 873/2000
1024/1024 - 0s - loss: 0.0445 - val_loss: 0.0030
Epoch 874/2000
1024/1024 - 0s - loss: 0.0444 - val_loss: 0.0031
Epoch 875/2000
1024/1024 - 0s - loss: 0.0443 - val_loss: 0.0028
Epoch 876/2000
1024/1024 - 0s - loss: 0.0442 - val_loss: 0.0024
Epoch 877/2000
1024/1024 - 0s - loss: 0.0441 - val_loss: 0.0029
Epoch 878/2000
1024/1024 - 0s - loss: 0.0440 - val_loss: 0.0025
Epoch 879/2000
1024/1024 - 0s - loss: 0.0439 - val_loss: 0.0025
Epoch 880/2000
1024/1024 - 0s - loss: 0.0438 - val_loss: 0.0025
Epoch 881/2000
1024/1024 - 0s - loss: 0.0437 - val_loss: 0.0025
Epoch 882/2000
1024/1024 - 0s - loss: 0.0436 - val_loss: 0.0024
Epoch 883/2000
1024/1024 - 0s - loss: 0.0435 - val_loss: 0.0024
Epoch 884/2000
1024/1024 - 0s - loss: 0.0434 - val_loss: 0.0025
Epoch 885/2000
1024/1024 - 0s - loss: 0.0433 - val_loss: 0.0028
Epoch 886/2000
1024/1024 - 0s - loss: 0.0432 - val_loss: 0.0030
Epoch 887/2000
1024/1024 - 0s - loss: 0.0431 - val_loss: 0.0026
Epoch 888/2000
1024/1024 - 0s - loss: 0.0430 - val_loss: 0.0027
Epoch 889/2000
1024/1024 - 0s - loss: 0.0429 - val_loss: 0.0030
Epoch 890/2000
1024/1024 - 0s - loss: 0.0428 - val_loss: 0.0026
Epoch 891/2000
1024/1024 - 0s - loss: 0.0427 - val_loss: 0.0029
Epoch 892/2000
1024/1024 - 0s - loss: 0.0426 - val_loss: 0.0029
Epoch 893/2000
1024/1024 - 0s - loss: 0.0425 - val_loss: 0.0030
Epoch 894/2000
1024/1024 - 0s - loss: 0.0424 - val_loss: 0.0034
Epoch 895/2000
1024/1024 - 0s - loss: 0.0423 - val_loss: 0.0034
Epoch 896/2000
1024/1024 - 0s - loss: 0.0422 - val_loss: 0.0033
Epoch 897/2000
1024/1024 - 0s - loss: 0.0416 - val_loss: 0.0038
Epoch 898/2000
1024/1024 - 0s - loss: 0.0415 - val_loss: 0.0036
Epoch 899/2000
1024/1024 - 0s - loss: 0.0414 - val_loss: 0.0032
Epoch 900/2000
1024/1024 - 0s - loss: 0.0414 - val_loss: 0.0029
Epoch 901/2000
1024/1024 - 0s - loss: 0.0414 - val_loss: 0.0027
Epoch 902/2000
1024/1024 - 0s - loss: 0.0413 - val_loss: 0.0035
Epoch 903/2000
1024/1024 - 0s - loss: 0.0413 - val_loss: 0.0036
Epoch 904/2000
1024/1024 - 0s - loss: 0.0412 - val_loss: 0.0033
Epoch 905/2000
1024/1024 - 0s - loss: 0.0412 - val_loss: 0.0028
Epoch 906/2000
1024/1024 - 0s - loss: 0.0411 - val_loss: 0.0027
Epoch 907/2000
1024/1024 - 0s - loss: 0.0411 - val_loss: 0.0025
Epoch 908/2000
1024/1024 - 0s - loss: 0.0411 - val_loss: 0.0025
Epoch 909/2000
1024/1024 - 0s - loss: 0.0410 - val_loss: 0.0026
Epoch 910/2000
1024/1024 - 0s - loss: 0.0410 - val_loss: 0.0029
Epoch 911/2000
1024/1024 - 0s - loss: 0.0410 - val_loss: 0.0025
Epoch 912/2000
1024/1024 - 0s - loss: 0.0409 - val_loss: 0.0027
Epoch 913/2000
1024/1024 - 0s - loss: 0.0409 - val_loss: 0.0028
Epoch 914/2000
1024/1024 - 0s - loss: 0.0409 - val_loss: 0.0025
Epoch 915/2000
1024/1024 - 0s - loss: 0.0408 - val_loss: 0.0024
Epoch 916/2000
1024/1024 - 0s - loss: 0.0408 - val_loss: 0.0028
Epoch 917/2000
1024/1024 - 0s - loss: 0.0408 - val_loss: 0.0028
Epoch 918/2000
1024/1024 - 0s - loss: 0.0408 - val_loss: 0.0026
Epoch 919/2000
1024/1024 - 0s - loss: 0.0407 - val_loss: 0.0026
Epoch 920/2000
1024/1024 - 0s - loss: 0.0407 - val_loss: 0.0032
Epoch 921/2000
1024/1024 - 0s - loss: 0.0407 - val_loss: 0.0030
Epoch 922/2000
1024/1024 - 0s - loss: 0.0407 - val_loss: 0.0027
Epoch 923/2000
1024/1024 - 0s - loss: 0.0406 - val_loss: 0.0028
Epoch 924/2000
1024/1024 - 0s - loss: 0.0406 - val_loss: 0.0026
Epoch 925/2000
1024/1024 - 0s - loss: 0.0406 - val_loss: 0.0025
Epoch 926/2000
1024/1024 - 0s - loss: 0.0406 - val_loss: 0.0030
Epoch 927/2000
1024/1024 - 0s - loss: 0.0405 - val_loss: 0.0027
Epoch 928/2000
1024/1024 - 0s - loss: 0.0405 - val_loss: 0.0027
Epoch 929/2000
1024/1024 - 0s - loss: 0.0405 - val_loss: 0.0029
Epoch 930/2000
1024/1024 - 0s - loss: 0.0405 - val_loss: 0.0028
Epoch 931/2000
1024/1024 - 0s - loss: 0.0404 - val_loss: 0.0026
Epoch 932/2000
1024/1024 - 0s - loss: 0.0404 - val_loss: 0.0028
Epoch 933/2000
1024/1024 - 0s - loss: 0.0404 - val_loss: 0.0028
Epoch 934/2000
1024/1024 - 0s - loss: 0.0404 - val_loss: 0.0028
Epoch 935/2000
1024/1024 - 0s - loss: 0.0404 - val_loss: 0.0030
Epoch 936/2000
1024/1024 - 0s - loss: 0.0403 - val_loss: 0.0026
Epoch 937/2000
1024/1024 - 0s - loss: 0.0403 - val_loss: 0.0027
Epoch 938/2000
1024/1024 - 0s - loss: 0.0403 - val_loss: 0.0030
Epoch 939/2000
1024/1024 - 0s - loss: 0.0403 - val_loss: 0.0031
Epoch 940/2000
1024/1024 - 0s - loss: 0.0403 - val_loss: 0.0029
Epoch 941/2000
1024/1024 - 0s - loss: 0.0402 - val_loss: 0.0033
Epoch 942/2000
1024/1024 - 0s - loss: 0.0402 - val_loss: 0.0029
Epoch 943/2000
1024/1024 - 0s - loss: 0.0402 - val_loss: 0.0031
Epoch 944/2000
1024/1024 - 0s - loss: 0.0402 - val_loss: 0.0031
Epoch 945/2000
1024/1024 - 0s - loss: 0.0402 - val_loss: 0.0030
Epoch 946/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0032
Epoch 947/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0028
Epoch 948/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0031
Epoch 949/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0034
Epoch 950/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0031
Epoch 951/2000
1024/1024 - 0s - loss: 0.0401 - val_loss: 0.0029
Epoch 952/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0032
Epoch 953/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0027
Epoch 954/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0038
Epoch 955/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0033
Epoch 956/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0033
Epoch 957/2000
1024/1024 - 0s - loss: 0.0400 - val_loss: 0.0031
Epoch 958/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0034
Epoch 959/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0029
Epoch 960/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0034
Epoch 961/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0039
Epoch 962/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0031
Epoch 963/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0035
Epoch 964/2000
1024/1024 - 0s - loss: 0.0399 - val_loss: 0.0037
Epoch 965/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0033
Epoch 966/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0034
Epoch 967/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0035
Epoch 968/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0033
Epoch 969/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0036
Epoch 970/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0030
Epoch 971/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0036
Epoch 972/2000
1024/1024 - 0s - loss: 0.0398 - val_loss: 0.0031
Epoch 973/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0035
Epoch 974/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0035
Epoch 975/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0035
Epoch 976/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0039
Epoch 977/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0038
Epoch 978/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0034
Epoch 979/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0038
Epoch 980/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0040
Epoch 981/2000
1024/1024 - 0s - loss: 0.0397 - val_loss: 0.0034
Epoch 982/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0039
Epoch 983/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0033
Epoch 984/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0037
Epoch 985/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0037
Epoch 986/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0037
Epoch 987/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0036
Epoch 988/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0038
Epoch 989/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0037
Epoch 990/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0044
Epoch 991/2000
1024/1024 - 0s - loss: 0.0396 - val_loss: 0.0035
Epoch 992/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0035
Epoch 993/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0039
Epoch 994/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0037
Epoch 995/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0038
Epoch 996/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0039
Epoch 997/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0041
Epoch 998/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0037
Epoch 999/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0037
Epoch 1000/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0038
Epoch 1001/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0039
Epoch 1002/2000
1024/1024 - 0s - loss: 0.0395 - val_loss: 0.0035
Epoch 1003/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0039
Epoch 1004/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0036
Epoch 1005/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0036
Epoch 1006/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0033
Epoch 1007/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0045
Epoch 1008/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0034
Epoch 1009/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0037
Epoch 1010/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0039
Epoch 1011/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0041
Epoch 1012/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0037
Epoch 1013/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0040
Epoch 1014/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0037
Epoch 1015/2000
1024/1024 - 0s - loss: 0.0394 - val_loss: 0.0037
Epoch 1016/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0040
Epoch 1017/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0041
Epoch 1018/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0044
Epoch 1019/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0048
Epoch 1020/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0037
Epoch 1021/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0040
Epoch 1022/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0040
Epoch 1023/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0039
Epoch 1024/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0033
Epoch 1025/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0046
Epoch 1026/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0035
Epoch 1027/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0041
Epoch 1028/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0040
Epoch 1029/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0039
Epoch 1030/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0036
Epoch 1031/2000
1024/1024 - 0s - loss: 0.0393 - val_loss: 0.0044
Epoch 1032/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0040
Epoch 1033/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0045
Epoch 1034/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0041
Epoch 1035/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0041
Epoch 1036/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0037
Epoch 1037/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0044
Epoch 1038/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0041
Epoch 1039/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0041
Epoch 1040/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0036
Epoch 1041/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0044
Epoch 1042/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0045
Epoch 1043/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0038
Epoch 1044/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0040
Epoch 1045/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0041
Epoch 1046/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0038
Epoch 1047/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0044
Epoch 1048/2000
1024/1024 - 0s - loss: 0.0392 - val_loss: 0.0044
Epoch 1049/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0038
Epoch 1050/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0040
Epoch 1051/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0048
Epoch 1052/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0043
Epoch 1053/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0040
Epoch 1054/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0042
Epoch 1055/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0043
Epoch 1056/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0045
Epoch 1057/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0039
Epoch 1058/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0042
Epoch 1059/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0040
Epoch 1060/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0043
Epoch 1061/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0045
Epoch 1062/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0042
Epoch 1063/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0043
Epoch 1064/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0044
Epoch 1065/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0044
Epoch 1066/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0040
Epoch 1067/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0046
Epoch 1068/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0046
Epoch 1069/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0035
Epoch 1070/2000
1024/1024 - 0s - loss: 0.0391 - val_loss: 0.0038
Epoch 1071/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0047
Epoch 1072/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0044
Epoch 1073/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0041
Epoch 1074/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1075/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0044
Epoch 1076/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1077/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0039
Epoch 1078/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0039
Epoch 1079/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0040
Epoch 1080/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1081/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0048
Epoch 1082/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1083/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0038
Epoch 1084/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1085/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0047
Epoch 1086/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1087/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0045
Epoch 1088/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0046
Epoch 1089/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0048
Epoch 1090/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0046
Epoch 1091/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0046
Epoch 1092/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0042
Epoch 1093/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0042
Epoch 1094/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0041
Epoch 1095/2000
1024/1024 - 0s - loss: 0.0390 - val_loss: 0.0044
Epoch 1096/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0045
Epoch 1097/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1098/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1099/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1100/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1101/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0041
Epoch 1102/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1103/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0041
Epoch 1104/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1105/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0045
Epoch 1106/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1107/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0047
Epoch 1108/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0048
Epoch 1109/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1110/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0045
Epoch 1111/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1112/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0046
Epoch 1113/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1114/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0050
Epoch 1115/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0051
Epoch 1116/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0053
Epoch 1117/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0047
Epoch 1118/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0047
Epoch 1119/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0052
Epoch 1120/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0044
Epoch 1121/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0050
Epoch 1122/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0042
Epoch 1123/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0045
Epoch 1124/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0039
Epoch 1125/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0041
Epoch 1126/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0043
Epoch 1127/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0041
Epoch 1128/2000
1024/1024 - 0s - loss: 0.0389 - val_loss: 0.0043
Epoch 1129/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0048
Epoch 1130/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1131/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1132/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0041
Epoch 1133/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0048
Epoch 1134/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0050
Epoch 1135/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1136/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0042
Epoch 1137/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0042
Epoch 1138/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0040
Epoch 1139/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1140/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0047
Epoch 1141/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1142/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0042
Epoch 1143/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1144/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1145/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0048
Epoch 1146/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1147/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1148/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1149/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0054
Epoch 1150/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0049
Epoch 1151/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1152/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1153/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0045
Epoch 1154/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0047
Epoch 1155/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1156/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0045
Epoch 1157/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0046
Epoch 1158/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1159/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1160/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1161/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1162/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1163/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0040
Epoch 1164/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0041
Epoch 1165/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0045
Epoch 1166/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1167/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1168/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0045
Epoch 1169/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0045
Epoch 1170/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0044
Epoch 1171/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0053
Epoch 1172/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0047
Epoch 1173/2000
1024/1024 - 0s - loss: 0.0388 - val_loss: 0.0043
Epoch 1174/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1175/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1176/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0048
Epoch 1177/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0043
Epoch 1178/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0048
Epoch 1179/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0051
Epoch 1180/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0050
Epoch 1181/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1182/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0043
Epoch 1183/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0048
Epoch 1184/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1185/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1186/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0043
Epoch 1187/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1188/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1189/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1190/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1191/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1192/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1193/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0040
Epoch 1194/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0041
Epoch 1195/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1196/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1197/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1198/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1199/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1200/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1201/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1202/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1203/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0044
Epoch 1204/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1205/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1206/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0043
Epoch 1207/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0043
Epoch 1208/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1209/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1210/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1211/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1212/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1213/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1214/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1215/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1216/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1217/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1218/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0052
Epoch 1219/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0042
Epoch 1220/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1221/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1222/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0040
Epoch 1223/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1224/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0047
Epoch 1225/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0045
Epoch 1226/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0049
Epoch 1227/2000
1024/1024 - 0s - loss: 0.0387 - val_loss: 0.0046
Epoch 1228/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1229/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0047
Epoch 1230/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1231/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1232/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1233/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0049
Epoch 1234/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0049
Epoch 1235/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0048
Epoch 1236/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1237/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1238/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1239/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1240/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1241/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1242/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1243/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1244/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1245/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1246/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1247/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1248/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1249/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0049
Epoch 1250/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1251/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1252/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1253/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1254/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1255/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0038
Epoch 1256/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1257/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0048
Epoch 1258/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1259/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1260/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1261/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1262/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1263/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1264/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1265/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1266/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0038
Epoch 1267/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0039
Epoch 1268/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1269/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1270/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0050
Epoch 1271/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1272/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1273/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1274/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1275/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1276/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0038
Epoch 1277/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0042
Epoch 1278/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0047
Epoch 1279/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1280/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1281/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1282/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1283/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1284/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1285/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1286/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1287/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1288/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1289/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1290/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1291/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1292/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0045
Epoch 1293/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0041
Epoch 1294/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1295/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1296/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0043
Epoch 1297/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0048
Epoch 1298/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0039
Epoch 1299/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0039
Epoch 1300/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0046
Epoch 1301/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0040
Epoch 1302/2000
1024/1024 - 0s - loss: 0.0386 - val_loss: 0.0044
Epoch 1303/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1304/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1305/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1306/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1307/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1308/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0045
Epoch 1309/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1310/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1311/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1312/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1313/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1314/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1315/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1316/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1317/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1318/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1319/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0045
Epoch 1320/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1321/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1322/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1323/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1324/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1325/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1326/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1327/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1328/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1329/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1330/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1331/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1332/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1333/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1334/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1335/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1336/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0035
Epoch 1337/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1338/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1339/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1340/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1341/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0047
Epoch 1342/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1343/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1344/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1345/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1346/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1347/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1348/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0035
Epoch 1349/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1350/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1351/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1352/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1353/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1354/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1355/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0047
Epoch 1356/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1357/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1358/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1359/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1360/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1361/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1362/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1363/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1364/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1365/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1366/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1367/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1368/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1369/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0048
Epoch 1370/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1371/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1372/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1373/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1374/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1375/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0045
Epoch 1376/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1377/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1378/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1379/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1380/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1381/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1382/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1383/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0035
Epoch 1384/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1385/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1386/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0045
Epoch 1387/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1388/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1389/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1390/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1391/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1392/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1393/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1394/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1395/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1396/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1397/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0047
Epoch 1398/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1399/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1400/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1401/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1402/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1403/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1404/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1405/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1406/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0034
Epoch 1407/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1408/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1409/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1410/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1411/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0053
Epoch 1412/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1413/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1414/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1415/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1416/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1417/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1418/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0046
Epoch 1419/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1420/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1421/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1422/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0047
Epoch 1423/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0034
Epoch 1424/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1425/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1426/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1427/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0042
Epoch 1428/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1429/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1430/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1431/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1432/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0037
Epoch 1433/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0038
Epoch 1434/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0036
Epoch 1435/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1436/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0045
Epoch 1437/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0035
Epoch 1438/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1439/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0044
Epoch 1440/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0031
Epoch 1441/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0043
Epoch 1442/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0031
Epoch 1443/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0041
Epoch 1444/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0043
Epoch 1445/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1446/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0040
Epoch 1447/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1448/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1449/2000
1024/1024 - 0s - loss: 0.0385 - val_loss: 0.0039
Epoch 1450/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1451/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0033
Epoch 1452/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1453/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1454/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1455/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0043
Epoch 1456/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0047
Epoch 1457/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1458/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1459/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1460/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1461/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1462/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1463/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1464/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1465/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1466/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1467/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1468/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1469/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1470/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1471/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1472/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1473/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1474/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1475/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1476/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1477/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1478/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1479/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0039
Epoch 1480/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0039
Epoch 1481/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1482/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1483/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1484/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1485/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1486/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0046
Epoch 1487/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1488/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1489/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1490/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1491/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1492/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1493/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1494/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1495/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1496/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1497/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0047
Epoch 1498/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1499/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1500/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1501/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0039
Epoch 1502/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0033
Epoch 1503/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1504/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1505/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1506/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1507/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1508/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0043
Epoch 1509/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0039
Epoch 1510/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1511/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0045
Epoch 1512/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0031
Epoch 1513/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1514/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1515/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1516/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1517/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1518/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1519/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0049
Epoch 1520/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1521/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1522/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1523/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1524/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1525/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1526/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1527/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1528/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1529/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1530/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1531/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0030
Epoch 1532/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0056
Epoch 1533/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0045
Epoch 1534/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1535/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0048
Epoch 1536/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1537/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1538/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1539/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1540/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1541/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1542/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0033
Epoch 1543/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0032
Epoch 1544/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1545/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1546/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1547/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1548/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0042
Epoch 1549/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1550/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0043
Epoch 1551/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1552/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1553/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0030
Epoch 1554/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1555/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0041
Epoch 1556/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1557/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0039
Epoch 1558/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1559/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0029
Epoch 1560/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1561/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1562/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1563/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1564/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0037
Epoch 1565/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0031
Epoch 1566/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1567/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1568/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0045
Epoch 1569/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1570/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0033
Epoch 1571/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0030
Epoch 1572/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1573/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0049
Epoch 1574/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0033
Epoch 1575/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0036
Epoch 1576/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1577/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1578/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1579/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1580/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0050
Epoch 1581/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1582/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0040
Epoch 1583/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1584/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0032
Epoch 1585/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0035
Epoch 1586/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0038
Epoch 1587/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0045
Epoch 1588/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0049
Epoch 1589/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0044
Epoch 1590/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0043
Epoch 1591/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0033
Epoch 1592/2000
1024/1024 - 0s - loss: 0.0384 - val_loss: 0.0034
Epoch 1593/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1594/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1595/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1596/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1597/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0041
Epoch 1598/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0041
Epoch 1599/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0041
Epoch 1600/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1601/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0045
Epoch 1602/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0037
Epoch 1603/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0037
Epoch 1604/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1605/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1606/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1607/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0037
Epoch 1608/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0028
Epoch 1609/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1610/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1611/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0034
Epoch 1612/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1613/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0034
Epoch 1614/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1615/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1616/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0045
Epoch 1617/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1618/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1619/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1620/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0046
Epoch 1621/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0037
Epoch 1622/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0041
Epoch 1623/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1624/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0049
Epoch 1625/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1626/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1627/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1628/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1629/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1630/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1631/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1632/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1633/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0043
Epoch 1634/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1635/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1636/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0033
Epoch 1637/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1638/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1639/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0034
Epoch 1640/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0032
Epoch 1641/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0034
Epoch 1642/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0030
Epoch 1643/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0045
Epoch 1644/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1645/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1646/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1647/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0043
Epoch 1648/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1649/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0046
Epoch 1650/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0050
Epoch 1651/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0045
Epoch 1652/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0040
Epoch 1653/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0044
Epoch 1654/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0052
Epoch 1655/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0041
Epoch 1656/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0051
Epoch 1657/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0054
Epoch 1658/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0034
Epoch 1659/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1660/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0036
Epoch 1661/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0035
Epoch 1662/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1663/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0043
Epoch 1664/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0043
Epoch 1665/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1666/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1667/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0037
Epoch 1668/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1669/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0042
Epoch 1670/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1671/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0038
Epoch 1672/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1673/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0043
Epoch 1674/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0040
Epoch 1675/2000
1024/1024 - 0s - loss: 0.0383 - val_loss: 0.0039
Epoch 1676/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1677/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0035
Epoch 1678/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0045
Epoch 1679/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1680/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0050
Epoch 1681/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0038
Epoch 1682/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0034
Epoch 1683/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1684/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0039
Epoch 1685/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0035
Epoch 1686/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0030
Epoch 1687/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1688/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1689/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1690/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1691/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1692/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0043
Epoch 1693/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0043
Epoch 1694/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0037
Epoch 1695/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0040
Epoch 1696/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1697/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0059
Epoch 1698/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1699/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0038
Epoch 1700/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1701/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1702/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1703/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1704/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1705/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0033
Epoch 1706/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1707/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1708/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1709/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0046
Epoch 1710/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0046
Epoch 1711/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1712/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1713/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0037
Epoch 1714/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0049
Epoch 1715/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0047
Epoch 1716/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0038
Epoch 1717/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1718/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1719/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0038
Epoch 1720/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0037
Epoch 1721/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0040
Epoch 1722/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1723/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0040
Epoch 1724/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1725/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0043
Epoch 1726/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0039
Epoch 1727/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0044
Epoch 1728/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0036
Epoch 1729/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0041
Epoch 1730/2000
1024/1024 - 0s - loss: 0.0382 - val_loss: 0.0042
Epoch 1731/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1732/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1733/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0045
Epoch 1734/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0040
Epoch 1735/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1736/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0042
Epoch 1737/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0042
Epoch 1738/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0039
Epoch 1739/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0040
Epoch 1740/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0047
Epoch 1741/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0036
Epoch 1742/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0044
Epoch 1743/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1744/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0038
Epoch 1745/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0049
Epoch 1746/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0047
Epoch 1747/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0037
Epoch 1748/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0035
Epoch 1749/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0051
Epoch 1750/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0046
Epoch 1751/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1752/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0051
Epoch 1753/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0050
Epoch 1754/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0046
Epoch 1755/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0042
Epoch 1756/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0039
Epoch 1757/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0050
Epoch 1758/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0042
Epoch 1759/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0039
Epoch 1760/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0041
Epoch 1761/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0043
Epoch 1762/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0042
Epoch 1763/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0045
Epoch 1764/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0044
Epoch 1765/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0036
Epoch 1766/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0048
Epoch 1767/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0035
Epoch 1768/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0043
Epoch 1769/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0044
Epoch 1770/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0035
Epoch 1771/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0038
Epoch 1772/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0035
Epoch 1773/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0045
Epoch 1774/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0043
Epoch 1775/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0044
Epoch 1776/2000
1024/1024 - 0s - loss: 0.0381 - val_loss: 0.0039
Epoch 1777/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0042
Epoch 1778/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0043
Epoch 1779/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0044
Epoch 1780/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0040
Epoch 1781/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0043
Epoch 1782/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0033
Epoch 1783/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0050
Epoch 1784/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0047
Epoch 1785/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0037
Epoch 1786/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0042
Epoch 1787/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0046
Epoch 1788/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0036
Epoch 1789/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0043
Epoch 1790/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0049
Epoch 1791/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0050
Epoch 1792/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0040
Epoch 1793/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0040
Epoch 1794/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0042
Epoch 1795/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0040
Epoch 1796/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0044
Epoch 1797/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0041
Epoch 1798/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0033
Epoch 1799/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0052
Epoch 1800/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0046
Epoch 1801/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0030
Epoch 1802/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0053
Epoch 1803/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0041
Epoch 1804/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0045
Epoch 1805/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0042
Epoch 1806/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0039
Epoch 1807/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0045
Epoch 1808/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0038
Epoch 1809/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0045
Epoch 1810/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0038
Epoch 1811/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0038
Epoch 1812/2000
1024/1024 - 0s - loss: 0.0380 - val_loss: 0.0040
Epoch 1813/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0040
Epoch 1814/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0039
Epoch 1815/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1816/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1817/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0043
Epoch 1818/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0036
Epoch 1819/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1820/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1821/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1822/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0050
Epoch 1823/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0041
Epoch 1824/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0034
Epoch 1825/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0043
Epoch 1826/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0051
Epoch 1827/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0035
Epoch 1828/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0041
Epoch 1829/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0043
Epoch 1830/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0039
Epoch 1831/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0048
Epoch 1832/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0038
Epoch 1833/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0040
Epoch 1834/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0036
Epoch 1835/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0042
Epoch 1836/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0041
Epoch 1837/2000
1024/1024 - 0s - loss: 0.0379 - val_loss: 0.0033
Epoch 1838/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0048
Epoch 1839/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0038
Epoch 1840/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0037
Epoch 1841/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0040
Epoch 1842/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0040
Epoch 1843/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0047
Epoch 1844/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0041
Epoch 1845/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0040
Epoch 1846/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0042
Epoch 1847/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0042
Epoch 1848/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0036
Epoch 1849/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0045
Epoch 1850/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0036
Epoch 1851/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0041
Epoch 1852/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0033
Epoch 1853/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0042
Epoch 1854/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0045
Epoch 1855/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0047
Epoch 1856/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0036
Epoch 1857/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0037
Epoch 1858/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0044
Epoch 1859/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0033
Epoch 1860/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0040
Epoch 1861/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0049
Epoch 1862/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0035
Epoch 1863/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0036
Epoch 1864/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0048
Epoch 1865/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0038
Epoch 1866/2000
1024/1024 - 0s - loss: 0.0378 - val_loss: 0.0050
Epoch 1867/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0037
Epoch 1868/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0040
Epoch 1869/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1870/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1871/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0044
Epoch 1872/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0042
Epoch 1873/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0047
Epoch 1874/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0039
Epoch 1875/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0041
Epoch 1876/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0039
Epoch 1877/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1878/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0034
Epoch 1879/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0045
Epoch 1880/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0040
Epoch 1881/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0040
Epoch 1882/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1883/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0039
Epoch 1884/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0039
Epoch 1885/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0043
Epoch 1886/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1887/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0037
Epoch 1888/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0041
Epoch 1889/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0046
Epoch 1890/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0035
Epoch 1891/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0042
Epoch 1892/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0047
Epoch 1893/2000
1024/1024 - 0s - loss: 0.0377 - val_loss: 0.0038
Epoch 1894/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0037
Epoch 1895/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0039
Epoch 1896/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0038
Epoch 1897/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0039
Epoch 1898/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0037
Epoch 1899/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0038
Epoch 1900/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0040
Epoch 1901/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0036
Epoch 1902/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0039
Epoch 1903/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0041
Epoch 1904/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0040
Epoch 1905/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0032
Epoch 1906/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0042
Epoch 1907/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0044
Epoch 1908/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0036
Epoch 1909/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0043
Epoch 1910/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0035
Epoch 1911/2000
1024/1024 - 0s - loss: 0.0376 - val_loss: 0.0037
Epoch 1912/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0040
Epoch 1913/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0041
Epoch 1914/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0038
Epoch 1915/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0033
Epoch 1916/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0035
Epoch 1917/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0037
Epoch 1918/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0033
Epoch 1919/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0037
Epoch 1920/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0035
Epoch 1921/2000
1024/1024 - 0s - loss: 0.0375 - val_loss: 0.0033
Epoch 1922/2000
1024/1024 - 0s - loss: 0.0374 - val_loss: 0.0037
Epoch 1923/2000
1024/1024 - 0s - loss: 0.0374 - val_loss: 0.0031
Epoch 1924/2000
1024/1024 - 0s - loss: 0.0374 - val_loss: 0.0041
Epoch 1925/2000
1024/1024 - 0s - loss: 0.0374 - val_loss: 0.0034
Epoch 1926/2000
1024/1024 - 0s - loss: 0.0374 - val_loss: 0.0032
Epoch 1927/2000
1024/1024 - 0s - loss: 0.0373 - val_loss: 0.0035
Epoch 1928/2000
1024/1024 - 0s - loss: 0.0373 - val_loss: 0.0037
Epoch 1929/2000
1024/1024 - 0s - loss: 0.0373 - val_loss: 0.0033
Epoch 1930/2000
1024/1024 - 0s - loss: 0.0373 - val_loss: 0.0032
Epoch 1931/2000
1024/1024 - 0s - loss: 0.0372 - val_loss: 0.0034
Epoch 1932/2000
1024/1024 - 0s - loss: 0.0372 - val_loss: 0.0036
Epoch 1933/2000
1024/1024 - 0s - loss: 0.0371 - val_loss: 0.0039
Epoch 1934/2000
1024/1024 - 0s - loss: 0.0370 - val_loss: 0.0038
Epoch 1935/2000
1024/1024 - 0s - loss: 0.0369 - val_loss: 0.0037
Epoch 1936/2000
1024/1024 - 0s - loss: 0.0368 - val_loss: 0.0031
Epoch 1937/2000
1024/1024 - 0s - loss: 0.0367 - val_loss: 0.0030
Epoch 1938/2000
1024/1024 - 0s - loss: 0.0365 - val_loss: 0.0033
Epoch 1939/2000
1024/1024 - 0s - loss: 0.0362 - val_loss: 0.0031
Epoch 1940/2000
1024/1024 - 0s - loss: 0.0357 - val_loss: 0.0032
Epoch 1941/2000
1024/1024 - 0s - loss: 0.0348 - val_loss: 0.0028
Epoch 1942/2000
1024/1024 - 0s - loss: 0.0330 - val_loss: 0.0020
Epoch 1943/2000
1024/1024 - 0s - loss: 0.0281 - val_loss: 6.3597e-04
Epoch 1944/2000
1024/1024 - 0s - loss: 0.0143 - val_loss: 5.4000e-05
Epoch 1945/2000
1024/1024 - 0s - loss: 0.0027 - val_loss: 3.1993e-04
Epoch 1946/2000
1024/1024 - 0s - loss: 7.2144e-04 - val_loss: 3.7396e-04
Epoch 1947/2000
1024/1024 - 0s - loss: 4.3508e-04 - val_loss: 3.8009e-04
Epoch 1948/2000
1024/1024 - 0s - loss: 3.4063e-04 - val_loss: 3.7875e-04
Epoch 1949/2000
1024/1024 - 0s - loss: 2.8817e-04 - val_loss: 3.7651e-04
Epoch 1950/2000
1024/1024 - 0s - loss: 2.5286e-04 - val_loss: 3.7415e-04
Epoch 1951/2000
1024/1024 - 0s - loss: 2.2627e-04 - val_loss: 3.7174e-04
Epoch 1952/2000
1024/1024 - 0s - loss: 2.0481e-04 - val_loss: 3.6939e-04
Epoch 1953/2000
1024/1024 - 0s - loss: 1.8739e-04 - val_loss: 3.6700e-04
Epoch 1954/2000
1024/1024 - 0s - loss: 1.7277e-04 - val_loss: 3.6467e-04
Epoch 1955/2000
1024/1024 - 0s - loss: 1.6037e-04 - val_loss: 3.6236e-04
Epoch 1956/2000
1024/1024 - 0s - loss: 1.4982e-04 - val_loss: 3.6002e-04
Epoch 1957/2000
1024/1024 - 0s - loss: 1.4046e-04 - val_loss: 3.5775e-04
Epoch 1958/2000
1024/1024 - 0s - loss: 1.3231e-04 - val_loss: 3.5550e-04
Epoch 1959/2000
1024/1024 - 0s - loss: 1.2511e-04 - val_loss: 3.5325e-04
Epoch 1960/2000
1024/1024 - 0s - loss: 1.1857e-04 - val_loss: 3.5109e-04
Epoch 1961/2000
1024/1024 - 0s - loss: 1.1282e-04 - val_loss: 3.4891e-04
Epoch 1962/2000
1024/1024 - 0s - loss: 1.0754e-04 - val_loss: 3.4678e-04
Epoch 1963/2000
1024/1024 - 0s - loss: 1.0276e-04 - val_loss: 3.4469e-04
Epoch 1964/2000
1024/1024 - 0s - loss: 9.8397e-05 - val_loss: 3.4262e-04
Epoch 1965/2000
1024/1024 - 0s - loss: 9.4350e-05 - val_loss: 3.4060e-04
Epoch 1966/2000
1024/1024 - 0s - loss: 9.0649e-05 - val_loss: 3.3863e-04
Epoch 1967/2000
1024/1024 - 0s - loss: 8.7255e-05 - val_loss: 3.3666e-04
Epoch 1968/2000
1024/1024 - 0s - loss: 8.4072e-05 - val_loss: 3.3474e-04
Epoch 1969/2000
1024/1024 - 0s - loss: 8.1133e-05 - val_loss: 3.3284e-04
Epoch 1970/2000
1024/1024 - 0s - loss: 7.8394e-05 - val_loss: 3.3095e-04
Epoch 1971/2000
1024/1024 - 0s - loss: 7.5799e-05 - val_loss: 3.2912e-04
Epoch 1972/2000
1024/1024 - 0s - loss: 7.3386e-05 - val_loss: 3.2733e-04
Epoch 1973/2000
1024/1024 - 0s - loss: 7.1108e-05 - val_loss: 3.2558e-04
Epoch 1974/2000
1024/1024 - 0s - loss: 6.9006e-05 - val_loss: 3.2381e-04
Epoch 1975/2000
1024/1024 - 0s - loss: 6.6993e-05 - val_loss: 3.2208e-04
Epoch 1976/2000
1024/1024 - 0s - loss: 6.5096e-05 - val_loss: 3.2036e-04
Epoch 1977/2000
1024/1024 - 0s - loss: 6.3263e-05 - val_loss: 3.1872e-04
Epoch 1978/2000
1024/1024 - 0s - loss: 6.1588e-05 - val_loss: 3.1706e-04
Epoch 1979/2000
1024/1024 - 0s - loss: 5.9968e-05 - val_loss: 3.1542e-04
Epoch 1980/2000
1024/1024 - 0s - loss: 5.8412e-05 - val_loss: 3.1383e-04
Epoch 1981/2000
1024/1024 - 0s - loss: 5.6938e-05 - val_loss: 3.1228e-04
Epoch 1982/2000
1024/1024 - 0s - loss: 5.5569e-05 - val_loss: 3.1069e-04
Epoch 1983/2000
1024/1024 - 0s - loss: 5.4220e-05 - val_loss: 3.0916e-04
Epoch 1984/2000
1024/1024 - 0s - loss: 5.2947e-05 - val_loss: 3.0764e-04
Epoch 1985/2000
1024/1024 - 0s - loss: 5.1718e-05 - val_loss: 3.0616e-04
Epoch 1986/2000
1024/1024 - 0s - loss: 5.0552e-05 - val_loss: 3.0470e-04
Epoch 1987/2000
1024/1024 - 0s - loss: 4.9439e-05 - val_loss: 3.0323e-04
Epoch 1988/2000
1024/1024 - 0s - loss: 4.8361e-05 - val_loss: 3.0180e-04
Epoch 1989/2000
1024/1024 - 0s - loss: 4.7333e-05 - val_loss: 3.0036e-04
Epoch 1990/2000
1024/1024 - 0s - loss: 4.6337e-05 - val_loss: 2.9896e-04
Epoch 1991/2000
1024/1024 - 0s - loss: 4.5385e-05 - val_loss: 2.9757e-04
Epoch 1992/2000
1024/1024 - 0s - loss: 4.4470e-05 - val_loss: 2.9619e-04
Epoch 1993/2000
1024/1024 - 0s - loss: 4.3580e-05 - val_loss: 2.9483e-04
Epoch 1994/2000
1024/1024 - 0s - loss: 4.2722e-05 - val_loss: 2.9350e-04
Epoch 1995/2000
1024/1024 - 0s - loss: 4.1907e-05 - val_loss: 2.9216e-04
Epoch 1996/2000
1024/1024 - 0s - loss: 4.1112e-05 - val_loss: 2.9084e-04
Epoch 1997/2000
1024/1024 - 0s - loss: 4.0335e-05 - val_loss: 2.8955e-04
Epoch 1998/2000
1024/1024 - 0s - loss: 3.9592e-05 - val_loss: 2.8828e-04
Epoch 1999/2000
1024/1024 - 0s - loss: 3.8882e-05 - val_loss: 2.8699e-04
Epoch 2000/2000
1024/1024 - 0s - loss: 3.8179e-05 - val_loss: 2.8574e-04
2024-03-29 17:04:18.938543: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 1024 samples, validate on 1024 samples
Epoch 1/1000

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0129e-06 - val_loss: 1.0057e-06
Epoch 2/1000

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0042e-06 - val_loss: 1.0004e-06
Epoch 3/1000

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9992e-07 - val_loss: 9.9674e-07
Epoch 4/1000

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9647e-07 - val_loss: 9.9348e-07
Epoch 5/1000

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9328e-07 - val_loss: 9.9039e-07
Epoch 6/1000

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9017e-07 - val_loss: 9.8733e-07
Epoch 7/1000

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.8712e-07 - val_loss: 9.8433e-07
Epoch 8/1000

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.8419e-07 - val_loss: 9.8140e-07
Epoch 9/1000

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.8128e-07 - val_loss: 9.7853e-07
Epoch 10/1000

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7844e-07 - val_loss: 9.7572e-07
Epoch 11/1000

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7567e-07 - val_loss: 9.7298e-07
Epoch 12/1000

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7293e-07 - val_loss: 9.7024e-07
Epoch 13/1000

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7024e-07 - val_loss: 9.6762e-07
Epoch 14/1000

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6760e-07 - val_loss: 9.6499e-07
Epoch 15/1000

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6501e-07 - val_loss: 9.6243e-07
Epoch 16/1000

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6254e-07 - val_loss: 9.6001e-07
Epoch 17/1000

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6007e-07 - val_loss: 9.5750e-07
Epoch 18/1000

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5765e-07 - val_loss: 9.5514e-07
Epoch 19/1000

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5526e-07 - val_loss: 9.5278e-07
Epoch 20/1000

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5295e-07 - val_loss: 9.5052e-07
Epoch 21/1000

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5066e-07 - val_loss: 9.4822e-07
Epoch 22/1000

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4835e-07 - val_loss: 9.4590e-07
Epoch 23/1000

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4616e-07 - val_loss: 9.4377e-07
Epoch 24/1000

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4397e-07 - val_loss: 9.4161e-07
Epoch 25/1000

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4184e-07 - val_loss: 9.3950e-07
Epoch 26/1000

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3975e-07 - val_loss: 9.3741e-07
Epoch 27/1000

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3771e-07 - val_loss: 9.3540e-07
Epoch 28/1000

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3566e-07 - val_loss: 9.3336e-07
Epoch 29/1000

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3367e-07 - val_loss: 9.3141e-07
Epoch 30/1000

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3170e-07 - val_loss: 9.2940e-07
Epoch 31/1000

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2971e-07 - val_loss: 9.2749e-07
Epoch 32/1000

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2781e-07 - val_loss: 9.2559e-07
Epoch 33/1000

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2593e-07 - val_loss: 9.2376e-07
Epoch 34/1000

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2409e-07 - val_loss: 9.2188e-07
Epoch 35/1000

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2222e-07 - val_loss: 9.2004e-07
Epoch 36/1000

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2043e-07 - val_loss: 9.1828e-07
Epoch 37/1000

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1863e-07 - val_loss: 9.1648e-07
Epoch 38/1000

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1686e-07 - val_loss: 9.1471e-07
Epoch 39/1000

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1506e-07 - val_loss: 9.1296e-07
Epoch 40/1000

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1336e-07 - val_loss: 9.1126e-07
Epoch 41/1000

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1165e-07 - val_loss: 9.0952e-07
Epoch 42/1000

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0996e-07 - val_loss: 9.0790e-07
Epoch 43/1000

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0829e-07 - val_loss: 9.0623e-07
Epoch 44/1000

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0664e-07 - val_loss: 9.0454e-07
Epoch 45/1000

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0497e-07 - val_loss: 9.0292e-07
Epoch 46/1000

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0332e-07 - val_loss: 9.0123e-07
Epoch 47/1000

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0167e-07 - val_loss: 8.9963e-07
Epoch 48/1000

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0010e-07 - val_loss: 8.9807e-07
Epoch 49/1000

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9848e-07 - val_loss: 8.9647e-07
Epoch 50/1000

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9692e-07 - val_loss: 8.9494e-07
Epoch 51/1000

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9537e-07 - val_loss: 8.9331e-07
Epoch 52/1000

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9380e-07 - val_loss: 8.9180e-07
Epoch 53/1000

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9221e-07 - val_loss: 8.9023e-07
Epoch 54/1000

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9067e-07 - val_loss: 8.8864e-07
Epoch 55/1000

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8911e-07 - val_loss: 8.8711e-07
Epoch 56/1000

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8759e-07 - val_loss: 8.8560e-07
Epoch 57/1000

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8608e-07 - val_loss: 8.8414e-07
Epoch 58/1000

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8457e-07 - val_loss: 8.8259e-07
Epoch 59/1000

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8304e-07 - val_loss: 8.8110e-07
Epoch 60/1000

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8151e-07 - val_loss: 8.7953e-07
Epoch 61/1000

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8000e-07 - val_loss: 8.7805e-07
Epoch 62/1000

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7850e-07 - val_loss: 8.7656e-07
Epoch 63/1000

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7702e-07 - val_loss: 8.7506e-07
Epoch 64/1000

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7552e-07 - val_loss: 8.7358e-07
Epoch 65/1000

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7407e-07 - val_loss: 8.7207e-07
Epoch 66/1000

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7256e-07 - val_loss: 8.7064e-07
Epoch 67/1000

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7105e-07 - val_loss: 8.6903e-07
Epoch 68/1000

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6953e-07 - val_loss: 8.6758e-07
Epoch 69/1000

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6805e-07 - val_loss: 8.6613e-07
Epoch 70/1000

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6662e-07 - val_loss: 8.6467e-07
Epoch 71/1000

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6509e-07 - val_loss: 8.6320e-07
Epoch 72/1000

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6366e-07 - val_loss: 8.6173e-07
Epoch 73/1000

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6214e-07 - val_loss: 8.6017e-07
Epoch 74/1000

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6063e-07 - val_loss: 8.5870e-07
Epoch 75/1000

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5917e-07 - val_loss: 8.5722e-07
Epoch 76/1000

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5767e-07 - val_loss: 8.5571e-07
Epoch 77/1000

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5619e-07 - val_loss: 8.5425e-07
Epoch 78/1000

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5471e-07 - val_loss: 8.5278e-07
Epoch 79/1000

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5321e-07 - val_loss: 8.5126e-07
Epoch 80/1000

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5169e-07 - val_loss: 8.4979e-07
Epoch 81/1000

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5018e-07 - val_loss: 8.4828e-07
Epoch 82/1000

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4872e-07 - val_loss: 8.4677e-07
Epoch 83/1000

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4720e-07 - val_loss: 8.4527e-07
Epoch 84/1000

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4570e-07 - val_loss: 8.4381e-07
Epoch 85/1000

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4423e-07 - val_loss: 8.4224e-07
Epoch 86/1000

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4264e-07 - val_loss: 8.4076e-07
Epoch 87/1000

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4118e-07 - val_loss: 8.3924e-07
Epoch 88/1000

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3962e-07 - val_loss: 8.3774e-07
Epoch 89/1000

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3814e-07 - val_loss: 8.3621e-07
Epoch 90/1000

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3660e-07 - val_loss: 8.3465e-07
Epoch 91/1000

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3508e-07 - val_loss: 8.3311e-07
Epoch 92/1000

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3349e-07 - val_loss: 8.3158e-07
Epoch 93/1000

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3197e-07 - val_loss: 8.3006e-07
Epoch 94/1000

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3046e-07 - val_loss: 8.2850e-07
Epoch 95/1000

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2890e-07 - val_loss: 8.2699e-07
Epoch 96/1000

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2735e-07 - val_loss: 8.2541e-07
Epoch 97/1000

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2578e-07 - val_loss: 8.2382e-07
Epoch 98/1000

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2419e-07 - val_loss: 8.2227e-07
Epoch 99/1000

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2265e-07 - val_loss: 8.2069e-07
Epoch 100/1000

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2104e-07 - val_loss: 8.1916e-07
Epoch 101/1000

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1951e-07 - val_loss: 8.1755e-07
Epoch 102/1000

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1793e-07 - val_loss: 8.1598e-07
Epoch 103/1000

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1632e-07 - val_loss: 8.1435e-07
Epoch 104/1000

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1470e-07 - val_loss: 8.1276e-07
Epoch 105/1000

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1312e-07 - val_loss: 8.1119e-07
Epoch 106/1000

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1150e-07 - val_loss: 8.0961e-07
Epoch 107/1000

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0994e-07 - val_loss: 8.0794e-07
Epoch 108/1000

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0831e-07 - val_loss: 8.0626e-07
Epoch 109/1000

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0663e-07 - val_loss: 8.0470e-07
Epoch 110/1000

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0502e-07 - val_loss: 8.0306e-07
Epoch 111/1000

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0340e-07 - val_loss: 8.0145e-07
Epoch 112/1000

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0177e-07 - val_loss: 7.9984e-07
Epoch 113/1000

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0015e-07 - val_loss: 7.9822e-07
Epoch 114/1000

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9845e-07 - val_loss: 7.9645e-07
Epoch 115/1000

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9678e-07 - val_loss: 7.9482e-07
Epoch 116/1000

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9513e-07 - val_loss: 7.9319e-07
Epoch 117/1000

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9348e-07 - val_loss: 7.9154e-07
Epoch 118/1000

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9182e-07 - val_loss: 7.8982e-07
Epoch 119/1000

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9012e-07 - val_loss: 7.8816e-07
Epoch 120/1000

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8842e-07 - val_loss: 7.8645e-07
Epoch 121/1000

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8673e-07 - val_loss: 7.8478e-07
Epoch 122/1000

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8506e-07 - val_loss: 7.8307e-07
Epoch 123/1000

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8336e-07 - val_loss: 7.8144e-07
Epoch 124/1000

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8168e-07 - val_loss: 7.7964e-07
Epoch 125/1000

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7989e-07 - val_loss: 7.7795e-07
Epoch 126/1000

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7818e-07 - val_loss: 7.7621e-07
Epoch 127/1000

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7646e-07 - val_loss: 7.7456e-07
Epoch 128/1000

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7477e-07 - val_loss: 7.7278e-07
Epoch 129/1000

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7302e-07 - val_loss: 7.7110e-07
Epoch 130/1000

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7127e-07 - val_loss: 7.6929e-07
Epoch 131/1000

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6949e-07 - val_loss: 7.6758e-07
Epoch 132/1000

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6776e-07 - val_loss: 7.6577e-07
Epoch 133/1000

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6601e-07 - val_loss: 7.6403e-07
Epoch 134/1000

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6426e-07 - val_loss: 7.6228e-07
Epoch 135/1000

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6243e-07 - val_loss: 7.6050e-07
Epoch 136/1000

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6066e-07 - val_loss: 7.5866e-07
Epoch 137/1000

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5889e-07 - val_loss: 7.5689e-07
Epoch 138/1000

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5712e-07 - val_loss: 7.5511e-07
Epoch 139/1000

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5531e-07 - val_loss: 7.5338e-07
Epoch 140/1000

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5350e-07 - val_loss: 7.5151e-07
Epoch 141/1000

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5170e-07 - val_loss: 7.4970e-07
Epoch 142/1000

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4987e-07 - val_loss: 7.4788e-07
Epoch 143/1000

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4807e-07 - val_loss: 7.4611e-07
Epoch 144/1000

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4625e-07 - val_loss: 7.4424e-07
Epoch 145/1000

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4439e-07 - val_loss: 7.4239e-07
Epoch 146/1000

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4257e-07 - val_loss: 7.4060e-07
Epoch 147/1000

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4074e-07 - val_loss: 7.3879e-07
Epoch 148/1000

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3892e-07 - val_loss: 7.3695e-07
Epoch 149/1000

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3706e-07 - val_loss: 7.3502e-07
Epoch 150/1000

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3517e-07 - val_loss: 7.3319e-07
Epoch 151/1000

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3333e-07 - val_loss: 7.3132e-07
Epoch 152/1000

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3145e-07 - val_loss: 7.2948e-07
Epoch 153/1000

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2958e-07 - val_loss: 7.2760e-07
Epoch 154/1000

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2772e-07 - val_loss: 7.2569e-07
Epoch 155/1000

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2579e-07 - val_loss: 7.2383e-07
Epoch 156/1000

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2395e-07 - val_loss: 7.2191e-07
Epoch 157/1000

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2204e-07 - val_loss: 7.2010e-07
Epoch 158/1000

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2018e-07 - val_loss: 7.1823e-07
Epoch 159/1000

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1823e-07 - val_loss: 7.1627e-07
Epoch 160/1000

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1633e-07 - val_loss: 7.1432e-07
Epoch 161/1000

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1443e-07 - val_loss: 7.1239e-07
Epoch 162/1000

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1250e-07 - val_loss: 7.1051e-07
Epoch 163/1000

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1062e-07 - val_loss: 7.0857e-07
Epoch 164/1000

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0862e-07 - val_loss: 7.0663e-07
Epoch 165/1000

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0672e-07 - val_loss: 7.0471e-07
Epoch 166/1000

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0478e-07 - val_loss: 7.0279e-07
Epoch 167/1000

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0286e-07 - val_loss: 7.0090e-07
Epoch 168/1000

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0092e-07 - val_loss: 6.9887e-07
Epoch 169/1000

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9891e-07 - val_loss: 6.9693e-07
Epoch 170/1000

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9699e-07 - val_loss: 6.9500e-07
Epoch 171/1000

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9504e-07 - val_loss: 6.9304e-07
Epoch 172/1000

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9309e-07 - val_loss: 6.9107e-07
Epoch 173/1000

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9107e-07 - val_loss: 6.8910e-07
Epoch 174/1000

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8910e-07 - val_loss: 6.8715e-07
Epoch 175/1000

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8717e-07 - val_loss: 6.8515e-07
Epoch 176/1000

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8516e-07 - val_loss: 6.8320e-07
Epoch 177/1000

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8320e-07 - val_loss: 6.8117e-07
Epoch 178/1000

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8117e-07 - val_loss: 6.7915e-07
Epoch 179/1000

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7919e-07 - val_loss: 6.7715e-07
Epoch 180/1000

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7718e-07 - val_loss: 6.7523e-07
Epoch 181/1000

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7522e-07 - val_loss: 6.7320e-07
Epoch 182/1000

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7316e-07 - val_loss: 6.7118e-07
Epoch 183/1000

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7115e-07 - val_loss: 6.6919e-07
Epoch 184/1000

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6916e-07 - val_loss: 6.6718e-07
Epoch 185/1000

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6713e-07 - val_loss: 6.6518e-07
Epoch 186/1000

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6515e-07 - val_loss: 6.6309e-07
Epoch 187/1000

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6307e-07 - val_loss: 6.6106e-07
Epoch 188/1000

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6102e-07 - val_loss: 6.5907e-07
Epoch 189/1000

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5903e-07 - val_loss: 6.5705e-07
Epoch 190/1000

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5699e-07 - val_loss: 6.5505e-07
Epoch 191/1000

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5493e-07 - val_loss: 6.5292e-07
Epoch 192/1000

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5288e-07 - val_loss: 6.5089e-07
Epoch 193/1000

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5085e-07 - val_loss: 6.4887e-07
Epoch 194/1000

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4881e-07 - val_loss: 6.4686e-07
Epoch 195/1000

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4673e-07 - val_loss: 6.4478e-07
Epoch 196/1000

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4469e-07 - val_loss: 6.4269e-07
Epoch 197/1000

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4263e-07 - val_loss: 6.4064e-07
Epoch 198/1000

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4056e-07 - val_loss: 6.3856e-07
Epoch 199/1000

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3851e-07 - val_loss: 6.3648e-07
Epoch 200/1000

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3639e-07 - val_loss: 6.3446e-07
Epoch 201/1000

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3438e-07 - val_loss: 6.3228e-07
Epoch 202/1000

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3226e-07 - val_loss: 6.3036e-07
Epoch 203/1000

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3020e-07 - val_loss: 6.2826e-07
Epoch 204/1000

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2810e-07 - val_loss: 6.2614e-07
Epoch 205/1000

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2602e-07 - val_loss: 6.2406e-07
Epoch 206/1000

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2396e-07 - val_loss: 6.2194e-07
Epoch 207/1000

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2185e-07 - val_loss: 6.1992e-07
Epoch 208/1000

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1976e-07 - val_loss: 6.1774e-07
Epoch 209/1000

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1763e-07 - val_loss: 6.1569e-07
Epoch 210/1000

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1555e-07 - val_loss: 6.1366e-07
Epoch 211/1000

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1349e-07 - val_loss: 6.1149e-07
Epoch 212/1000

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1137e-07 - val_loss: 6.0949e-07
Epoch 213/1000

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0926e-07 - val_loss: 6.0727e-07
Epoch 214/1000

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0715e-07 - val_loss: 6.0516e-07
Epoch 215/1000

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0504e-07 - val_loss: 6.0305e-07
Epoch 216/1000

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0294e-07 - val_loss: 6.0096e-07
Epoch 217/1000

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0079e-07 - val_loss: 5.9889e-07
Epoch 218/1000

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9869e-07 - val_loss: 5.9680e-07
Epoch 219/1000

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9662e-07 - val_loss: 5.9464e-07
Epoch 220/1000

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9446e-07 - val_loss: 5.9255e-07
Epoch 221/1000

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9237e-07 - val_loss: 5.9040e-07
Epoch 222/1000

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9021e-07 - val_loss: 5.8826e-07
Epoch 223/1000

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8808e-07 - val_loss: 5.8618e-07
Epoch 224/1000

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8599e-07 - val_loss: 5.8403e-07
Epoch 225/1000

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8388e-07 - val_loss: 5.8188e-07
Epoch 226/1000

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8170e-07 - val_loss: 5.7973e-07
Epoch 227/1000

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7957e-07 - val_loss: 5.7766e-07
Epoch 228/1000

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7747e-07 - val_loss: 5.7556e-07
Epoch 229/1000

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7534e-07 - val_loss: 5.7343e-07
Epoch 230/1000

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7320e-07 - val_loss: 5.7122e-07
Epoch 231/1000

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7102e-07 - val_loss: 5.6920e-07
Epoch 232/1000

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6895e-07 - val_loss: 5.6700e-07
Epoch 233/1000

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6681e-07 - val_loss: 5.6487e-07
Epoch 234/1000

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6465e-07 - val_loss: 5.6269e-07
Epoch 235/1000

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6251e-07 - val_loss: 5.6056e-07
Epoch 236/1000

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6036e-07 - val_loss: 5.5846e-07
Epoch 237/1000

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5823e-07 - val_loss: 5.5637e-07
Epoch 238/1000

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5612e-07 - val_loss: 5.5418e-07
Epoch 239/1000

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5394e-07 - val_loss: 5.5204e-07
Epoch 240/1000

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5180e-07 - val_loss: 5.4990e-07
Epoch 241/1000

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4970e-07 - val_loss: 5.4779e-07
Epoch 242/1000

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4753e-07 - val_loss: 5.4564e-07
Epoch 243/1000

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4539e-07 - val_loss: 5.4343e-07
Epoch 244/1000

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4324e-07 - val_loss: 5.4133e-07
Epoch 245/1000

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4110e-07 - val_loss: 5.3919e-07
Epoch 246/1000

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3897e-07 - val_loss: 5.3706e-07
Epoch 247/1000

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3682e-07 - val_loss: 5.3493e-07
Epoch 248/1000

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3467e-07 - val_loss: 5.3281e-07
Epoch 249/1000

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3253e-07 - val_loss: 5.3066e-07
Epoch 250/1000

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3039e-07 - val_loss: 5.2854e-07
Epoch 251/1000

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2827e-07 - val_loss: 5.2633e-07
Epoch 252/1000

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2608e-07 - val_loss: 5.2425e-07
Epoch 253/1000

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2398e-07 - val_loss: 5.2205e-07
Epoch 254/1000

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2181e-07 - val_loss: 5.1998e-07
Epoch 255/1000

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1968e-07 - val_loss: 5.1780e-07
Epoch 256/1000

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1754e-07 - val_loss: 5.1567e-07
Epoch 257/1000

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1539e-07 - val_loss: 5.1354e-07
Epoch 258/1000

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1327e-07 - val_loss: 5.1145e-07
Epoch 259/1000

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1113e-07 - val_loss: 5.0935e-07
Epoch 260/1000

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0898e-07 - val_loss: 5.0714e-07
Epoch 261/1000

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0684e-07 - val_loss: 5.0503e-07
Epoch 262/1000

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0473e-07 - val_loss: 5.0284e-07
Epoch 263/1000

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0258e-07 - val_loss: 5.0075e-07
Epoch 264/1000

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0043e-07 - val_loss: 4.9859e-07
Epoch 265/1000

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9831e-07 - val_loss: 4.9645e-07
Epoch 266/1000

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9619e-07 - val_loss: 4.9437e-07
Epoch 267/1000

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9406e-07 - val_loss: 4.9227e-07
Epoch 268/1000

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9193e-07 - val_loss: 4.9008e-07
Epoch 269/1000

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8979e-07 - val_loss: 4.8793e-07
Epoch 270/1000

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8765e-07 - val_loss: 4.8587e-07
Epoch 271/1000

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8557e-07 - val_loss: 4.8377e-07
Epoch 272/1000

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8344e-07 - val_loss: 4.8165e-07
Epoch 273/1000

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8129e-07 - val_loss: 4.7951e-07
Epoch 274/1000

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7916e-07 - val_loss: 4.7741e-07
Epoch 275/1000

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7710e-07 - val_loss: 4.7524e-07
Epoch 276/1000

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7498e-07 - val_loss: 4.7321e-07
Epoch 277/1000

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7284e-07 - val_loss: 4.7104e-07
Epoch 278/1000

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7072e-07 - val_loss: 4.6895e-07
Epoch 279/1000

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6863e-07 - val_loss: 4.6685e-07
Epoch 280/1000

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6652e-07 - val_loss: 4.6477e-07
Epoch 281/1000

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6442e-07 - val_loss: 4.6264e-07
Epoch 282/1000

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6230e-07 - val_loss: 4.6058e-07
Epoch 283/1000

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6020e-07 - val_loss: 4.5847e-07
Epoch 284/1000

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5813e-07 - val_loss: 4.5638e-07
Epoch 285/1000

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5604e-07 - val_loss: 4.5431e-07
Epoch 286/1000

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5392e-07 - val_loss: 4.5219e-07
Epoch 287/1000

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5183e-07 - val_loss: 4.5010e-07
Epoch 288/1000

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4975e-07 - val_loss: 4.4802e-07
Epoch 289/1000

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4769e-07 - val_loss: 4.4595e-07
Epoch 290/1000

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4559e-07 - val_loss: 4.4387e-07
Epoch 291/1000

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4349e-07 - val_loss: 4.4176e-07
Epoch 292/1000

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4143e-07 - val_loss: 4.3971e-07
Epoch 293/1000

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3939e-07 - val_loss: 4.3762e-07
Epoch 294/1000

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3728e-07 - val_loss: 4.3557e-07
Epoch 295/1000

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3521e-07 - val_loss: 4.3349e-07
Epoch 296/1000

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3314e-07 - val_loss: 4.3142e-07
Epoch 297/1000

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3109e-07 - val_loss: 4.2939e-07
Epoch 298/1000

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2904e-07 - val_loss: 4.2738e-07
Epoch 299/1000

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2697e-07 - val_loss: 4.2527e-07
Epoch 300/1000

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2493e-07 - val_loss: 4.2322e-07
Epoch 301/1000

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2288e-07 - val_loss: 4.2121e-07
Epoch 302/1000

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2086e-07 - val_loss: 4.1917e-07
Epoch 303/1000

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1880e-07 - val_loss: 4.1713e-07
Epoch 304/1000

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1674e-07 - val_loss: 4.1511e-07
Epoch 305/1000

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1477e-07 - val_loss: 4.1299e-07
Epoch 306/1000

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1266e-07 - val_loss: 4.1105e-07
Epoch 307/1000

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1069e-07 - val_loss: 4.0904e-07
Epoch 308/1000

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0866e-07 - val_loss: 4.0697e-07
Epoch 309/1000

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0663e-07 - val_loss: 4.0497e-07
Epoch 310/1000

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0460e-07 - val_loss: 4.0298e-07
Epoch 311/1000

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0261e-07 - val_loss: 4.0100e-07
Epoch 312/1000

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0062e-07 - val_loss: 3.9895e-07
Epoch 313/1000

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9858e-07 - val_loss: 3.9700e-07
Epoch 314/1000

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9662e-07 - val_loss: 3.9497e-07
Epoch 315/1000

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9462e-07 - val_loss: 3.9297e-07
Epoch 316/1000

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9260e-07 - val_loss: 3.9104e-07
Epoch 317/1000

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9062e-07 - val_loss: 3.8903e-07
Epoch 318/1000

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8866e-07 - val_loss: 3.8706e-07
Epoch 319/1000

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8667e-07 - val_loss: 3.8505e-07
Epoch 320/1000

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8470e-07 - val_loss: 3.8310e-07
Epoch 321/1000

Epoch 00321: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8274e-07 - val_loss: 3.8112e-07
Epoch 322/1000

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8075e-07 - val_loss: 3.7919e-07
Epoch 323/1000

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7880e-07 - val_loss: 3.7724e-07
Epoch 324/1000

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7685e-07 - val_loss: 3.7529e-07
Epoch 325/1000

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7492e-07 - val_loss: 3.7335e-07
Epoch 326/1000

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7297e-07 - val_loss: 3.7137e-07
Epoch 327/1000

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7100e-07 - val_loss: 3.6944e-07
Epoch 328/1000

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6910e-07 - val_loss: 3.6748e-07
Epoch 329/1000

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6714e-07 - val_loss: 3.6559e-07
Epoch 330/1000

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6522e-07 - val_loss: 3.6372e-07
Epoch 331/1000

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6331e-07 - val_loss: 3.6175e-07
Epoch 332/1000

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6137e-07 - val_loss: 3.5984e-07
Epoch 333/1000

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5947e-07 - val_loss: 3.5795e-07
Epoch 334/1000

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5758e-07 - val_loss: 3.5606e-07
Epoch 335/1000

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5568e-07 - val_loss: 3.5413e-07
Epoch 336/1000

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5375e-07 - val_loss: 3.5224e-07
Epoch 337/1000

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5187e-07 - val_loss: 3.5036e-07
Epoch 338/1000

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4999e-07 - val_loss: 3.4852e-07
Epoch 339/1000

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4812e-07 - val_loss: 3.4662e-07
Epoch 340/1000

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4624e-07 - val_loss: 3.4474e-07
Epoch 341/1000

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4436e-07 - val_loss: 3.4287e-07
Epoch 342/1000

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4249e-07 - val_loss: 3.4103e-07
Epoch 343/1000

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4066e-07 - val_loss: 3.3918e-07
Epoch 344/1000

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3881e-07 - val_loss: 3.3730e-07
Epoch 345/1000

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3692e-07 - val_loss: 3.3547e-07
Epoch 346/1000

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3508e-07 - val_loss: 3.3364e-07
Epoch 347/1000

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3326e-07 - val_loss: 3.3183e-07
Epoch 348/1000

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3144e-07 - val_loss: 3.3001e-07
Epoch 349/1000

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2964e-07 - val_loss: 3.2814e-07
Epoch 350/1000

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2777e-07 - val_loss: 3.2633e-07
Epoch 351/1000

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2596e-07 - val_loss: 3.2451e-07
Epoch 352/1000

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2414e-07 - val_loss: 3.2275e-07
Epoch 353/1000

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2236e-07 - val_loss: 3.2095e-07
Epoch 354/1000

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2059e-07 - val_loss: 3.1914e-07
Epoch 355/1000

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1872e-07 - val_loss: 3.1735e-07
Epoch 356/1000

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1698e-07 - val_loss: 3.1554e-07
Epoch 357/1000

Epoch 00357: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1518e-07 - val_loss: 3.1379e-07
Epoch 358/1000

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1341e-07 - val_loss: 3.1201e-07
Epoch 359/1000

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1168e-07 - val_loss: 3.1027e-07
Epoch 360/1000

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0986e-07 - val_loss: 3.0847e-07
Epoch 361/1000

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0810e-07 - val_loss: 3.0676e-07
Epoch 362/1000

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0639e-07 - val_loss: 3.0499e-07
Epoch 363/1000

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0464e-07 - val_loss: 3.0328e-07
Epoch 364/1000

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0290e-07 - val_loss: 3.0157e-07
Epoch 365/1000

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0116e-07 - val_loss: 2.9976e-07
Epoch 366/1000

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9941e-07 - val_loss: 2.9806e-07
Epoch 367/1000

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9770e-07 - val_loss: 2.9634e-07
Epoch 368/1000

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9599e-07 - val_loss: 2.9468e-07
Epoch 369/1000

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9430e-07 - val_loss: 2.9297e-07
Epoch 370/1000

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9258e-07 - val_loss: 2.9121e-07
Epoch 371/1000

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9087e-07 - val_loss: 2.8951e-07
Epoch 372/1000

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8918e-07 - val_loss: 2.8789e-07
Epoch 373/1000

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8749e-07 - val_loss: 2.8623e-07
Epoch 374/1000

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8585e-07 - val_loss: 2.8452e-07
Epoch 375/1000

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8416e-07 - val_loss: 2.8283e-07
Epoch 376/1000

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8248e-07 - val_loss: 2.8116e-07
Epoch 377/1000

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8080e-07 - val_loss: 2.7953e-07
Epoch 378/1000

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7918e-07 - val_loss: 2.7786e-07
Epoch 379/1000

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7753e-07 - val_loss: 2.7625e-07
Epoch 380/1000

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7589e-07 - val_loss: 2.7464e-07
Epoch 381/1000

Epoch 00381: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7424e-07 - val_loss: 2.7298e-07
Epoch 382/1000

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7263e-07 - val_loss: 2.7131e-07
Epoch 383/1000

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7100e-07 - val_loss: 2.6974e-07
Epoch 384/1000

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6938e-07 - val_loss: 2.6815e-07
Epoch 385/1000

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6779e-07 - val_loss: 2.6654e-07
Epoch 386/1000

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6617e-07 - val_loss: 2.6490e-07
Epoch 387/1000

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6456e-07 - val_loss: 2.6336e-07
Epoch 388/1000

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6299e-07 - val_loss: 2.6173e-07
Epoch 389/1000

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6140e-07 - val_loss: 2.6019e-07
Epoch 390/1000

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5984e-07 - val_loss: 2.5857e-07
Epoch 391/1000

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5828e-07 - val_loss: 2.5704e-07
Epoch 392/1000

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5667e-07 - val_loss: 2.5549e-07
Epoch 393/1000

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5513e-07 - val_loss: 2.5390e-07
Epoch 394/1000

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5358e-07 - val_loss: 2.5239e-07
Epoch 395/1000

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5204e-07 - val_loss: 2.5085e-07
Epoch 396/1000

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5051e-07 - val_loss: 2.4933e-07
Epoch 397/1000

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4899e-07 - val_loss: 2.4775e-07
Epoch 398/1000

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4743e-07 - val_loss: 2.4626e-07
Epoch 399/1000

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4592e-07 - val_loss: 2.4475e-07
Epoch 400/1000

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4441e-07 - val_loss: 2.4328e-07
Epoch 401/1000

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4292e-07 - val_loss: 2.4179e-07
Epoch 402/1000

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4141e-07 - val_loss: 2.4029e-07
Epoch 403/1000

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3993e-07 - val_loss: 2.3871e-07
Epoch 404/1000

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3841e-07 - val_loss: 2.3728e-07
Epoch 405/1000

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3695e-07 - val_loss: 2.3579e-07
Epoch 406/1000

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3547e-07 - val_loss: 2.3435e-07
Epoch 407/1000

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3403e-07 - val_loss: 2.3289e-07
Epoch 408/1000

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3256e-07 - val_loss: 2.3147e-07
Epoch 409/1000

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3110e-07 - val_loss: 2.2995e-07
Epoch 410/1000

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2964e-07 - val_loss: 2.2854e-07
Epoch 411/1000

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2821e-07 - val_loss: 2.2712e-07
Epoch 412/1000

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2678e-07 - val_loss: 2.2569e-07
Epoch 413/1000

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2536e-07 - val_loss: 2.2430e-07
Epoch 414/1000

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2396e-07 - val_loss: 2.2284e-07
Epoch 415/1000

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2251e-07 - val_loss: 2.2145e-07
Epoch 416/1000

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2112e-07 - val_loss: 2.2003e-07
Epoch 417/1000

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1972e-07 - val_loss: 2.1863e-07
Epoch 418/1000

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1831e-07 - val_loss: 2.1726e-07
Epoch 419/1000

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1694e-07 - val_loss: 2.1589e-07
Epoch 420/1000

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1558e-07 - val_loss: 2.1447e-07
Epoch 421/1000

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1416e-07 - val_loss: 2.1313e-07
Epoch 422/1000

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1280e-07 - val_loss: 2.1180e-07
Epoch 423/1000

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1145e-07 - val_loss: 2.1042e-07
Epoch 424/1000

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1010e-07 - val_loss: 2.0906e-07
Epoch 425/1000

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0876e-07 - val_loss: 2.0772e-07
Epoch 426/1000

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0742e-07 - val_loss: 2.0639e-07
Epoch 427/1000

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0607e-07 - val_loss: 2.0504e-07
Epoch 428/1000

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0475e-07 - val_loss: 2.0372e-07
Epoch 429/1000

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0341e-07 - val_loss: 2.0240e-07
Epoch 430/1000

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0211e-07 - val_loss: 2.0114e-07
Epoch 431/1000

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0081e-07 - val_loss: 1.9981e-07
Epoch 432/1000

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9951e-07 - val_loss: 1.9852e-07
Epoch 433/1000

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9820e-07 - val_loss: 1.9722e-07
Epoch 434/1000

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9690e-07 - val_loss: 1.9596e-07
Epoch 435/1000

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9563e-07 - val_loss: 1.9463e-07
Epoch 436/1000

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9436e-07 - val_loss: 1.9340e-07
Epoch 437/1000

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9309e-07 - val_loss: 1.9211e-07
Epoch 438/1000

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9183e-07 - val_loss: 1.9088e-07
Epoch 439/1000

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9058e-07 - val_loss: 1.8959e-07
Epoch 440/1000

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8931e-07 - val_loss: 1.8835e-07
Epoch 441/1000

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8806e-07 - val_loss: 1.8715e-07
Epoch 442/1000

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8683e-07 - val_loss: 1.8589e-07
Epoch 443/1000

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8562e-07 - val_loss: 1.8468e-07
Epoch 444/1000

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8438e-07 - val_loss: 1.8346e-07
Epoch 445/1000

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8317e-07 - val_loss: 1.8227e-07
Epoch 446/1000

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8197e-07 - val_loss: 1.8103e-07
Epoch 447/1000

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8073e-07 - val_loss: 1.7984e-07
Epoch 448/1000

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7956e-07 - val_loss: 1.7865e-07
Epoch 449/1000

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7836e-07 - val_loss: 1.7747e-07
Epoch 450/1000

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7718e-07 - val_loss: 1.7629e-07
Epoch 451/1000

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7600e-07 - val_loss: 1.7514e-07
Epoch 452/1000

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7483e-07 - val_loss: 1.7394e-07
Epoch 453/1000

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7366e-07 - val_loss: 1.7272e-07
Epoch 454/1000

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7248e-07 - val_loss: 1.7160e-07
Epoch 455/1000

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7133e-07 - val_loss: 1.7047e-07
Epoch 456/1000

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7019e-07 - val_loss: 1.6930e-07
Epoch 457/1000

Epoch 00457: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6904e-07 - val_loss: 1.6821e-07
Epoch 458/1000

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6794e-07 - val_loss: 1.6706e-07
Epoch 459/1000

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6679e-07 - val_loss: 1.6593e-07
Epoch 460/1000

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6566e-07 - val_loss: 1.6479e-07
Epoch 461/1000

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6453e-07 - val_loss: 1.6370e-07
Epoch 462/1000

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6342e-07 - val_loss: 1.6259e-07
Epoch 463/1000

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6233e-07 - val_loss: 1.6150e-07
Epoch 464/1000

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6122e-07 - val_loss: 1.6040e-07
Epoch 465/1000

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6013e-07 - val_loss: 1.5933e-07
Epoch 466/1000

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5907e-07 - val_loss: 1.5822e-07
Epoch 467/1000

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5797e-07 - val_loss: 1.5716e-07
Epoch 468/1000

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5688e-07 - val_loss: 1.5609e-07
Epoch 469/1000

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5583e-07 - val_loss: 1.5500e-07
Epoch 470/1000

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5476e-07 - val_loss: 1.5395e-07
Epoch 471/1000

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5370e-07 - val_loss: 1.5291e-07
Epoch 472/1000

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5266e-07 - val_loss: 1.5189e-07
Epoch 473/1000

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5162e-07 - val_loss: 1.5083e-07
Epoch 474/1000

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5058e-07 - val_loss: 1.4980e-07
Epoch 475/1000

Epoch 00475: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4952e-07 - val_loss: 1.4875e-07
Epoch 476/1000

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4851e-07 - val_loss: 1.4774e-07
Epoch 477/1000

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4749e-07 - val_loss: 1.4673e-07
Epoch 478/1000

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4648e-07 - val_loss: 1.4573e-07
Epoch 479/1000

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4547e-07 - val_loss: 1.4472e-07
Epoch 480/1000

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4448e-07 - val_loss: 1.4372e-07
Epoch 481/1000

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4348e-07 - val_loss: 1.4274e-07
Epoch 482/1000

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4249e-07 - val_loss: 1.4173e-07
Epoch 483/1000

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4148e-07 - val_loss: 1.4074e-07
Epoch 484/1000

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4051e-07 - val_loss: 1.3977e-07
Epoch 485/1000

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3954e-07 - val_loss: 1.3881e-07
Epoch 486/1000

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3856e-07 - val_loss: 1.3785e-07
Epoch 487/1000

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3762e-07 - val_loss: 1.3690e-07
Epoch 488/1000

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3667e-07 - val_loss: 1.3593e-07
Epoch 489/1000

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3570e-07 - val_loss: 1.3499e-07
Epoch 490/1000

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3477e-07 - val_loss: 1.3403e-07
Epoch 491/1000

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3381e-07 - val_loss: 1.3310e-07
Epoch 492/1000

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3288e-07 - val_loss: 1.3219e-07
Epoch 493/1000

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3196e-07 - val_loss: 1.3127e-07
Epoch 494/1000

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3104e-07 - val_loss: 1.3032e-07
Epoch 495/1000

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3011e-07 - val_loss: 1.2943e-07
Epoch 496/1000

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2921e-07 - val_loss: 1.2852e-07
Epoch 497/1000

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2832e-07 - val_loss: 1.2761e-07
Epoch 498/1000

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2740e-07 - val_loss: 1.2670e-07
Epoch 499/1000

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2648e-07 - val_loss: 1.2584e-07
Epoch 500/1000

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2562e-07 - val_loss: 1.2496e-07
Epoch 501/1000

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2473e-07 - val_loss: 1.2406e-07
Epoch 502/1000

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2385e-07 - val_loss: 1.2318e-07
Epoch 503/1000

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2297e-07 - val_loss: 1.2232e-07
Epoch 504/1000

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2212e-07 - val_loss: 1.2146e-07
Epoch 505/1000

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2125e-07 - val_loss: 1.2062e-07
Epoch 506/1000

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2041e-07 - val_loss: 1.1976e-07
Epoch 507/1000

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1953e-07 - val_loss: 1.1888e-07
Epoch 508/1000

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1867e-07 - val_loss: 1.1805e-07
Epoch 509/1000

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1784e-07 - val_loss: 1.1720e-07
Epoch 510/1000

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1701e-07 - val_loss: 1.1639e-07
Epoch 511/1000

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1617e-07 - val_loss: 1.1556e-07
Epoch 512/1000

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1537e-07 - val_loss: 1.1474e-07
Epoch 513/1000

Epoch 00513: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1455e-07 - val_loss: 1.1393e-07
Epoch 514/1000

Epoch 00514: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1372e-07 - val_loss: 1.1312e-07
Epoch 515/1000

Epoch 00515: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1293e-07 - val_loss: 1.1232e-07
Epoch 516/1000

Epoch 00516: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1210e-07 - val_loss: 1.1149e-07
Epoch 517/1000

Epoch 00517: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1130e-07 - val_loss: 1.1068e-07
Epoch 518/1000

Epoch 00518: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1049e-07 - val_loss: 1.0990e-07
Epoch 519/1000

Epoch 00519: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0972e-07 - val_loss: 1.0912e-07
Epoch 520/1000

Epoch 00520: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0893e-07 - val_loss: 1.0834e-07
Epoch 521/1000

Epoch 00521: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0816e-07 - val_loss: 1.0757e-07
Epoch 522/1000

Epoch 00522: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0738e-07 - val_loss: 1.0680e-07
Epoch 523/1000

Epoch 00523: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0662e-07 - val_loss: 1.0603e-07
Epoch 524/1000

Epoch 00524: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0585e-07 - val_loss: 1.0528e-07
Epoch 525/1000

Epoch 00525: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0509e-07 - val_loss: 1.0449e-07
Epoch 526/1000

Epoch 00526: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0431e-07 - val_loss: 1.0373e-07
Epoch 527/1000

Epoch 00527: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0357e-07 - val_loss: 1.0299e-07
Epoch 528/1000

Epoch 00528: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0283e-07 - val_loss: 1.0227e-07
Epoch 529/1000

Epoch 00529: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0208e-07 - val_loss: 1.0153e-07
Epoch 530/1000

Epoch 00530: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0136e-07 - val_loss: 1.0080e-07
Epoch 531/1000

Epoch 00531: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0062e-07 - val_loss: 1.0008e-07
Epoch 532/1000

Epoch 00532: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9908e-08 - val_loss: 9.9357e-08
Epoch 533/1000

Epoch 00533: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9177e-08 - val_loss: 9.8628e-08
Epoch 534/1000

Epoch 00534: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.8465e-08 - val_loss: 9.7932e-08
Epoch 535/1000

Epoch 00535: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7747e-08 - val_loss: 9.7202e-08
Epoch 536/1000

Epoch 00536: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7033e-08 - val_loss: 9.6478e-08
Epoch 537/1000

Epoch 00537: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6319e-08 - val_loss: 9.5806e-08
Epoch 538/1000

Epoch 00538: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5641e-08 - val_loss: 9.5104e-08
Epoch 539/1000

Epoch 00539: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4945e-08 - val_loss: 9.4407e-08
Epoch 540/1000

Epoch 00540: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4249e-08 - val_loss: 9.3737e-08
Epoch 541/1000

Epoch 00541: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3572e-08 - val_loss: 9.3065e-08
Epoch 542/1000

Epoch 00542: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2895e-08 - val_loss: 9.2382e-08
Epoch 543/1000

Epoch 00543: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2210e-08 - val_loss: 9.1718e-08
Epoch 544/1000

Epoch 00544: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1555e-08 - val_loss: 9.1049e-08
Epoch 545/1000

Epoch 00545: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0880e-08 - val_loss: 9.0350e-08
Epoch 546/1000

Epoch 00546: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0197e-08 - val_loss: 8.9690e-08
Epoch 547/1000

Epoch 00547: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9548e-08 - val_loss: 8.9042e-08
Epoch 548/1000

Epoch 00548: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8893e-08 - val_loss: 8.8403e-08
Epoch 549/1000

Epoch 00549: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8243e-08 - val_loss: 8.7759e-08
Epoch 550/1000

Epoch 00550: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7605e-08 - val_loss: 8.7115e-08
Epoch 551/1000

Epoch 00551: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6964e-08 - val_loss: 8.6491e-08
Epoch 552/1000

Epoch 00552: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6335e-08 - val_loss: 8.5847e-08
Epoch 553/1000

Epoch 00553: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5702e-08 - val_loss: 8.5213e-08
Epoch 554/1000

Epoch 00554: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5074e-08 - val_loss: 8.4601e-08
Epoch 555/1000

Epoch 00555: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4464e-08 - val_loss: 8.3977e-08
Epoch 556/1000

Epoch 00556: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3814e-08 - val_loss: 8.3342e-08
Epoch 557/1000

Epoch 00557: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3206e-08 - val_loss: 8.2736e-08
Epoch 558/1000

Epoch 00558: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2595e-08 - val_loss: 8.2123e-08
Epoch 559/1000

Epoch 00559: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1985e-08 - val_loss: 8.1545e-08
Epoch 560/1000

Epoch 00560: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1385e-08 - val_loss: 8.0938e-08
Epoch 561/1000

Epoch 00561: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0792e-08 - val_loss: 8.0332e-08
Epoch 562/1000

Epoch 00562: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0198e-08 - val_loss: 7.9739e-08
Epoch 563/1000

Epoch 00563: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9599e-08 - val_loss: 7.9158e-08
Epoch 564/1000

Epoch 00564: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9024e-08 - val_loss: 7.8579e-08
Epoch 565/1000

Epoch 00565: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8443e-08 - val_loss: 7.8005e-08
Epoch 566/1000

Epoch 00566: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7873e-08 - val_loss: 7.7413e-08
Epoch 567/1000

Epoch 00567: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7272e-08 - val_loss: 7.6833e-08
Epoch 568/1000

Epoch 00568: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6708e-08 - val_loss: 7.6290e-08
Epoch 569/1000

Epoch 00569: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6146e-08 - val_loss: 7.5703e-08
Epoch 570/1000

Epoch 00570: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5573e-08 - val_loss: 7.5145e-08
Epoch 571/1000

Epoch 00571: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5015e-08 - val_loss: 7.4588e-08
Epoch 572/1000

Epoch 00572: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4457e-08 - val_loss: 7.4049e-08
Epoch 573/1000

Epoch 00573: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3922e-08 - val_loss: 7.3503e-08
Epoch 574/1000

Epoch 00574: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3367e-08 - val_loss: 7.2952e-08
Epoch 575/1000

Epoch 00575: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2825e-08 - val_loss: 7.2418e-08
Epoch 576/1000

Epoch 00576: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2294e-08 - val_loss: 7.1883e-08
Epoch 577/1000

Epoch 00577: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1758e-08 - val_loss: 7.1351e-08
Epoch 578/1000

Epoch 00578: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1216e-08 - val_loss: 7.0819e-08
Epoch 579/1000

Epoch 00579: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0684e-08 - val_loss: 7.0285e-08
Epoch 580/1000

Epoch 00580: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0156e-08 - val_loss: 6.9756e-08
Epoch 581/1000

Epoch 00581: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9637e-08 - val_loss: 6.9223e-08
Epoch 582/1000

Epoch 00582: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9117e-08 - val_loss: 6.8716e-08
Epoch 583/1000

Epoch 00583: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8600e-08 - val_loss: 6.8222e-08
Epoch 584/1000

Epoch 00584: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8093e-08 - val_loss: 6.7707e-08
Epoch 585/1000

Epoch 00585: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7593e-08 - val_loss: 6.7199e-08
Epoch 586/1000

Epoch 00586: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7078e-08 - val_loss: 6.6698e-08
Epoch 587/1000

Epoch 00587: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6583e-08 - val_loss: 6.6211e-08
Epoch 588/1000

Epoch 00588: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6100e-08 - val_loss: 6.5710e-08
Epoch 589/1000

Epoch 00589: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5600e-08 - val_loss: 6.5223e-08
Epoch 590/1000

Epoch 00590: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5109e-08 - val_loss: 6.4736e-08
Epoch 591/1000

Epoch 00591: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4629e-08 - val_loss: 6.4250e-08
Epoch 592/1000

Epoch 00592: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4136e-08 - val_loss: 6.3757e-08
Epoch 593/1000

Epoch 00593: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3652e-08 - val_loss: 6.3266e-08
Epoch 594/1000

Epoch 00594: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3174e-08 - val_loss: 6.2801e-08
Epoch 595/1000

Epoch 00595: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2699e-08 - val_loss: 6.2335e-08
Epoch 596/1000

Epoch 00596: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2233e-08 - val_loss: 6.1891e-08
Epoch 597/1000

Epoch 00597: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1777e-08 - val_loss: 6.1416e-08
Epoch 598/1000

Epoch 00598: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1300e-08 - val_loss: 6.0952e-08
Epoch 599/1000

Epoch 00599: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0848e-08 - val_loss: 6.0500e-08
Epoch 600/1000

Epoch 00600: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0400e-08 - val_loss: 6.0048e-08
Epoch 601/1000

Epoch 00601: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9948e-08 - val_loss: 5.9595e-08
Epoch 602/1000

Epoch 00602: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9492e-08 - val_loss: 5.9150e-08
Epoch 603/1000

Epoch 00603: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9048e-08 - val_loss: 5.8717e-08
Epoch 604/1000

Epoch 00604: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8621e-08 - val_loss: 5.8257e-08
Epoch 605/1000

Epoch 00605: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8153e-08 - val_loss: 5.7823e-08
Epoch 606/1000

Epoch 00606: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7723e-08 - val_loss: 5.7385e-08
Epoch 607/1000

Epoch 00607: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7286e-08 - val_loss: 5.6937e-08
Epoch 608/1000

Epoch 00608: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6848e-08 - val_loss: 5.6533e-08
Epoch 609/1000

Epoch 00609: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6428e-08 - val_loss: 5.6096e-08
Epoch 610/1000

Epoch 00610: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6010e-08 - val_loss: 5.5674e-08
Epoch 611/1000

Epoch 00611: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5575e-08 - val_loss: 5.5251e-08
Epoch 612/1000

Epoch 00612: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5161e-08 - val_loss: 5.4840e-08
Epoch 613/1000

Epoch 00613: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4753e-08 - val_loss: 5.4431e-08
Epoch 614/1000

Epoch 00614: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4343e-08 - val_loss: 5.4013e-08
Epoch 615/1000

Epoch 00615: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3926e-08 - val_loss: 5.3614e-08
Epoch 616/1000

Epoch 00616: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3524e-08 - val_loss: 5.3215e-08
Epoch 617/1000

Epoch 00617: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3120e-08 - val_loss: 5.2816e-08
Epoch 618/1000

Epoch 00618: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2728e-08 - val_loss: 5.2418e-08
Epoch 619/1000

Epoch 00619: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2324e-08 - val_loss: 5.2010e-08
Epoch 620/1000

Epoch 00620: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1922e-08 - val_loss: 5.1619e-08
Epoch 621/1000

Epoch 00621: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1522e-08 - val_loss: 5.1220e-08
Epoch 622/1000

Epoch 00622: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1132e-08 - val_loss: 5.0846e-08
Epoch 623/1000

Epoch 00623: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0753e-08 - val_loss: 5.0453e-08
Epoch 624/1000

Epoch 00624: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0372e-08 - val_loss: 5.0073e-08
Epoch 625/1000

Epoch 00625: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9980e-08 - val_loss: 4.9707e-08
Epoch 626/1000

Epoch 00626: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9610e-08 - val_loss: 4.9328e-08
Epoch 627/1000

Epoch 00627: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9239e-08 - val_loss: 4.8950e-08
Epoch 628/1000

Epoch 00628: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8861e-08 - val_loss: 4.8579e-08
Epoch 629/1000

Epoch 00629: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8496e-08 - val_loss: 4.8206e-08
Epoch 630/1000

Epoch 00630: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8125e-08 - val_loss: 4.7847e-08
Epoch 631/1000

Epoch 00631: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7769e-08 - val_loss: 4.7469e-08
Epoch 632/1000

Epoch 00632: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7400e-08 - val_loss: 4.7121e-08
Epoch 633/1000

Epoch 00633: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7041e-08 - val_loss: 4.6775e-08
Epoch 634/1000

Epoch 00634: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6693e-08 - val_loss: 4.6418e-08
Epoch 635/1000

Epoch 00635: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6328e-08 - val_loss: 4.6041e-08
Epoch 636/1000

Epoch 00636: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5969e-08 - val_loss: 4.5696e-08
Epoch 637/1000

Epoch 00637: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5626e-08 - val_loss: 4.5346e-08
Epoch 638/1000

Epoch 00638: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5284e-08 - val_loss: 4.5017e-08
Epoch 639/1000

Epoch 00639: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4932e-08 - val_loss: 4.4651e-08
Epoch 640/1000

Epoch 00640: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4584e-08 - val_loss: 4.4335e-08
Epoch 641/1000

Epoch 00641: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4255e-08 - val_loss: 4.3989e-08
Epoch 642/1000

Epoch 00642: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3922e-08 - val_loss: 4.3651e-08
Epoch 643/1000

Epoch 00643: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3582e-08 - val_loss: 4.3324e-08
Epoch 644/1000

Epoch 00644: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3257e-08 - val_loss: 4.2998e-08
Epoch 645/1000

Epoch 00645: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2927e-08 - val_loss: 4.2669e-08
Epoch 646/1000

Epoch 00646: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2599e-08 - val_loss: 4.2341e-08
Epoch 647/1000

Epoch 00647: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2273e-08 - val_loss: 4.2029e-08
Epoch 648/1000

Epoch 00648: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1961e-08 - val_loss: 4.1709e-08
Epoch 649/1000

Epoch 00649: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1639e-08 - val_loss: 4.1396e-08
Epoch 650/1000

Epoch 00650: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1322e-08 - val_loss: 4.1064e-08
Epoch 651/1000

Epoch 00651: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1004e-08 - val_loss: 4.0774e-08
Epoch 652/1000

Epoch 00652: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0691e-08 - val_loss: 4.0445e-08
Epoch 653/1000

Epoch 00653: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0378e-08 - val_loss: 4.0139e-08
Epoch 654/1000

Epoch 00654: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0071e-08 - val_loss: 3.9822e-08
Epoch 655/1000

Epoch 00655: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9757e-08 - val_loss: 3.9524e-08
Epoch 656/1000

Epoch 00656: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9456e-08 - val_loss: 3.9218e-08
Epoch 657/1000

Epoch 00657: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9163e-08 - val_loss: 3.8923e-08
Epoch 658/1000

Epoch 00658: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8860e-08 - val_loss: 3.8617e-08
Epoch 659/1000

Epoch 00659: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8562e-08 - val_loss: 3.8327e-08
Epoch 660/1000

Epoch 00660: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8268e-08 - val_loss: 3.8044e-08
Epoch 661/1000

Epoch 00661: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7977e-08 - val_loss: 3.7750e-08
Epoch 662/1000

Epoch 00662: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7688e-08 - val_loss: 3.7459e-08
Epoch 663/1000

Epoch 00663: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7403e-08 - val_loss: 3.7171e-08
Epoch 664/1000

Epoch 00664: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7114e-08 - val_loss: 3.6894e-08
Epoch 665/1000

Epoch 00665: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6837e-08 - val_loss: 3.6621e-08
Epoch 666/1000

Epoch 00666: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6557e-08 - val_loss: 3.6325e-08
Epoch 667/1000

Epoch 00667: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6270e-08 - val_loss: 3.6054e-08
Epoch 668/1000

Epoch 00668: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5999e-08 - val_loss: 3.5779e-08
Epoch 669/1000

Epoch 00669: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5727e-08 - val_loss: 3.5510e-08
Epoch 670/1000

Epoch 00670: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5453e-08 - val_loss: 3.5240e-08
Epoch 671/1000

Epoch 00671: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5174e-08 - val_loss: 3.4962e-08
Epoch 672/1000

Epoch 00672: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4898e-08 - val_loss: 3.4686e-08
Epoch 673/1000

Epoch 00673: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4633e-08 - val_loss: 3.4430e-08
Epoch 674/1000

Epoch 00674: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4373e-08 - val_loss: 3.4167e-08
Epoch 675/1000

Epoch 00675: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4108e-08 - val_loss: 3.3895e-08
Epoch 676/1000

Epoch 00676: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3847e-08 - val_loss: 3.3639e-08
Epoch 677/1000

Epoch 00677: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3589e-08 - val_loss: 3.3384e-08
Epoch 678/1000

Epoch 00678: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3330e-08 - val_loss: 3.3136e-08
Epoch 679/1000

Epoch 00679: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3079e-08 - val_loss: 3.2871e-08
Epoch 680/1000

Epoch 00680: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2818e-08 - val_loss: 3.2626e-08
Epoch 681/1000

Epoch 00681: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2568e-08 - val_loss: 3.2370e-08
Epoch 682/1000

Epoch 00682: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2321e-08 - val_loss: 3.2126e-08
Epoch 683/1000

Epoch 00683: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2081e-08 - val_loss: 3.1882e-08
Epoch 684/1000

Epoch 00684: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1831e-08 - val_loss: 3.1627e-08
Epoch 685/1000

Epoch 00685: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1582e-08 - val_loss: 3.1389e-08
Epoch 686/1000

Epoch 00686: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1343e-08 - val_loss: 3.1153e-08
Epoch 687/1000

Epoch 00687: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1107e-08 - val_loss: 3.0915e-08
Epoch 688/1000

Epoch 00688: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0870e-08 - val_loss: 3.0676e-08
Epoch 689/1000

Epoch 00689: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0630e-08 - val_loss: 3.0445e-08
Epoch 690/1000

Epoch 00690: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0400e-08 - val_loss: 3.0212e-08
Epoch 691/1000

Epoch 00691: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0164e-08 - val_loss: 2.9970e-08
Epoch 692/1000

Epoch 00692: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9921e-08 - val_loss: 2.9740e-08
Epoch 693/1000

Epoch 00693: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9694e-08 - val_loss: 2.9516e-08
Epoch 694/1000

Epoch 00694: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9462e-08 - val_loss: 2.9287e-08
Epoch 695/1000

Epoch 00695: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9239e-08 - val_loss: 2.9065e-08
Epoch 696/1000

Epoch 00696: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9020e-08 - val_loss: 2.8840e-08
Epoch 697/1000

Epoch 00697: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8798e-08 - val_loss: 2.8616e-08
Epoch 698/1000

Epoch 00698: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8573e-08 - val_loss: 2.8396e-08
Epoch 699/1000

Epoch 00699: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8349e-08 - val_loss: 2.8176e-08
Epoch 700/1000

Epoch 00700: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.8140e-08 - val_loss: 2.7964e-08
Epoch 701/1000

Epoch 00701: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7923e-08 - val_loss: 2.7750e-08
Epoch 702/1000

Epoch 00702: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7710e-08 - val_loss: 2.7536e-08
Epoch 703/1000

Epoch 00703: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7498e-08 - val_loss: 2.7325e-08
Epoch 704/1000

Epoch 00704: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7282e-08 - val_loss: 2.7107e-08
Epoch 705/1000

Epoch 00705: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.7068e-08 - val_loss: 2.6902e-08
Epoch 706/1000

Epoch 00706: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6864e-08 - val_loss: 2.6700e-08
Epoch 707/1000

Epoch 00707: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6662e-08 - val_loss: 2.6495e-08
Epoch 708/1000

Epoch 00708: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6460e-08 - val_loss: 2.6295e-08
Epoch 709/1000

Epoch 00709: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6252e-08 - val_loss: 2.6090e-08
Epoch 710/1000

Epoch 00710: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.6053e-08 - val_loss: 2.5897e-08
Epoch 711/1000

Epoch 00711: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5853e-08 - val_loss: 2.5689e-08
Epoch 712/1000

Epoch 00712: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5654e-08 - val_loss: 2.5493e-08
Epoch 713/1000

Epoch 00713: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5458e-08 - val_loss: 2.5301e-08
Epoch 714/1000

Epoch 00714: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5261e-08 - val_loss: 2.5102e-08
Epoch 715/1000

Epoch 00715: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.5062e-08 - val_loss: 2.4898e-08
Epoch 716/1000

Epoch 00716: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4865e-08 - val_loss: 2.4712e-08
Epoch 717/1000

Epoch 00717: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4677e-08 - val_loss: 2.4520e-08
Epoch 718/1000

Epoch 00718: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4487e-08 - val_loss: 2.4329e-08
Epoch 719/1000

Epoch 00719: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4296e-08 - val_loss: 2.4145e-08
Epoch 720/1000

Epoch 00720: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.4108e-08 - val_loss: 2.3962e-08
Epoch 721/1000

Epoch 00721: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3925e-08 - val_loss: 2.3775e-08
Epoch 722/1000

Epoch 00722: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3742e-08 - val_loss: 2.3592e-08
Epoch 723/1000

Epoch 00723: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3561e-08 - val_loss: 2.3420e-08
Epoch 724/1000

Epoch 00724: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3385e-08 - val_loss: 2.3236e-08
Epoch 725/1000

Epoch 00725: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3201e-08 - val_loss: 2.3049e-08
Epoch 726/1000

Epoch 00726: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.3016e-08 - val_loss: 2.2876e-08
Epoch 727/1000

Epoch 00727: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2844e-08 - val_loss: 2.2704e-08
Epoch 728/1000

Epoch 00728: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2670e-08 - val_loss: 2.2522e-08
Epoch 729/1000

Epoch 00729: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2494e-08 - val_loss: 2.2351e-08
Epoch 730/1000

Epoch 00730: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2324e-08 - val_loss: 2.2183e-08
Epoch 731/1000

Epoch 00731: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.2151e-08 - val_loss: 2.2008e-08
Epoch 732/1000

Epoch 00732: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1984e-08 - val_loss: 2.1838e-08
Epoch 733/1000

Epoch 00733: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1811e-08 - val_loss: 2.1671e-08
Epoch 734/1000

Epoch 00734: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1645e-08 - val_loss: 2.1509e-08
Epoch 735/1000

Epoch 00735: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1477e-08 - val_loss: 2.1341e-08
Epoch 736/1000

Epoch 00736: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1311e-08 - val_loss: 2.1175e-08
Epoch 737/1000

Epoch 00737: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.1148e-08 - val_loss: 2.1013e-08
Epoch 738/1000

Epoch 00738: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0987e-08 - val_loss: 2.0853e-08
Epoch 739/1000

Epoch 00739: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0827e-08 - val_loss: 2.0694e-08
Epoch 740/1000

Epoch 00740: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0667e-08 - val_loss: 2.0538e-08
Epoch 741/1000

Epoch 00741: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0511e-08 - val_loss: 2.0376e-08
Epoch 742/1000

Epoch 00742: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0343e-08 - val_loss: 2.0204e-08
Epoch 743/1000

Epoch 00743: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0180e-08 - val_loss: 2.0043e-08
Epoch 744/1000

Epoch 00744: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.0027e-08 - val_loss: 1.9891e-08
Epoch 745/1000

Epoch 00745: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9875e-08 - val_loss: 1.9742e-08
Epoch 746/1000

Epoch 00746: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9724e-08 - val_loss: 1.9599e-08
Epoch 747/1000

Epoch 00747: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9576e-08 - val_loss: 1.9440e-08
Epoch 748/1000

Epoch 00748: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9422e-08 - val_loss: 1.9293e-08
Epoch 749/1000

Epoch 00749: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9269e-08 - val_loss: 1.9144e-08
Epoch 750/1000

Epoch 00750: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.9122e-08 - val_loss: 1.8994e-08
Epoch 751/1000

Epoch 00751: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8972e-08 - val_loss: 1.8853e-08
Epoch 752/1000

Epoch 00752: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8829e-08 - val_loss: 1.8710e-08
Epoch 753/1000

Epoch 00753: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8688e-08 - val_loss: 1.8563e-08
Epoch 754/1000

Epoch 00754: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8542e-08 - val_loss: 1.8425e-08
Epoch 755/1000

Epoch 00755: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8397e-08 - val_loss: 1.8278e-08
Epoch 756/1000

Epoch 00756: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8257e-08 - val_loss: 1.8147e-08
Epoch 757/1000

Epoch 00757: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.8119e-08 - val_loss: 1.8000e-08
Epoch 758/1000

Epoch 00758: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7980e-08 - val_loss: 1.7863e-08
Epoch 759/1000

Epoch 00759: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7840e-08 - val_loss: 1.7729e-08
Epoch 760/1000

Epoch 00760: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7706e-08 - val_loss: 1.7591e-08
Epoch 761/1000

Epoch 00761: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7568e-08 - val_loss: 1.7446e-08
Epoch 762/1000

Epoch 00762: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7430e-08 - val_loss: 1.7319e-08
Epoch 763/1000

Epoch 00763: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7300e-08 - val_loss: 1.7185e-08
Epoch 764/1000

Epoch 00764: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7168e-08 - val_loss: 1.7055e-08
Epoch 765/1000

Epoch 00765: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.7037e-08 - val_loss: 1.6933e-08
Epoch 766/1000

Epoch 00766: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6908e-08 - val_loss: 1.6796e-08
Epoch 767/1000

Epoch 00767: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6779e-08 - val_loss: 1.6673e-08
Epoch 768/1000

Epoch 00768: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6644e-08 - val_loss: 1.6534e-08
Epoch 769/1000

Epoch 00769: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6515e-08 - val_loss: 1.6409e-08
Epoch 770/1000

Epoch 00770: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6390e-08 - val_loss: 1.6283e-08
Epoch 771/1000

Epoch 00771: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6264e-08 - val_loss: 1.6160e-08
Epoch 772/1000

Epoch 00772: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6141e-08 - val_loss: 1.6037e-08
Epoch 773/1000

Epoch 00773: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.6014e-08 - val_loss: 1.5907e-08
Epoch 774/1000

Epoch 00774: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5887e-08 - val_loss: 1.5785e-08
Epoch 775/1000

Epoch 00775: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5764e-08 - val_loss: 1.5660e-08
Epoch 776/1000

Epoch 00776: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5643e-08 - val_loss: 1.5542e-08
Epoch 777/1000

Epoch 00777: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5522e-08 - val_loss: 1.5419e-08
Epoch 778/1000

Epoch 00778: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5401e-08 - val_loss: 1.5297e-08
Epoch 779/1000

Epoch 00779: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5284e-08 - val_loss: 1.5183e-08
Epoch 780/1000

Epoch 00780: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5168e-08 - val_loss: 1.5066e-08
Epoch 781/1000

Epoch 00781: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.5053e-08 - val_loss: 1.4952e-08
Epoch 782/1000

Epoch 00782: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4937e-08 - val_loss: 1.4834e-08
Epoch 783/1000

Epoch 00783: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4821e-08 - val_loss: 1.4721e-08
Epoch 784/1000

Epoch 00784: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4706e-08 - val_loss: 1.4612e-08
Epoch 785/1000

Epoch 00785: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4594e-08 - val_loss: 1.4502e-08
Epoch 786/1000

Epoch 00786: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4486e-08 - val_loss: 1.4392e-08
Epoch 787/1000

Epoch 00787: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4375e-08 - val_loss: 1.4280e-08
Epoch 788/1000

Epoch 00788: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4260e-08 - val_loss: 1.4163e-08
Epoch 789/1000

Epoch 00789: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4151e-08 - val_loss: 1.4060e-08
Epoch 790/1000

Epoch 00790: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.4040e-08 - val_loss: 1.3946e-08
Epoch 791/1000

Epoch 00791: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3933e-08 - val_loss: 1.3844e-08
Epoch 792/1000

Epoch 00792: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3827e-08 - val_loss: 1.3734e-08
Epoch 793/1000

Epoch 00793: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3723e-08 - val_loss: 1.3630e-08
Epoch 794/1000

Epoch 00794: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3618e-08 - val_loss: 1.3534e-08
Epoch 795/1000

Epoch 00795: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3515e-08 - val_loss: 1.3423e-08
Epoch 796/1000

Epoch 00796: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3411e-08 - val_loss: 1.3321e-08
Epoch 797/1000

Epoch 00797: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3310e-08 - val_loss: 1.3217e-08
Epoch 798/1000

Epoch 00798: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3203e-08 - val_loss: 1.3113e-08
Epoch 799/1000

Epoch 00799: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3101e-08 - val_loss: 1.3013e-08
Epoch 800/1000

Epoch 00800: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.3001e-08 - val_loss: 1.2914e-08
Epoch 801/1000

Epoch 00801: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2904e-08 - val_loss: 1.2821e-08
Epoch 802/1000

Epoch 00802: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2806e-08 - val_loss: 1.2719e-08
Epoch 803/1000

Epoch 00803: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2708e-08 - val_loss: 1.2623e-08
Epoch 804/1000

Epoch 00804: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2612e-08 - val_loss: 1.2527e-08
Epoch 805/1000

Epoch 00805: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2515e-08 - val_loss: 1.2427e-08
Epoch 806/1000

Epoch 00806: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2418e-08 - val_loss: 1.2334e-08
Epoch 807/1000

Epoch 00807: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2324e-08 - val_loss: 1.2241e-08
Epoch 808/1000

Epoch 00808: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2230e-08 - val_loss: 1.2148e-08
Epoch 809/1000

Epoch 00809: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2135e-08 - val_loss: 1.2057e-08
Epoch 810/1000

Epoch 00810: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.2043e-08 - val_loss: 1.1958e-08
Epoch 811/1000

Epoch 00811: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1948e-08 - val_loss: 1.1865e-08
Epoch 812/1000

Epoch 00812: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1853e-08 - val_loss: 1.1773e-08
Epoch 813/1000

Epoch 00813: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1763e-08 - val_loss: 1.1684e-08
Epoch 814/1000

Epoch 00814: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1670e-08 - val_loss: 1.1589e-08
Epoch 815/1000

Epoch 00815: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1582e-08 - val_loss: 1.1501e-08
Epoch 816/1000

Epoch 00816: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1493e-08 - val_loss: 1.1417e-08
Epoch 817/1000

Epoch 00817: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1405e-08 - val_loss: 1.1331e-08
Epoch 818/1000

Epoch 00818: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1319e-08 - val_loss: 1.1240e-08
Epoch 819/1000

Epoch 00819: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1234e-08 - val_loss: 1.1159e-08
Epoch 820/1000

Epoch 00820: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1148e-08 - val_loss: 1.1071e-08
Epoch 821/1000

Epoch 00821: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.1061e-08 - val_loss: 1.0988e-08
Epoch 822/1000

Epoch 00822: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0976e-08 - val_loss: 1.0904e-08
Epoch 823/1000

Epoch 00823: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0891e-08 - val_loss: 1.0811e-08
Epoch 824/1000

Epoch 00824: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0808e-08 - val_loss: 1.0728e-08
Epoch 825/1000

Epoch 00825: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0725e-08 - val_loss: 1.0654e-08
Epoch 826/1000

Epoch 00826: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0645e-08 - val_loss: 1.0571e-08
Epoch 827/1000

Epoch 00827: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0564e-08 - val_loss: 1.0491e-08
Epoch 828/1000

Epoch 00828: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0484e-08 - val_loss: 1.0413e-08
Epoch 829/1000

Epoch 00829: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0406e-08 - val_loss: 1.0334e-08
Epoch 830/1000

Epoch 00830: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0327e-08 - val_loss: 1.0259e-08
Epoch 831/1000

Epoch 00831: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0248e-08 - val_loss: 1.0178e-08
Epoch 832/1000

Epoch 00832: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0168e-08 - val_loss: 1.0101e-08
Epoch 833/1000

Epoch 00833: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0091e-08 - val_loss: 1.0018e-08
Epoch 834/1000

Epoch 00834: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 1.0012e-08 - val_loss: 9.9425e-09
Epoch 835/1000

Epoch 00835: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.9368e-09 - val_loss: 9.8667e-09
Epoch 836/1000

Epoch 00836: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.8611e-09 - val_loss: 9.7928e-09
Epoch 837/1000

Epoch 00837: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7857e-09 - val_loss: 9.7170e-09
Epoch 838/1000

Epoch 00838: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.7117e-09 - val_loss: 9.6503e-09
Epoch 839/1000

Epoch 00839: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.6394e-09 - val_loss: 9.5702e-09
Epoch 840/1000

Epoch 00840: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.5664e-09 - val_loss: 9.4948e-09
Epoch 841/1000

Epoch 00841: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4937e-09 - val_loss: 9.4266e-09
Epoch 842/1000

Epoch 00842: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.4190e-09 - val_loss: 9.3519e-09
Epoch 843/1000

Epoch 00843: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.3473e-09 - val_loss: 9.2848e-09
Epoch 844/1000

Epoch 00844: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2777e-09 - val_loss: 9.2115e-09
Epoch 845/1000

Epoch 00845: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.2073e-09 - val_loss: 9.1417e-09
Epoch 846/1000

Epoch 00846: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.1372e-09 - val_loss: 9.0737e-09
Epoch 847/1000

Epoch 00847: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 9.0663e-09 - val_loss: 9.0023e-09
Epoch 848/1000

Epoch 00848: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9981e-09 - val_loss: 8.9328e-09
Epoch 849/1000

Epoch 00849: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.9286e-09 - val_loss: 8.8662e-09
Epoch 850/1000

Epoch 00850: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.8627e-09 - val_loss: 8.7997e-09
Epoch 851/1000

Epoch 00851: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7963e-09 - val_loss: 8.7326e-09
Epoch 852/1000

Epoch 00852: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.7268e-09 - val_loss: 8.6618e-09
Epoch 853/1000

Epoch 00853: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.6607e-09 - val_loss: 8.6024e-09
Epoch 854/1000

Epoch 00854: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5962e-09 - val_loss: 8.5345e-09
Epoch 855/1000

Epoch 00855: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.5309e-09 - val_loss: 8.4704e-09
Epoch 856/1000

Epoch 00856: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4675e-09 - val_loss: 8.4074e-09
Epoch 857/1000

Epoch 00857: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.4037e-09 - val_loss: 8.3443e-09
Epoch 858/1000

Epoch 00858: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.3387e-09 - val_loss: 8.2745e-09
Epoch 859/1000

Epoch 00859: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2713e-09 - val_loss: 8.2137e-09
Epoch 860/1000

Epoch 00860: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.2095e-09 - val_loss: 8.1513e-09
Epoch 861/1000

Epoch 00861: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.1474e-09 - val_loss: 8.0876e-09
Epoch 862/1000

Epoch 00862: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0844e-09 - val_loss: 8.0234e-09
Epoch 863/1000

Epoch 00863: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 8.0200e-09 - val_loss: 7.9582e-09
Epoch 864/1000

Epoch 00864: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9606e-09 - val_loss: 7.8978e-09
Epoch 865/1000

Epoch 00865: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.9009e-09 - val_loss: 7.8430e-09
Epoch 866/1000

Epoch 00866: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.8418e-09 - val_loss: 7.7844e-09
Epoch 867/1000

Epoch 00867: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7807e-09 - val_loss: 7.7232e-09
Epoch 868/1000

Epoch 00868: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.7227e-09 - val_loss: 7.6693e-09
Epoch 869/1000

Epoch 00869: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6634e-09 - val_loss: 7.6094e-09
Epoch 870/1000

Epoch 00870: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.6075e-09 - val_loss: 7.5508e-09
Epoch 871/1000

Epoch 00871: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.5496e-09 - val_loss: 7.4956e-09
Epoch 872/1000

Epoch 00872: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4944e-09 - val_loss: 7.4412e-09
Epoch 873/1000

Epoch 00873: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.4387e-09 - val_loss: 7.3859e-09
Epoch 874/1000

Epoch 00874: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3791e-09 - val_loss: 7.3255e-09
Epoch 875/1000

Epoch 00875: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.3241e-09 - val_loss: 7.2701e-09
Epoch 876/1000

Epoch 00876: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2674e-09 - val_loss: 7.2130e-09
Epoch 877/1000

Epoch 00877: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.2131e-09 - val_loss: 7.1591e-09
Epoch 878/1000

Epoch 00878: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1575e-09 - val_loss: 7.1073e-09
Epoch 879/1000

Epoch 00879: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.1032e-09 - val_loss: 7.0522e-09
Epoch 880/1000

Epoch 00880: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 7.0497e-09 - val_loss: 6.9961e-09
Epoch 881/1000

Epoch 00881: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9975e-09 - val_loss: 6.9444e-09
Epoch 882/1000

Epoch 00882: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.9470e-09 - val_loss: 6.8939e-09
Epoch 883/1000

Epoch 00883: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8924e-09 - val_loss: 6.8435e-09
Epoch 884/1000

Epoch 00884: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.8429e-09 - val_loss: 6.7948e-09
Epoch 885/1000

Epoch 00885: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7906e-09 - val_loss: 6.7435e-09
Epoch 886/1000

Epoch 00886: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.7379e-09 - val_loss: 6.6857e-09
Epoch 887/1000

Epoch 00887: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6870e-09 - val_loss: 6.6357e-09
Epoch 888/1000

Epoch 00888: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.6367e-09 - val_loss: 6.5895e-09
Epoch 889/1000

Epoch 00889: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5884e-09 - val_loss: 6.5411e-09
Epoch 890/1000

Epoch 00890: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.5394e-09 - val_loss: 6.4881e-09
Epoch 891/1000

Epoch 00891: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4896e-09 - val_loss: 6.4437e-09
Epoch 892/1000

Epoch 00892: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.4419e-09 - val_loss: 6.3924e-09
Epoch 893/1000

Epoch 00893: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3941e-09 - val_loss: 6.3448e-09
Epoch 894/1000

Epoch 00894: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.3468e-09 - val_loss: 6.2970e-09
Epoch 895/1000

Epoch 00895: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2972e-09 - val_loss: 6.2489e-09
Epoch 896/1000

Epoch 00896: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2503e-09 - val_loss: 6.2049e-09
Epoch 897/1000

Epoch 00897: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.2040e-09 - val_loss: 6.1585e-09
Epoch 898/1000

Epoch 00898: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1587e-09 - val_loss: 6.1109e-09
Epoch 899/1000

Epoch 00899: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.1101e-09 - val_loss: 6.0625e-09
Epoch 900/1000

Epoch 00900: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0642e-09 - val_loss: 6.0178e-09
Epoch 901/1000

Epoch 00901: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 6.0186e-09 - val_loss: 5.9733e-09
Epoch 902/1000

Epoch 00902: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9747e-09 - val_loss: 5.9312e-09
Epoch 903/1000

Epoch 00903: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.9316e-09 - val_loss: 5.8853e-09
Epoch 904/1000

Epoch 00904: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8868e-09 - val_loss: 5.8412e-09
Epoch 905/1000

Epoch 00905: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8428e-09 - val_loss: 5.8037e-09
Epoch 906/1000

Epoch 00906: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.8002e-09 - val_loss: 5.7558e-09
Epoch 907/1000

Epoch 00907: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7571e-09 - val_loss: 5.7124e-09
Epoch 908/1000

Epoch 00908: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.7143e-09 - val_loss: 5.6710e-09
Epoch 909/1000

Epoch 00909: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6727e-09 - val_loss: 5.6279e-09
Epoch 910/1000

Epoch 00910: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.6296e-09 - val_loss: 5.5873e-09
Epoch 911/1000

Epoch 00911: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5887e-09 - val_loss: 5.5456e-09
Epoch 912/1000

Epoch 00912: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5470e-09 - val_loss: 5.5019e-09
Epoch 913/1000

Epoch 00913: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.5014e-09 - val_loss: 5.4581e-09
Epoch 914/1000

Epoch 00914: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4596e-09 - val_loss: 5.4181e-09
Epoch 915/1000

Epoch 00915: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.4200e-09 - val_loss: 5.3779e-09
Epoch 916/1000

Epoch 00916: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3798e-09 - val_loss: 5.3385e-09
Epoch 917/1000

Epoch 00917: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3395e-09 - val_loss: 5.2983e-09
Epoch 918/1000

Epoch 00918: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.3006e-09 - val_loss: 5.2580e-09
Epoch 919/1000

Epoch 00919: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2616e-09 - val_loss: 5.2207e-09
Epoch 920/1000

Epoch 00920: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.2224e-09 - val_loss: 5.1828e-09
Epoch 921/1000

Epoch 00921: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1795e-09 - val_loss: 5.1389e-09
Epoch 922/1000

Epoch 00922: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1411e-09 - val_loss: 5.1020e-09
Epoch 923/1000

Epoch 00923: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.1037e-09 - val_loss: 5.0639e-09
Epoch 924/1000

Epoch 00924: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0666e-09 - val_loss: 5.0262e-09
Epoch 925/1000

Epoch 00925: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 5.0283e-09 - val_loss: 4.9894e-09
Epoch 926/1000

Epoch 00926: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9919e-09 - val_loss: 4.9516e-09
Epoch 927/1000

Epoch 00927: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9532e-09 - val_loss: 4.9145e-09
Epoch 928/1000

Epoch 00928: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.9161e-09 - val_loss: 4.8767e-09
Epoch 929/1000

Epoch 00929: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8806e-09 - val_loss: 4.8424e-09
Epoch 930/1000

Epoch 00930: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8447e-09 - val_loss: 4.8070e-09
Epoch 931/1000

Epoch 00931: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.8092e-09 - val_loss: 4.7711e-09
Epoch 932/1000

Epoch 00932: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7742e-09 - val_loss: 4.7364e-09
Epoch 933/1000

Epoch 00933: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7388e-09 - val_loss: 4.6996e-09
Epoch 934/1000

Epoch 00934: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.7019e-09 - val_loss: 4.6656e-09
Epoch 935/1000

Epoch 00935: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6685e-09 - val_loss: 4.6283e-09
Epoch 936/1000

Epoch 00936: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.6345e-09 - val_loss: 4.5975e-09
Epoch 937/1000

Epoch 00937: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5999e-09 - val_loss: 4.5641e-09
Epoch 938/1000

Epoch 00938: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5663e-09 - val_loss: 4.5302e-09
Epoch 939/1000

Epoch 00939: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.5322e-09 - val_loss: 4.4971e-09
Epoch 940/1000

Epoch 00940: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4996e-09 - val_loss: 4.4648e-09
Epoch 941/1000

Epoch 00941: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4660e-09 - val_loss: 4.4321e-09
Epoch 942/1000

Epoch 00942: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4349e-09 - val_loss: 4.3997e-09
Epoch 943/1000

Epoch 00943: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.4011e-09 - val_loss: 4.3651e-09
Epoch 944/1000

Epoch 00944: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3676e-09 - val_loss: 4.3289e-09
Epoch 945/1000

Epoch 00945: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3350e-09 - val_loss: 4.3016e-09
Epoch 946/1000

Epoch 00946: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.3046e-09 - val_loss: 4.2675e-09
Epoch 947/1000

Epoch 00947: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2725e-09 - val_loss: 4.2391e-09
Epoch 948/1000

Epoch 00948: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2418e-09 - val_loss: 4.2059e-09
Epoch 949/1000

Epoch 00949: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.2103e-09 - val_loss: 4.1777e-09
Epoch 950/1000

Epoch 00950: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1814e-09 - val_loss: 4.1423e-09
Epoch 951/1000

Epoch 00951: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1489e-09 - val_loss: 4.1171e-09
Epoch 952/1000

Epoch 00952: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.1196e-09 - val_loss: 4.0873e-09
Epoch 953/1000

Epoch 00953: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0897e-09 - val_loss: 4.0553e-09
Epoch 954/1000

Epoch 00954: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0592e-09 - val_loss: 4.0244e-09
Epoch 955/1000

Epoch 00955: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 4.0302e-09 - val_loss: 3.9939e-09
Epoch 956/1000

Epoch 00956: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9998e-09 - val_loss: 3.9653e-09
Epoch 957/1000

Epoch 00957: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9696e-09 - val_loss: 3.9348e-09
Epoch 958/1000

Epoch 00958: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9406e-09 - val_loss: 3.9060e-09
Epoch 959/1000

Epoch 00959: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.9123e-09 - val_loss: 3.8816e-09
Epoch 960/1000

Epoch 00960: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8824e-09 - val_loss: 3.8472e-09
Epoch 961/1000

Epoch 00961: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8522e-09 - val_loss: 3.8188e-09
Epoch 962/1000

Epoch 00962: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.8245e-09 - val_loss: 3.7920e-09
Epoch 963/1000

Epoch 00963: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7969e-09 - val_loss: 3.7647e-09
Epoch 964/1000

Epoch 00964: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7696e-09 - val_loss: 3.7395e-09
Epoch 965/1000

Epoch 00965: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7421e-09 - val_loss: 3.7138e-09
Epoch 966/1000

Epoch 00966: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.7154e-09 - val_loss: 3.6830e-09
Epoch 967/1000

Epoch 00967: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6882e-09 - val_loss: 3.6595e-09
Epoch 968/1000

Epoch 00968: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6624e-09 - val_loss: 3.6324e-09
Epoch 969/1000

Epoch 00969: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6357e-09 - val_loss: 3.6037e-09
Epoch 970/1000

Epoch 00970: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.6093e-09 - val_loss: 3.5804e-09
Epoch 971/1000

Epoch 00971: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5836e-09 - val_loss: 3.5525e-09
Epoch 972/1000

Epoch 00972: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5578e-09 - val_loss: 3.5268e-09
Epoch 973/1000

Epoch 00973: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5324e-09 - val_loss: 3.5018e-09
Epoch 974/1000

Epoch 00974: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.5074e-09 - val_loss: 3.4760e-09
Epoch 975/1000

Epoch 00975: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4820e-09 - val_loss: 3.4512e-09
Epoch 976/1000

Epoch 00976: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4567e-09 - val_loss: 3.4262e-09
Epoch 977/1000

Epoch 00977: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4321e-09 - val_loss: 3.4022e-09
Epoch 978/1000

Epoch 00978: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.4081e-09 - val_loss: 3.3777e-09
Epoch 979/1000

Epoch 00979: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3834e-09 - val_loss: 3.3518e-09
Epoch 980/1000

Epoch 00980: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3571e-09 - val_loss: 3.3278e-09
Epoch 981/1000

Epoch 00981: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3337e-09 - val_loss: 3.3036e-09
Epoch 982/1000

Epoch 00982: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.3095e-09 - val_loss: 3.2798e-09
Epoch 983/1000

Epoch 00983: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2848e-09 - val_loss: 3.2564e-09
Epoch 984/1000

Epoch 00984: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2607e-09 - val_loss: 3.2317e-09
Epoch 985/1000

Epoch 00985: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2375e-09 - val_loss: 3.2088e-09
Epoch 986/1000

Epoch 00986: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.2146e-09 - val_loss: 3.1862e-09
Epoch 987/1000

Epoch 00987: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1915e-09 - val_loss: 3.1637e-09
Epoch 988/1000

Epoch 00988: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1692e-09 - val_loss: 3.1404e-09
Epoch 989/1000

Epoch 00989: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1462e-09 - val_loss: 3.1181e-09
Epoch 990/1000

Epoch 00990: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1244e-09 - val_loss: 3.0961e-09
Epoch 991/1000

Epoch 00991: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.1018e-09 - val_loss: 3.0745e-09
Epoch 992/1000

Epoch 00992: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0797e-09 - val_loss: 3.0522e-09
Epoch 993/1000

Epoch 00993: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0581e-09 - val_loss: 3.0323e-09
Epoch 994/1000

Epoch 00994: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0368e-09 - val_loss: 3.0109e-09
Epoch 995/1000

Epoch 00995: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 3.0152e-09 - val_loss: 2.9881e-09
Epoch 996/1000

Epoch 00996: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9937e-09 - val_loss: 2.9664e-09
Epoch 997/1000

Epoch 00997: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9724e-09 - val_loss: 2.9453e-09
Epoch 998/1000

Epoch 00998: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9516e-09 - val_loss: 2.9248e-09
Epoch 999/1000

Epoch 00999: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9306e-09 - val_loss: 2.9039e-09
Epoch 1000/1000

Epoch 01000: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-addition-test-33-512b-0.1dr-a-loss-new-dataset-con-12-08alice-lr001/multiplication_weights.h5
1024/1024 - 0s - loss: 2.9100e-09 - val_loss: 2.8836e-09
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 10.098 | eve: 10.842 | bob: 9.601Epoch   0:   0% | abe: 9.881 | eve: 10.711 | bob: 9.514Epoch   0:   1% | abe: 9.747 | eve: 10.639 | bob: 9.448Epoch   0:   2% | abe: 9.652 | eve: 10.572 | bob: 9.401Epoch   0:   3% | abe: 9.598 | eve: 10.517 | bob: 9.379Epoch   0:   3% | abe: 9.549 | eve: 10.489 | bob: 9.351Epoch   0:   4% | abe: 9.520 | eve: 10.472 | bob: 9.347Epoch   0:   5% | abe: 9.497 | eve: 10.454 | bob: 9.328Epoch   0:   6% | abe: 9.477 | eve: 10.443 | bob: 9.316Epoch   0:   7% | abe: 9.455 | eve: 10.439 | bob: 9.309Epoch   0:   7% | abe: 9.440 | eve: 10.434 | bob: 9.304Epoch   0:   8% | abe: 9.423 | eve: 10.430 | bob: 9.292Epoch   0:   9% | abe: 9.413 | eve: 10.429 | bob: 9.287Epoch   0:  10% | abe: 9.400 | eve: 10.422 | bob: 9.278Epoch   0:  10% | abe: 9.391 | eve: 10.420 | bob: 9.274Epoch   0:  11% | abe: 9.383 | eve: 10.416 | bob: 9.271Epoch   0:  12% | abe: 9.375 | eve: 10.409 | bob: 9.267Epoch   0:  13% | abe: 9.368 | eve: 10.403 | bob: 9.258Epoch   0:  14% | abe: 9.363 | eve: 10.393 | bob: 9.255Epoch   0:  14% | abe: 9.358 | eve: 10.389 | bob: 9.254Epoch   0:  15% | abe: 9.354 | eve: 10.385 | bob: 9.253Epoch   0:  16% | abe: 9.350 | eve: 10.381 | bob: 9.252Epoch   0:  17% | abe: 9.346 | eve: 10.375 | bob: 9.248Epoch   0:  17% | abe: 9.341 | eve: 10.370 | bob: 9.243Epoch   0:  18% | abe: 9.338 | eve: 10.368 | bob: 9.242Epoch   0:  19% | abe: 9.336 | eve: 10.366 | bob: 9.241Epoch   0:  20% | abe: 9.333 | eve: 10.365 | bob: 9.239Epoch   0:  21% | abe: 9.330 | eve: 10.366 | bob: 9.237Epoch   0:  21% | abe: 9.328 | eve: 10.368 | bob: 9.237Epoch   0:  22% | abe: 9.327 | eve: 10.367 | bob: 9.237Epoch   0:  23% | abe: 9.325 | eve: 10.368 | bob: 9.237Epoch   0:  24% | abe: 9.322 | eve: 10.368 | bob: 9.235Epoch   0:  25% | abe: 9.320 | eve: 10.368 | bob: 9.235Epoch   0:  25% | abe: 9.319 | eve: 10.368 | bob: 9.234Epoch   0:  26% | abe: 9.317 | eve: 10.368 | bob: 9.233Epoch   0:  27% | abe: 9.314 | eve: 10.368 | bob: 9.232Epoch   0:  28% | abe: 9.312 | eve: 10.369 | bob: 9.231Epoch   0:  28% | abe: 9.310 | eve: 10.369 | bob: 9.230Epoch   0:  29% | abe: 9.310 | eve: 10.370 | bob: 9.230Epoch   0:  30% | abe: 9.309 | eve: 10.370 | bob: 9.230Epoch   0:  31% | abe: 9.307 | eve: 10.369 | bob: 9.230Epoch   0:  32% | abe: 9.305 | eve: 10.372 | bob: 9.229Epoch   0:  32% | abe: 9.304 | eve: 10.374 | bob: 9.229Epoch   0:  33% | abe: 9.303 | eve: 10.379 | bob: 9.228Epoch   0:  34% | abe: 9.301 | eve: 10.384 | bob: 9.226Epoch   0:  35% | abe: 9.300 | eve: 10.388 | bob: 9.226Epoch   0:  35% | abe: 9.300 | eve: 10.392 | bob: 9.225Epoch   0:  36% | abe: 9.299 | eve: 10.394 | bob: 9.224Epoch   0:  37% | abe: 9.298 | eve: 10.399 | bob: 9.225Epoch   0:  38% | abe: 9.297 | eve: 10.404 | bob: 9.224Epoch   0:  39% | abe: 9.297 | eve: 10.409 | bob: 9.224Epoch   0:  39% | abe: 9.296 | eve: 10.412 | bob: 9.223Epoch   0:  40% | abe: 9.295 | eve: 10.415 | bob: 9.223Epoch   0:  41% | abe: 9.294 | eve: 10.418 | bob: 9.222Epoch   0:  42% | abe: 9.293 | eve: 10.421 | bob: 9.222Epoch   0:  42% | abe: 9.293 | eve: 10.424 | bob: 9.222Epoch   0:  43% | abe: 9.292 | eve: 10.426 | bob: 9.221Epoch   0:  44% | abe: 9.291 | eve: 10.429 | bob: 9.220Epoch   0:  45% | abe: 9.289 | eve: 10.430 | bob: 9.218Epoch   0:  46% | abe: 9.289 | eve: 10.433 | bob: 9.218Epoch   0:  46% | abe: 9.289 | eve: 10.436 | bob: 9.217Epoch   0:  47% | abe: 9.288 | eve: 10.437 | bob: 9.217Epoch   0:  48% | abe: 9.287 | eve: 10.439 | bob: 9.217Epoch   0:  49% | abe: 9.286 | eve: 10.442 | bob: 9.216Epoch   0:  50% | abe: 9.285 | eve: 10.444 | bob: 9.215Epoch   0:  50% | abe: 9.285 | eve: 10.446 | bob: 9.215Epoch   0:  51% | abe: 9.284 | eve: 10.448 | bob: 9.214Epoch   0:  52% | abe: 9.283 | eve: 10.451 | bob: 9.213Epoch   0:  53% | abe: 9.283 | eve: 10.451 | bob: 9.213Epoch   0:  53% | abe: 9.283 | eve: 10.453 | bob: 9.213Epoch   0:  54% | abe: 9.282 | eve: 10.454 | bob: 9.213Epoch   0:  55% | abe: 9.281 | eve: 10.456 | bob: 9.212Epoch   0:  56% | abe: 9.281 | eve: 10.457 | bob: 9.212Epoch   0:  57% | abe: 9.280 | eve: 10.458 | bob: 9.211Epoch   0:  57% | abe: 9.280 | eve: 10.460 | bob: 9.211Epoch   0:  58% | abe: 9.279 | eve: 10.461 | bob: 9.211Epoch   0:  59% | abe: 9.279 | eve: 10.462 | bob: 9.211Epoch   0:  60% | abe: 9.279 | eve: 10.463 | bob: 9.211Epoch   0:  60% | abe: 9.279 | eve: 10.465 | bob: 9.212Epoch   0:  61% | abe: 9.279 | eve: 10.466 | bob: 9.212Epoch   0:  62% | abe: 9.278 | eve: 10.467 | bob: 9.211Epoch   0:  63% | abe: 9.278 | eve: 10.468 | bob: 9.212Epoch   0:  64% | abe: 9.278 | eve: 10.469 | bob: 9.212Epoch   0:  64% | abe: 9.278 | eve: 10.471 | bob: 9.212Epoch   0:  65% | abe: 9.277 | eve: 10.472 | bob: 9.212Epoch   0:  66% | abe: 9.277 | eve: 10.473 | bob: 9.212Epoch   0:  67% | abe: 9.277 | eve: 10.474 | bob: 9.212Epoch   0:  67% | abe: 9.277 | eve: 10.476 | bob: 9.212Epoch   0:  68% | abe: 9.276 | eve: 10.477 | bob: 9.211Epoch   0:  69% | abe: 9.276 | eve: 10.479 | bob: 9.211Epoch   0:  70% | abe: 9.276 | eve: 10.480 | bob: 9.211Epoch   0:  71% | abe: 9.275 | eve: 10.482 | bob: 9.210Epoch   0:  71% | abe: 9.275 | eve: 10.482 | bob: 9.210Epoch   0:  72% | abe: 9.274 | eve: 10.483 | bob: 9.209Epoch   0:  73% | abe: 9.274 | eve: 10.484 | bob: 9.209Epoch   0:  74% | abe: 9.274 | eve: 10.485 | bob: 9.209Epoch   0:  75% | abe: 9.273 | eve: 10.486 | bob: 9.209Epoch   0:  75% | abe: 9.273 | eve: 10.487 | bob: 9.209Epoch   0:  76% | abe: 9.273 | eve: 10.487 | bob: 9.209Epoch   0:  77% | abe: 9.273 | eve: 10.488 | bob: 9.209Epoch   0:  78% | abe: 9.273 | eve: 10.488 | bob: 9.208Epoch   0:  78% | abe: 9.273 | eve: 10.489 | bob: 9.208Epoch   0:  79% | abe: 9.272 | eve: 10.490 | bob: 9.208Epoch   0:  80% | abe: 9.272 | eve: 10.491 | bob: 9.208Epoch   0:  81% | abe: 9.271 | eve: 10.492 | bob: 9.207Epoch   0:  82% | abe: 9.271 | eve: 10.493 | bob: 9.207Epoch   0:  82% | abe: 9.271 | eve: 10.494 | bob: 9.207Epoch   0:  83% | abe: 9.271 | eve: 10.494 | bob: 9.207Epoch   0:  84% | abe: 9.271 | eve: 10.495 | bob: 9.207Epoch   0:  85% | abe: 9.271 | eve: 10.495 | bob: 9.207Epoch   0:  85% | abe: 9.271 | eve: 10.495 | bob: 9.207Epoch   0:  86% | abe: 9.270 | eve: 10.493 | bob: 9.207Epoch   0:  87% | abe: 9.270 | eve: 10.491 | bob: 9.206Epoch   0:  88% | abe: 9.270 | eve: 10.489 | bob: 9.206Epoch   0:  89% | abe: 9.270 | eve: 10.487 | bob: 9.205Epoch   0:  89% | abe: 9.269 | eve: 10.484 | bob: 9.205Epoch   0:  90% | abe: 9.269 | eve: 10.482 | bob: 9.204Epoch   0:  91% | abe: 9.269 | eve: 10.480 | bob: 9.204Epoch   0:  92% | abe: 9.269 | eve: 10.478 | bob: 9.203Epoch   0:  92% | abe: 9.269 | eve: 10.476 | bob: 9.203Epoch   0:  93% | abe: 9.269 | eve: 10.474 | bob: 9.203Epoch   0:  94% | abe: 9.269 | eve: 10.472 | bob: 9.202Epoch   0:  95% | abe: 9.269 | eve: 10.470 | bob: 9.202Epoch   0:  96% | abe: 9.269 | eve: 10.468 | bob: 9.202Epoch   0:  96% | abe: 9.268 | eve: 10.466 | bob: 9.202Epoch   0:  97% | abe: 9.268 | eve: 10.465 | bob: 9.202Epoch   0:  98% | abe: 9.268 | eve: 10.463 | bob: 9.202Epoch   0:  99% | abe: 9.268 | eve: 10.462 | bob: 9.202
New best Bob loss 9.201614302669533 at epoch 0
Epoch   1:   0% | abe: 9.252 | eve: 10.366 | bob: 9.175Epoch   1:   0% | abe: 9.259 | eve: 10.320 | bob: 9.182Epoch   1:   1% | abe: 9.251 | eve: 10.321 | bob: 9.184Epoch   1:   2% | abe: 9.253 | eve: 10.316 | bob: 9.196Epoch   1:   3% | abe: 9.243 | eve: 10.320 | bob: 9.188Epoch   1:   3% | abe: 9.251 | eve: 10.324 | bob: 9.198Epoch   1:   4% | abe: 9.252 | eve: 10.320 | bob: 9.200Epoch   1:   5% | abe: 9.253 | eve: 10.314 | bob: 9.198Epoch   1:   6% | abe: 9.254 | eve: 10.319 | bob: 9.198Epoch   1:   7% | abe: 9.254 | eve: 10.317 | bob: 9.200Epoch   1:   7% | abe: 9.252 | eve: 10.316 | bob: 9.195Epoch   1:   8% | abe: 9.251 | eve: 10.321 | bob: 9.192Epoch   1:   9% | abe: 9.248 | eve: 10.323 | bob: 9.190Epoch   1:  10% | abe: 9.248 | eve: 10.323 | bob: 9.190Epoch   1:  10% | abe: 9.249 | eve: 10.326 | bob: 9.190Epoch   1:  11% | abe: 9.249 | eve: 10.327 | bob: 9.189Epoch   1:  12% | abe: 9.249 | eve: 10.328 | bob: 9.188Epoch   1:  13% | abe: 9.249 | eve: 10.331 | bob: 9.189Epoch   1:  14% | abe: 9.248 | eve: 10.334 | bob: 9.188Epoch   1:  14% | abe: 9.246 | eve: 10.337 | bob: 9.186Epoch   1:  15% | abe: 9.247 | eve: 10.338 | bob: 9.187Epoch   1:  16% | abe: 9.247 | eve: 10.339 | bob: 9.187Epoch   1:  17% | abe: 9.246 | eve: 10.339 | bob: 9.184Epoch   1:  17% | abe: 9.246 | eve: 10.339 | bob: 9.183Epoch   1:  18% | abe: 9.245 | eve: 10.341 | bob: 9.183Epoch   1:  19% | abe: 9.245 | eve: 10.344 | bob: 9.183Epoch   1:  20% | abe: 9.244 | eve: 10.345 | bob: 9.182Epoch   1:  21% | abe: 9.243 | eve: 10.347 | bob: 9.182Epoch   1:  21% | abe: 9.244 | eve: 10.347 | bob: 9.181Epoch   1:  22% | abe: 9.243 | eve: 10.346 | bob: 9.180Epoch   1:  23% | abe: 9.242 | eve: 10.346 | bob: 9.179Epoch   1:  24% | abe: 9.242 | eve: 10.348 | bob: 9.178Epoch   1:  25% | abe: 9.243 | eve: 10.349 | bob: 9.178Epoch   1:  25% | abe: 9.242 | eve: 10.351 | bob: 9.177Epoch   1:  26% | abe: 9.242 | eve: 10.351 | bob: 9.178Epoch   1:  27% | abe: 9.242 | eve: 10.352 | bob: 9.177Epoch   1:  28% | abe: 9.242 | eve: 10.352 | bob: 9.177Epoch   1:  28% | abe: 9.243 | eve: 10.353 | bob: 9.178Epoch   1:  29% | abe: 9.243 | eve: 10.356 | bob: 9.178Epoch   1:  30% | abe: 9.243 | eve: 10.357 | bob: 9.178Epoch   1:  31% | abe: 9.242 | eve: 10.358 | bob: 9.178Epoch   1:  32% | abe: 9.242 | eve: 10.359 | bob: 9.178Epoch   1:  32% | abe: 9.242 | eve: 10.360 | bob: 9.178Epoch   1:  33% | abe: 9.241 | eve: 10.359 | bob: 9.178Epoch   1:  34% | abe: 9.242 | eve: 10.360 | bob: 9.179Epoch   1:  35% | abe: 9.241 | eve: 10.360 | bob: 9.178Epoch   1:  35% | abe: 9.241 | eve: 10.361 | bob: 9.178Epoch   1:  36% | abe: 9.240 | eve: 10.361 | bob: 9.177Epoch   1:  37% | abe: 9.240 | eve: 10.361 | bob: 9.177Epoch   1:  38% | abe: 9.240 | eve: 10.362 | bob: 9.177Epoch   1:  39% | abe: 9.239 | eve: 10.362 | bob: 9.177Epoch   1:  39% | abe: 9.238 | eve: 10.362 | bob: 9.175Epoch   1:  40% | abe: 9.237 | eve: 10.363 | bob: 9.175Epoch   1:  41% | abe: 9.237 | eve: 10.362 | bob: 9.174Epoch   1:  42% | abe: 9.236 | eve: 10.362 | bob: 9.173Epoch   1:  42% | abe: 9.236 | eve: 10.362 | bob: 9.173Epoch   1:  43% | abe: 9.235 | eve: 10.362 | bob: 9.172Epoch   1:  44% | abe: 9.235 | eve: 10.363 | bob: 9.172Epoch   1:  45% | abe: 9.234 | eve: 10.362 | bob: 9.171Epoch   1:  46% | abe: 9.234 | eve: 10.363 | bob: 9.170Epoch   1:  46% | abe: 9.232 | eve: 10.363 | bob: 9.169Epoch   1:  47% | abe: 9.232 | eve: 10.363 | bob: 9.167Epoch   1:  48% | abe: 9.231 | eve: 10.364 | bob: 9.166Epoch   1:  49% | abe: 9.229 | eve: 10.365 | bob: 9.165Epoch   1:  50% | abe: 9.228 | eve: 10.364 | bob: 9.164Epoch   1:  50% | abe: 9.227 | eve: 10.363 | bob: 9.163Epoch   1:  51% | abe: 9.226 | eve: 10.360 | bob: 9.161Epoch   1:  52% | abe: 9.225 | eve: 10.358 | bob: 9.160Epoch   1:  53% | abe: 9.224 | eve: 10.355 | bob: 9.159Epoch   1:  53% | abe: 9.223 | eve: 10.352 | bob: 9.158Epoch   1:  54% | abe: 9.221 | eve: 10.349 | bob: 9.156Epoch   1:  55% | abe: 9.220 | eve: 10.347 | bob: 9.154Epoch   1:  56% | abe: 9.220 | eve: 10.344 | bob: 9.153Epoch   1:  57% | abe: 9.218 | eve: 10.342 | bob: 9.152Epoch   1:  57% | abe: 9.217 | eve: 10.340 | bob: 9.151Epoch   1:  58% | abe: 9.215 | eve: 10.338 | bob: 9.149Epoch   1:  59% | abe: 9.213 | eve: 10.337 | bob: 9.148Epoch   1:  60% | abe: 9.212 | eve: 10.337 | bob: 9.146Epoch   1:  60% | abe: 9.210 | eve: 10.339 | bob: 9.144Epoch   1:  61% | abe: 9.208 | eve: 10.340 | bob: 9.143Epoch   1:  62% | abe: 9.207 | eve: 10.341 | bob: 9.141Epoch   1:  63% | abe: 9.205 | eve: 10.342 | bob: 9.138Epoch   1:  64% | abe: 9.202 | eve: 10.343 | bob: 9.136Epoch   1:  64% | abe: 9.200 | eve: 10.344 | bob: 9.134Epoch   1:  65% | abe: 9.198 | eve: 10.346 | bob: 9.132Epoch   1:  66% | abe: 9.196 | eve: 10.347 | bob: 9.129Epoch   1:  67% | abe: 9.193 | eve: 10.348 | bob: 9.127Epoch   1:  67% | abe: 9.191 | eve: 10.349 | bob: 9.124Epoch   1:  68% | abe: 9.189 | eve: 10.351 | bob: 9.122Epoch   1:  69% | abe: 9.186 | eve: 10.352 | bob: 9.120Epoch   1:  70% | abe: 9.184 | eve: 10.354 | bob: 9.117Epoch   1:  71% | abe: 9.181 | eve: 10.356 | bob: 9.115Epoch   1:  71% | abe: 9.179 | eve: 10.356 | bob: 9.111Epoch   1:  72% | abe: 9.176 | eve: 10.358 | bob: 9.109Epoch   1:  73% | abe: 9.173 | eve: 10.359 | bob: 9.105Epoch   1:  74% | abe: 9.170 | eve: 10.360 | bob: 9.103Epoch   1:  75% | abe: 9.167 | eve: 10.361 | bob: 9.100Epoch   1:  75% | abe: 9.165 | eve: 10.361 | bob: 9.098Epoch   1:  76% | abe: 9.162 | eve: 10.362 | bob: 9.094Epoch   1:  77% | abe: 9.159 | eve: 10.363 | bob: 9.091Epoch   1:  78% | abe: 9.156 | eve: 10.364 | bob: 9.087Epoch   1:  78% | abe: 9.152 | eve: 10.365 | bob: 9.084Epoch   1:  79% | abe: 9.148 | eve: 10.366 | bob: 9.079Epoch   1:  80% | abe: 9.144 | eve: 10.366 | bob: 9.075Epoch   1:  81% | abe: 9.141 | eve: 10.368 | bob: 9.071Epoch   1:  82% | abe: 9.137 | eve: 10.369 | bob: 9.067Epoch   1:  82% | abe: 9.133 | eve: 10.369 | bob: 9.063Epoch   1:  83% | abe: 9.129 | eve: 10.371 | bob: 9.059Epoch   1:  84% | abe: 9.125 | eve: 10.371 | bob: 9.054Epoch   1:  85% | abe: 9.120 | eve: 10.372 | bob: 9.050Epoch   1:  85% | abe: 9.116 | eve: 10.373 | bob: 9.046Epoch   1:  86% | abe: 9.112 | eve: 10.373 | bob: 9.041Epoch   1:  87% | abe: 9.107 | eve: 10.374 | bob: 9.037Epoch   1:  88% | abe: 9.103 | eve: 10.375 | bob: 9.032Epoch   1:  89% | abe: 9.099 | eve: 10.377 | bob: 9.027Epoch   1:  89% | abe: 9.093 | eve: 10.378 | bob: 9.022Epoch   1:  90% | abe: 9.089 | eve: 10.379 | bob: 9.017Epoch   1:  91% | abe: 9.084 | eve: 10.380 | bob: 9.012Epoch   1:  92% | abe: 9.078 | eve: 10.381 | bob: 9.007Epoch   1:  92% | abe: 9.073 | eve: 10.381 | bob: 9.002Epoch   1:  93% | abe: 9.068 | eve: 10.382 | bob: 8.997Epoch   1:  94% | abe: 9.063 | eve: 10.384 | bob: 8.991Epoch   1:  95% | abe: 9.057 | eve: 10.384 | bob: 8.986Epoch   1:  96% | abe: 9.051 | eve: 10.385 | bob: 8.980Epoch   1:  96% | abe: 9.046 | eve: 10.386 | bob: 8.975Epoch   1:  97% | abe: 9.041 | eve: 10.387 | bob: 8.970Epoch   1:  98% | abe: 9.037 | eve: 10.389 | bob: 8.966Epoch   1:  99% | abe: 9.032 | eve: 10.389 | bob: 8.961
New best Bob loss 8.960913923201861 at epoch 1
Epoch   2:   0% | abe: 8.416 | eve: 10.508 | bob: 8.367Epoch   2:   0% | abe: 8.403 | eve: 10.513 | bob: 8.317Epoch   2:   1% | abe: 8.370 | eve: 10.528 | bob: 8.279Epoch   2:   2% | abe: 8.343 | eve: 10.514 | bob: 8.267Epoch   2:   3% | abe: 8.335 | eve: 10.516 | bob: 8.245Epoch   2:   3% | abe: 8.323 | eve: 10.524 | bob: 8.232Epoch   2:   4% | abe: 8.308 | eve: 10.527 | bob: 8.210Epoch   2:   5% | abe: 8.288 | eve: 10.529 | bob: 8.190Epoch   2:   6% | abe: 8.268 | eve: 10.525 | bob: 8.169Epoch   2:   7% | abe: 8.245 | eve: 10.524 | bob: 8.158Epoch   2:   7% | abe: 8.227 | eve: 10.525 | bob: 8.131Epoch   2:   8% | abe: 8.201 | eve: 10.524 | bob: 8.112Epoch   2:   9% | abe: 8.188 | eve: 10.527 | bob: 8.092Epoch   2:  10% | abe: 8.169 | eve: 10.523 | bob: 8.075Epoch   2:  10% | abe: 8.154 | eve: 10.522 | bob: 8.057Epoch   2:  11% | abe: 8.135 | eve: 10.524 | bob: 8.041Epoch   2:  12% | abe: 8.122 | eve: 10.524 | bob: 8.025Epoch   2:  13% | abe: 8.104 | eve: 10.526 | bob: 8.001Epoch   2:  14% | abe: 8.080 | eve: 10.526 | bob: 7.987Epoch   2:  14% | abe: 8.065 | eve: 10.528 | bob: 7.969Epoch   2:  15% | abe: 8.050 | eve: 10.531 | bob: 7.950Epoch   2:  16% | abe: 8.031 | eve: 10.532 | bob: 7.935Epoch   2:  17% | abe: 8.014 | eve: 10.534 | bob: 7.917Epoch   2:  17% | abe: 7.996 | eve: 10.533 | bob: 7.898Epoch   2:  18% | abe: 7.979 | eve: 10.533 | bob: 7.879Epoch   2:  19% | abe: 7.960 | eve: 10.532 | bob: 7.860Epoch   2:  20% | abe: 7.941 | eve: 10.533 | bob: 7.845Epoch   2:  21% | abe: 7.926 | eve: 10.534 | bob: 7.830Epoch   2:  21% | abe: 7.907 | eve: 10.533 | bob: 7.813Epoch   2:  22% | abe: 7.891 | eve: 10.535 | bob: 7.794Epoch   2:  23% | abe: 7.875 | eve: 10.536 | bob: 7.779Epoch   2:  24% | abe: 7.857 | eve: 10.535 | bob: 7.764Epoch   2:  25% | abe: 7.841 | eve: 10.536 | bob: 7.747Epoch   2:  25% | abe: 7.823 | eve: 10.537 | bob: 7.729Epoch   2:  26% | abe: 7.806 | eve: 10.537 | bob: 7.712Epoch   2:  27% | abe: 7.789 | eve: 10.539 | bob: 7.694Epoch   2:  28% | abe: 7.772 | eve: 10.539 | bob: 7.680Epoch   2:  28% | abe: 7.758 | eve: 10.540 | bob: 7.667Epoch   2:  29% | abe: 7.743 | eve: 10.540 | bob: 7.653Epoch   2:  30% | abe: 7.729 | eve: 10.539 | bob: 7.641Epoch   2:  31% | abe: 7.717 | eve: 10.540 | bob: 7.626Epoch   2:  32% | abe: 7.702 | eve: 10.540 | bob: 7.615Epoch   2:  32% | abe: 7.691 | eve: 10.542 | bob: 7.606Epoch   2:  33% | abe: 7.681 | eve: 10.542 | bob: 7.593Epoch   2:  34% | abe: 7.669 | eve: 10.542 | bob: 7.585Epoch   2:  35% | abe: 7.660 | eve: 10.543 | bob: 7.576Epoch   2:  35% | abe: 7.648 | eve: 10.543 | bob: 7.565Epoch   2:  36% | abe: 7.639 | eve: 10.542 | bob: 7.559Epoch   2:  37% | abe: 7.631 | eve: 10.543 | bob: 7.550Epoch   2:  38% | abe: 7.619 | eve: 10.543 | bob: 7.539Epoch   2:  39% | abe: 7.609 | eve: 10.544 | bob: 7.528Epoch   2:  39% | abe: 7.598 | eve: 10.545 | bob: 7.516Epoch   2:  40% | abe: 7.587 | eve: 10.546 | bob: 7.508Epoch   2:  41% | abe: 7.577 | eve: 10.548 | bob: 7.494Epoch   2:  42% | abe: 7.567 | eve: 10.548 | bob: 7.487Epoch   2:  42% | abe: 7.558 | eve: 10.548 | bob: 7.476Epoch   2:  43% | abe: 7.545 | eve: 10.549 | bob: 7.464Epoch   2:  44% | abe: 7.534 | eve: 10.549 | bob: 7.453Epoch   2:  45% | abe: 7.523 | eve: 10.548 | bob: 7.442Epoch   2:  46% | abe: 7.512 | eve: 10.549 | bob: 7.432Epoch   2:  46% | abe: 7.503 | eve: 10.549 | bob: 7.424Epoch   2:  47% | abe: 7.494 | eve: 10.550 | bob: 7.415Epoch   2:  48% | abe: 7.485 | eve: 10.551 | bob: 7.405Epoch   2:  49% | abe: 7.476 | eve: 10.552 | bob: 7.396Epoch   2:  50% | abe: 7.467 | eve: 10.552 | bob: 7.389Epoch   2:  50% | abe: 7.460 | eve: 10.552 | bob: 7.381Epoch   2:  51% | abe: 7.451 | eve: 10.552 | bob: 7.373Epoch   2:  52% | abe: 7.443 | eve: 10.553 | bob: 7.366Epoch   2:  53% | abe: 7.436 | eve: 10.554 | bob: 7.359Epoch   2:  53% | abe: 7.430 | eve: 10.555 | bob: 7.352Epoch   2:  54% | abe: 7.422 | eve: 10.555 | bob: 7.344Epoch   2:  55% | abe: 7.415 | eve: 10.556 | bob: 7.337Epoch   2:  56% | abe: 7.406 | eve: 10.555 | bob: 7.329Epoch   2:  57% | abe: 7.399 | eve: 10.556 | bob: 7.323Epoch   2:  57% | abe: 7.391 | eve: 10.556 | bob: 7.316Epoch   2:  58% | abe: 7.383 | eve: 10.556 | bob: 7.308Epoch   2:  59% | abe: 7.375 | eve: 10.555 | bob: 7.299Epoch   2:  60% | abe: 7.367 | eve: 10.556 | bob: 7.291Epoch   2:  60% | abe: 7.359 | eve: 10.556 | bob: 7.284Epoch   2:  61% | abe: 7.351 | eve: 10.557 | bob: 7.276Epoch   2:  62% | abe: 7.344 | eve: 10.557 | bob: 7.269Epoch   2:  63% | abe: 7.337 | eve: 10.557 | bob: 7.262Epoch   2:  64% | abe: 7.330 | eve: 10.557 | bob: 7.256Epoch   2:  64% | abe: 7.323 | eve: 10.558 | bob: 7.249Epoch   2:  65% | abe: 7.315 | eve: 10.559 | bob: 7.242Epoch   2:  66% | abe: 7.309 | eve: 10.558 | bob: 7.235Epoch   2:  67% | abe: 7.301 | eve: 10.558 | bob: 7.228Epoch   2:  67% | abe: 7.294 | eve: 10.559 | bob: 7.222Epoch   2:  68% | abe: 7.288 | eve: 10.560 | bob: 7.215Epoch   2:  69% | abe: 7.281 | eve: 10.560 | bob: 7.208Epoch   2:  70% | abe: 7.274 | eve: 10.561 | bob: 7.203Epoch   2:  71% | abe: 7.267 | eve: 10.561 | bob: 7.196Epoch   2:  71% | abe: 7.261 | eve: 10.561 | bob: 7.191Epoch   2:  72% | abe: 7.254 | eve: 10.561 | bob: 7.185Epoch   2:  73% | abe: 7.248 | eve: 10.561 | bob: 7.179Epoch   2:  74% | abe: 7.242 | eve: 10.562 | bob: 7.172Epoch   2:  75% | abe: 7.236 | eve: 10.562 | bob: 7.167Epoch   2:  75% | abe: 7.229 | eve: 10.562 | bob: 7.159Epoch   2:  76% | abe: 7.223 | eve: 10.563 | bob: 7.153Epoch   2:  77% | abe: 7.217 | eve: 10.563 | bob: 7.147Epoch   2:  78% | abe: 7.211 | eve: 10.563 | bob: 7.141Epoch   2:  78% | abe: 7.206 | eve: 10.563 | bob: 7.137Epoch   2:  79% | abe: 7.200 | eve: 10.563 | bob: 7.131Epoch   2:  80% | abe: 7.194 | eve: 10.563 | bob: 7.126Epoch   2:  81% | abe: 7.188 | eve: 10.563 | bob: 7.121Epoch   2:  82% | abe: 7.183 | eve: 10.563 | bob: 7.115Epoch   2:  82% | abe: 7.177 | eve: 10.563 | bob: 7.109Epoch   2:  83% | abe: 7.172 | eve: 10.563 | bob: 7.104Epoch   2:  84% | abe: 7.166 | eve: 10.563 | bob: 7.098Epoch   2:  85% | abe: 7.160 | eve: 10.564 | bob: 7.093Epoch   2:  85% | abe: 7.154 | eve: 10.564 | bob: 7.087Epoch   2:  86% | abe: 7.149 | eve: 10.565 | bob: 7.082Epoch   2:  87% | abe: 7.143 | eve: 10.566 | bob: 7.076Epoch   2:  88% | abe: 7.138 | eve: 10.566 | bob: 7.071Epoch   2:  89% | abe: 7.132 | eve: 10.567 | bob: 7.067Epoch   2:  89% | abe: 7.126 | eve: 10.567 | bob: 7.061Epoch   2:  90% | abe: 7.121 | eve: 10.567 | bob: 7.056Epoch   2:  91% | abe: 7.115 | eve: 10.568 | bob: 7.050Epoch   2:  92% | abe: 7.109 | eve: 10.568 | bob: 7.044Epoch   2:  92% | abe: 7.104 | eve: 10.568 | bob: 7.039Epoch   2:  93% | abe: 7.099 | eve: 10.568 | bob: 7.034Epoch   2:  94% | abe: 7.094 | eve: 10.569 | bob: 7.029Epoch   2:  95% | abe: 7.089 | eve: 10.569 | bob: 7.024Epoch   2:  96% | abe: 7.084 | eve: 10.569 | bob: 7.019Epoch   2:  96% | abe: 7.078 | eve: 10.570 | bob: 7.014Epoch   2:  97% | abe: 7.073 | eve: 10.570 | bob: 7.009Epoch   2:  98% | abe: 7.068 | eve: 10.571 | bob: 7.004Epoch   2:  99% | abe: 7.063 | eve: 10.571 | bob: 6.999
New best Bob loss 6.9990856123517915 at epoch 2
Epoch   3:   0% | abe: 6.401 | eve: 10.632 | bob: 6.330Epoch   3:   0% | abe: 6.396 | eve: 10.620 | bob: 6.349Epoch   3:   1% | abe: 6.393 | eve: 10.638 | bob: 6.356Epoch   3:   2% | abe: 6.393 | eve: 10.624 | bob: 6.347Epoch   3:   3% | abe: 6.389 | eve: 10.635 | bob: 6.338Epoch   3:   3% | abe: 6.387 | eve: 10.629 | bob: 6.342Epoch   3:   4% | abe: 6.384 | eve: 10.634 | bob: 6.337Epoch   3:   5% | abe: 6.387 | eve: 10.635 | bob: 6.346Epoch   3:   6% | abe: 6.385 | eve: 10.633 | bob: 6.344Epoch   3:   7% | abe: 6.385 | eve: 10.628 | bob: 6.348Epoch   3:   7% | abe: 6.390 | eve: 10.628 | bob: 6.352Epoch   3:   8% | abe: 6.383 | eve: 10.628 | bob: 6.348Epoch   3:   9% | abe: 6.377 | eve: 10.625 | bob: 6.341Epoch   3:  10% | abe: 6.373 | eve: 10.625 | bob: 6.338Epoch   3:  10% | abe: 6.370 | eve: 10.619 | bob: 6.336Epoch   3:  11% | abe: 6.372 | eve: 10.618 | bob: 6.341Epoch   3:  12% | abe: 6.370 | eve: 10.621 | bob: 6.339Epoch   3:  13% | abe: 6.366 | eve: 10.622 | bob: 6.335Epoch   3:  14% | abe: 6.362 | eve: 10.624 | bob: 6.331Epoch   3:  14% | abe: 6.360 | eve: 10.627 | bob: 6.327Epoch   3:  15% | abe: 6.356 | eve: 10.626 | bob: 6.323Epoch   3:  16% | abe: 6.355 | eve: 10.626 | bob: 6.323Epoch   3:  17% | abe: 6.353 | eve: 10.626 | bob: 6.324Epoch   3:  17% | abe: 6.350 | eve: 10.627 | bob: 6.324Epoch   3:  18% | abe: 6.349 | eve: 10.627 | bob: 6.323Epoch   3:  19% | abe: 6.348 | eve: 10.627 | bob: 6.323Epoch   3:  20% | abe: 6.343 | eve: 10.628 | bob: 6.317Epoch   3:  21% | abe: 6.340 | eve: 10.628 | bob: 6.315Epoch   3:  21% | abe: 6.336 | eve: 10.628 | bob: 6.311Epoch   3:  22% | abe: 6.333 | eve: 10.627 | bob: 6.309Epoch   3:  23% | abe: 6.330 | eve: 10.628 | bob: 6.309Epoch   3:  24% | abe: 6.328 | eve: 10.629 | bob: 6.308Epoch   3:  25% | abe: 6.328 | eve: 10.630 | bob: 6.307Epoch   3:  25% | abe: 6.324 | eve: 10.630 | bob: 6.304Epoch   3:  26% | abe: 6.320 | eve: 10.630 | bob: 6.301Epoch   3:  27% | abe: 6.316 | eve: 10.630 | bob: 6.298Epoch   3:  28% | abe: 6.311 | eve: 10.630 | bob: 6.293Epoch   3:  28% | abe: 6.309 | eve: 10.630 | bob: 6.291Epoch   3:  29% | abe: 6.307 | eve: 10.629 | bob: 6.289Epoch   3:  30% | abe: 6.305 | eve: 10.629 | bob: 6.287Epoch   3:  31% | abe: 6.301 | eve: 10.628 | bob: 6.284Epoch   3:  32% | abe: 6.299 | eve: 10.628 | bob: 6.282Epoch   3:  32% | abe: 6.297 | eve: 10.628 | bob: 6.279Epoch   3:  33% | abe: 6.295 | eve: 10.628 | bob: 6.277Epoch   3:  34% | abe: 6.293 | eve: 10.629 | bob: 6.276Epoch   3:  35% | abe: 6.288 | eve: 10.629 | bob: 6.272Epoch   3:  35% | abe: 6.285 | eve: 10.628 | bob: 6.270Epoch   3:  36% | abe: 6.284 | eve: 10.628 | bob: 6.268Epoch   3:  37% | abe: 6.279 | eve: 10.629 | bob: 6.265Epoch   3:  38% | abe: 6.277 | eve: 10.630 | bob: 6.262Epoch   3:  39% | abe: 6.274 | eve: 10.629 | bob: 6.260Epoch   3:  39% | abe: 6.272 | eve: 10.629 | bob: 6.258Epoch   3:  40% | abe: 6.270 | eve: 10.629 | bob: 6.256Epoch   3:  41% | abe: 6.267 | eve: 10.629 | bob: 6.253Epoch   3:  42% | abe: 6.266 | eve: 10.629 | bob: 6.252Epoch   3:  42% | abe: 6.263 | eve: 10.629 | bob: 6.250Epoch   3:  43% | abe: 6.261 | eve: 10.629 | bob: 6.248Epoch   3:  44% | abe: 6.257 | eve: 10.630 | bob: 6.245Epoch   3:  45% | abe: 6.256 | eve: 10.631 | bob: 6.243Epoch   3:  46% | abe: 6.254 | eve: 10.631 | bob: 6.242Epoch   3:  46% | abe: 6.253 | eve: 10.631 | bob: 6.241Epoch   3:  47% | abe: 6.251 | eve: 10.631 | bob: 6.238Epoch   3:  48% | abe: 6.249 | eve: 10.632 | bob: 6.237Epoch   3:  49% | abe: 6.247 | eve: 10.631 | bob: 6.236Epoch   3:  50% | abe: 6.246 | eve: 10.631 | bob: 6.235Epoch   3:  50% | abe: 6.244 | eve: 10.631 | bob: 6.233Epoch   3:  51% | abe: 6.242 | eve: 10.631 | bob: 6.231Epoch   3:  52% | abe: 6.240 | eve: 10.629 | bob: 6.230Epoch   3:  53% | abe: 6.237 | eve: 10.629 | bob: 6.228Epoch   3:  53% | abe: 6.236 | eve: 10.629 | bob: 6.228Epoch   3:  54% | abe: 6.233 | eve: 10.629 | bob: 6.225Epoch   3:  55% | abe: 6.231 | eve: 10.629 | bob: 6.222Epoch   3:  56% | abe: 6.229 | eve: 10.629 | bob: 6.220Epoch   3:  57% | abe: 6.227 | eve: 10.628 | bob: 6.219Epoch   3:  57% | abe: 6.225 | eve: 10.628 | bob: 6.218Epoch   3:  58% | abe: 6.223 | eve: 10.628 | bob: 6.216Epoch   3:  59% | abe: 6.222 | eve: 10.628 | bob: 6.215Epoch   3:  60% | abe: 6.220 | eve: 10.627 | bob: 6.214Epoch   3:  60% | abe: 6.218 | eve: 10.627 | bob: 6.213Epoch   3:  61% | abe: 6.217 | eve: 10.627 | bob: 6.211Epoch   3:  62% | abe: 6.215 | eve: 10.627 | bob: 6.209Epoch   3:  63% | abe: 6.214 | eve: 10.627 | bob: 6.208Epoch   3:  64% | abe: 6.212 | eve: 10.627 | bob: 6.206Epoch   3:  64% | abe: 6.210 | eve: 10.627 | bob: 6.205Epoch   3:  65% | abe: 6.209 | eve: 10.626 | bob: 6.204Epoch   3:  66% | abe: 6.207 | eve: 10.627 | bob: 6.202Epoch   3:  67% | abe: 6.205 | eve: 10.627 | bob: 6.200Epoch   3:  67% | abe: 6.204 | eve: 10.627 | bob: 6.199Epoch   3:  68% | abe: 6.203 | eve: 10.628 | bob: 6.197Epoch   3:  69% | abe: 6.202 | eve: 10.628 | bob: 6.197Epoch   3:  70% | abe: 6.201 | eve: 10.629 | bob: 6.195Epoch   3:  71% | abe: 6.199 | eve: 10.629 | bob: 6.194Epoch   3:  71% | abe: 6.198 | eve: 10.629 | bob: 6.193Epoch   3:  72% | abe: 6.197 | eve: 10.629 | bob: 6.191Epoch   3:  73% | abe: 6.196 | eve: 10.629 | bob: 6.191Epoch   3:  74% | abe: 6.194 | eve: 10.629 | bob: 6.189Epoch   3:  75% | abe: 6.193 | eve: 10.628 | bob: 6.188Epoch   3:  75% | abe: 6.193 | eve: 10.628 | bob: 6.187Epoch   3:  76% | abe: 6.191 | eve: 10.628 | bob: 6.185Epoch   3:  77% | abe: 6.190 | eve: 10.628 | bob: 6.184Epoch   3:  78% | abe: 6.190 | eve: 10.627 | bob: 6.183Epoch   3:  78% | abe: 6.190 | eve: 10.627 | bob: 6.183Epoch   3:  79% | abe: 6.188 | eve: 10.628 | bob: 6.181Epoch   3:  80% | abe: 6.187 | eve: 10.628 | bob: 6.180Epoch   3:  81% | abe: 6.186 | eve: 10.628 | bob: 6.179Epoch   3:  82% | abe: 6.185 | eve: 10.628 | bob: 6.178Epoch   3:  82% | abe: 6.184 | eve: 10.628 | bob: 6.177Epoch   3:  83% | abe: 6.184 | eve: 10.628 | bob: 6.176Epoch   3:  84% | abe: 6.182 | eve: 10.628 | bob: 6.174Epoch   3:  85% | abe: 6.181 | eve: 10.628 | bob: 6.174Epoch   3:  85% | abe: 6.180 | eve: 10.628 | bob: 6.172Epoch   3:  86% | abe: 6.179 | eve: 10.628 | bob: 6.172Epoch   3:  87% | abe: 6.178 | eve: 10.628 | bob: 6.171Epoch   3:  88% | abe: 6.176 | eve: 10.628 | bob: 6.169Epoch   3:  89% | abe: 6.176 | eve: 10.628 | bob: 6.169Epoch   3:  89% | abe: 6.175 | eve: 10.628 | bob: 6.169Epoch   3:  90% | abe: 6.175 | eve: 10.628 | bob: 6.169Epoch   3:  91% | abe: 6.174 | eve: 10.628 | bob: 6.168Epoch   3:  92% | abe: 6.173 | eve: 10.628 | bob: 6.167Epoch   3:  92% | abe: 6.173 | eve: 10.628 | bob: 6.167Epoch   3:  93% | abe: 6.172 | eve: 10.627 | bob: 6.166Epoch   3:  94% | abe: 6.171 | eve: 10.627 | bob: 6.164Epoch   3:  95% | abe: 6.171 | eve: 10.627 | bob: 6.163Epoch   3:  96% | abe: 6.170 | eve: 10.627 | bob: 6.162Epoch   3:  96% | abe: 6.169 | eve: 10.627 | bob: 6.161Epoch   3:  97% | abe: 6.167 | eve: 10.627 | bob: 6.160Epoch   3:  98% | abe: 6.167 | eve: 10.627 | bob: 6.160Epoch   3:  99% | abe: 6.166 | eve: 10.627 | bob: 6.159
New best Bob loss 6.159033556608938 at epoch 3
Epoch   4:   0% | abe: 6.016 | eve: 10.615 | bob: 6.018Epoch   4:   0% | abe: 6.039 | eve: 10.609 | bob: 6.045Epoch   4:   1% | abe: 6.046 | eve: 10.606 | bob: 6.048Epoch   4:   2% | abe: 6.057 | eve: 10.632 | bob: 6.049Epoch   4:   3% | abe: 6.048 | eve: 10.630 | bob: 6.051Epoch   4:   3% | abe: 6.054 | eve: 10.630 | bob: 6.062Epoch   4:   4% | abe: 6.066 | eve: 10.637 | bob: 6.072Epoch   4:   5% | abe: 6.073 | eve: 10.641 | bob: 6.076Epoch   4:   6% | abe: 6.073 | eve: 10.640 | bob: 6.076Epoch   4:   7% | abe: 6.072 | eve: 10.635 | bob: 6.070Epoch   4:   7% | abe: 6.070 | eve: 10.632 | bob: 6.062Epoch   4:   8% | abe: 6.068 | eve: 10.632 | bob: 6.058Epoch   4:   9% | abe: 6.066 | eve: 10.631 | bob: 6.057Epoch   4:  10% | abe: 6.066 | eve: 10.635 | bob: 6.051Epoch   4:  10% | abe: 6.065 | eve: 10.632 | bob: 6.047Epoch   4:  11% | abe: 6.069 | eve: 10.630 | bob: 6.050Epoch   4:  12% | abe: 6.068 | eve: 10.633 | bob: 6.049Epoch   4:  13% | abe: 6.070 | eve: 10.633 | bob: 6.054Epoch   4:  14% | abe: 6.070 | eve: 10.632 | bob: 6.053Epoch   4:  14% | abe: 6.067 | eve: 10.633 | bob: 6.052Epoch   4:  15% | abe: 6.064 | eve: 10.634 | bob: 6.050Epoch   4:  16% | abe: 6.067 | eve: 10.633 | bob: 6.054Epoch   4:  17% | abe: 6.070 | eve: 10.632 | bob: 6.055Epoch   4:  17% | abe: 6.067 | eve: 10.632 | bob: 6.054Epoch   4:  18% | abe: 6.066 | eve: 10.632 | bob: 6.054Epoch   4:  19% | abe: 6.066 | eve: 10.632 | bob: 6.054Epoch   4:  20% | abe: 6.067 | eve: 10.633 | bob: 6.055Epoch   4:  21% | abe: 6.068 | eve: 10.633 | bob: 6.055Epoch   4:  21% | abe: 6.068 | eve: 10.635 | bob: 6.055Epoch   4:  22% | abe: 6.066 | eve: 10.635 | bob: 6.053Epoch   4:  23% | abe: 6.066 | eve: 10.633 | bob: 6.054Epoch   4:  24% | abe: 6.067 | eve: 10.634 | bob: 6.056Epoch   4:  25% | abe: 6.069 | eve: 10.634 | bob: 6.058Epoch   4:  25% | abe: 6.071 | eve: 10.635 | bob: 6.060Epoch   4:  26% | abe: 6.071 | eve: 10.636 | bob: 6.059Epoch   4:  27% | abe: 6.069 | eve: 10.637 | bob: 6.058Epoch   4:  28% | abe: 6.068 | eve: 10.636 | bob: 6.058Epoch   4:  28% | abe: 6.068 | eve: 10.635 | bob: 6.057Epoch   4:  29% | abe: 6.068 | eve: 10.636 | bob: 6.058Epoch   4:  30% | abe: 6.068 | eve: 10.636 | bob: 6.059Epoch   4:  31% | abe: 6.069 | eve: 10.636 | bob: 6.060Epoch   4:  32% | abe: 6.068 | eve: 10.636 | bob: 6.059Epoch   4:  32% | abe: 6.068 | eve: 10.635 | bob: 6.058Epoch   4:  33% | abe: 6.067 | eve: 10.635 | bob: 6.059Epoch   4:  34% | abe: 6.068 | eve: 10.634 | bob: 6.061Epoch   4:  35% | abe: 6.069 | eve: 10.633 | bob: 6.061Epoch   4:  35% | abe: 6.068 | eve: 10.634 | bob: 6.062Epoch   4:  36% | abe: 6.069 | eve: 10.634 | bob: 6.062Epoch   4:  37% | abe: 6.069 | eve: 10.634 | bob: 6.062Epoch   4:  38% | abe: 6.068 | eve: 10.634 | bob: 6.062Epoch   4:  39% | abe: 6.068 | eve: 10.633 | bob: 6.062Epoch   4:  39% | abe: 6.068 | eve: 10.633 | bob: 6.060Epoch   4:  40% | abe: 6.067 | eve: 10.633 | bob: 6.060Epoch   4:  41% | abe: 6.067 | eve: 10.632 | bob: 6.061Epoch   4:  42% | abe: 6.068 | eve: 10.631 | bob: 6.063Epoch   4:  42% | abe: 6.069 | eve: 10.632 | bob: 6.065Epoch   4:  43% | abe: 6.070 | eve: 10.631 | bob: 6.067Epoch   4:  44% | abe: 6.070 | eve: 10.632 | bob: 6.067Epoch   4:  45% | abe: 6.070 | eve: 10.632 | bob: 6.067Epoch   4:  46% | abe: 6.071 | eve: 10.632 | bob: 6.068Epoch   4:  46% | abe: 6.070 | eve: 10.632 | bob: 6.067Epoch   4:  47% | abe: 6.071 | eve: 10.631 | bob: 6.067Epoch   4:  48% | abe: 6.070 | eve: 10.631 | bob: 6.066Epoch   4:  49% | abe: 6.069 | eve: 10.631 | bob: 6.065Epoch   4:  50% | abe: 6.069 | eve: 10.632 | bob: 6.065Epoch   4:  50% | abe: 6.069 | eve: 10.632 | bob: 6.065Epoch   4:  51% | abe: 6.070 | eve: 10.632 | bob: 6.066Epoch   4:  52% | abe: 6.069 | eve: 10.632 | bob: 6.065Epoch   4:  53% | abe: 6.070 | eve: 10.632 | bob: 6.066Epoch   4:  53% | abe: 6.071 | eve: 10.632 | bob: 6.067Epoch   4:  54% | abe: 6.071 | eve: 10.633 | bob: 6.067Epoch   4:  55% | abe: 6.071 | eve: 10.633 | bob: 6.067Epoch   4:  56% | abe: 6.072 | eve: 10.633 | bob: 6.068Epoch   4:  57% | abe: 6.071 | eve: 10.633 | bob: 6.068Epoch   4:  57% | abe: 6.071 | eve: 10.633 | bob: 6.068Epoch   4:  58% | abe: 6.070 | eve: 10.633 | bob: 6.068Epoch   4:  59% | abe: 6.071 | eve: 10.632 | bob: 6.069Epoch   4:  60% | abe: 6.070 | eve: 10.632 | bob: 6.068Epoch   4:  60% | abe: 6.069 | eve: 10.632 | bob: 6.067Epoch   4:  61% | abe: 6.068 | eve: 10.632 | bob: 6.066Epoch   4:  62% | abe: 6.068 | eve: 10.632 | bob: 6.066Epoch   4:  63% | abe: 6.068 | eve: 10.632 | bob: 6.065Epoch   4:  64% | abe: 6.067 | eve: 10.632 | bob: 6.065Epoch   4:  64% | abe: 6.067 | eve: 10.631 | bob: 6.065Epoch   4:  65% | abe: 6.067 | eve: 10.631 | bob: 6.064Epoch   4:  66% | abe: 6.066 | eve: 10.631 | bob: 6.064Epoch   4:  67% | abe: 6.067 | eve: 10.630 | bob: 6.065Epoch   4:  67% | abe: 6.067 | eve: 10.630 | bob: 6.065Epoch   4:  68% | abe: 6.067 | eve: 10.630 | bob: 6.065Epoch   4:  69% | abe: 6.066 | eve: 10.630 | bob: 6.064Epoch   4:  70% | abe: 6.066 | eve: 10.630 | bob: 6.064Epoch   4:  71% | abe: 6.066 | eve: 10.631 | bob: 6.064Epoch   4:  71% | abe: 6.067 | eve: 10.631 | bob: 6.065Epoch   4:  72% | abe: 6.066 | eve: 10.631 | bob: 6.065Epoch   4:  73% | abe: 6.067 | eve: 10.631 | bob: 6.066Epoch   4:  74% | abe: 6.067 | eve: 10.632 | bob: 6.067Epoch   4:  75% | abe: 6.067 | eve: 10.632 | bob: 6.066Epoch   4:  75% | abe: 6.067 | eve: 10.632 | bob: 6.067Epoch   4:  76% | abe: 6.066 | eve: 10.632 | bob: 6.067Epoch   4:  77% | abe: 6.067 | eve: 10.632 | bob: 6.067Epoch   4:  78% | abe: 6.066 | eve: 10.632 | bob: 6.066Epoch   4:  78% | abe: 6.065 | eve: 10.632 | bob: 6.065Epoch   4:  79% | abe: 6.065 | eve: 10.631 | bob: 6.065Epoch   4:  80% | abe: 6.065 | eve: 10.631 | bob: 6.065Epoch   4:  81% | abe: 6.064 | eve: 10.631 | bob: 6.064Epoch   4:  82% | abe: 6.064 | eve: 10.631 | bob: 6.064Epoch   4:  82% | abe: 6.064 | eve: 10.631 | bob: 6.063Epoch   4:  83% | abe: 6.064 | eve: 10.631 | bob: 6.064Epoch   4:  84% | abe: 6.064 | eve: 10.631 | bob: 6.064Epoch   4:  85% | abe: 6.064 | eve: 10.631 | bob: 6.064Epoch   4:  85% | abe: 6.063 | eve: 10.632 | bob: 6.064Epoch   4:  86% | abe: 6.063 | eve: 10.631 | bob: 6.063Epoch   4:  87% | abe: 6.063 | eve: 10.631 | bob: 6.063Epoch   4:  88% | abe: 6.062 | eve: 10.631 | bob: 6.063Epoch   4:  89% | abe: 6.062 | eve: 10.630 | bob: 6.063Epoch   4:  89% | abe: 6.062 | eve: 10.630 | bob: 6.064Epoch   4:  90% | abe: 6.062 | eve: 10.630 | bob: 6.063Epoch   4:  91% | abe: 6.062 | eve: 10.630 | bob: 6.063Epoch   4:  92% | abe: 6.061 | eve: 10.630 | bob: 6.063Epoch   4:  92% | abe: 6.060 | eve: 10.629 | bob: 6.062Epoch   4:  93% | abe: 6.060 | eve: 10.629 | bob: 6.062Epoch   4:  94% | abe: 6.060 | eve: 10.629 | bob: 6.062Epoch   4:  95% | abe: 6.060 | eve: 10.629 | bob: 6.062Epoch   4:  96% | abe: 6.060 | eve: 10.628 | bob: 6.062Epoch   4:  96% | abe: 6.060 | eve: 10.628 | bob: 6.062Epoch   4:  97% | abe: 6.061 | eve: 10.628 | bob: 6.063Epoch   4:  98% | abe: 6.060 | eve: 10.628 | bob: 6.062Epoch   4:  99% | abe: 6.060 | eve: 10.628 | bob: 6.062
New best Bob loss 6.0620196655466305 at epoch 4
Epoch   5:   0% | abe: 6.065 | eve: 10.630 | bob: 6.092Epoch   5:   0% | abe: 6.044 | eve: 10.633 | bob: 6.054Epoch   5:   1% | abe: 6.036 | eve: 10.622 | bob: 6.035Epoch   5:   2% | abe: 6.036 | eve: 10.627 | bob: 6.046Epoch   5:   3% | abe: 6.032 | eve: 10.630 | bob: 6.047Epoch   5:   3% | abe: 6.038 | eve: 10.623 | bob: 6.056Epoch   5:   4% | abe: 6.040 | eve: 10.624 | bob: 6.057Epoch   5:   5% | abe: 6.036 | eve: 10.624 | bob: 6.060Epoch   5:   6% | abe: 6.036 | eve: 10.626 | bob: 6.059Epoch   5:   7% | abe: 6.031 | eve: 10.624 | bob: 6.053Epoch   5:   7% | abe: 6.027 | eve: 10.621 | bob: 6.046Epoch   5:   8% | abe: 6.035 | eve: 10.619 | bob: 6.055Epoch   5:   9% | abe: 6.040 | eve: 10.619 | bob: 6.066Epoch   5:  10% | abe: 6.041 | eve: 10.619 | bob: 6.063Epoch   5:  10% | abe: 6.045 | eve: 10.620 | bob: 6.070Epoch   5:  11% | abe: 6.047 | eve: 10.621 | bob: 6.072Epoch   5:  12% | abe: 6.044 | eve: 10.622 | bob: 6.070Epoch   5:  13% | abe: 6.044 | eve: 10.623 | bob: 6.071Epoch   5:  14% | abe: 6.045 | eve: 10.623 | bob: 6.072Epoch   5:  14% | abe: 6.043 | eve: 10.622 | bob: 6.075Epoch   5:  15% | abe: 6.043 | eve: 10.621 | bob: 6.077Epoch   5:  16% | abe: 6.043 | eve: 10.622 | bob: 6.079Epoch   5:  17% | abe: 6.043 | eve: 10.622 | bob: 6.082Epoch   5:  17% | abe: 6.043 | eve: 10.621 | bob: 6.085Epoch   5:  18% | abe: 6.044 | eve: 10.621 | bob: 6.085Epoch   5:  19% | abe: 6.043 | eve: 10.620 | bob: 6.085Epoch   5:  20% | abe: 6.045 | eve: 10.619 | bob: 6.089Epoch   5:  21% | abe: 6.044 | eve: 10.619 | bob: 6.086Epoch   5:  21% | abe: 6.045 | eve: 10.618 | bob: 6.086Epoch   5:  22% | abe: 6.045 | eve: 10.617 | bob: 6.085Epoch   5:  23% | abe: 6.045 | eve: 10.618 | bob: 6.084Epoch   5:  24% | abe: 6.045 | eve: 10.619 | bob: 6.085Epoch   5:  25% | abe: 6.044 | eve: 10.618 | bob: 6.083Epoch   5:  25% | abe: 6.045 | eve: 10.620 | bob: 6.085Epoch   5:  26% | abe: 6.046 | eve: 10.620 | bob: 6.085Epoch   5:  27% | abe: 6.046 | eve: 10.619 | bob: 6.086Epoch   5:  28% | abe: 6.045 | eve: 10.619 | bob: 6.084Epoch   5:  28% | abe: 6.046 | eve: 10.620 | bob: 6.083Epoch   5:  29% | abe: 6.045 | eve: 10.621 | bob: 6.084Epoch   5:  30% | abe: 6.045 | eve: 10.620 | bob: 6.083Epoch   5:  31% | abe: 6.045 | eve: 10.619 | bob: 6.083Epoch   5:  32% | abe: 6.046 | eve: 10.618 | bob: 6.083Epoch   5:  32% | abe: 6.044 | eve: 10.617 | bob: 6.081Epoch   5:  33% | abe: 6.044 | eve: 10.617 | bob: 6.081Epoch   5:  34% | abe: 6.046 | eve: 10.617 | bob: 6.081Epoch   5:  35% | abe: 6.047 | eve: 10.617 | bob: 6.081Epoch   5:  35% | abe: 6.046 | eve: 10.616 | bob: 6.080Epoch   5:  36% | abe: 6.046 | eve: 10.616 | bob: 6.078Epoch   5:  37% | abe: 6.045 | eve: 10.617 | bob: 6.078Epoch   5:  38% | abe: 6.047 | eve: 10.617 | bob: 6.078Epoch   5:  39% | abe: 6.049 | eve: 10.617 | bob: 6.080Epoch   5:  39% | abe: 6.049 | eve: 10.618 | bob: 6.079Epoch   5:  40% | abe: 6.049 | eve: 10.618 | bob: 6.079Epoch   5:  41% | abe: 6.048 | eve: 10.619 | bob: 6.078Epoch   5:  42% | abe: 6.048 | eve: 10.619 | bob: 6.076Epoch   5:  42% | abe: 6.048 | eve: 10.618 | bob: 6.076Epoch   5:  43% | abe: 6.048 | eve: 10.618 | bob: 6.077Epoch   5:  44% | abe: 6.047 | eve: 10.617 | bob: 6.076Epoch   5:  45% | abe: 6.047 | eve: 10.617 | bob: 6.076Epoch   5:  46% | abe: 6.046 | eve: 10.618 | bob: 6.075Epoch   5:  46% | abe: 6.045 | eve: 10.618 | bob: 6.075Epoch   5:  47% | abe: 6.045 | eve: 10.617 | bob: 6.075Epoch   5:  48% | abe: 6.045 | eve: 10.617 | bob: 6.076Epoch   5:  49% | abe: 6.045 | eve: 10.617 | bob: 6.075Epoch   5:  50% | abe: 6.045 | eve: 10.617 | bob: 6.075Epoch   5:  50% | abe: 6.044 | eve: 10.617 | bob: 6.073Epoch   5:  51% | abe: 6.043 | eve: 10.616 | bob: 6.072Epoch   5:  52% | abe: 6.043 | eve: 10.617 | bob: 6.071Epoch   5:  53% | abe: 6.043 | eve: 10.616 | bob: 6.072Epoch   5:  53% | abe: 6.043 | eve: 10.617 | bob: 6.072Epoch   5:  54% | abe: 6.043 | eve: 10.616 | bob: 6.072Epoch   5:  55% | abe: 6.043 | eve: 10.616 | bob: 6.072Epoch   5:  56% | abe: 6.044 | eve: 10.616 | bob: 6.073Epoch   5:  57% | abe: 6.045 | eve: 10.616 | bob: 6.073Epoch   5:  57% | abe: 6.045 | eve: 10.616 | bob: 6.073Epoch   5:  58% | abe: 6.045 | eve: 10.615 | bob: 6.075Epoch   5:  59% | abe: 6.046 | eve: 10.615 | bob: 6.077Epoch   5:  60% | abe: 6.047 | eve: 10.616 | bob: 6.078Epoch   5:  60% | abe: 6.048 | eve: 10.616 | bob: 6.078Epoch   5:  61% | abe: 6.048 | eve: 10.616 | bob: 6.079Epoch   5:  62% | abe: 6.048 | eve: 10.616 | bob: 6.080Epoch   5:  63% | abe: 6.048 | eve: 10.617 | bob: 6.079Epoch   5:  64% | abe: 6.048 | eve: 10.617 | bob: 6.079Epoch   5:  64% | abe: 6.048 | eve: 10.617 | bob: 6.080Epoch   5:  65% | abe: 6.048 | eve: 10.617 | bob: 6.082Epoch   5:  66% | abe: 6.049 | eve: 10.617 | bob: 6.082Epoch   5:  67% | abe: 6.049 | eve: 10.617 | bob: 6.083Epoch   5:  67% | abe: 6.050 | eve: 10.617 | bob: 6.083Epoch   5:  68% | abe: 6.051 | eve: 10.617 | bob: 6.084Epoch   5:  69% | abe: 6.051 | eve: 10.617 | bob: 6.083Epoch   5:  70% | abe: 6.051 | eve: 10.617 | bob: 6.083Epoch   5:  71% | abe: 6.051 | eve: 10.617 | bob: 6.083Epoch   5:  71% | abe: 6.052 | eve: 10.617 | bob: 6.084Epoch   5:  72% | abe: 6.052 | eve: 10.617 | bob: 6.084Epoch   5:  73% | abe: 6.052 | eve: 10.617 | bob: 6.083Epoch   5:  74% | abe: 6.053 | eve: 10.617 | bob: 6.083Epoch   5:  75% | abe: 6.053 | eve: 10.617 | bob: 6.084Epoch   5:  75% | abe: 6.053 | eve: 10.617 | bob: 6.084Epoch   5:  76% | abe: 6.053 | eve: 10.617 | bob: 6.084Epoch   5:  77% | abe: 6.053 | eve: 10.617 | bob: 6.084Epoch   5:  78% | abe: 6.053 | eve: 10.617 | bob: 6.085Epoch   5:  78% | abe: 6.054 | eve: 10.617 | bob: 6.086Epoch   5:  79% | abe: 6.054 | eve: 10.617 | bob: 6.086Epoch   5:  80% | abe: 6.053 | eve: 10.617 | bob: 6.087Epoch   5:  81% | abe: 6.054 | eve: 10.618 | bob: 6.087Epoch   5:  82% | abe: 6.054 | eve: 10.617 | bob: 6.088Epoch   5:  82% | abe: 6.055 | eve: 10.617 | bob: 6.090Epoch   5:  83% | abe: 6.055 | eve: 10.617 | bob: 6.090Epoch   5:  84% | abe: 6.056 | eve: 10.617 | bob: 6.092Epoch   5:  85% | abe: 6.056 | eve: 10.617 | bob: 6.093Epoch   5:  85% | abe: 6.056 | eve: 10.617 | bob: 6.095Epoch   5:  86% | abe: 6.056 | eve: 10.616 | bob: 6.096Epoch   5:  87% | abe: 6.056 | eve: 10.616 | bob: 6.096Epoch   5:  88% | abe: 6.056 | eve: 10.616 | bob: 6.097Epoch   5:  89% | abe: 6.055 | eve: 10.616 | bob: 6.097Epoch   5:  89% | abe: 6.056 | eve: 10.616 | bob: 6.098Epoch   5:  90% | abe: 6.056 | eve: 10.617 | bob: 6.098Epoch   5:  91% | abe: 6.056 | eve: 10.616 | bob: 6.099Epoch   5:  92% | abe: 6.056 | eve: 10.616 | bob: 6.099Epoch   5:  92% | abe: 6.056 | eve: 10.616 | bob: 6.099Epoch   5:  93% | abe: 6.056 | eve: 10.617 | bob: 6.098Epoch   5:  94% | abe: 6.055 | eve: 10.617 | bob: 6.098Epoch   5:  95% | abe: 6.055 | eve: 10.617 | bob: 6.099Epoch   5:  96% | abe: 6.055 | eve: 10.617 | bob: 6.099Epoch   5:  96% | abe: 6.054 | eve: 10.617 | bob: 6.099Epoch   5:  97% | abe: 6.054 | eve: 10.616 | bob: 6.099Epoch   5:  98% | abe: 6.053 | eve: 10.616 | bob: 6.098Epoch   5:  99% | abe: 6.053 | eve: 10.616 | bob: 6.099Epoch   6:   0% | abe: 5.992 | eve: 10.601 | bob: 6.092Epoch   6:   0% | abe: 5.998 | eve: 10.585 | bob: 6.106Epoch   6:   1% | abe: 6.026 | eve: 10.602 | bob: 6.143Epoch   6:   2% | abe: 6.026 | eve: 10.619 | bob: 6.147Epoch   6:   3% | abe: 6.021 | eve: 10.619 | bob: 6.131Epoch   6:   3% | abe: 6.016 | eve: 10.613 | bob: 6.129Epoch   6:   4% | abe: 6.011 | eve: 10.614 | bob: 6.116Epoch   6:   5% | abe: 6.002 | eve: 10.613 | bob: 6.095Epoch   6:   6% | abe: 6.001 | eve: 10.612 | bob: 6.092Epoch   6:   7% | abe: 6.006 | eve: 10.614 | bob: 6.098Epoch   6:   7% | abe: 6.008 | eve: 10.618 | bob: 6.101Epoch   6:   8% | abe: 6.002 | eve: 10.616 | bob: 6.101Epoch   6:   9% | abe: 6.004 | eve: 10.618 | bob: 6.100Epoch   6:  10% | abe: 6.003 | eve: 10.619 | bob: 6.100Epoch   6:  10% | abe: 5.999 | eve: 10.620 | bob: 6.099Epoch   6:  11% | abe: 6.001 | eve: 10.617 | bob: 6.101Epoch   6:  12% | abe: 6.005 | eve: 10.619 | bob: 6.109Epoch   6:  13% | abe: 6.007 | eve: 10.621 | bob: 6.111Epoch   6:  14% | abe: 6.004 | eve: 10.622 | bob: 6.110Epoch   6:  14% | abe: 6.004 | eve: 10.624 | bob: 6.113Epoch   6:  15% | abe: 6.004 | eve: 10.624 | bob: 6.117Epoch   6:  16% | abe: 6.006 | eve: 10.624 | bob: 6.120Epoch   6:  17% | abe: 6.008 | eve: 10.622 | bob: 6.123Epoch   6:  17% | abe: 6.007 | eve: 10.624 | bob: 6.124Epoch   6:  18% | abe: 6.008 | eve: 10.624 | bob: 6.125Epoch   6:  19% | abe: 6.010 | eve: 10.625 | bob: 6.127Epoch   6:  20% | abe: 6.012 | eve: 10.623 | bob: 6.129Epoch   6:  21% | abe: 6.013 | eve: 10.622 | bob: 6.130Epoch   6:  21% | abe: 6.011 | eve: 10.622 | bob: 6.128Epoch   6:  22% | abe: 6.014 | eve: 10.623 | bob: 6.130Epoch   6:  23% | abe: 6.015 | eve: 10.624 | bob: 6.130Epoch   6:  24% | abe: 6.018 | eve: 10.623 | bob: 6.131Epoch   6:  25% | abe: 6.017 | eve: 10.622 | bob: 6.130Epoch   6:  25% | abe: 6.018 | eve: 10.621 | bob: 6.131Epoch   6:  26% | abe: 6.017 | eve: 10.622 | bob: 6.130Epoch   6:  27% | abe: 6.017 | eve: 10.622 | bob: 6.129Epoch   6:  28% | abe: 6.018 | eve: 10.623 | bob: 6.130Epoch   6:  28% | abe: 6.018 | eve: 10.624 | bob: 6.130Epoch   6:  29% | abe: 6.020 | eve: 10.625 | bob: 6.129Epoch   6:  30% | abe: 6.019 | eve: 10.625 | bob: 6.128Epoch   6:  31% | abe: 6.019 | eve: 10.625 | bob: 6.127Epoch   6:  32% | abe: 6.019 | eve: 10.625 | bob: 6.126Epoch   6:  32% | abe: 6.020 | eve: 10.625 | bob: 6.124Epoch   6:  33% | abe: 6.020 | eve: 10.624 | bob: 6.123Epoch   6:  34% | abe: 6.021 | eve: 10.623 | bob: 6.122Epoch   6:  35% | abe: 6.023 | eve: 10.623 | bob: 6.123Epoch   6:  35% | abe: 6.021 | eve: 10.624 | bob: 6.121Epoch   6:  36% | abe: 6.021 | eve: 10.624 | bob: 6.121Epoch   6:  37% | abe: 6.020 | eve: 10.625 | bob: 6.120Epoch   6:  38% | abe: 6.020 | eve: 10.625 | bob: 6.119Epoch   6:  39% | abe: 6.019 | eve: 10.626 | bob: 6.117Epoch   6:  39% | abe: 6.021 | eve: 10.625 | bob: 6.118Epoch   6:  40% | abe: 6.021 | eve: 10.625 | bob: 6.119Epoch   6:  41% | abe: 6.020 | eve: 10.626 | bob: 6.117Epoch   6:  42% | abe: 6.019 | eve: 10.626 | bob: 6.117Epoch   6:  42% | abe: 6.018 | eve: 10.626 | bob: 6.117Epoch   6:  43% | abe: 6.017 | eve: 10.625 | bob: 6.117Epoch   6:  44% | abe: 6.017 | eve: 10.626 | bob: 6.117Epoch   6:  45% | abe: 6.017 | eve: 10.625 | bob: 6.117Epoch   6:  46% | abe: 6.016 | eve: 10.625 | bob: 6.117Epoch   6:  46% | abe: 6.016 | eve: 10.625 | bob: 6.117Epoch   6:  47% | abe: 6.016 | eve: 10.625 | bob: 6.117Epoch   6:  48% | abe: 6.014 | eve: 10.624 | bob: 6.115Epoch   6:  49% | abe: 6.014 | eve: 10.625 | bob: 6.115Epoch   6:  50% | abe: 6.013 | eve: 10.625 | bob: 6.114Epoch   6:  50% | abe: 6.013 | eve: 10.625 | bob: 6.114Epoch   6:  51% | abe: 6.012 | eve: 10.625 | bob: 6.114Epoch   6:  52% | abe: 6.011 | eve: 10.624 | bob: 6.113Epoch   6:  53% | abe: 6.011 | eve: 10.625 | bob: 6.113Epoch   6:  53% | abe: 6.011 | eve: 10.624 | bob: 6.112Epoch   6:  54% | abe: 6.010 | eve: 10.625 | bob: 6.111Epoch   6:  55% | abe: 6.010 | eve: 10.624 | bob: 6.111Epoch   6:  56% | abe: 6.010 | eve: 10.624 | bob: 6.111Epoch   6:  57% | abe: 6.009 | eve: 10.624 | bob: 6.111Epoch   6:  57% | abe: 6.009 | eve: 10.624 | bob: 6.111Epoch   6:  58% | abe: 6.009 | eve: 10.624 | bob: 6.111Epoch   6:  59% | abe: 6.009 | eve: 10.624 | bob: 6.111Epoch   6:  60% | abe: 6.008 | eve: 10.624 | bob: 6.111Epoch   6:  60% | abe: 6.008 | eve: 10.624 | bob: 6.110Epoch   6:  61% | abe: 6.008 | eve: 10.624 | bob: 6.110Epoch   6:  62% | abe: 6.007 | eve: 10.624 | bob: 6.109Epoch   6:  63% | abe: 6.006 | eve: 10.625 | bob: 6.107Epoch   6:  64% | abe: 6.006 | eve: 10.625 | bob: 6.107Epoch   6:  64% | abe: 6.004 | eve: 10.625 | bob: 6.106Epoch   6:  65% | abe: 6.004 | eve: 10.625 | bob: 6.106Epoch   6:  66% | abe: 6.003 | eve: 10.625 | bob: 6.106Epoch   6:  67% | abe: 6.004 | eve: 10.625 | bob: 6.106Epoch   6:  67% | abe: 6.005 | eve: 10.624 | bob: 6.108Epoch   6:  68% | abe: 6.005 | eve: 10.624 | bob: 6.108Epoch   6:  69% | abe: 6.004 | eve: 10.624 | bob: 6.107Epoch   6:  70% | abe: 6.004 | eve: 10.624 | bob: 6.105Epoch   6:  71% | abe: 6.004 | eve: 10.624 | bob: 6.106Epoch   6:  71% | abe: 6.004 | eve: 10.624 | bob: 6.106Epoch   6:  72% | abe: 6.005 | eve: 10.624 | bob: 6.106Epoch   6:  73% | abe: 6.005 | eve: 10.624 | bob: 6.106Epoch   6:  74% | abe: 6.005 | eve: 10.624 | bob: 6.105Epoch   6:  75% | abe: 6.005 | eve: 10.624 | bob: 6.104Epoch   6:  75% | abe: 6.005 | eve: 10.623 | bob: 6.104Epoch   6:  76% | abe: 6.005 | eve: 10.623 | bob: 6.104Epoch   6:  77% | abe: 6.005 | eve: 10.623 | bob: 6.104Epoch   6:  78% | abe: 6.005 | eve: 10.623 | bob: 6.104Epoch   6:  78% | abe: 6.005 | eve: 10.623 | bob: 6.104Epoch   6:  79% | abe: 6.006 | eve: 10.622 | bob: 6.105Epoch   6:  80% | abe: 6.006 | eve: 10.622 | bob: 6.105Epoch   6:  81% | abe: 6.007 | eve: 10.622 | bob: 6.106Epoch   6:  82% | abe: 6.007 | eve: 10.622 | bob: 6.107Epoch   6:  82% | abe: 6.007 | eve: 10.622 | bob: 6.107Epoch   6:  83% | abe: 6.007 | eve: 10.622 | bob: 6.107Epoch   6:  84% | abe: 6.008 | eve: 10.622 | bob: 6.107Epoch   6:  85% | abe: 6.008 | eve: 10.622 | bob: 6.107Epoch   6:  85% | abe: 6.008 | eve: 10.622 | bob: 6.107Epoch   6:  86% | abe: 6.008 | eve: 10.621 | bob: 6.107Epoch   6:  87% | abe: 6.009 | eve: 10.622 | bob: 6.107Epoch   6:  88% | abe: 6.009 | eve: 10.621 | bob: 6.108Epoch   6:  89% | abe: 6.010 | eve: 10.622 | bob: 6.108Epoch   6:  89% | abe: 6.010 | eve: 10.622 | bob: 6.108Epoch   6:  90% | abe: 6.010 | eve: 10.622 | bob: 6.108Epoch   6:  91% | abe: 6.010 | eve: 10.622 | bob: 6.108Epoch   6:  92% | abe: 6.010 | eve: 10.622 | bob: 6.108Epoch   6:  92% | abe: 6.010 | eve: 10.622 | bob: 6.107Epoch   6:  93% | abe: 6.010 | eve: 10.622 | bob: 6.107Epoch   6:  94% | abe: 6.010 | eve: 10.622 | bob: 6.107Epoch   6:  95% | abe: 6.011 | eve: 10.622 | bob: 6.108Epoch   6:  96% | abe: 6.011 | eve: 10.621 | bob: 6.108Epoch   6:  96% | abe: 6.011 | eve: 10.621 | bob: 6.107Epoch   6:  97% | abe: 6.011 | eve: 10.621 | bob: 6.108Epoch   6:  98% | abe: 6.011 | eve: 10.621 | bob: 6.110Epoch   6:  99% | abe: 6.012 | eve: 10.621 | bob: 6.110Epoch   7:   0% | abe: 6.010 | eve: 10.600 | bob: 6.133Epoch   7:   0% | abe: 6.018 | eve: 10.605 | bob: 6.153Epoch   7:   1% | abe: 6.055 | eve: 10.596 | bob: 6.210Epoch   7:   2% | abe: 6.064 | eve: 10.620 | bob: 6.212Epoch   7:   3% | abe: 6.057 | eve: 10.623 | bob: 6.214Epoch   7:   3% | abe: 6.060 | eve: 10.616 | bob: 6.220Epoch   7:   4% | abe: 6.063 | eve: 10.613 | bob: 6.230Epoch   7:   5% | abe: 6.059 | eve: 10.619 | bob: 6.230Epoch   7:   6% | abe: 6.057 | eve: 10.622 | bob: 6.222Epoch   7:   7% | abe: 6.057 | eve: 10.621 | bob: 6.224Epoch   7:   7% | abe: 6.056 | eve: 10.618 | bob: 6.219Epoch   7:   8% | abe: 6.052 | eve: 10.616 | bob: 6.217Epoch   7:   9% | abe: 6.056 | eve: 10.614 | bob: 6.209Epoch   7:  10% | abe: 6.054 | eve: 10.612 | bob: 6.204Epoch   7:  10% | abe: 6.060 | eve: 10.609 | bob: 6.207Epoch   7:  11% | abe: 6.059 | eve: 10.611 | bob: 6.205Epoch   7:  12% | abe: 6.060 | eve: 10.610 | bob: 6.204Epoch   7:  13% | abe: 6.060 | eve: 10.612 | bob: 6.200Epoch   7:  14% | abe: 6.057 | eve: 10.612 | bob: 6.193Epoch   7:  14% | abe: 6.057 | eve: 10.616 | bob: 6.191Epoch   7:  15% | abe: 6.054 | eve: 10.615 | bob: 6.185Epoch   7:  16% | abe: 6.061 | eve: 10.612 | bob: 6.187Epoch   7:  17% | abe: 6.063 | eve: 10.613 | bob: 6.193Epoch   7:  17% | abe: 6.063 | eve: 10.610 | bob: 6.193Epoch   7:  18% | abe: 6.062 | eve: 10.611 | bob: 6.191Epoch   7:  19% | abe: 6.063 | eve: 10.609 | bob: 6.192Epoch   7:  20% | abe: 6.065 | eve: 10.609 | bob: 6.193Epoch   7:  21% | abe: 6.065 | eve: 10.607 | bob: 6.195Epoch   7:  21% | abe: 6.068 | eve: 10.608 | bob: 6.193Epoch   7:  22% | abe: 6.069 | eve: 10.609 | bob: 6.191Epoch   7:  23% | abe: 6.067 | eve: 10.609 | bob: 6.188Epoch   7:  24% | abe: 6.065 | eve: 10.608 | bob: 6.186Epoch   7:  25% | abe: 6.064 | eve: 10.607 | bob: 6.182Epoch   7:  25% | abe: 6.064 | eve: 10.607 | bob: 6.184Epoch   7:  26% | abe: 6.063 | eve: 10.606 | bob: 6.185Epoch   7:  27% | abe: 6.063 | eve: 10.607 | bob: 6.185Epoch   7:  28% | abe: 6.063 | eve: 10.608 | bob: 6.187Epoch   7:  28% | abe: 6.063 | eve: 10.609 | bob: 6.186Epoch   7:  29% | abe: 6.064 | eve: 10.610 | bob: 6.186Epoch   7:  30% | abe: 6.064 | eve: 10.611 | bob: 6.186Epoch   7:  31% | abe: 6.062 | eve: 10.611 | bob: 6.183Epoch   7:  32% | abe: 6.062 | eve: 10.612 | bob: 6.182Epoch   7:  32% | abe: 6.062 | eve: 10.612 | bob: 6.181Epoch   7:  33% | abe: 6.062 | eve: 10.612 | bob: 6.181Epoch   7:  34% | abe: 6.062 | eve: 10.612 | bob: 6.179Epoch   7:  35% | abe: 6.061 | eve: 10.612 | bob: 6.179Epoch   7:  35% | abe: 6.060 | eve: 10.613 | bob: 6.177Epoch   7:  36% | abe: 6.059 | eve: 10.612 | bob: 6.174Epoch   7:  37% | abe: 6.056 | eve: 10.612 | bob: 6.172Epoch   7:  38% | abe: 6.055 | eve: 10.612 | bob: 6.171Epoch   7:  39% | abe: 6.055 | eve: 10.611 | bob: 6.170Epoch   7:  39% | abe: 6.054 | eve: 10.611 | bob: 6.171Epoch   7:  40% | abe: 6.053 | eve: 10.611 | bob: 6.170Epoch   7:  41% | abe: 6.053 | eve: 10.611 | bob: 6.170Epoch   7:  42% | abe: 6.053 | eve: 10.612 | bob: 6.171Epoch   7:  42% | abe: 6.052 | eve: 10.612 | bob: 6.170Epoch   7:  43% | abe: 6.053 | eve: 10.612 | bob: 6.171Epoch   7:  44% | abe: 6.054 | eve: 10.612 | bob: 6.170Epoch   7:  45% | abe: 6.053 | eve: 10.612 | bob: 6.171Epoch   7:  46% | abe: 6.053 | eve: 10.611 | bob: 6.170Epoch   7:  46% | abe: 6.051 | eve: 10.611 | bob: 6.169Epoch   7:  47% | abe: 6.052 | eve: 10.612 | bob: 6.169Epoch   7:  48% | abe: 6.051 | eve: 10.612 | bob: 6.168Epoch   7:  49% | abe: 6.051 | eve: 10.611 | bob: 6.168Epoch   7:  50% | abe: 6.051 | eve: 10.611 | bob: 6.166Epoch   7:  50% | abe: 6.050 | eve: 10.611 | bob: 6.165Epoch   7:  51% | abe: 6.049 | eve: 10.611 | bob: 6.161Epoch   7:  52% | abe: 6.049 | eve: 10.610 | bob: 6.160Epoch   7:  53% | abe: 6.048 | eve: 10.610 | bob: 6.159Epoch   7:  53% | abe: 6.047 | eve: 10.610 | bob: 6.158Epoch   7:  54% | abe: 6.047 | eve: 10.610 | bob: 6.157Epoch   7:  55% | abe: 6.046 | eve: 10.610 | bob: 6.157Epoch   7:  56% | abe: 6.045 | eve: 10.610 | bob: 6.156Epoch   7:  57% | abe: 6.045 | eve: 10.610 | bob: 6.155Epoch   7:  57% | abe: 6.044 | eve: 10.611 | bob: 6.154Epoch   7:  58% | abe: 6.043 | eve: 10.611 | bob: 6.153Epoch   7:  59% | abe: 6.044 | eve: 10.610 | bob: 6.155Epoch   7:  60% | abe: 6.043 | eve: 10.610 | bob: 6.155Epoch   7:  60% | abe: 6.043 | eve: 10.611 | bob: 6.156Epoch   7:  61% | abe: 6.043 | eve: 10.611 | bob: 6.156Epoch   7:  62% | abe: 6.043 | eve: 10.612 | bob: 6.157Epoch   7:  63% | abe: 6.043 | eve: 10.612 | bob: 6.157Epoch   7:  64% | abe: 6.041 | eve: 10.612 | bob: 6.156Epoch   7:  64% | abe: 6.040 | eve: 10.612 | bob: 6.155Epoch   7:  65% | abe: 6.040 | eve: 10.612 | bob: 6.155Epoch   7:  66% | abe: 6.039 | eve: 10.612 | bob: 6.155Epoch   7:  67% | abe: 6.038 | eve: 10.612 | bob: 6.154Epoch   7:  67% | abe: 6.037 | eve: 10.612 | bob: 6.155Epoch   7:  68% | abe: 6.037 | eve: 10.613 | bob: 6.155Epoch   7:  69% | abe: 6.037 | eve: 10.614 | bob: 6.156Epoch   7:  70% | abe: 6.038 | eve: 10.613 | bob: 6.156Epoch   7:  71% | abe: 6.037 | eve: 10.613 | bob: 6.156Epoch   7:  71% | abe: 6.037 | eve: 10.614 | bob: 6.156Epoch   7:  72% | abe: 6.036 | eve: 10.614 | bob: 6.155Epoch   7:  73% | abe: 6.036 | eve: 10.614 | bob: 6.154Epoch   7:  74% | abe: 6.035 | eve: 10.614 | bob: 6.154Epoch   7:  75% | abe: 6.035 | eve: 10.613 | bob: 6.154Epoch   7:  75% | abe: 6.035 | eve: 10.613 | bob: 6.153Epoch   7:  76% | abe: 6.034 | eve: 10.613 | bob: 6.152Epoch   7:  77% | abe: 6.034 | eve: 10.613 | bob: 6.152Epoch   7:  78% | abe: 6.034 | eve: 10.613 | bob: 6.151Epoch   7:  78% | abe: 6.033 | eve: 10.613 | bob: 6.150Epoch   7:  79% | abe: 6.033 | eve: 10.613 | bob: 6.151Epoch   7:  80% | abe: 6.033 | eve: 10.613 | bob: 6.151Epoch   7:  81% | abe: 6.033 | eve: 10.613 | bob: 6.151Epoch   7:  82% | abe: 6.033 | eve: 10.613 | bob: 6.150Epoch   7:  82% | abe: 6.033 | eve: 10.613 | bob: 6.150Epoch   7:  83% | abe: 6.032 | eve: 10.613 | bob: 6.150Epoch   7:  84% | abe: 6.032 | eve: 10.613 | bob: 6.149Epoch   7:  85% | abe: 6.032 | eve: 10.613 | bob: 6.149Epoch   7:  85% | abe: 6.033 | eve: 10.614 | bob: 6.148Epoch   7:  86% | abe: 6.032 | eve: 10.613 | bob: 6.147Epoch   7:  87% | abe: 6.031 | eve: 10.613 | bob: 6.146Epoch   7:  88% | abe: 6.031 | eve: 10.613 | bob: 6.146Epoch   7:  89% | abe: 6.030 | eve: 10.613 | bob: 6.145Epoch   7:  89% | abe: 6.030 | eve: 10.613 | bob: 6.145Epoch   7:  90% | abe: 6.030 | eve: 10.613 | bob: 6.145Epoch   7:  91% | abe: 6.030 | eve: 10.613 | bob: 6.145Epoch   7:  92% | abe: 6.030 | eve: 10.613 | bob: 6.145Epoch   7:  92% | abe: 6.029 | eve: 10.613 | bob: 6.144Epoch   7:  93% | abe: 6.029 | eve: 10.613 | bob: 6.144Epoch   7:  94% | abe: 6.029 | eve: 10.613 | bob: 6.143Epoch   7:  95% | abe: 6.028 | eve: 10.613 | bob: 6.143Epoch   7:  96% | abe: 6.028 | eve: 10.613 | bob: 6.143Epoch   7:  96% | abe: 6.028 | eve: 10.613 | bob: 6.143Epoch   7:  97% | abe: 6.027 | eve: 10.613 | bob: 6.143Epoch   7:  98% | abe: 6.027 | eve: 10.613 | bob: 6.142Epoch   7:  99% | abe: 6.026 | eve: 10.613 | bob: 6.142Epoch   8:   0% | abe: 5.917 | eve: 10.598 | bob: 6.067Epoch   8:   0% | abe: 5.950 | eve: 10.575 | bob: 6.086Epoch   8:   1% | abe: 5.944 | eve: 10.597 | bob: 6.080Epoch   8:   2% | abe: 5.944 | eve: 10.608 | bob: 6.076Epoch   8:   3% | abe: 5.961 | eve: 10.605 | bob: 6.099Epoch   8:   3% | abe: 5.961 | eve: 10.611 | bob: 6.102Epoch   8:   4% | abe: 5.978 | eve: 10.610 | bob: 6.114Epoch   8:   5% | abe: 5.973 | eve: 10.607 | bob: 6.106Epoch   8:   6% | abe: 5.985 | eve: 10.607 | bob: 6.108Epoch   8:   7% | abe: 5.991 | eve: 10.612 | bob: 6.111Epoch   8:   7% | abe: 5.985 | eve: 10.610 | bob: 6.101Epoch   8:   8% | abe: 5.984 | eve: 10.608 | bob: 6.095Epoch   8:   9% | abe: 5.982 | eve: 10.609 | bob: 6.091Epoch   8:  10% | abe: 5.983 | eve: 10.611 | bob: 6.089Epoch   8:  10% | abe: 5.986 | eve: 10.609 | bob: 6.088Epoch   8:  11% | abe: 5.987 | eve: 10.610 | bob: 6.089Epoch   8:  12% | abe: 5.989 | eve: 10.611 | bob: 6.094Epoch   8:  13% | abe: 5.988 | eve: 10.611 | bob: 6.093Epoch   8:  14% | abe: 5.986 | eve: 10.609 | bob: 6.094Epoch   8:  14% | abe: 5.988 | eve: 10.611 | bob: 6.092Epoch   8:  15% | abe: 5.989 | eve: 10.612 | bob: 6.093Epoch   8:  16% | abe: 5.992 | eve: 10.612 | bob: 6.096Epoch   8:  17% | abe: 5.996 | eve: 10.611 | bob: 6.097Epoch   8:  17% | abe: 5.995 | eve: 10.610 | bob: 6.098Epoch   8:  18% | abe: 5.999 | eve: 10.611 | bob: 6.101Epoch   8:  19% | abe: 6.002 | eve: 10.611 | bob: 6.105Epoch   8:  20% | abe: 6.002 | eve: 10.610 | bob: 6.104Epoch   8:  21% | abe: 6.001 | eve: 10.610 | bob: 6.106Epoch   8:  21% | abe: 6.001 | eve: 10.611 | bob: 6.106Epoch   8:  22% | abe: 6.001 | eve: 10.611 | bob: 6.107Epoch   8:  23% | abe: 6.002 | eve: 10.612 | bob: 6.110Epoch   8:  24% | abe: 6.003 | eve: 10.613 | bob: 6.111Epoch   8:  25% | abe: 6.000 | eve: 10.613 | bob: 6.109Epoch   8:  25% | abe: 6.001 | eve: 10.615 | bob: 6.110Epoch   8:  26% | abe: 5.999 | eve: 10.616 | bob: 6.110Epoch   8:  27% | abe: 6.002 | eve: 10.615 | bob: 6.114Epoch   8:  28% | abe: 6.000 | eve: 10.613 | bob: 6.114Epoch   8:  28% | abe: 6.000 | eve: 10.613 | bob: 6.117Epoch   8:  29% | abe: 6.001 | eve: 10.614 | bob: 6.119Epoch   8:  30% | abe: 6.001 | eve: 10.616 | bob: 6.116Epoch   8:  31% | abe: 6.001 | eve: 10.616 | bob: 6.118Epoch   8:  32% | abe: 6.002 | eve: 10.616 | bob: 6.119Epoch   8:  32% | abe: 6.001 | eve: 10.615 | bob: 6.119Epoch   8:  33% | abe: 6.001 | eve: 10.615 | bob: 6.118Epoch   8:  34% | abe: 6.001 | eve: 10.615 | bob: 6.119Epoch   8:  35% | abe: 6.002 | eve: 10.615 | bob: 6.120Epoch   8:  35% | abe: 6.002 | eve: 10.615 | bob: 6.119Epoch   8:  36% | abe: 6.002 | eve: 10.615 | bob: 6.121Epoch   8:  37% | abe: 6.005 | eve: 10.616 | bob: 6.123Epoch   8:  38% | abe: 6.007 | eve: 10.616 | bob: 6.126Epoch   8:  39% | abe: 6.010 | eve: 10.616 | bob: 6.133Epoch   8:  39% | abe: 6.015 | eve: 10.614 | bob: 6.140Epoch   8:  40% | abe: 6.020 | eve: 10.615 | bob: 6.142Epoch   8:  41% | abe: 6.021 | eve: 10.615 | bob: 6.143Epoch   8:  42% | abe: 6.022 | eve: 10.614 | bob: 6.148Epoch   8:  42% | abe: 6.025 | eve: 10.614 | bob: 6.151Epoch   8:  43% | abe: 6.027 | eve: 10.614 | bob: 6.153Epoch   8:  44% | abe: 6.029 | eve: 10.614 | bob: 6.156Epoch   8:  45% | abe: 6.032 | eve: 10.613 | bob: 6.157Epoch   8:  46% | abe: 6.034 | eve: 10.614 | bob: 6.160Epoch   8:  46% | abe: 6.036 | eve: 10.614 | bob: 6.164Epoch   8:  47% | abe: 6.039 | eve: 10.615 | bob: 6.167Epoch   8:  48% | abe: 6.042 | eve: 10.615 | bob: 6.168Epoch   8:  49% | abe: 6.043 | eve: 10.615 | bob: 6.168Epoch   8:  50% | abe: 6.043 | eve: 10.615 | bob: 6.169Epoch   8:  50% | abe: 6.045 | eve: 10.615 | bob: 6.171Epoch   8:  51% | abe: 6.047 | eve: 10.615 | bob: 6.171Epoch   8:  52% | abe: 6.047 | eve: 10.615 | bob: 6.171Epoch   8:  53% | abe: 6.048 | eve: 10.615 | bob: 6.172Epoch   8:  53% | abe: 6.049 | eve: 10.615 | bob: 6.172Epoch   8:  54% | abe: 6.049 | eve: 10.615 | bob: 6.172Epoch   8:  55% | abe: 6.049 | eve: 10.616 | bob: 6.172Epoch   8:  56% | abe: 6.049 | eve: 10.616 | bob: 6.172Epoch   8:  57% | abe: 6.049 | eve: 10.616 | bob: 6.171Epoch   8:  57% | abe: 6.048 | eve: 10.616 | bob: 6.171Epoch   8:  58% | abe: 6.048 | eve: 10.615 | bob: 6.170Epoch   8:  59% | abe: 6.049 | eve: 10.615 | bob: 6.169Epoch   8:  60% | abe: 6.048 | eve: 10.615 | bob: 6.170Epoch   8:  60% | abe: 6.048 | eve: 10.614 | bob: 6.170Epoch   8:  61% | abe: 6.048 | eve: 10.614 | bob: 6.170Epoch   8:  62% | abe: 6.047 | eve: 10.615 | bob: 6.168Epoch   8:  63% | abe: 6.046 | eve: 10.614 | bob: 6.166Epoch   8:  64% | abe: 6.046 | eve: 10.615 | bob: 6.165Epoch   8:  64% | abe: 6.045 | eve: 10.615 | bob: 6.163Epoch   8:  65% | abe: 6.045 | eve: 10.614 | bob: 6.162Epoch   8:  66% | abe: 6.045 | eve: 10.615 | bob: 6.161Epoch   8:  67% | abe: 6.044 | eve: 10.614 | bob: 6.160Epoch   8:  67% | abe: 6.044 | eve: 10.614 | bob: 6.158Epoch   8:  68% | abe: 6.044 | eve: 10.614 | bob: 6.157Epoch   8:  69% | abe: 6.044 | eve: 10.614 | bob: 6.156Epoch   8:  70% | abe: 6.044 | eve: 10.614 | bob: 6.157Epoch   8:  71% | abe: 6.046 | eve: 10.613 | bob: 6.157Epoch   8:  71% | abe: 6.045 | eve: 10.614 | bob: 6.156Epoch   8:  72% | abe: 6.046 | eve: 10.613 | bob: 6.157Epoch   8:  73% | abe: 6.046 | eve: 10.613 | bob: 6.157Epoch   8:  74% | abe: 6.046 | eve: 10.613 | bob: 6.157Epoch   8:  75% | abe: 6.046 | eve: 10.614 | bob: 6.157Epoch   8:  75% | abe: 6.046 | eve: 10.614 | bob: 6.158Epoch   8:  76% | abe: 6.046 | eve: 10.614 | bob: 6.158Epoch   8:  77% | abe: 6.046 | eve: 10.615 | bob: 6.159Epoch   8:  78% | abe: 6.046 | eve: 10.614 | bob: 6.159Epoch   8:  78% | abe: 6.046 | eve: 10.615 | bob: 6.159Epoch   8:  79% | abe: 6.046 | eve: 10.614 | bob: 6.159Epoch   8:  80% | abe: 6.046 | eve: 10.614 | bob: 6.159Epoch   8:  81% | abe: 6.045 | eve: 10.614 | bob: 6.158Epoch   8:  82% | abe: 6.045 | eve: 10.614 | bob: 6.158Epoch   8:  82% | abe: 6.046 | eve: 10.614 | bob: 6.157Epoch   8:  83% | abe: 6.046 | eve: 10.614 | bob: 6.156Epoch   8:  84% | abe: 6.046 | eve: 10.615 | bob: 6.156Epoch   8:  85% | abe: 6.045 | eve: 10.615 | bob: 6.156Epoch   8:  85% | abe: 6.046 | eve: 10.615 | bob: 6.155Epoch   8:  86% | abe: 6.046 | eve: 10.615 | bob: 6.155Epoch   8:  87% | abe: 6.046 | eve: 10.614 | bob: 6.154Epoch   8:  88% | abe: 6.046 | eve: 10.614 | bob: 6.154Epoch   8:  89% | abe: 6.045 | eve: 10.614 | bob: 6.153Epoch   8:  89% | abe: 6.046 | eve: 10.614 | bob: 6.153Epoch   8:  90% | abe: 6.045 | eve: 10.614 | bob: 6.153Epoch   8:  91% | abe: 6.045 | eve: 10.613 | bob: 6.154Epoch   8:  92% | abe: 6.045 | eve: 10.614 | bob: 6.153Epoch   8:  92% | abe: 6.045 | eve: 10.613 | bob: 6.154Epoch   8:  93% | abe: 6.046 | eve: 10.614 | bob: 6.153Epoch   8:  94% | abe: 6.045 | eve: 10.614 | bob: 6.152Epoch   8:  95% | abe: 6.045 | eve: 10.613 | bob: 6.152Epoch   8:  96% | abe: 6.045 | eve: 10.613 | bob: 6.152Epoch   8:  96% | abe: 6.044 | eve: 10.613 | bob: 6.151Epoch   8:  97% | abe: 6.044 | eve: 10.613 | bob: 6.150Epoch   8:  98% | abe: 6.043 | eve: 10.614 | bob: 6.150Epoch   8:  99% | abe: 6.043 | eve: 10.613 | bob: 6.150Epoch   9:   0% | abe: 5.920 | eve: 10.607 | bob: 6.013Epoch   9:   0% | abe: 5.939 | eve: 10.596 | bob: 6.025Epoch   9:   1% | abe: 5.954 | eve: 10.615 | bob: 6.059Epoch   9:   2% | abe: 5.971 | eve: 10.608 | bob: 6.075Epoch   9:   3% | abe: 5.976 | eve: 10.609 | bob: 6.097Epoch   9:   3% | abe: 5.974 | eve: 10.609 | bob: 6.104Epoch   9:   4% | abe: 5.980 | eve: 10.606 | bob: 6.116Epoch   9:   5% | abe: 5.983 | eve: 10.602 | bob: 6.124Epoch   9:   6% | abe: 5.985 | eve: 10.601 | bob: 6.125Epoch   9:   7% | abe: 5.988 | eve: 10.600 | bob: 6.127Epoch   9:   7% | abe: 5.988 | eve: 10.600 | bob: 6.131Epoch   9:   8% | abe: 5.986 | eve: 10.603 | bob: 6.124Epoch   9:   9% | abe: 5.990 | eve: 10.605 | bob: 6.126Epoch   9:  10% | abe: 5.989 | eve: 10.603 | bob: 6.124Epoch   9:  10% | abe: 5.982 | eve: 10.605 | bob: 6.115Epoch   9:  11% | abe: 5.981 | eve: 10.606 | bob: 6.112Epoch   9:  12% | abe: 5.978 | eve: 10.604 | bob: 6.108Epoch   9:  13% | abe: 5.977 | eve: 10.604 | bob: 6.109Epoch   9:  14% | abe: 5.981 | eve: 10.603 | bob: 6.113Epoch   9:  14% | abe: 5.986 | eve: 10.605 | bob: 6.118Epoch   9:  15% | abe: 5.985 | eve: 10.606 | bob: 6.118Epoch   9:  16% | abe: 5.981 | eve: 10.608 | bob: 6.115Epoch   9:  17% | abe: 5.981 | eve: 10.609 | bob: 6.116Epoch   9:  17% | abe: 5.981 | eve: 10.608 | bob: 6.116Epoch   9:  18% | abe: 5.984 | eve: 10.608 | bob: 6.116Epoch   9:  19% | abe: 5.984 | eve: 10.609 | bob: 6.116Epoch   9:  20% | abe: 5.983 | eve: 10.608 | bob: 6.115Epoch   9:  21% | abe: 5.985 | eve: 10.608 | bob: 6.119Epoch   9:  21% | abe: 5.987 | eve: 10.609 | bob: 6.119Epoch   9:  22% | abe: 5.986 | eve: 10.611 | bob: 6.119Epoch   9:  23% | abe: 5.984 | eve: 10.612 | bob: 6.116Epoch   9:  24% | abe: 5.984 | eve: 10.613 | bob: 6.116Epoch   9:  25% | abe: 5.984 | eve: 10.613 | bob: 6.116Epoch   9:  25% | abe: 5.985 | eve: 10.612 | bob: 6.115Epoch   9:  26% | abe: 5.986 | eve: 10.612 | bob: 6.116Epoch   9:  27% | abe: 5.985 | eve: 10.613 | bob: 6.116Epoch   9:  28% | abe: 5.986 | eve: 10.612 | bob: 6.117Epoch   9:  28% | abe: 5.987 | eve: 10.613 | bob: 6.119Epoch   9:  29% | abe: 5.988 | eve: 10.615 | bob: 6.119Epoch   9:  30% | abe: 5.989 | eve: 10.615 | bob: 6.120Epoch   9:  31% | abe: 5.989 | eve: 10.614 | bob: 6.120Epoch   9:  32% | abe: 5.989 | eve: 10.615 | bob: 6.121Epoch   9:  32% | abe: 5.988 | eve: 10.614 | bob: 6.120Epoch   9:  33% | abe: 5.989 | eve: 10.616 | bob: 6.125Epoch   9:  34% | abe: 5.990 | eve: 10.616 | bob: 6.126Epoch   9:  35% | abe: 5.990 | eve: 10.616 | bob: 6.126Epoch   9:  35% | abe: 5.992 | eve: 10.617 | bob: 6.129Epoch   9:  36% | abe: 5.994 | eve: 10.617 | bob: 6.129Epoch   9:  37% | abe: 5.995 | eve: 10.616 | bob: 6.130Epoch   9:  38% | abe: 5.996 | eve: 10.616 | bob: 6.131Epoch   9:  39% | abe: 5.998 | eve: 10.617 | bob: 6.132Epoch   9:  39% | abe: 5.998 | eve: 10.616 | bob: 6.132Epoch   9:  40% | abe: 6.001 | eve: 10.617 | bob: 6.136Epoch   9:  41% | abe: 6.002 | eve: 10.617 | bob: 6.136Epoch   9:  42% | abe: 6.003 | eve: 10.617 | bob: 6.138Epoch   9:  42% | abe: 6.003 | eve: 10.617 | bob: 6.136Epoch   9:  43% | abe: 6.004 | eve: 10.617 | bob: 6.136Epoch   9:  44% | abe: 6.004 | eve: 10.617 | bob: 6.136Epoch   9:  45% | abe: 6.005 | eve: 10.617 | bob: 6.137Epoch   9:  46% | abe: 6.005 | eve: 10.616 | bob: 6.138Epoch   9:  46% | abe: 6.005 | eve: 10.615 | bob: 6.138Epoch   9:  47% | abe: 6.006 | eve: 10.615 | bob: 6.139Epoch   9:  48% | abe: 6.006 | eve: 10.615 | bob: 6.140Epoch   9:  49% | abe: 6.006 | eve: 10.615 | bob: 6.140Epoch   9:  50% | abe: 6.006 | eve: 10.614 | bob: 6.140Epoch   9:  50% | abe: 6.006 | eve: 10.614 | bob: 6.140Epoch   9:  51% | abe: 6.005 | eve: 10.614 | bob: 6.139Epoch   9:  52% | abe: 6.006 | eve: 10.613 | bob: 6.140Epoch   9:  53% | abe: 6.006 | eve: 10.613 | bob: 6.142Epoch   9:  53% | abe: 6.005 | eve: 10.613 | bob: 6.142Epoch   9:  54% | abe: 6.005 | eve: 10.613 | bob: 6.142Epoch   9:  55% | abe: 6.005 | eve: 10.612 | bob: 6.142Epoch   9:  56% | abe: 6.005 | eve: 10.612 | bob: 6.141Epoch   9:  57% | abe: 6.004 | eve: 10.612 | bob: 6.141Epoch   9:  57% | abe: 6.006 | eve: 10.610 | bob: 6.142Epoch   9:  58% | abe: 6.006 | eve: 10.611 | bob: 6.143Epoch   9:  59% | abe: 6.006 | eve: 10.610 | bob: 6.143Epoch   9:  60% | abe: 6.006 | eve: 10.610 | bob: 6.143Epoch   9:  60% | abe: 6.006 | eve: 10.610 | bob: 6.143Epoch   9:  61% | abe: 6.006 | eve: 10.609 | bob: 6.143Epoch   9:  62% | abe: 6.007 | eve: 10.610 | bob: 6.144Epoch   9:  63% | abe: 6.008 | eve: 10.610 | bob: 6.144Epoch   9:  64% | abe: 6.009 | eve: 10.610 | bob: 6.145Epoch   9:  64% | abe: 6.009 | eve: 10.610 | bob: 6.144Epoch   9:  65% | abe: 6.008 | eve: 10.611 | bob: 6.144Epoch   9:  66% | abe: 6.009 | eve: 10.610 | bob: 6.143Epoch   9:  67% | abe: 6.008 | eve: 10.609 | bob: 6.143Epoch   9:  67% | abe: 6.007 | eve: 10.610 | bob: 6.142Epoch   9:  68% | abe: 6.008 | eve: 10.610 | bob: 6.142Epoch   9:  69% | abe: 6.007 | eve: 10.610 | bob: 6.142Epoch   9:  70% | abe: 6.007 | eve: 10.610 | bob: 6.142Epoch   9:  71% | abe: 6.007 | eve: 10.610 | bob: 6.142Epoch   9:  71% | abe: 6.007 | eve: 10.610 | bob: 6.143Epoch   9:  72% | abe: 6.008 | eve: 10.609 | bob: 6.144Epoch   9:  73% | abe: 6.008 | eve: 10.609 | bob: 6.144Epoch   9:  74% | abe: 6.007 | eve: 10.609 | bob: 6.143Epoch   9:  75% | abe: 6.007 | eve: 10.609 | bob: 6.143Epoch   9:  75% | abe: 6.007 | eve: 10.609 | bob: 6.142Epoch   9:  76% | abe: 6.007 | eve: 10.609 | bob: 6.142Epoch   9:  77% | abe: 6.008 | eve: 10.610 | bob: 6.143Epoch   9:  78% | abe: 6.008 | eve: 10.609 | bob: 6.143Epoch   9:  78% | abe: 6.008 | eve: 10.609 | bob: 6.142Epoch   9:  79% | abe: 6.008 | eve: 10.609 | bob: 6.142Epoch   9:  80% | abe: 6.007 | eve: 10.609 | bob: 6.141Epoch   9:  81% | abe: 6.007 | eve: 10.610 | bob: 6.140Epoch   9:  82% | abe: 6.007 | eve: 10.610 | bob: 6.140Epoch   9:  82% | abe: 6.007 | eve: 10.610 | bob: 6.141Epoch   9:  83% | abe: 6.007 | eve: 10.610 | bob: 6.140Epoch   9:  84% | abe: 6.007 | eve: 10.609 | bob: 6.140Epoch   9:  85% | abe: 6.007 | eve: 10.609 | bob: 6.140Epoch   9:  85% | abe: 6.007 | eve: 10.609 | bob: 6.140Epoch   9:  86% | abe: 6.007 | eve: 10.609 | bob: 6.139Epoch   9:  87% | abe: 6.007 | eve: 10.609 | bob: 6.139Epoch   9:  88% | abe: 6.007 | eve: 10.610 | bob: 6.140Epoch   9:  89% | abe: 6.008 | eve: 10.610 | bob: 6.141Epoch   9:  89% | abe: 6.008 | eve: 10.610 | bob: 6.142Epoch   9:  90% | abe: 6.009 | eve: 10.609 | bob: 6.142Epoch   9:  91% | abe: 6.009 | eve: 10.610 | bob: 6.142Epoch   9:  92% | abe: 6.008 | eve: 10.610 | bob: 6.141Epoch   9:  92% | abe: 6.008 | eve: 10.609 | bob: 6.140Epoch   9:  93% | abe: 6.007 | eve: 10.610 | bob: 6.140Epoch   9:  94% | abe: 6.007 | eve: 10.610 | bob: 6.139Epoch   9:  95% | abe: 6.007 | eve: 10.609 | bob: 6.139Epoch   9:  96% | abe: 6.006 | eve: 10.610 | bob: 6.139Epoch   9:  96% | abe: 6.006 | eve: 10.610 | bob: 6.138Epoch   9:  97% | abe: 6.006 | eve: 10.610 | bob: 6.138Epoch   9:  98% | abe: 6.006 | eve: 10.610 | bob: 6.138Epoch   9:  99% | abe: 6.006 | eve: 10.610 | bob: 6.139Epoch  10:   0% | abe: 6.047 | eve: 10.602 | bob: 6.132Epoch  10:   0% | abe: 6.008 | eve: 10.602 | bob: 6.136Epoch  10:   1% | abe: 6.003 | eve: 10.619 | bob: 6.134Epoch  10:   2% | abe: 5.992 | eve: 10.610 | bob: 6.121Epoch  10:   3% | abe: 5.985 | eve: 10.611 | bob: 6.105Epoch  10:   3% | abe: 5.994 | eve: 10.595 | bob: 6.121Epoch  10:   4% | abe: 6.004 | eve: 10.593 | bob: 6.140Epoch  10:   5% | abe: 6.007 | eve: 10.601 | bob: 6.136Epoch  10:   6% | abe: 6.007 | eve: 10.604 | bob: 6.133Epoch  10:   7% | abe: 6.006 | eve: 10.610 | bob: 6.128Epoch  10:   7% | abe: 6.004 | eve: 10.610 | bob: 6.124Epoch  10:   8% | abe: 6.007 | eve: 10.611 | bob: 6.123Epoch  10:   9% | abe: 6.005 | eve: 10.609 | bob: 6.122Epoch  10:  10% | abe: 6.001 | eve: 10.609 | bob: 6.118Epoch  10:  10% | abe: 6.000 | eve: 10.610 | bob: 6.114Epoch  10:  11% | abe: 6.002 | eve: 10.611 | bob: 6.115Epoch  10:  12% | abe: 6.000 | eve: 10.613 | bob: 6.115Epoch  10:  13% | abe: 6.002 | eve: 10.614 | bob: 6.117Epoch  10:  14% | abe: 6.002 | eve: 10.616 | bob: 6.118Epoch  10:  14% | abe: 6.001 | eve: 10.618 | bob: 6.120Epoch  10:  15% | abe: 6.003 | eve: 10.619 | bob: 6.122Epoch  10:  16% | abe: 6.001 | eve: 10.618 | bob: 6.121Epoch  10:  17% | abe: 5.998 | eve: 10.617 | bob: 6.119Epoch  10:  17% | abe: 5.998 | eve: 10.616 | bob: 6.118Epoch  10:  18% | abe: 5.999 | eve: 10.617 | bob: 6.118Epoch  10:  19% | abe: 5.999 | eve: 10.616 | bob: 6.118Epoch  10:  20% | abe: 6.000 | eve: 10.616 | bob: 6.120Epoch  10:  21% | abe: 6.002 | eve: 10.616 | bob: 6.122Epoch  10:  21% | abe: 6.005 | eve: 10.615 | bob: 6.126Epoch  10:  22% | abe: 6.008 | eve: 10.614 | bob: 6.129Epoch  10:  23% | abe: 6.008 | eve: 10.614 | bob: 6.130Epoch  10:  24% | abe: 6.009 | eve: 10.613 | bob: 6.132Epoch  10:  25% | abe: 6.010 | eve: 10.614 | bob: 6.134Epoch  10:  25% | abe: 6.008 | eve: 10.613 | bob: 6.131Epoch  10:  26% | abe: 6.010 | eve: 10.612 | bob: 6.133Epoch  10:  27% | abe: 6.007 | eve: 10.611 | bob: 6.132Epoch  10:  28% | abe: 6.007 | eve: 10.611 | bob: 6.131Epoch  10:  28% | abe: 6.006 | eve: 10.610 | bob: 6.130Epoch  10:  29% | abe: 6.005 | eve: 10.610 | bob: 6.130Epoch  10:  30% | abe: 6.006 | eve: 10.610 | bob: 6.132Epoch  10:  31% | abe: 6.005 | eve: 10.611 | bob: 6.129Epoch  10:  32% | abe: 6.006 | eve: 10.611 | bob: 6.127Epoch  10:  32% | abe: 6.006 | eve: 10.611 | bob: 6.127Epoch  10:  33% | abe: 6.005 | eve: 10.610 | bob: 6.125Epoch  10:  34% | abe: 6.004 | eve: 10.610 | bob: 6.123Epoch  10:  35% | abe: 6.005 | eve: 10.610 | bob: 6.125Epoch  10:  35% | abe: 6.004 | eve: 10.611 | bob: 6.124Epoch  10:  36% | abe: 6.004 | eve: 10.611 | bob: 6.123Epoch  10:  37% | abe: 6.003 | eve: 10.612 | bob: 6.122Epoch  10:  38% | abe: 6.002 | eve: 10.612 | bob: 6.121Epoch  10:  39% | abe: 6.003 | eve: 10.613 | bob: 6.122Epoch  10:  39% | abe: 6.003 | eve: 10.612 | bob: 6.122Epoch  10:  40% | abe: 6.004 | eve: 10.612 | bob: 6.124Epoch  10:  41% | abe: 6.005 | eve: 10.613 | bob: 6.125Epoch  10:  42% | abe: 6.005 | eve: 10.613 | bob: 6.126Epoch  10:  42% | abe: 6.004 | eve: 10.614 | bob: 6.126Epoch  10:  43% | abe: 6.005 | eve: 10.614 | bob: 6.126Epoch  10:  44% | abe: 6.004 | eve: 10.614 | bob: 6.125Epoch  10:  45% | abe: 6.003 | eve: 10.614 | bob: 6.125Epoch  10:  46% | abe: 6.003 | eve: 10.614 | bob: 6.125Epoch  10:  46% | abe: 6.003 | eve: 10.614 | bob: 6.125Epoch  10:  47% | abe: 6.002 | eve: 10.613 | bob: 6.125Epoch  10:  48% | abe: 6.002 | eve: 10.613 | bob: 6.125Epoch  10:  49% | abe: 6.003 | eve: 10.613 | bob: 6.126Epoch  10:  50% | abe: 6.003 | eve: 10.613 | bob: 6.125Epoch  10:  50% | abe: 6.003 | eve: 10.613 | bob: 6.126Epoch  10:  51% | abe: 6.004 | eve: 10.613 | bob: 6.126Epoch  10:  52% | abe: 6.003 | eve: 10.612 | bob: 6.126Epoch  10:  53% | abe: 6.004 | eve: 10.612 | bob: 6.127Epoch  10:  53% | abe: 6.005 | eve: 10.613 | bob: 6.127Epoch  10:  54% | abe: 6.004 | eve: 10.613 | bob: 6.127Epoch  10:  55% | abe: 6.004 | eve: 10.613 | bob: 6.126Epoch  10:  56% | abe: 6.004 | eve: 10.612 | bob: 6.127Epoch  10:  57% | abe: 6.004 | eve: 10.613 | bob: 6.127Epoch  10:  57% | abe: 6.004 | eve: 10.613 | bob: 6.127Epoch  10:  58% | abe: 6.004 | eve: 10.613 | bob: 6.128Epoch  10:  59% | abe: 6.004 | eve: 10.613 | bob: 6.128Epoch  10:  60% | abe: 6.003 | eve: 10.613 | bob: 6.128Epoch  10:  60% | abe: 6.004 | eve: 10.614 | bob: 6.128Epoch  10:  61% | abe: 6.004 | eve: 10.614 | bob: 6.128Epoch  10:  62% | abe: 6.004 | eve: 10.614 | bob: 6.128Epoch  10:  63% | abe: 6.004 | eve: 10.614 | bob: 6.128Epoch  10:  64% | abe: 6.004 | eve: 10.614 | bob: 6.129Epoch  10:  64% | abe: 6.004 | eve: 10.614 | bob: 6.129Epoch  10:  65% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  66% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  67% | abe: 6.004 | eve: 10.614 | bob: 6.130Epoch  10:  67% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  68% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  69% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  70% | abe: 6.003 | eve: 10.614 | bob: 6.129Epoch  10:  71% | abe: 6.003 | eve: 10.613 | bob: 6.130Epoch  10:  71% | abe: 6.004 | eve: 10.613 | bob: 6.131Epoch  10:  72% | abe: 6.005 | eve: 10.613 | bob: 6.131Epoch  10:  73% | abe: 6.005 | eve: 10.613 | bob: 6.132Epoch  10:  74% | abe: 6.005 | eve: 10.613 | bob: 6.132Epoch  10:  75% | abe: 6.004 | eve: 10.612 | bob: 6.131Epoch  10:  75% | abe: 6.004 | eve: 10.612 | bob: 6.130Epoch  10:  76% | abe: 6.004 | eve: 10.612 | bob: 6.130Epoch  10:  77% | abe: 6.004 | eve: 10.611 | bob: 6.130Epoch  10:  78% | abe: 6.004 | eve: 10.611 | bob: 6.131Epoch  10:  78% | abe: 6.005 | eve: 10.611 | bob: 6.131Epoch  10:  79% | abe: 6.005 | eve: 10.611 | bob: 6.133Epoch  10:  80% | abe: 6.006 | eve: 10.611 | bob: 6.133Epoch  10:  81% | abe: 6.006 | eve: 10.611 | bob: 6.133Epoch  10:  82% | abe: 6.006 | eve: 10.611 | bob: 6.133Epoch  10:  82% | abe: 6.006 | eve: 10.611 | bob: 6.134Epoch  10:  83% | abe: 6.006 | eve: 10.611 | bob: 6.134Epoch  10:  84% | abe: 6.006 | eve: 10.611 | bob: 6.135Epoch  10:  85% | abe: 6.006 | eve: 10.611 | bob: 6.135Epoch  10:  85% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  86% | abe: 6.007 | eve: 10.610 | bob: 6.135Epoch  10:  87% | abe: 6.007 | eve: 10.610 | bob: 6.135Epoch  10:  88% | abe: 6.008 | eve: 10.610 | bob: 6.136Epoch  10:  89% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  89% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  90% | abe: 6.008 | eve: 10.610 | bob: 6.136Epoch  10:  91% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  92% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  92% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  93% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  94% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  95% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  96% | abe: 6.007 | eve: 10.610 | bob: 6.136Epoch  10:  96% | abe: 6.007 | eve: 10.609 | bob: 6.135Epoch  10:  97% | abe: 6.007 | eve: 10.610 | bob: 6.135Epoch  10:  98% | abe: 6.007 | eve: 10.610 | bob: 6.135Epoch  10:  99% | abe: 6.007 | eve: 10.610 | bob: 6.136
Early stopping: No improvement after 5 epochs since epoch 4. Best Bob loss: 6.0620196655466305
Training complete.
cipher1 + cipher2
[[0.13150612 1.0097127  1.500586   ... 0.5279612  2.         2.        ]
 [0.05706447 0.34374052 1.         ... 0.9661294  2.         2.        ]
 [0.         0.01311749 1.500126   ... 0.         1.6746583  1.8583432 ]
 ...
 [1.         0.31262136 1.5037498  ... 1.         2.         2.        ]
 [0.09930748 0.429369   1.5000045  ... 1.         1.1248577  2.        ]
 [0.8038099  1.2217644  1.4995372  ... 1.9900322  2.         2.        ]]
HO addition:
[[1.0484217e-01 1.0096562e+00 1.5000852e+00 ... 5.2631617e-01
  1.9988741e+00 1.9988741e+00]
 [3.6784649e-02 3.3543932e-01 9.9993885e-01 ... 9.6608061e-01
  1.9988741e+00 1.9988741e+00]
 [4.9923138e-08 7.0299516e-03 1.4996254e+00 ... 4.9923138e-08
  1.6739539e+00 1.8574066e+00]
 ...
 [9.9993885e-01 3.0182749e-01 1.5032454e+00 ... 9.9993885e-01
  1.9988741e+00 1.9988741e+00]
 [7.3264733e-02 4.2519704e-01 1.4995043e+00 ... 9.9994761e-01
  1.1247236e+00 1.9988741e+00]
 [8.0370867e-01 1.2215483e+00 1.4990374e+00 ... 1.9889199e+00
  1.9988741e+00 1.9988741e+00]]
cipher1 * cipher2
[[0.         0.1550892  0.5005861  ... 0.03829812 1.         1.        ]
 [0.         0.         0.         ... 0.         1.         1.        ]
 [0.         0.         0.500126   ... 0.         0.6746583  0.85834324]
 ...
 [0.         0.02007254 0.50374985 ... 0.         1.         1.        ]
 [0.         0.04503588 0.5000046  ... 0.         0.12485769 1.        ]
 [0.         0.3279463  0.4995372  ... 0.99003226 1.         1.        ]]
HO multiplication
[[1.37009835e-08 1.55117020e-01 5.01310110e-01 ... 3.83061282e-02
  1.11851132e+00 1.11851132e+00]
 [6.04008621e-09 3.51435681e-08 3.14051113e-06 ... 2.11690781e-06
  1.11851132e+00 1.11851132e+00]
 [9.99275939e-15 1.41078571e-09 5.00846863e-01 ... 9.99275939e-15
  6.79289937e-01 8.88259590e-01]
 ...
 [3.14051113e-06 2.00777240e-02 5.04496098e-01 ... 3.14051113e-06
  1.11851132e+00 1.11851132e+00]
 [1.03984741e-08 4.50448692e-02 5.00724792e-01 ... 4.24522705e-06
  1.24892280e-01 1.11851132e+00]
 [4.29720728e-07 3.28039646e-01 5.00254035e-01 ... 1.09780765e+00
  1.11851132e+00 1.11851132e+00]]
HO model Accuracy Percentage Addition: 63.72%
HO model Accuracy Percentage Multiplication: 67.19%
Bob decrypted addition: [[0.         0.         0.         ... 0.99852926 0.         0.41521156]
 [0.         0.         0.         ... 0.99444747 0.81864    0.        ]
 [0.         0.         0.41373873 ... 0.9585057  0.         0.99927735]
 ...
 [0.         0.32679605 1.0010421  ... 1.0031986  0.         0.998381  ]
 [0.         0.         0.         ... 1.0017577  0.         1.002421  ]
 [0.         1.0356414  0.         ... 0.99459517 0.         1.0025837 ]]
Bob decrypted bits addition: [[0 0 0 ... 1 0 0]
 [0 0 0 ... 1 1 0]
 [0 0 0 ... 1 0 1]
 ...
 [0 0 1 ... 1 0 1]
 [0 0 0 ... 1 0 1]
 [0 1 0 ... 1 0 1]]
Number of correctly decrypted bits addition: 2997
Total number of bits addition: 8192
Decryption accuracy addition: 36.58447265625%
Bob decrypted multiplication: [[0.         0.40179092 0.19565344 ... 0.9883125  0.         0.8148302 ]
 [0.         0.40446746 0.         ... 0.956913   0.9137486  0.        ]
 [0.         0.         0.3941875  ... 0.8692523  0.         0.9757922 ]
 ...
 [0.         0.13423967 1.0028107  ... 1.0026402  0.         0.96560526]
 [0.         0.         0.         ... 0.9901587  0.         0.99973595]
 [0.         1.0688734  0.30212963 ... 0.9767648  0.         1.0019451 ]]
Bob decrypted bits multiplication: [[0 0 0 ... 1 0 1]
 [0 0 0 ... 1 1 0]
 [0 0 0 ... 1 0 1]
 ...
 [0 0 1 ... 1 0 1]
 [0 0 0 ... 1 0 1]
 [0 1 0 ... 1 0 1]]
Number of correctly decrypted bits multiplication: 6026
Total number of bits multiplication: 8192
Decryption accuracy multiplication: 73.5595703125%
Eve decrypted addition: [[1.3402    1.3627323 1.1862448 ... 1.3693596 1.3171487 0.7659821]
 [1.3404555 1.3628697 1.1863645 ... 1.3692203 1.3170611 0.7661675]
 [1.3399773 1.3626094 1.1861182 ... 1.3694742 1.3172183 0.7658185]
 ...
 [1.3403506 1.362813  1.186313  ... 1.3692775 1.317097  0.7660904]
 [1.3404027 1.3628414 1.1863409 ... 1.3692496 1.3170797 0.7661289]
 [1.3403736 1.3628266 1.1863321 ... 1.3692663 1.3170905 0.7661091]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 4154
Total number of bits addition: 8192
Decryption accuracy by Eve addition: 50.7080078125%
Eve decrypted mulitplication: [[1.3404353  1.3628192  1.1859925  ... 1.3691835  1.3170433  0.76627004]
 [1.340413   1.3624277  1.1842675  ... 1.3690808  1.3168962  0.7665882 ]
 [1.3404799  1.3626536  1.1852002  ... 1.369158   1.3170072  0.76640785]
 ...
 [1.3403825  1.3623178  1.1837257  ... 1.369056   1.3168738  0.76668835]
 [1.3404408  1.3625641  1.184921   ... 1.3691107  1.316931   0.76647395]
 [1.3404623  1.3625911  1.1850814  ... 1.3691212  1.3169378  0.7664449 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 2034
Total number of bits mulitplication: 8192
Decryption accuracy by Eve mulitplication: 24.8291015625%
Bob decrypted P1: [[0.         1.0005225  0.         ... 1.0030298  0.98994803 0.        ]
 [0.         0.99918026 1.0023997  ... 0.         0.92272985 0.        ]
 [0.         0.         1.0012066  ... 0.         0.98866326 0.        ]
 ...
 [0.         0.         1.0118904  ... 0.994178   0.         0.99464554]
 [0.         0.         0.         ... 1.0025306  0.9974266  0.        ]
 [0.         1.0079013  1.0013182  ... 1.0028167  0.9711375  0.        ]]
Bob decrypted bits P1: [[0 1 0 ... 1 1 0]
 [0 1 1 ... 0 1 0]
 [0 0 1 ... 0 1 0]
 ...
 [0 0 1 ... 1 0 1]
 [0 0 0 ... 1 1 0]
 [0 1 1 ... 1 1 0]]
Number of correctly decrypted bits P1: 6409
Total number of bits P1: 8192
Decryption accuracy P1: 78.23486328125%
Bob decrypted P2: [[0.         0.         1.0013725  ... 0.         0.         0.8226268 ]
 [0.         0.         0.         ... 1.0031537  0.9391417  0.        ]
 [0.         1.0028746  0.         ... 1.0024976  0.         1.0002538 ]
 ...
 [0.         1.0036331  1.0024126  ... 1.0027092  0.9708009  0.        ]
 [0.         0.         1.0160134  ... 0.28076065 0.         1.0026035 ]
 [0.         1.003717   0.         ... 0.         0.         1.0025909 ]]
Bob decrypted bits P2: [[0 0 1 ... 0 0 1]
 [0 0 0 ... 1 1 0]
 [0 1 0 ... 1 0 1]
 ...
 [0 1 1 ... 1 1 0]
 [0 0 1 ... 0 0 1]
 [0 1 0 ... 0 0 1]]
Number of correctly decrypted bits P2: 6455
Total number of bits P2: 8192
Decryption accuracy P2: 78.79638671875%
