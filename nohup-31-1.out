WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-11 17:33:01.944867: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-11 17:33:02.053851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-11 17:33:02.054431: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:33:02.057413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 17:33:02.059109: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 17:33:02.059640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 17:33:02.063234: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 17:33:02.065716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 17:33:02.073236: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 17:33:02.082972: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 17:33:02.083426: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-11 17:33:02.100111: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-11 17:33:02.103171: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4fe5810 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-11 17:33:02.103222: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-11 17:33:02.380475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x49aecb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-11 17:33:02.380559: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-11 17:33:02.385272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-11 17:33:02.385443: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:33:02.385497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 17:33:02.385546: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 17:33:02.385596: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 17:33:02.385642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 17:33:02.385687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 17:33:02.385742: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 17:33:02.400524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 17:33:02.400680: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 17:33:02.405795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-11 17:33:02.405852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-11 17:33:02.405864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-11 17:33:02.418343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-11 17:33:06.369716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.9978 - val_loss: 0.0062
Epoch 2/512
512/512 - 0s - loss: 0.4706 - val_loss: 0.0028
Epoch 3/512
512/512 - 0s - loss: 0.2208 - val_loss: 0.0014
Epoch 4/512
512/512 - 0s - loss: 0.1116 - val_loss: 7.3038e-04
Epoch 5/512
512/512 - 0s - loss: 0.0573 - val_loss: 3.4888e-04
Epoch 6/512
512/512 - 0s - loss: 0.0252 - val_loss: 1.2083e-04
Epoch 7/512
512/512 - 0s - loss: 0.0077 - val_loss: 2.7739e-05
Epoch 8/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.1158e-05
Epoch 9/512
512/512 - 0s - loss: 9.9148e-04 - val_loss: 8.2609e-06
Epoch 10/512
512/512 - 0s - loss: 7.4374e-04 - val_loss: 6.1115e-06
Epoch 11/512
512/512 - 0s - loss: 5.3789e-04 - val_loss: 4.2201e-06
Epoch 12/512
512/512 - 0s - loss: 3.6083e-04 - val_loss: 2.6658e-06
Epoch 13/512
512/512 - 0s - loss: 2.1976e-04 - val_loss: 1.5005e-06
Epoch 14/512
512/512 - 0s - loss: 1.1811e-04 - val_loss: 7.2643e-07
Epoch 15/512
512/512 - 0s - loss: 5.3922e-05 - val_loss: 2.8836e-07
Epoch 16/512
512/512 - 0s - loss: 1.9919e-05 - val_loss: 8.8543e-08
Epoch 17/512
512/512 - 0s - loss: 5.6287e-06 - val_loss: 2.0949e-08
Epoch 18/512
512/512 - 0s - loss: 4.2274e-06 - val_loss: 5.2325e-07
Epoch 19/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.2991e-05
Epoch 20/512
512/512 - 0s - loss: 5.9061e-04 - val_loss: 7.9373e-07
Epoch 21/512
512/512 - 0s - loss: 6.1065e-05 - val_loss: 6.8437e-07
Epoch 22/512
512/512 - 0s - loss: 1.6678e-04 - val_loss: 5.9099e-06
Epoch 23/512
512/512 - 0s - loss: 9.6150e-04 - val_loss: 7.0391e-06
Epoch 24/512
512/512 - 0s - loss: 4.1751e-04 - val_loss: 1.6961e-06
Epoch 25/512
512/512 - 0s - loss: 1.7455e-04 - val_loss: 2.6706e-06
Epoch 26/512
512/512 - 0s - loss: 4.6736e-04 - val_loss: 7.8158e-06
Epoch 27/512
512/512 - 0s - loss: 6.4796e-04 - val_loss: 3.4483e-06
Epoch 28/512
512/512 - 0s - loss: 2.6423e-04 - val_loss: 2.2309e-06
Epoch 29/512
512/512 - 0s - loss: 2.8683e-04 - val_loss: 4.5398e-06
Epoch 30/512
512/512 - 0s - loss: 5.1786e-04 - val_loss: 4.7205e-06
Epoch 31/512
512/512 - 0s - loss: 3.6463e-04 - val_loss: 2.4930e-06
Epoch 32/512
512/512 - 0s - loss: 2.4786e-04 - val_loss: 2.9669e-06
Epoch 33/512
512/512 - 0s - loss: 3.5929e-04 - val_loss: 4.2562e-06
Epoch 34/512
512/512 - 0s - loss: 3.8246e-04 - val_loss: 2.8599e-06
Epoch 35/512
512/512 - 0s - loss: 2.5578e-04 - val_loss: 2.3939e-06
Epoch 36/512
512/512 - 0s - loss: 2.6850e-04 - val_loss: 3.2125e-06
Epoch 37/512
512/512 - 0s - loss: 3.3047e-04 - val_loss: 2.9952e-06
Epoch 38/512
512/512 - 0s - loss: 2.6430e-04 - val_loss: 2.2911e-06
Epoch 39/512
512/512 - 0s - loss: 2.3463e-04 - val_loss: 2.4484e-06
Epoch 40/512
512/512 - 0s - loss: 2.5954e-04 - val_loss: 2.6818e-06
Epoch 41/512
512/512 - 0s - loss: 2.5679e-04 - val_loss: 2.2344e-06
Epoch 42/512
512/512 - 0s - loss: 2.1312e-04 - val_loss: 2.0767e-06
Epoch 43/512
512/512 - 0s - loss: 2.1668e-04 - val_loss: 2.2611e-06
Epoch 44/512
512/512 - 0s - loss: 2.2502e-04 - val_loss: 2.0914e-06
Epoch 45/512
512/512 - 0s - loss: 1.9961e-04 - val_loss: 1.8327e-06
Epoch 46/512
512/512 - 0s - loss: 1.8684e-04 - val_loss: 1.8708e-06
Epoch 47/512
512/512 - 0s - loss: 1.9127e-04 - val_loss: 1.8757e-06
Epoch 48/512
512/512 - 0s - loss: 1.8347e-04 - val_loss: 1.6933e-06
Epoch 49/512
512/512 - 0s - loss: 1.6742e-04 - val_loss: 1.5919e-06
Epoch 50/512
512/512 - 0s - loss: 1.6151e-04 - val_loss: 1.6146e-06
Epoch 51/512
512/512 - 0s - loss: 1.6209e-04 - val_loss: 1.5688e-06
Epoch 52/512
512/512 - 0s - loss: 1.5319e-04 - val_loss: 1.4156e-06
Epoch 53/512
512/512 - 0s - loss: 1.4098e-04 - val_loss: 1.3709e-06
Epoch 54/512
512/512 - 0s - loss: 1.3969e-04 - val_loss: 1.3612e-06
Epoch 55/512
512/512 - 0s - loss: 1.3446e-04 - val_loss: 1.2961e-06
Epoch 56/512
512/512 - 0s - loss: 1.2788e-04 - val_loss: 1.2228e-06
Epoch 57/512
512/512 - 0s - loss: 1.2230e-04 - val_loss: 1.1605e-06
Epoch 58/512
512/512 - 0s - loss: 1.1662e-04 - val_loss: 1.1278e-06
Epoch 59/512
512/512 - 0s - loss: 1.1371e-04 - val_loss: 1.0730e-06
Epoch 60/512
512/512 - 0s - loss: 1.0687e-04 - val_loss: 1.0250e-06
Epoch 61/512
512/512 - 0s - loss: 1.0326e-04 - val_loss: 9.9296e-07
Epoch 62/512
512/512 - 0s - loss: 9.9585e-05 - val_loss: 9.3545e-07
Epoch 63/512
512/512 - 0s - loss: 9.4277e-05 - val_loss: 8.9441e-07
Epoch 64/512
512/512 - 0s - loss: 9.0732e-05 - val_loss: 8.6084e-07
Epoch 65/512
512/512 - 0s - loss: 8.6974e-05 - val_loss: 8.2697e-07
Epoch 66/512
512/512 - 0s - loss: 8.3037e-05 - val_loss: 7.9638e-07
Epoch 67/512
512/512 - 0s - loss: 8.0297e-05 - val_loss: 7.5695e-07
Epoch 68/512
512/512 - 0s - loss: 7.6358e-05 - val_loss: 7.1674e-07
Epoch 69/512
512/512 - 0s - loss: 7.2754e-05 - val_loss: 6.9109e-07
Epoch 70/512
512/512 - 0s - loss: 7.0087e-05 - val_loss: 6.6866e-07
Epoch 71/512
512/512 - 0s - loss: 6.7826e-05 - val_loss: 6.2641e-07
Epoch 72/512
512/512 - 0s - loss: 6.3305e-05 - val_loss: 6.0010e-07
Epoch 73/512
512/512 - 0s - loss: 6.1014e-05 - val_loss: 5.9286e-07
Epoch 74/512
512/512 - 0s - loss: 6.0688e-05 - val_loss: 5.4581e-07
Epoch 75/512
512/512 - 0s - loss: 5.4929e-05 - val_loss: 5.1646e-07
Epoch 76/512
512/512 - 0s - loss: 5.3234e-05 - val_loss: 5.1984e-07
Epoch 77/512
512/512 - 0s - loss: 5.2952e-05 - val_loss: 4.9304e-07
Epoch 78/512
512/512 - 0s - loss: 4.9650e-05 - val_loss: 4.4595e-07
Epoch 79/512
512/512 - 0s - loss: 4.5743e-05 - val_loss: 4.3645e-07
Epoch 80/512
512/512 - 0s - loss: 4.5289e-05 - val_loss: 4.4528e-07
Epoch 81/512
512/512 - 0s - loss: 4.4822e-05 - val_loss: 4.0804e-07
Epoch 82/512
512/512 - 0s - loss: 4.0601e-05 - val_loss: 3.7454e-07
Epoch 83/512
512/512 - 0s - loss: 3.8849e-05 - val_loss: 3.7423e-07
Epoch 84/512
512/512 - 0s - loss: 3.8462e-05 - val_loss: 3.6623e-07
Epoch 85/512
512/512 - 0s - loss: 3.6969e-05 - val_loss: 3.3304e-07
Epoch 86/512
512/512 - 0s - loss: 3.3700e-05 - val_loss: 3.1739e-07
Epoch 87/512
512/512 - 0s - loss: 3.3064e-05 - val_loss: 3.1840e-07
Epoch 88/512
512/512 - 0s - loss: 3.2500e-05 - val_loss: 2.9792e-07
Epoch 89/512
512/512 - 0s - loss: 2.9817e-05 - val_loss: 2.8025e-07
Epoch 90/512
512/512 - 0s - loss: 2.8764e-05 - val_loss: 2.7638e-07
Epoch 91/512
512/512 - 0s - loss: 2.8272e-05 - val_loss: 2.6301e-07
Epoch 92/512
512/512 - 0s - loss: 2.6535e-05 - val_loss: 2.4181e-07
Epoch 93/512
512/512 - 0s - loss: 2.5000e-05 - val_loss: 2.3292e-07
Epoch 94/512
512/512 - 0s - loss: 2.4084e-05 - val_loss: 2.2955e-07
Epoch 95/512
512/512 - 0s - loss: 2.3673e-05 - val_loss: 2.1596e-07
Epoch 96/512
512/512 - 0s - loss: 2.2044e-05 - val_loss: 1.9953e-07
Epoch 97/512
512/512 - 0s - loss: 2.0496e-05 - val_loss: 2.0057e-07
Epoch 98/512
512/512 - 0s - loss: 2.0866e-05 - val_loss: 1.9356e-07
Epoch 99/512
512/512 - 0s - loss: 1.9479e-05 - val_loss: 1.7361e-07
Epoch 100/512
512/512 - 0s - loss: 1.7906e-05 - val_loss: 1.6867e-07
Epoch 101/512
512/512 - 0s - loss: 1.7750e-05 - val_loss: 1.6683e-07
Epoch 102/512
512/512 - 0s - loss: 1.7060e-05 - val_loss: 1.5711e-07
Epoch 103/512
512/512 - 0s - loss: 1.5864e-05 - val_loss: 1.5106e-07
Epoch 104/512
512/512 - 0s - loss: 1.5524e-05 - val_loss: 1.4487e-07
Epoch 105/512
512/512 - 0s - loss: 1.4853e-05 - val_loss: 1.3470e-07
Epoch 106/512
512/512 - 0s - loss: 1.3722e-05 - val_loss: 1.3018e-07
Epoch 107/512
512/512 - 0s - loss: 1.3499e-05 - val_loss: 1.2736e-07
Epoch 108/512
512/512 - 0s - loss: 1.3132e-05 - val_loss: 1.1536e-07
Epoch 109/512
512/512 - 0s - loss: 1.1728e-05 - val_loss: 1.1246e-07
Epoch 110/512
512/512 - 0s - loss: 1.1820e-05 - val_loss: 1.1215e-07
Epoch 111/512
512/512 - 0s - loss: 1.1482e-05 - val_loss: 1.0127e-07
Epoch 112/512
512/512 - 0s - loss: 1.0405e-05 - val_loss: 9.3180e-08
Epoch 113/512
512/512 - 0s - loss: 9.9106e-06 - val_loss: 9.4412e-08
Epoch 114/512
512/512 - 0s - loss: 1.0007e-05 - val_loss: 9.0896e-08
Epoch 115/512
512/512 - 0s - loss: 9.2344e-06 - val_loss: 8.3006e-08
Epoch 116/512
512/512 - 0s - loss: 8.6133e-06 - val_loss: 8.1297e-08
Epoch 117/512
512/512 - 0s - loss: 8.5126e-06 - val_loss: 7.9707e-08
Epoch 118/512
512/512 - 0s - loss: 8.1816e-06 - val_loss: 7.4190e-08
Epoch 119/512
512/512 - 0s - loss: 7.5575e-06 - val_loss: 6.9517e-08
Epoch 120/512
512/512 - 0s - loss: 7.2841e-06 - val_loss: 6.8106e-08
Epoch 121/512
512/512 - 0s - loss: 7.0722e-06 - val_loss: 6.4619e-08
Epoch 122/512
512/512 - 0s - loss: 6.7046e-06 - val_loss: 5.9940e-08
Epoch 123/512
512/512 - 0s - loss: 6.2047e-06 - val_loss: 5.8083e-08
Epoch 124/512
512/512 - 0s - loss: 6.1152e-06 - val_loss: 5.7102e-08
Epoch 125/512
512/512 - 0s - loss: 5.8949e-06 - val_loss: 5.2303e-08
Epoch 126/512
512/512 - 0s - loss: 5.3985e-06 - val_loss: 4.9461e-08
Epoch 127/512
512/512 - 0s - loss: 5.1851e-06 - val_loss: 4.9861e-08
Epoch 128/512
512/512 - 0s - loss: 5.2047e-06 - val_loss: 4.6516e-08
Epoch 129/512
512/512 - 0s - loss: 4.7204e-06 - val_loss: 4.2483e-08
Epoch 130/512
512/512 - 0s - loss: 4.4631e-06 - val_loss: 4.1970e-08
Epoch 131/512
512/512 - 0s - loss: 4.4161e-06 - val_loss: 4.0536e-08
Epoch 132/512
512/512 - 0s - loss: 4.1605e-06 - val_loss: 3.7687e-08
Epoch 133/512
512/512 - 0s - loss: 3.9182e-06 - val_loss: 3.5948e-08
Epoch 134/512
512/512 - 0s - loss: 3.7476e-06 - val_loss: 3.4780e-08
Epoch 135/512
512/512 - 0s - loss: 3.6173e-06 - val_loss: 3.3043e-08
Epoch 136/512
512/512 - 0s - loss: 3.4219e-06 - val_loss: 3.1117e-08
Epoch 137/512
512/512 - 0s - loss: 3.2425e-06 - val_loss: 2.9371e-08
Epoch 138/512
512/512 - 0s - loss: 3.0844e-06 - val_loss: 2.8618e-08
Epoch 139/512
512/512 - 0s - loss: 2.9885e-06 - val_loss: 2.7297e-08
Epoch 140/512
512/512 - 0s - loss: 2.8097e-06 - val_loss: 2.5802e-08
Epoch 141/512
512/512 - 0s - loss: 2.6830e-06 - val_loss: 2.4286e-08
Epoch 142/512
512/512 - 0s - loss: 2.5531e-06 - val_loss: 2.2978e-08
Epoch 143/512
512/512 - 0s - loss: 2.4152e-06 - val_loss: 2.2297e-08
Epoch 144/512
512/512 - 0s - loss: 2.3374e-06 - val_loss: 2.1362e-08
Epoch 145/512
512/512 - 0s - loss: 2.2157e-06 - val_loss: 2.0058e-08
Epoch 146/512
512/512 - 0s - loss: 2.0907e-06 - val_loss: 1.8921e-08
Epoch 147/512
512/512 - 0s - loss: 1.9891e-06 - val_loss: 1.8417e-08
Epoch 148/512
512/512 - 0s - loss: 1.9242e-06 - val_loss: 1.7718e-08
Epoch 149/512
512/512 - 0s - loss: 1.8361e-06 - val_loss: 1.6333e-08
Epoch 150/512
512/512 - 0s - loss: 1.6971e-06 - val_loss: 1.5615e-08
Epoch 151/512
512/512 - 0s - loss: 1.6574e-06 - val_loss: 1.5019e-08
Epoch 152/512
512/512 - 0s - loss: 1.5721e-06 - val_loss: 1.4160e-08
Epoch 153/512
512/512 - 0s - loss: 1.4780e-06 - val_loss: 1.3438e-08
Epoch 154/512
512/512 - 0s - loss: 1.4179e-06 - val_loss: 1.3003e-08
Epoch 155/512
512/512 - 0s - loss: 1.3600e-06 - val_loss: 1.2410e-08
Epoch 156/512
512/512 - 0s - loss: 1.2950e-06 - val_loss: 1.1457e-08
Epoch 157/512
512/512 - 0s - loss: 1.2005e-06 - val_loss: 1.0921e-08
Epoch 158/512
512/512 - 0s - loss: 1.1605e-06 - val_loss: 1.0739e-08
Epoch 159/512
512/512 - 0s - loss: 1.1282e-06 - val_loss: 1.0155e-08
Epoch 160/512
512/512 - 0s - loss: 1.0572e-06 - val_loss: 9.2475e-09
Epoch 161/512
512/512 - 0s - loss: 9.7349e-07 - val_loss: 9.0081e-09
Epoch 162/512
512/512 - 0s - loss: 9.6646e-07 - val_loss: 8.7658e-09
Epoch 163/512
512/512 - 0s - loss: 9.1709e-07 - val_loss: 8.1671e-09
Epoch 164/512
512/512 - 0s - loss: 8.4634e-07 - val_loss: 7.8089e-09
Epoch 165/512
512/512 - 0s - loss: 8.2762e-07 - val_loss: 7.4207e-09
Epoch 166/512
512/512 - 0s - loss: 7.8456e-07 - val_loss: 6.9937e-09
Epoch 167/512
512/512 - 0s - loss: 7.3272e-07 - val_loss: 6.6375e-09
Epoch 168/512
512/512 - 0s - loss: 7.0104e-07 - val_loss: 6.5378e-09
Epoch 169/512
512/512 - 0s - loss: 6.7825e-07 - val_loss: 6.2457e-09
Epoch 170/512
512/512 - 0s - loss: 6.4922e-07 - val_loss: 5.6693e-09
Epoch 171/512
512/512 - 0s - loss: 5.9070e-07 - val_loss: 5.3963e-09
Epoch 172/512
512/512 - 0s - loss: 5.7675e-07 - val_loss: 5.2927e-09
Epoch 173/512
512/512 - 0s - loss: 5.5825e-07 - val_loss: 4.9471e-09
Epoch 174/512
512/512 - 0s - loss: 5.1820e-07 - val_loss: 4.6110e-09
Epoch 175/512
512/512 - 0s - loss: 4.8986e-07 - val_loss: 4.4484e-09
Epoch 176/512
512/512 - 0s - loss: 4.7556e-07 - val_loss: 4.2431e-09
Epoch 177/512
512/512 - 0s - loss: 4.4367e-07 - val_loss: 4.0484e-09
Epoch 178/512
512/512 - 0s - loss: 4.2991e-07 - val_loss: 3.7826e-09
Epoch 179/512
512/512 - 0s - loss: 3.9702e-07 - val_loss: 3.6270e-09
Epoch 180/512
512/512 - 0s - loss: 3.8709e-07 - val_loss: 3.5348e-09
Epoch 181/512
512/512 - 0s - loss: 3.6866e-07 - val_loss: 3.3252e-09
Epoch 182/512
512/512 - 0s - loss: 3.4452e-07 - val_loss: 3.1296e-09
Epoch 183/512
512/512 - 0s - loss: 3.3000e-07 - val_loss: 2.9692e-09
Epoch 184/512
512/512 - 0s - loss: 3.1286e-07 - val_loss: 2.8170e-09
Epoch 185/512
512/512 - 0s - loss: 2.9832e-07 - val_loss: 2.6877e-09
Epoch 186/512
512/512 - 0s - loss: 2.8187e-07 - val_loss: 2.5543e-09
Epoch 187/512
512/512 - 0s - loss: 2.7072e-07 - val_loss: 2.4221e-09
Epoch 188/512
512/512 - 0s - loss: 2.5427e-07 - val_loss: 2.2811e-09
Epoch 189/512
512/512 - 0s - loss: 2.4110e-07 - val_loss: 2.1916e-09
Epoch 190/512
512/512 - 0s - loss: 2.3253e-07 - val_loss: 2.0797e-09
Epoch 191/512
512/512 - 0s - loss: 2.1926e-07 - val_loss: 1.9578e-09
Epoch 192/512
512/512 - 0s - loss: 2.0540e-07 - val_loss: 1.8715e-09
Epoch 193/512
512/512 - 0s - loss: 1.9818e-07 - val_loss: 1.8126e-09
Epoch 194/512
512/512 - 0s - loss: 1.9079e-07 - val_loss: 1.6844e-09
Epoch 195/512
512/512 - 0s - loss: 1.7685e-07 - val_loss: 1.5625e-09
Epoch 196/512
512/512 - 0s - loss: 1.6728e-07 - val_loss: 1.5342e-09
Epoch 197/512
512/512 - 0s - loss: 1.6230e-07 - val_loss: 1.4807e-09
Epoch 198/512
512/512 - 0s - loss: 1.5609e-07 - val_loss: 1.3518e-09
Epoch 199/512
512/512 - 0s - loss: 1.4247e-07 - val_loss: 1.2561e-09
Epoch 200/512
512/512 - 0s - loss: 1.3643e-07 - val_loss: 1.2416e-09
Epoch 201/512
512/512 - 0s - loss: 1.3227e-07 - val_loss: 1.2024e-09
Epoch 202/512
512/512 - 0s - loss: 1.2647e-07 - val_loss: 1.1125e-09
Epoch 203/512
512/512 - 0s - loss: 1.1707e-07 - val_loss: 1.0375e-09
Epoch 204/512
512/512 - 0s - loss: 1.1096e-07 - val_loss: 1.0226e-09
Epoch 205/512
512/512 - 0s - loss: 1.0945e-07 - val_loss: 9.6172e-10
Epoch 206/512
512/512 - 0s - loss: 1.0082e-07 - val_loss: 8.8894e-10
Epoch 207/512
512/512 - 0s - loss: 9.4513e-08 - val_loss: 8.6996e-10
Epoch 208/512
512/512 - 0s - loss: 9.3465e-08 - val_loss: 8.3987e-10
Epoch 209/512
512/512 - 0s - loss: 8.7302e-08 - val_loss: 7.8394e-10
Epoch 210/512
512/512 - 0s - loss: 8.2636e-08 - val_loss: 7.3003e-10
Epoch 211/512
512/512 - 0s - loss: 7.7781e-08 - val_loss: 7.0294e-10
Epoch 212/512
512/512 - 0s - loss: 7.4712e-08 - val_loss: 6.8292e-10
Epoch 213/512
512/512 - 0s - loss: 7.2463e-08 - val_loss: 6.4031e-10
Epoch 214/512
512/512 - 0s - loss: 6.7629e-08 - val_loss: 5.9382e-10
Epoch 215/512
512/512 - 0s - loss: 6.3333e-08 - val_loss: 5.6627e-10
Epoch 216/512
512/512 - 0s - loss: 6.0514e-08 - val_loss: 5.5966e-10
Epoch 217/512
512/512 - 0s - loss: 5.9617e-08 - val_loss: 5.2617e-10
Epoch 218/512
512/512 - 0s - loss: 5.4785e-08 - val_loss: 4.9018e-10
Epoch 219/512
512/512 - 0s - loss: 5.2211e-08 - val_loss: 4.6619e-10
Epoch 220/512
512/512 - 0s - loss: 4.9405e-08 - val_loss: 4.5709e-10
Epoch 221/512
512/512 - 0s - loss: 4.8580e-08 - val_loss: 4.3348e-10
Epoch 222/512
512/512 - 0s - loss: 4.5176e-08 - val_loss: 4.0115e-10
Epoch 223/512
512/512 - 0s - loss: 4.2478e-08 - val_loss: 3.8714e-10
Epoch 224/512
512/512 - 0s - loss: 4.1440e-08 - val_loss: 3.6879e-10
Epoch 225/512
512/512 - 0s - loss: 3.9029e-08 - val_loss: 3.4547e-10
Epoch 226/512
512/512 - 0s - loss: 3.6770e-08 - val_loss: 3.3324e-10
Epoch 227/512
512/512 - 0s - loss: 3.5604e-08 - val_loss: 3.1751e-10
Epoch 228/512
512/512 - 0s - loss: 3.3848e-08 - val_loss: 2.9932e-10
Epoch 229/512
512/512 - 0s - loss: 3.1692e-08 - val_loss: 2.8748e-10
Epoch 230/512
512/512 - 0s - loss: 3.0447e-08 - val_loss: 2.8092e-10
Epoch 231/512
512/512 - 0s - loss: 2.9881e-08 - val_loss: 2.6202e-10
Epoch 232/512
512/512 - 0s - loss: 2.7505e-08 - val_loss: 2.4269e-10
Epoch 233/512
512/512 - 0s - loss: 2.6035e-08 - val_loss: 2.3695e-10
Epoch 234/512
512/512 - 0s - loss: 2.5326e-08 - val_loss: 2.3181e-10
Epoch 235/512
512/512 - 0s - loss: 2.4534e-08 - val_loss: 2.1759e-10
Epoch 236/512
512/512 - 0s - loss: 2.2917e-08 - val_loss: 2.0246e-10
Epoch 237/512
512/512 - 0s - loss: 2.1581e-08 - val_loss: 1.9574e-10
Epoch 238/512
512/512 - 0s - loss: 2.0962e-08 - val_loss: 1.9035e-10
Epoch 239/512
512/512 - 0s - loss: 2.0221e-08 - val_loss: 1.8019e-10
Epoch 240/512
512/512 - 0s - loss: 1.9036e-08 - val_loss: 1.7059e-10
Epoch 241/512
512/512 - 0s - loss: 1.8007e-08 - val_loss: 1.6395e-10
Epoch 242/512
512/512 - 0s - loss: 1.7470e-08 - val_loss: 1.5996e-10
Epoch 243/512
512/512 - 0s - loss: 1.7006e-08 - val_loss: 1.5008e-10
Epoch 244/512
512/512 - 0s - loss: 1.5738e-08 - val_loss: 1.4020e-10
Epoch 245/512
512/512 - 0s - loss: 1.4934e-08 - val_loss: 1.3804e-10
Epoch 246/512
512/512 - 0s - loss: 1.4805e-08 - val_loss: 1.3397e-10
Epoch 247/512
512/512 - 0s - loss: 1.4054e-08 - val_loss: 1.2679e-10
Epoch 248/512
512/512 - 0s - loss: 1.3229e-08 - val_loss: 1.2148e-10
Epoch 249/512
512/512 - 0s - loss: 1.2937e-08 - val_loss: 1.1548e-10
Epoch 250/512
512/512 - 0s - loss: 1.2115e-08 - val_loss: 1.1077e-10
Epoch 251/512
512/512 - 0s - loss: 1.1887e-08 - val_loss: 1.0614e-10
Epoch 252/512
512/512 - 0s - loss: 1.1154e-08 - val_loss: 1.0158e-10
Epoch 253/512
512/512 - 0s - loss: 1.0867e-08 - val_loss: 9.7403e-11
Epoch 254/512
512/512 - 0s - loss: 1.0219e-08 - val_loss: 9.4652e-11
Epoch 255/512
512/512 - 0s - loss: 1.0030e-08 - val_loss: 9.1080e-11
Epoch 256/512
512/512 - 0s - loss: 9.5983e-09 - val_loss: 8.5734e-11
Epoch 257/512
512/512 - 0s - loss: 9.0735e-09 - val_loss: 8.1813e-11
Epoch 258/512
512/512 - 0s - loss: 8.6151e-09 - val_loss: 8.0309e-11
Epoch 259/512
512/512 - 0s - loss: 8.5563e-09 - val_loss: 7.9218e-11
Epoch 260/512
512/512 - 0s - loss: 8.2385e-09 - val_loss: 7.5743e-11
Epoch 261/512
512/512 - 0s - loss: 7.9107e-09 - val_loss: 7.0844e-11
Epoch 262/512
512/512 - 0s - loss: 7.4974e-09 - val_loss: 6.7387e-11
Epoch 263/512
512/512 - 0s - loss: 7.1288e-09 - val_loss: 6.5573e-11
Epoch 264/512
512/512 - 0s - loss: 7.0878e-09 - val_loss: 6.3185e-11
Epoch 265/512
512/512 - 0s - loss: 6.6453e-09 - val_loss: 6.1252e-11
Epoch 266/512
512/512 - 0s - loss: 6.4416e-09 - val_loss: 6.0272e-11
Epoch 267/512
512/512 - 0s - loss: 6.3605e-09 - val_loss: 5.6867e-11
Epoch 268/512
512/512 - 0s - loss: 5.9496e-09 - val_loss: 5.4050e-11
Epoch 269/512
512/512 - 0s - loss: 5.7350e-09 - val_loss: 5.2497e-11
Epoch 270/512
512/512 - 0s - loss: 5.5433e-09 - val_loss: 5.2237e-11
Epoch 271/512
512/512 - 0s - loss: 5.4889e-09 - val_loss: 5.0657e-11
Epoch 272/512
512/512 - 0s - loss: 5.2920e-09 - val_loss: 4.7647e-11
Epoch 273/512
512/512 - 0s - loss: 4.9702e-09 - val_loss: 4.5535e-11
Epoch 274/512
512/512 - 0s - loss: 4.8183e-09 - val_loss: 4.5044e-11
Epoch 275/512
512/512 - 0s - loss: 4.7739e-09 - val_loss: 4.3200e-11
Epoch 276/512
512/512 - 0s - loss: 4.4859e-09 - val_loss: 4.1377e-11
Epoch 277/512
512/512 - 0s - loss: 4.3408e-09 - val_loss: 4.0649e-11
Epoch 278/512
512/512 - 0s - loss: 4.2519e-09 - val_loss: 3.9817e-11
Epoch 279/512
512/512 - 0s - loss: 4.1816e-09 - val_loss: 3.8192e-11
Epoch 280/512
512/512 - 0s - loss: 3.9888e-09 - val_loss: 3.6184e-11
Epoch 281/512
512/512 - 0s - loss: 3.7696e-09 - val_loss: 3.5158e-11
Epoch 282/512
512/512 - 0s - loss: 3.7315e-09 - val_loss: 3.4266e-11
Epoch 283/512
512/512 - 0s - loss: 3.6061e-09 - val_loss: 3.3635e-11
Epoch 284/512
512/512 - 0s - loss: 3.5205e-09 - val_loss: 3.2611e-11
Epoch 285/512
512/512 - 0s - loss: 3.3584e-09 - val_loss: 3.1729e-11
Epoch 286/512
512/512 - 0s - loss: 3.3164e-09 - val_loss: 3.0925e-11
Epoch 287/512
512/512 - 0s - loss: 3.2111e-09 - val_loss: 2.9835e-11
Epoch 288/512
512/512 - 0s - loss: 3.1373e-09 - val_loss: 2.8708e-11
Epoch 289/512
512/512 - 0s - loss: 3.0042e-09 - val_loss: 2.7545e-11
Epoch 290/512
512/512 - 0s - loss: 2.9109e-09 - val_loss: 2.6664e-11
Epoch 291/512
512/512 - 0s - loss: 2.7976e-09 - val_loss: 2.6281e-11
Epoch 292/512
512/512 - 0s - loss: 2.7568e-09 - val_loss: 2.5620e-11
Epoch 293/512
512/512 - 0s - loss: 2.6978e-09 - val_loss: 2.4831e-11
Epoch 294/512
512/512 - 0s - loss: 2.6309e-09 - val_loss: 2.4155e-11
Epoch 295/512
512/512 - 0s - loss: 2.5264e-09 - val_loss: 2.3305e-11
Epoch 296/512
512/512 - 0s - loss: 2.4285e-09 - val_loss: 2.2758e-11
Epoch 297/512
512/512 - 0s - loss: 2.4120e-09 - val_loss: 2.2659e-11
Epoch 298/512
512/512 - 0s - loss: 2.3630e-09 - val_loss: 2.1963e-11
Epoch 299/512
512/512 - 0s - loss: 2.2660e-09 - val_loss: 2.1548e-11
Epoch 300/512
512/512 - 0s - loss: 2.2258e-09 - val_loss: 2.1021e-11
Epoch 301/512
512/512 - 0s - loss: 2.1864e-09 - val_loss: 2.0308e-11
Epoch 302/512
512/512 - 0s - loss: 2.0945e-09 - val_loss: 1.9380e-11
Epoch 303/512
512/512 - 0s - loss: 1.9896e-09 - val_loss: 1.8712e-11
Epoch 304/512
512/512 - 0s - loss: 1.9639e-09 - val_loss: 1.8546e-11
Epoch 305/512
512/512 - 0s - loss: 1.9235e-09 - val_loss: 1.8490e-11
Epoch 306/512
512/512 - 0s - loss: 1.9204e-09 - val_loss: 1.8087e-11
Epoch 307/512
512/512 - 0s - loss: 1.8668e-09 - val_loss: 1.7249e-11
Epoch 308/512
512/512 - 0s - loss: 1.7888e-09 - val_loss: 1.6427e-11
Epoch 309/512
512/512 - 0s - loss: 1.7051e-09 - val_loss: 1.6377e-11
Epoch 310/512
512/512 - 0s - loss: 1.7188e-09 - val_loss: 1.6371e-11
Epoch 311/512
512/512 - 0s - loss: 1.7194e-09 - val_loss: 1.5807e-11
Epoch 312/512
512/512 - 0s - loss: 1.6412e-09 - val_loss: 1.5113e-11
Epoch 313/512
512/512 - 0s - loss: 1.5603e-09 - val_loss: 1.4577e-11
Epoch 314/512
512/512 - 0s - loss: 1.5375e-09 - val_loss: 1.4408e-11
Epoch 315/512
512/512 - 0s - loss: 1.5117e-09 - val_loss: 1.4337e-11
Epoch 316/512
512/512 - 0s - loss: 1.4912e-09 - val_loss: 1.4244e-11
Epoch 317/512
512/512 - 0s - loss: 1.4746e-09 - val_loss: 1.3812e-11
Epoch 318/512
512/512 - 0s - loss: 1.4459e-09 - val_loss: 1.3314e-11
Epoch 319/512
512/512 - 0s - loss: 1.3727e-09 - val_loss: 1.2914e-11
Epoch 320/512
512/512 - 0s - loss: 1.3360e-09 - val_loss: 1.2707e-11
Epoch 321/512
512/512 - 0s - loss: 1.3121e-09 - val_loss: 1.2517e-11
Epoch 322/512
512/512 - 0s - loss: 1.2894e-09 - val_loss: 1.2454e-11
Epoch 323/512
512/512 - 0s - loss: 1.2927e-09 - val_loss: 1.2246e-11
Epoch 324/512
512/512 - 0s - loss: 1.2637e-09 - val_loss: 1.1971e-11
Epoch 325/512
512/512 - 0s - loss: 1.2374e-09 - val_loss: 1.1483e-11
Epoch 326/512
512/512 - 0s - loss: 1.1868e-09 - val_loss: 1.1001e-11
Epoch 327/512
512/512 - 0s - loss: 1.1302e-09 - val_loss: 1.0832e-11
Epoch 328/512
512/512 - 0s - loss: 1.1133e-09 - val_loss: 1.0788e-11
Epoch 329/512
512/512 - 0s - loss: 1.1297e-09 - val_loss: 1.0775e-11
Epoch 330/512
512/512 - 0s - loss: 1.1283e-09 - val_loss: 1.0604e-11
Epoch 331/512
512/512 - 0s - loss: 1.0942e-09 - val_loss: 1.0350e-11
Epoch 332/512
512/512 - 0s - loss: 1.0579e-09 - val_loss: 9.9483e-12
Epoch 333/512
512/512 - 0s - loss: 1.0279e-09 - val_loss: 9.7176e-12
Epoch 334/512
512/512 - 0s - loss: 1.0103e-09 - val_loss: 9.6623e-12
Epoch 335/512
512/512 - 0s - loss: 1.0141e-09 - val_loss: 9.5012e-12
Epoch 336/512
512/512 - 0s - loss: 9.7738e-10 - val_loss: 9.3263e-12
Epoch 337/512
512/512 - 0s - loss: 9.6180e-10 - val_loss: 8.9906e-12
Epoch 338/512
512/512 - 0s - loss: 9.1974e-10 - val_loss: 8.8481e-12
Epoch 339/512
512/512 - 0s - loss: 9.1669e-10 - val_loss: 8.8232e-12
Epoch 340/512
512/512 - 0s - loss: 9.2156e-10 - val_loss: 8.6576e-12
Epoch 341/512
512/512 - 0s - loss: 8.8744e-10 - val_loss: 8.3545e-12
Epoch 342/512
512/512 - 0s - loss: 8.5774e-10 - val_loss: 8.1774e-12
Epoch 343/512
512/512 - 0s - loss: 8.4617e-10 - val_loss: 8.0917e-12
Epoch 344/512
512/512 - 0s - loss: 8.3198e-10 - val_loss: 8.1317e-12
Epoch 345/512
512/512 - 0s - loss: 8.3999e-10 - val_loss: 7.9970e-12
Epoch 346/512
512/512 - 0s - loss: 8.2988e-10 - val_loss: 7.9218e-12
Epoch 347/512
512/512 - 0s - loss: 8.0563e-10 - val_loss: 7.5869e-12
Epoch 348/512
512/512 - 0s - loss: 7.7632e-10 - val_loss: 7.2192e-12
Epoch 349/512
512/512 - 0s - loss: 7.4327e-10 - val_loss: 7.0777e-12
Epoch 350/512
512/512 - 0s - loss: 7.2643e-10 - val_loss: 7.0482e-12
Epoch 351/512
512/512 - 0s - loss: 7.3148e-10 - val_loss: 7.2442e-12
Epoch 352/512
512/512 - 0s - loss: 7.5639e-10 - val_loss: 7.3185e-12
Epoch 353/512
512/512 - 0s - loss: 7.4502e-10 - val_loss: 7.0172e-12
Epoch 354/512
512/512 - 0s - loss: 7.1226e-10 - val_loss: 6.7689e-12
Epoch 355/512
512/512 - 0s - loss: 6.8744e-10 - val_loss: 6.5661e-12
Epoch 356/512
512/512 - 0s - loss: 6.7569e-10 - val_loss: 6.3850e-12
Epoch 357/512
512/512 - 0s - loss: 6.5624e-10 - val_loss: 6.3106e-12
Epoch 358/512
512/512 - 0s - loss: 6.4343e-10 - val_loss: 6.2478e-12
Epoch 359/512
512/512 - 0s - loss: 6.4382e-10 - val_loss: 6.2713e-12
Epoch 360/512
512/512 - 0s - loss: 6.4555e-10 - val_loss: 6.1626e-12
Epoch 361/512
512/512 - 0s - loss: 6.3476e-10 - val_loss: 6.0574e-12
Epoch 362/512
512/512 - 0s - loss: 6.2008e-10 - val_loss: 5.9559e-12
Epoch 363/512
512/512 - 0s - loss: 6.0133e-10 - val_loss: 5.8432e-12
Epoch 364/512
512/512 - 0s - loss: 5.9602e-10 - val_loss: 5.7178e-12
Epoch 365/512
512/512 - 0s - loss: 5.8281e-10 - val_loss: 5.6112e-12
Epoch 366/512
512/512 - 0s - loss: 5.8024e-10 - val_loss: 5.5705e-12
Epoch 367/512
512/512 - 0s - loss: 5.6378e-10 - val_loss: 5.4184e-12
Epoch 368/512
512/512 - 0s - loss: 5.5554e-10 - val_loss: 5.3631e-12
Epoch 369/512
512/512 - 0s - loss: 5.4751e-10 - val_loss: 5.2357e-12
Epoch 370/512
512/512 - 0s - loss: 5.4064e-10 - val_loss: 5.3131e-12
Epoch 371/512
512/512 - 0s - loss: 5.5117e-10 - val_loss: 5.2329e-12
Epoch 372/512
512/512 - 0s - loss: 5.2495e-10 - val_loss: 5.0624e-12
Epoch 373/512
512/512 - 0s - loss: 5.1092e-10 - val_loss: 4.9230e-12
Epoch 374/512
512/512 - 0s - loss: 5.0496e-10 - val_loss: 4.9029e-12
Epoch 375/512
512/512 - 0s - loss: 5.0259e-10 - val_loss: 4.8290e-12
Epoch 376/512
512/512 - 0s - loss: 4.9892e-10 - val_loss: 4.7933e-12
Epoch 377/512
512/512 - 0s - loss: 4.9100e-10 - val_loss: 4.7090e-12
Epoch 378/512
512/512 - 0s - loss: 4.8040e-10 - val_loss: 4.7145e-12
Epoch 379/512
512/512 - 0s - loss: 4.8138e-10 - val_loss: 4.5542e-12
Epoch 380/512
512/512 - 0s - loss: 4.6600e-10 - val_loss: 4.4998e-12
Epoch 381/512
512/512 - 0s - loss: 4.6188e-10 - val_loss: 4.4305e-12
Epoch 382/512
512/512 - 0s - loss: 4.5136e-10 - val_loss: 4.2887e-12
Epoch 383/512
512/512 - 0s - loss: 4.4639e-10 - val_loss: 4.4353e-12
Epoch 384/512
512/512 - 0s - loss: 4.6456e-10 - val_loss: 4.4221e-12
Epoch 385/512
512/512 - 0s - loss: 4.4896e-10 - val_loss: 4.2355e-12
Epoch 386/512
512/512 - 0s - loss: 4.3179e-10 - val_loss: 4.1505e-12
Epoch 387/512
512/512 - 0s - loss: 4.2466e-10 - val_loss: 4.0864e-12
Epoch 388/512
512/512 - 0s - loss: 4.1271e-10 - val_loss: 3.9608e-12
Epoch 389/512
512/512 - 0s - loss: 4.0841e-10 - val_loss: 3.9286e-12
Epoch 390/512
512/512 - 0s - loss: 3.9749e-10 - val_loss: 3.8287e-12
Epoch 391/512
512/512 - 0s - loss: 3.9386e-10 - val_loss: 3.8253e-12
Epoch 392/512
512/512 - 0s - loss: 3.9088e-10 - val_loss: 3.7699e-12
Epoch 393/512
512/512 - 0s - loss: 3.8465e-10 - val_loss: 3.7231e-12
Epoch 394/512
512/512 - 0s - loss: 3.7917e-10 - val_loss: 3.6439e-12
Epoch 395/512
512/512 - 0s - loss: 3.6919e-10 - val_loss: 3.5226e-12
Epoch 396/512
512/512 - 0s - loss: 3.5956e-10 - val_loss: 3.4285e-12
Epoch 397/512
512/512 - 0s - loss: 3.5570e-10 - val_loss: 3.5721e-12
Epoch 398/512
512/512 - 0s - loss: 3.7127e-10 - val_loss: 3.6197e-12
Epoch 399/512
512/512 - 0s - loss: 3.7373e-10 - val_loss: 3.5521e-12
Epoch 400/512
512/512 - 0s - loss: 3.5700e-10 - val_loss: 3.3968e-12
Epoch 401/512
512/512 - 0s - loss: 3.4736e-10 - val_loss: 3.3717e-12
Epoch 402/512
512/512 - 0s - loss: 3.4106e-10 - val_loss: 3.2643e-12
Epoch 403/512
512/512 - 0s - loss: 3.3291e-10 - val_loss: 3.3008e-12
Epoch 404/512
512/512 - 0s - loss: 3.4638e-10 - val_loss: 3.4304e-12
Epoch 405/512
512/512 - 0s - loss: 3.5101e-10 - val_loss: 3.3082e-12
Epoch 406/512
512/512 - 0s - loss: 3.3584e-10 - val_loss: 3.1624e-12
Epoch 407/512
512/512 - 0s - loss: 3.1892e-10 - val_loss: 3.1740e-12
Epoch 408/512
512/512 - 0s - loss: 3.2840e-10 - val_loss: 3.1886e-12
Epoch 409/512
512/512 - 0s - loss: 3.2676e-10 - val_loss: 3.1641e-12
Epoch 410/512
512/512 - 0s - loss: 3.2547e-10 - val_loss: 3.1658e-12
Epoch 411/512
512/512 - 0s - loss: 3.1828e-10 - val_loss: 3.0564e-12
Epoch 412/512
512/512 - 0s - loss: 3.0623e-10 - val_loss: 2.9574e-12
Epoch 413/512
512/512 - 0s - loss: 3.0284e-10 - val_loss: 2.9558e-12
Epoch 414/512
512/512 - 0s - loss: 3.0308e-10 - val_loss: 2.8997e-12
Epoch 415/512
512/512 - 0s - loss: 2.9375e-10 - val_loss: 2.8810e-12
Epoch 416/512
512/512 - 0s - loss: 2.9419e-10 - val_loss: 2.8392e-12
Epoch 417/512
512/512 - 0s - loss: 2.8682e-10 - val_loss: 2.8340e-12
Epoch 418/512
512/512 - 0s - loss: 2.8559e-10 - val_loss: 2.7905e-12
Epoch 419/512
512/512 - 0s - loss: 2.9150e-10 - val_loss: 2.8089e-12
Epoch 420/512
512/512 - 0s - loss: 2.8465e-10 - val_loss: 2.7223e-12
Epoch 421/512
512/512 - 0s - loss: 2.7732e-10 - val_loss: 2.7029e-12
Epoch 422/512
512/512 - 0s - loss: 2.7051e-10 - val_loss: 2.6204e-12
Epoch 423/512
512/512 - 0s - loss: 2.6488e-10 - val_loss: 2.6108e-12
Epoch 424/512
512/512 - 0s - loss: 2.6769e-10 - val_loss: 2.6088e-12
Epoch 425/512
512/512 - 0s - loss: 2.6485e-10 - val_loss: 2.5951e-12
Epoch 426/512
512/512 - 0s - loss: 2.6509e-10 - val_loss: 2.5745e-12
Epoch 427/512
512/512 - 0s - loss: 2.6451e-10 - val_loss: 2.5666e-12
Epoch 428/512
512/512 - 0s - loss: 2.5758e-10 - val_loss: 2.4991e-12
Epoch 429/512
512/512 - 0s - loss: 2.6045e-10 - val_loss: 2.5355e-12
Epoch 430/512
512/512 - 0s - loss: 2.6285e-10 - val_loss: 2.5636e-12
Epoch 431/512
512/512 - 0s - loss: 2.6141e-10 - val_loss: 2.5696e-12
Epoch 432/512
512/512 - 0s - loss: 2.6180e-10 - val_loss: 2.4897e-12
Epoch 433/512
512/512 - 0s - loss: 2.5754e-10 - val_loss: 2.4969e-12
Epoch 434/512
512/512 - 0s - loss: 2.5555e-10 - val_loss: 2.4612e-12
Epoch 435/512
512/512 - 0s - loss: 2.4772e-10 - val_loss: 2.4204e-12
Epoch 436/512
512/512 - 0s - loss: 2.4774e-10 - val_loss: 2.4283e-12
Epoch 437/512
512/512 - 0s - loss: 2.4345e-10 - val_loss: 2.4029e-12
Epoch 438/512
512/512 - 0s - loss: 2.4260e-10 - val_loss: 2.3655e-12
Epoch 439/512
512/512 - 0s - loss: 2.3900e-10 - val_loss: 2.3694e-12
Epoch 440/512
512/512 - 0s - loss: 2.4283e-10 - val_loss: 2.3397e-12
Epoch 441/512
512/512 - 0s - loss: 2.4077e-10 - val_loss: 2.3316e-12
Epoch 442/512
512/512 - 0s - loss: 2.3611e-10 - val_loss: 2.2766e-12
Epoch 443/512
512/512 - 0s - loss: 2.2823e-10 - val_loss: 2.1778e-12
Epoch 444/512
512/512 - 0s - loss: 2.2015e-10 - val_loss: 2.1289e-12
Epoch 445/512
512/512 - 0s - loss: 2.2192e-10 - val_loss: 2.1560e-12
Epoch 446/512
512/512 - 0s - loss: 2.2512e-10 - val_loss: 2.2035e-12
Epoch 447/512
512/512 - 0s - loss: 2.2476e-10 - val_loss: 2.1658e-12
Epoch 448/512
512/512 - 0s - loss: 2.1271e-10 - val_loss: 2.0156e-12
Epoch 449/512
512/512 - 0s - loss: 2.0055e-10 - val_loss: 1.9031e-12
Epoch 450/512
512/512 - 0s - loss: 1.9312e-10 - val_loss: 1.8245e-12
Epoch 451/512
512/512 - 0s - loss: 1.8794e-10 - val_loss: 1.8251e-12
Epoch 452/512
512/512 - 0s - loss: 1.8590e-10 - val_loss: 1.8348e-12
Epoch 453/512
512/512 - 0s - loss: 1.8846e-10 - val_loss: 1.8565e-12
Epoch 454/512
512/512 - 0s - loss: 1.9107e-10 - val_loss: 1.9373e-12
Epoch 455/512
512/512 - 0s - loss: 1.9845e-10 - val_loss: 1.9764e-12
Epoch 456/512
512/512 - 0s - loss: 2.0151e-10 - val_loss: 1.9420e-12
Epoch 457/512
512/512 - 0s - loss: 1.9791e-10 - val_loss: 1.9123e-12
Epoch 458/512
512/512 - 0s - loss: 1.9511e-10 - val_loss: 1.8983e-12
Epoch 459/512
512/512 - 0s - loss: 1.9302e-10 - val_loss: 1.8840e-12
Epoch 460/512
512/512 - 0s - loss: 1.8942e-10 - val_loss: 1.8543e-12
Epoch 461/512
512/512 - 0s - loss: 1.8437e-10 - val_loss: 1.7421e-12
Epoch 462/512
512/512 - 0s - loss: 1.7823e-10 - val_loss: 1.7286e-12
Epoch 463/512
512/512 - 0s - loss: 1.7524e-10 - val_loss: 1.7587e-12
Epoch 464/512
512/512 - 0s - loss: 1.7836e-10 - val_loss: 1.7228e-12
Epoch 465/512
512/512 - 0s - loss: 1.7257e-10 - val_loss: 1.6833e-12
Epoch 466/512
512/512 - 0s - loss: 1.6965e-10 - val_loss: 1.6645e-12
Epoch 467/512
512/512 - 0s - loss: 1.7355e-10 - val_loss: 1.7290e-12
Epoch 468/512
512/512 - 0s - loss: 1.7783e-10 - val_loss: 1.7472e-12
Epoch 469/512
512/512 - 0s - loss: 1.7700e-10 - val_loss: 1.6722e-12
Epoch 470/512
512/512 - 0s - loss: 1.6660e-10 - val_loss: 1.6374e-12
Epoch 471/512
512/512 - 0s - loss: 1.6653e-10 - val_loss: 1.5675e-12
Epoch 472/512
512/512 - 0s - loss: 1.6129e-10 - val_loss: 1.6286e-12
Epoch 473/512
512/512 - 0s - loss: 1.6529e-10 - val_loss: 1.6247e-12
Epoch 474/512
512/512 - 0s - loss: 1.6390e-10 - val_loss: 1.6259e-12
Epoch 475/512
512/512 - 0s - loss: 1.6840e-10 - val_loss: 1.6076e-12
Epoch 476/512
512/512 - 0s - loss: 1.6220e-10 - val_loss: 1.5804e-12
Epoch 477/512
512/512 - 0s - loss: 1.6250e-10 - val_loss: 1.5999e-12
Epoch 478/512
512/512 - 0s - loss: 1.6389e-10 - val_loss: 1.6083e-12
Epoch 479/512
512/512 - 0s - loss: 1.6190e-10 - val_loss: 1.5691e-12
Epoch 480/512
512/512 - 0s - loss: 1.5683e-10 - val_loss: 1.5045e-12
Epoch 481/512
512/512 - 0s - loss: 1.5289e-10 - val_loss: 1.4889e-12
Epoch 482/512
512/512 - 0s - loss: 1.5197e-10 - val_loss: 1.4695e-12
Epoch 483/512
512/512 - 0s - loss: 1.4625e-10 - val_loss: 1.4080e-12
Epoch 484/512
512/512 - 0s - loss: 1.4411e-10 - val_loss: 1.4678e-12
Epoch 485/512
512/512 - 0s - loss: 1.5286e-10 - val_loss: 1.4874e-12
Epoch 486/512
512/512 - 0s - loss: 1.5162e-10 - val_loss: 1.5063e-12
Epoch 487/512
512/512 - 0s - loss: 1.5087e-10 - val_loss: 1.4679e-12
Epoch 488/512
512/512 - 0s - loss: 1.4790e-10 - val_loss: 1.4127e-12
Epoch 489/512
512/512 - 0s - loss: 1.4109e-10 - val_loss: 1.3788e-12
Epoch 490/512
512/512 - 0s - loss: 1.4162e-10 - val_loss: 1.4022e-12
Epoch 491/512
512/512 - 0s - loss: 1.4247e-10 - val_loss: 1.3649e-12
Epoch 492/512
512/512 - 0s - loss: 1.4073e-10 - val_loss: 1.3963e-12
Epoch 493/512
512/512 - 0s - loss: 1.4302e-10 - val_loss: 1.4082e-12
Epoch 494/512
512/512 - 0s - loss: 1.4313e-10 - val_loss: 1.3397e-12
Epoch 495/512
512/512 - 0s - loss: 1.3556e-10 - val_loss: 1.3576e-12
Epoch 496/512
512/512 - 0s - loss: 1.3574e-10 - val_loss: 1.3109e-12
Epoch 497/512
512/512 - 0s - loss: 1.3453e-10 - val_loss: 1.2736e-12
Epoch 498/512
512/512 - 0s - loss: 1.3128e-10 - val_loss: 1.3503e-12
Epoch 499/512
512/512 - 0s - loss: 1.3387e-10 - val_loss: 1.2522e-12
Epoch 500/512
512/512 - 0s - loss: 1.2549e-10 - val_loss: 1.2328e-12
Epoch 501/512
512/512 - 0s - loss: 1.2557e-10 - val_loss: 1.2006e-12
Epoch 502/512
512/512 - 0s - loss: 1.2533e-10 - val_loss: 1.2400e-12
Epoch 503/512
512/512 - 0s - loss: 1.2304e-10 - val_loss: 1.1875e-12
Epoch 504/512
512/512 - 0s - loss: 1.2233e-10 - val_loss: 1.1639e-12
Epoch 505/512
512/512 - 0s - loss: 1.1701e-10 - val_loss: 1.1520e-12
Epoch 506/512
512/512 - 0s - loss: 1.1717e-10 - val_loss: 1.1568e-12
Epoch 507/512
512/512 - 0s - loss: 1.1986e-10 - val_loss: 1.2177e-12
Epoch 508/512
512/512 - 0s - loss: 1.2406e-10 - val_loss: 1.2079e-12
Epoch 509/512
512/512 - 0s - loss: 1.2418e-10 - val_loss: 1.2448e-12
Epoch 510/512
512/512 - 0s - loss: 1.2517e-10 - val_loss: 1.2378e-12
Epoch 511/512
512/512 - 0s - loss: 1.2496e-10 - val_loss: 1.1983e-12
Epoch 512/512
512/512 - 0s - loss: 1.2468e-10 - val_loss: 1.2149e-12
2024-04-11 17:33:24.202598: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1429e-10 - val_loss: 3.8104e-10
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1913e-10 - val_loss: 6.4610e-10
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5481e-10 - val_loss: 5.5212e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8779e-10 - val_loss: 3.5204e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1311e-10 - val_loss: 2.4396e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2998e-10 - val_loss: 2.0885e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1371e-10 - val_loss: 2.1966e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3608e-10 - val_loss: 2.5519e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8062e-10 - val_loss: 3.0341e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2691e-10 - val_loss: 3.3124e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4049e-10 - val_loss: 3.2886e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3159e-10 - val_loss: 3.1093e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0849e-10 - val_loss: 2.8549e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8432e-10 - val_loss: 2.6730e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6566e-10 - val_loss: 2.4312e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4938e-10 - val_loss: 2.4722e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5544e-10 - val_loss: 2.5159e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5609e-10 - val_loss: 2.5319e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6342e-10 - val_loss: 2.7079e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7712e-10 - val_loss: 2.6975e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7487e-10 - val_loss: 2.6092e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6647e-10 - val_loss: 2.5354e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5600e-10 - val_loss: 2.4032e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4319e-10 - val_loss: 2.3331e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3581e-10 - val_loss: 2.3139e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3858e-10 - val_loss: 2.3735e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3941e-10 - val_loss: 2.3500e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4214e-10 - val_loss: 2.4117e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4279e-10 - val_loss: 2.2904e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3366e-10 - val_loss: 2.2708e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2926e-10 - val_loss: 2.2165e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2440e-10 - val_loss: 2.2055e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2457e-10 - val_loss: 2.2304e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2818e-10 - val_loss: 2.2608e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3258e-10 - val_loss: 2.3015e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3077e-10 - val_loss: 2.1590e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1604e-10 - val_loss: 2.0652e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1087e-10 - val_loss: 2.0669e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1385e-10 - val_loss: 2.0871e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1307e-10 - val_loss: 2.0751e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1342e-10 - val_loss: 2.0778e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1483e-10 - val_loss: 2.0966e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1436e-10 - val_loss: 2.0956e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1468e-10 - val_loss: 2.0892e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1296e-10 - val_loss: 2.0341e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0259e-10 - val_loss: 1.9582e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9907e-10 - val_loss: 1.9354e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9504e-10 - val_loss: 1.8891e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9024e-10 - val_loss: 1.8344e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8460e-10 - val_loss: 1.8094e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8195e-10 - val_loss: 1.7933e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8304e-10 - val_loss: 1.8375e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8979e-10 - val_loss: 1.8693e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9082e-10 - val_loss: 1.8196e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8373e-10 - val_loss: 1.7641e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8012e-10 - val_loss: 1.7533e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7813e-10 - val_loss: 1.7115e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7343e-10 - val_loss: 1.6698e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7003e-10 - val_loss: 1.6750e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7182e-10 - val_loss: 1.7221e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7703e-10 - val_loss: 1.7376e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7498e-10 - val_loss: 1.6591e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6776e-10 - val_loss: 1.5822e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6082e-10 - val_loss: 1.5664e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5990e-10 - val_loss: 1.5921e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6272e-10 - val_loss: 1.6532e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7113e-10 - val_loss: 1.6595e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6805e-10 - val_loss: 1.5925e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6187e-10 - val_loss: 1.5825e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6212e-10 - val_loss: 1.6091e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6304e-10 - val_loss: 1.5685e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5659e-10 - val_loss: 1.5290e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5868e-10 - val_loss: 1.5722e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5942e-10 - val_loss: 1.5670e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6039e-10 - val_loss: 1.5742e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5676e-10 - val_loss: 1.5130e-10
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5301e-10 - val_loss: 1.4806e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5098e-10 - val_loss: 1.4589e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4830e-10 - val_loss: 1.4742e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4734e-10 - val_loss: 1.4231e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4725e-10 - val_loss: 1.4305e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4707e-10 - val_loss: 1.4889e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4848e-10 - val_loss: 1.4365e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4450e-10 - val_loss: 1.3957e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4276e-10 - val_loss: 1.3884e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3996e-10 - val_loss: 1.3881e-10
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4096e-10 - val_loss: 1.3654e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4025e-10 - val_loss: 1.3692e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3610e-10 - val_loss: 1.2677e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2905e-10 - val_loss: 1.3017e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3219e-10 - val_loss: 1.2963e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3485e-10 - val_loss: 1.3479e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3746e-10 - val_loss: 1.3596e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3779e-10 - val_loss: 1.3448e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3706e-10 - val_loss: 1.3500e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3529e-10 - val_loss: 1.3135e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3201e-10 - val_loss: 1.2421e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2562e-10 - val_loss: 1.2511e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2626e-10 - val_loss: 1.2145e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2560e-10 - val_loss: 1.2152e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2438e-10 - val_loss: 1.2582e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3090e-10 - val_loss: 1.2691e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2935e-10 - val_loss: 1.2846e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2752e-10 - val_loss: 1.2008e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2032e-10 - val_loss: 1.1735e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1994e-10 - val_loss: 1.1702e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1799e-10 - val_loss: 1.1673e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1961e-10 - val_loss: 1.1677e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2034e-10 - val_loss: 1.1784e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1940e-10 - val_loss: 1.1612e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1857e-10 - val_loss: 1.1674e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1611e-10 - val_loss: 1.1243e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1502e-10 - val_loss: 1.1402e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1333e-10 - val_loss: 1.1162e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1378e-10 - val_loss: 1.0997e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1012e-10 - val_loss: 1.0893e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1265e-10 - val_loss: 1.0962e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1289e-10 - val_loss: 1.1324e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1378e-10 - val_loss: 1.0716e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0544e-10 - val_loss: 9.6405e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6471e-11 - val_loss: 9.6879e-11
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0057e-10 - val_loss: 1.0274e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0343e-10 - val_loss: 1.0041e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0279e-10 - val_loss: 1.0264e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0477e-10 - val_loss: 1.0352e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0468e-10 - val_loss: 1.0334e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0456e-10 - val_loss: 1.0074e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0239e-10 - val_loss: 9.9019e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0027e-10 - val_loss: 9.8443e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0130e-10 - val_loss: 1.0059e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0123e-10 - val_loss: 9.9458e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0101e-10 - val_loss: 9.9001e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0017e-10 - val_loss: 9.7193e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9988e-11 - val_loss: 9.9167e-11
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8726e-11 - val_loss: 9.5933e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7462e-11 - val_loss: 9.7203e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7169e-11 - val_loss: 9.6685e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9894e-11 - val_loss: 9.7285e-11
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.5431e-11 - val_loss: 8.9606e-11
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9451e-11 - val_loss: 8.6817e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7440e-11 - val_loss: 8.7380e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8720e-11 - val_loss: 9.0274e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5758e-11 - val_loss: 9.9034e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7518e-11 - val_loss: 9.2705e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4805e-11 - val_loss: 9.4989e-11
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3333e-11 - val_loss: 8.6556e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6365e-11 - val_loss: 8.3912e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5822e-11 - val_loss: 8.4753e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6826e-11 - val_loss: 8.8044e-11
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7256e-11 - val_loss: 8.3276e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5564e-11 - val_loss: 8.5903e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8031e-11 - val_loss: 8.7248e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7693e-11 - val_loss: 8.4087e-11
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.4146e-11 - val_loss: 8.0675e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1339e-11 - val_loss: 8.1167e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3601e-11 - val_loss: 8.3370e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3046e-11 - val_loss: 7.7706e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0135e-11 - val_loss: 8.1263e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2586e-11 - val_loss: 8.0945e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9918e-11 - val_loss: 7.8909e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0573e-11 - val_loss: 8.1266e-11
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1012e-11 - val_loss: 7.6875e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9048e-11 - val_loss: 7.9884e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2969e-11 - val_loss: 8.4004e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1176e-11 - val_loss: 7.8899e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0826e-11 - val_loss: 8.2244e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4218e-11 - val_loss: 7.8095e-11
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7241e-11 - val_loss: 7.6333e-11
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7881e-11 - val_loss: 7.4584e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4970e-11 - val_loss: 7.1598e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2356e-11 - val_loss: 6.8024e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8904e-11 - val_loss: 6.9266e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0882e-11 - val_loss: 7.1264e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3322e-11 - val_loss: 7.5156e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7095e-11 - val_loss: 7.5848e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6050e-11 - val_loss: 7.4218e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6638e-11 - val_loss: 7.8440e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8665e-11 - val_loss: 7.5791e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6442e-11 - val_loss: 7.4145e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4831e-11 - val_loss: 7.3443e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4225e-11 - val_loss: 7.1613e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3697e-11 - val_loss: 7.3422e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2792e-11 - val_loss: 7.1562e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2438e-11 - val_loss: 7.0348e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1690e-11 - val_loss: 7.2652e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1963e-11 - val_loss: 7.0360e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1984e-11 - val_loss: 7.0989e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2292e-11 - val_loss: 7.1566e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2223e-11 - val_loss: 7.1534e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2193e-11 - val_loss: 7.1730e-11
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1978e-11 - val_loss: 6.7604e-11
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5403e-11 - val_loss: 6.1218e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3732e-11 - val_loss: 6.7237e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0457e-11 - val_loss: 7.1306e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9048e-11 - val_loss: 6.7933e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0725e-11 - val_loss: 7.1009e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2328e-11 - val_loss: 7.0828e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8448e-11 - val_loss: 6.6979e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8067e-11 - val_loss: 6.7795e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8685e-11 - val_loss: 6.4777e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4462e-11 - val_loss: 6.0917e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2310e-11 - val_loss: 6.5146e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8143e-11 - val_loss: 7.0550e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7930e-11 - val_loss: 6.4687e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6328e-11 - val_loss: 6.6557e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8823e-11 - val_loss: 7.0116e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7208e-11 - val_loss: 6.1048e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0591e-11 - val_loss: 5.8472e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6919e-11 - val_loss: 5.5447e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6390e-11 - val_loss: 5.6116e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7992e-11 - val_loss: 5.8592e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9607e-11 - val_loss: 6.0503e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0327e-11 - val_loss: 5.9939e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1400e-11 - val_loss: 6.0191e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0738e-11 - val_loss: 5.8835e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7316e-11 - val_loss: 5.4612e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5400e-11 - val_loss: 5.6693e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8667e-11 - val_loss: 6.0180e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1684e-11 - val_loss: 6.2223e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5379e-11 - val_loss: 6.7481e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6746e-11 - val_loss: 6.2360e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2210e-11 - val_loss: 5.9193e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.7463e-11 - val_loss: 5.2221e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3458e-11 - val_loss: 5.6177e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8340e-11 - val_loss: 5.8393e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0346e-11 - val_loss: 6.2814e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4744e-11 - val_loss: 6.2143e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1078e-11 - val_loss: 5.7128e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7486e-11 - val_loss: 5.6715e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5925e-11 - val_loss: 5.2968e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2308e-11 - val_loss: 4.9216e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1470e-11 - val_loss: 5.5012e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7379e-11 - val_loss: 5.8857e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7221e-11 - val_loss: 5.2853e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1525e-11 - val_loss: 5.1633e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3913e-11 - val_loss: 5.4826e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6452e-11 - val_loss: 5.4731e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4343e-11 - val_loss: 5.1902e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1299e-11 - val_loss: 5.1535e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4329e-11 - val_loss: 5.6138e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8233e-11 - val_loss: 5.9534e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7650e-11 - val_loss: 5.6124e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8138e-11 - val_loss: 5.8329e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8961e-11 - val_loss: 5.7706e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7006e-11 - val_loss: 5.2328e-11
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1088e-11 - val_loss: 4.8191e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8442e-11 - val_loss: 4.9688e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0990e-11 - val_loss: 4.9368e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9532e-11 - val_loss: 4.8571e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8385e-11 - val_loss: 4.7447e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8385e-11 - val_loss: 4.9630e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0505e-11 - val_loss: 4.8389e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8124e-11 - val_loss: 4.7044e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7569e-11 - val_loss: 4.6755e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7847e-11 - val_loss: 4.8731e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1107e-11 - val_loss: 5.3858e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5566e-11 - val_loss: 5.3733e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4562e-11 - val_loss: 5.3285e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2803e-11 - val_loss: 5.2523e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5495e-11 - val_loss: 5.7745e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7337e-11 - val_loss: 5.2590e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1569e-11 - val_loss: 4.9494e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9749e-11 - val_loss: 4.7733e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9487e-11 - val_loss: 5.2029e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3971e-11 - val_loss: 5.2515e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1639e-11 - val_loss: 4.9002e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0041e-11 - val_loss: 4.8219e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7799e-11 - val_loss: 4.4743e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3754e-11 - val_loss: 4.5127e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6961e-11 - val_loss: 4.6951e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7241e-11 - val_loss: 4.7619e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8916e-11 - val_loss: 5.0507e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2570e-11 - val_loss: 5.4761e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4661e-11 - val_loss: 5.2003e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1429e-11 - val_loss: 4.8654e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8451e-11 - val_loss: 4.7314e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4437e-11 - val_loss: 4.0816e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9591e-11 - val_loss: 3.8558e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0492e-11 - val_loss: 4.3056e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5443e-11 - val_loss: 4.6869e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6503e-11 - val_loss: 4.2132e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1216e-11 - val_loss: 3.8341e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8102e-11 - val_loss: 3.7413e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0039e-11 - val_loss: 4.2722e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3720e-11 - val_loss: 4.4178e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4202e-11 - val_loss: 4.0866e-11
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0149e-11 - val_loss: 3.7318e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7345e-11 - val_loss: 3.6431e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6008e-11 - val_loss: 3.3855e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3115e-11 - val_loss: 3.1817e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1845e-11 - val_loss: 3.3079e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3997e-11 - val_loss: 3.4889e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6339e-11 - val_loss: 3.8238e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8870e-11 - val_loss: 3.9498e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1172e-11 - val_loss: 4.3426e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5303e-11 - val_loss: 4.6730e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5598e-11 - val_loss: 4.1184e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9697e-11 - val_loss: 3.6653e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6811e-11 - val_loss: 3.6069e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6316e-11 - val_loss: 3.6403e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5781e-11 - val_loss: 3.6119e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7558e-11 - val_loss: 3.8720e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8865e-11 - val_loss: 3.7624e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7452e-11 - val_loss: 3.7175e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8081e-11 - val_loss: 3.7224e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8990e-11 - val_loss: 4.0191e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1455e-11 - val_loss: 4.2393e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1787e-11 - val_loss: 3.9635e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9066e-11 - val_loss: 3.7620e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8278e-11 - val_loss: 3.7750e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7240e-11 - val_loss: 3.5987e-11
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4547e-11 - val_loss: 3.1736e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1717e-11 - val_loss: 3.2511e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4602e-11 - val_loss: 3.6937e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7892e-11 - val_loss: 3.6868e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6224e-11 - val_loss: 3.5440e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6207e-11 - val_loss: 3.4000e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2498e-11 - val_loss: 3.1113e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1121e-11 - val_loss: 3.0850e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1575e-11 - val_loss: 3.3296e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4175e-11 - val_loss: 3.4887e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5755e-11 - val_loss: 3.5942e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7121e-11 - val_loss: 3.9213e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1373e-11 - val_loss: 4.3117e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3758e-11 - val_loss: 4.0212e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8800e-11 - val_loss: 3.6631e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6242e-11 - val_loss: 3.5508e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5833e-11 - val_loss: 3.5314e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3139e-11 - val_loss: 3.0312e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0075e-11 - val_loss: 2.8810e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9066e-11 - val_loss: 2.9181e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0221e-11 - val_loss: 3.2219e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3115e-11 - val_loss: 3.3910e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5008e-11 - val_loss: 3.5880e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5950e-11 - val_loss: 3.2737e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2135e-11 - val_loss: 3.0519e-11
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9847e-11 - val_loss: 2.8245e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8306e-11 - val_loss: 2.8769e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0955e-11 - val_loss: 3.3846e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4539e-11 - val_loss: 3.5235e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5824e-11 - val_loss: 3.6569e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8552e-11 - val_loss: 4.1519e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1918e-11 - val_loss: 3.8584e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7142e-11 - val_loss: 3.4108e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4233e-11 - val_loss: 3.3088e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2942e-11 - val_loss: 3.1389e-11
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0061e-11 - val_loss: 2.7995e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6905e-11 - val_loss: 2.5819e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5715e-11 - val_loss: 2.5549e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6451e-11 - val_loss: 2.7700e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8941e-11 - val_loss: 3.1085e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3545e-11 - val_loss: 3.6364e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7434e-11 - val_loss: 3.7998e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7798e-11 - val_loss: 3.5484e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3691e-11 - val_loss: 3.0934e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0307e-11 - val_loss: 2.9021e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9026e-11 - val_loss: 2.9239e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1019e-11 - val_loss: 3.3636e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5095e-11 - val_loss: 3.6062e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6722e-11 - val_loss: 3.5707e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3860e-11 - val_loss: 3.0690e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9747e-11 - val_loss: 2.8133e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8473e-11 - val_loss: 2.8728e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7835e-11 - val_loss: 2.7541e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9516e-11 - val_loss: 3.2125e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4182e-11 - val_loss: 3.6993e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8467e-11 - val_loss: 3.8666e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9586e-11 - val_loss: 4.0513e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8854e-11 - val_loss: 3.6283e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5948e-11 - val_loss: 3.4373e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4367e-11 - val_loss: 3.4792e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4754e-11 - val_loss: 3.3333e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1913e-11 - val_loss: 3.0474e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9745e-11 - val_loss: 2.7942e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7962e-11 - val_loss: 2.8107e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7543e-11 - val_loss: 2.6563e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6944e-11 - val_loss: 2.9186e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1166e-11 - val_loss: 3.3246e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4204e-11 - val_loss: 3.3798e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2492e-11 - val_loss: 3.1013e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0031e-11 - val_loss: 2.8174e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8253e-11 - val_loss: 2.7748e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7281e-11 - val_loss: 2.6023e-11
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4263e-11 - val_loss: 2.1636e-11
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1461e-11 - val_loss: 2.1049e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1238e-11 - val_loss: 2.2171e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3224e-11 - val_loss: 2.5218e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6523e-11 - val_loss: 2.7728e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8769e-11 - val_loss: 2.8907e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8963e-11 - val_loss: 2.9946e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0939e-11 - val_loss: 3.2066e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3857e-11 - val_loss: 3.5520e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5452e-11 - val_loss: 3.4887e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4732e-11 - val_loss: 3.3348e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1831e-11 - val_loss: 2.8499e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7902e-11 - val_loss: 2.6681e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6194e-11 - val_loss: 2.5004e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5063e-11 - val_loss: 2.5016e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4037e-11 - val_loss: 2.1756e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0921e-11 - val_loss: 2.0163e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0146e-11 - val_loss: 1.9474e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9527e-11 - val_loss: 1.9852e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0406e-11 - val_loss: 2.0233e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1640e-11 - val_loss: 2.3612e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4858e-11 - val_loss: 2.6100e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6573e-11 - val_loss: 2.6412e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7020e-11 - val_loss: 2.7422e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7633e-11 - val_loss: 2.7688e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8614e-11 - val_loss: 3.1232e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2397e-11 - val_loss: 3.3346e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2743e-11 - val_loss: 2.9105e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8316e-11 - val_loss: 2.6787e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6425e-11 - val_loss: 2.5862e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5282e-11 - val_loss: 2.4638e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5432e-11 - val_loss: 2.6219e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5293e-11 - val_loss: 2.3021e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2708e-11 - val_loss: 2.2387e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2322e-11 - val_loss: 2.0724e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0718e-11 - val_loss: 2.0152e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0238e-11 - val_loss: 2.1163e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2807e-11 - val_loss: 2.4654e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4950e-11 - val_loss: 2.4994e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6291e-11 - val_loss: 2.7171e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7640e-11 - val_loss: 2.7787e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6764e-11 - val_loss: 2.3860e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3071e-11 - val_loss: 2.1623e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0675e-11 - val_loss: 1.9606e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0014e-11 - val_loss: 2.0387e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1396e-11 - val_loss: 2.3485e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4805e-11 - val_loss: 2.5713e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5842e-11 - val_loss: 2.6266e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7000e-11 - val_loss: 2.7084e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6173e-11 - val_loss: 2.2802e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2467e-11 - val_loss: 2.1478e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0926e-11 - val_loss: 2.0090e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0607e-11 - val_loss: 2.0879e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0820e-11 - val_loss: 2.0187e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1407e-11 - val_loss: 2.3458e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4620e-11 - val_loss: 2.5500e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5510e-11 - val_loss: 2.5182e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5764e-11 - val_loss: 2.5043e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4843e-11 - val_loss: 2.2842e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2311e-11 - val_loss: 2.0927e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0229e-11 - val_loss: 1.9245e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9938e-11 - val_loss: 1.9415e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9310e-11 - val_loss: 1.9455e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9879e-11 - val_loss: 1.9341e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9099e-11 - val_loss: 1.7383e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6601e-11 - val_loss: 1.5917e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5452e-11 - val_loss: 1.4607e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4594e-11 - val_loss: 1.4182e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4316e-11 - val_loss: 1.4416e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5551e-11 - val_loss: 1.7467e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8350e-11 - val_loss: 1.9361e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9231e-11 - val_loss: 1.9129e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9797e-11 - val_loss: 1.9863e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0140e-11 - val_loss: 2.0400e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0648e-11 - val_loss: 2.0485e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9967e-11 - val_loss: 1.8794e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9524e-11 - val_loss: 2.0939e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2364e-11 - val_loss: 2.3392e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2977e-11 - val_loss: 2.0597e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9940e-11 - val_loss: 1.8583e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8506e-11 - val_loss: 1.8439e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9380e-11 - val_loss: 2.0021e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0307e-11 - val_loss: 2.0184e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0252e-11 - val_loss: 1.9312e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9443e-11 - val_loss: 1.8631e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8304e-11 - val_loss: 1.7701e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6831e-11 - val_loss: 1.5953e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5756e-11 - val_loss: 1.4899e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4954e-11 - val_loss: 1.4597e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4364e-11 - val_loss: 1.4220e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4301e-11 - val_loss: 1.4568e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4848e-11 - val_loss: 1.4867e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5938e-11 - val_loss: 1.7659e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8553e-11 - val_loss: 1.9734e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0315e-11 - val_loss: 2.0094e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9824e-11 - val_loss: 1.9466e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9592e-11 - val_loss: 1.8760e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8758e-11 - val_loss: 1.7955e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8278e-11 - val_loss: 1.9216e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0587e-11 - val_loss: 2.2281e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2998e-11 - val_loss: 2.3636e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4374e-11 - val_loss: 2.5342e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5289e-11 - val_loss: 2.3358e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2702e-11 - val_loss: 2.1165e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0379e-11 - val_loss: 1.9402e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9515e-11 - val_loss: 1.8492e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8487e-11 - val_loss: 1.8387e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8620e-11 - val_loss: 1.9062e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9581e-11 - val_loss: 1.9410e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0754e-11 - val_loss: 2.2310e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2344e-11 - val_loss: 2.3099e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4400e-11 - val_loss: 2.5020e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4344e-11 - val_loss: 2.1922e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1165e-11 - val_loss: 1.9823e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9824e-11 - val_loss: 1.9486e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9506e-11 - val_loss: 1.8315e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7893e-11 - val_loss: 1.7271e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7618e-11 - val_loss: 1.7413e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6278e-11 - val_loss: 1.4461e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4272e-11 - val_loss: 1.3947e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4276e-11 - val_loss: 1.4151e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4172e-11 - val_loss: 1.3792e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3517e-11 - val_loss: 1.3023e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3360e-11 - val_loss: 1.3929e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3854e-11 - val_loss: 1.3719e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3900e-11 - val_loss: 1.3687e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5152e-11 - val_loss: 1.7428e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7956e-11 - val_loss: 1.7999e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8861e-11 - val_loss: 1.9182e-11
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 1.5007 - val_loss: 0.0725
Epoch 2/512
512/512 - 0s - loss: 0.0441 - val_loss: 0.0370
Epoch 3/512
512/512 - 0s - loss: 0.0314 - val_loss: 0.0106
Epoch 4/512
512/512 - 0s - loss: 0.0204 - val_loss: 0.0054
Epoch 5/512
512/512 - 0s - loss: 0.0136 - val_loss: 0.0040
Epoch 6/512
512/512 - 0s - loss: 0.0095 - val_loss: 0.0025
Epoch 7/512
512/512 - 0s - loss: 0.0066 - val_loss: 0.0015
Epoch 8/512
512/512 - 0s - loss: 0.0045 - val_loss: 8.1970e-04
Epoch 9/512
512/512 - 0s - loss: 0.0030 - val_loss: 4.9318e-04
Epoch 10/512
512/512 - 0s - loss: 0.0022 - val_loss: 3.2670e-04
Epoch 11/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.8027e-04
Epoch 12/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.4626e-04
Epoch 13/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.4259e-04
Epoch 14/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.7093e-04
Epoch 15/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.8152e-04
Epoch 16/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.0342e-04
Epoch 17/512
512/512 - 0s - loss: 0.0012 - val_loss: 2.7624e-04
Epoch 18/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.6732e-04
Epoch 19/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.7212e-04
Epoch 20/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.8967e-04
Epoch 21/512
512/512 - 0s - loss: 0.0011 - val_loss: 2.8927e-04
Epoch 22/512
512/512 - 0s - loss: 0.0010 - val_loss: 2.8245e-04
Epoch 23/512
512/512 - 0s - loss: 9.8104e-04 - val_loss: 2.7432e-04
Epoch 24/512
512/512 - 0s - loss: 9.1362e-04 - val_loss: 2.6502e-04
Epoch 25/512
512/512 - 0s - loss: 9.0072e-04 - val_loss: 2.6663e-04
Epoch 26/512
512/512 - 0s - loss: 8.4239e-04 - val_loss: 2.5391e-04
Epoch 27/512
512/512 - 0s - loss: 7.5764e-04 - val_loss: 2.3997e-04
Epoch 28/512
512/512 - 0s - loss: 6.8039e-04 - val_loss: 2.2774e-04
Epoch 29/512
512/512 - 0s - loss: 6.4250e-04 - val_loss: 2.1552e-04
Epoch 30/512
512/512 - 0s - loss: 5.5935e-04 - val_loss: 1.9159e-04
Epoch 31/512
512/512 - 0s - loss: 4.5971e-04 - val_loss: 1.7188e-04
Epoch 32/512
512/512 - 0s - loss: 3.9508e-04 - val_loss: 1.4775e-04
Epoch 33/512
512/512 - 0s - loss: 3.5276e-04 - val_loss: 1.2702e-04
Epoch 34/512
512/512 - 0s - loss: 3.0118e-04 - val_loss: 1.0028e-04
Epoch 35/512
512/512 - 0s - loss: 2.4669e-04 - val_loss: 8.5111e-05
Epoch 36/512
512/512 - 0s - loss: 1.9644e-04 - val_loss: 7.5832e-05
Epoch 37/512
512/512 - 0s - loss: 1.7557e-04 - val_loss: 6.3155e-05
Epoch 38/512
512/512 - 0s - loss: 1.5881e-04 - val_loss: 5.5813e-05
Epoch 39/512
512/512 - 0s - loss: 1.1701e-04 - val_loss: 5.1492e-05
Epoch 40/512
512/512 - 0s - loss: 9.4155e-05 - val_loss: 4.3358e-05
Epoch 41/512
512/512 - 0s - loss: 9.0676e-05 - val_loss: 3.3940e-05
Epoch 42/512
512/512 - 0s - loss: 7.7933e-05 - val_loss: 3.1542e-05
Epoch 43/512
512/512 - 0s - loss: 5.1715e-05 - val_loss: 2.8549e-05
Epoch 44/512
512/512 - 0s - loss: 4.6866e-05 - val_loss: 2.2336e-05
Epoch 45/512
512/512 - 0s - loss: 4.8380e-05 - val_loss: 1.7351e-05
Epoch 46/512
512/512 - 0s - loss: 3.3812e-05 - val_loss: 1.4610e-05
Epoch 47/512
512/512 - 0s - loss: 2.4512e-05 - val_loss: 1.3942e-05
Epoch 48/512
512/512 - 0s - loss: 2.4222e-05 - val_loss: 1.0909e-05
Epoch 49/512
512/512 - 0s - loss: 2.3046e-05 - val_loss: 7.4677e-06
Epoch 50/512
512/512 - 0s - loss: 1.5223e-05 - val_loss: 7.1564e-06
Epoch 51/512
512/512 - 0s - loss: 1.2297e-05 - val_loss: 7.1268e-06
Epoch 52/512
512/512 - 0s - loss: 1.3000e-05 - val_loss: 7.3719e-06
Epoch 53/512
512/512 - 0s - loss: 9.7837e-06 - val_loss: 6.4294e-06
Epoch 54/512
512/512 - 0s - loss: 7.6224e-06 - val_loss: 7.4746e-06
Epoch 55/512
512/512 - 0s - loss: 6.8417e-06 - val_loss: 9.2772e-06
Epoch 56/512
512/512 - 0s - loss: 5.9844e-06 - val_loss: 9.6839e-06
Epoch 57/512
512/512 - 0s - loss: 5.0406e-06 - val_loss: 3.2136e-05
Epoch 58/512
512/512 - 0s - loss: 1.3675e-05 - val_loss: 1.2641e-05
Epoch 59/512
512/512 - 0s - loss: 2.5283e-06 - val_loss: 1.1625e-05
Epoch 60/512
512/512 - 0s - loss: 1.8668e-06 - val_loss: 1.0435e-05
Epoch 61/512
512/512 - 0s - loss: 1.5704e-06 - val_loss: 1.0747e-05
Epoch 62/512
512/512 - 0s - loss: 1.2822e-06 - val_loss: 1.0169e-05
Epoch 63/512
512/512 - 0s - loss: 1.2689e-06 - val_loss: 8.6945e-06
Epoch 64/512
512/512 - 0s - loss: 2.5921e-06 - val_loss: 7.2666e-06
Epoch 65/512
512/512 - 0s - loss: 1.1153e-06 - val_loss: 7.2278e-06
Epoch 66/512
512/512 - 0s - loss: 1.0753e-06 - val_loss: 6.5991e-06
Epoch 67/512
512/512 - 0s - loss: 8.9914e-07 - val_loss: 6.5967e-06
Epoch 68/512
512/512 - 0s - loss: 4.0207e-06 - val_loss: 3.7043e-06
Epoch 69/512
512/512 - 0s - loss: 6.8643e-06 - val_loss: 3.3201e-06
Epoch 70/512
512/512 - 0s - loss: 3.1581e-07 - val_loss: 4.3533e-06
Epoch 71/512
512/512 - 0s - loss: 2.4264e-07 - val_loss: 4.5575e-06
Epoch 72/512
512/512 - 0s - loss: 2.0807e-07 - val_loss: 4.3755e-06
Epoch 73/512
512/512 - 0s - loss: 1.8328e-07 - val_loss: 4.1141e-06
Epoch 74/512
512/512 - 0s - loss: 1.5841e-07 - val_loss: 3.6583e-06
Epoch 75/512
512/512 - 0s - loss: 2.0472e-07 - val_loss: 3.3741e-06
Epoch 76/512
512/512 - 0s - loss: 4.5456e-07 - val_loss: 2.5191e-06
Epoch 77/512
512/512 - 0s - loss: 3.3517e-07 - val_loss: 1.6072e-06
Epoch 78/512
512/512 - 0s - loss: 4.1289e-06 - val_loss: 9.4323e-07
Epoch 79/512
512/512 - 0s - loss: 7.1471e-07 - val_loss: 8.0875e-07
Epoch 80/512
512/512 - 0s - loss: 5.0521e-07 - val_loss: 6.9751e-07
Epoch 81/512
512/512 - 0s - loss: 3.8413e-07 - val_loss: 6.0314e-07
Epoch 82/512
512/512 - 0s - loss: 5.9000e-07 - val_loss: 5.2310e-07
Epoch 83/512
512/512 - 0s - loss: 1.0823e-06 - val_loss: 5.5458e-07
Epoch 84/512
512/512 - 0s - loss: 1.9599e-07 - val_loss: 5.5124e-07
Epoch 85/512
512/512 - 0s - loss: 1.6375e-07 - val_loss: 4.8813e-07
Epoch 86/512
512/512 - 0s - loss: 1.3811e-07 - val_loss: 4.3955e-07
Epoch 87/512
512/512 - 0s - loss: 1.1766e-07 - val_loss: 3.8694e-07
Epoch 88/512
512/512 - 0s - loss: 1.0792e-07 - val_loss: 3.1378e-07
Epoch 89/512
512/512 - 0s - loss: 4.3464e-07 - val_loss: 2.5391e-07
Epoch 90/512
512/512 - 0s - loss: 7.2682e-08 - val_loss: 3.1353e-07
Epoch 91/512
512/512 - 0s - loss: 5.6293e-08 - val_loss: 2.9303e-07
Epoch 92/512
512/512 - 0s - loss: 4.8415e-08 - val_loss: 2.6711e-07
Epoch 93/512
512/512 - 0s - loss: 4.0681e-08 - val_loss: 2.5223e-07
Epoch 94/512
512/512 - 0s - loss: 3.4479e-08 - val_loss: 2.3657e-07
Epoch 95/512
512/512 - 0s - loss: 3.2315e-08 - val_loss: 2.7346e-07
Epoch 96/512
512/512 - 0s - loss: 2.7055e-07 - val_loss: 2.7411e-07
Epoch 97/512
512/512 - 0s - loss: 8.1312e-08 - val_loss: 1.8580e-07
Epoch 98/512
512/512 - 0s - loss: 3.1711e-08 - val_loss: 1.7310e-07
Epoch 99/512
512/512 - 0s - loss: 2.7382e-08 - val_loss: 1.6489e-07
Epoch 100/512
512/512 - 0s - loss: 2.3262e-08 - val_loss: 1.5626e-07
Epoch 101/512
512/512 - 0s - loss: 1.9649e-08 - val_loss: 1.4450e-07
Epoch 102/512
512/512 - 0s - loss: 1.6660e-08 - val_loss: 1.4183e-07
Epoch 103/512
512/512 - 0s - loss: 1.6226e-08 - val_loss: 1.1474e-07
Epoch 104/512
512/512 - 0s - loss: 2.8323e-08 - val_loss: 7.5926e-08
Epoch 105/512
512/512 - 0s - loss: 2.5968e-08 - val_loss: 1.2242e-07
Epoch 106/512
512/512 - 0s - loss: 7.4280e-09 - val_loss: 1.1343e-07
Epoch 107/512
512/512 - 0s - loss: 6.9928e-09 - val_loss: 1.1366e-07
Epoch 108/512
512/512 - 0s - loss: 6.4901e-09 - val_loss: 1.1043e-07
Epoch 109/512
512/512 - 0s - loss: 8.3821e-09 - val_loss: 6.8445e-08
Epoch 110/512
512/512 - 0s - loss: 2.7083e-08 - val_loss: 7.9636e-08
Epoch 111/512
512/512 - 0s - loss: 9.1299e-09 - val_loss: 1.0723e-07
Epoch 112/512
512/512 - 0s - loss: 5.3740e-09 - val_loss: 1.0422e-07
Epoch 113/512
512/512 - 0s - loss: 5.2969e-09 - val_loss: 1.0219e-07
Epoch 114/512
512/512 - 0s - loss: 5.1996e-09 - val_loss: 9.0562e-08
Epoch 115/512
512/512 - 0s - loss: 9.3936e-09 - val_loss: 7.4262e-08
Epoch 116/512
512/512 - 0s - loss: 1.0467e-08 - val_loss: 9.6050e-08
Epoch 117/512
512/512 - 0s - loss: 5.0655e-09 - val_loss: 9.4087e-08
Epoch 118/512
512/512 - 0s - loss: 5.5998e-09 - val_loss: 8.4335e-08
Epoch 119/512
512/512 - 0s - loss: 1.0750e-08 - val_loss: 9.1231e-08
Epoch 120/512
512/512 - 0s - loss: 5.7875e-09 - val_loss: 1.0570e-07
Epoch 121/512
512/512 - 0s - loss: 4.4444e-09 - val_loss: 1.0292e-07
Epoch 122/512
512/512 - 0s - loss: 4.0517e-09 - val_loss: 9.7044e-08
Epoch 123/512
512/512 - 0s - loss: 4.3056e-09 - val_loss: 8.2538e-08
Epoch 124/512
512/512 - 0s - loss: 1.1171e-08 - val_loss: 8.1398e-08
Epoch 125/512
512/512 - 0s - loss: 4.7806e-09 - val_loss: 9.2224e-08
Epoch 126/512
512/512 - 0s - loss: 3.7706e-09 - val_loss: 9.3501e-08
Epoch 127/512
512/512 - 0s - loss: 3.5278e-09 - val_loss: 9.1910e-08
Epoch 128/512
512/512 - 0s - loss: 3.3466e-09 - val_loss: 8.8025e-08
Epoch 129/512
512/512 - 0s - loss: 3.9296e-09 - val_loss: 6.8904e-08
Epoch 130/512
512/512 - 0s - loss: 1.0016e-08 - val_loss: 7.7226e-08
Epoch 131/512
512/512 - 0s - loss: 5.0334e-09 - val_loss: 8.7091e-08
Epoch 132/512
512/512 - 0s - loss: 3.3720e-09 - val_loss: 8.5611e-08
Epoch 133/512
512/512 - 0s - loss: 3.2627e-09 - val_loss: 8.4006e-08
Epoch 134/512
512/512 - 0s - loss: 3.8716e-09 - val_loss: 7.3811e-08
Epoch 135/512
512/512 - 0s - loss: 6.2722e-09 - val_loss: 7.3414e-08
Epoch 136/512
512/512 - 0s - loss: 4.2301e-09 - val_loss: 7.5087e-08
Epoch 137/512
512/512 - 0s - loss: 3.3221e-09 - val_loss: 7.4360e-08
Epoch 138/512
512/512 - 0s - loss: 3.5665e-09 - val_loss: 7.0378e-08
Epoch 139/512
512/512 - 0s - loss: 4.3060e-09 - val_loss: 6.8570e-08
Epoch 140/512
512/512 - 0s - loss: 4.0560e-09 - val_loss: 6.8256e-08
Epoch 141/512
512/512 - 0s - loss: 3.5755e-09 - val_loss: 6.9171e-08
Epoch 142/512
512/512 - 0s - loss: 3.2833e-09 - val_loss: 6.7613e-08
Epoch 143/512
512/512 - 0s - loss: 3.5017e-09 - val_loss: 6.5524e-08
Epoch 144/512
512/512 - 0s - loss: 3.4650e-09 - val_loss: 6.6987e-08
Epoch 145/512
512/512 - 0s - loss: 3.1543e-09 - val_loss: 6.5737e-08
Epoch 146/512
512/512 - 0s - loss: 3.3325e-09 - val_loss: 6.3348e-08
Epoch 147/512
512/512 - 0s - loss: 3.7996e-09 - val_loss: 6.4463e-08
Epoch 148/512
512/512 - 0s - loss: 2.8749e-09 - val_loss: 6.5652e-08
Epoch 149/512
512/512 - 0s - loss: 2.7463e-09 - val_loss: 6.3019e-08
Epoch 150/512
512/512 - 0s - loss: 3.3481e-09 - val_loss: 6.0636e-08
Epoch 151/512
512/512 - 0s - loss: 3.2286e-09 - val_loss: 6.2454e-08
Epoch 152/512
512/512 - 0s - loss: 2.7328e-09 - val_loss: 6.2682e-08
Epoch 153/512
512/512 - 0s - loss: 2.8713e-09 - val_loss: 5.9047e-08
Epoch 154/512
512/512 - 0s - loss: 3.1487e-09 - val_loss: 5.9080e-08
Epoch 155/512
512/512 - 0s - loss: 2.9629e-09 - val_loss: 5.6070e-08
Epoch 156/512
512/512 - 0s - loss: 3.3637e-09 - val_loss: 6.4899e-08
Epoch 157/512
512/512 - 0s - loss: 2.1060e-09 - val_loss: 6.4706e-08
Epoch 158/512
512/512 - 0s - loss: 2.0174e-09 - val_loss: 6.3986e-08
Epoch 159/512
512/512 - 0s - loss: 1.9851e-09 - val_loss: 6.2833e-08
Epoch 160/512
512/512 - 0s - loss: 2.1372e-09 - val_loss: 6.4917e-08
Epoch 161/512
512/512 - 0s - loss: 2.6053e-09 - val_loss: 6.8719e-08
Epoch 162/512
512/512 - 0s - loss: 3.2122e-09 - val_loss: 6.6715e-08
Epoch 163/512
512/512 - 0s - loss: 2.5191e-09 - val_loss: 6.4589e-08
Epoch 164/512
512/512 - 0s - loss: 2.2299e-09 - val_loss: 6.4165e-08
Epoch 165/512
512/512 - 0s - loss: 2.1734e-09 - val_loss: 6.4280e-08
Epoch 166/512
512/512 - 0s - loss: 3.0205e-09 - val_loss: 6.3404e-08
Epoch 167/512
512/512 - 0s - loss: 2.4042e-09 - val_loss: 6.1520e-08
Epoch 168/512
512/512 - 0s - loss: 2.1691e-09 - val_loss: 6.1509e-08
Epoch 169/512
512/512 - 0s - loss: 2.6341e-09 - val_loss: 6.0872e-08
Epoch 170/512
512/512 - 0s - loss: 2.4266e-09 - val_loss: 5.8066e-08
Epoch 171/512
512/512 - 0s - loss: 1.8687e-09 - val_loss: 5.7668e-08
Epoch 172/512
512/512 - 0s - loss: 1.8892e-09 - val_loss: 5.8093e-08
Epoch 173/512
512/512 - 0s - loss: 1.8191e-09 - val_loss: 5.8545e-08
Epoch 174/512
512/512 - 0s - loss: 1.8855e-09 - val_loss: 5.8791e-08
Epoch 175/512
512/512 - 0s - loss: 2.1092e-09 - val_loss: 5.9424e-08
Epoch 176/512
512/512 - 0s - loss: 2.4065e-09 - val_loss: 5.8463e-08
Epoch 177/512
512/512 - 0s - loss: 2.2213e-09 - val_loss: 5.7301e-08
Epoch 178/512
512/512 - 0s - loss: 1.9661e-09 - val_loss: 5.6725e-08
Epoch 179/512
512/512 - 0s - loss: 2.0837e-09 - val_loss: 5.5806e-08
Epoch 180/512
512/512 - 0s - loss: 2.0786e-09 - val_loss: 5.4477e-08
Epoch 181/512
512/512 - 0s - loss: 1.8409e-09 - val_loss: 5.4259e-08
Epoch 182/512
512/512 - 0s - loss: 1.8789e-09 - val_loss: 5.3945e-08
Epoch 183/512
512/512 - 0s - loss: 2.2324e-09 - val_loss: 5.4624e-08
Epoch 184/512
512/512 - 0s - loss: 2.3679e-09 - val_loss: 5.3063e-08
Epoch 185/512
512/512 - 0s - loss: 1.8505e-09 - val_loss: 5.2172e-08
Epoch 186/512
512/512 - 0s - loss: 2.3030e-09 - val_loss: 5.0141e-08
Epoch 187/512
512/512 - 0s - loss: 1.6321e-09 - val_loss: 5.2107e-08
Epoch 188/512
512/512 - 0s - loss: 1.5687e-09 - val_loss: 5.2240e-08
Epoch 189/512
512/512 - 0s - loss: 1.5230e-09 - val_loss: 5.1413e-08
Epoch 190/512
512/512 - 0s - loss: 1.4874e-09 - val_loss: 5.0995e-08
Epoch 191/512
512/512 - 0s - loss: 1.6018e-09 - val_loss: 5.1824e-08
Epoch 192/512
512/512 - 0s - loss: 1.8458e-09 - val_loss: 5.4061e-08
Epoch 193/512
512/512 - 0s - loss: 2.3686e-09 - val_loss: 5.1581e-08
Epoch 194/512
512/512 - 0s - loss: 1.7428e-09 - val_loss: 4.9697e-08
Epoch 195/512
512/512 - 0s - loss: 1.5634e-09 - val_loss: 4.8957e-08
Epoch 196/512
512/512 - 0s - loss: 1.5750e-09 - val_loss: 4.9193e-08
Epoch 197/512
512/512 - 0s - loss: 1.7589e-09 - val_loss: 4.9123e-08
Epoch 198/512
512/512 - 0s - loss: 1.8141e-09 - val_loss: 4.8821e-08
Epoch 199/512
512/512 - 0s - loss: 2.2834e-09 - val_loss: 5.1470e-08
Epoch 200/512
512/512 - 0s - loss: 2.6020e-09 - val_loss: 4.7750e-08
Epoch 201/512
512/512 - 0s - loss: 1.6559e-09 - val_loss: 4.5030e-08
Epoch 202/512
512/512 - 0s - loss: 1.4876e-09 - val_loss: 4.5081e-08
Epoch 203/512
512/512 - 0s - loss: 1.5015e-09 - val_loss: 4.5450e-08
Epoch 204/512
512/512 - 0s - loss: 1.5878e-09 - val_loss: 4.5257e-08
Epoch 205/512
512/512 - 0s - loss: 1.8041e-09 - val_loss: 4.5868e-08
Epoch 206/512
512/512 - 0s - loss: 1.8193e-09 - val_loss: 4.7377e-08
Epoch 207/512
512/512 - 0s - loss: 2.6704e-09 - val_loss: 4.4621e-08
Epoch 208/512
512/512 - 0s - loss: 1.5435e-09 - val_loss: 4.2656e-08
Epoch 209/512
512/512 - 0s - loss: 1.3206e-09 - val_loss: 4.2202e-08
Epoch 210/512
512/512 - 0s - loss: 1.3417e-09 - val_loss: 4.2240e-08
Epoch 211/512
512/512 - 0s - loss: 1.5322e-09 - val_loss: 4.3708e-08
Epoch 212/512
512/512 - 0s - loss: 1.8929e-09 - val_loss: 4.2758e-08
Epoch 213/512
512/512 - 0s - loss: 1.4254e-09 - val_loss: 4.2127e-08
Epoch 214/512
512/512 - 0s - loss: 1.3387e-09 - val_loss: 4.1782e-08
Epoch 215/512
512/512 - 0s - loss: 1.2647e-09 - val_loss: 4.1513e-08
Epoch 216/512
512/512 - 0s - loss: 1.2754e-09 - val_loss: 4.1989e-08
Epoch 217/512
512/512 - 0s - loss: 1.6362e-09 - val_loss: 4.1274e-08
Epoch 218/512
512/512 - 0s - loss: 1.3612e-09 - val_loss: 4.1397e-08
Epoch 219/512
512/512 - 0s - loss: 1.2469e-09 - val_loss: 4.1686e-08
Epoch 220/512
512/512 - 0s - loss: 1.2455e-09 - val_loss: 4.1620e-08
Epoch 221/512
512/512 - 0s - loss: 1.2725e-09 - val_loss: 4.1361e-08
Epoch 222/512
512/512 - 0s - loss: 1.3815e-09 - val_loss: 4.1957e-08
Epoch 223/512
512/512 - 0s - loss: 1.4912e-09 - val_loss: 4.0934e-08
Epoch 224/512
512/512 - 0s - loss: 1.3406e-09 - val_loss: 4.0295e-08
Epoch 225/512
512/512 - 0s - loss: 1.2795e-09 - val_loss: 4.0082e-08
Epoch 226/512
512/512 - 0s - loss: 1.2064e-09 - val_loss: 4.0231e-08
Epoch 227/512
512/512 - 0s - loss: 1.2994e-09 - val_loss: 3.9710e-08
Epoch 228/512
512/512 - 0s - loss: 1.2315e-09 - val_loss: 3.9616e-08
Epoch 229/512
512/512 - 0s - loss: 1.1987e-09 - val_loss: 3.9744e-08
Epoch 230/512
512/512 - 0s - loss: 1.2901e-09 - val_loss: 3.8954e-08
Epoch 231/512
512/512 - 0s - loss: 1.2809e-09 - val_loss: 3.9389e-08
Epoch 232/512
512/512 - 0s - loss: 1.1838e-09 - val_loss: 3.7377e-08
Epoch 233/512
512/512 - 0s - loss: 1.3338e-09 - val_loss: 3.4338e-08
Epoch 234/512
512/512 - 0s - loss: 1.7538e-09 - val_loss: 3.4614e-08
Epoch 235/512
512/512 - 0s - loss: 1.3673e-09 - val_loss: 3.4984e-08
Epoch 236/512
512/512 - 0s - loss: 1.2203e-09 - val_loss: 3.4767e-08
Epoch 237/512
512/512 - 0s - loss: 1.0937e-09 - val_loss: 3.5913e-08
Epoch 238/512
512/512 - 0s - loss: 1.0808e-09 - val_loss: 3.6677e-08
Epoch 239/512
512/512 - 0s - loss: 1.0581e-09 - val_loss: 3.7139e-08
Epoch 240/512
512/512 - 0s - loss: 1.0261e-09 - val_loss: 3.6544e-08
Epoch 241/512
512/512 - 0s - loss: 1.0181e-09 - val_loss: 3.6415e-08
Epoch 242/512
512/512 - 0s - loss: 1.0433e-09 - val_loss: 3.6688e-08
Epoch 243/512
512/512 - 0s - loss: 1.2548e-09 - val_loss: 3.7278e-08
Epoch 244/512
512/512 - 0s - loss: 1.4151e-09 - val_loss: 3.6031e-08
Epoch 245/512
512/512 - 0s - loss: 1.1439e-09 - val_loss: 3.6463e-08
Epoch 246/512
512/512 - 0s - loss: 1.0496e-09 - val_loss: 3.6340e-08
Epoch 247/512
512/512 - 0s - loss: 1.0165e-09 - val_loss: 3.5948e-08
Epoch 248/512
512/512 - 0s - loss: 9.9463e-10 - val_loss: 3.5229e-08
Epoch 249/512
512/512 - 0s - loss: 1.0099e-09 - val_loss: 3.5261e-08
Epoch 250/512
512/512 - 0s - loss: 1.2049e-09 - val_loss: 3.6179e-08
Epoch 251/512
512/512 - 0s - loss: 1.9071e-09 - val_loss: 3.6160e-08
Epoch 252/512
512/512 - 0s - loss: 1.4691e-09 - val_loss: 3.4479e-08
Epoch 253/512
512/512 - 0s - loss: 1.4709e-09 - val_loss: 3.5980e-08
Epoch 254/512
512/512 - 0s - loss: 3.2545e-09 - val_loss: 3.2225e-08
Epoch 255/512
512/512 - 0s - loss: 1.6448e-09 - val_loss: 3.4639e-08
Epoch 256/512
512/512 - 0s - loss: 1.6657e-09 - val_loss: 3.0815e-08
Epoch 257/512
512/512 - 0s - loss: 1.0463e-09 - val_loss: 3.0980e-08
Epoch 258/512
512/512 - 0s - loss: 9.5959e-10 - val_loss: 3.1179e-08
Epoch 259/512
512/512 - 0s - loss: 9.1950e-10 - val_loss: 3.0205e-08
Epoch 260/512
512/512 - 0s - loss: 1.0058e-09 - val_loss: 3.1905e-08
Epoch 261/512
512/512 - 0s - loss: 9.3683e-10 - val_loss: 3.2238e-08
Epoch 262/512
512/512 - 0s - loss: 9.1632e-10 - val_loss: 3.2207e-08
Epoch 263/512
512/512 - 0s - loss: 9.2658e-10 - val_loss: 3.2496e-08
Epoch 264/512
512/512 - 0s - loss: 1.0351e-09 - val_loss: 3.2733e-08
Epoch 265/512
512/512 - 0s - loss: 1.0033e-09 - val_loss: 3.2558e-08
Epoch 266/512
512/512 - 0s - loss: 1.0225e-09 - val_loss: 3.2566e-08
Epoch 267/512
512/512 - 0s - loss: 1.1998e-09 - val_loss: 3.2766e-08
Epoch 268/512
512/512 - 0s - loss: 1.5440e-09 - val_loss: 3.1126e-08
Epoch 269/512
512/512 - 0s - loss: 9.6730e-10 - val_loss: 3.0717e-08
Epoch 270/512
512/512 - 0s - loss: 8.8477e-10 - val_loss: 3.0531e-08
Epoch 271/512
512/512 - 0s - loss: 8.8518e-10 - val_loss: 3.0665e-08
Epoch 272/512
512/512 - 0s - loss: 8.7518e-10 - val_loss: 3.0644e-08
Epoch 273/512
512/512 - 0s - loss: 8.6960e-10 - val_loss: 3.0333e-08
Epoch 274/512
512/512 - 0s - loss: 8.6572e-10 - val_loss: 3.0216e-08
Epoch 275/512
512/512 - 0s - loss: 9.1574e-10 - val_loss: 3.0129e-08
Epoch 276/512
512/512 - 0s - loss: 9.1934e-10 - val_loss: 3.0915e-08
Epoch 277/512
512/512 - 0s - loss: 8.8595e-10 - val_loss: 3.1131e-08
Epoch 278/512
512/512 - 0s - loss: 8.7391e-10 - val_loss: 3.0608e-08
Epoch 279/512
512/512 - 0s - loss: 8.6946e-10 - val_loss: 3.0627e-08
Epoch 280/512
512/512 - 0s - loss: 8.5826e-10 - val_loss: 3.0378e-08
Epoch 281/512
512/512 - 0s - loss: 8.4480e-10 - val_loss: 3.0196e-08
Epoch 282/512
512/512 - 0s - loss: 8.8040e-10 - val_loss: 3.0719e-08
Epoch 283/512
512/512 - 0s - loss: 1.0870e-09 - val_loss: 3.1062e-08
Epoch 284/512
512/512 - 0s - loss: 1.2043e-09 - val_loss: 3.0226e-08
Epoch 285/512
512/512 - 0s - loss: 9.2283e-10 - val_loss: 2.9722e-08
Epoch 286/512
512/512 - 0s - loss: 8.8830e-10 - val_loss: 2.9390e-08
Epoch 287/512
512/512 - 0s - loss: 8.7340e-10 - val_loss: 2.9186e-08
Epoch 288/512
512/512 - 0s - loss: 8.7121e-10 - val_loss: 2.8601e-08
Epoch 289/512
512/512 - 0s - loss: 5.4930e-09 - val_loss: 3.2455e-08
Epoch 290/512
512/512 - 0s - loss: 1.7062e-09 - val_loss: 2.6131e-08
Epoch 291/512
512/512 - 0s - loss: 8.8216e-10 - val_loss: 2.5929e-08
Epoch 292/512
512/512 - 0s - loss: 8.5320e-10 - val_loss: 2.5660e-08
Epoch 293/512
512/512 - 0s - loss: 8.5892e-10 - val_loss: 2.6380e-08
Epoch 294/512
512/512 - 0s - loss: 8.8708e-10 - val_loss: 2.5998e-08
Epoch 295/512
512/512 - 0s - loss: 1.9186e-08 - val_loss: 1.4523e-08
Epoch 296/512
512/512 - 0s - loss: 1.2116e-08 - val_loss: 1.6030e-08
Epoch 297/512
512/512 - 0s - loss: 8.0648e-09 - val_loss: 1.5956e-08
Epoch 298/512
512/512 - 0s - loss: 7.7314e-09 - val_loss: 1.5637e-08
Epoch 299/512
512/512 - 0s - loss: 7.4850e-09 - val_loss: 1.5168e-08
Epoch 300/512
512/512 - 0s - loss: 7.5368e-09 - val_loss: 1.4073e-08
Epoch 301/512
512/512 - 0s - loss: 1.0592e-08 - val_loss: 1.3188e-08
Epoch 302/512
512/512 - 0s - loss: 8.3616e-09 - val_loss: 1.4898e-08
Epoch 303/512
512/512 - 0s - loss: 5.9840e-09 - val_loss: 1.5429e-08
Epoch 304/512
512/512 - 0s - loss: 5.4004e-09 - val_loss: 1.5864e-08
Epoch 305/512
512/512 - 0s - loss: 4.6321e-09 - val_loss: 1.6733e-08
Epoch 306/512
512/512 - 0s - loss: 3.3903e-09 - val_loss: 1.8557e-08
Epoch 307/512
512/512 - 0s - loss: 1.6026e-09 - val_loss: 2.0621e-08
Epoch 308/512
512/512 - 0s - loss: 8.6623e-10 - val_loss: 2.0460e-08
Epoch 309/512
512/512 - 0s - loss: 1.1598e-09 - val_loss: 1.8219e-08
Epoch 310/512
512/512 - 0s - loss: 9.7205e-10 - val_loss: 1.9406e-08
Epoch 311/512
512/512 - 0s - loss: 8.6775e-10 - val_loss: 2.1071e-08
Epoch 312/512
512/512 - 0s - loss: 1.8076e-08 - val_loss: 1.2475e-08
Epoch 313/512
512/512 - 0s - loss: 5.1708e-09 - val_loss: 1.3056e-08
Epoch 314/512
512/512 - 0s - loss: 4.7965e-09 - val_loss: 1.3421e-08
Epoch 315/512
512/512 - 0s - loss: 4.3297e-09 - val_loss: 1.3970e-08
Epoch 316/512
512/512 - 0s - loss: 3.7164e-09 - val_loss: 1.4783e-08
Epoch 317/512
512/512 - 0s - loss: 2.8787e-09 - val_loss: 1.6061e-08
Epoch 318/512
512/512 - 0s - loss: 1.8737e-09 - val_loss: 1.7558e-08
Epoch 319/512
512/512 - 0s - loss: 1.2169e-09 - val_loss: 1.8074e-08
Epoch 320/512
512/512 - 0s - loss: 1.1132e-09 - val_loss: 1.8175e-08
Epoch 321/512
512/512 - 0s - loss: 1.0308e-09 - val_loss: 1.9122e-08
Epoch 322/512
512/512 - 0s - loss: 8.0093e-10 - val_loss: 1.9154e-08
Epoch 323/512
512/512 - 0s - loss: 7.6777e-10 - val_loss: 1.9321e-08
Epoch 324/512
512/512 - 0s - loss: 7.6148e-10 - val_loss: 1.9273e-08
Epoch 325/512
512/512 - 0s - loss: 7.6360e-10 - val_loss: 1.9550e-08
Epoch 326/512
512/512 - 0s - loss: 7.4322e-10 - val_loss: 1.9095e-08
Epoch 327/512
512/512 - 0s - loss: 7.6336e-10 - val_loss: 1.9665e-08
Epoch 328/512
512/512 - 0s - loss: 7.4597e-10 - val_loss: 1.9775e-08
Epoch 329/512
512/512 - 0s - loss: 1.5582e-08 - val_loss: 1.9617e-08
Epoch 330/512
512/512 - 0s - loss: 1.8039e-08 - val_loss: 8.9001e-09
Epoch 331/512
512/512 - 0s - loss: 5.1640e-09 - val_loss: 7.9348e-09
Epoch 332/512
512/512 - 0s - loss: 4.8970e-09 - val_loss: 7.4390e-09
Epoch 333/512
512/512 - 0s - loss: 4.6540e-09 - val_loss: 7.1407e-09
Epoch 334/512
512/512 - 0s - loss: 4.4284e-09 - val_loss: 6.9634e-09
Epoch 335/512
512/512 - 0s - loss: 4.2177e-09 - val_loss: 6.8363e-09
Epoch 336/512
512/512 - 0s - loss: 4.0226e-09 - val_loss: 6.7864e-09
Epoch 337/512
512/512 - 0s - loss: 3.8693e-09 - val_loss: 7.0846e-09
Epoch 338/512
512/512 - 0s - loss: 7.6605e-09 - val_loss: 1.0222e-08
Epoch 339/512
512/512 - 0s - loss: 8.8162e-09 - val_loss: 7.0889e-09
Epoch 340/512
512/512 - 0s - loss: 3.5529e-09 - val_loss: 6.7791e-09
Epoch 341/512
512/512 - 0s - loss: 3.2797e-09 - val_loss: 6.7861e-09
Epoch 342/512
512/512 - 0s - loss: 3.2044e-09 - val_loss: 7.0001e-09
Epoch 343/512
512/512 - 0s - loss: 3.6158e-09 - val_loss: 7.9548e-09
Epoch 344/512
512/512 - 0s - loss: 5.9874e-09 - val_loss: 8.2530e-09
Epoch 345/512
512/512 - 0s - loss: 4.5193e-09 - val_loss: 7.3207e-09
Epoch 346/512
512/512 - 0s - loss: 3.1561e-09 - val_loss: 7.1107e-09
Epoch 347/512
512/512 - 0s - loss: 2.9501e-09 - val_loss: 7.2802e-09
Epoch 348/512
512/512 - 0s - loss: 3.2620e-09 - val_loss: 7.7790e-09
Epoch 349/512
512/512 - 0s - loss: 4.0602e-09 - val_loss: 7.9791e-09
Epoch 350/512
512/512 - 0s - loss: 3.7414e-09 - val_loss: 7.5798e-09
Epoch 351/512
512/512 - 0s - loss: 3.0269e-09 - val_loss: 7.4146e-09
Epoch 352/512
512/512 - 0s - loss: 2.8002e-09 - val_loss: 7.4435e-09
Epoch 353/512
512/512 - 0s - loss: 2.8553e-09 - val_loss: 7.6674e-09
Epoch 354/512
512/512 - 0s - loss: 3.0825e-09 - val_loss: 7.7783e-09
Epoch 355/512
512/512 - 0s - loss: 3.0435e-09 - val_loss: 7.6889e-09
Epoch 356/512
512/512 - 0s - loss: 2.7899e-09 - val_loss: 7.5969e-09
Epoch 357/512
512/512 - 0s - loss: 2.5969e-09 - val_loss: 7.5512e-09
Epoch 358/512
512/512 - 0s - loss: 2.5157e-09 - val_loss: 7.6282e-09
Epoch 359/512
512/512 - 0s - loss: 2.5580e-09 - val_loss: 7.7199e-09
Epoch 360/512
512/512 - 0s - loss: 2.5854e-09 - val_loss: 7.7479e-09
Epoch 361/512
512/512 - 0s - loss: 2.4683e-09 - val_loss: 7.6445e-09
Epoch 362/512
512/512 - 0s - loss: 2.2917e-09 - val_loss: 7.5950e-09
Epoch 363/512
512/512 - 0s - loss: 2.2290e-09 - val_loss: 7.6517e-09
Epoch 364/512
512/512 - 0s - loss: 2.2551e-09 - val_loss: 7.7218e-09
Epoch 365/512
512/512 - 0s - loss: 2.2492e-09 - val_loss: 7.7318e-09
Epoch 366/512
512/512 - 0s - loss: 2.1778e-09 - val_loss: 7.6925e-09
Epoch 367/512
512/512 - 0s - loss: 2.0913e-09 - val_loss: 7.6663e-09
Epoch 368/512
512/512 - 0s - loss: 2.0327e-09 - val_loss: 7.6755e-09
Epoch 369/512
512/512 - 0s - loss: 2.0126e-09 - val_loss: 7.7126e-09
Epoch 370/512
512/512 - 0s - loss: 1.9691e-09 - val_loss: 7.6894e-09
Epoch 371/512
512/512 - 0s - loss: 1.9193e-09 - val_loss: 7.7046e-09
Epoch 372/512
512/512 - 0s - loss: 1.8889e-09 - val_loss: 7.6933e-09
Epoch 373/512
512/512 - 0s - loss: 1.8355e-09 - val_loss: 7.6985e-09
Epoch 374/512
512/512 - 0s - loss: 1.8073e-09 - val_loss: 7.7232e-09
Epoch 375/512
512/512 - 0s - loss: 1.7839e-09 - val_loss: 7.7339e-09
Epoch 376/512
512/512 - 0s - loss: 1.7388e-09 - val_loss: 7.7086e-09
Epoch 377/512
512/512 - 0s - loss: 1.6846e-09 - val_loss: 7.7107e-09
Epoch 378/512
512/512 - 0s - loss: 1.6662e-09 - val_loss: 7.7276e-09
Epoch 379/512
512/512 - 0s - loss: 1.6455e-09 - val_loss: 7.7404e-09
Epoch 380/512
512/512 - 0s - loss: 1.6232e-09 - val_loss: 7.7517e-09
Epoch 381/512
512/512 - 0s - loss: 1.5958e-09 - val_loss: 7.7457e-09
Epoch 382/512
512/512 - 0s - loss: 1.5462e-09 - val_loss: 7.7170e-09
Epoch 383/512
512/512 - 0s - loss: 1.5090e-09 - val_loss: 7.7294e-09
Epoch 384/512
512/512 - 0s - loss: 1.4832e-09 - val_loss: 7.7383e-09
Epoch 385/512
512/512 - 0s - loss: 1.4541e-09 - val_loss: 7.7402e-09
Epoch 386/512
512/512 - 0s - loss: 1.4366e-09 - val_loss: 7.7454e-09
Epoch 387/512
512/512 - 0s - loss: 1.4202e-09 - val_loss: 7.7642e-09
Epoch 388/512
512/512 - 0s - loss: 1.4087e-09 - val_loss: 7.7820e-09
Epoch 389/512
512/512 - 0s - loss: 1.3901e-09 - val_loss: 7.7767e-09
Epoch 390/512
512/512 - 0s - loss: 1.3424e-09 - val_loss: 7.7560e-09
Epoch 391/512
512/512 - 0s - loss: 1.3136e-09 - val_loss: 7.7417e-09
Epoch 392/512
512/512 - 0s - loss: 1.3036e-09 - val_loss: 7.7989e-09
Epoch 393/512
512/512 - 0s - loss: 1.2995e-09 - val_loss: 7.8266e-09
Epoch 394/512
512/512 - 0s - loss: 1.2908e-09 - val_loss: 7.8117e-09
Epoch 395/512
512/512 - 0s - loss: 1.2528e-09 - val_loss: 7.7873e-09
Epoch 396/512
512/512 - 0s - loss: 1.2358e-09 - val_loss: 7.8126e-09
Epoch 397/512
512/512 - 0s - loss: 1.2138e-09 - val_loss: 7.7886e-09
Epoch 398/512
512/512 - 0s - loss: 1.1899e-09 - val_loss: 7.7988e-09
Epoch 399/512
512/512 - 0s - loss: 1.1775e-09 - val_loss: 7.8206e-09
Epoch 400/512
512/512 - 0s - loss: 1.1607e-09 - val_loss: 7.8152e-09
Epoch 401/512
512/512 - 0s - loss: 1.1404e-09 - val_loss: 7.8122e-09
Epoch 402/512
512/512 - 0s - loss: 1.1258e-09 - val_loss: 7.8287e-09
Epoch 403/512
512/512 - 0s - loss: 1.1068e-09 - val_loss: 7.8293e-09
Epoch 404/512
512/512 - 0s - loss: 1.0939e-09 - val_loss: 7.8652e-09
Epoch 405/512
512/512 - 0s - loss: 1.0953e-09 - val_loss: 7.8895e-09
Epoch 406/512
512/512 - 0s - loss: 1.0871e-09 - val_loss: 7.8906e-09
Epoch 407/512
512/512 - 0s - loss: 1.0646e-09 - val_loss: 7.8645e-09
Epoch 408/512
512/512 - 0s - loss: 1.0376e-09 - val_loss: 7.8513e-09
Epoch 409/512
512/512 - 0s - loss: 1.0118e-09 - val_loss: 7.8171e-09
Epoch 410/512
512/512 - 0s - loss: 9.9603e-10 - val_loss: 7.8526e-09
Epoch 411/512
512/512 - 0s - loss: 1.0065e-09 - val_loss: 7.9181e-09
Epoch 412/512
512/512 - 0s - loss: 1.0128e-09 - val_loss: 7.9300e-09
Epoch 413/512
512/512 - 0s - loss: 9.9628e-10 - val_loss: 7.9074e-09
Epoch 414/512
512/512 - 0s - loss: 9.7183e-10 - val_loss: 7.8736e-09
Epoch 415/512
512/512 - 0s - loss: 9.4661e-10 - val_loss: 7.8632e-09
Epoch 416/512
512/512 - 0s - loss: 9.3008e-10 - val_loss: 7.8625e-09
Epoch 417/512
512/512 - 0s - loss: 9.2092e-10 - val_loss: 7.8945e-09
Epoch 418/512
512/512 - 0s - loss: 9.1848e-10 - val_loss: 7.9120e-09
Epoch 419/512
512/512 - 0s - loss: 9.1404e-10 - val_loss: 7.9239e-09
Epoch 420/512
512/512 - 0s - loss: 9.0663e-10 - val_loss: 7.9248e-09
Epoch 421/512
512/512 - 0s - loss: 8.9769e-10 - val_loss: 7.9320e-09
Epoch 422/512
512/512 - 0s - loss: 8.8118e-10 - val_loss: 7.9117e-09
Epoch 423/512
512/512 - 0s - loss: 8.6481e-10 - val_loss: 7.9083e-09
Epoch 424/512
512/512 - 0s - loss: 8.5322e-10 - val_loss: 7.9130e-09
Epoch 425/512
512/512 - 0s - loss: 8.4422e-10 - val_loss: 7.9161e-09
Epoch 426/512
512/512 - 0s - loss: 8.3872e-10 - val_loss: 7.9553e-09
Epoch 427/512
512/512 - 0s - loss: 8.3975e-10 - val_loss: 7.9641e-09
Epoch 428/512
512/512 - 0s - loss: 8.3480e-10 - val_loss: 7.9731e-09
Epoch 429/512
512/512 - 0s - loss: 8.2884e-10 - val_loss: 7.9741e-09
Epoch 430/512
512/512 - 0s - loss: 8.1285e-10 - val_loss: 7.9483e-09
Epoch 431/512
512/512 - 0s - loss: 7.9867e-10 - val_loss: 7.9403e-09
Epoch 432/512
512/512 - 0s - loss: 7.8383e-10 - val_loss: 7.9438e-09
Epoch 433/512
512/512 - 0s - loss: 7.7903e-10 - val_loss: 7.9493e-09
Epoch 434/512
512/512 - 0s - loss: 7.6959e-10 - val_loss: 7.9733e-09
Epoch 435/512
512/512 - 0s - loss: 7.6651e-10 - val_loss: 7.9786e-09
Epoch 436/512
512/512 - 0s - loss: 7.6169e-10 - val_loss: 8.0019e-09
Epoch 437/512
512/512 - 0s - loss: 7.5692e-10 - val_loss: 7.9854e-09
Epoch 438/512
512/512 - 0s - loss: 7.4378e-10 - val_loss: 7.9872e-09
Epoch 439/512
512/512 - 0s - loss: 7.4004e-10 - val_loss: 8.0059e-09
Epoch 440/512
512/512 - 0s - loss: 7.3437e-10 - val_loss: 7.9957e-09
Epoch 441/512
512/512 - 0s - loss: 7.1890e-10 - val_loss: 7.9637e-09
Epoch 442/512
512/512 - 0s - loss: 7.1190e-10 - val_loss: 7.9973e-09
Epoch 443/512
512/512 - 0s - loss: 7.1193e-10 - val_loss: 8.0097e-09
Epoch 444/512
512/512 - 0s - loss: 7.0618e-10 - val_loss: 8.0112e-09
Epoch 445/512
512/512 - 0s - loss: 7.0298e-10 - val_loss: 8.0092e-09
Epoch 446/512
512/512 - 0s - loss: 6.9574e-10 - val_loss: 8.0171e-09
Epoch 447/512
512/512 - 0s - loss: 6.8903e-10 - val_loss: 8.0128e-09
Epoch 448/512
512/512 - 0s - loss: 6.8124e-10 - val_loss: 8.0313e-09
Epoch 449/512
512/512 - 0s - loss: 6.8061e-10 - val_loss: 8.0310e-09
Epoch 450/512
512/512 - 0s - loss: 6.7364e-10 - val_loss: 8.0285e-09
Epoch 451/512
512/512 - 0s - loss: 6.6389e-10 - val_loss: 8.0253e-09
Epoch 452/512
512/512 - 0s - loss: 6.5767e-10 - val_loss: 8.0335e-09
Epoch 453/512
512/512 - 0s - loss: 6.5887e-10 - val_loss: 8.0562e-09
Epoch 454/512
512/512 - 0s - loss: 6.5520e-10 - val_loss: 8.0528e-09
Epoch 455/512
512/512 - 0s - loss: 6.4429e-10 - val_loss: 8.0326e-09
Epoch 456/512
512/512 - 0s - loss: 6.3399e-10 - val_loss: 8.0284e-09
Epoch 457/512
512/512 - 0s - loss: 6.2968e-10 - val_loss: 8.0406e-09
Epoch 458/512
512/512 - 0s - loss: 6.2408e-10 - val_loss: 8.0337e-09
Epoch 459/512
512/512 - 0s - loss: 6.2135e-10 - val_loss: 8.0620e-09
Epoch 460/512
512/512 - 0s - loss: 6.2474e-10 - val_loss: 8.0842e-09
Epoch 461/512
512/512 - 0s - loss: 6.1745e-10 - val_loss: 8.0537e-09
Epoch 462/512
512/512 - 0s - loss: 6.0724e-10 - val_loss: 8.0379e-09
Epoch 463/512
512/512 - 0s - loss: 5.9999e-10 - val_loss: 8.0391e-09
Epoch 464/512
512/512 - 0s - loss: 5.9870e-10 - val_loss: 8.0727e-09
Epoch 465/512
512/512 - 0s - loss: 6.0033e-10 - val_loss: 8.0935e-09
Epoch 466/512
512/512 - 0s - loss: 5.9912e-10 - val_loss: 8.0778e-09
Epoch 467/512
512/512 - 0s - loss: 5.8727e-10 - val_loss: 8.0465e-09
Epoch 468/512
512/512 - 0s - loss: 5.7941e-10 - val_loss: 8.0572e-09
Epoch 469/512
512/512 - 0s - loss: 5.7310e-10 - val_loss: 8.0441e-09
Epoch 470/512
512/512 - 0s - loss: 5.6818e-10 - val_loss: 8.0645e-09
Epoch 471/512
512/512 - 0s - loss: 5.6960e-10 - val_loss: 8.0836e-09
Epoch 472/512
512/512 - 0s - loss: 5.6762e-10 - val_loss: 8.0754e-09
Epoch 473/512
512/512 - 0s - loss: 5.6290e-10 - val_loss: 8.0901e-09
Epoch 474/512
512/512 - 0s - loss: 5.6168e-10 - val_loss: 8.0960e-09
Epoch 475/512
512/512 - 0s - loss: 5.5698e-10 - val_loss: 8.0908e-09
Epoch 476/512
512/512 - 0s - loss: 5.5247e-10 - val_loss: 8.1004e-09
Epoch 477/512
512/512 - 0s - loss: 5.4859e-10 - val_loss: 8.0890e-09
Epoch 478/512
512/512 - 0s - loss: 5.4251e-10 - val_loss: 8.0892e-09
Epoch 479/512
512/512 - 0s - loss: 5.4045e-10 - val_loss: 8.0921e-09
Epoch 480/512
512/512 - 0s - loss: 5.3715e-10 - val_loss: 8.0981e-09
Epoch 481/512
512/512 - 0s - loss: 5.3308e-10 - val_loss: 8.0982e-09
Epoch 482/512
512/512 - 0s - loss: 5.2938e-10 - val_loss: 8.0860e-09
Epoch 483/512
512/512 - 0s - loss: 5.2327e-10 - val_loss: 8.0803e-09
Epoch 484/512
512/512 - 0s - loss: 5.2144e-10 - val_loss: 8.1008e-09
Epoch 485/512
512/512 - 0s - loss: 5.2111e-10 - val_loss: 8.1073e-09
Epoch 486/512
512/512 - 0s - loss: 5.1829e-10 - val_loss: 8.1125e-09
Epoch 487/512
512/512 - 0s - loss: 5.1646e-10 - val_loss: 8.1132e-09
Epoch 488/512
512/512 - 0s - loss: 5.1309e-10 - val_loss: 8.1096e-09
Epoch 489/512
512/512 - 0s - loss: 5.0943e-10 - val_loss: 8.1038e-09
Epoch 490/512
512/512 - 0s - loss: 5.0370e-10 - val_loss: 8.0939e-09
Epoch 491/512
512/512 - 0s - loss: 4.9894e-10 - val_loss: 8.0902e-09
Epoch 492/512
512/512 - 0s - loss: 4.9173e-10 - val_loss: 8.0660e-09
Epoch 493/512
512/512 - 0s - loss: 4.8714e-10 - val_loss: 8.0693e-09
Epoch 494/512
512/512 - 0s - loss: 4.8422e-10 - val_loss: 8.0663e-09
Epoch 495/512
512/512 - 0s - loss: 4.8053e-10 - val_loss: 8.0693e-09
Epoch 496/512
512/512 - 0s - loss: 4.8087e-10 - val_loss: 8.0878e-09
Epoch 497/512
512/512 - 0s - loss: 4.8063e-10 - val_loss: 8.1065e-09
Epoch 498/512
512/512 - 0s - loss: 4.8010e-10 - val_loss: 8.1090e-09
Epoch 499/512
512/512 - 0s - loss: 4.8097e-10 - val_loss: 8.1297e-09
Epoch 500/512
512/512 - 0s - loss: 4.7876e-10 - val_loss: 8.1277e-09
Epoch 501/512
512/512 - 0s - loss: 4.7608e-10 - val_loss: 8.1269e-09
Epoch 502/512
512/512 - 0s - loss: 4.7298e-10 - val_loss: 8.1224e-09
Epoch 503/512
512/512 - 0s - loss: 4.6985e-10 - val_loss: 8.1102e-09
Epoch 504/512
512/512 - 0s - loss: 4.6275e-10 - val_loss: 8.0923e-09
Epoch 505/512
512/512 - 0s - loss: 4.5839e-10 - val_loss: 8.0910e-09
Epoch 506/512
512/512 - 0s - loss: 4.5636e-10 - val_loss: 8.0951e-09
Epoch 507/512
512/512 - 0s - loss: 4.5304e-10 - val_loss: 8.0867e-09
Epoch 508/512
512/512 - 0s - loss: 4.5047e-10 - val_loss: 8.0918e-09
Epoch 509/512
512/512 - 0s - loss: 4.4862e-10 - val_loss: 8.1026e-09
Epoch 510/512
512/512 - 0s - loss: 4.4703e-10 - val_loss: 8.1043e-09
Epoch 511/512
512/512 - 0s - loss: 4.4543e-10 - val_loss: 8.1099e-09
Epoch 512/512
512/512 - 0s - loss: 4.4478e-10 - val_loss: 8.1214e-09
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1510e-09 - val_loss: 8.6852e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.9988e-09 - val_loss: 7.4597e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4056e-10 - val_loss: 7.4192e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7858e-11 - val_loss: 4.5809e-11
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8192e-11 - val_loss: 6.1081e-11
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6562e-11 - val_loss: 2.3425e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2455e-10 - val_loss: 1.8369e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3262e-09 - val_loss: 1.7765e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1353e-09 - val_loss: 4.5395e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4303e-10 - val_loss: 2.4683e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5924e-10 - val_loss: 3.2759e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5138e-10 - val_loss: 7.4436e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6150e-10 - val_loss: 1.1725e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0639e-09 - val_loss: 7.8304e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4226e-10 - val_loss: 4.6798e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3323e-10 - val_loss: 4.1348e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4368e-10 - val_loss: 5.1732e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8101e-10 - val_loss: 6.7476e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9470e-10 - val_loss: 6.7721e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3183e-10 - val_loss: 5.4532e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0650e-10 - val_loss: 4.5552e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4768e-10 - val_loss: 4.4596e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6012e-10 - val_loss: 4.8548e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9961e-10 - val_loss: 5.1344e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1012e-10 - val_loss: 4.9119e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7787e-10 - val_loss: 4.5008e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3642e-10 - val_loss: 4.1663e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1220e-10 - val_loss: 4.0816e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1019e-10 - val_loss: 4.1134e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1230e-10 - val_loss: 4.0988e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0561e-10 - val_loss: 3.9540e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9079e-10 - val_loss: 3.8140e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7663e-10 - val_loss: 3.6906e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6591e-10 - val_loss: 3.5796e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5469e-10 - val_loss: 3.4920e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4785e-10 - val_loss: 3.4488e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4337e-10 - val_loss: 3.3850e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3563e-10 - val_loss: 3.3006e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2650e-10 - val_loss: 3.1938e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1716e-10 - val_loss: 3.1247e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0912e-10 - val_loss: 3.0289e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9982e-10 - val_loss: 2.9506e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9307e-10 - val_loss: 2.8948e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8762e-10 - val_loss: 2.8338e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8114e-10 - val_loss: 2.7791e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7729e-10 - val_loss: 2.7448e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7160e-10 - val_loss: 2.6529e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6117e-10 - val_loss: 2.5480e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5348e-10 - val_loss: 2.5160e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5093e-10 - val_loss: 2.5032e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4929e-10 - val_loss: 2.4560e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4458e-10 - val_loss: 2.4202e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4016e-10 - val_loss: 2.3549e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3308e-10 - val_loss: 2.2772e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2556e-10 - val_loss: 2.2248e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2063e-10 - val_loss: 2.1831e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1772e-10 - val_loss: 2.1505e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1535e-10 - val_loss: 2.1396e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1223e-10 - val_loss: 2.1020e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0788e-10 - val_loss: 2.0301e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0228e-10 - val_loss: 1.9962e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9871e-10 - val_loss: 1.9731e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9630e-10 - val_loss: 1.9471e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9357e-10 - val_loss: 1.9187e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9086e-10 - val_loss: 1.8919e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8778e-10 - val_loss: 1.8397e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8317e-10 - val_loss: 1.8012e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7760e-10 - val_loss: 1.7476e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7297e-10 - val_loss: 1.7112e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7058e-10 - val_loss: 1.6972e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6991e-10 - val_loss: 1.7006e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6946e-10 - val_loss: 1.6800e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6624e-10 - val_loss: 1.6285e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6091e-10 - val_loss: 1.5813e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5640e-10 - val_loss: 1.5575e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5641e-10 - val_loss: 1.5668e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5610e-10 - val_loss: 1.5532e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5396e-10 - val_loss: 1.5106e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4980e-10 - val_loss: 1.4693e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4552e-10 - val_loss: 1.4334e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4298e-10 - val_loss: 1.4141e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4045e-10 - val_loss: 1.3958e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3941e-10 - val_loss: 1.3809e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3801e-10 - val_loss: 1.3757e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3706e-10 - val_loss: 1.3596e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3576e-10 - val_loss: 1.3440e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3307e-10 - val_loss: 1.3078e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2890e-10 - val_loss: 1.2734e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2727e-10 - val_loss: 1.2610e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2614e-10 - val_loss: 1.2598e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2612e-10 - val_loss: 1.2577e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2492e-10 - val_loss: 1.2305e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2231e-10 - val_loss: 1.2080e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2017e-10 - val_loss: 1.1968e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1839e-10 - val_loss: 1.1695e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1619e-10 - val_loss: 1.1491e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1451e-10 - val_loss: 1.1389e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1360e-10 - val_loss: 1.1265e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1261e-10 - val_loss: 1.1218e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1172e-10 - val_loss: 1.1070e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1012e-10 - val_loss: 1.0891e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0795e-10 - val_loss: 1.0724e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0673e-10 - val_loss: 1.0516e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0483e-10 - val_loss: 1.0431e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0388e-10 - val_loss: 1.0340e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0267e-10 - val_loss: 1.0097e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0055e-10 - val_loss: 9.9867e-11
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9223e-11 - val_loss: 9.7917e-11
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7488e-11 - val_loss: 9.7000e-11
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6583e-11 - val_loss: 9.6409e-11
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6169e-11 - val_loss: 9.6028e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6271e-11 - val_loss: 9.6139e-11
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5668e-11 - val_loss: 9.4828e-11
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3833e-11 - val_loss: 9.2117e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1686e-11 - val_loss: 9.1029e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0178e-11 - val_loss: 8.9534e-11
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9325e-11 - val_loss: 8.8973e-11
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9190e-11 - val_loss: 8.9608e-11
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9259e-11 - val_loss: 8.8044e-11
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7645e-11 - val_loss: 8.7106e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6244e-11 - val_loss: 8.6085e-11
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5754e-11 - val_loss: 8.4787e-11
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3815e-11 - val_loss: 8.2927e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2806e-11 - val_loss: 8.2536e-11
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2148e-11 - val_loss: 8.1713e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1361e-11 - val_loss: 8.0938e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0867e-11 - val_loss: 8.0818e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0646e-11 - val_loss: 8.0460e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9954e-11 - val_loss: 7.8912e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8786e-11 - val_loss: 7.8133e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7817e-11 - val_loss: 7.7268e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6781e-11 - val_loss: 7.6694e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6822e-11 - val_loss: 7.6866e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6572e-11 - val_loss: 7.5852e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4842e-11 - val_loss: 7.3663e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3318e-11 - val_loss: 7.2941e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2475e-11 - val_loss: 7.1670e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1640e-11 - val_loss: 7.0903e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0626e-11 - val_loss: 7.0334e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0637e-11 - val_loss: 7.0678e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0219e-11 - val_loss: 6.9898e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9733e-11 - val_loss: 6.8816e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8576e-11 - val_loss: 6.8175e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7565e-11 - val_loss: 6.6992e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6945e-11 - val_loss: 6.6979e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6632e-11 - val_loss: 6.6465e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6538e-11 - val_loss: 6.6665e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6721e-11 - val_loss: 6.6693e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6320e-11 - val_loss: 6.5848e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5549e-11 - val_loss: 6.4398e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3718e-11 - val_loss: 6.2985e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2779e-11 - val_loss: 6.2522e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2429e-11 - val_loss: 6.2489e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2402e-11 - val_loss: 6.2112e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1954e-11 - val_loss: 6.1468e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0876e-11 - val_loss: 6.0406e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0694e-11 - val_loss: 6.0675e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0481e-11 - val_loss: 6.0350e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0303e-11 - val_loss: 6.0264e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0328e-11 - val_loss: 6.0019e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9285e-11 - val_loss: 5.8752e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8674e-11 - val_loss: 5.8200e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7953e-11 - val_loss: 5.7642e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7147e-11 - val_loss: 5.6103e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5993e-11 - val_loss: 5.6018e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6235e-11 - val_loss: 5.5878e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5945e-11 - val_loss: 5.6351e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6127e-11 - val_loss: 5.6241e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6000e-11 - val_loss: 5.5658e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5846e-11 - val_loss: 5.5635e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5236e-11 - val_loss: 5.4432e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4366e-11 - val_loss: 5.4031e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3417e-11 - val_loss: 5.2139e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1781e-11 - val_loss: 5.1238e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1039e-11 - val_loss: 5.1569e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1847e-11 - val_loss: 5.1899e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2107e-11 - val_loss: 5.2557e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2334e-11 - val_loss: 5.1470e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1375e-11 - val_loss: 5.1436e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1112e-11 - val_loss: 5.0433e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0122e-11 - val_loss: 4.9331e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8903e-11 - val_loss: 4.8529e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8660e-11 - val_loss: 4.8737e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8811e-11 - val_loss: 4.8981e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8890e-11 - val_loss: 4.9008e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8930e-11 - val_loss: 4.8724e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8412e-11 - val_loss: 4.7575e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7214e-11 - val_loss: 4.6903e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6407e-11 - val_loss: 4.5927e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6141e-11 - val_loss: 4.6118e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6072e-11 - val_loss: 4.6348e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6752e-11 - val_loss: 4.6910e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7168e-11 - val_loss: 4.7814e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7805e-11 - val_loss: 4.7293e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7008e-11 - val_loss: 4.6430e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.5922e-11 - val_loss: 4.4981e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4748e-11 - val_loss: 4.4426e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.4193e-11 - val_loss: 4.3780e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3979e-11 - val_loss: 4.4161e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3969e-11 - val_loss: 4.3952e-11
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3914e-11 - val_loss: 4.3398e-11
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.3157e-11 - val_loss: 4.2936e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2605e-11 - val_loss: 4.2226e-11
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.2049e-11 - val_loss: 4.2043e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2505e-11 - val_loss: 4.2443e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2392e-11 - val_loss: 4.2064e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1671e-11 - val_loss: 4.1412e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1565e-11 - val_loss: 4.1822e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1795e-11 - val_loss: 4.1972e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2245e-11 - val_loss: 4.2207e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1959e-11 - val_loss: 4.1792e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1667e-11 - val_loss: 4.1280e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.1071e-11 - val_loss: 4.0857e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0936e-11 - val_loss: 4.0894e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0668e-11 - val_loss: 4.0304e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 4.0172e-11 - val_loss: 3.9568e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9363e-11 - val_loss: 3.9547e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.9471e-11 - val_loss: 3.9252e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9551e-11 - val_loss: 3.9446e-11
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8889e-11 - val_loss: 3.8313e-11
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.8223e-11 - val_loss: 3.8183e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7807e-11 - val_loss: 3.7483e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7629e-11 - val_loss: 3.7818e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7898e-11 - val_loss: 3.7880e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8227e-11 - val_loss: 3.8467e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8582e-11 - val_loss: 3.8690e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8793e-11 - val_loss: 3.8432e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7923e-11 - val_loss: 3.7261e-11
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.7057e-11 - val_loss: 3.6491e-11
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6320e-11 - val_loss: 3.6255e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.6238e-11 - val_loss: 3.6043e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5851e-11 - val_loss: 3.6085e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.5819e-11 - val_loss: 3.5138e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5275e-11 - val_loss: 3.5767e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5722e-11 - val_loss: 3.5500e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5230e-11 - val_loss: 3.5198e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4999e-11 - val_loss: 3.4943e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5132e-11 - val_loss: 3.5572e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5541e-11 - val_loss: 3.5253e-11
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4835e-11 - val_loss: 3.4368e-11
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.4030e-11 - val_loss: 3.3657e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3645e-11 - val_loss: 3.3785e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3916e-11 - val_loss: 3.4150e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4401e-11 - val_loss: 3.4845e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4621e-11 - val_loss: 3.4508e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4348e-11 - val_loss: 3.4066e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.3653e-11 - val_loss: 3.2940e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2652e-11 - val_loss: 3.2326e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2089e-11 - val_loss: 3.1899e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1987e-11 - val_loss: 3.2160e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2186e-11 - val_loss: 3.2146e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2350e-11 - val_loss: 3.2233e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.2014e-11 - val_loss: 3.1691e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1604e-11 - val_loss: 3.1792e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1612e-11 - val_loss: 3.1469e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1529e-11 - val_loss: 3.1734e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1665e-11 - val_loss: 3.1532e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1436e-11 - val_loss: 3.1360e-11
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1341e-11 - val_loss: 3.1322e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1381e-11 - val_loss: 3.1766e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1663e-11 - val_loss: 3.1480e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1461e-11 - val_loss: 3.1419e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.1019e-11 - val_loss: 3.0401e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0362e-11 - val_loss: 3.0513e-11
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0241e-11 - val_loss: 3.0283e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 3.0212e-11 - val_loss: 3.0130e-11
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9966e-11 - val_loss: 2.9565e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9545e-11 - val_loss: 2.9537e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9533e-11 - val_loss: 2.9620e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9704e-11 - val_loss: 2.9940e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0061e-11 - val_loss: 3.0362e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0419e-11 - val_loss: 3.0520e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0607e-11 - val_loss: 3.0309e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0143e-11 - val_loss: 2.9679e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9581e-11 - val_loss: 2.9313e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.9207e-11 - val_loss: 2.8992e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8936e-11 - val_loss: 2.8798e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8538e-11 - val_loss: 2.8304e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8257e-11 - val_loss: 2.8424e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8431e-11 - val_loss: 2.8432e-11
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8418e-11 - val_loss: 2.8262e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.8034e-11 - val_loss: 2.7855e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7615e-11 - val_loss: 2.7218e-11
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.7132e-11 - val_loss: 2.6972e-11
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6874e-11 - val_loss: 2.6892e-11
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6826e-11 - val_loss: 2.6859e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6973e-11 - val_loss: 2.7022e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7244e-11 - val_loss: 2.7722e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7644e-11 - val_loss: 2.7660e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7552e-11 - val_loss: 2.7343e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7091e-11 - val_loss: 2.6938e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6856e-11 - val_loss: 2.6680e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6557e-11 - val_loss: 2.6419e-11
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6328e-11 - val_loss: 2.6301e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6365e-11 - val_loss: 2.6342e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6255e-11 - val_loss: 2.6002e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5903e-11 - val_loss: 2.5859e-11
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5720e-11 - val_loss: 2.5682e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5720e-11 - val_loss: 2.5991e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5988e-11 - val_loss: 2.6181e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6207e-11 - val_loss: 2.6296e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6166e-11 - val_loss: 2.5838e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5778e-11 - val_loss: 2.5828e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.5431e-11 - val_loss: 2.4879e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4894e-11 - val_loss: 2.5162e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5170e-11 - val_loss: 2.5310e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5639e-11 - val_loss: 2.5794e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5665e-11 - val_loss: 2.5473e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5274e-11 - val_loss: 2.4902e-11
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4680e-11 - val_loss: 2.4549e-11
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4487e-11 - val_loss: 2.4327e-11
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4211e-11 - val_loss: 2.4045e-11
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4081e-11 - val_loss: 2.4005e-11
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3976e-11 - val_loss: 2.3778e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3885e-11 - val_loss: 2.4052e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4327e-11 - val_loss: 2.4593e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4589e-11 - val_loss: 2.4508e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4413e-11 - val_loss: 2.4276e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4372e-11 - val_loss: 2.4276e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4432e-11 - val_loss: 2.4506e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4418e-11 - val_loss: 2.4386e-11
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.4125e-11 - val_loss: 2.3664e-11
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3401e-11 - val_loss: 2.3047e-11
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3031e-11 - val_loss: 2.3043e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3140e-11 - val_loss: 2.3164e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3217e-11 - val_loss: 2.3211e-11
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.3174e-11 - val_loss: 2.3040e-11
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2737e-11 - val_loss: 2.2279e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.2306e-11 - val_loss: 2.2054e-11
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1972e-11 - val_loss: 2.1881e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1949e-11 - val_loss: 2.2199e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2307e-11 - val_loss: 2.2328e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2458e-11 - val_loss: 2.2370e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2215e-11 - val_loss: 2.2200e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2306e-11 - val_loss: 2.2481e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2641e-11 - val_loss: 2.2478e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2558e-11 - val_loss: 2.2633e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2405e-11 - val_loss: 2.2176e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2190e-11 - val_loss: 2.2138e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2038e-11 - val_loss: 2.2049e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2161e-11 - val_loss: 2.2273e-11
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1996e-11 - val_loss: 2.1652e-11
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1561e-11 - val_loss: 2.1422e-11
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1375e-11 - val_loss: 2.1306e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1353e-11 - val_loss: 2.1470e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1452e-11 - val_loss: 2.1557e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1629e-11 - val_loss: 2.1728e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1797e-11 - val_loss: 2.1925e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1827e-11 - val_loss: 2.1697e-11
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1574e-11 - val_loss: 2.1234e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.1125e-11 - val_loss: 2.0941e-11
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0878e-11 - val_loss: 2.0764e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0842e-11 - val_loss: 2.0846e-11
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0700e-11 - val_loss: 2.0474e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0485e-11 - val_loss: 2.0206e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0175e-11 - val_loss: 2.0272e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0401e-11 - val_loss: 2.0740e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0831e-11 - val_loss: 2.1066e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1277e-11 - val_loss: 2.1490e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1567e-11 - val_loss: 2.1442e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1304e-11 - val_loss: 2.1269e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1308e-11 - val_loss: 2.1360e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1647e-11 - val_loss: 2.1751e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1826e-11 - val_loss: 2.1946e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1732e-11 - val_loss: 2.1470e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1184e-11 - val_loss: 2.0796e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0731e-11 - val_loss: 2.0497e-11
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.0177e-11 - val_loss: 1.9869e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9781e-11 - val_loss: 1.9551e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9589e-11 - val_loss: 1.9348e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9338e-11 - val_loss: 1.9295e-11
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.9192e-11 - val_loss: 1.9055e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8879e-11 - val_loss: 1.8750e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8669e-11 - val_loss: 1.8674e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8661e-11 - val_loss: 1.8530e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8714e-11 - val_loss: 1.9156e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9222e-11 - val_loss: 1.9490e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9526e-11 - val_loss: 1.9446e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9514e-11 - val_loss: 1.9735e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9665e-11 - val_loss: 1.9375e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9291e-11 - val_loss: 1.9167e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9124e-11 - val_loss: 1.9042e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8880e-11 - val_loss: 1.8718e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8702e-11 - val_loss: 1.8768e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8811e-11 - val_loss: 1.8726e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8777e-11 - val_loss: 1.8828e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8896e-11 - val_loss: 1.9193e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9249e-11 - val_loss: 1.9216e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9124e-11 - val_loss: 1.9011e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8919e-11 - val_loss: 1.8634e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8446e-11 - val_loss: 1.8206e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8084e-11 - val_loss: 1.8077e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8194e-11 - val_loss: 1.8047e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7961e-11 - val_loss: 1.7996e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8161e-11 - val_loss: 1.8392e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8484e-11 - val_loss: 1.8488e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8350e-11 - val_loss: 1.8163e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8175e-11 - val_loss: 1.8085e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8157e-11 - val_loss: 1.8223e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8264e-11 - val_loss: 1.8279e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.8141e-11 - val_loss: 1.7827e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7761e-11 - val_loss: 1.7556e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7581e-11 - val_loss: 1.7679e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7567e-11 - val_loss: 1.7400e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7398e-11 - val_loss: 1.7508e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7530e-11 - val_loss: 1.7347e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7292e-11 - val_loss: 1.7401e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7342e-11 - val_loss: 1.7300e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.7241e-11 - val_loss: 1.6913e-11
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6868e-11 - val_loss: 1.6764e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6694e-11 - val_loss: 1.6819e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6883e-11 - val_loss: 1.6891e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6946e-11 - val_loss: 1.7076e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7173e-11 - val_loss: 1.7229e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7190e-11 - val_loss: 1.7279e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7313e-11 - val_loss: 1.7420e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7435e-11 - val_loss: 1.7196e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7110e-11 - val_loss: 1.7064e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7029e-11 - val_loss: 1.6979e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6827e-11 - val_loss: 1.6574e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6709e-11 - val_loss: 1.6847e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6871e-11 - val_loss: 1.6855e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6657e-11 - val_loss: 1.6500e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6568e-11 - val_loss: 1.6602e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6577e-11 - val_loss: 1.6425e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6247e-11 - val_loss: 1.6188e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6117e-11 - val_loss: 1.6054e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.6104e-11 - val_loss: 1.6016e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5792e-11 - val_loss: 1.5628e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5653e-11 - val_loss: 1.5873e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5903e-11 - val_loss: 1.5814e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5819e-11 - val_loss: 1.5902e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5934e-11 - val_loss: 1.6165e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6166e-11 - val_loss: 1.6010e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6003e-11 - val_loss: 1.6112e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6190e-11 - val_loss: 1.6286e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6237e-11 - val_loss: 1.6067e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6032e-11 - val_loss: 1.6255e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6459e-11 - val_loss: 1.6581e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6435e-11 - val_loss: 1.6050e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6114e-11 - val_loss: 1.6233e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6199e-11 - val_loss: 1.6199e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6182e-11 - val_loss: 1.6106e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6137e-11 - val_loss: 1.6096e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6130e-11 - val_loss: 1.6017e-11
Epoch 446/512

Epoch 00446: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5707e-11 - val_loss: 1.5383e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5360e-11 - val_loss: 1.5290e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5282e-11 - val_loss: 1.5163e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5090e-11 - val_loss: 1.5014e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.5002e-11 - val_loss: 1.5004e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4975e-11 - val_loss: 1.4872e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4789e-11 - val_loss: 1.4694e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4855e-11 - val_loss: 1.5015e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5146e-11 - val_loss: 1.5192e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4994e-11 - val_loss: 1.4861e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4948e-11 - val_loss: 1.5205e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5320e-11 - val_loss: 1.5202e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5022e-11 - val_loss: 1.4861e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4949e-11 - val_loss: 1.5128e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5274e-11 - val_loss: 1.5298e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5274e-11 - val_loss: 1.5410e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5435e-11 - val_loss: 1.5450e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5515e-11 - val_loss: 1.5450e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5114e-11 - val_loss: 1.4931e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4942e-11 - val_loss: 1.5006e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5111e-11 - val_loss: 1.5186e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5112e-11 - val_loss: 1.5017e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4942e-11 - val_loss: 1.4946e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4840e-11 - val_loss: 1.4524e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4420e-11 - val_loss: 1.4291e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4405e-11 - val_loss: 1.4619e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4712e-11 - val_loss: 1.4671e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4621e-11 - val_loss: 1.4487e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4510e-11 - val_loss: 1.4635e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4633e-11 - val_loss: 1.4654e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4565e-11 - val_loss: 1.4353e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4315e-11 - val_loss: 1.4354e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4416e-11 - val_loss: 1.4542e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4398e-11 - val_loss: 1.4096e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4073e-11 - val_loss: 1.4155e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4152e-11 - val_loss: 1.4212e-11
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.4075e-11 - val_loss: 1.3801e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3702e-11 - val_loss: 1.3655e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3719e-11 - val_loss: 1.3950e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4070e-11 - val_loss: 1.3955e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3922e-11 - val_loss: 1.3846e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3905e-11 - val_loss: 1.3931e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3924e-11 - val_loss: 1.3827e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3810e-11 - val_loss: 1.3889e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3938e-11 - val_loss: 1.4005e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3929e-11 - val_loss: 1.3755e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3655e-11 - val_loss: 1.3361e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3347e-11 - val_loss: 1.3382e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3391e-11 - val_loss: 1.3398e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3378e-11 - val_loss: 1.3341e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3594e-11 - val_loss: 1.3921e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4031e-11 - val_loss: 1.4076e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3918e-11 - val_loss: 1.3569e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3382e-11 - val_loss: 1.3337e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3338e-11 - val_loss: 1.3381e-11
Epoch 501/512

Epoch 00501: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3408e-11 - val_loss: 1.3315e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3184e-11 - val_loss: 1.3117e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3227e-11 - val_loss: 1.3283e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3235e-11 - val_loss: 1.3212e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3118e-11 - val_loss: 1.2946e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3038e-11 - val_loss: 1.3131e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3200e-11 - val_loss: 1.3315e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3330e-11 - val_loss: 1.3211e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3112e-11 - val_loss: 1.3100e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3145e-11 - val_loss: 1.3204e-11
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3082e-11 - val_loss: 1.2904e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-RMS-31/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2825e-11 - val_loss: 1.2738e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.370 | eve: 10.087 | bob: 9.215Epoch   0:   0% | abe: 9.285 | eve: 10.046 | bob: 9.165Epoch   0:   1% | abe: 9.234 | eve: 10.017 | bob: 9.140Epoch   0:   2% | abe: 9.218 | eve: 10.013 | bob: 9.141Epoch   0:   3% | abe: 9.185 | eve: 9.994 | bob: 9.119Epoch   0:   3% | abe: 9.176 | eve: 9.990 | bob: 9.119Epoch   0:   4% | abe: 9.154 | eve: 9.988 | bob: 9.103Epoch   0:   5% | abe: 9.148 | eve: 9.980 | bob: 9.102Epoch   0:   6% | abe: 9.146 | eve: 9.984 | bob: 9.105Epoch   0:   7% | abe: 9.140 | eve: 9.988 | bob: 9.103Epoch   0:   7% | abe: 9.140 | eve: 9.990 | bob: 9.106Epoch   0:   8% | abe: 9.134 | eve: 9.988 | bob: 9.102Epoch   0:   9% | abe: 9.125 | eve: 9.996 | bob: 9.095Epoch   0:  10% | abe: 9.121 | eve: 9.995 | bob: 9.093Epoch   0:  10% | abe: 9.117 | eve: 10.000 | bob: 9.090Epoch   0:  11% | abe: 9.113 | eve: 9.995 | bob: 9.087Epoch   0:  12% | abe: 9.109 | eve: 9.995 | bob: 9.085Epoch   0:  13% | abe: 9.109 | eve: 9.996 | bob: 9.086Epoch   0:  14% | abe: 9.110 | eve: 9.996 | bob: 9.088Epoch   0:  14% | abe: 9.106 | eve: 9.993 | bob: 9.085Epoch   0:  15% | abe: 9.107 | eve: 9.994 | bob: 9.087Epoch   0:  16% | abe: 9.103 | eve: 9.997 | bob: 9.083Epoch   0:  17% | abe: 9.099 | eve: 9.994 | bob: 9.080Epoch   0:  17% | abe: 9.096 | eve: 9.994 | bob: 9.078Epoch   0:  18% | abe: 9.097 | eve: 9.994 | bob: 9.079Epoch   0:  19% | abe: 9.097 | eve: 9.998 | bob: 9.079Epoch   0:  20% | abe: 9.096 | eve: 10.000 | bob: 9.080Epoch   0:  21% | abe: 9.095 | eve: 10.002 | bob: 9.078Epoch   0:  21% | abe: 9.095 | eve: 10.003 | bob: 9.079Epoch   0:  22% | abe: 9.094 | eve: 10.004 | bob: 9.078Epoch   0:  23% | abe: 9.092 | eve: 10.009 | bob: 9.077Epoch   0:  24% | abe: 9.091 | eve: 10.010 | bob: 9.077Epoch   0:  25% | abe: 9.093 | eve: 10.013 | bob: 9.078Epoch   0:  25% | abe: 9.093 | eve: 10.016 | bob: 9.078Epoch   0:  26% | abe: 9.089 | eve: 10.018 | bob: 9.075Epoch   0:  27% | abe: 9.089 | eve: 10.020 | bob: 9.075Epoch   0:  28% | abe: 9.087 | eve: 10.021 | bob: 9.074Epoch   0:  28% | abe: 9.087 | eve: 10.023 | bob: 9.074Epoch   0:  29% | abe: 9.086 | eve: 10.026 | bob: 9.073Epoch   0:  30% | abe: 9.085 | eve: 10.028 | bob: 9.072Epoch   0:  31% | abe: 9.084 | eve: 10.031 | bob: 9.071Epoch   0:  32% | abe: 9.083 | eve: 10.032 | bob: 9.071Epoch   0:  32% | abe: 9.084 | eve: 10.035 | bob: 9.072Epoch   0:  33% | abe: 9.084 | eve: 10.037 | bob: 9.072Epoch   0:  34% | abe: 9.082 | eve: 10.039 | bob: 9.070Epoch   0:  35% | abe: 9.080 | eve: 10.041 | bob: 9.068Epoch   0:  35% | abe: 9.080 | eve: 10.043 | bob: 9.068Epoch   0:  36% | abe: 9.078 | eve: 10.044 | bob: 9.067Epoch   0:  37% | abe: 9.078 | eve: 10.047 | bob: 9.067Epoch   0:  38% | abe: 9.078 | eve: 10.048 | bob: 9.067Epoch   0:  39% | abe: 9.077 | eve: 10.049 | bob: 9.067Epoch   0:  39% | abe: 9.078 | eve: 10.051 | bob: 9.068Epoch   0:  40% | abe: 9.078 | eve: 10.053 | bob: 9.068Epoch   0:  41% | abe: 9.079 | eve: 10.055 | bob: 9.068Epoch   0:  42% | abe: 9.079 | eve: 10.057 | bob: 9.068Epoch   0:  42% | abe: 9.079 | eve: 10.059 | bob: 9.069Epoch   0:  43% | abe: 9.078 | eve: 10.059 | bob: 9.068Epoch   0:  44% | abe: 9.078 | eve: 10.061 | bob: 9.068Epoch   0:  45% | abe: 9.077 | eve: 10.062 | bob: 9.067Epoch   0:  46% | abe: 9.077 | eve: 10.065 | bob: 9.067Epoch   0:  46% | abe: 9.077 | eve: 10.066 | bob: 9.067Epoch   0:  47% | abe: 9.076 | eve: 10.068 | bob: 9.067Epoch   0:  48% | abe: 9.076 | eve: 10.069 | bob: 9.067Epoch   0:  49% | abe: 9.076 | eve: 10.070 | bob: 9.067Epoch   0:  50% | abe: 9.075 | eve: 10.072 | bob: 9.066Epoch   0:  50% | abe: 9.075 | eve: 10.074 | bob: 9.066Epoch   0:  51% | abe: 9.074 | eve: 10.075 | bob: 9.065Epoch   0:  52% | abe: 9.074 | eve: 10.075 | bob: 9.065Epoch   0:  53% | abe: 9.074 | eve: 10.076 | bob: 9.065Epoch   0:  53% | abe: 9.074 | eve: 10.078 | bob: 9.065Epoch   0:  54% | abe: 9.075 | eve: 10.079 | bob: 9.066Epoch   0:  55% | abe: 9.074 | eve: 10.082 | bob: 9.065Epoch   0:  56% | abe: 9.073 | eve: 10.082 | bob: 9.064Epoch   0:  57% | abe: 9.072 | eve: 10.084 | bob: 9.063Epoch   0:  57% | abe: 9.072 | eve: 10.085 | bob: 9.063Epoch   0:  58% | abe: 9.071 | eve: 10.086 | bob: 9.062Epoch   0:  59% | abe: 9.071 | eve: 10.087 | bob: 9.062Epoch   0:  60% | abe: 9.070 | eve: 10.088 | bob: 9.062Epoch   0:  60% | abe: 9.071 | eve: 10.089 | bob: 9.063Epoch   0:  61% | abe: 9.071 | eve: 10.091 | bob: 9.062Epoch   0:  62% | abe: 9.071 | eve: 10.092 | bob: 9.063Epoch   0:  63% | abe: 9.071 | eve: 10.092 | bob: 9.062Epoch   0:  64% | abe: 9.071 | eve: 10.093 | bob: 9.062Epoch   0:  64% | abe: 9.070 | eve: 10.093 | bob: 9.062Epoch   0:  65% | abe: 9.070 | eve: 10.094 | bob: 9.062Epoch   0:  66% | abe: 9.070 | eve: 10.095 | bob: 9.062Epoch   0:  67% | abe: 9.070 | eve: 10.096 | bob: 9.062Epoch   0:  67% | abe: 9.069 | eve: 10.098 | bob: 9.061Epoch   0:  68% | abe: 9.069 | eve: 10.099 | bob: 9.062Epoch   0:  69% | abe: 9.069 | eve: 10.100 | bob: 9.062Epoch   0:  70% | abe: 9.069 | eve: 10.101 | bob: 9.062Epoch   0:  71% | abe: 9.069 | eve: 10.102 | bob: 9.062Epoch   0:  71% | abe: 9.069 | eve: 10.103 | bob: 9.061Epoch   0:  72% | abe: 9.068 | eve: 10.104 | bob: 9.061Epoch   0:  73% | abe: 9.067 | eve: 10.106 | bob: 9.060Epoch   0:  74% | abe: 9.067 | eve: 10.107 | bob: 9.060Epoch   0:  75% | abe: 9.067 | eve: 10.107 | bob: 9.060Epoch   0:  75% | abe: 9.067 | eve: 10.108 | bob: 9.060Epoch   0:  76% | abe: 9.067 | eve: 10.109 | bob: 9.059Epoch   0:  77% | abe: 9.066 | eve: 10.110 | bob: 9.059Epoch   0:  78% | abe: 9.066 | eve: 10.111 | bob: 9.059Epoch   0:  78% | abe: 9.066 | eve: 10.112 | bob: 9.059Epoch   0:  79% | abe: 9.066 | eve: 10.113 | bob: 9.059Epoch   0:  80% | abe: 9.065 | eve: 10.113 | bob: 9.058Epoch   0:  81% | abe: 9.065 | eve: 10.113 | bob: 9.058Epoch   0:  82% | abe: 9.065 | eve: 10.114 | bob: 9.059Epoch   0:  82% | abe: 9.065 | eve: 10.115 | bob: 9.059Epoch   0:  83% | abe: 9.065 | eve: 10.116 | bob: 9.058Epoch   0:  84% | abe: 9.065 | eve: 10.116 | bob: 9.059Epoch   0:  85% | abe: 9.065 | eve: 10.117 | bob: 9.059Epoch   0:  85% | abe: 9.065 | eve: 10.118 | bob: 9.059Epoch   0:  86% | abe: 9.065 | eve: 10.119 | bob: 9.058Epoch   0:  87% | abe: 9.064 | eve: 10.120 | bob: 9.058Epoch   0:  88% | abe: 9.065 | eve: 10.121 | bob: 9.059Epoch   0:  89% | abe: 9.065 | eve: 10.122 | bob: 9.059Epoch   0:  89% | abe: 9.065 | eve: 10.123 | bob: 9.059Epoch   0:  90% | abe: 9.065 | eve: 10.124 | bob: 9.059Epoch   0:  91% | abe: 9.065 | eve: 10.124 | bob: 9.059Epoch   0:  92% | abe: 9.065 | eve: 10.125 | bob: 9.059Epoch   0:  92% | abe: 9.064 | eve: 10.125 | bob: 9.058Epoch   0:  93% | abe: 9.064 | eve: 10.125 | bob: 9.059Epoch   0:  94% | abe: 9.064 | eve: 10.126 | bob: 9.059Epoch   0:  95% | abe: 9.065 | eve: 10.126 | bob: 9.059