WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-12 10:00:32.098177: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-12 10:00:32.188885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:00:32.189544: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:00:32.191819: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:00:32.193731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:00:32.194458: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:00:32.197107: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:00:32.199255: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:00:32.204421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:00:32.211878: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:00:32.212391: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-12 10:00:32.231664: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-12 10:00:32.235082: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x50489f0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:00:32.235163: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-12 10:00:32.517768: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4a102c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:00:32.517847: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-12 10:00:32.522276: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:00:32.522388: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:00:32.522424: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:00:32.522452: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:00:32.522481: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:00:32.522509: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:00:32.522536: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:00:32.522565: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:00:32.532544: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:00:32.532658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:00:32.538642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-12 10:00:32.538668: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-12 10:00:32.538692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-12 10:00:32.543188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-12 10:00:38.492327: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 10 samples, validate on 10 samples
Epoch 1/512
10/10 - 1s - loss: 0.9388 - val_loss: 0.0095
Epoch 2/512
10/10 - 0s - loss: 0.9487 - val_loss: 0.0092
Epoch 3/512
10/10 - 0s - loss: 0.9195 - val_loss: 0.0089
Epoch 4/512
10/10 - 0s - loss: 0.8860 - val_loss: 0.0081
Epoch 5/512
10/10 - 0s - loss: 0.8081 - val_loss: 0.0069
Epoch 6/512
10/10 - 0s - loss: 0.6865 - val_loss: 0.0053
Epoch 7/512
10/10 - 0s - loss: 0.5338 - val_loss: 0.0037
Epoch 8/512
10/10 - 0s - loss: 0.3726 - val_loss: 0.0023
Epoch 9/512
10/10 - 0s - loss: 0.2334 - val_loss: 0.0013
Epoch 10/512
10/10 - 0s - loss: 0.1335 - val_loss: 7.1599e-04
Epoch 11/512
10/10 - 0s - loss: 0.0716 - val_loss: 3.6798e-04
Epoch 12/512
10/10 - 0s - loss: 0.0368 - val_loss: 1.8351e-04
Epoch 13/512
10/10 - 0s - loss: 0.0184 - val_loss: 8.9483e-05
Epoch 14/512
10/10 - 0s - loss: 0.0089 - val_loss: 4.3067e-05
Epoch 15/512
10/10 - 0s - loss: 0.0043 - val_loss: 2.0868e-05
Epoch 16/512
10/10 - 0s - loss: 0.0021 - val_loss: 1.0604e-05
Epoch 17/512
10/10 - 0s - loss: 0.0011 - val_loss: 6.0199e-06
Epoch 18/512
10/10 - 0s - loss: 6.0199e-04 - val_loss: 4.0298e-06
Epoch 19/512
10/10 - 0s - loss: 4.0298e-04 - val_loss: 3.1682e-06
Epoch 20/512
10/10 - 0s - loss: 3.1682e-04 - val_loss: 2.7707e-06
Epoch 21/512
10/10 - 0s - loss: 2.7707e-04 - val_loss: 2.5521e-06
Epoch 22/512
10/10 - 0s - loss: 2.5521e-04 - val_loss: 2.3978e-06
Epoch 23/512
10/10 - 0s - loss: 2.3978e-04 - val_loss: 2.2648e-06
Epoch 24/512
10/10 - 0s - loss: 2.2648e-04 - val_loss: 2.1387e-06
Epoch 25/512
10/10 - 0s - loss: 2.1387e-04 - val_loss: 2.0150e-06
Epoch 26/512
10/10 - 0s - loss: 2.0150e-04 - val_loss: 1.8929e-06
Epoch 27/512
10/10 - 0s - loss: 1.8929e-04 - val_loss: 1.7723e-06
Epoch 28/512
10/10 - 0s - loss: 1.7723e-04 - val_loss: 1.6536e-06
Epoch 29/512
10/10 - 0s - loss: 1.6536e-04 - val_loss: 1.5372e-06
Epoch 30/512
10/10 - 0s - loss: 1.5372e-04 - val_loss: 1.4234e-06
Epoch 31/512
10/10 - 0s - loss: 1.4234e-04 - val_loss: 1.3126e-06
Epoch 32/512
10/10 - 0s - loss: 1.3126e-04 - val_loss: 1.2052e-06
Epoch 33/512
10/10 - 0s - loss: 1.2052e-04 - val_loss: 1.1016e-06
Epoch 34/512
10/10 - 0s - loss: 1.1016e-04 - val_loss: 1.0019e-06
Epoch 35/512
10/10 - 0s - loss: 1.0019e-04 - val_loss: 9.0669e-07
Epoch 36/512
10/10 - 0s - loss: 9.0669e-05 - val_loss: 8.1608e-07
Epoch 37/512
10/10 - 0s - loss: 8.1608e-05 - val_loss: 7.3034e-07
Epoch 38/512
10/10 - 0s - loss: 7.3034e-05 - val_loss: 6.4969e-07
Epoch 39/512
10/10 - 0s - loss: 6.4969e-05 - val_loss: 5.7427e-07
Epoch 40/512
10/10 - 0s - loss: 5.7427e-05 - val_loss: 5.0419e-07
Epoch 41/512
10/10 - 0s - loss: 5.0419e-05 - val_loss: 4.3952e-07
Epoch 42/512
10/10 - 0s - loss: 4.3952e-05 - val_loss: 3.8026e-07
Epoch 43/512
10/10 - 0s - loss: 3.8026e-05 - val_loss: 3.2637e-07
Epoch 44/512
10/10 - 0s - loss: 3.2637e-05 - val_loss: 2.7776e-07
Epoch 45/512
10/10 - 0s - loss: 2.7776e-05 - val_loss: 2.3427e-07
Epoch 46/512
10/10 - 0s - loss: 2.3427e-05 - val_loss: 1.9573e-07
Epoch 47/512
10/10 - 0s - loss: 1.9573e-05 - val_loss: 1.6188e-07
Epoch 48/512
10/10 - 0s - loss: 1.6188e-05 - val_loss: 1.3247e-07
Epoch 49/512
10/10 - 0s - loss: 1.3247e-05 - val_loss: 1.0717e-07
Epoch 50/512
10/10 - 0s - loss: 1.0717e-05 - val_loss: 8.5672e-08
Epoch 51/512
10/10 - 0s - loss: 8.5672e-06 - val_loss: 6.7616e-08
Epoch 52/512
10/10 - 0s - loss: 6.7616e-06 - val_loss: 5.2645e-08
Epoch 53/512
10/10 - 0s - loss: 5.2645e-06 - val_loss: 4.0403e-08
Epoch 54/512
10/10 - 0s - loss: 4.0403e-06 - val_loss: 3.0537e-08
Epoch 55/512
10/10 - 0s - loss: 3.0537e-06 - val_loss: 2.2707e-08
Epoch 56/512
10/10 - 0s - loss: 2.2707e-06 - val_loss: 1.6595e-08
Epoch 57/512
10/10 - 0s - loss: 1.6595e-06 - val_loss: 1.1907e-08
Epoch 58/512
10/10 - 0s - loss: 1.1907e-06 - val_loss: 8.3775e-09
Epoch 59/512
10/10 - 0s - loss: 8.3775e-07 - val_loss: 5.7747e-09
Epoch 60/512
10/10 - 0s - loss: 5.7747e-07 - val_loss: 3.9087e-09
Epoch 61/512
10/10 - 0s - loss: 3.9087e-07 - val_loss: 2.6955e-09
Epoch 62/512
10/10 - 0s - loss: 2.6955e-07 - val_loss: 2.7664e-09
Epoch 63/512
10/10 - 0s - loss: 2.7664e-07 - val_loss: 1.1690e-08
Epoch 64/512
10/10 - 0s - loss: 1.1690e-06 - val_loss: 1.1680e-07
Epoch 65/512
10/10 - 0s - loss: 1.1680e-05 - val_loss: 1.4376e-06
Epoch 66/512
10/10 - 0s - loss: 1.4376e-04 - val_loss: 1.8398e-05
Epoch 67/512
10/10 - 0s - loss: 0.0018 - val_loss: 1.0792e-04
Epoch 68/512
10/10 - 0s - loss: 0.0108 - val_loss: 7.3628e-05
Epoch 69/512
10/10 - 0s - loss: 0.0074 - val_loss: 3.4838e-05
Epoch 70/512
10/10 - 0s - loss: 0.0035 - val_loss: 8.0066e-06
Epoch 71/512
10/10 - 0s - loss: 8.0067e-04 - val_loss: 3.0598e-06
Epoch 72/512
10/10 - 0s - loss: 3.0598e-04 - val_loss: 1.2624e-06
Epoch 73/512
10/10 - 0s - loss: 1.2624e-04 - val_loss: 7.2026e-07
Epoch 74/512
10/10 - 0s - loss: 7.2026e-05 - val_loss: 4.8922e-07
Epoch 75/512
10/10 - 0s - loss: 4.8922e-05 - val_loss: 4.2857e-07
Epoch 76/512
10/10 - 0s - loss: 4.2857e-05 - val_loss: 4.4762e-07
Epoch 77/512
10/10 - 0s - loss: 4.4762e-05 - val_loss: 5.8317e-07
Epoch 78/512
10/10 - 0s - loss: 5.8317e-05 - val_loss: 8.8977e-07
Epoch 79/512
10/10 - 0s - loss: 8.8977e-05 - val_loss: 1.6527e-06
Epoch 80/512
10/10 - 0s - loss: 1.6527e-04 - val_loss: 3.4413e-06
Epoch 81/512
10/10 - 0s - loss: 3.4413e-04 - val_loss: 8.2611e-06
Epoch 82/512
10/10 - 0s - loss: 8.2611e-04 - val_loss: 1.8576e-05
Epoch 83/512
10/10 - 0s - loss: 0.0019 - val_loss: 3.6809e-05
Epoch 84/512
10/10 - 0s - loss: 0.0037 - val_loss: 3.8029e-05
Epoch 85/512
10/10 - 0s - loss: 0.0038 - val_loss: 3.2838e-05
Epoch 86/512
10/10 - 0s - loss: 0.0033 - val_loss: 1.5850e-05
Epoch 87/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.0006e-05
Epoch 88/512
10/10 - 0s - loss: 0.0010 - val_loss: 5.6505e-06
Epoch 89/512
10/10 - 0s - loss: 5.6505e-04 - val_loss: 4.2590e-06
Epoch 90/512
10/10 - 0s - loss: 4.2590e-04 - val_loss: 3.3651e-06
Epoch 91/512
10/10 - 0s - loss: 3.3651e-04 - val_loss: 3.4023e-06
Epoch 92/512
10/10 - 0s - loss: 3.4023e-04 - val_loss: 3.6953e-06
Epoch 93/512
10/10 - 0s - loss: 3.6953e-04 - val_loss: 4.9295e-06
Epoch 94/512
10/10 - 0s - loss: 4.9295e-04 - val_loss: 6.7628e-06
Epoch 95/512
10/10 - 0s - loss: 6.7628e-04 - val_loss: 1.0686e-05
Epoch 96/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.5059e-05
Epoch 97/512
10/10 - 0s - loss: 0.0015 - val_loss: 2.1655e-05
Epoch 98/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.1886e-05
Epoch 99/512
10/10 - 0s - loss: 0.0022 - val_loss: 2.2204e-05
Epoch 100/512
10/10 - 0s - loss: 0.0022 - val_loss: 1.5602e-05
Epoch 101/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.2883e-05
Epoch 102/512
10/10 - 0s - loss: 0.0013 - val_loss: 8.9704e-06
Epoch 103/512
10/10 - 0s - loss: 8.9704e-04 - val_loss: 7.8048e-06
Epoch 104/512
10/10 - 0s - loss: 7.8048e-04 - val_loss: 6.4822e-06
Epoch 105/512
10/10 - 0s - loss: 6.4822e-04 - val_loss: 6.6604e-06
Epoch 106/512
10/10 - 0s - loss: 6.6604e-04 - val_loss: 6.7324e-06
Epoch 107/512
10/10 - 0s - loss: 6.7324e-04 - val_loss: 8.1437e-06
Epoch 108/512
10/10 - 0s - loss: 8.1437e-04 - val_loss: 9.3329e-06
Epoch 109/512
10/10 - 0s - loss: 9.3329e-04 - val_loss: 1.2151e-05
Epoch 110/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.3613e-05
Epoch 111/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.6419e-05
Epoch 112/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.5469e-05
Epoch 113/512
10/10 - 0s - loss: 0.0015 - val_loss: 1.5851e-05
Epoch 114/512
10/10 - 0s - loss: 0.0016 - val_loss: 1.2827e-05
Epoch 115/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2005e-05
Epoch 116/512
10/10 - 0s - loss: 0.0012 - val_loss: 9.6761e-06
Epoch 117/512
10/10 - 0s - loss: 9.6761e-04 - val_loss: 9.3268e-06
Epoch 118/512
10/10 - 0s - loss: 9.3268e-04 - val_loss: 8.2593e-06
Epoch 119/512
10/10 - 0s - loss: 8.2593e-04 - val_loss: 8.7250e-06
Epoch 120/512
10/10 - 0s - loss: 8.7250e-04 - val_loss: 8.5949e-06
Epoch 121/512
10/10 - 0s - loss: 8.5949e-04 - val_loss: 9.8724e-06
Epoch 122/512
10/10 - 0s - loss: 9.8724e-04 - val_loss: 1.0275e-05
Epoch 123/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.2076e-05
Epoch 124/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.2191e-05
Epoch 125/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.3605e-05
Epoch 126/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.2578e-05
Epoch 127/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.2987e-05
Epoch 128/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.1263e-05
Epoch 129/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1208e-05
Epoch 130/512
10/10 - 0s - loss: 0.0011 - val_loss: 9.7689e-06
Epoch 131/512
10/10 - 0s - loss: 9.7689e-04 - val_loss: 9.9232e-06
Epoch 132/512
10/10 - 0s - loss: 9.9232e-04 - val_loss: 9.1086e-06
Epoch 133/512
10/10 - 0s - loss: 9.1086e-04 - val_loss: 9.7203e-06
Epoch 134/512
10/10 - 0s - loss: 9.7203e-04 - val_loss: 9.4166e-06
Epoch 135/512
10/10 - 0s - loss: 9.4166e-04 - val_loss: 1.0453e-05
Epoch 136/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0338e-05
Epoch 137/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.1513e-05
Epoch 138/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1128e-05
Epoch 139/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.2013e-05
Epoch 140/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.1117e-05
Epoch 141/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.1564e-05
Epoch 142/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.0423e-05
Epoch 143/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0691e-05
Epoch 144/512
10/10 - 0s - loss: 0.0011 - val_loss: 9.7048e-06
Epoch 145/512
10/10 - 0s - loss: 9.7048e-04 - val_loss: 1.0093e-05
Epoch 146/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.4244e-06
Epoch 147/512
10/10 - 0s - loss: 9.4244e-04 - val_loss: 1.0056e-05
Epoch 148/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.6375e-06
Epoch 149/512
10/10 - 0s - loss: 9.6375e-04 - val_loss: 1.0461e-05
Epoch 150/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0092e-05
Epoch 151/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0928e-05
Epoch 152/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0392e-05
Epoch 153/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.1055e-05
Epoch 154/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.0294e-05
Epoch 155/512
10/10 - 0s - loss: 0.0010 - val_loss: 1.0761e-05
Epoch 156/512
10/10 - 0s - loss: 0.0011 - val_loss: 9.9179e-06
Epoch 157/512
10/10 - 0s - loss: 9.9179e-04 - val_loss: 1.0319e-05
Epoch 158/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.5684e-06
Epoch 159/512
10/10 - 0s - loss: 9.5684e-04 - val_loss: 1.0040e-05
Epoch 160/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.4522e-06
Epoch 161/512
10/10 - 0s - loss: 9.4522e-04 - val_loss: 1.0045e-05
Epoch 162/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.5736e-06
Epoch 163/512
10/10 - 0s - loss: 9.5736e-04 - val_loss: 1.0245e-05
Epoch 164/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.7780e-06
Epoch 165/512
10/10 - 0s - loss: 9.7780e-04 - val_loss: 1.0429e-05
Epoch 166/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.8725e-06
Epoch 167/512
10/10 - 0s - loss: 9.8725e-04 - val_loss: 1.0428e-05
Epoch 168/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.7753e-06
Epoch 169/512
10/10 - 0s - loss: 9.7753e-04 - val_loss: 1.0241e-05
Epoch 170/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.5662e-06
Epoch 171/512
10/10 - 0s - loss: 9.5662e-04 - val_loss: 1.0009e-05
Epoch 172/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.3907e-06
Epoch 173/512
10/10 - 0s - loss: 9.3907e-04 - val_loss: 9.8728e-06
Epoch 174/512
10/10 - 0s - loss: 9.8728e-04 - val_loss: 9.3379e-06
Epoch 175/512
10/10 - 0s - loss: 9.3379e-04 - val_loss: 9.8769e-06
Epoch 176/512
10/10 - 0s - loss: 9.8769e-04 - val_loss: 9.3945e-06
Epoch 177/512
10/10 - 0s - loss: 9.3945e-04 - val_loss: 9.9604e-06
Epoch 178/512
10/10 - 0s - loss: 9.9604e-04 - val_loss: 9.4735e-06
Epoch 179/512
10/10 - 0s - loss: 9.4735e-04 - val_loss: 1.0017e-05
Epoch 180/512
10/10 - 0s - loss: 0.0010 - val_loss: 9.4864e-06
Epoch 181/512
10/10 - 0s - loss: 9.4864e-04 - val_loss: 9.9782e-06
Epoch 182/512
10/10 - 0s - loss: 9.9782e-04 - val_loss: 9.4086e-06
Epoch 183/512
10/10 - 0s - loss: 9.4086e-04 - val_loss: 9.8584e-06
Epoch 184/512
10/10 - 0s - loss: 9.8584e-04 - val_loss: 9.2880e-06
Epoch 185/512
10/10 - 0s - loss: 9.2880e-04 - val_loss: 9.7298e-06
Epoch 186/512
10/10 - 0s - loss: 9.7298e-04 - val_loss: 9.1940e-06
Epoch 187/512
10/10 - 0s - loss: 9.1940e-04 - val_loss: 9.6558e-06
Epoch 188/512
10/10 - 0s - loss: 9.6558e-04 - val_loss: 9.1629e-06
Epoch 189/512
10/10 - 0s - loss: 9.1629e-04 - val_loss: 9.6489e-06
Epoch 190/512
10/10 - 0s - loss: 9.6489e-04 - val_loss: 9.1803e-06
Epoch 191/512
10/10 - 0s - loss: 9.1803e-04 - val_loss: 9.6722e-06
Epoch 192/512
10/10 - 0s - loss: 9.6722e-04 - val_loss: 9.2000e-06
Epoch 193/512
10/10 - 0s - loss: 9.2000e-04 - val_loss: 9.6743e-06
Epoch 194/512
10/10 - 0s - loss: 9.6743e-04 - val_loss: 9.1828e-06
Epoch 195/512
10/10 - 0s - loss: 9.1828e-04 - val_loss: 9.6287e-06
Epoch 196/512
10/10 - 0s - loss: 9.6287e-04 - val_loss: 9.1239e-06
Epoch 197/512
10/10 - 0s - loss: 9.1239e-04 - val_loss: 9.5494e-06
Epoch 198/512
10/10 - 0s - loss: 9.5494e-04 - val_loss: 9.0501e-06
Epoch 199/512
10/10 - 0s - loss: 9.0501e-04 - val_loss: 9.4720e-06
Epoch 200/512
10/10 - 0s - loss: 9.4720e-04 - val_loss: 8.9939e-06
Epoch 201/512
10/10 - 0s - loss: 8.9939e-04 - val_loss: 9.4245e-06
Epoch 202/512
10/10 - 0s - loss: 9.4245e-04 - val_loss: 8.9694e-06
Epoch 203/512
10/10 - 0s - loss: 8.9694e-04 - val_loss: 9.4083e-06
Epoch 204/512
10/10 - 0s - loss: 9.4083e-04 - val_loss: 8.9655e-06
Epoch 205/512
10/10 - 0s - loss: 8.9655e-04 - val_loss: 9.4026e-06
Epoch 206/512
10/10 - 0s - loss: 9.4026e-04 - val_loss: 8.9589e-06
Epoch 207/512
10/10 - 0s - loss: 8.9589e-04 - val_loss: 9.3833e-06
Epoch 208/512
10/10 - 0s - loss: 9.3833e-04 - val_loss: 8.9328e-06
Epoch 209/512
10/10 - 0s - loss: 8.9328e-04 - val_loss: 9.3412e-06
Epoch 210/512
10/10 - 0s - loss: 9.3412e-04 - val_loss: 8.8880e-06
Epoch 211/512
10/10 - 0s - loss: 8.8880e-04 - val_loss: 9.2853e-06
Epoch 212/512
10/10 - 0s - loss: 9.2853e-04 - val_loss: 8.8390e-06
Epoch 213/512
10/10 - 0s - loss: 8.8390e-04 - val_loss: 9.2339e-06
Epoch 214/512
10/10 - 0s - loss: 9.2339e-04 - val_loss: 8.8010e-06
Epoch 215/512
10/10 - 0s - loss: 8.8010e-04 - val_loss: 9.1982e-06
Epoch 216/512
10/10 - 0s - loss: 9.1982e-04 - val_loss: 8.7782e-06
Epoch 217/512
10/10 - 0s - loss: 8.7782e-04 - val_loss: 9.1764e-06
Epoch 218/512
10/10 - 0s - loss: 9.1764e-04 - val_loss: 8.7638e-06
Epoch 219/512
10/10 - 0s - loss: 8.7638e-04 - val_loss: 9.1579e-06
Epoch 220/512
10/10 - 0s - loss: 9.1579e-04 - val_loss: 8.7467e-06
Epoch 221/512
10/10 - 0s - loss: 8.7467e-04 - val_loss: 9.1320e-06
Epoch 222/512
10/10 - 0s - loss: 9.1320e-04 - val_loss: 8.7199e-06
Epoch 223/512
10/10 - 0s - loss: 8.7199e-04 - val_loss: 9.0953e-06
Epoch 224/512
10/10 - 0s - loss: 9.0953e-04 - val_loss: 8.6848e-06
Epoch 225/512
10/10 - 0s - loss: 8.6848e-04 - val_loss: 9.0535e-06
Epoch 226/512
10/10 - 0s - loss: 9.0535e-04 - val_loss: 8.6493e-06
Epoch 227/512
10/10 - 0s - loss: 8.6493e-04 - val_loss: 9.0153e-06
Epoch 228/512
10/10 - 0s - loss: 9.0153e-04 - val_loss: 8.6200e-06
Epoch 229/512
10/10 - 0s - loss: 8.6200e-04 - val_loss: 8.9852e-06
Epoch 230/512
10/10 - 0s - loss: 8.9852e-04 - val_loss: 8.5983e-06
Epoch 231/512
10/10 - 0s - loss: 8.5983e-04 - val_loss: 8.9618e-06
Epoch 232/512
10/10 - 0s - loss: 8.9618e-04 - val_loss: 8.5801e-06
Epoch 233/512
10/10 - 0s - loss: 8.5801e-04 - val_loss: 8.9390e-06
Epoch 234/512
10/10 - 0s - loss: 8.9390e-04 - val_loss: 8.5597e-06
Epoch 235/512
10/10 - 0s - loss: 8.5597e-04 - val_loss: 8.9120e-06
Epoch 236/512
10/10 - 0s - loss: 8.9120e-04 - val_loss: 8.5345e-06
Epoch 237/512
10/10 - 0s - loss: 8.5345e-04 - val_loss: 8.8803e-06
Epoch 238/512
10/10 - 0s - loss: 8.8803e-04 - val_loss: 8.5060e-06
Epoch 239/512
10/10 - 0s - loss: 8.5060e-04 - val_loss: 8.8471e-06
Epoch 240/512
10/10 - 0s - loss: 8.8471e-04 - val_loss: 8.4782e-06
Epoch 241/512
10/10 - 0s - loss: 8.4782e-04 - val_loss: 8.8163e-06
Epoch 242/512
10/10 - 0s - loss: 8.8163e-04 - val_loss: 8.4539e-06
Epoch 243/512
10/10 - 0s - loss: 8.4539e-04 - val_loss: 8.7900e-06
Epoch 244/512
10/10 - 0s - loss: 8.7900e-04 - val_loss: 8.4334e-06
Epoch 245/512
10/10 - 0s - loss: 8.4334e-04 - val_loss: 8.7666e-06
Epoch 246/512
10/10 - 0s - loss: 8.7666e-04 - val_loss: 8.4142e-06
Epoch 247/512
10/10 - 0s - loss: 8.4142e-04 - val_loss: 8.7433e-06
Epoch 248/512
10/10 - 0s - loss: 8.7433e-04 - val_loss: 8.3939e-06
Epoch 249/512
10/10 - 0s - loss: 8.3939e-04 - val_loss: 8.7179e-06
Epoch 250/512
10/10 - 0s - loss: 8.7179e-04 - val_loss: 8.3713e-06
Epoch 251/512
10/10 - 0s - loss: 8.3713e-04 - val_loss: 8.6904e-06
Epoch 252/512
10/10 - 0s - loss: 8.6904e-04 - val_loss: 8.3474e-06
Epoch 253/512
10/10 - 0s - loss: 8.3474e-04 - val_loss: 8.6628e-06
Epoch 254/512
10/10 - 0s - loss: 8.6628e-04 - val_loss: 8.3243e-06
Epoch 255/512
10/10 - 0s - loss: 8.3243e-04 - val_loss: 8.6368e-06
Epoch 256/512
10/10 - 0s - loss: 8.6368e-04 - val_loss: 8.3034e-06
Epoch 257/512
10/10 - 0s - loss: 8.3034e-04 - val_loss: 8.6132e-06
Epoch 258/512
10/10 - 0s - loss: 8.6132e-04 - val_loss: 8.2842e-06
Epoch 259/512
10/10 - 0s - loss: 8.2842e-04 - val_loss: 8.5909e-06
Epoch 260/512
10/10 - 0s - loss: 8.5909e-04 - val_loss: 8.2656e-06
Epoch 261/512
10/10 - 0s - loss: 8.2656e-04 - val_loss: 8.5687e-06
Epoch 262/512
10/10 - 0s - loss: 8.5687e-04 - val_loss: 8.2465e-06
Epoch 263/512
10/10 - 0s - loss: 8.2465e-04 - val_loss: 8.5456e-06
Epoch 264/512
10/10 - 0s - loss: 8.5456e-04 - val_loss: 8.2265e-06
Epoch 265/512
10/10 - 0s - loss: 8.2265e-04 - val_loss: 8.5218e-06
Epoch 266/512
10/10 - 0s - loss: 8.5218e-04 - val_loss: 8.2061e-06
Epoch 267/512
10/10 - 0s - loss: 8.2061e-04 - val_loss: 8.4980e-06
Epoch 268/512
10/10 - 0s - loss: 8.4980e-04 - val_loss: 8.1862e-06
Epoch 269/512
10/10 - 0s - loss: 8.1862e-04 - val_loss: 8.4753e-06
Epoch 270/512
10/10 - 0s - loss: 8.4753e-04 - val_loss: 8.1676e-06
Epoch 271/512
10/10 - 0s - loss: 8.1676e-04 - val_loss: 8.4541e-06
Epoch 272/512
10/10 - 0s - loss: 8.4541e-04 - val_loss: 8.1502e-06
Epoch 273/512
10/10 - 0s - loss: 8.1502e-04 - val_loss: 8.4337e-06
Epoch 274/512
10/10 - 0s - loss: 8.4337e-04 - val_loss: 8.1330e-06
Epoch 275/512
10/10 - 0s - loss: 8.1330e-04 - val_loss: 8.4132e-06
Epoch 276/512
10/10 - 0s - loss: 8.4132e-04 - val_loss: 8.1154e-06
Epoch 277/512
10/10 - 0s - loss: 8.1154e-04 - val_loss: 8.3923e-06
Epoch 278/512
10/10 - 0s - loss: 8.3923e-04 - val_loss: 8.0974e-06
Epoch 279/512
10/10 - 0s - loss: 8.0974e-04 - val_loss: 8.3711e-06
Epoch 280/512
10/10 - 0s - loss: 8.3711e-04 - val_loss: 8.0796e-06
Epoch 281/512
10/10 - 0s - loss: 8.0796e-04 - val_loss: 8.3506e-06
Epoch 282/512
10/10 - 0s - loss: 8.3506e-04 - val_loss: 8.0626e-06
Epoch 283/512
10/10 - 0s - loss: 8.0626e-04 - val_loss: 8.3310e-06
Epoch 284/512
10/10 - 0s - loss: 8.3310e-04 - val_loss: 8.0463e-06
Epoch 285/512
10/10 - 0s - loss: 8.0463e-04 - val_loss: 8.3119e-06
Epoch 286/512
10/10 - 0s - loss: 8.3119e-04 - val_loss: 8.0302e-06
Epoch 287/512
10/10 - 0s - loss: 8.0302e-04 - val_loss: 8.2931e-06
Epoch 288/512
10/10 - 0s - loss: 8.2931e-04 - val_loss: 8.0143e-06
Epoch 289/512
10/10 - 0s - loss: 8.0143e-04 - val_loss: 8.2743e-06
Epoch 290/512
10/10 - 0s - loss: 8.2743e-04 - val_loss: 7.9984e-06
Epoch 291/512
10/10 - 0s - loss: 7.9984e-04 - val_loss: 8.2557e-06
Epoch 292/512
10/10 - 0s - loss: 8.2557e-04 - val_loss: 7.9827e-06
Epoch 293/512
10/10 - 0s - loss: 7.9827e-04 - val_loss: 8.2373e-06
Epoch 294/512
10/10 - 0s - loss: 8.2373e-04 - val_loss: 7.9671e-06
Epoch 295/512
10/10 - 0s - loss: 7.9671e-04 - val_loss: 8.2192e-06
Epoch 296/512
10/10 - 0s - loss: 8.2192e-04 - val_loss: 7.9520e-06
Epoch 297/512
10/10 - 0s - loss: 7.9520e-04 - val_loss: 8.2016e-06
Epoch 298/512
10/10 - 0s - loss: 8.2016e-04 - val_loss: 7.9372e-06
Epoch 299/512
10/10 - 0s - loss: 7.9372e-04 - val_loss: 8.1845e-06
Epoch 300/512
10/10 - 0s - loss: 8.1845e-04 - val_loss: 7.9228e-06
Epoch 301/512
10/10 - 0s - loss: 7.9228e-04 - val_loss: 8.1676e-06
Epoch 302/512
10/10 - 0s - loss: 8.1676e-04 - val_loss: 7.9086e-06
Epoch 303/512
10/10 - 0s - loss: 7.9086e-04 - val_loss: 8.1509e-06
Epoch 304/512
10/10 - 0s - loss: 8.1509e-04 - val_loss: 7.8945e-06
Epoch 305/512
10/10 - 0s - loss: 7.8945e-04 - val_loss: 8.1344e-06
Epoch 306/512
10/10 - 0s - loss: 8.1344e-04 - val_loss: 7.8805e-06
Epoch 307/512
10/10 - 0s - loss: 7.8805e-04 - val_loss: 8.1180e-06
Epoch 308/512
10/10 - 0s - loss: 8.1180e-04 - val_loss: 7.8666e-06
Epoch 309/512
10/10 - 0s - loss: 7.8666e-04 - val_loss: 8.1018e-06
Epoch 310/512
10/10 - 0s - loss: 8.1018e-04 - val_loss: 7.8530e-06
Epoch 311/512
10/10 - 0s - loss: 7.8530e-04 - val_loss: 8.0859e-06
Epoch 312/512
10/10 - 0s - loss: 8.0859e-04 - val_loss: 7.8397e-06
Epoch 313/512
10/10 - 0s - loss: 7.8397e-04 - val_loss: 8.0706e-06
Epoch 314/512
10/10 - 0s - loss: 8.0706e-04 - val_loss: 7.8269e-06
Epoch 315/512
10/10 - 0s - loss: 7.8269e-04 - val_loss: 8.0557e-06
Epoch 316/512
10/10 - 0s - loss: 8.0557e-04 - val_loss: 7.8144e-06
Epoch 317/512
10/10 - 0s - loss: 7.8144e-04 - val_loss: 8.0409e-06
Epoch 318/512
10/10 - 0s - loss: 8.0409e-04 - val_loss: 7.8017e-06
Epoch 319/512
10/10 - 0s - loss: 7.8017e-04 - val_loss: 8.0261e-06
Epoch 320/512
10/10 - 0s - loss: 8.0261e-04 - val_loss: 7.7891e-06
Epoch 321/512
10/10 - 0s - loss: 7.7891e-04 - val_loss: 8.0114e-06
Epoch 322/512
10/10 - 0s - loss: 8.0114e-04 - val_loss: 7.7767e-06
Epoch 323/512
10/10 - 0s - loss: 7.7767e-04 - val_loss: 7.9969e-06
Epoch 324/512
10/10 - 0s - loss: 7.9969e-04 - val_loss: 7.7646e-06
Epoch 325/512
10/10 - 0s - loss: 7.7646e-04 - val_loss: 7.9830e-06
Epoch 326/512
10/10 - 0s - loss: 7.9830e-04 - val_loss: 7.7530e-06
Epoch 327/512
10/10 - 0s - loss: 7.7530e-04 - val_loss: 7.9694e-06
Epoch 328/512
10/10 - 0s - loss: 7.9694e-04 - val_loss: 7.7416e-06
Epoch 329/512
10/10 - 0s - loss: 7.7416e-04 - val_loss: 7.9561e-06
Epoch 330/512
10/10 - 0s - loss: 7.9561e-04 - val_loss: 7.7303e-06
Epoch 331/512
10/10 - 0s - loss: 7.7303e-04 - val_loss: 7.9427e-06
Epoch 332/512
10/10 - 0s - loss: 7.9427e-04 - val_loss: 7.7188e-06
Epoch 333/512
10/10 - 0s - loss: 7.7188e-04 - val_loss: 7.9294e-06
Epoch 334/512
10/10 - 0s - loss: 7.9294e-04 - val_loss: 7.7076e-06
Epoch 335/512
10/10 - 0s - loss: 7.7076e-04 - val_loss: 7.9163e-06
Epoch 336/512
10/10 - 0s - loss: 7.9163e-04 - val_loss: 7.6965e-06
Epoch 337/512
10/10 - 0s - loss: 7.6965e-04 - val_loss: 7.9035e-06
Epoch 338/512
10/10 - 0s - loss: 7.9035e-04 - val_loss: 7.6859e-06
Epoch 339/512
10/10 - 0s - loss: 7.6859e-04 - val_loss: 7.8912e-06
Epoch 340/512
10/10 - 0s - loss: 7.8912e-04 - val_loss: 7.6755e-06
Epoch 341/512
10/10 - 0s - loss: 7.6755e-04 - val_loss: 7.8789e-06
Epoch 342/512
10/10 - 0s - loss: 7.8789e-04 - val_loss: 7.6651e-06
Epoch 343/512
10/10 - 0s - loss: 7.6651e-04 - val_loss: 7.8668e-06
Epoch 344/512
10/10 - 0s - loss: 7.8668e-04 - val_loss: 7.6548e-06
Epoch 345/512
10/10 - 0s - loss: 7.6548e-04 - val_loss: 7.8549e-06
Epoch 346/512
10/10 - 0s - loss: 7.8549e-04 - val_loss: 7.6448e-06
Epoch 347/512
10/10 - 0s - loss: 7.6448e-04 - val_loss: 7.8432e-06
Epoch 348/512
10/10 - 0s - loss: 7.8432e-04 - val_loss: 7.6349e-06
Epoch 349/512
10/10 - 0s - loss: 7.6349e-04 - val_loss: 7.8316e-06
Epoch 350/512
10/10 - 0s - loss: 7.8316e-04 - val_loss: 7.6250e-06
Epoch 351/512
10/10 - 0s - loss: 7.6250e-04 - val_loss: 7.8202e-06
Epoch 352/512
10/10 - 0s - loss: 7.8202e-04 - val_loss: 7.6155e-06
Epoch 353/512
10/10 - 0s - loss: 7.6155e-04 - val_loss: 7.8091e-06
Epoch 354/512
10/10 - 0s - loss: 7.8091e-04 - val_loss: 7.6061e-06
Epoch 355/512
10/10 - 0s - loss: 7.6061e-04 - val_loss: 7.7981e-06
Epoch 356/512
10/10 - 0s - loss: 7.7981e-04 - val_loss: 7.5967e-06
Epoch 357/512
10/10 - 0s - loss: 7.5967e-04 - val_loss: 7.7872e-06
Epoch 358/512
10/10 - 0s - loss: 7.7872e-04 - val_loss: 7.5876e-06
Epoch 359/512
10/10 - 0s - loss: 7.5876e-04 - val_loss: 7.7766e-06
Epoch 360/512
10/10 - 0s - loss: 7.7766e-04 - val_loss: 7.5786e-06
Epoch 361/512
10/10 - 0s - loss: 7.5786e-04 - val_loss: 7.7662e-06
Epoch 362/512
10/10 - 0s - loss: 7.7662e-04 - val_loss: 7.5699e-06
Epoch 363/512
10/10 - 0s - loss: 7.5699e-04 - val_loss: 7.7560e-06
Epoch 364/512
10/10 - 0s - loss: 7.7560e-04 - val_loss: 7.5612e-06
Epoch 365/512
10/10 - 0s - loss: 7.5612e-04 - val_loss: 7.7459e-06
Epoch 366/512
10/10 - 0s - loss: 7.7459e-04 - val_loss: 7.5526e-06
Epoch 367/512
10/10 - 0s - loss: 7.5526e-04 - val_loss: 7.7357e-06
Epoch 368/512
10/10 - 0s - loss: 7.7357e-04 - val_loss: 7.5439e-06
Epoch 369/512
10/10 - 0s - loss: 7.5439e-04 - val_loss: 7.7257e-06
Epoch 370/512
10/10 - 0s - loss: 7.7257e-04 - val_loss: 7.5355e-06
Epoch 371/512
10/10 - 0s - loss: 7.5355e-04 - val_loss: 7.7160e-06
Epoch 372/512
10/10 - 0s - loss: 7.7160e-04 - val_loss: 7.5274e-06
Epoch 373/512
10/10 - 0s - loss: 7.5274e-04 - val_loss: 7.7067e-06
Epoch 374/512
10/10 - 0s - loss: 7.7067e-04 - val_loss: 7.5195e-06
Epoch 375/512
10/10 - 0s - loss: 7.5195e-04 - val_loss: 7.6974e-06
Epoch 376/512
10/10 - 0s - loss: 7.6974e-04 - val_loss: 7.5116e-06
Epoch 377/512
10/10 - 0s - loss: 7.5116e-04 - val_loss: 7.6882e-06
Epoch 378/512
10/10 - 0s - loss: 7.6882e-04 - val_loss: 7.5037e-06
Epoch 379/512
10/10 - 0s - loss: 7.5037e-04 - val_loss: 7.6790e-06
Epoch 380/512
10/10 - 0s - loss: 7.6790e-04 - val_loss: 7.4960e-06
Epoch 381/512
10/10 - 0s - loss: 7.4960e-04 - val_loss: 7.6700e-06
Epoch 382/512
10/10 - 0s - loss: 7.6700e-04 - val_loss: 7.4884e-06
Epoch 383/512
10/10 - 0s - loss: 7.4884e-04 - val_loss: 7.6612e-06
Epoch 384/512
10/10 - 0s - loss: 7.6612e-04 - val_loss: 7.4809e-06
Epoch 385/512
10/10 - 0s - loss: 7.4809e-04 - val_loss: 7.6525e-06
Epoch 386/512
10/10 - 0s - loss: 7.6525e-04 - val_loss: 7.4735e-06
Epoch 387/512
10/10 - 0s - loss: 7.4735e-04 - val_loss: 7.6439e-06
Epoch 388/512
10/10 - 0s - loss: 7.6439e-04 - val_loss: 7.4663e-06
Epoch 389/512
10/10 - 0s - loss: 7.4663e-04 - val_loss: 7.6356e-06
Epoch 390/512
10/10 - 0s - loss: 7.6356e-04 - val_loss: 7.4593e-06
Epoch 391/512
10/10 - 0s - loss: 7.4593e-04 - val_loss: 7.6274e-06
Epoch 392/512
10/10 - 0s - loss: 7.6274e-04 - val_loss: 7.4523e-06
Epoch 393/512
10/10 - 0s - loss: 7.4523e-04 - val_loss: 7.6193e-06
Epoch 394/512
10/10 - 0s - loss: 7.6193e-04 - val_loss: 7.4454e-06
Epoch 395/512
10/10 - 0s - loss: 7.4454e-04 - val_loss: 7.6112e-06
Epoch 396/512
10/10 - 0s - loss: 7.6112e-04 - val_loss: 7.4385e-06
Epoch 397/512
10/10 - 0s - loss: 7.4385e-04 - val_loss: 7.6031e-06
Epoch 398/512
10/10 - 0s - loss: 7.6031e-04 - val_loss: 7.4316e-06
Epoch 399/512
10/10 - 0s - loss: 7.4316e-04 - val_loss: 7.5953e-06
Epoch 400/512
10/10 - 0s - loss: 7.5953e-04 - val_loss: 7.4252e-06
Epoch 401/512
10/10 - 0s - loss: 7.4252e-04 - val_loss: 7.5879e-06
Epoch 402/512
10/10 - 0s - loss: 7.5879e-04 - val_loss: 7.4188e-06
Epoch 403/512
10/10 - 0s - loss: 7.4188e-04 - val_loss: 7.5804e-06
Epoch 404/512
10/10 - 0s - loss: 7.5804e-04 - val_loss: 7.4124e-06
Epoch 405/512
10/10 - 0s - loss: 7.4124e-04 - val_loss: 7.5729e-06
Epoch 406/512
10/10 - 0s - loss: 7.5729e-04 - val_loss: 7.4060e-06
Epoch 407/512
10/10 - 0s - loss: 7.4060e-04 - val_loss: 7.5655e-06
Epoch 408/512
10/10 - 0s - loss: 7.5655e-04 - val_loss: 7.3997e-06
Epoch 409/512
10/10 - 0s - loss: 7.3997e-04 - val_loss: 7.5582e-06
Epoch 410/512
10/10 - 0s - loss: 7.5582e-04 - val_loss: 7.3935e-06
Epoch 411/512
10/10 - 0s - loss: 7.3935e-04 - val_loss: 7.5511e-06
Epoch 412/512
10/10 - 0s - loss: 7.5511e-04 - val_loss: 7.3875e-06
Epoch 413/512
10/10 - 0s - loss: 7.3875e-04 - val_loss: 7.5442e-06
Epoch 414/512
10/10 - 0s - loss: 7.5442e-04 - val_loss: 7.3816e-06
Epoch 415/512
10/10 - 0s - loss: 7.3816e-04 - val_loss: 7.5374e-06
Epoch 416/512
10/10 - 0s - loss: 7.5374e-04 - val_loss: 7.3758e-06
Epoch 417/512
10/10 - 0s - loss: 7.3758e-04 - val_loss: 7.5305e-06
Epoch 418/512
10/10 - 0s - loss: 7.5305e-04 - val_loss: 7.3700e-06
Epoch 419/512
10/10 - 0s - loss: 7.3700e-04 - val_loss: 7.5239e-06
Epoch 420/512
10/10 - 0s - loss: 7.5239e-04 - val_loss: 7.3644e-06
Epoch 421/512
10/10 - 0s - loss: 7.3644e-04 - val_loss: 7.5172e-06
Epoch 422/512
10/10 - 0s - loss: 7.5172e-04 - val_loss: 7.3586e-06
Epoch 423/512
10/10 - 0s - loss: 7.3586e-04 - val_loss: 7.5106e-06
Epoch 424/512
10/10 - 0s - loss: 7.5106e-04 - val_loss: 7.3530e-06
Epoch 425/512
10/10 - 0s - loss: 7.3530e-04 - val_loss: 7.5042e-06
Epoch 426/512
10/10 - 0s - loss: 7.5042e-04 - val_loss: 7.3476e-06
Epoch 427/512
10/10 - 0s - loss: 7.3476e-04 - val_loss: 7.4979e-06
Epoch 428/512
10/10 - 0s - loss: 7.4979e-04 - val_loss: 7.3423e-06
Epoch 429/512
10/10 - 0s - loss: 7.3423e-04 - val_loss: 7.4918e-06
Epoch 430/512
10/10 - 0s - loss: 7.4918e-04 - val_loss: 7.3370e-06
Epoch 431/512
10/10 - 0s - loss: 7.3370e-04 - val_loss: 7.4856e-06
Epoch 432/512
10/10 - 0s - loss: 7.4856e-04 - val_loss: 7.3319e-06
Epoch 433/512
10/10 - 0s - loss: 7.3319e-04 - val_loss: 7.4797e-06
Epoch 434/512
10/10 - 0s - loss: 7.4797e-04 - val_loss: 7.3268e-06
Epoch 435/512
10/10 - 0s - loss: 7.3267e-04 - val_loss: 7.4737e-06
Epoch 436/512
10/10 - 0s - loss: 7.4737e-04 - val_loss: 7.3216e-06
Epoch 437/512
10/10 - 0s - loss: 7.3216e-04 - val_loss: 7.4677e-06
Epoch 438/512
10/10 - 0s - loss: 7.4677e-04 - val_loss: 7.3165e-06
Epoch 439/512
10/10 - 0s - loss: 7.3165e-04 - val_loss: 7.4619e-06
Epoch 440/512
10/10 - 0s - loss: 7.4619e-04 - val_loss: 7.3116e-06
Epoch 441/512
10/10 - 0s - loss: 7.3116e-04 - val_loss: 7.4562e-06
Epoch 442/512
10/10 - 0s - loss: 7.4562e-04 - val_loss: 7.3067e-06
Epoch 443/512
10/10 - 0s - loss: 7.3067e-04 - val_loss: 7.4506e-06
Epoch 444/512
10/10 - 0s - loss: 7.4506e-04 - val_loss: 7.3020e-06
Epoch 445/512
10/10 - 0s - loss: 7.3020e-04 - val_loss: 7.4451e-06
Epoch 446/512
10/10 - 0s - loss: 7.4451e-04 - val_loss: 7.2972e-06
Epoch 447/512
10/10 - 0s - loss: 7.2972e-04 - val_loss: 7.4396e-06
Epoch 448/512
10/10 - 0s - loss: 7.4396e-04 - val_loss: 7.2926e-06
Epoch 449/512
10/10 - 0s - loss: 7.2926e-04 - val_loss: 7.4342e-06
Epoch 450/512
10/10 - 0s - loss: 7.4342e-04 - val_loss: 7.2880e-06
Epoch 451/512
10/10 - 0s - loss: 7.2880e-04 - val_loss: 7.4288e-06
Epoch 452/512
10/10 - 0s - loss: 7.4288e-04 - val_loss: 7.2834e-06
Epoch 453/512
10/10 - 0s - loss: 7.2834e-04 - val_loss: 7.4236e-06
Epoch 454/512
10/10 - 0s - loss: 7.4236e-04 - val_loss: 7.2789e-06
Epoch 455/512
10/10 - 0s - loss: 7.2789e-04 - val_loss: 7.4185e-06
Epoch 456/512
10/10 - 0s - loss: 7.4185e-04 - val_loss: 7.2745e-06
Epoch 457/512
10/10 - 0s - loss: 7.2745e-04 - val_loss: 7.4133e-06
Epoch 458/512
10/10 - 0s - loss: 7.4133e-04 - val_loss: 7.2701e-06
Epoch 459/512
10/10 - 0s - loss: 7.2701e-04 - val_loss: 7.4082e-06
Epoch 460/512
10/10 - 0s - loss: 7.4082e-04 - val_loss: 7.2658e-06
Epoch 461/512
10/10 - 0s - loss: 7.2658e-04 - val_loss: 7.4032e-06
Epoch 462/512
10/10 - 0s - loss: 7.4032e-04 - val_loss: 7.2615e-06
Epoch 463/512
10/10 - 0s - loss: 7.2615e-04 - val_loss: 7.3983e-06
Epoch 464/512
10/10 - 0s - loss: 7.3983e-04 - val_loss: 7.2572e-06
Epoch 465/512
10/10 - 0s - loss: 7.2572e-04 - val_loss: 7.3935e-06
Epoch 466/512
10/10 - 0s - loss: 7.3935e-04 - val_loss: 7.2532e-06
Epoch 467/512
10/10 - 0s - loss: 7.2532e-04 - val_loss: 7.3888e-06
Epoch 468/512
10/10 - 0s - loss: 7.3888e-04 - val_loss: 7.2493e-06
Epoch 469/512
10/10 - 0s - loss: 7.2493e-04 - val_loss: 7.3841e-06
Epoch 470/512
10/10 - 0s - loss: 7.3841e-04 - val_loss: 7.2451e-06
Epoch 471/512
10/10 - 0s - loss: 7.2451e-04 - val_loss: 7.3794e-06
Epoch 472/512
10/10 - 0s - loss: 7.3794e-04 - val_loss: 7.2412e-06
Epoch 473/512
10/10 - 0s - loss: 7.2412e-04 - val_loss: 7.3749e-06
Epoch 474/512
10/10 - 0s - loss: 7.3749e-04 - val_loss: 7.2373e-06
Epoch 475/512
10/10 - 0s - loss: 7.2373e-04 - val_loss: 7.3703e-06
Epoch 476/512
10/10 - 0s - loss: 7.3703e-04 - val_loss: 7.2333e-06
Epoch 477/512
10/10 - 0s - loss: 7.2333e-04 - val_loss: 7.3657e-06
Epoch 478/512
10/10 - 0s - loss: 7.3657e-04 - val_loss: 7.2293e-06
Epoch 479/512
10/10 - 0s - loss: 7.2293e-04 - val_loss: 7.3612e-06
Epoch 480/512
10/10 - 0s - loss: 7.3612e-04 - val_loss: 7.2256e-06
Epoch 481/512
10/10 - 0s - loss: 7.2256e-04 - val_loss: 7.3570e-06
Epoch 482/512
10/10 - 0s - loss: 7.3570e-04 - val_loss: 7.2219e-06
Epoch 483/512
10/10 - 0s - loss: 7.2219e-04 - val_loss: 7.3527e-06
Epoch 484/512
10/10 - 0s - loss: 7.3527e-04 - val_loss: 7.2183e-06
Epoch 485/512
10/10 - 0s - loss: 7.2183e-04 - val_loss: 7.3486e-06
Epoch 486/512
10/10 - 0s - loss: 7.3486e-04 - val_loss: 7.2147e-06
Epoch 487/512
10/10 - 0s - loss: 7.2147e-04 - val_loss: 7.3444e-06
Epoch 488/512
10/10 - 0s - loss: 7.3444e-04 - val_loss: 7.2111e-06
Epoch 489/512
10/10 - 0s - loss: 7.2111e-04 - val_loss: 7.3401e-06
Epoch 490/512
10/10 - 0s - loss: 7.3401e-04 - val_loss: 7.2074e-06
Epoch 491/512
10/10 - 0s - loss: 7.2074e-04 - val_loss: 7.3359e-06
Epoch 492/512
10/10 - 0s - loss: 7.3359e-04 - val_loss: 7.2038e-06
Epoch 493/512
10/10 - 0s - loss: 7.2038e-04 - val_loss: 7.3317e-06
Epoch 494/512
10/10 - 0s - loss: 7.3317e-04 - val_loss: 7.2002e-06
Epoch 495/512
10/10 - 0s - loss: 7.2002e-04 - val_loss: 7.3278e-06
Epoch 496/512
10/10 - 0s - loss: 7.3278e-04 - val_loss: 7.1970e-06
Epoch 497/512
10/10 - 0s - loss: 7.1970e-04 - val_loss: 7.3241e-06
Epoch 498/512
10/10 - 0s - loss: 7.3241e-04 - val_loss: 7.1938e-06
Epoch 499/512
10/10 - 0s - loss: 7.1938e-04 - val_loss: 7.3204e-06
Epoch 500/512
10/10 - 0s - loss: 7.3204e-04 - val_loss: 7.1905e-06
Epoch 501/512
10/10 - 0s - loss: 7.1905e-04 - val_loss: 7.3164e-06
Epoch 502/512
10/10 - 0s - loss: 7.3164e-04 - val_loss: 7.1870e-06
Epoch 503/512
10/10 - 0s - loss: 7.1870e-04 - val_loss: 7.3124e-06
Epoch 504/512
10/10 - 0s - loss: 7.3124e-04 - val_loss: 7.1836e-06
Epoch 505/512
10/10 - 0s - loss: 7.1836e-04 - val_loss: 7.3085e-06
Epoch 506/512
10/10 - 0s - loss: 7.3085e-04 - val_loss: 7.1803e-06
Epoch 507/512
10/10 - 0s - loss: 7.1803e-04 - val_loss: 7.3048e-06
Epoch 508/512
10/10 - 0s - loss: 7.3048e-04 - val_loss: 7.1771e-06
Epoch 509/512
10/10 - 0s - loss: 7.1771e-04 - val_loss: 7.3012e-06
Epoch 510/512
10/10 - 0s - loss: 7.3012e-04 - val_loss: 7.1741e-06
Epoch 511/512
10/10 - 0s - loss: 7.1741e-04 - val_loss: 7.2978e-06
Epoch 512/512
10/10 - 0s - loss: 7.2978e-04 - val_loss: 7.1711e-06
2024-04-12 10:00:42.033315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 10 samples, validate on 10 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00124, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 8.1035e-04 - val_loss: 0.0012
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00124
10/10 - 0s - loss: 0.0012 - val_loss: 0.0016
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00124
10/10 - 0s - loss: 0.0016 - val_loss: 0.0017
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00124
10/10 - 0s - loss: 0.0017 - val_loss: 0.0014
Epoch 5/512

Epoch 00005: val_loss improved from 0.00124 to 0.00105, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 0.0014 - val_loss: 0.0011
Epoch 6/512

Epoch 00006: val_loss improved from 0.00105 to 0.00075, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 0.0011 - val_loss: 7.5365e-04
Epoch 7/512

Epoch 00007: val_loss improved from 0.00075 to 0.00059, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 7.5556e-04 - val_loss: 5.8836e-04
Epoch 8/512

Epoch 00008: val_loss improved from 0.00059 to 0.00048, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 5.8985e-04 - val_loss: 4.8090e-04
Epoch 9/512

Epoch 00009: val_loss improved from 0.00048 to 0.00045, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.8212e-04 - val_loss: 4.4597e-04
Epoch 10/512

Epoch 00010: val_loss improved from 0.00045 to 0.00044, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_addition_weights.h5
10/10 - 0s - loss: 4.4710e-04 - val_loss: 4.4499e-04
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00044
10/10 - 0s - loss: 4.4611e-04 - val_loss: 4.9771e-04
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00044
10/10 - 0s - loss: 4.9897e-04 - val_loss: 5.8339e-04
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00044
10/10 - 0s - loss: 5.8486e-04 - val_loss: 7.2982e-04
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.3167e-04 - val_loss: 8.8937e-04
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.9162e-04 - val_loss: 0.0011
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00044
10/10 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00044
10/10 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00044
10/10 - 0s - loss: 0.0012 - val_loss: 0.0010
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00044
10/10 - 0s - loss: 0.0010 - val_loss: 9.2465e-04
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00044
10/10 - 0s - loss: 9.2699e-04 - val_loss: 7.8240e-04
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8437e-04 - val_loss: 7.0065e-04
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.0242e-04 - val_loss: 6.3205e-04
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00044
10/10 - 0s - loss: 6.3365e-04 - val_loss: 6.1889e-04
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00044
10/10 - 0s - loss: 6.2046e-04 - val_loss: 6.1997e-04
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00044
10/10 - 0s - loss: 6.2153e-04 - val_loss: 6.6801e-04
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00044
10/10 - 0s - loss: 6.6970e-04 - val_loss: 7.2159e-04
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.2341e-04 - val_loss: 8.1165e-04
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1370e-04 - val_loss: 8.7724e-04
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.7945e-04 - val_loss: 9.5155e-04
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00044
10/10 - 0s - loss: 9.5395e-04 - val_loss: 9.5833e-04
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00044
10/10 - 0s - loss: 9.6075e-04 - val_loss: 9.5939e-04
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00044
10/10 - 0s - loss: 9.6181e-04 - val_loss: 8.9618e-04
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.9844e-04 - val_loss: 8.5230e-04
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.5445e-04 - val_loss: 7.8114e-04
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8311e-04 - val_loss: 7.4964e-04
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5153e-04 - val_loss: 7.1179e-04
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.1359e-04 - val_loss: 7.1567e-04
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.1748e-04 - val_loss: 7.1589e-04
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.1770e-04 - val_loss: 7.5285e-04
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5475e-04 - val_loss: 7.7713e-04
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7910e-04 - val_loss: 8.2821e-04
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.3030e-04 - val_loss: 8.4812e-04
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.5026e-04 - val_loss: 8.8274e-04
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.8497e-04 - val_loss: 8.7183e-04
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.7403e-04 - val_loss: 8.7431e-04
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.7651e-04 - val_loss: 8.3646e-04
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.3858e-04 - val_loss: 8.2237e-04
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2445e-04 - val_loss: 7.8324e-04
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8523e-04 - val_loss: 7.7626e-04
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7822e-04 - val_loss: 7.5397e-04
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5588e-04 - val_loss: 7.6523e-04
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6716e-04 - val_loss: 7.6196e-04
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6389e-04 - val_loss: 7.8886e-04
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9085e-04 - val_loss: 7.9494e-04
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9695e-04 - val_loss: 8.2523e-04
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2731e-04 - val_loss: 8.2560e-04
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2769e-04 - val_loss: 8.4552e-04
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.4765e-04 - val_loss: 8.3094e-04
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.3305e-04 - val_loss: 8.3666e-04
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.3877e-04 - val_loss: 8.1143e-04
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1349e-04 - val_loss: 8.1114e-04
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1319e-04 - val_loss: 7.8679e-04
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8878e-04 - val_loss: 7.9095e-04
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9295e-04 - val_loss: 7.7536e-04
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7732e-04 - val_loss: 7.8869e-04
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9068e-04 - val_loss: 7.8201e-04
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8399e-04 - val_loss: 8.0213e-04
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0415e-04 - val_loss: 7.9859e-04
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0062e-04 - val_loss: 8.1882e-04
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2088e-04 - val_loss: 8.1129e-04
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1335e-04 - val_loss: 8.2579e-04
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2787e-04 - val_loss: 8.1123e-04
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1329e-04 - val_loss: 8.1947e-04
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.2154e-04 - val_loss: 8.0077e-04
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0280e-04 - val_loss: 8.0699e-04
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0902e-04 - val_loss: 7.8948e-04
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9149e-04 - val_loss: 7.9841e-04
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0042e-04 - val_loss: 7.8539e-04
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8738e-04 - val_loss: 7.9876e-04
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0077e-04 - val_loss: 7.8974e-04
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9175e-04 - val_loss: 8.0586e-04
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0789e-04 - val_loss: 7.9768e-04
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9971e-04 - val_loss: 8.1319e-04
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1523e-04 - val_loss: 8.0265e-04
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0469e-04 - val_loss: 8.1518e-04
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1723e-04 - val_loss: 8.0142e-04
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0345e-04 - val_loss: 8.1120e-04
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1324e-04 - val_loss: 7.9590e-04
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9792e-04 - val_loss: 8.0514e-04
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0716e-04 - val_loss: 7.9080e-04
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9281e-04 - val_loss: 8.0158e-04
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0360e-04 - val_loss: 7.8950e-04
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9150e-04 - val_loss: 8.0236e-04
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0438e-04 - val_loss: 7.9201e-04
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9402e-04 - val_loss: 8.0590e-04
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0792e-04 - val_loss: 7.9563e-04
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9765e-04 - val_loss: 8.0893e-04
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1097e-04 - val_loss: 7.9740e-04
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9942e-04 - val_loss: 8.0919e-04
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.1122e-04 - val_loss: 7.9620e-04
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9822e-04 - val_loss: 8.0681e-04
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0884e-04 - val_loss: 7.9332e-04
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9534e-04 - val_loss: 8.0387e-04
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0589e-04 - val_loss: 7.9101e-04
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9303e-04 - val_loss: 8.0239e-04
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0441e-04 - val_loss: 7.9065e-04
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9266e-04 - val_loss: 8.0297e-04
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0499e-04 - val_loss: 7.9196e-04
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9397e-04 - val_loss: 8.0462e-04
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0664e-04 - val_loss: 7.9351e-04
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9553e-04 - val_loss: 8.0575e-04
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0777e-04 - val_loss: 7.9397e-04
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9599e-04 - val_loss: 8.0546e-04
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0749e-04 - val_loss: 7.9307e-04
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9509e-04 - val_loss: 8.0408e-04
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0610e-04 - val_loss: 7.9157e-04
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9359e-04 - val_loss: 8.0262e-04
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0463e-04 - val_loss: 7.9050e-04
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9251e-04 - val_loss: 8.0197e-04
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0399e-04 - val_loss: 7.9040e-04
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9241e-04 - val_loss: 8.0227e-04
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0429e-04 - val_loss: 7.9100e-04
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9301e-04 - val_loss: 8.0295e-04
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0497e-04 - val_loss: 7.9157e-04
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9359e-04 - val_loss: 8.0326e-04
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0528e-04 - val_loss: 7.9157e-04
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9359e-04 - val_loss: 8.0290e-04
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0491e-04 - val_loss: 7.9095e-04
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9296e-04 - val_loss: 8.0206e-04
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0407e-04 - val_loss: 7.9012e-04
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9213e-04 - val_loss: 8.0128e-04
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0329e-04 - val_loss: 7.8957e-04
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9158e-04 - val_loss: 8.0095e-04
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0296e-04 - val_loss: 7.8951e-04
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9152e-04 - val_loss: 8.0104e-04
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0305e-04 - val_loss: 7.8972e-04
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9173e-04 - val_loss: 8.0124e-04
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0325e-04 - val_loss: 7.8985e-04
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9186e-04 - val_loss: 8.0121e-04
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0322e-04 - val_loss: 7.8968e-04
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9169e-04 - val_loss: 8.0086e-04
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0287e-04 - val_loss: 7.8924e-04
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9125e-04 - val_loss: 8.0033e-04
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0234e-04 - val_loss: 7.8875e-04
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9076e-04 - val_loss: 7.9988e-04
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0189e-04 - val_loss: 7.8844e-04
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9045e-04 - val_loss: 7.9965e-04
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0166e-04 - val_loss: 7.8834e-04
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9034e-04 - val_loss: 7.9960e-04
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0160e-04 - val_loss: 7.8834e-04
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9035e-04 - val_loss: 7.9957e-04
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0157e-04 - val_loss: 7.8828e-04
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9029e-04 - val_loss: 7.9942e-04
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0143e-04 - val_loss: 7.8809e-04
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9010e-04 - val_loss: 7.9912e-04
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0113e-04 - val_loss: 7.8776e-04
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8977e-04 - val_loss: 7.9875e-04
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0075e-04 - val_loss: 7.8744e-04
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8944e-04 - val_loss: 7.9844e-04
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0044e-04 - val_loss: 7.8720e-04
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8921e-04 - val_loss: 7.9823e-04
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0023e-04 - val_loss: 7.8706e-04
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8906e-04 - val_loss: 7.9810e-04
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00044
10/10 - 0s - loss: 8.0010e-04 - val_loss: 7.8697e-04
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8898e-04 - val_loss: 7.9798e-04
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9998e-04 - val_loss: 7.8685e-04
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8885e-04 - val_loss: 7.9779e-04
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9979e-04 - val_loss: 7.8664e-04
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8865e-04 - val_loss: 7.9752e-04
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9952e-04 - val_loss: 7.8638e-04
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8838e-04 - val_loss: 7.9723e-04
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9923e-04 - val_loss: 7.8613e-04
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8813e-04 - val_loss: 7.9698e-04
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9898e-04 - val_loss: 7.8593e-04
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8793e-04 - val_loss: 7.9679e-04
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9878e-04 - val_loss: 7.8578e-04
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8778e-04 - val_loss: 7.9663e-04
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9862e-04 - val_loss: 7.8564e-04
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8764e-04 - val_loss: 7.9645e-04
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9844e-04 - val_loss: 7.8547e-04
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8747e-04 - val_loss: 7.9623e-04
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9822e-04 - val_loss: 7.8526e-04
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8726e-04 - val_loss: 7.9600e-04
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9799e-04 - val_loss: 7.8506e-04
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8706e-04 - val_loss: 7.9578e-04
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9777e-04 - val_loss: 7.8486e-04
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8686e-04 - val_loss: 7.9556e-04
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9755e-04 - val_loss: 7.8467e-04
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8666e-04 - val_loss: 7.9535e-04
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9734e-04 - val_loss: 7.8449e-04
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8649e-04 - val_loss: 7.9516e-04
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9715e-04 - val_loss: 7.8432e-04
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8632e-04 - val_loss: 7.9496e-04
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9695e-04 - val_loss: 7.8414e-04
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8614e-04 - val_loss: 7.9475e-04
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9674e-04 - val_loss: 7.8396e-04
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8596e-04 - val_loss: 7.9455e-04
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9654e-04 - val_loss: 7.8377e-04
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8577e-04 - val_loss: 7.9434e-04
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9632e-04 - val_loss: 7.8359e-04
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8558e-04 - val_loss: 7.9413e-04
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9611e-04 - val_loss: 7.8339e-04
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8539e-04 - val_loss: 7.9392e-04
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9591e-04 - val_loss: 7.8323e-04
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8523e-04 - val_loss: 7.9374e-04
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9572e-04 - val_loss: 7.8306e-04
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8505e-04 - val_loss: 7.9353e-04
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9552e-04 - val_loss: 7.8287e-04
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8486e-04 - val_loss: 7.9332e-04
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9531e-04 - val_loss: 7.8269e-04
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8468e-04 - val_loss: 7.9313e-04
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9511e-04 - val_loss: 7.8252e-04
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8451e-04 - val_loss: 7.9292e-04
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9491e-04 - val_loss: 7.8233e-04
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8432e-04 - val_loss: 7.9271e-04
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9469e-04 - val_loss: 7.8214e-04
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8413e-04 - val_loss: 7.9252e-04
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9450e-04 - val_loss: 7.8198e-04
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8398e-04 - val_loss: 7.9234e-04
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9432e-04 - val_loss: 7.8181e-04
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8381e-04 - val_loss: 7.9214e-04
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9412e-04 - val_loss: 7.8164e-04
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8363e-04 - val_loss: 7.9194e-04
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9392e-04 - val_loss: 7.8146e-04
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8345e-04 - val_loss: 7.9173e-04
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9371e-04 - val_loss: 7.8127e-04
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8326e-04 - val_loss: 7.9153e-04
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9351e-04 - val_loss: 7.8109e-04
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8308e-04 - val_loss: 7.9133e-04
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9331e-04 - val_loss: 7.8092e-04
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8291e-04 - val_loss: 7.9114e-04
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9312e-04 - val_loss: 7.8074e-04
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8273e-04 - val_loss: 7.9095e-04
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9293e-04 - val_loss: 7.8058e-04
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8257e-04 - val_loss: 7.9077e-04
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9275e-04 - val_loss: 7.8043e-04
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8242e-04 - val_loss: 7.9059e-04
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9256e-04 - val_loss: 7.8025e-04
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8224e-04 - val_loss: 7.9038e-04
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9236e-04 - val_loss: 7.8006e-04
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8205e-04 - val_loss: 7.9016e-04
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9214e-04 - val_loss: 7.7987e-04
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8185e-04 - val_loss: 7.8996e-04
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9193e-04 - val_loss: 7.7969e-04
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8168e-04 - val_loss: 7.8977e-04
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9174e-04 - val_loss: 7.7953e-04
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8152e-04 - val_loss: 7.8958e-04
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9156e-04 - val_loss: 7.7936e-04
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8134e-04 - val_loss: 7.8939e-04
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9137e-04 - val_loss: 7.7919e-04
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8118e-04 - val_loss: 7.8922e-04
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9119e-04 - val_loss: 7.7903e-04
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8102e-04 - val_loss: 7.8903e-04
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9101e-04 - val_loss: 7.7887e-04
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8086e-04 - val_loss: 7.8885e-04
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9082e-04 - val_loss: 7.7870e-04
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8068e-04 - val_loss: 7.8864e-04
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9061e-04 - val_loss: 7.7849e-04
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8048e-04 - val_loss: 7.8841e-04
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9038e-04 - val_loss: 7.7830e-04
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8029e-04 - val_loss: 7.8822e-04
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9019e-04 - val_loss: 7.7815e-04
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8013e-04 - val_loss: 7.8806e-04
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.9003e-04 - val_loss: 7.7801e-04
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7999e-04 - val_loss: 7.8790e-04
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8987e-04 - val_loss: 7.7786e-04
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7984e-04 - val_loss: 7.8771e-04
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8968e-04 - val_loss: 7.7768e-04
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7966e-04 - val_loss: 7.8750e-04
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8947e-04 - val_loss: 7.7748e-04
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7946e-04 - val_loss: 7.8728e-04
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8925e-04 - val_loss: 7.7730e-04
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7928e-04 - val_loss: 7.8710e-04
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8906e-04 - val_loss: 7.7713e-04
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7911e-04 - val_loss: 7.8691e-04
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8888e-04 - val_loss: 7.7697e-04
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7895e-04 - val_loss: 7.8674e-04
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8870e-04 - val_loss: 7.7682e-04
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7880e-04 - val_loss: 7.8656e-04
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8853e-04 - val_loss: 7.7666e-04
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7864e-04 - val_loss: 7.8638e-04
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8834e-04 - val_loss: 7.7648e-04
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7846e-04 - val_loss: 7.8618e-04
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8814e-04 - val_loss: 7.7630e-04
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7828e-04 - val_loss: 7.8597e-04
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8794e-04 - val_loss: 7.7612e-04
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7810e-04 - val_loss: 7.8577e-04
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8774e-04 - val_loss: 7.7594e-04
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7792e-04 - val_loss: 7.8559e-04
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8755e-04 - val_loss: 7.7579e-04
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7777e-04 - val_loss: 7.8542e-04
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8738e-04 - val_loss: 7.7563e-04
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7760e-04 - val_loss: 7.8523e-04
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8719e-04 - val_loss: 7.7546e-04
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7744e-04 - val_loss: 7.8505e-04
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8700e-04 - val_loss: 7.7529e-04
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7727e-04 - val_loss: 7.8486e-04
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8681e-04 - val_loss: 7.7513e-04
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7710e-04 - val_loss: 7.8468e-04
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8664e-04 - val_loss: 7.7496e-04
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7694e-04 - val_loss: 7.8449e-04
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8644e-04 - val_loss: 7.7478e-04
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7676e-04 - val_loss: 7.8429e-04
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8625e-04 - val_loss: 7.7461e-04
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7659e-04 - val_loss: 7.8410e-04
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8605e-04 - val_loss: 7.7443e-04
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7641e-04 - val_loss: 7.8390e-04
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8585e-04 - val_loss: 7.7426e-04
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7623e-04 - val_loss: 7.8371e-04
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8566e-04 - val_loss: 7.7409e-04
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7606e-04 - val_loss: 7.8353e-04
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8549e-04 - val_loss: 7.7394e-04
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7591e-04 - val_loss: 7.8336e-04
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8532e-04 - val_loss: 7.7378e-04
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7576e-04 - val_loss: 7.8318e-04
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8513e-04 - val_loss: 7.7361e-04
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7558e-04 - val_loss: 7.8298e-04
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8494e-04 - val_loss: 7.7343e-04
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7541e-04 - val_loss: 7.8279e-04
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8474e-04 - val_loss: 7.7325e-04
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7523e-04 - val_loss: 7.8260e-04
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8455e-04 - val_loss: 7.7309e-04
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7506e-04 - val_loss: 7.8241e-04
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8436e-04 - val_loss: 7.7292e-04
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7489e-04 - val_loss: 7.8222e-04
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8417e-04 - val_loss: 7.7274e-04
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7471e-04 - val_loss: 7.8203e-04
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8398e-04 - val_loss: 7.7259e-04
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7456e-04 - val_loss: 7.8186e-04
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8381e-04 - val_loss: 7.7242e-04
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7439e-04 - val_loss: 7.8167e-04
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8362e-04 - val_loss: 7.7225e-04
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7422e-04 - val_loss: 7.8148e-04
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8343e-04 - val_loss: 7.7208e-04
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7405e-04 - val_loss: 7.8129e-04
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8323e-04 - val_loss: 7.7190e-04
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7387e-04 - val_loss: 7.8110e-04
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8305e-04 - val_loss: 7.7173e-04
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7370e-04 - val_loss: 7.8091e-04
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8285e-04 - val_loss: 7.7156e-04
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7353e-04 - val_loss: 7.8072e-04
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8267e-04 - val_loss: 7.7139e-04
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7336e-04 - val_loss: 7.8054e-04
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8248e-04 - val_loss: 7.7123e-04
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7320e-04 - val_loss: 7.8036e-04
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8231e-04 - val_loss: 7.7106e-04
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7303e-04 - val_loss: 7.8016e-04
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8210e-04 - val_loss: 7.7087e-04
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7284e-04 - val_loss: 7.7995e-04
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8190e-04 - val_loss: 7.7069e-04
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7266e-04 - val_loss: 7.7977e-04
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8171e-04 - val_loss: 7.7053e-04
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7249e-04 - val_loss: 7.7958e-04
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8153e-04 - val_loss: 7.7036e-04
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7233e-04 - val_loss: 7.7940e-04
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8134e-04 - val_loss: 7.7020e-04
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7216e-04 - val_loss: 7.7922e-04
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8116e-04 - val_loss: 7.7004e-04
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7201e-04 - val_loss: 7.7905e-04
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8099e-04 - val_loss: 7.6988e-04
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7184e-04 - val_loss: 7.7885e-04
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8079e-04 - val_loss: 7.6969e-04
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7166e-04 - val_loss: 7.7865e-04
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8058e-04 - val_loss: 7.6950e-04
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7147e-04 - val_loss: 7.7844e-04
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8038e-04 - val_loss: 7.6932e-04
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7129e-04 - val_loss: 7.7824e-04
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8018e-04 - val_loss: 7.6915e-04
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7111e-04 - val_loss: 7.7806e-04
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.8000e-04 - val_loss: 7.6898e-04
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7095e-04 - val_loss: 7.7788e-04
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7981e-04 - val_loss: 7.6882e-04
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7079e-04 - val_loss: 7.7770e-04
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7964e-04 - val_loss: 7.6866e-04
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7062e-04 - val_loss: 7.7752e-04
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7945e-04 - val_loss: 7.6848e-04
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7044e-04 - val_loss: 7.7731e-04
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7924e-04 - val_loss: 7.6830e-04
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7026e-04 - val_loss: 7.7712e-04
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7905e-04 - val_loss: 7.6812e-04
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7008e-04 - val_loss: 7.7692e-04
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7885e-04 - val_loss: 7.6793e-04
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6989e-04 - val_loss: 7.7671e-04
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7864e-04 - val_loss: 7.6776e-04
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6972e-04 - val_loss: 7.7654e-04
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7847e-04 - val_loss: 7.6761e-04
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6957e-04 - val_loss: 7.7636e-04
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7829e-04 - val_loss: 7.6744e-04
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6940e-04 - val_loss: 7.7617e-04
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7810e-04 - val_loss: 7.6725e-04
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6921e-04 - val_loss: 7.7596e-04
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7789e-04 - val_loss: 7.6706e-04
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6902e-04 - val_loss: 7.7575e-04
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7768e-04 - val_loss: 7.6688e-04
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6884e-04 - val_loss: 7.7556e-04
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7749e-04 - val_loss: 7.6671e-04
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6867e-04 - val_loss: 7.7537e-04
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7730e-04 - val_loss: 7.6654e-04
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6849e-04 - val_loss: 7.7518e-04
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7711e-04 - val_loss: 7.6636e-04
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6832e-04 - val_loss: 7.7499e-04
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7691e-04 - val_loss: 7.6618e-04
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6813e-04 - val_loss: 7.7479e-04
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7671e-04 - val_loss: 7.6600e-04
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6796e-04 - val_loss: 7.7460e-04
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7652e-04 - val_loss: 7.6583e-04
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6779e-04 - val_loss: 7.7440e-04
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7632e-04 - val_loss: 7.6564e-04
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6760e-04 - val_loss: 7.7420e-04
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7612e-04 - val_loss: 7.6546e-04
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6742e-04 - val_loss: 7.7400e-04
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7592e-04 - val_loss: 7.6527e-04
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6723e-04 - val_loss: 7.7379e-04
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7571e-04 - val_loss: 7.6509e-04
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6704e-04 - val_loss: 7.7359e-04
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7551e-04 - val_loss: 7.6490e-04
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6686e-04 - val_loss: 7.7340e-04
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7532e-04 - val_loss: 7.6474e-04
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6669e-04 - val_loss: 7.7322e-04
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7514e-04 - val_loss: 7.6456e-04
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6652e-04 - val_loss: 7.7302e-04
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7494e-04 - val_loss: 7.6438e-04
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6633e-04 - val_loss: 7.7280e-04
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7472e-04 - val_loss: 7.6418e-04
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6613e-04 - val_loss: 7.7259e-04
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7451e-04 - val_loss: 7.6398e-04
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6593e-04 - val_loss: 7.7238e-04
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7430e-04 - val_loss: 7.6380e-04
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6575e-04 - val_loss: 7.7219e-04
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7410e-04 - val_loss: 7.6362e-04
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6557e-04 - val_loss: 7.7200e-04
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7392e-04 - val_loss: 7.6346e-04
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6541e-04 - val_loss: 7.7182e-04
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7373e-04 - val_loss: 7.6328e-04
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6523e-04 - val_loss: 7.7160e-04
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7351e-04 - val_loss: 7.6307e-04
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6502e-04 - val_loss: 7.7138e-04
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7329e-04 - val_loss: 7.6287e-04
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6482e-04 - val_loss: 7.7117e-04
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7308e-04 - val_loss: 7.6269e-04
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6464e-04 - val_loss: 7.7097e-04
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7288e-04 - val_loss: 7.6250e-04
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6445e-04 - val_loss: 7.7077e-04
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7268e-04 - val_loss: 7.6232e-04
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6427e-04 - val_loss: 7.7057e-04
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7248e-04 - val_loss: 7.6214e-04
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6409e-04 - val_loss: 7.7037e-04
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7227e-04 - val_loss: 7.6194e-04
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6388e-04 - val_loss: 7.7014e-04
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7205e-04 - val_loss: 7.6174e-04
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6369e-04 - val_loss: 7.6994e-04
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7184e-04 - val_loss: 7.6155e-04
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6350e-04 - val_loss: 7.6973e-04
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7164e-04 - val_loss: 7.6136e-04
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6331e-04 - val_loss: 7.6952e-04
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7143e-04 - val_loss: 7.6117e-04
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6312e-04 - val_loss: 7.6931e-04
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7122e-04 - val_loss: 7.6098e-04
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6292e-04 - val_loss: 7.6910e-04
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7100e-04 - val_loss: 7.6078e-04
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6272e-04 - val_loss: 7.6888e-04
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7078e-04 - val_loss: 7.6058e-04
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6252e-04 - val_loss: 7.6867e-04
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7057e-04 - val_loss: 7.6038e-04
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6233e-04 - val_loss: 7.6846e-04
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7036e-04 - val_loss: 7.6020e-04
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6214e-04 - val_loss: 7.6826e-04
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.7016e-04 - val_loss: 7.6001e-04
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6195e-04 - val_loss: 7.6804e-04
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6994e-04 - val_loss: 7.5979e-04
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6174e-04 - val_loss: 7.6782e-04
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6972e-04 - val_loss: 7.5960e-04
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6154e-04 - val_loss: 7.6759e-04
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6949e-04 - val_loss: 7.5939e-04
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6133e-04 - val_loss: 7.6737e-04
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6927e-04 - val_loss: 7.5919e-04
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6113e-04 - val_loss: 7.6716e-04
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6906e-04 - val_loss: 7.5900e-04
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6094e-04 - val_loss: 7.6697e-04
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6886e-04 - val_loss: 7.5882e-04
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6076e-04 - val_loss: 7.6675e-04
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6865e-04 - val_loss: 7.5861e-04
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6055e-04 - val_loss: 7.6652e-04
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6841e-04 - val_loss: 7.5838e-04
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6032e-04 - val_loss: 7.6627e-04
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6817e-04 - val_loss: 7.5816e-04
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6010e-04 - val_loss: 7.6605e-04
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6794e-04 - val_loss: 7.5796e-04
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5990e-04 - val_loss: 7.6583e-04
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6773e-04 - val_loss: 7.5777e-04
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5971e-04 - val_loss: 7.6562e-04
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6751e-04 - val_loss: 7.5757e-04
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5950e-04 - val_loss: 7.6540e-04
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6729e-04 - val_loss: 7.5737e-04
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5931e-04 - val_loss: 7.6518e-04
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6707e-04 - val_loss: 7.5716e-04
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5910e-04 - val_loss: 7.6495e-04
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6684e-04 - val_loss: 7.5694e-04
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5888e-04 - val_loss: 7.6471e-04
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6660e-04 - val_loss: 7.5672e-04
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5865e-04 - val_loss: 7.6447e-04
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6636e-04 - val_loss: 7.5650e-04
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5843e-04 - val_loss: 7.6425e-04
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6613e-04 - val_loss: 7.5630e-04
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5824e-04 - val_loss: 7.6404e-04
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6593e-04 - val_loss: 7.5610e-04
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5804e-04 - val_loss: 7.6382e-04
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6570e-04 - val_loss: 7.5590e-04
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5783e-04 - val_loss: 7.6358e-04
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6547e-04 - val_loss: 7.5567e-04
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.5760e-04 - val_loss: 7.6333e-04
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00044
10/10 - 0s - loss: 7.6521e-04 - val_loss: 7.5543e-04
Train on 10 samples, validate on 10 samples
Epoch 1/512
10/10 - 1s - loss: 0.0994 - val_loss: 0.0246
Epoch 2/512
10/10 - 0s - loss: 0.0332 - val_loss: 0.0031
Epoch 3/512
10/10 - 0s - loss: 0.0257 - val_loss: 0.0055
Epoch 4/512
10/10 - 0s - loss: 0.0219 - val_loss: 0.0050
Epoch 5/512
10/10 - 0s - loss: 0.0204 - val_loss: 0.0030
Epoch 6/512
10/10 - 0s - loss: 0.0201 - val_loss: 8.1100e-04
Epoch 7/512
10/10 - 0s - loss: 0.0138 - val_loss: 4.9082e-04
Epoch 8/512
10/10 - 0s - loss: 0.0112 - val_loss: 0.0015
Epoch 9/512
10/10 - 0s - loss: 0.0094 - val_loss: 0.0016
Epoch 10/512
10/10 - 0s - loss: 0.0081 - val_loss: 0.0027
Epoch 11/512
10/10 - 0s - loss: 0.0071 - val_loss: 0.0026
Epoch 12/512
10/10 - 0s - loss: 0.0064 - val_loss: 0.0036
Epoch 13/512
10/10 - 0s - loss: 0.0059 - val_loss: 0.0031
Epoch 14/512
10/10 - 0s - loss: 0.0054 - val_loss: 0.0040
Epoch 15/512
10/10 - 0s - loss: 0.0050 - val_loss: 0.0033
Epoch 16/512
10/10 - 0s - loss: 0.0047 - val_loss: 0.0044
Epoch 17/512
10/10 - 0s - loss: 0.0044 - val_loss: 0.0031
Epoch 18/512
10/10 - 0s - loss: 0.0043 - val_loss: 0.0047
Epoch 19/512
10/10 - 0s - loss: 0.0042 - val_loss: 0.0025
Epoch 20/512
10/10 - 0s - loss: 0.0043 - val_loss: 0.0052
Epoch 21/512
10/10 - 0s - loss: 0.0044 - val_loss: 0.0018
Epoch 22/512
10/10 - 0s - loss: 0.0049 - val_loss: 0.0050
Epoch 23/512
10/10 - 0s - loss: 0.0044 - val_loss: 0.0016
Epoch 24/512
10/10 - 0s - loss: 0.0045 - val_loss: 0.0040
Epoch 25/512
10/10 - 0s - loss: 0.0035 - val_loss: 0.0018
Epoch 26/512
10/10 - 0s - loss: 0.0033 - val_loss: 0.0032
Epoch 27/512
10/10 - 0s - loss: 0.0029 - val_loss: 0.0019
Epoch 28/512
10/10 - 0s - loss: 0.0028 - val_loss: 0.0027
Epoch 29/512
10/10 - 0s - loss: 0.0026 - val_loss: 0.0018
Epoch 30/512
10/10 - 0s - loss: 0.0024 - val_loss: 0.0023
Epoch 31/512
10/10 - 0s - loss: 0.0023 - val_loss: 0.0016
Epoch 32/512
10/10 - 0s - loss: 0.0022 - val_loss: 0.0020
Epoch 33/512
10/10 - 0s - loss: 0.0021 - val_loss: 0.0014
Epoch 34/512
10/10 - 0s - loss: 0.0021 - val_loss: 0.0016
Epoch 35/512
10/10 - 0s - loss: 0.0020 - val_loss: 0.0011
Epoch 36/512
10/10 - 0s - loss: 0.0019 - val_loss: 0.0013
Epoch 37/512
10/10 - 0s - loss: 0.0019 - val_loss: 8.8820e-04
Epoch 38/512
10/10 - 0s - loss: 0.0018 - val_loss: 0.0011
Epoch 39/512
10/10 - 0s - loss: 0.0018 - val_loss: 6.7403e-04
Epoch 40/512
10/10 - 0s - loss: 0.0018 - val_loss: 8.6575e-04
Epoch 41/512
10/10 - 0s - loss: 0.0017 - val_loss: 4.9972e-04
Epoch 42/512
10/10 - 0s - loss: 0.0017 - val_loss: 6.6458e-04
Epoch 43/512
10/10 - 0s - loss: 0.0016 - val_loss: 3.6945e-04
Epoch 44/512
10/10 - 0s - loss: 0.0016 - val_loss: 4.9419e-04
Epoch 45/512
10/10 - 0s - loss: 0.0015 - val_loss: 2.8049e-04
Epoch 46/512
10/10 - 0s - loss: 0.0015 - val_loss: 3.6165e-04
Epoch 47/512
10/10 - 0s - loss: 0.0014 - val_loss: 2.1798e-04
Epoch 48/512
10/10 - 0s - loss: 0.0014 - val_loss: 2.6656e-04
Epoch 49/512
10/10 - 0s - loss: 0.0014 - val_loss: 1.7133e-04
Epoch 50/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.9882e-04
Epoch 51/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.3272e-04
Epoch 52/512
10/10 - 0s - loss: 0.0013 - val_loss: 1.4860e-04
Epoch 53/512
10/10 - 0s - loss: 0.0012 - val_loss: 9.8202e-05
Epoch 54/512
10/10 - 0s - loss: 0.0012 - val_loss: 1.0815e-04
Epoch 55/512
10/10 - 0s - loss: 0.0012 - val_loss: 6.7057e-05
Epoch 56/512
10/10 - 0s - loss: 0.0012 - val_loss: 7.3944e-05
Epoch 57/512
10/10 - 0s - loss: 0.0012 - val_loss: 3.9715e-05
Epoch 58/512
10/10 - 0s - loss: 0.0012 - val_loss: 4.4045e-05
Epoch 59/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.6922e-05
Epoch 60/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.9120e-05
Epoch 61/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.8396e-06
Epoch 62/512
10/10 - 0s - loss: 0.0011 - val_loss: 3.8227e-06
Epoch 63/512
10/10 - 0s - loss: 0.0011 - val_loss: 3.2601e-06
Epoch 64/512
10/10 - 0s - loss: 0.0011 - val_loss: 2.1905e-06
Epoch 65/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.9047e-05
Epoch 66/512
10/10 - 0s - loss: 0.0011 - val_loss: 1.2478e-05
Epoch 67/512
10/10 - 0s - loss: 0.0011 - val_loss: 3.5846e-05
Epoch 68/512
10/10 - 0s - loss: 0.0010 - val_loss: 2.7127e-05
Epoch 69/512
10/10 - 0s - loss: 0.0010 - val_loss: 5.2667e-05
Epoch 70/512
10/10 - 0s - loss: 9.5989e-04 - val_loss: 4.7444e-05
Epoch 71/512
10/10 - 0s - loss: 9.3645e-04 - val_loss: 9.4000e-05
Epoch 72/512
10/10 - 0s - loss: 8.9926e-04 - val_loss: 8.1799e-05
Epoch 73/512
10/10 - 0s - loss: 9.0103e-04 - val_loss: 1.4302e-04
Epoch 74/512
10/10 - 0s - loss: 8.8419e-04 - val_loss: 1.1859e-04
Epoch 75/512
10/10 - 0s - loss: 9.1784e-04 - val_loss: 1.5258e-04
Epoch 76/512
10/10 - 0s - loss: 8.8294e-04 - val_loss: 1.5627e-04
Epoch 77/512
10/10 - 0s - loss: 9.1507e-04 - val_loss: 1.3712e-04
Epoch 78/512
10/10 - 0s - loss: 8.4159e-04 - val_loss: 2.0435e-04
Epoch 79/512
10/10 - 0s - loss: 8.4224e-04 - val_loss: 1.6168e-04
Epoch 80/512
10/10 - 0s - loss: 7.9388e-04 - val_loss: 2.7056e-04
Epoch 81/512
10/10 - 0s - loss: 8.0124e-04 - val_loss: 1.7862e-04
Epoch 82/512
10/10 - 0s - loss: 7.8080e-04 - val_loss: 3.3068e-04
Epoch 83/512
10/10 - 0s - loss: 8.0896e-04 - val_loss: 1.5287e-04
Epoch 84/512
10/10 - 0s - loss: 7.7912e-04 - val_loss: 3.6972e-04
Epoch 85/512
10/10 - 0s - loss: 7.9341e-04 - val_loss: 1.4568e-04
Epoch 86/512
10/10 - 0s - loss: 7.4424e-04 - val_loss: 3.9718e-04
Epoch 87/512
10/10 - 0s - loss: 7.3620e-04 - val_loss: 1.7709e-04
Epoch 88/512
10/10 - 0s - loss: 7.0240e-04 - val_loss: 3.8898e-04
Epoch 89/512
10/10 - 0s - loss: 7.0107e-04 - val_loss: 2.3219e-04
Epoch 90/512
10/10 - 0s - loss: 6.8450e-04 - val_loss: 2.9152e-04
Epoch 91/512
10/10 - 0s - loss: 7.0797e-04 - val_loss: 3.5353e-04
Epoch 92/512
10/10 - 0s - loss: 7.0602e-04 - val_loss: 1.4749e-04
Epoch 93/512
10/10 - 0s - loss: 6.8617e-04 - val_loss: 6.1632e-04
Epoch 94/512
10/10 - 0s - loss: 7.2485e-04 - val_loss: 1.4192e-04
Epoch 95/512
10/10 - 0s - loss: 6.1558e-04 - val_loss: 7.2679e-04
Epoch 96/512
10/10 - 0s - loss: 6.5007e-04 - val_loss: 1.2593e-04
Epoch 97/512
10/10 - 0s - loss: 5.9445e-04 - val_loss: 5.8647e-04
Epoch 98/512
10/10 - 0s - loss: 5.8879e-04 - val_loss: 1.3196e-04
Epoch 99/512
10/10 - 0s - loss: 5.4724e-04 - val_loss: 5.2353e-04
Epoch 100/512
10/10 - 0s - loss: 5.3841e-04 - val_loss: 1.2094e-04
Epoch 101/512
10/10 - 0s - loss: 5.1968e-04 - val_loss: 4.8130e-04
Epoch 102/512
10/10 - 0s - loss: 5.1412e-04 - val_loss: 1.0747e-04
Epoch 103/512
10/10 - 0s - loss: 5.0059e-04 - val_loss: 4.3090e-04
Epoch 104/512
10/10 - 0s - loss: 5.2151e-04 - val_loss: 1.0736e-04
Epoch 105/512
10/10 - 0s - loss: 5.4450e-04 - val_loss: 1.7401e-04
Epoch 106/512
10/10 - 0s - loss: 5.6113e-04 - val_loss: 1.7301e-04
Epoch 107/512
10/10 - 0s - loss: 5.5586e-04 - val_loss: 1.0348e-04
Epoch 108/512
10/10 - 0s - loss: 4.2093e-04 - val_loss: 2.1454e-04
Epoch 109/512
10/10 - 0s - loss: 4.0361e-04 - val_loss: 1.1934e-04
Epoch 110/512
10/10 - 0s - loss: 4.1093e-04 - val_loss: 2.0039e-04
Epoch 111/512
10/10 - 0s - loss: 3.9442e-04 - val_loss: 1.1763e-04
Epoch 112/512
10/10 - 0s - loss: 4.3378e-04 - val_loss: 1.9429e-04
Epoch 113/512
10/10 - 0s - loss: 3.4869e-04 - val_loss: 1.6881e-04
Epoch 114/512
10/10 - 0s - loss: 3.5799e-04 - val_loss: 2.0764e-04
Epoch 115/512
10/10 - 0s - loss: 3.3517e-04 - val_loss: 1.1972e-04
Epoch 116/512
10/10 - 0s - loss: 3.6322e-04 - val_loss: 2.0419e-04
Epoch 117/512
10/10 - 0s - loss: 2.9469e-04 - val_loss: 1.3704e-04
Epoch 118/512
10/10 - 0s - loss: 3.0330e-04 - val_loss: 2.0887e-04
Epoch 119/512
10/10 - 0s - loss: 2.7601e-04 - val_loss: 9.9964e-05
Epoch 120/512
10/10 - 0s - loss: 2.9737e-04 - val_loss: 1.9867e-04
Epoch 121/512
10/10 - 0s - loss: 2.4221e-04 - val_loss: 1.0420e-04
Epoch 122/512
10/10 - 0s - loss: 2.5134e-04 - val_loss: 1.9068e-04
Epoch 123/512
10/10 - 0s - loss: 2.2238e-04 - val_loss: 8.2697e-05
Epoch 124/512
10/10 - 0s - loss: 2.3720e-04 - val_loss: 1.7425e-04
Epoch 125/512
10/10 - 0s - loss: 1.9553e-04 - val_loss: 8.2223e-05
Epoch 126/512
10/10 - 0s - loss: 2.0267e-04 - val_loss: 1.6129e-04
Epoch 127/512
10/10 - 0s - loss: 1.7605e-04 - val_loss: 6.7227e-05
Epoch 128/512
10/10 - 0s - loss: 1.8492e-04 - val_loss: 1.4658e-04
Epoch 129/512
10/10 - 0s - loss: 1.5495e-04 - val_loss: 6.1225e-05
Epoch 130/512
10/10 - 0s - loss: 1.6077e-04 - val_loss: 1.3566e-04
Epoch 131/512
10/10 - 0s - loss: 1.3739e-04 - val_loss: 5.1792e-05
Epoch 132/512
10/10 - 0s - loss: 1.4287e-04 - val_loss: 1.2289e-04
Epoch 133/512
10/10 - 0s - loss: 1.2075e-04 - val_loss: 4.5378e-05
Epoch 134/512
10/10 - 0s - loss: 1.2497e-04 - val_loss: 1.1179e-04
Epoch 135/512
10/10 - 0s - loss: 1.0575e-04 - val_loss: 3.8492e-05
Epoch 136/512
10/10 - 0s - loss: 1.0939e-04 - val_loss: 1.0075e-04
Epoch 137/512
10/10 - 0s - loss: 9.2335e-05 - val_loss: 3.2802e-05
Epoch 138/512
10/10 - 0s - loss: 9.5749e-05 - val_loss: 9.1389e-05
Epoch 139/512
10/10 - 0s - loss: 8.0890e-05 - val_loss: 2.8283e-05
Epoch 140/512
10/10 - 0s - loss: 8.4227e-05 - val_loss: 8.2959e-05
Epoch 141/512
10/10 - 0s - loss: 7.1069e-05 - val_loss: 2.4664e-05
Epoch 142/512
10/10 - 0s - loss: 7.4494e-05 - val_loss: 7.6075e-05
Epoch 143/512
10/10 - 0s - loss: 6.2684e-05 - val_loss: 2.1658e-05
Epoch 144/512
10/10 - 0s - loss: 6.5993e-05 - val_loss: 6.9323e-05
Epoch 145/512
10/10 - 0s - loss: 5.5018e-05 - val_loss: 1.8760e-05
Epoch 146/512
10/10 - 0s - loss: 5.8243e-05 - val_loss: 6.3451e-05
Epoch 147/512
10/10 - 0s - loss: 4.8354e-05 - val_loss: 1.6260e-05
Epoch 148/512
10/10 - 0s - loss: 5.1593e-05 - val_loss: 5.8769e-05
Epoch 149/512
10/10 - 0s - loss: 4.2624e-05 - val_loss: 1.4129e-05
Epoch 150/512
10/10 - 0s - loss: 4.5901e-05 - val_loss: 5.4465e-05
Epoch 151/512
10/10 - 0s - loss: 3.7586e-05 - val_loss: 1.2359e-05
Epoch 152/512
10/10 - 0s - loss: 4.0832e-05 - val_loss: 5.0240e-05
Epoch 153/512
10/10 - 0s - loss: 3.3134e-05 - val_loss: 1.0810e-05
Epoch 154/512
10/10 - 0s - loss: 3.6339e-05 - val_loss: 4.6698e-05
Epoch 155/512
10/10 - 0s - loss: 2.9258e-05 - val_loss: 9.4526e-06
Epoch 156/512
10/10 - 0s - loss: 3.2448e-05 - val_loss: 4.3611e-05
Epoch 157/512
10/10 - 0s - loss: 2.5868e-05 - val_loss: 8.2898e-06
Epoch 158/512
10/10 - 0s - loss: 2.9054e-05 - val_loss: 4.0814e-05
Epoch 159/512
10/10 - 0s - loss: 2.2952e-05 - val_loss: 7.3412e-06
Epoch 160/512
10/10 - 0s - loss: 2.6135e-05 - val_loss: 3.8286e-05
Epoch 161/512
10/10 - 0s - loss: 2.0461e-05 - val_loss: 6.5744e-06
Epoch 162/512
10/10 - 0s - loss: 2.3623e-05 - val_loss: 3.6026e-05
Epoch 163/512
10/10 - 0s - loss: 1.8243e-05 - val_loss: 5.8630e-06
Epoch 164/512
10/10 - 0s - loss: 2.1311e-05 - val_loss: 3.3798e-05
Epoch 165/512
10/10 - 0s - loss: 1.6202e-05 - val_loss: 5.2128e-06
Epoch 166/512
10/10 - 0s - loss: 1.9124e-05 - val_loss: 3.1853e-05
Epoch 167/512
10/10 - 0s - loss: 1.4384e-05 - val_loss: 4.5867e-06
Epoch 168/512
10/10 - 0s - loss: 1.7216e-05 - val_loss: 3.0168e-05
Epoch 169/512
10/10 - 0s - loss: 1.2798e-05 - val_loss: 4.1002e-06
Epoch 170/512
10/10 - 0s - loss: 1.5527e-05 - val_loss: 2.8318e-05
Epoch 171/512
10/10 - 0s - loss: 1.1394e-05 - val_loss: 3.6981e-06
Epoch 172/512
10/10 - 0s - loss: 1.4009e-05 - val_loss: 2.6488e-05
Epoch 173/512
10/10 - 0s - loss: 1.0147e-05 - val_loss: 3.3502e-06
Epoch 174/512
10/10 - 0s - loss: 1.2640e-05 - val_loss: 2.4886e-05
Epoch 175/512
10/10 - 0s - loss: 9.0223e-06 - val_loss: 3.0101e-06
Epoch 176/512
10/10 - 0s - loss: 1.1394e-05 - val_loss: 2.3287e-05
Epoch 177/512
10/10 - 0s - loss: 8.0003e-06 - val_loss: 2.7595e-06
Epoch 178/512
10/10 - 0s - loss: 1.0221e-05 - val_loss: 2.1807e-05
Epoch 179/512
10/10 - 0s - loss: 7.1066e-06 - val_loss: 2.5087e-06
Epoch 180/512
10/10 - 0s - loss: 9.2323e-06 - val_loss: 2.0201e-05
Epoch 181/512
10/10 - 0s - loss: 6.2942e-06 - val_loss: 2.3772e-06
Epoch 182/512
10/10 - 0s - loss: 8.2675e-06 - val_loss: 1.8807e-05
Epoch 183/512
10/10 - 0s - loss: 5.6333e-06 - val_loss: 2.2031e-06
Epoch 184/512
10/10 - 0s - loss: 7.5602e-06 - val_loss: 1.7308e-05
Epoch 185/512
10/10 - 0s - loss: 4.9830e-06 - val_loss: 2.1448e-06
Epoch 186/512
10/10 - 0s - loss: 6.7285e-06 - val_loss: 1.6224e-05
Epoch 187/512
10/10 - 0s - loss: 4.4990e-06 - val_loss: 2.0081e-06
Epoch 188/512
10/10 - 0s - loss: 6.2468e-06 - val_loss: 1.4783e-05
Epoch 189/512
10/10 - 0s - loss: 3.9465e-06 - val_loss: 2.0195e-06
Epoch 190/512
10/10 - 0s - loss: 5.4477e-06 - val_loss: 1.4134e-05
Epoch 191/512
10/10 - 0s - loss: 3.6552e-06 - val_loss: 1.9142e-06
Epoch 192/512
10/10 - 0s - loss: 5.2968e-06 - val_loss: 1.2471e-05
Epoch 193/512
10/10 - 0s - loss: 3.1049e-06 - val_loss: 1.9878e-06
Epoch 194/512
10/10 - 0s - loss: 4.3059e-06 - val_loss: 1.2585e-05
Epoch 195/512
10/10 - 0s - loss: 3.0805e-06 - val_loss: 1.9698e-06
Epoch 196/512
10/10 - 0s - loss: 4.7548e-06 - val_loss: 1.0038e-05
Epoch 197/512
10/10 - 0s - loss: 2.3396e-06 - val_loss: 2.1175e-06
Epoch 198/512
10/10 - 0s - loss: 3.0394e-06 - val_loss: 1.1492e-05
Epoch 199/512
10/10 - 0s - loss: 2.7366e-06 - val_loss: 2.4130e-06
Epoch 200/512
10/10 - 0s - loss: 4.6177e-06 - val_loss: 7.1739e-06
Epoch 201/512
10/10 - 0s - loss: 1.6431e-06 - val_loss: 2.8886e-06
Epoch 202/512
10/10 - 0s - loss: 1.7178e-06 - val_loss: 8.5335e-06
Epoch 203/512
10/10 - 0s - loss: 1.8990e-06 - val_loss: 2.0090e-06
Epoch 204/512
10/10 - 0s - loss: 3.0360e-06 - val_loss: 9.3448e-06
Epoch 205/512
10/10 - 0s - loss: 1.9961e-06 - val_loss: 2.7092e-06
Epoch 206/512
10/10 - 0s - loss: 3.5516e-06 - val_loss: 6.4987e-06
Epoch 207/512
10/10 - 0s - loss: 1.2715e-06 - val_loss: 2.1339e-06
Epoch 208/512
10/10 - 0s - loss: 1.4856e-06 - val_loss: 7.9669e-06
Epoch 209/512
10/10 - 0s - loss: 1.7333e-06 - val_loss: 3.1690e-06
Epoch 210/512
10/10 - 0s - loss: 3.2898e-06 - val_loss: 4.6813e-06
Epoch 211/512
10/10 - 0s - loss: 9.5124e-07 - val_loss: 2.4433e-06
Epoch 212/512
10/10 - 0s - loss: 9.4551e-07 - val_loss: 5.1267e-06
Epoch 213/512
10/10 - 0s - loss: 1.0277e-06 - val_loss: 2.0375e-06
Epoch 214/512
10/10 - 0s - loss: 1.5632e-06 - val_loss: 7.6537e-06
Epoch 215/512
10/10 - 0s - loss: 1.7091e-06 - val_loss: 6.2928e-06
Epoch 216/512
10/10 - 0s - loss: 3.5828e-06 - val_loss: 2.0676e-06
Epoch 217/512
10/10 - 0s - loss: 1.2485e-06 - val_loss: 3.1977e-06
Epoch 218/512
10/10 - 0s - loss: 9.9759e-07 - val_loss: 2.9171e-06
Epoch 219/512
10/10 - 0s - loss: 1.6072e-06 - val_loss: 5.0938e-06
Epoch 220/512
10/10 - 0s - loss: 8.3809e-07 - val_loss: 2.7093e-06
Epoch 221/512
10/10 - 0s - loss: 1.2175e-06 - val_loss: 4.9133e-06
Epoch 222/512
10/10 - 0s - loss: 9.0237e-07 - val_loss: 3.1711e-06
Epoch 223/512
10/10 - 0s - loss: 1.5491e-06 - val_loss: 3.0290e-06
Epoch 224/512
10/10 - 0s - loss: 5.3121e-07 - val_loss: 1.9572e-06
Epoch 225/512
10/10 - 0s - loss: 6.1048e-07 - val_loss: 3.3062e-06
Epoch 226/512
10/10 - 0s - loss: 6.9069e-07 - val_loss: 2.9987e-06
Epoch 227/512
10/10 - 0s - loss: 1.2606e-06 - val_loss: 2.9730e-06
Epoch 228/512
10/10 - 0s - loss: 5.1540e-07 - val_loss: 2.2748e-06
Epoch 229/512
10/10 - 0s - loss: 7.5104e-07 - val_loss: 3.2941e-06
Epoch 230/512
10/10 - 0s - loss: 6.9483e-07 - val_loss: 3.9246e-06
Epoch 231/512
10/10 - 0s - loss: 1.3635e-06 - val_loss: 1.9914e-06
Epoch 232/512
10/10 - 0s - loss: 3.4159e-07 - val_loss: 1.7662e-06
Epoch 233/512
10/10 - 0s - loss: 3.1185e-07 - val_loss: 1.7406e-06
Epoch 234/512
10/10 - 0s - loss: 2.9539e-07 - val_loss: 1.6868e-06
Epoch 235/512
10/10 - 0s - loss: 2.8352e-07 - val_loss: 1.6575e-06
Epoch 236/512
10/10 - 0s - loss: 2.7246e-07 - val_loss: 1.6035e-06
Epoch 237/512
10/10 - 0s - loss: 2.6231e-07 - val_loss: 1.6050e-06
Epoch 238/512
10/10 - 0s - loss: 2.5513e-07 - val_loss: 1.5629e-06
Epoch 239/512
10/10 - 0s - loss: 2.6753e-07 - val_loss: 2.0170e-06
Epoch 240/512
10/10 - 0s - loss: 4.0884e-07 - val_loss: 5.3499e-06
Epoch 241/512
10/10 - 0s - loss: 1.2630e-06 - val_loss: 2.9710e-06
Epoch 242/512
10/10 - 0s - loss: 4.4633e-07 - val_loss: 2.8490e-06
Epoch 243/512
10/10 - 0s - loss: 4.4021e-07 - val_loss: 2.2471e-06
Epoch 244/512
10/10 - 0s - loss: 7.0920e-07 - val_loss: 6.3294e-06
Epoch 245/512
10/10 - 0s - loss: 1.4340e-06 - val_loss: 4.2238e-06
Epoch 246/512
10/10 - 0s - loss: 1.0004e-06 - val_loss: 2.4271e-06
Epoch 247/512
10/10 - 0s - loss: 5.4433e-07 - val_loss: 1.0780e-06
Epoch 248/512
10/10 - 0s - loss: 1.5883e-07 - val_loss: 1.0544e-06
Epoch 249/512
10/10 - 0s - loss: 1.5120e-07 - val_loss: 1.2081e-06
Epoch 250/512
10/10 - 0s - loss: 1.5519e-07 - val_loss: 9.9069e-07
Epoch 251/512
10/10 - 0s - loss: 1.6409e-07 - val_loss: 1.5749e-06
Epoch 252/512
10/10 - 0s - loss: 2.2333e-07 - val_loss: 1.0797e-06
Epoch 253/512
10/10 - 0s - loss: 2.4987e-07 - val_loss: 3.0347e-06
Epoch 254/512
10/10 - 0s - loss: 4.9949e-07 - val_loss: 8.5117e-07
Epoch 255/512
10/10 - 0s - loss: 1.4686e-07 - val_loss: 1.1380e-06
Epoch 256/512
10/10 - 0s - loss: 1.1753e-07 - val_loss: 8.4630e-07
Epoch 257/512
10/10 - 0s - loss: 1.1335e-07 - val_loss: 1.1114e-06
Epoch 258/512
10/10 - 0s - loss: 1.2857e-07 - val_loss: 8.0133e-07
Epoch 259/512
10/10 - 0s - loss: 1.7663e-07 - val_loss: 2.6509e-06
Epoch 260/512
10/10 - 0s - loss: 4.0901e-07 - val_loss: 7.7145e-07
Epoch 261/512
10/10 - 0s - loss: 1.4375e-07 - val_loss: 1.4710e-06
Epoch 262/512
10/10 - 0s - loss: 1.5849e-07 - val_loss: 7.5608e-07
Epoch 263/512
10/10 - 0s - loss: 2.2198e-07 - val_loss: 3.1348e-06
Epoch 264/512
10/10 - 0s - loss: 4.8073e-07 - val_loss: 7.8549e-07
Epoch 265/512
10/10 - 0s - loss: 1.9437e-07 - val_loss: 7.3163e-07
Epoch 266/512
10/10 - 0s - loss: 1.0858e-07 - val_loss: 1.3978e-06
Epoch 267/512
10/10 - 0s - loss: 1.5544e-07 - val_loss: 5.6631e-07
Epoch 268/512
10/10 - 0s - loss: 1.4977e-07 - val_loss: 2.0635e-06
Epoch 269/512
10/10 - 0s - loss: 2.8053e-07 - val_loss: 4.7059e-07
Epoch 270/512
10/10 - 0s - loss: 7.9772e-08 - val_loss: 6.8120e-07
Epoch 271/512
10/10 - 0s - loss: 6.3904e-08 - val_loss: 6.4175e-07
Epoch 272/512
10/10 - 0s - loss: 6.0231e-08 - val_loss: 5.0391e-07
Epoch 273/512
10/10 - 0s - loss: 6.1927e-08 - val_loss: 8.0740e-07
Epoch 274/512
10/10 - 0s - loss: 7.7071e-08 - val_loss: 4.2006e-07
Epoch 275/512
10/10 - 0s - loss: 1.1768e-07 - val_loss: 2.2548e-06
Epoch 276/512
10/10 - 0s - loss: 2.9092e-07 - val_loss: 3.4598e-07
Epoch 277/512
10/10 - 0s - loss: 1.0265e-07 - val_loss: 7.2316e-07
Epoch 278/512
10/10 - 0s - loss: 7.6387e-08 - val_loss: 1.0139e-06
Epoch 279/512
10/10 - 0s - loss: 8.7952e-08 - val_loss: 3.7465e-07
Epoch 280/512
10/10 - 0s - loss: 1.4416e-07 - val_loss: 2.3975e-06
Epoch 281/512
10/10 - 0s - loss: 2.9798e-07 - val_loss: 4.6182e-07
Epoch 282/512
10/10 - 0s - loss: 1.7623e-07 - val_loss: 9.1526e-07
Epoch 283/512
10/10 - 0s - loss: 6.6076e-08 - val_loss: 3.1494e-07
Epoch 284/512
10/10 - 0s - loss: 6.2079e-08 - val_loss: 9.9695e-07
Epoch 285/512
10/10 - 0s - loss: 1.0554e-07 - val_loss: 2.4721e-07
Epoch 286/512
10/10 - 0s - loss: 6.2361e-08 - val_loss: 8.3930e-07
Epoch 287/512
10/10 - 0s - loss: 8.6382e-08 - val_loss: 2.4097e-07
Epoch 288/512
10/10 - 0s - loss: 5.6994e-08 - val_loss: 8.2422e-07
Epoch 289/512
10/10 - 0s - loss: 8.7543e-08 - val_loss: 2.3169e-07
Epoch 290/512
10/10 - 0s - loss: 5.1389e-08 - val_loss: 7.5089e-07
Epoch 291/512
10/10 - 0s - loss: 7.6971e-08 - val_loss: 2.2081e-07
Epoch 292/512
10/10 - 0s - loss: 5.2560e-08 - val_loss: 8.0280e-07
Epoch 293/512
10/10 - 0s - loss: 8.6655e-08 - val_loss: 2.0850e-07
Epoch 294/512
10/10 - 0s - loss: 4.5191e-08 - val_loss: 6.6728e-07
Epoch 295/512
10/10 - 0s - loss: 6.5529e-08 - val_loss: 1.9784e-07
Epoch 296/512
10/10 - 0s - loss: 5.1343e-08 - val_loss: 8.1640e-07
Epoch 297/512
10/10 - 0s - loss: 9.1233e-08 - val_loss: 1.8864e-07
Epoch 298/512
10/10 - 0s - loss: 3.6731e-08 - val_loss: 5.1416e-07
Epoch 299/512
10/10 - 0s - loss: 4.2870e-08 - val_loss: 1.8092e-07
Epoch 300/512
10/10 - 0s - loss: 4.9108e-08 - val_loss: 8.3207e-07
Epoch 301/512
10/10 - 0s - loss: 9.5350e-08 - val_loss: 1.6859e-07
Epoch 302/512
10/10 - 0s - loss: 3.2481e-08 - val_loss: 4.1571e-07
Epoch 303/512
10/10 - 0s - loss: 2.9603e-08 - val_loss: 1.7585e-07
Epoch 304/512
10/10 - 0s - loss: 3.5796e-08 - val_loss: 6.2124e-07
Epoch 305/512
10/10 - 0s - loss: 6.5505e-08 - val_loss: 1.4241e-07
Epoch 306/512
10/10 - 0s - loss: 4.3471e-08 - val_loss: 7.1432e-07
Epoch 307/512
10/10 - 0s - loss: 7.9093e-08 - val_loss: 1.3755e-07
Epoch 308/512
10/10 - 0s - loss: 3.1742e-08 - val_loss: 4.4474e-07
Epoch 309/512
10/10 - 0s - loss: 3.5474e-08 - val_loss: 1.3366e-07
Epoch 310/512
10/10 - 0s - loss: 4.3988e-08 - val_loss: 7.4582e-07
Epoch 311/512
10/10 - 0s - loss: 8.8439e-08 - val_loss: 1.4318e-07
Epoch 312/512
10/10 - 0s - loss: 2.7390e-08 - val_loss: 2.7906e-07
Epoch 313/512
10/10 - 0s - loss: 2.2687e-08 - val_loss: 2.4198e-07
Epoch 314/512
10/10 - 0s - loss: 1.9962e-08 - val_loss: 1.6204e-07
Epoch 315/512
10/10 - 0s - loss: 2.1859e-08 - val_loss: 3.6612e-07
Epoch 316/512
10/10 - 0s - loss: 3.2827e-08 - val_loss: 1.0468e-07
Epoch 317/512
10/10 - 0s - loss: 4.7991e-08 - val_loss: 9.4017e-07
Epoch 318/512
10/10 - 0s - loss: 1.1038e-07 - val_loss: 1.0538e-07
Epoch 319/512
10/10 - 0s - loss: 6.5000e-08 - val_loss: 4.8209e-07
Epoch 320/512
10/10 - 0s - loss: 4.0169e-08 - val_loss: 2.0811e-07
Epoch 321/512
10/10 - 0s - loss: 2.5427e-08 - val_loss: 3.8842e-07
Epoch 322/512
10/10 - 0s - loss: 3.3539e-08 - val_loss: 8.2378e-08
Epoch 323/512
10/10 - 0s - loss: 4.3260e-08 - val_loss: 6.6102e-07
Epoch 324/512
10/10 - 0s - loss: 7.9577e-08 - val_loss: 8.8753e-08
Epoch 325/512
10/10 - 0s - loss: 3.1398e-08 - val_loss: 2.2636e-07
Epoch 326/512
10/10 - 0s - loss: 1.9667e-08 - val_loss: 2.0797e-07
Epoch 327/512
10/10 - 0s - loss: 1.5358e-08 - val_loss: 1.0546e-07
Epoch 328/512
10/10 - 0s - loss: 1.6764e-08 - val_loss: 2.6818e-07
Epoch 329/512
10/10 - 0s - loss: 2.3494e-08 - val_loss: 7.2037e-08
Epoch 330/512
10/10 - 0s - loss: 2.9464e-08 - val_loss: 5.0799e-07
Epoch 331/512
10/10 - 0s - loss: 5.9479e-08 - val_loss: 6.4765e-08
Epoch 332/512
10/10 - 0s - loss: 2.4280e-08 - val_loss: 2.5360e-07
Epoch 333/512
10/10 - 0s - loss: 1.7383e-08 - val_loss: 8.1062e-08
Epoch 334/512
10/10 - 0s - loss: 2.0798e-08 - val_loss: 3.4470e-07
Epoch 335/512
10/10 - 0s - loss: 3.7385e-08 - val_loss: 6.2982e-08
Epoch 336/512
10/10 - 0s - loss: 2.2771e-08 - val_loss: 3.1963e-07
Epoch 337/512
10/10 - 0s - loss: 3.3199e-08 - val_loss: 6.2929e-08
Epoch 338/512
10/10 - 0s - loss: 2.1873e-08 - val_loss: 3.0662e-07
Epoch 339/512
10/10 - 0s - loss: 3.2811e-08 - val_loss: 6.4271e-08
Epoch 340/512
10/10 - 0s - loss: 1.9071e-08 - val_loss: 2.6489e-07
Epoch 341/512
10/10 - 0s - loss: 2.6586e-08 - val_loss: 6.3740e-08
Epoch 342/512
10/10 - 0s - loss: 1.9723e-08 - val_loss: 2.8472e-07
Epoch 343/512
10/10 - 0s - loss: 3.0881e-08 - val_loss: 6.2069e-08
Epoch 344/512
10/10 - 0s - loss: 1.7325e-08 - val_loss: 2.3983e-07
Epoch 345/512
10/10 - 0s - loss: 2.3380e-08 - val_loss: 6.0619e-08
Epoch 346/512
10/10 - 0s - loss: 1.8918e-08 - val_loss: 2.7468e-07
Epoch 347/512
10/10 - 0s - loss: 3.0061e-08 - val_loss: 5.8390e-08
Epoch 348/512
10/10 - 0s - loss: 1.6227e-08 - val_loss: 2.2113e-07
Epoch 349/512
10/10 - 0s - loss: 2.0906e-08 - val_loss: 5.7553e-08
Epoch 350/512
10/10 - 0s - loss: 1.8137e-08 - val_loss: 2.6326e-07
Epoch 351/512
10/10 - 0s - loss: 2.8913e-08 - val_loss: 5.5050e-08
Epoch 352/512
10/10 - 0s - loss: 1.5362e-08 - val_loss: 2.0661e-07
Epoch 353/512
10/10 - 0s - loss: 1.9171e-08 - val_loss: 5.4738e-08
Epoch 354/512
10/10 - 0s - loss: 1.7220e-08 - val_loss: 2.4863e-07
Epoch 355/512
10/10 - 0s - loss: 2.7202e-08 - val_loss: 5.2003e-08
Epoch 356/512
10/10 - 0s - loss: 1.4731e-08 - val_loss: 1.9677e-07
Epoch 357/512
10/10 - 0s - loss: 1.8281e-08 - val_loss: 5.1960e-08
Epoch 358/512
10/10 - 0s - loss: 1.6270e-08 - val_loss: 2.3234e-07
Epoch 359/512
10/10 - 0s - loss: 2.5158e-08 - val_loss: 4.9403e-08
Epoch 360/512
10/10 - 0s - loss: 1.4221e-08 - val_loss: 1.8955e-07
Epoch 361/512
10/10 - 0s - loss: 1.7818e-08 - val_loss: 4.9466e-08
Epoch 362/512
10/10 - 0s - loss: 1.5333e-08 - val_loss: 2.1589e-07
Epoch 363/512
10/10 - 0s - loss: 2.3019e-08 - val_loss: 4.7219e-08
Epoch 364/512
10/10 - 0s - loss: 1.3763e-08 - val_loss: 1.8339e-07
Epoch 365/512
10/10 - 0s - loss: 1.7516e-08 - val_loss: 4.7264e-08
Epoch 366/512
10/10 - 0s - loss: 1.4425e-08 - val_loss: 2.0011e-07
Epoch 367/512
10/10 - 0s - loss: 2.0928e-08 - val_loss: 4.5379e-08
Epoch 368/512
10/10 - 0s - loss: 1.3331e-08 - val_loss: 1.7777e-07
Epoch 369/512
10/10 - 0s - loss: 1.7223e-08 - val_loss: 4.5298e-08
Epoch 370/512
10/10 - 0s - loss: 1.3592e-08 - val_loss: 1.8591e-07
Epoch 371/512
10/10 - 0s - loss: 1.9037e-08 - val_loss: 4.3736e-08
Epoch 372/512
10/10 - 0s - loss: 1.2896e-08 - val_loss: 1.7196e-07
Epoch 373/512
10/10 - 0s - loss: 1.6836e-08 - val_loss: 4.3415e-08
Epoch 374/512
10/10 - 0s - loss: 1.2874e-08 - val_loss: 1.7382e-07
Epoch 375/512
10/10 - 0s - loss: 1.7487e-08 - val_loss: 4.2187e-08
Epoch 376/512
10/10 - 0s - loss: 1.2431e-08 - val_loss: 1.6536e-07
Epoch 377/512
10/10 - 0s - loss: 1.6266e-08 - val_loss: 4.1688e-08
Epoch 378/512
10/10 - 0s - loss: 1.2251e-08 - val_loss: 1.6365e-07
Epoch 379/512
10/10 - 0s - loss: 1.6244e-08 - val_loss: 4.0779e-08
Epoch 380/512
10/10 - 0s - loss: 1.1943e-08 - val_loss: 1.5838e-07
Epoch 381/512
10/10 - 0s - loss: 1.5566e-08 - val_loss: 4.0174e-08
Epoch 382/512
10/10 - 0s - loss: 1.1723e-08 - val_loss: 1.5540e-07
Epoch 383/512
10/10 - 0s - loss: 1.5291e-08 - val_loss: 3.9440e-08
Epoch 384/512
10/10 - 0s - loss: 1.1473e-08 - val_loss: 1.5145e-07
Epoch 385/512
10/10 - 0s - loss: 1.4828e-08 - val_loss: 3.8842e-08
Epoch 386/512
10/10 - 0s - loss: 1.1245e-08 - val_loss: 1.4811e-07
Epoch 387/512
10/10 - 0s - loss: 1.4477e-08 - val_loss: 3.8144e-08
Epoch 388/512
10/10 - 0s - loss: 1.1014e-08 - val_loss: 1.4451e-07
Epoch 389/512
10/10 - 0s - loss: 1.4091e-08 - val_loss: 3.7469e-08
Epoch 390/512
10/10 - 0s - loss: 1.0796e-08 - val_loss: 1.4122e-07
Epoch 391/512
10/10 - 0s - loss: 1.3746e-08 - val_loss: 3.6825e-08
Epoch 392/512
10/10 - 0s - loss: 1.0580e-08 - val_loss: 1.3799e-07
Epoch 393/512
10/10 - 0s - loss: 1.3402e-08 - val_loss: 3.6221e-08
Epoch 394/512
10/10 - 0s - loss: 1.0377e-08 - val_loss: 1.3498e-07
Epoch 395/512
10/10 - 0s - loss: 1.3086e-08 - val_loss: 3.5654e-08
Epoch 396/512
10/10 - 0s - loss: 1.0178e-08 - val_loss: 1.3202e-07
Epoch 397/512
10/10 - 0s - loss: 1.2771e-08 - val_loss: 3.5123e-08
Epoch 398/512
10/10 - 0s - loss: 9.9810e-09 - val_loss: 1.2916e-07
Epoch 399/512
10/10 - 0s - loss: 1.2465e-08 - val_loss: 3.4615e-08
Epoch 400/512
10/10 - 0s - loss: 9.7951e-09 - val_loss: 1.2649e-07
Epoch 401/512
10/10 - 0s - loss: 1.2182e-08 - val_loss: 3.4119e-08
Epoch 402/512
10/10 - 0s - loss: 9.6164e-09 - val_loss: 1.2390e-07
Epoch 403/512
10/10 - 0s - loss: 1.1908e-08 - val_loss: 3.3653e-08
Epoch 404/512
10/10 - 0s - loss: 9.4402e-09 - val_loss: 1.2136e-07
Epoch 405/512
10/10 - 0s - loss: 1.1640e-08 - val_loss: 3.3175e-08
Epoch 406/512
10/10 - 0s - loss: 9.2672e-09 - val_loss: 1.1889e-07
Epoch 407/512
10/10 - 0s - loss: 1.1378e-08 - val_loss: 3.2712e-08
Epoch 408/512
10/10 - 0s - loss: 9.1040e-09 - val_loss: 1.1656e-07
Epoch 409/512
10/10 - 0s - loss: 1.1137e-08 - val_loss: 3.2267e-08
Epoch 410/512
10/10 - 0s - loss: 8.9465e-09 - val_loss: 1.1428e-07
Epoch 411/512
10/10 - 0s - loss: 1.0903e-08 - val_loss: 3.1813e-08
Epoch 412/512
10/10 - 0s - loss: 8.7878e-09 - val_loss: 1.1199e-07
Epoch 413/512
10/10 - 0s - loss: 1.0666e-08 - val_loss: 3.1396e-08
Epoch 414/512
10/10 - 0s - loss: 8.6343e-09 - val_loss: 1.0981e-07
Epoch 415/512
10/10 - 0s - loss: 1.0444e-08 - val_loss: 3.0988e-08
Epoch 416/512
10/10 - 0s - loss: 8.4874e-09 - val_loss: 1.0774e-07
Epoch 417/512
10/10 - 0s - loss: 1.0230e-08 - val_loss: 3.0596e-08
Epoch 418/512
10/10 - 0s - loss: 8.3435e-09 - val_loss: 1.0570e-07
Epoch 419/512
10/10 - 0s - loss: 1.0020e-08 - val_loss: 3.0194e-08
Epoch 420/512
10/10 - 0s - loss: 8.2039e-09 - val_loss: 1.0374e-07
Epoch 421/512
10/10 - 0s - loss: 9.8238e-09 - val_loss: 2.9800e-08
Epoch 422/512
10/10 - 0s - loss: 8.0703e-09 - val_loss: 1.0185e-07
Epoch 423/512
10/10 - 0s - loss: 9.6321e-09 - val_loss: 2.9427e-08
Epoch 424/512
10/10 - 0s - loss: 7.9387e-09 - val_loss: 9.9996e-08
Epoch 425/512
10/10 - 0s - loss: 9.4429e-09 - val_loss: 2.9085e-08
Epoch 426/512
10/10 - 0s - loss: 7.8081e-09 - val_loss: 9.8212e-08
Epoch 427/512
10/10 - 0s - loss: 9.2611e-09 - val_loss: 2.8740e-08
Epoch 428/512
10/10 - 0s - loss: 7.6854e-09 - val_loss: 9.6492e-08
Epoch 429/512
10/10 - 0s - loss: 9.0897e-09 - val_loss: 2.8385e-08
Epoch 430/512
10/10 - 0s - loss: 7.5644e-09 - val_loss: 9.4799e-08
Epoch 431/512
10/10 - 0s - loss: 8.9193e-09 - val_loss: 2.8049e-08
Epoch 432/512
10/10 - 0s - loss: 7.4438e-09 - val_loss: 9.3141e-08
Epoch 433/512
10/10 - 0s - loss: 8.7497e-09 - val_loss: 2.7727e-08
Epoch 434/512
10/10 - 0s - loss: 7.3295e-09 - val_loss: 9.1561e-08
Epoch 435/512
10/10 - 0s - loss: 8.5964e-09 - val_loss: 2.7407e-08
Epoch 436/512
10/10 - 0s - loss: 7.2199e-09 - val_loss: 9.0040e-08
Epoch 437/512
10/10 - 0s - loss: 8.4440e-09 - val_loss: 2.7106e-08
Epoch 438/512
10/10 - 0s - loss: 7.1090e-09 - val_loss: 8.8516e-08
Epoch 439/512
10/10 - 0s - loss: 8.2900e-09 - val_loss: 2.6823e-08
Epoch 440/512
10/10 - 0s - loss: 7.0024e-09 - val_loss: 8.7094e-08
Epoch 441/512
10/10 - 0s - loss: 8.1477e-09 - val_loss: 2.6537e-08
Epoch 442/512
10/10 - 0s - loss: 6.9010e-09 - val_loss: 8.5735e-08
Epoch 443/512
10/10 - 0s - loss: 8.0121e-09 - val_loss: 2.6251e-08
Epoch 444/512
10/10 - 0s - loss: 6.8020e-09 - val_loss: 8.4359e-08
Epoch 445/512
10/10 - 0s - loss: 7.8758e-09 - val_loss: 2.5959e-08
Epoch 446/512
10/10 - 0s - loss: 6.7042e-09 - val_loss: 8.3003e-08
Epoch 447/512
10/10 - 0s - loss: 7.7439e-09 - val_loss: 2.5688e-08
Epoch 448/512
10/10 - 0s - loss: 6.6064e-09 - val_loss: 8.1680e-08
Epoch 449/512
10/10 - 0s - loss: 7.6127e-09 - val_loss: 2.5423e-08
Epoch 450/512
10/10 - 0s - loss: 6.5129e-09 - val_loss: 8.0426e-08
Epoch 451/512
10/10 - 0s - loss: 7.4901e-09 - val_loss: 2.5158e-08
Epoch 452/512
10/10 - 0s - loss: 6.4229e-09 - val_loss: 7.9216e-08
Epoch 453/512
10/10 - 0s - loss: 7.3704e-09 - val_loss: 2.4901e-08
Epoch 454/512
10/10 - 0s - loss: 6.3355e-09 - val_loss: 7.8027e-08
Epoch 455/512
10/10 - 0s - loss: 7.2544e-09 - val_loss: 2.4660e-08
Epoch 456/512
10/10 - 0s - loss: 6.2481e-09 - val_loss: 7.6867e-08
Epoch 457/512
10/10 - 0s - loss: 7.1384e-09 - val_loss: 2.4424e-08
Epoch 458/512
10/10 - 0s - loss: 6.1644e-09 - val_loss: 7.5750e-08
Epoch 459/512
10/10 - 0s - loss: 7.0288e-09 - val_loss: 2.4197e-08
Epoch 460/512
10/10 - 0s - loss: 6.0819e-09 - val_loss: 7.4665e-08
Epoch 461/512
10/10 - 0s - loss: 6.9221e-09 - val_loss: 2.3965e-08
Epoch 462/512
10/10 - 0s - loss: 6.0031e-09 - val_loss: 7.3611e-08
Epoch 463/512
10/10 - 0s - loss: 6.8198e-09 - val_loss: 2.3746e-08
Epoch 464/512
10/10 - 0s - loss: 5.9255e-09 - val_loss: 7.2584e-08
Epoch 465/512
10/10 - 0s - loss: 6.7186e-09 - val_loss: 2.3533e-08
Epoch 466/512
10/10 - 0s - loss: 5.8492e-09 - val_loss: 7.1581e-08
Epoch 467/512
10/10 - 0s - loss: 6.6200e-09 - val_loss: 2.3321e-08
Epoch 468/512
10/10 - 0s - loss: 5.7750e-09 - val_loss: 7.0592e-08
Epoch 469/512
10/10 - 0s - loss: 6.5216e-09 - val_loss: 2.3108e-08
Epoch 470/512
10/10 - 0s - loss: 5.6998e-09 - val_loss: 6.9589e-08
Epoch 471/512
10/10 - 0s - loss: 6.4259e-09 - val_loss: 2.2899e-08
Epoch 472/512
10/10 - 0s - loss: 5.6289e-09 - val_loss: 6.8661e-08
Epoch 473/512
10/10 - 0s - loss: 6.3371e-09 - val_loss: 2.2686e-08
Epoch 474/512
10/10 - 0s - loss: 5.5615e-09 - val_loss: 6.7763e-08
Epoch 475/512
10/10 - 0s - loss: 6.2531e-09 - val_loss: 2.2474e-08
Epoch 476/512
10/10 - 0s - loss: 5.4960e-09 - val_loss: 6.6883e-08
Epoch 477/512
10/10 - 0s - loss: 6.1683e-09 - val_loss: 2.2279e-08
Epoch 478/512
10/10 - 0s - loss: 5.4275e-09 - val_loss: 6.5966e-08
Epoch 479/512
10/10 - 0s - loss: 6.0786e-09 - val_loss: 2.2102e-08
Epoch 480/512
10/10 - 0s - loss: 5.3609e-09 - val_loss: 6.5112e-08
Epoch 481/512
10/10 - 0s - loss: 5.9958e-09 - val_loss: 2.1919e-08
Epoch 482/512
10/10 - 0s - loss: 5.2972e-09 - val_loss: 6.4287e-08
Epoch 483/512
10/10 - 0s - loss: 5.9147e-09 - val_loss: 2.1736e-08
Epoch 484/512
10/10 - 0s - loss: 5.2335e-09 - val_loss: 6.3460e-08
Epoch 485/512
10/10 - 0s - loss: 5.8344e-09 - val_loss: 2.1559e-08
Epoch 486/512
10/10 - 0s - loss: 5.1740e-09 - val_loss: 6.2695e-08
Epoch 487/512
10/10 - 0s - loss: 5.7610e-09 - val_loss: 2.1370e-08
Epoch 488/512
10/10 - 0s - loss: 5.1155e-09 - val_loss: 6.1903e-08
Epoch 489/512
10/10 - 0s - loss: 5.6885e-09 - val_loss: 2.1182e-08
Epoch 490/512
10/10 - 0s - loss: 5.0589e-09 - val_loss: 6.1160e-08
Epoch 491/512
10/10 - 0s - loss: 5.6189e-09 - val_loss: 2.0998e-08
Epoch 492/512
10/10 - 0s - loss: 5.0018e-09 - val_loss: 6.0390e-08
Epoch 493/512
10/10 - 0s - loss: 5.5468e-09 - val_loss: 2.0812e-08
Epoch 494/512
10/10 - 0s - loss: 4.9450e-09 - val_loss: 5.9629e-08
Epoch 495/512
10/10 - 0s - loss: 5.4771e-09 - val_loss: 2.0628e-08
Epoch 496/512
10/10 - 0s - loss: 4.8904e-09 - val_loss: 5.8889e-08
Epoch 497/512
10/10 - 0s - loss: 5.4077e-09 - val_loss: 2.0461e-08
Epoch 498/512
10/10 - 0s - loss: 4.8322e-09 - val_loss: 5.8139e-08
Epoch 499/512
10/10 - 0s - loss: 5.3353e-09 - val_loss: 2.0297e-08
Epoch 500/512
10/10 - 0s - loss: 4.7788e-09 - val_loss: 5.7453e-08
Epoch 501/512
10/10 - 0s - loss: 5.2706e-09 - val_loss: 2.0127e-08
Epoch 502/512
10/10 - 0s - loss: 4.7273e-09 - val_loss: 5.6778e-08
Epoch 503/512
10/10 - 0s - loss: 5.2082e-09 - val_loss: 1.9962e-08
Epoch 504/512
10/10 - 0s - loss: 4.6775e-09 - val_loss: 5.6121e-08
Epoch 505/512
10/10 - 0s - loss: 5.1482e-09 - val_loss: 1.9797e-08
Epoch 506/512
10/10 - 0s - loss: 4.6279e-09 - val_loss: 5.5478e-08
Epoch 507/512
10/10 - 0s - loss: 5.0878e-09 - val_loss: 1.9640e-08
Epoch 508/512
10/10 - 0s - loss: 4.5807e-09 - val_loss: 5.4859e-08
Epoch 509/512
10/10 - 0s - loss: 5.0302e-09 - val_loss: 1.9488e-08
Epoch 510/512
10/10 - 0s - loss: 4.5326e-09 - val_loss: 5.4232e-08
Epoch 511/512
10/10 - 0s - loss: 4.9707e-09 - val_loss: 1.9346e-08
Epoch 512/512
10/10 - 0s - loss: 4.4848e-09 - val_loss: 5.3628e-08
Train on 10 samples, validate on 10 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.6799e-10 - val_loss: 2.4495e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4459e-09 - val_loss: 2.1446e-08
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1436e-08 - val_loss: 3.3218e-08
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3190e-08 - val_loss: 4.5131e-08
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.5108e-08 - val_loss: 3.3246e-08
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3228e-08 - val_loss: 1.6997e-08
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6988e-08 - val_loss: 1.5195e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5183e-09 - val_loss: 6.8226e-10
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 6.8033e-10 - val_loss: 6.1858e-10
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 6.1780e-10 - val_loss: 4.7108e-10
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.6956e-10 - val_loss: 4.6224e-10
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.6164e-10 - val_loss: 4.1521e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.1387e-10 - val_loss: 4.4258e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.4213e-10 - val_loss: 4.3918e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.3784e-10 - val_loss: 5.3155e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.3127e-10 - val_loss: 5.8927e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.8775e-10 - val_loss: 8.8235e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.8225e-10 - val_loss: 1.0805e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0783e-09 - val_loss: 2.0683e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0681e-09 - val_loss: 2.3072e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3036e-09 - val_loss: 4.9826e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.9816e-09 - val_loss: 3.0181e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0136e-09 - val_loss: 6.0881e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.0865e-09 - val_loss: 2.2378e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2342e-09 - val_loss: 4.0439e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.0431e-09 - val_loss: 2.0760e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0727e-09 - val_loss: 3.6452e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.6445e-09 - val_loss: 1.9623e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9590e-09 - val_loss: 3.3867e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3862e-09 - val_loss: 1.8806e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8774e-09 - val_loss: 3.2095e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.2090e-09 - val_loss: 1.8180e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8150e-09 - val_loss: 3.0795e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0791e-09 - val_loss: 1.7698e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7667e-09 - val_loss: 2.9777e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9774e-09 - val_loss: 1.7204e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7174e-09 - val_loss: 2.8761e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8760e-09 - val_loss: 1.6777e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6748e-09 - val_loss: 2.7882e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.7880e-09 - val_loss: 1.6361e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6332e-09 - val_loss: 2.6943e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6942e-09 - val_loss: 1.5850e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5822e-09 - val_loss: 2.5901e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5901e-09 - val_loss: 1.5421e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5393e-09 - val_loss: 2.5039e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5040e-09 - val_loss: 1.5032e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5004e-09 - val_loss: 2.4235e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4236e-09 - val_loss: 1.4617e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4590e-09 - val_loss: 2.3415e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3416e-09 - val_loss: 1.4259e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4233e-09 - val_loss: 2.2680e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2681e-09 - val_loss: 1.3885e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3859e-09 - val_loss: 2.1962e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1964e-09 - val_loss: 1.3560e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3534e-09 - val_loss: 2.1303e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1305e-09 - val_loss: 1.3193e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3168e-09 - val_loss: 2.0564e-09
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0566e-09 - val_loss: 1.2838e-09
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2814e-09 - val_loss: 1.9893e-09
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9895e-09 - val_loss: 1.2534e-09
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2509e-09 - val_loss: 1.9319e-09
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9322e-09 - val_loss: 1.2243e-09
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2219e-09 - val_loss: 1.8743e-09
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8745e-09 - val_loss: 1.1920e-09
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1896e-09 - val_loss: 1.8155e-09
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8158e-09 - val_loss: 1.1661e-09
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1638e-09 - val_loss: 1.7654e-09
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7656e-09 - val_loss: 1.1373e-09
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1350e-09 - val_loss: 1.7109e-09
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7111e-09 - val_loss: 1.1114e-09
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1092e-09 - val_loss: 1.6641e-09
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6644e-09 - val_loss: 1.0858e-09
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0836e-09 - val_loss: 1.6151e-09
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6153e-09 - val_loss: 1.0594e-09
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0572e-09 - val_loss: 1.5671e-09
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5673e-09 - val_loss: 1.0330e-09
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0308e-09 - val_loss: 1.5192e-09
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5194e-09 - val_loss: 1.0093e-09
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0072e-09 - val_loss: 1.4760e-09
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4763e-09 - val_loss: 9.8669e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.8459e-10 - val_loss: 1.4362e-09
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4364e-09 - val_loss: 9.6444e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.6232e-10 - val_loss: 1.3959e-09
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3961e-09 - val_loss: 9.4304e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.4095e-10 - val_loss: 1.3600e-09
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3603e-09 - val_loss: 9.2293e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.2094e-10 - val_loss: 1.3247e-09
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3249e-09 - val_loss: 9.0667e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.0461e-10 - val_loss: 1.2972e-09
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2974e-09 - val_loss: 8.8855e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.8658e-10 - val_loss: 1.2642e-09
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2644e-09 - val_loss: 8.6870e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.6674e-10 - val_loss: 1.2288e-09
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2290e-09 - val_loss: 8.4926e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.4730e-10 - val_loss: 1.1957e-09
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1959e-09 - val_loss: 8.3024e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.2833e-10 - val_loss: 1.1630e-09
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1632e-09 - val_loss: 8.1199e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.1010e-10 - val_loss: 1.1315e-09
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1316e-09 - val_loss: 7.9300e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.9114e-10 - val_loss: 1.1004e-09
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1007e-09 - val_loss: 7.7890e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.7702e-10 - val_loss: 1.0779e-09
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0780e-09 - val_loss: 7.6476e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.6284e-10 - val_loss: 1.0539e-09
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0540e-09 - val_loss: 7.4972e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.4784e-10 - val_loss: 1.0301e-09
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0303e-09 - val_loss: 7.3681e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.3502e-10 - val_loss: 1.0071e-09
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0073e-09 - val_loss: 7.2039e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.1856e-10 - val_loss: 9.8037e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.8043e-10 - val_loss: 7.0638e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.0460e-10 - val_loss: 9.5787e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.5795e-10 - val_loss: 6.9306e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.9129e-10 - val_loss: 9.3563e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.3568e-10 - val_loss: 6.7910e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.7732e-10 - val_loss: 9.1450e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
10/10 - 0s - loss: 9.1454e-10 - val_loss: 6.6750e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.6578e-10 - val_loss: 8.9519e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.9525e-10 - val_loss: 6.5414e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.5242e-10 - val_loss: 8.7449e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.7452e-10 - val_loss: 6.4441e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.4269e-10 - val_loss: 8.5852e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.5855e-10 - val_loss: 6.3186e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.3014e-10 - val_loss: 8.3853e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.3851e-10 - val_loss: 6.2030e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.1859e-10 - val_loss: 8.2051e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.2049e-10 - val_loss: 6.0880e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.0712e-10 - val_loss: 8.0231e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
10/10 - 0s - loss: 8.0229e-10 - val_loss: 5.9749e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.9579e-10 - val_loss: 7.8409e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.8407e-10 - val_loss: 5.8569e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.8403e-10 - val_loss: 7.6694e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.6693e-10 - val_loss: 5.7724e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.7561e-10 - val_loss: 7.5334e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.5331e-10 - val_loss: 5.6697e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.6533e-10 - val_loss: 7.3831e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.3824e-10 - val_loss: 5.5791e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.5633e-10 - val_loss: 7.2422e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.2416e-10 - val_loss: 5.4913e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.4755e-10 - val_loss: 7.1062e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
10/10 - 0s - loss: 7.1052e-10 - val_loss: 5.3961e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.3805e-10 - val_loss: 6.9578e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.9569e-10 - val_loss: 5.3048e-10
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.2891e-10 - val_loss: 6.8260e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.8249e-10 - val_loss: 5.2192e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.2033e-10 - val_loss: 6.6914e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.6902e-10 - val_loss: 5.1351e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.1191e-10 - val_loss: 6.5645e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.5633e-10 - val_loss: 5.0409e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.0253e-10 - val_loss: 6.4136e-10
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.4120e-10 - val_loss: 4.9416e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.9261e-10 - val_loss: 6.2673e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.2657e-10 - val_loss: 4.8564e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.8410e-10 - val_loss: 6.1480e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.1464e-10 - val_loss: 4.7883e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.7730e-10 - val_loss: 6.0589e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
10/10 - 0s - loss: 6.0574e-10 - val_loss: 4.7272e-10
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.7119e-10 - val_loss: 5.9653e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.9636e-10 - val_loss: 4.6506e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.6351e-10 - val_loss: 5.8543e-10
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.8525e-10 - val_loss: 4.5887e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.5735e-10 - val_loss: 5.7529e-10
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.7510e-10 - val_loss: 4.5135e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.4983e-10 - val_loss: 5.6464e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.6447e-10 - val_loss: 4.4447e-10
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.4296e-10 - val_loss: 5.5543e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.5523e-10 - val_loss: 4.3849e-10
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.3703e-10 - val_loss: 5.4698e-10
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.4677e-10 - val_loss: 4.3310e-10
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.3159e-10 - val_loss: 5.3936e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.3911e-10 - val_loss: 4.2731e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.2585e-10 - val_loss: 5.3028e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
10/10 - 0s - loss: 5.3006e-10 - val_loss: 4.2076e-10
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.1928e-10 - val_loss: 5.1988e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 5.1962e-10 - val_loss: 4.1230e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.1086e-10 - val_loss: 5.0821e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 5.0792e-10 - val_loss: 4.0619e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
10/10 - 0s - loss: 4.0475e-10 - val_loss: 5.0023e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.9998e-10 - val_loss: 4.0091e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.9950e-10 - val_loss: 4.9327e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.9296e-10 - val_loss: 3.9567e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.9425e-10 - val_loss: 4.8496e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.8471e-10 - val_loss: 3.9042e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.8903e-10 - val_loss: 4.7780e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.7751e-10 - val_loss: 3.8498e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.8358e-10 - val_loss: 4.7025e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.6995e-10 - val_loss: 3.7949e-10
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.7807e-10 - val_loss: 4.6247e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.6217e-10 - val_loss: 3.7490e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.7348e-10 - val_loss: 4.5623e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.5588e-10 - val_loss: 3.7024e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.6882e-10 - val_loss: 4.4987e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.4956e-10 - val_loss: 3.6540e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.6400e-10 - val_loss: 4.4317e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.4283e-10 - val_loss: 3.6065e-10
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.5925e-10 - val_loss: 4.3640e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.3609e-10 - val_loss: 3.5531e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.5392e-10 - val_loss: 4.2809e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.2774e-10 - val_loss: 3.4988e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.4850e-10 - val_loss: 4.2080e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.2046e-10 - val_loss: 3.4396e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.4259e-10 - val_loss: 4.1303e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.1267e-10 - val_loss: 3.3988e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3852e-10 - val_loss: 4.0773e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.0737e-10 - val_loss: 3.3632e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3496e-10 - val_loss: 4.0369e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 4.0332e-10 - val_loss: 3.3352e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.3216e-10 - val_loss: 3.9945e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.9907e-10 - val_loss: 3.2965e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.2830e-10 - val_loss: 3.9406e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.9367e-10 - val_loss: 3.2622e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.2488e-10 - val_loss: 3.8939e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.8904e-10 - val_loss: 3.2218e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.2086e-10 - val_loss: 3.8374e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.8334e-10 - val_loss: 3.1806e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.1674e-10 - val_loss: 3.7764e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.7726e-10 - val_loss: 3.1323e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.1191e-10 - val_loss: 3.7111e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.7071e-10 - val_loss: 3.0873e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0742e-10 - val_loss: 3.6504e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.6463e-10 - val_loss: 3.0384e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
10/10 - 0s - loss: 3.0256e-10 - val_loss: 3.5873e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.5832e-10 - val_loss: 3.0114e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9984e-10 - val_loss: 3.5616e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.5575e-10 - val_loss: 2.9869e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9740e-10 - val_loss: 3.5280e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.5237e-10 - val_loss: 2.9648e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9521e-10 - val_loss: 3.4962e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.4918e-10 - val_loss: 2.9347e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.9217e-10 - val_loss: 3.4530e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.4486e-10 - val_loss: 2.8975e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8848e-10 - val_loss: 3.3992e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.3950e-10 - val_loss: 2.8550e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8422e-10 - val_loss: 3.3418e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.3374e-10 - val_loss: 2.8190e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.8063e-10 - val_loss: 3.2994e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.2951e-10 - val_loss: 2.7834e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.7708e-10 - val_loss: 3.2474e-10
Epoch 240/512

Epoch 00240: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.2430e-10 - val_loss: 2.7492e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.7363e-10 - val_loss: 3.2046e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.2002e-10 - val_loss: 2.7179e-10
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.7054e-10 - val_loss: 3.1669e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.1625e-10 - val_loss: 2.6952e-10
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6827e-10 - val_loss: 3.1445e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.1398e-10 - val_loss: 2.6813e-10
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6687e-10 - val_loss: 3.1266e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.1219e-10 - val_loss: 2.6633e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6509e-10 - val_loss: 3.1001e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.0954e-10 - val_loss: 2.6373e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6251e-10 - val_loss: 3.0647e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.0598e-10 - val_loss: 2.6192e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.6071e-10 - val_loss: 3.0387e-10
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 3.0339e-10 - val_loss: 2.5908e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5785e-10 - val_loss: 2.9979e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.9934e-10 - val_loss: 2.5532e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5411e-10 - val_loss: 2.9482e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.9434e-10 - val_loss: 2.5159e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.5037e-10 - val_loss: 2.8965e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.8917e-10 - val_loss: 2.4812e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4690e-10 - val_loss: 2.8538e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.8489e-10 - val_loss: 2.4427e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4307e-10 - val_loss: 2.8067e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.8018e-10 - val_loss: 2.4185e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.4063e-10 - val_loss: 2.7725e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.7676e-10 - val_loss: 2.3902e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3783e-10 - val_loss: 2.7397e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.7346e-10 - val_loss: 2.3681e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3560e-10 - val_loss: 2.7248e-10
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.7200e-10 - val_loss: 2.3677e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3558e-10 - val_loss: 2.7198e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.7148e-10 - val_loss: 2.3547e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3429e-10 - val_loss: 2.7055e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.7004e-10 - val_loss: 2.3403e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3286e-10 - val_loss: 2.6736e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.6687e-10 - val_loss: 2.3148e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.3030e-10 - val_loss: 2.6505e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.6455e-10 - val_loss: 2.2972e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2854e-10 - val_loss: 2.6203e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.6150e-10 - val_loss: 2.2685e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2568e-10 - val_loss: 2.5869e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.5818e-10 - val_loss: 2.2422e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2306e-10 - val_loss: 2.5508e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.5456e-10 - val_loss: 2.2155e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.2040e-10 - val_loss: 2.5163e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.5112e-10 - val_loss: 2.1963e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1848e-10 - val_loss: 2.5028e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.4976e-10 - val_loss: 2.1862e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1745e-10 - val_loss: 2.4829e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.4777e-10 - val_loss: 2.1629e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1511e-10 - val_loss: 2.4494e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.4443e-10 - val_loss: 2.1412e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1299e-10 - val_loss: 2.4257e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.4206e-10 - val_loss: 2.1193e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.1078e-10 - val_loss: 2.3973e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.3921e-10 - val_loss: 2.0967e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0854e-10 - val_loss: 2.3683e-10
Epoch 298/512

Epoch 00298: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.3632e-10 - val_loss: 2.0743e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0630e-10 - val_loss: 2.3371e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.3319e-10 - val_loss: 2.0545e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0434e-10 - val_loss: 2.3210e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.3157e-10 - val_loss: 2.0410e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0298e-10 - val_loss: 2.3019e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2967e-10 - val_loss: 2.0254e-10
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
10/10 - 0s - loss: 2.0143e-10 - val_loss: 2.2811e-10
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2759e-10 - val_loss: 2.0099e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9991e-10 - val_loss: 2.2605e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2552e-10 - val_loss: 1.9893e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9782e-10 - val_loss: 2.2359e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2307e-10 - val_loss: 1.9743e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9634e-10 - val_loss: 2.2230e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2176e-10 - val_loss: 1.9648e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9538e-10 - val_loss: 2.2070e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.2014e-10 - val_loss: 1.9515e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9405e-10 - val_loss: 2.1927e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.1875e-10 - val_loss: 1.9388e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9276e-10 - val_loss: 2.1750e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.1696e-10 - val_loss: 1.9240e-10
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.9132e-10 - val_loss: 2.1524e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.1470e-10 - val_loss: 1.9023e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8914e-10 - val_loss: 2.1291e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.1236e-10 - val_loss: 1.8882e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8775e-10 - val_loss: 2.1097e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.1043e-10 - val_loss: 1.8728e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8619e-10 - val_loss: 2.0910e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.0854e-10 - val_loss: 1.8520e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8411e-10 - val_loss: 2.0661e-10
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.0606e-10 - val_loss: 1.8359e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8252e-10 - val_loss: 2.0454e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.0399e-10 - val_loss: 1.8211e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8104e-10 - val_loss: 2.0305e-10
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.0249e-10 - val_loss: 1.8095e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7988e-10 - val_loss: 2.0149e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 2.0095e-10 - val_loss: 1.7985e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7879e-10 - val_loss: 2.0021e-10
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9966e-10 - val_loss: 1.7856e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7750e-10 - val_loss: 1.9893e-10
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9839e-10 - val_loss: 1.7808e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7703e-10 - val_loss: 1.9809e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9755e-10 - val_loss: 1.7689e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7584e-10 - val_loss: 1.9666e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9610e-10 - val_loss: 1.7531e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7427e-10 - val_loss: 1.9447e-10
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9392e-10 - val_loss: 1.7417e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7314e-10 - val_loss: 1.9308e-10
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9253e-10 - val_loss: 1.7258e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7152e-10 - val_loss: 1.9139e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.9085e-10 - val_loss: 1.7161e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.7057e-10 - val_loss: 1.9006e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8948e-10 - val_loss: 1.7002e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6897e-10 - val_loss: 1.8796e-10
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8739e-10 - val_loss: 1.6835e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6731e-10 - val_loss: 1.8563e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8507e-10 - val_loss: 1.6619e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6516e-10 - val_loss: 1.8296e-10
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8241e-10 - val_loss: 1.6471e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6368e-10 - val_loss: 1.8238e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.8182e-10 - val_loss: 1.6476e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6374e-10 - val_loss: 1.8204e-10
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8148e-10 - val_loss: 1.6438e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6334e-10 - val_loss: 1.8175e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8120e-10 - val_loss: 1.6384e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6282e-10 - val_loss: 1.8088e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.8033e-10 - val_loss: 1.6268e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6165e-10 - val_loss: 1.7924e-10
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7866e-10 - val_loss: 1.6129e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.6028e-10 - val_loss: 1.7738e-10
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7680e-10 - val_loss: 1.5987e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5886e-10 - val_loss: 1.7550e-10
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7495e-10 - val_loss: 1.5825e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5722e-10 - val_loss: 1.7443e-10
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7385e-10 - val_loss: 1.5808e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5709e-10 - val_loss: 1.7412e-10
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7355e-10 - val_loss: 1.5725e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5625e-10 - val_loss: 1.7238e-10
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.7182e-10 - val_loss: 1.5561e-10
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5462e-10 - val_loss: 1.7051e-10
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6993e-10 - val_loss: 1.5425e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5325e-10 - val_loss: 1.6889e-10
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6833e-10 - val_loss: 1.5259e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5162e-10 - val_loss: 1.6750e-10
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6692e-10 - val_loss: 1.5248e-10
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5150e-10 - val_loss: 1.6716e-10
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6661e-10 - val_loss: 1.5170e-10
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.5071e-10 - val_loss: 1.6605e-10
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6549e-10 - val_loss: 1.5046e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4949e-10 - val_loss: 1.6426e-10
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6367e-10 - val_loss: 1.4866e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4771e-10 - val_loss: 1.6204e-10
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6146e-10 - val_loss: 1.4746e-10
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4651e-10 - val_loss: 1.6130e-10
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6073e-10 - val_loss: 1.4708e-10
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4611e-10 - val_loss: 1.6085e-10
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.6027e-10 - val_loss: 1.4616e-10
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4519e-10 - val_loss: 1.5926e-10
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5867e-10 - val_loss: 1.4547e-10
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4451e-10 - val_loss: 1.5912e-10
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5855e-10 - val_loss: 1.4507e-10
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4412e-10 - val_loss: 1.5810e-10
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5753e-10 - val_loss: 1.4371e-10
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4275e-10 - val_loss: 1.5659e-10
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5601e-10 - val_loss: 1.4316e-10
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4222e-10 - val_loss: 1.5625e-10
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5565e-10 - val_loss: 1.4289e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4195e-10 - val_loss: 1.5569e-10
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5513e-10 - val_loss: 1.4182e-10
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4086e-10 - val_loss: 1.5446e-10
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5388e-10 - val_loss: 1.4118e-10
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.4023e-10 - val_loss: 1.5349e-10
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5293e-10 - val_loss: 1.4020e-10
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3927e-10 - val_loss: 1.5237e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5181e-10 - val_loss: 1.3915e-10
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3820e-10 - val_loss: 1.5114e-10
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.5057e-10 - val_loss: 1.3836e-10
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3743e-10 - val_loss: 1.5049e-10
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4992e-10 - val_loss: 1.3768e-10
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3673e-10 - val_loss: 1.4895e-10
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4838e-10 - val_loss: 1.3604e-10
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3510e-10 - val_loss: 1.4764e-10
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4707e-10 - val_loss: 1.3549e-10
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3456e-10 - val_loss: 1.4668e-10
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4612e-10 - val_loss: 1.3419e-10
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3325e-10 - val_loss: 1.4521e-10
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4465e-10 - val_loss: 1.3383e-10
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3290e-10 - val_loss: 1.4532e-10
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4474e-10 - val_loss: 1.3364e-10
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3272e-10 - val_loss: 1.4492e-10
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4436e-10 - val_loss: 1.3312e-10
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3219e-10 - val_loss: 1.4441e-10
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4385e-10 - val_loss: 1.3275e-10
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3183e-10 - val_loss: 1.4328e-10
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4271e-10 - val_loss: 1.3139e-10
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3047e-10 - val_loss: 1.4205e-10
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4149e-10 - val_loss: 1.3047e-10
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2955e-10 - val_loss: 1.4089e-10
Epoch 436/512

Epoch 00436: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4032e-10 - val_loss: 1.2995e-10
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2903e-10 - val_loss: 1.4061e-10
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.4005e-10 - val_loss: 1.2915e-10
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2824e-10 - val_loss: 1.3906e-10
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3847e-10 - val_loss: 1.2814e-10
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2723e-10 - val_loss: 1.3821e-10
Epoch 442/512

Epoch 00442: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3766e-10 - val_loss: 1.2731e-10
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2642e-10 - val_loss: 1.3683e-10
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3627e-10 - val_loss: 1.2604e-10
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2515e-10 - val_loss: 1.3627e-10
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.3571e-10 - val_loss: 1.2618e-10
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2530e-10 - val_loss: 1.3597e-10
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3540e-10 - val_loss: 1.2553e-10
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2466e-10 - val_loss: 1.3541e-10
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3484e-10 - val_loss: 1.2509e-10
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2418e-10 - val_loss: 1.3465e-10
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3407e-10 - val_loss: 1.2473e-10
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2383e-10 - val_loss: 1.3492e-10
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3436e-10 - val_loss: 1.2464e-10
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2374e-10 - val_loss: 1.3384e-10
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3327e-10 - val_loss: 1.2354e-10
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2266e-10 - val_loss: 1.3295e-10
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3237e-10 - val_loss: 1.2297e-10
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2209e-10 - val_loss: 1.3228e-10
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3172e-10 - val_loss: 1.2243e-10
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2155e-10 - val_loss: 1.3180e-10
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3124e-10 - val_loss: 1.2205e-10
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2116e-10 - val_loss: 1.3105e-10
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.3049e-10 - val_loss: 1.2076e-10
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1988e-10 - val_loss: 1.2911e-10
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2854e-10 - val_loss: 1.1970e-10
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1882e-10 - val_loss: 1.2858e-10
Epoch 468/512

Epoch 00468: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2801e-10 - val_loss: 1.1918e-10
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1831e-10 - val_loss: 1.2773e-10
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2715e-10 - val_loss: 1.1834e-10
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1748e-10 - val_loss: 1.2662e-10
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2604e-10 - val_loss: 1.1730e-10
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1643e-10 - val_loss: 1.2555e-10
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2496e-10 - val_loss: 1.1655e-10
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1569e-10 - val_loss: 1.2493e-10
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2437e-10 - val_loss: 1.1645e-10
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1560e-10 - val_loss: 1.2465e-10
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2408e-10 - val_loss: 1.1573e-10
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1488e-10 - val_loss: 1.2409e-10
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2353e-10 - val_loss: 1.1578e-10
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1493e-10 - val_loss: 1.2380e-10
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2323e-10 - val_loss: 1.1481e-10
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1396e-10 - val_loss: 1.2274e-10
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2215e-10 - val_loss: 1.1449e-10
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1364e-10 - val_loss: 1.2307e-10
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2250e-10 - val_loss: 1.1509e-10
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1424e-10 - val_loss: 1.2317e-10
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.2263e-10 - val_loss: 1.1455e-10
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1370e-10 - val_loss: 1.2256e-10
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2199e-10 - val_loss: 1.1431e-10
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1347e-10 - val_loss: 1.2256e-10
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2198e-10 - val_loss: 1.1408e-10
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1323e-10 - val_loss: 1.2191e-10
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2134e-10 - val_loss: 1.1302e-10
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1216e-10 - val_loss: 1.2072e-10
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.2016e-10 - val_loss: 1.1290e-10
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1205e-10 - val_loss: 1.2019e-10
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1964e-10 - val_loss: 1.1126e-10
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1043e-10 - val_loss: 1.1823e-10
Epoch 500/512

Epoch 00500: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1768e-10 - val_loss: 1.0992e-10
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0910e-10 - val_loss: 1.1730e-10
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.1674e-10 - val_loss: 1.1012e-10
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0929e-10 - val_loss: 1.1753e-10
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1696e-10 - val_loss: 1.0969e-10
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0887e-10 - val_loss: 1.1726e-10
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1671e-10 - val_loss: 1.0951e-10
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0869e-10 - val_loss: 1.1678e-10
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1622e-10 - val_loss: 1.0914e-10
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0833e-10 - val_loss: 1.1587e-10
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1533e-10 - val_loss: 1.0821e-10
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
10/10 - 0s - loss: 1.0738e-10 - val_loss: 1.1546e-10
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.1-cuvre-secp224r1-test/multiplication_multiplication_weights.h5
10/10 - 0s - loss: 1.1487e-10 - val_loss: 1.0800e-10
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 10.347 | eve: 9.307 | bob: 10.132Epoch   0:   0% | abe: 10.177 | eve: 9.436 | bob: 9.979Epoch   0:   1% | abe: 10.295 | eve: 9.589 | bob: 10.113Epoch   0:   2% | abe: 10.162 | eve: 9.494 | bob: 10.001Epoch   0:   3% | abe: 10.080 | eve: 9.544 | bob: 9.931Epoch   0:   3% | abe: 10.096 | eve: 9.556 | bob: 9.950Epoch   0:   4% | abe: 10.112 | eve: 9.650 | bob: 9.979Epoch   0:   5% | abe: 10.059 | eve: 9.660 | bob: 9.932Epoch   0:   6% | abe: 9.994 | eve: 9.660 | bob: 9.871Epoch   0:   7% | abe: 9.969 | eve: 9.654 | bob: 9.852Epoch   0:   7% | abe: 9.937 | eve: 9.637 | bob: 9.826Epoch   0:   8% | abe: 9.955 | eve: 9.631 | bob: 9.850Epoch   0:   9% | abe: 9.915 | eve: 9.616 | bob: 9.813Epoch   0:  10% | abe: 9.898 | eve: 9.606 | bob: 9.799Epoch   0:  10% | abe: 9.891 | eve: 9.592 | bob: 9.795Epoch   0:  11% | abe: 9.840 | eve: 9.624 | bob: 9.747Epoch   0:  12% | abe: 9.830 | eve: 9.657 | bob: 9.738Epoch   0:  13% | abe: 9.802 | eve: 9.671 | bob: 9.712Epoch   0:  14% | abe: 9.781 | eve: 9.671 | bob: 9.693Epoch   0:  14% | abe: 9.757 | eve: 9.640 | bob: 9.671Epoch   0:  15% | abe: 9.770 | eve: 9.612 | bob: 9.687Epoch   0:  16% | abe: 9.765 | eve: 9.636 | bob: 9.683Epoch   0:  17% | abe: 9.748 | eve: 9.633 | bob: 9.668Epoch   0:  17% | abe: 9.737 | eve: 9.609 | bob: 9.657Epoch   0:  18% | abe: 9.728 | eve: 9.591 | bob: 9.649Epoch   0:  19% | abe: 9.703 | eve: 9.598 | bob: 9.625Epoch   0:  20% | abe: 9.685 | eve: 9.576 | bob: 9.607Epoch   0:  21% | abe: 9.672 | eve: 9.564 | bob: 9.596Epoch   0:  21% | abe: 9.645 | eve: 9.563 | bob: 9.567Epoch   0:  22% | abe: 9.627 | eve: 9.554 | bob: 9.550Epoch   0:  23% | abe: 9.627 | eve: 9.530 | bob: 9.550Epoch   0:  24% | abe: 9.609 | eve: 9.544 | bob: 9.533Epoch   0:  25% | abe: 9.590 | eve: 9.553 | bob: 9.513Epoch   0:  25% | abe: 9.584 | eve: 9.550 | bob: 9.508Epoch   0:  26% | abe: 9.573 | eve: 9.546 | bob: 9.498Epoch   0:  27% | abe: 9.578 | eve: 9.543 | bob: 9.504Epoch   0:  28% | abe: 9.569 | eve: 9.525 | bob: 9.495Epoch   0:  28% | abe: 9.555 | eve: 9.539 | bob: 9.481Epoch   0:  29% | abe: 9.541 | eve: 9.540 | bob: 9.467Epoch   0:  30% | abe: 9.535 | eve: 9.539 | bob: 9.462Epoch   0:  31% | abe: 9.527 | eve: 9.532 | bob: 9.454Epoch   0:  32% | abe: 9.519 | eve: 9.516 | bob: 9.445Epoch   0:  32% | abe: 9.505 | eve: 9.518 | bob: 9.432Epoch   0:  33% | abe: 9.498 | eve: 9.517 | bob: 9.425Epoch   0:  34% | abe: 9.483 | eve: 9.516 | bob: 9.410Epoch   0:  35% | abe: 9.465 | eve: 9.518 | bob: 9.393Epoch   0:  35% | abe: 9.450 | eve: 9.514 | bob: 9.378Epoch   0:  36% | abe: 9.446 | eve: 9.518 | bob: 9.373Epoch   0:  37% | abe: 9.439 | eve: 9.512 | bob: 9.366Epoch   0:  38% | abe: 9.433 | eve: 9.501 | bob: 9.360Epoch   0:  39% | abe: 9.426 | eve: 9.496 | bob: 9.353Epoch   0:  39% | abe: 9.412 | eve: 9.502 | bob: 9.340Epoch   0:  40% | abe: 9.404 | eve: 9.507 | bob: 9.332Epoch   0:  41% | abe: 9.394 | eve: 9.513 | bob: 9.322Epoch   0:  42% | abe: 9.391 | eve: 9.525 | bob: 9.318Epoch   0:  42% | abe: 9.390 | eve: 9.514 | bob: 9.318Epoch   0:  43% | abe: 9.389 | eve: 9.506 | bob: 9.317Epoch   0:  44% | abe: 9.380 | eve: 9.510 | bob: 9.309Epoch   0:  45% | abe: 9.383 | eve: 9.507 | bob: 9.312Epoch   0:  46% | abe: 9.376 | eve: 9.511 | bob: 9.305Epoch   0:  46% | abe: 9.369 | eve: 9.511 | bob: 9.298Epoch   0:  47% | abe: 9.366 | eve: 9.511 | bob: 9.296Epoch   0:  48% | abe: 9.359 | eve: 9.508 | bob: 9.288Epoch   0:  49% | abe: 9.356 | eve: 9.514 | bob: 9.286Epoch   0:  50% | abe: 9.353 | eve: 9.507 | bob: 9.283Epoch   0:  50% | abe: 9.348 | eve: 9.499 | bob: 9.278Epoch   0:  51% | abe: 9.344 | eve: 9.512 | bob: 9.275Epoch   0:  52% | abe: 9.336 | eve: 9.526 | bob: 9.266Epoch   0:  53% | abe: 9.333 | eve: 9.528 | bob: 9.263Epoch   0:  53% | abe: 9.333 | eve: 9.532 | bob: 9.264Epoch   0:  54% | abe: 9.328 | eve: 9.536 | bob: 9.259Epoch   0:  55% | abe: 9.325 | eve: 9.540 | bob: 9.256Epoch   0:  56% | abe: 9.316 | eve: 9.540 | bob: 9.246Epoch   0:  57% | abe: 9.315 | eve: 9.543 | bob: 9.245Epoch   0:  57% | abe: 9.311 | eve: 9.538 | bob: 9.242Epoch   0:  58% | abe: 9.305 | eve: 9.541 | bob: 9.236Epoch   0:  59% | abe: 9.301 | eve: 9.544 | bob: 9.232Epoch   0:  60% | abe: 9.297 | eve: 9.548 | bob: 9.228Epoch   0:  60% | abe: 9.294 | eve: 9.548 | bob: 9.225Epoch   0:  61% | abe: 9.287 | eve: 9.550 | bob: 9.217Epoch   0:  62% | abe: 9.286 | eve: 9.544 | bob: 9.217Epoch   0:  63% | abe: 9.288 | eve: 9.538 | bob: 9.219Epoch   0:  64% | abe: 9.284 | eve: 9.537 | bob: 9.214Epoch   0:  64% | abe: 9.280 | eve: 9.535 | bob: 9.210Epoch   0:  65% | abe: 9.280 | eve: 9.536 | bob: 9.211Epoch   0:  66% | abe: 9.273 | eve: 9.540 | bob: 9.204Epoch   0:  67% | abe: 9.269 | eve: 9.540 | bob: 9.200Epoch   0:  67% | abe: 9.260 | eve: 9.552 | bob: 9.191Epoch   0:  68% | abe: 9.257 | eve: 9.558 | bob: 9.188Epoch   0:  69% | abe: 9.256 | eve: 9.560 | bob: 9.186Epoch   0:  70% | abe: 9.256 | eve: 9.558 | bob: 9.187Epoch   0:  71% | abe: 9.252 | eve: 9.560 | bob: 9.183Epoch   0:  71% | abe: 9.250 | eve: 9.554 | bob: 9.180Epoch   0:  72% | abe: 9.247 | eve: 9.552 | bob: 9.178Epoch   0:  73% | abe: 9.241 | eve: 9.554 | bob: 9.172Epoch   0:  74% | abe: 9.239 | eve: 9.549 | bob: 9.170Epoch   0:  75% | abe: 9.238 | eve: 9.550 | bob: 9.169Epoch   0:  75% | abe: 9.237 | eve: 9.550 | bob: 9.168Epoch   0:  76% | abe: 9.238 | eve: 9.548 | bob: 9.168Epoch   0:  77% | abe: 9.235 | eve: 9.545 | bob: 9.165Epoch   0:  78% | abe: 9.233 | eve: 9.546 | bob: 9.164Epoch   0:  78% | abe: 9.233 | eve: 9.547 | bob: 9.164Epoch   0:  79% | abe: 9.232 | eve: 9.543 | bob: 9.162Epoch   0:  80% | abe: 9.231 | eve: 9.544 | bob: 9.161Epoch   0:  81% | abe: 9.228 | eve: 9.548 | bob: 9.158Epoch   0:  82% | abe: 9.227 | eve: 9.550 | bob: 9.157Epoch   0:  82% | abe: 9.224 | eve: 9.556 | bob: 9.154Epoch   0:  83% | abe: 9.226 | eve: 9.552 | bob: 9.156Epoch   0:  84% | abe: 9.227 | eve: 9.547 | bob: 9.157Epoch   0:  85% | abe: 9.225 | eve: 9.543 | bob: 9.155Epoch   0:  85% | abe: 9.222 | eve: 9.546 | bob: 9.152Epoch   0:  86% | abe: 9.224 | eve: 9.543 | bob: 9.154Epoch   0:  87% | abe: 9.227 | eve: 9.541 | bob: 9.157Epoch   0:  88% | abe: 9.225 | eve: 9.541 | bob: 9.155Epoch   0:  89% | abe: 9.225 | eve: 9.545 | bob: 9.154Epoch   0:  89% | abe: 9.222 | eve: 9.544 | bob: 9.152Epoch   0:  90% | abe: 9.220 | eve: 9.541 | bob: 9.150Epoch   0:  91% | abe: 9.220 | eve: 9.537 | bob: 9.150Epoch   0:  92% | abe: 9.215 | eve: 9.532 | bob: 9.145Epoch   0:  92% | abe: 9.216 | eve: 9.532 | bob: 9.146Epoch   0:  93% | abe: 9.217 | eve: 9.534 | bob: 9.146Epoch   0:  94% | abe: 9.217 | eve: 9.536 | bob: 9.147Epoch   0:  95% | abe: 9.221 | eve: 9.538 | bob: 9.151Epoch   0:  96% | abe: 9.218 | eve: 9.538 | bob: 9.147Epoch   0:  96% | abe: 9.218 | eve: 9.535 | bob: 9.148Epoch   0:  97% | abe: 9.217 | eve: 9.535 | bob: 9.146Epoch   0:  98% | abe: 9.214 | eve: 9.536 | bob: 9.144Epoch   0:  99% | abe: 9.217 | eve: 9.540 | bob: 9.146
New best Bob loss 9.145966526085978 at epoch 0
Epoch   1:   0% | abe: 8.790 | eve: 9.654 | bob: 9.724Epoch   1:   0% | abe: 9.073 | eve: 9.556 | bob: 9.999Epoch   1:   1% | abe: 9.021 | eve: 9.721 | bob: 9.952Epoch   1:   2% | abe: 9.009 | eve: 9.519 | bob: 9.937Epoch   1:   3% | abe: 9.007 | eve: 9.526 | bob: 9.936Epoch   1:   3% | abe: 8.987 | eve: 9.444 | bob: 9.914Epoch   1:   4% | abe: 8.977 | eve: 9.453 | bob: 9.903Epoch   1:   5% | abe: 9.057 | eve: 9.472 | bob: 9.986Epoch   1:   6% | abe: 9.057 | eve: 9.510 | bob: 9.986Epoch   1:   7% | abe: 9.049 | eve: 9.506 | bob: 9.977Epoch   1:   7% | abe: 9.080 | eve: 9.551 | bob: 10.010Epoch   1:   8% | abe: 9.047 | eve: 9.652 | bob: 9.975Epoch   1:   9% | abe: 9.036 | eve: 9.650 | bob: 9.963Epoch   1:  10% | abe: 9.043 | eve: 9.650 | bob: 9.971Epoch   1:  10% | abe: 9.057 | eve: 9.690 | bob: 9.986Epoch   1:  11% | abe: 9.060 | eve: 9.704 | bob: 9.989Epoch   1:  12% | abe: 9.054 | eve: 9.693 | bob: 9.984Epoch   1:  13% | abe: 9.066 | eve: 9.664 | bob: 9.997Epoch   1:  14% | abe: 9.063 | eve: 9.660 | bob: 9.994Epoch   1:  14% | abe: 9.070 | eve: 9.667 | bob: 10.001Epoch   1:  15% | abe: 9.076 | eve: 9.656 | bob: 10.007Epoch   1:  16% | abe: 9.081 | eve: 9.673 | bob: 10.012Epoch   1:  17% | abe: 9.080 | eve: 9.646 | bob: 10.011Epoch   1:  17% | abe: 9.066 | eve: 9.636 | bob: 9.997Epoch   1:  18% | abe: 9.054 | eve: 9.649 | bob: 9.985Epoch   1:  19% | abe: 9.066 | eve: 9.639 | bob: 9.997Epoch   1:  20% | abe: 9.074 | eve: 9.659 | bob: 10.005Epoch   1:  21% | abe: 9.085 | eve: 9.647 | bob: 10.017Epoch   1:  21% | abe: 9.089 | eve: 9.623 | bob: 10.021Epoch   1:  22% | abe: 9.087 | eve: 9.623 | bob: 10.019Epoch   1:  23% | abe: 9.086 | eve: 9.623 | bob: 10.017Epoch   1:  24% | abe: 9.101 | eve: 9.631 | bob: 10.033Epoch   1:  25% | abe: 9.091 | eve: 9.621 | bob: 10.022Epoch   1:  25% | abe: 9.097 | eve: 9.620 | bob: 10.028Epoch   1:  26% | abe: 9.102 | eve: 9.621 | bob: 10.034Epoch   1:  27% | abe: 9.097 | eve: 9.620 | bob: 10.028Epoch   1:  28% | abe: 9.099 | eve: 9.628 | bob: 10.030Epoch   1:  28% | abe: 9.110 | eve: 9.630 | bob: 10.041Epoch   1:  29% | abe: 9.111 | eve: 9.635 | bob: 10.042Epoch   1:  30% | abe: 9.109 | eve: 9.634 | bob: 10.040Epoch   1:  31% | abe: 9.115 | eve: 9.628 | bob: 10.046Epoch   1:  32% | abe: 9.107 | eve: 9.632 | bob: 10.039Epoch   1:  32% | abe: 9.113 | eve: 9.633 | bob: 10.044Epoch   1:  33% | abe: 9.117 | eve: 9.636 | bob: 10.049Epoch   1:  34% | abe: 9.118 | eve: 9.642 | bob: 10.050Epoch   1:  35% | abe: 9.122 | eve: 9.635 | bob: 10.054Epoch   1:  35% | abe: 9.110 | eve: 9.619 | bob: 10.042Epoch   1:  36% | abe: 9.097 | eve: 9.600 | bob: 10.029Epoch   1:  37% | abe: 9.103 | eve: 9.588 | bob: 10.034Epoch   1:  38% | abe: 9.102 | eve: 9.591 | bob: 10.034Epoch   1:  39% | abe: 9.104 | eve: 9.590 | bob: 10.035Epoch   1:  39% | abe: 9.101 | eve: 9.598 | bob: 10.033Epoch   1:  40% | abe: 9.103 | eve: 9.598 | bob: 10.034Epoch   1:  41% | abe: 9.102 | eve: 9.599 | bob: 10.033Epoch   1:  42% | abe: 9.105 | eve: 9.592 | bob: 10.036Epoch   1:  42% | abe: 9.097 | eve: 9.583 | bob: 10.027Epoch   1:  43% | abe: 9.093 | eve: 9.592 | bob: 10.024Epoch   1:  44% | abe: 9.090 | eve: 9.594 | bob: 10.020Epoch   1:  45% | abe: 9.092 | eve: 9.589 | bob: 10.022Epoch   1:  46% | abe: 9.088 | eve: 9.591 | bob: 10.018Epoch   1:  46% | abe: 9.088 | eve: 9.592 | bob: 10.019Epoch   1:  47% | abe: 9.089 | eve: 9.596 | bob: 10.020Epoch   1:  48% | abe: 9.091 | eve: 9.597 | bob: 10.022Epoch   1:  49% | abe: 9.096 | eve: 9.590 | bob: 10.027Epoch   1:  50% | abe: 9.089 | eve: 9.586 | bob: 10.020Epoch   1:  50% | abe: 9.083 | eve: 9.583 | bob: 10.013Epoch   1:  51% | abe: 9.086 | eve: 9.586 | bob: 10.016Epoch   1:  52% | abe: 9.084 | eve: 9.584 | bob: 10.014Epoch   1:  53% | abe: 9.081 | eve: 9.584 | bob: 10.011Epoch   1:  53% | abe: 9.082 | eve: 9.589 | bob: 10.012Epoch   1:  54% | abe: 9.079 | eve: 9.595 | bob: 10.009Epoch   1:  55% | abe: 9.076 | eve: 9.595 | bob: 10.006Epoch   1:  56% | abe: 9.074 | eve: 9.601 | bob: 10.004Epoch   1:  57% | abe: 9.075 | eve: 9.599 | bob: 10.005Epoch   1:  57% | abe: 9.076 | eve: 9.603 | bob: 10.007Epoch   1:  58% | abe: 9.075 | eve: 9.602 | bob: 10.005Epoch   1:  59% | abe: 9.072 | eve: 9.606 | bob: 10.003Epoch   1:  60% | abe: 9.071 | eve: 9.606 | bob: 10.001Epoch   1:  60% | abe: 9.072 | eve: 9.604 | bob: 10.002Epoch   1:  61% | abe: 9.070 | eve: 9.601 | bob: 10.000Epoch   1:  62% | abe: 9.075 | eve: 9.600 | bob: 10.004Epoch   1:  63% | abe: 9.071 | eve: 9.598 | bob: 10.001Epoch   1:  64% | abe: 9.071 | eve: 9.591 | bob: 10.000Epoch   1:  64% | abe: 9.074 | eve: 9.587 | bob: 10.004Epoch   1:  65% | abe: 9.069 | eve: 9.586 | bob: 9.999Epoch   1:  66% | abe: 9.069 | eve: 9.584 | bob: 9.999Epoch   1:  67% | abe: 9.065 | eve: 9.589 | bob: 9.995Epoch   1:  67% | abe: 9.064 | eve: 9.589 | bob: 9.994Epoch   1:  68% | abe: 9.061 | eve: 9.588 | bob: 9.990Epoch   1:  69% | abe: 9.058 | eve: 9.585 | bob: 9.988Epoch   1:  70% | abe: 9.057 | eve: 9.588 | bob: 9.986Epoch   1:  71% | abe: 9.055 | eve: 9.587 | bob: 9.985Epoch   1:  71% | abe: 9.057 | eve: 9.589 | bob: 9.986Epoch   1:  72% | abe: 9.058 | eve: 9.593 | bob: 9.988Epoch   1:  73% | abe: 9.059 | eve: 9.590 | bob: 9.988Epoch   1:  74% | abe: 9.060 | eve: 9.594 | bob: 9.989Epoch   1:  75% | abe: 9.058 | eve: 9.591 | bob: 9.987Epoch   1:  75% | abe: 9.063 | eve: 9.594 | bob: 9.992Epoch   1:  76% | abe: 9.072 | eve: 9.601 | bob: 10.001Epoch   1:  77% | abe: 9.071 | eve: 9.597 | bob: 10.001Epoch   1:  78% | abe: 9.068 | eve: 9.601 | bob: 9.997Epoch   1:  78% | abe: 9.067 | eve: 9.599 | bob: 9.997Epoch   1:  79% | abe: 9.068 | eve: 9.599 | bob: 9.997Epoch   1:  80% | abe: 9.068 | eve: 9.600 | bob: 9.998Epoch   1:  81% | abe: 9.065 | eve: 9.597 | bob: 9.994Epoch   1:  82% | abe: 9.065 | eve: 9.597 | bob: 9.994Epoch   1:  82% | abe: 9.064 | eve: 9.598 | bob: 9.993Epoch   1:  83% | abe: 9.063 | eve: 9.598 | bob: 9.992Epoch   1:  84% | abe: 9.065 | eve: 9.598 | bob: 9.994Epoch   1:  85% | abe: 9.062 | eve: 9.595 | bob: 9.990Epoch   1:  85% | abe: 9.060 | eve: 9.592 | bob: 9.989Epoch   1:  86% | abe: 9.062 | eve: 9.593 | bob: 9.991Epoch   1:  87% | abe: 9.061 | eve: 9.590 | bob: 9.989Epoch   1:  88% | abe: 9.059 | eve: 9.591 | bob: 9.988Epoch   1:  89% | abe: 9.058 | eve: 9.591 | bob: 9.987Epoch   1:  89% | abe: 9.057 | eve: 9.589 | bob: 9.986Epoch   1:  90% | abe: 9.057 | eve: 9.586 | bob: 9.986Epoch   1:  91% | abe: 9.055 | eve: 9.588 | bob: 9.983Epoch   1:  92% | abe: 9.055 | eve: 9.588 | bob: 9.984Epoch   1:  92% | abe: 9.051 | eve: 9.590 | bob: 9.979Epoch   1:  93% | abe: 9.051 | eve: 9.593 | bob: 9.979Epoch   1:  94% | abe: 9.048 | eve: 9.594 | bob: 9.977Epoch   1:  95% | abe: 9.047 | eve: 9.590 | bob: 9.975Epoch   1:  96% | abe: 9.046 | eve: 9.592 | bob: 9.975Epoch   1:  96% | abe: 9.045 | eve: 9.595 | bob: 9.973Epoch   1:  97% | abe: 9.044 | eve: 9.594 | bob: 9.972Epoch   1:  98% | abe: 9.043 | eve: 9.594 | bob: 9.971Epoch   1:  99% | abe: 9.043 | eve: 9.596 | bob: 9.971Epoch   2:   0% | abe: 9.291 | eve: 9.630 | bob: 11.243Epoch   2:   0% | abe: 9.275 | eve: 9.639 | bob: 11.220Epoch   2:   1% | abe: 9.132 | eve: 9.497 | bob: 11.073Epoch   2:   2% | abe: 9.034 | eve: 9.477 | bob: 10.969Epoch   2:   3% | abe: 9.105 | eve: 9.398 | bob: 11.041Epoch   2:   3% | abe: 9.189 | eve: 9.410 | bob: 11.126Epoch   2:   4% | abe: 9.183 | eve: 9.461 | bob: 11.121Epoch   2:   5% | abe: 9.190 | eve: 9.545 | bob: 11.127Epoch   2:   6% | abe: 9.198 | eve: 9.519 | bob: 11.137Epoch   2:   7% | abe: 9.183 | eve: 9.530 | bob: 11.120Epoch   2:   7% | abe: 9.162 | eve: 9.578 | bob: 11.098Epoch   2:   8% | abe: 9.111 | eve: 9.638 | bob: 11.045Epoch   2:   9% | abe: 9.091 | eve: 9.656 | bob: 11.025Epoch   2:  10% | abe: 9.082 | eve: 9.644 | bob: 11.016Epoch   2:  10% | abe: 9.053 | eve: 9.647 | bob: 10.984Epoch   2:  11% | abe: 9.049 | eve: 9.650 | bob: 10.979Epoch   2:  12% | abe: 9.060 | eve: 9.622 | bob: 10.989Epoch   2:  13% | abe: 9.067 | eve: 9.622 | bob: 10.996Epoch   2:  14% | abe: 9.060 | eve: 9.621 | bob: 10.988Epoch   2:  14% | abe: 9.053 | eve: 9.615 | bob: 10.982Epoch   2:  15% | abe: 9.074 | eve: 9.636 | bob: 11.003Epoch   2:  16% | abe: 9.074 | eve: 9.632 | bob: 11.003Epoch   2:  17% | abe: 9.096 | eve: 9.625 | bob: 11.025Epoch   2:  17% | abe: 9.103 | eve: 9.636 | bob: 11.032Epoch   2:  18% | abe: 9.094 | eve: 9.621 | bob: 11.022Epoch   2:  19% | abe: 9.086 | eve: 9.633 | bob: 11.014Epoch   2:  20% | abe: 9.074 | eve: 9.658 | bob: 11.002Epoch   2:  21% | abe: 9.074 | eve: 9.664 | bob: 11.002Epoch   2:  21% | abe: 9.077 | eve: 9.669 | bob: 11.004Epoch   2:  22% | abe: 9.070 | eve: 9.655 | bob: 10.998Epoch   2:  23% | abe: 9.066 | eve: 9.650 | bob: 10.993Epoch   2:  24% | abe: 9.062 | eve: 9.642 | bob: 10.989Epoch   2:  25% | abe: 9.058 | eve: 9.645 | bob: 10.985Epoch   2:  25% | abe: 9.062 | eve: 9.613 | bob: 10.988Epoch   2:  26% | abe: 9.049 | eve: 9.606 | bob: 10.976Epoch   2:  27% | abe: 9.050 | eve: 9.614 | bob: 10.976Epoch   2:  28% | abe: 9.055 | eve: 9.612 | bob: 10.981Epoch   2:  28% | abe: 9.045 | eve: 9.601 | bob: 10.971Epoch   2:  29% | abe: 9.046 | eve: 9.613 | bob: 10.971Epoch   2:  30% | abe: 9.039 | eve: 9.620 | bob: 10.965Epoch   2:  31% | abe: 9.035 | eve: 9.628 | bob: 10.960Epoch   2:  32% | abe: 9.022 | eve: 9.636 | bob: 10.947Epoch   2:  32% | abe: 9.026 | eve: 9.638 | bob: 10.951Epoch   2:  33% | abe: 9.024 | eve: 9.645 | bob: 10.948Epoch   2:  34% | abe: 9.015 | eve: 9.648 | bob: 10.939Epoch   2:  35% | abe: 9.012 | eve: 9.645 | bob: 10.936Epoch   2:  35% | abe: 9.011 | eve: 9.642 | bob: 10.934Epoch   2:  36% | abe: 9.013 | eve: 9.643 | bob: 10.936Epoch   2:  37% | abe: 9.013 | eve: 9.636 | bob: 10.936Epoch   2:  38% | abe: 9.014 | eve: 9.628 | bob: 10.938Epoch   2:  39% | abe: 9.010 | eve: 9.619 | bob: 10.933Epoch   2:  39% | abe: 9.008 | eve: 9.616 | bob: 10.932Epoch   2:  40% | abe: 9.008 | eve: 9.612 | bob: 10.931Epoch   2:  41% | abe: 9.004 | eve: 9.607 | bob: 10.928Epoch   2:  42% | abe: 8.998 | eve: 9.613 | bob: 10.921Epoch   2:  42% | abe: 8.992 | eve: 9.616 | bob: 10.915Epoch   2:  43% | abe: 8.990 | eve: 9.625 | bob: 10.912Epoch   2:  44% | abe: 8.992 | eve: 9.618 | bob: 10.914Epoch   2:  45% | abe: 8.987 | eve: 9.615 | bob: 10.909Epoch   2:  46% | abe: 8.992 | eve: 9.618 | bob: 10.914Epoch   2:  46% | abe: 8.994 | eve: 9.610 | bob: 10.916Epoch   2:  47% | abe: 8.992 | eve: 9.610 | bob: 10.914Epoch   2:  48% | abe: 8.995 | eve: 9.615 | bob: 10.917Epoch   2:  49% | abe: 8.989 | eve: 9.616 | bob: 10.911Epoch   2:  50% | abe: 8.986 | eve: 9.621 | bob: 10.908Epoch   2:  50% | abe: 8.983 | eve: 9.624 | bob: 10.904Epoch   2:  51% | abe: 8.982 | eve: 9.624 | bob: 10.903Epoch   2:  52% | abe: 8.987 | eve: 9.630 | bob: 10.908Epoch   2:  53% | abe: 8.988 | eve: 9.628 | bob: 10.909Epoch   2:  53% | abe: 8.988 | eve: 9.634 | bob: 10.909Epoch   2:  54% | abe: 8.989 | eve: 9.621 | bob: 10.911Epoch   2:  55% | abe: 8.984 | eve: 9.628 | bob: 10.905Epoch   2:  56% | abe: 8.985 | eve: 9.635 | bob: 10.906Epoch   2:  57% | abe: 8.987 | eve: 9.636 | bob: 10.909Epoch   2:  57% | abe: 8.988 | eve: 9.640 | bob: 10.909Epoch   2:  58% | abe: 8.985 | eve: 9.644 | bob: 10.906Epoch   2:  59% | abe: 8.982 | eve: 9.648 | bob: 10.903Epoch   2:  60% | abe: 8.980 | eve: 9.649 | bob: 10.901Epoch   2:  60% | abe: 8.977 | eve: 9.654 | bob: 10.898Epoch   2:  61% | abe: 8.974 | eve: 9.648 | bob: 10.895Epoch   2:  62% | abe: 8.970 | eve: 9.645 | bob: 10.890Epoch   2:  63% | abe: 8.967 | eve: 9.639 | bob: 10.888Epoch   2:  64% | abe: 8.965 | eve: 9.636 | bob: 10.885Epoch   2:  64% | abe: 8.967 | eve: 9.637 | bob: 10.887Epoch   2:  65% | abe: 8.964 | eve: 9.629 | bob: 10.884Epoch   2:  66% | abe: 8.964 | eve: 9.631 | bob: 10.884Epoch   2:  67% | abe: 8.970 | eve: 9.624 | bob: 10.890Epoch   2:  67% | abe: 8.971 | eve: 9.623 | bob: 10.892Epoch   2:  68% | abe: 8.970 | eve: 9.626 | bob: 10.891Epoch   2:  69% | abe: 8.971 | eve: 9.622 | bob: 10.891Epoch   2:  70% | abe: 8.970 | eve: 9.622 | bob: 10.891Epoch   2:  71% | abe: 8.970 | eve: 9.620 | bob: 10.891Epoch   2:  71% | abe: 8.969 | eve: 9.616 | bob: 10.890Epoch   2:  72% | abe: 8.972 | eve: 9.615 | bob: 10.892Epoch   2:  73% | abe: 8.971 | eve: 9.613 | bob: 10.891Epoch   2:  74% | abe: 8.970 | eve: 9.617 | bob: 10.891Epoch   2:  75% | abe: 8.973 | eve: 9.616 | bob: 10.894Epoch   2:  75% | abe: 8.974 | eve: 9.618 | bob: 10.894Epoch   2:  76% | abe: 8.972 | eve: 9.624 | bob: 10.892Epoch   2:  77% | abe: 8.972 | eve: 9.630 | bob: 10.893Epoch   2:  78% | abe: 8.976 | eve: 9.625 | bob: 10.897Epoch   2:  78% | abe: 8.972 | eve: 9.626 | bob: 10.892Epoch   2:  79% | abe: 8.970 | eve: 9.627 | bob: 10.891Epoch   2:  80% | abe: 8.973 | eve: 9.627 | bob: 10.893Epoch   2:  81% | abe: 8.971 | eve: 9.625 | bob: 10.892Epoch   2:  82% | abe: 8.975 | eve: 9.624 | bob: 10.896Epoch   2:  82% | abe: 8.971 | eve: 9.622 | bob: 10.892Epoch   2:  83% | abe: 8.973 | eve: 9.626 | bob: 10.893Epoch   2:  84% | abe: 8.970 | eve: 9.622 | bob: 10.890Epoch   2:  85% | abe: 8.973 | eve: 9.620 | bob: 10.894Epoch   2:  85% | abe: 8.968 | eve: 9.619 | bob: 10.889Epoch   2:  86% | abe: 8.971 | eve: 9.620 | bob: 10.892Epoch   2:  87% | abe: 8.971 | eve: 9.616 | bob: 10.892Epoch   2:  88% | abe: 8.973 | eve: 9.617 | bob: 10.894Epoch   2:  89% | abe: 8.974 | eve: 9.617 | bob: 10.895Epoch   2:  89% | abe: 8.972 | eve: 9.614 | bob: 10.893Epoch   2:  90% | abe: 8.971 | eve: 9.619 | bob: 10.892Epoch   2:  91% | abe: 8.970 | eve: 9.614 | bob: 10.891Epoch   2:  92% | abe: 8.969 | eve: 9.612 | bob: 10.890Epoch   2:  92% | abe: 8.967 | eve: 9.614 | bob: 10.888Epoch   2:  93% | abe: 8.967 | eve: 9.611 | bob: 10.888Epoch   2:  94% | abe: 8.967 | eve: 9.611 | bob: 10.888Epoch   2:  95% | abe: 8.966 | eve: 9.611 | bob: 10.887Epoch   2:  96% | abe: 8.967 | eve: 9.611 | bob: 10.888Epoch   2:  96% | abe: 8.963 | eve: 9.612 | bob: 10.884Epoch   2:  97% | abe: 8.960 | eve: 9.608 | bob: 10.881Epoch   2:  98% | abe: 8.959 | eve: 9.609 | bob: 10.881Epoch   2:  99% | abe: 8.957 | eve: 9.616 | bob: 10.878Epoch   3:   0% | abe: 9.228 | eve: 9.208 | bob: 12.147Epoch   3:   0% | abe: 8.894 | eve: 9.277 | bob: 11.806Epoch   3:   1% | abe: 8.991 | eve: 9.345 | bob: 11.914Epoch   3:   2% | abe: 9.030 | eve: 9.428 | bob: 11.959Epoch   3:   3% | abe: 9.053 | eve: 9.544 | bob: 11.978Epoch   3:   3% | abe: 9.079 | eve: 9.587 | bob: 12.006Epoch   3:   4% | abe: 9.051 | eve: 9.596 | bob: 11.976Epoch   3:   5% | abe: 9.059 | eve: 9.636 | bob: 11.985Epoch   3:   6% | abe: 9.098 | eve: 9.639 | bob: 12.025Epoch   3:   7% | abe: 9.084 | eve: 9.640 | bob: 12.010Epoch   3:   7% | abe: 9.060 | eve: 9.625 | bob: 11.985Epoch   3:   8% | abe: 9.064 | eve: 9.609 | bob: 11.988Epoch   3:   9% | abe: 9.046 | eve: 9.599 | bob: 11.970Epoch   3:  10% | abe: 9.040 | eve: 9.605 | bob: 11.964Epoch   3:  10% | abe: 9.015 | eve: 9.608 | bob: 11.939Epoch   3:  11% | abe: 8.999 | eve: 9.612 | bob: 11.922Epoch   3:  12% | abe: 8.998 | eve: 9.644 | bob: 11.921Epoch   3:  13% | abe: 8.981 | eve: 9.670 | bob: 11.905Epoch   3:  14% | abe: 8.965 | eve: 9.723 | bob: 11.888Epoch   3:  14% | abe: 8.952 | eve: 9.712 | bob: 11.874Epoch   3:  15% | abe: 8.947 | eve: 9.724 | bob: 11.868Epoch   3:  16% | abe: 8.951 | eve: 9.699 | bob: 11.872Epoch   3:  17% | abe: 8.946 | eve: 9.728 | bob: 11.866Epoch   3:  17% | abe: 8.950 | eve: 9.740 | bob: 11.870Epoch   3:  18% | abe: 8.959 | eve: 9.716 | bob: 11.879Epoch   3:  19% | abe: 8.942 | eve: 9.700 | bob: 11.862Epoch   3:  20% | abe: 8.935 | eve: 9.697 | bob: 11.855Epoch   3:  21% | abe: 8.923 | eve: 9.702 | bob: 11.844Epoch   3:  21% | abe: 8.924 | eve: 9.685 | bob: 11.845Epoch   3:  22% | abe: 8.918 | eve: 9.698 | bob: 11.838Epoch   3:  23% | abe: 8.926 | eve: 9.684 | bob: 11.847Epoch   3:  24% | abe: 8.926 | eve: 9.679 | bob: 11.846Epoch   3:  25% | abe: 8.913 | eve: 9.672 | bob: 11.832Epoch   3:  25% | abe: 8.917 | eve: 9.660 | bob: 11.837Epoch   3:  26% | abe: 8.926 | eve: 9.647 | bob: 11.847Epoch   3:  27% | abe: 8.931 | eve: 9.658 | bob: 11.852Epoch   3:  28% | abe: 8.933 | eve: 9.646 | bob: 11.855Epoch   3:  28% | abe: 8.934 | eve: 9.643 | bob: 11.855Epoch   3:  29% | abe: 8.940 | eve: 9.642 | bob: 11.862Epoch   3:  30% | abe: 8.944 | eve: 9.643 | bob: 11.866Epoch   3:  31% | abe: 8.940 | eve: 9.654 | bob: 11.863Epoch   3:  32% | abe: 8.948 | eve: 9.663 | bob: 11.870Epoch   3:  32% | abe: 8.957 | eve: 9.662 | bob: 11.880Epoch   3:  33% | abe: 8.952 | eve: 9.667 | bob: 11.874Epoch   3:  34% | abe: 8.958 | eve: 9.666 | bob: 11.880Epoch   3:  35% | abe: 8.967 | eve: 9.672 | bob: 11.888Epoch   3:  35% | abe: 8.963 | eve: 9.671 | bob: 11.884Epoch   3:  36% | abe: 8.964 | eve: 9.670 | bob: 11.886Epoch   3:  37% | abe: 8.959 | eve: 9.673 | bob: 11.880Epoch   3:  38% | abe: 8.959 | eve: 9.659 | bob: 11.880Epoch   3:  39% | abe: 8.961 | eve: 9.657 | bob: 11.883Epoch   3:  39% | abe: 8.959 | eve: 9.641 | bob: 11.880Epoch   3:  40% | abe: 8.953 | eve: 9.641 | bob: 11.874Epoch   3:  41% | abe: 8.957 | eve: 9.644 | bob: 11.878Epoch   3:  42% | abe: 8.963 | eve: 9.648 | bob: 11.884Epoch   3:  42% | abe: 8.958 | eve: 9.646 | bob: 11.880Epoch   3:  43% | abe: 8.959 | eve: 9.642 | bob: 11.881Epoch   3:  44% | abe: 8.962 | eve: 9.639 | bob: 11.884Epoch   3:  45% | abe: 8.965 | eve: 9.643 | bob: 11.887Epoch   3:  46% | abe: 8.972 | eve: 9.632 | bob: 11.894Epoch   3:  46% | abe: 8.977 | eve: 9.637 | bob: 11.899Epoch   3:  47% | abe: 8.978 | eve: 9.638 | bob: 11.901Epoch   3:  48% | abe: 8.984 | eve: 9.638 | bob: 11.907Epoch   3:  49% | abe: 8.977 | eve: 9.644 | bob: 11.899Epoch   3:  50% | abe: 8.981 | eve: 9.641 | bob: 11.904Epoch   3:  50% | abe: 8.982 | eve: 9.643 | bob: 11.906Epoch   3:  51% | abe: 8.982 | eve: 9.649 | bob: 11.905Epoch   3:  52% | abe: 8.984 | eve: 9.649 | bob: 11.907Epoch   3:  53% | abe: 8.977 | eve: 9.655 | bob: 11.900Epoch   3:  53% | abe: 8.976 | eve: 9.659 | bob: 11.899Epoch   3:  54% | abe: 8.977 | eve: 9.658 | bob: 11.899Epoch   3:  55% | abe: 8.980 | eve: 9.660 | bob: 11.903Epoch   3:  56% | abe: 8.976 | eve: 9.660 | bob: 11.899Epoch   3:  57% | abe: 8.976 | eve: 9.662 | bob: 11.899Epoch   3:  57% | abe: 8.970 | eve: 9.668 | bob: 11.893Epoch   3:  58% | abe: 8.965 | eve: 9.674 | bob: 11.888Epoch   3:  59% | abe: 8.960 | eve: 9.676 | bob: 11.882Epoch   3:  60% | abe: 8.962 | eve: 9.671 | bob: 11.884Epoch   3:  60% | abe: 8.962 | eve: 9.677 | bob: 11.885Epoch   3:  61% | abe: 8.961 | eve: 9.678 | bob: 11.883Epoch   3:  62% | abe: 8.961 | eve: 9.683 | bob: 11.883Epoch   3:  63% | abe: 8.961 | eve: 9.680 | bob: 11.883Epoch   3:  64% | abe: 8.959 | eve: 9.680 | bob: 11.881Epoch   3:  64% | abe: 8.955 | eve: 9.681 | bob: 11.877Epoch   3:  65% | abe: 8.952 | eve: 9.682 | bob: 11.874Epoch   3:  66% | abe: 8.951 | eve: 9.678 | bob: 11.873Epoch   3:  67% | abe: 8.952 | eve: 9.679 | bob: 11.874Epoch   3:  67% | abe: 8.953 | eve: 9.678 | bob: 11.875Epoch   3:  68% | abe: 8.953 | eve: 9.678 | bob: 11.874Epoch   3:  69% | abe: 8.952 | eve: 9.678 | bob: 11.873Epoch   3:  70% | abe: 8.948 | eve: 9.683 | bob: 11.869Epoch   3:  71% | abe: 8.947 | eve: 9.686 | bob: 11.869Epoch   3:  71% | abe: 8.949 | eve: 9.684 | bob: 11.871Epoch   3:  72% | abe: 8.947 | eve: 9.684 | bob: 11.869Epoch   3:  73% | abe: 8.951 | eve: 9.683 | bob: 11.872Epoch   3:  74% | abe: 8.952 | eve: 9.683 | bob: 11.873Epoch   3:  75% | abe: 8.952 | eve: 9.683 | bob: 11.873Epoch   3:  75% | abe: 8.949 | eve: 9.680 | bob: 11.870Epoch   3:  76% | abe: 8.947 | eve: 9.681 | bob: 11.868Epoch   3:  77% | abe: 8.945 | eve: 9.684 | bob: 11.866Epoch   3:  78% | abe: 8.946 | eve: 9.687 | bob: 11.867Epoch   3:  78% | abe: 8.947 | eve: 9.686 | bob: 11.868Epoch   3:  79% | abe: 8.950 | eve: 9.688 | bob: 11.871Epoch   3:  80% | abe: 8.949 | eve: 9.683 | bob: 11.870Epoch   3:  81% | abe: 8.947 | eve: 9.683 | bob: 11.868Epoch   3:  82% | abe: 8.944 | eve: 9.686 | bob: 11.865Epoch   3:  82% | abe: 8.944 | eve: 9.686 | bob: 11.864Epoch   3:  83% | abe: 8.942 | eve: 9.686 | bob: 11.862Epoch   3:  84% | abe: 8.940 | eve: 9.681 | bob: 11.860Epoch   3:  85% | abe: 8.942 | eve: 9.681 | bob: 11.862Epoch   3:  85% | abe: 8.941 | eve: 9.684 | bob: 11.861Epoch   3:  86% | abe: 8.942 | eve: 9.688 | bob: 11.863Epoch   3:  87% | abe: 8.940 | eve: 9.686 | bob: 11.860Epoch   3:  88% | abe: 8.941 | eve: 9.683 | bob: 11.861Epoch   3:  89% | abe: 8.941 | eve: 9.684 | bob: 11.861Epoch   3:  89% | abe: 8.940 | eve: 9.683 | bob: 11.861Epoch   3:  90% | abe: 8.942 | eve: 9.677 | bob: 11.862Epoch   3:  91% | abe: 8.941 | eve: 9.675 | bob: 11.861Epoch   3:  92% | abe: 8.943 | eve: 9.676 | bob: 11.863Epoch   3:  92% | abe: 8.943 | eve: 9.674 | bob: 11.863Epoch   3:  93% | abe: 8.942 | eve: 9.680 | bob: 11.862Epoch   3:  94% | abe: 8.943 | eve: 9.681 | bob: 11.863Epoch   3:  95% | abe: 8.943 | eve: 9.690 | bob: 11.863Epoch   3:  96% | abe: 8.943 | eve: 9.689 | bob: 11.863Epoch   3:  96% | abe: 8.942 | eve: 9.691 | bob: 11.862Epoch   3:  97% | abe: 8.937 | eve: 9.693 | bob: 11.857Epoch   3:  98% | abe: 8.936 | eve: 9.692 | bob: 11.856Epoch   3:  99% | abe: 8.937 | eve: 9.688 | bob: 11.857
Early stopping: No improvement after 2 epochs since epoch 0. Best Bob loss: 9.145966526085978
Training complete.
cipher1 + cipher2
[[0.87646186 0.7158222  0.7888293  ... 0.7626934  0.70153224 0.68266475]
 [0.8587129  0.711862   0.9952793  ... 0.7314373  0.7216667  0.70592785]
 [1.0696254  0.7815318  0.7445512  ... 0.73243976 0.6687517  0.84143794]
 ...
 [1.1622763  0.7272959  0.7720777  ... 0.73735857 0.6677351  0.76947504]
 [1.1796229  0.758348   0.9715002  ... 0.71024764 0.7112342  0.73161525]
 [0.9192922  0.66963804 0.73695445 ... 0.71939814 0.6919266  0.66603357]]
HO addition:
[[0.8483912  0.69279814 0.7634615  ... 0.7383855  0.6791332  0.660848  ]
 [0.8312259  0.68896747 0.9631689  ... 0.70813596 0.6984925  0.68336964]
 [1.0350919  0.7563156  0.720737   ... 0.70907736 0.64736927 0.8144522 ]
 ...
 [1.1246533  0.70387536 0.7472974  ... 0.71387196 0.6463891  0.74483895]
 [1.1413862  0.7338817  0.94020194 ... 0.6876008  0.6884618  0.70817596]
 [0.8897797  0.64816195 0.7133015  ... 0.6964636  0.66980326 0.6447526 ]]
cipher1 * cipher2
[[0.17519756 0.12448886 0.15089798 ... 0.11699417 0.10448336 0.10133779]
 [0.16631421 0.12299847 0.24621266 ... 0.10601938 0.12407381 0.10814133]
 [0.2851696  0.15186575 0.12292839 ... 0.11065198 0.09839817 0.16546972]
 ...
 [0.3375218  0.12991685 0.14098568 ... 0.10720489 0.09766697 0.13415912]
 [0.3463664  0.14294112 0.23259765 ... 0.10297073 0.11471683 0.12284083]
 [0.20058732 0.10481001 0.12844728 ... 0.105047   0.10532911 0.09598265]]
HO multiplication
[[0.17519826 0.12448159 0.15088958 ... 0.11701106 0.1044896  0.10134031]
 [0.16631678 0.12299131 0.2461966  ... 0.10603553 0.12406845 0.10814554]
 [0.28515336 0.15185492 0.12293184 ... 0.11066362 0.09839858 0.16546616]
 ...
 [0.337507   0.1299085  0.14098093 ... 0.10722215 0.09766769 0.13416028]
 [0.34635356 0.1429311  0.2325831  ... 0.10298173 0.11471659 0.12283975]
 [0.20058042 0.10480542 0.12844267 ... 0.10505944 0.10533113 0.09598422]]
HO model Accuracy Percentage Addition: 0.00%
HO model Accuracy Percentage Multiplication: 100.00%
Bob decrypted addition: [[0.9829565  0.96480674 1.0492803  0.9193213  0.82338923 0.9778429
  1.0820167  0.8148835  0.8662945  0.8877308  0.8766905  0.8837719
  0.8935451  0.93915564 0.6704277  0.8726808 ]
 [0.99818754 0.9281072  0.90308416 0.9990798  0.8009254  1.0251243
  0.9709446  0.83954036 0.8603364  0.88462853 0.8707867  0.8866843
  0.8922427  0.95666766 0.6633965  0.90672594]
 [0.9694981  0.96540654 0.9130831  0.9136388  0.831483   0.9965401
  0.9692109  0.87354386 0.8630834  0.8746234  0.87191284 0.87964135
  0.8909812  0.97331375 0.66927934 0.8753472 ]
 [0.897183   0.99891084 1.0679092  0.9765128  0.8001292  0.9569541
  0.99212235 0.8194699  0.8816112  0.8734988  0.87933844 0.8641782
  0.8942458  0.9302793  0.6623653  0.905873  ]
 [0.8892571  1.0016291  1.0213723  1.0021856  0.79405403 0.9868466
  1.0234963  0.88756055 0.83957773 0.88463736 0.8838622  0.8704888
  0.89111245 0.963529   0.6641619  0.8987378 ]
 [0.8368373  1.024293   1.0086931  0.94792235 0.8156235  0.97472686
  0.9831427  0.82233196 0.88190967 0.8826959  0.87624204 0.860949
  0.87885916 0.96685386 0.66193163 0.9109054 ]
 [0.88485926 1.010025   0.9670263  0.9988072  0.79972607 0.9721209
  1.0294505  0.80091155 0.8974721  0.8730416  0.88940626 0.8783353
  0.89144963 0.9711274  0.66175735 0.9179229 ]
 [0.94929194 1.002966   1.0997463  1.0477471  0.7766849  0.9827435
  1.0000383  0.8155012  0.87249136 0.8878226  0.8812578  0.8183228
  0.89216256 0.9607672  0.66430056 0.92012334]
 [0.93785423 0.9837791  1.0647217  1.0461367  0.7787118  1.0093509
  1.0566182  0.78282315 0.8922093  0.88021815 0.86586213 0.88567907
  0.890891   0.95519364 0.6631998  0.9107634 ]
 [0.8721156  1.029189   1.02978    0.83364314 0.85286194 0.96351886
  1.0510477  0.8059899  0.8762371  0.88492364 0.8799685  0.8577728
  0.89167976 0.9633881  0.66082    0.91779655]]
Bob decrypted bits addition: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits addition: 80
Total number of bits addition: 160
Decryption accuracy addition: 50.0%
Bob decrypted multiplication: [[0.7159891  0.8809334  0.7787108  0.87180084 0.9199489  0.90991366
  0.7485072  0.8859991  0.8560627  0.93855935 0.71407723 0.81591505
  0.93466276 0.7529013  0.73838353 0.7811802 ]
 [0.7258378  0.8641776  0.73398066 0.8956129  0.8754735  0.9610358
  0.6858945  0.90891695 0.850856   0.93850845 0.6926619  0.8458824
  0.90577495 0.7817835  0.7165694  0.8459964 ]
 [0.69894433 0.9394591  0.67941236 0.8785048  0.88590014 0.9304271
  0.69832623 0.9393114  0.8433416  0.9323724  0.7135729  0.7762212
  0.88066477 0.90612805 0.7125554  0.79166126]
 [0.6842932  0.9481092  0.73427117 0.86271816 0.9048787  0.85511655
  0.71182984 0.9130831  0.8364135  0.9264882  0.75991225 0.76724076
  0.9117949  0.7877601  0.6826542  0.84413016]
 [0.66732574 0.90873116 0.751025   0.89518636 0.88340527 0.9758762
  0.7008227  0.9493267  0.8082462  0.9218599  0.77364504 0.74331963
  0.8992419  0.8094083  0.6980367  0.8314537 ]
 [0.65185386 0.92477345 0.7353269  0.8875059  0.9093668  0.89024174
  0.7035105  0.89293057 0.88207304 0.96598125 0.75069374 0.738393
  0.8319988  0.8330079  0.69268465 0.85434   ]
 [0.7029056  0.9435653  0.6927376  0.8911556  0.9016656  0.87175435
  0.7205498  0.8841658  0.8916428  0.8802395  0.83655214 0.7733059
  0.89255226 0.8472344  0.721346   0.8910657 ]
 [0.6894303  0.9831509  0.81767917 0.88830435 0.8543124  0.9083416
  0.6977148  0.89376944 0.84871787 0.95603955 0.8019737  0.67113006
  0.9025997  0.7868104  0.80347776 0.8937812 ]
 [0.6831236  0.915348   0.73928297 0.90395474 0.8582851  0.9526
  0.75841963 0.8526571  0.8902313  0.9347286  0.6956725  0.82558703
  0.90162647 0.7817608  0.7180518  0.85550153]
 [0.6639204  0.94694906 0.7665907  0.85431325 0.8876702  0.89227396
  0.7389282  0.89007884 0.8538507  0.92987615 0.7924188  0.6988828
  0.9167301  0.8208433  0.690935   0.8809555 ]]
Bob decrypted bits multiplication: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits multiplication: 38
Total number of bits multiplication: 160
Decryption accuracy multiplication: 23.75%
Eve decrypted addition: [[1.1003491  0.8746399  0.9725523  0.99911314 0.8612994  1.1455127
  0.9872184  1.1251405  1.0536835  1.1214565  1.1076919  1.0148681
  1.1527196  0.9850839  0.9475356  0.92663515]
 [1.0928503  0.8683461  0.9537641  0.9894923  0.85506856 1.1343958
  1.0046962  1.11937    1.0607401  1.1219268  1.1168755  1.008006
  1.1472995  1.0152205  0.97805536 0.9514176 ]
 [1.0906378  0.8686998  0.9590209  0.98663354 0.86134833 1.1494385
  1.0075346  1.1368119  1.0271951  1.1345372  1.0967416  0.9972703
  1.1513184  0.9807433  0.9792839  0.9653446 ]
 [1.100311   0.86748236 0.9799193  0.98865795 0.8569095  1.1430341
  1.0239296  1.1536872  0.98990387 1.1162175  1.1117722  1.0055957
  1.1536137  0.9805139  0.97647583 0.92939115]
 [1.1064386  0.88424236 0.9475731  0.9925775  0.8626127  1.1490867
  1.0145215  1.1177953  1.0298016  1.1213628  1.120532   1.0438554
  1.1149888  0.9849236  0.92857593 0.9224358 ]
 [1.0967323  0.87260395 0.9777766  0.98772657 0.86982596 1.1422476
  0.9907913  1.1377395  1.0399586  1.1089418  1.1241122  1.0075364
  1.1513944  0.9807397  0.9656682  0.92743224]
 [1.0901401  0.874399   0.9484756  0.9899512  0.8515086  1.1393061
  1.0244778  1.1366835  1.031756   1.1250651  1.1013373  1.0146035
  1.1430019  1.0133663  0.96290714 0.92278266]
 [1.0987906  0.8852491  0.96180856 0.9996516  0.8438654  1.1506793
  0.9937938  1.1346272  1.0180407  1.1342211  1.0783749  1.0198712
  1.1445806  0.9665104  0.93796384 0.92393583]
 [1.0999538  0.8696031  0.97689253 0.9960785  0.86501807 1.1412026
  0.97759116 1.1364971  1.0318387  1.1120698  1.107017   1.0324862
  1.1406343  0.9506783  0.96610564 0.9177875 ]
 [1.1019056  0.86223364 0.9781102  0.9808106  0.8934088  1.1416994
  1.0195843  1.150068   1.0077549  1.1119173  1.1111103  1.0240602
  1.1501757  0.9414029  0.98416656 0.94253   ]]
Eve decrypted bits addition: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits by Eve addition: 80
Total number of bits addition: 160
Decryption accuracy by Eve addition: 50.0%
Eve decrypted mulitplication: [[1.0869901  0.92251885 0.9597174  0.9886374  0.88578683 1.128742
  0.97884446 1.1014111  1.0554765  1.1160371  1.0480236  1.0530207
  1.1269022  0.95644987 0.93072236 0.9298564 ]
 [1.0736735  0.8953487  0.9548557  0.9699545  0.8601927  1.1032526
  0.9735312  1.0796238  1.0902498  1.1053119  1.0917373  1.0015224
  1.1357499  1.0399565  0.9805473  0.9451843 ]
 [1.0828233  0.9007284  0.9451171  0.9726998  0.8750004  1.1392536
  0.97696817 1.1122026  1.0298991  1.1324253  1.0518706  1.0165999
  1.1138114  0.9501927  0.9565264  0.99377966]
 [1.0785087  0.92100483 1.0019418  0.9616228  0.8841011  1.1187727
  1.0054523  1.1445866  0.98324156 1.1013767  1.059825   1.0048594
  1.1566311  0.9449539  1.015232   0.91346616]
 [1.09254    0.99413955 0.9273002  0.9566846  0.8628437  1.1173935
  1.0096254  1.0294459  1.0476741  1.1327512  1.0618652  1.077162
  1.0671675  0.96227723 0.9248402  0.91425306]
 [1.0781436  0.93428564 0.9786976  0.9630257  0.8763506  1.1158949
  0.9770662  1.1222428  1.051435   1.093363   1.0840298  1.0235366
  1.1291473  0.9461117  0.96395975 0.9069526 ]
 [1.06619    0.92177296 0.93865186 0.96559644 0.8625709  1.1078507
  1.0351585  1.1142492  1.0221827  1.1295254  1.0340497  1.0501252
  1.098591   0.9912502  0.9614175  0.9120938 ]
 [1.0855683  0.9366324  0.9505626  0.97711396 0.84902036 1.127363
  0.9346322  1.0826637  1.0148683  1.1194623  1.0325842  1.0703328
  1.0973145  0.9360227  0.90984696 0.9477464 ]
 [1.0851934  0.90100074 1.0052829  0.9808659  0.88196033 1.0987804
  0.921534   1.1200794  1.0316703  1.0932568  1.0395122  1.0597554
  1.1112018  0.90759945 0.99653256 0.91013914]
 [1.0923473  0.88724124 0.97948456 0.9689214  0.94161433 1.1264405
  1.013835   1.1497153  1.0044698  1.0950619  1.0451622  1.0628674
  1.1242414  0.89233595 0.994854   0.9296475 ]]
Eve decrypted bits mulitplication: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 38
Total number of bits mulitplication: 160
Decryption accuracy by Eve mulitplication: 23.75%
Bob decrypted P1: [[0.8810374  0.9072471  0.9012266  0.84412795 0.9041518  0.8987963
  0.9495478  0.8675184  0.8546719  0.8873579  0.83957183 0.87609464
  0.8897623  0.9202857  0.6828414  0.8591408 ]
 [0.86924636 0.86715823 0.76422477 0.907139   0.85699594 0.96852535
  0.77929395 0.9103848  0.84307677 0.8791354  0.8190449  0.8870186
  0.8832007  0.94009256 0.67590547 0.8986678 ]
 [0.81911683 0.9442994  0.73383635 0.8771226  0.8767643  0.9132385
  0.7944931  0.9733303  0.86517733 0.86182857 0.82629514 0.8686491
  0.8836742  0.966661   0.6770023  0.8641212 ]
 [0.74927294 0.98352486 0.9360399  0.865784   0.87265205 0.8565701
  0.81559193 0.9050415  0.89071155 0.85225165 0.84778494 0.877883
  0.8659784  0.91428804 0.6669042  0.89078623]
 [0.7984853  0.94931036 0.8799885  0.91064644 0.85971344 0.9417773
  0.8363438  0.9781488  0.8112238  0.8820942  0.8702853  0.8525171
  0.8792785  0.9551573  0.6702484  0.8901855 ]
 [0.7131056  0.9697355  0.82459414 0.88743275 0.88660324 0.87903523
  0.8202327  0.88807786 0.8793257  0.88979787 0.8269762  0.8529678
  0.8382574  0.9595951  0.66883147 0.89969057]
 [0.84127635 0.97382027 0.77947974 0.90631807 0.87493163 0.8683485
  0.87450045 0.8633279  0.907382   0.8572195  0.8859721  0.86596465
  0.8844374  0.95866805 0.6738421  0.9195762 ]
 [0.8448446  1.0042962  1.0403556  0.970498   0.8103095  0.89944154
  0.80841196 0.88341016 0.85593796 0.8953449  0.8578511  0.7468005
  0.8897128  0.91713476 0.7129512  0.91742694]
 [0.7623931  0.95232123 0.90685064 0.9556771  0.82428217 0.9583672
  0.95514095 0.81566644 0.89743805 0.86987525 0.8159462  0.8833921
  0.8777603  0.9478978  0.67601204 0.9018382 ]
 [0.7997246  1.0010216  0.9540904  0.80693966 0.885638   0.89271295
  0.8885007  0.8683103  0.86217785 0.87601745 0.8627667  0.81546205
  0.88610244 0.9680586  0.66564757 0.9117526 ]]
Bob decrypted bits P1: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits P1: 79
Total number of bits P1: 160
Decryption accuracy P1: 49.375%
Bob decrypted P2: [[0.7920637  0.9082708  0.809232   0.8589544  0.88376766 0.95715636
  0.8601644  0.8525699  0.86591816 0.90723056 0.76320446 0.84371525
  0.92879665 0.7914924  0.70499986 0.79500246]
 [0.81131226 0.87854904 0.70734644 0.910702   0.85622466 0.98111004
  0.73057044 0.89105666 0.85979855 0.9094737  0.73280597 0.8590267
  0.9060037  0.8321096  0.6827588  0.862859  ]
 [0.76083684 0.9394642  0.6979182  0.87919956 0.8751577  0.95892125
  0.73295057 0.91383207 0.8563796  0.9011767  0.74212205 0.81353736
  0.88686824 0.9371895  0.69599867 0.80405086]
 [0.71335787 0.97492754 0.8262675  0.8674387  0.8647516  0.9071435
  0.75622946 0.8819414  0.85746753 0.88971317 0.79089314 0.76331466
  0.92774636 0.8030002  0.67283237 0.8664314 ]
 [0.7254511  0.94041646 0.75797045 0.9056795  0.85019064 0.9697566
  0.7549591  0.9227918  0.83164126 0.8957311  0.81099033 0.78129196
  0.9030143  0.8523377  0.6820917  0.84398687]
 [0.6708774  0.9600136  0.74804705 0.8821939  0.8773271  0.9373903
  0.73852    0.87659204 0.8844545  0.9075996  0.77835405 0.7458134
  0.8613268  0.8817485  0.67428195 0.8750241 ]
 [0.8019358  0.9616439  0.7130842  0.9069562  0.8625946  0.931706
  0.7853825  0.8571048  0.8950885  0.87426287 0.8680345  0.8112438
  0.89435625 0.8969362  0.68174934 0.8940785 ]
 [0.7611378  1.0032895  0.96516687 0.9332953  0.8182376  0.9421635
  0.75801873 0.86893666 0.863342   0.91072065 0.81170446 0.689618
  0.89785916 0.8417036  0.7213221  0.90045613]
 [0.73355067 0.9432397  0.8147795  0.94349235 0.823465   0.99579436
  0.8873057  0.8177829  0.89069074 0.9135317  0.7183162  0.8521307
  0.90670663 0.82001954 0.6833068  0.8737413 ]
 [0.7342949  0.99030036 0.8319192  0.8323805  0.88119596 0.93065715
  0.81970215 0.8598378  0.8688933  0.90210694 0.79807305 0.7364009
  0.90921426 0.8483807  0.6726105  0.8921573 ]]
Bob decrypted bits P2: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
 [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]
Number of correctly decrypted bits P2: 77
Total number of bits P2: 160
Decryption accuracy P2: 48.125%
