WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-14 21:38:42.093218: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-14 21:38:42.177866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-04-14 21:38:42.178439: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:38:42.181717: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:38:42.184444: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:38:42.185281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:38:42.189117: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:38:42.192008: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:38:42.200580: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:38:42.205646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:38:42.206167: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-14 21:38:42.225583: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-14 21:38:42.228928: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4784eb0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:38:42.228986: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-14 21:38:42.459883: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x38cb600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-14 21:38:42.459949: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-14 21:38:42.465024: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-04-14 21:38:42.465144: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:38:42.465189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-14 21:38:42.465220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-14 21:38:42.465251: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-14 21:38:42.465281: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-14 21:38:42.465310: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-14 21:38:42.465358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-14 21:38:42.484988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-14 21:38:42.485096: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-14 21:38:42.490859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-14 21:38:42.490897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-14 21:38:42.490905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-14 21:38:42.505177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-14 21:38:45.999430: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.8020 - val_loss: 0.0035
Epoch 2/512
448/448 - 0s - loss: 0.2218 - val_loss: 5.9770e-04
Epoch 3/512
448/448 - 0s - loss: 0.0357 - val_loss: 7.7283e-05
Epoch 4/512
448/448 - 0s - loss: 0.0047 - val_loss: 1.4926e-05
Epoch 5/512
448/448 - 0s - loss: 0.0012 - val_loss: 8.2176e-06
Epoch 6/512
448/448 - 0s - loss: 7.2227e-04 - val_loss: 5.3204e-06
Epoch 7/512
448/448 - 0s - loss: 4.5635e-04 - val_loss: 3.1251e-06
Epoch 8/512
448/448 - 0s - loss: 2.5857e-04 - val_loss: 1.6048e-06
Epoch 9/512
448/448 - 0s - loss: 1.2707e-04 - val_loss: 6.9495e-07
Epoch 10/512
448/448 - 0s - loss: 5.2189e-05 - val_loss: 2.4308e-07
Epoch 11/512
448/448 - 0s - loss: 1.7128e-05 - val_loss: 6.4753e-08
Epoch 12/512
448/448 - 0s - loss: 4.2324e-06 - val_loss: 1.2209e-08
Epoch 13/512
448/448 - 0s - loss: 7.3096e-07 - val_loss: 1.5008e-09
Epoch 14/512
448/448 - 0s - loss: 9.4616e-08 - val_loss: 1.9527e-09
Epoch 15/512
448/448 - 0s - loss: 4.2014e-06 - val_loss: 1.2992e-06
Epoch 16/512
448/448 - 0s - loss: 0.0028 - val_loss: 5.6386e-05
Epoch 17/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.3057e-06
Epoch 18/512
448/448 - 0s - loss: 9.2631e-05 - val_loss: 9.1706e-07
Epoch 19/512
448/448 - 0s - loss: 1.9996e-04 - val_loss: 1.0333e-05
Epoch 20/512
448/448 - 0s - loss: 0.0024 - val_loss: 2.9273e-05
Epoch 21/512
448/448 - 0s - loss: 0.0016 - val_loss: 4.5691e-06
Epoch 22/512
448/448 - 0s - loss: 4.0924e-04 - val_loss: 5.7537e-06
Epoch 23/512
448/448 - 0s - loss: 9.5847e-04 - val_loss: 2.2206e-05
Epoch 24/512
448/448 - 0s - loss: 0.0020 - val_loss: 1.1937e-05
Epoch 25/512
448/448 - 0s - loss: 8.9467e-04 - val_loss: 6.8416e-06
Epoch 26/512
448/448 - 0s - loss: 7.8373e-04 - val_loss: 1.2811e-05
Epoch 27/512
448/448 - 0s - loss: 0.0015 - val_loss: 1.5054e-05
Epoch 28/512
448/448 - 0s - loss: 0.0012 - val_loss: 9.1175e-06
Epoch 29/512
448/448 - 0s - loss: 8.7181e-04 - val_loss: 9.9428e-06
Epoch 30/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.3185e-05
Epoch 31/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0828e-05
Epoch 32/512
448/448 - 0s - loss: 9.8290e-04 - val_loss: 9.5374e-06
Epoch 33/512
448/448 - 0s - loss: 9.7678e-04 - val_loss: 1.0988e-05
Epoch 34/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1263e-05
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 9.9831e-06
Epoch 36/512
448/448 - 0s - loss: 9.5584e-04 - val_loss: 9.8963e-06
Epoch 37/512
448/448 - 0s - loss: 9.8033e-04 - val_loss: 1.0304e-05
Epoch 38/512
448/448 - 0s - loss: 9.9554e-04 - val_loss: 9.9210e-06
Epoch 39/512
448/448 - 0s - loss: 9.4864e-04 - val_loss: 9.5048e-06
Epoch 40/512
448/448 - 0s - loss: 9.2439e-04 - val_loss: 9.8539e-06
Epoch 41/512
448/448 - 0s - loss: 9.5483e-04 - val_loss: 9.6226e-06
Epoch 42/512
448/448 - 0s - loss: 9.1207e-04 - val_loss: 9.2050e-06
Epoch 43/512
448/448 - 0s - loss: 8.9433e-04 - val_loss: 9.1383e-06
Epoch 44/512
448/448 - 0s - loss: 8.9620e-04 - val_loss: 9.0475e-06
Epoch 45/512
448/448 - 0s - loss: 8.8721e-04 - val_loss: 8.6475e-06
Epoch 46/512
448/448 - 0s - loss: 8.4806e-04 - val_loss: 8.7727e-06
Epoch 47/512
448/448 - 0s - loss: 8.6114e-04 - val_loss: 8.9592e-06
Epoch 48/512
448/448 - 0s - loss: 8.6409e-04 - val_loss: 8.3377e-06
Epoch 49/512
448/448 - 0s - loss: 8.0901e-04 - val_loss: 8.2837e-06
Epoch 50/512
448/448 - 0s - loss: 8.2475e-04 - val_loss: 8.3779e-06
Epoch 51/512
448/448 - 0s - loss: 8.1127e-04 - val_loss: 8.4424e-06
Epoch 52/512
448/448 - 0s - loss: 8.1342e-04 - val_loss: 8.0930e-06
Epoch 53/512
448/448 - 0s - loss: 7.8068e-04 - val_loss: 7.8730e-06
Epoch 54/512
448/448 - 0s - loss: 7.7534e-04 - val_loss: 7.9080e-06
Epoch 55/512
448/448 - 0s - loss: 7.7642e-04 - val_loss: 7.6953e-06
Epoch 56/512
448/448 - 0s - loss: 7.5509e-04 - val_loss: 7.5752e-06
Epoch 57/512
448/448 - 0s - loss: 7.4809e-04 - val_loss: 7.5341e-06
Epoch 58/512
448/448 - 0s - loss: 7.3933e-04 - val_loss: 7.5310e-06
Epoch 59/512
448/448 - 0s - loss: 7.3707e-04 - val_loss: 7.3551e-06
Epoch 60/512
448/448 - 0s - loss: 7.1801e-04 - val_loss: 7.1878e-06
Epoch 61/512
448/448 - 0s - loss: 7.1248e-04 - val_loss: 7.0416e-06
Epoch 62/512
448/448 - 0s - loss: 6.9450e-04 - val_loss: 7.1431e-06
Epoch 63/512
448/448 - 0s - loss: 7.0354e-04 - val_loss: 7.0260e-06
Epoch 64/512
448/448 - 0s - loss: 6.8773e-04 - val_loss: 6.7592e-06
Epoch 65/512
448/448 - 0s - loss: 6.6966e-04 - val_loss: 6.7041e-06
Epoch 66/512
448/448 - 0s - loss: 6.6697e-04 - val_loss: 6.7348e-06
Epoch 67/512
448/448 - 0s - loss: 6.6465e-04 - val_loss: 6.6437e-06
Epoch 68/512
448/448 - 0s - loss: 6.5258e-04 - val_loss: 6.4470e-06
Epoch 69/512
448/448 - 0s - loss: 6.3789e-04 - val_loss: 6.5351e-06
Epoch 70/512
448/448 - 0s - loss: 6.4462e-04 - val_loss: 6.4362e-06
Epoch 71/512
448/448 - 0s - loss: 6.2857e-04 - val_loss: 6.2433e-06
Epoch 72/512
448/448 - 0s - loss: 6.1678e-04 - val_loss: 6.2731e-06
Epoch 73/512
448/448 - 0s - loss: 6.1600e-04 - val_loss: 6.2696e-06
Epoch 74/512
448/448 - 0s - loss: 6.1502e-04 - val_loss: 6.0194e-06
Epoch 75/512
448/448 - 0s - loss: 5.9208e-04 - val_loss: 5.9881e-06
Epoch 76/512
448/448 - 0s - loss: 5.9230e-04 - val_loss: 6.0458e-06
Epoch 77/512
448/448 - 0s - loss: 5.9536e-04 - val_loss: 5.7990e-06
Epoch 78/512
448/448 - 0s - loss: 5.7144e-04 - val_loss: 5.6867e-06
Epoch 79/512
448/448 - 0s - loss: 5.6701e-04 - val_loss: 5.8536e-06
Epoch 80/512
448/448 - 0s - loss: 5.7967e-04 - val_loss: 5.6697e-06
Epoch 81/512
448/448 - 0s - loss: 5.5091e-04 - val_loss: 5.6135e-06
Epoch 82/512
448/448 - 0s - loss: 5.5527e-04 - val_loss: 5.5664e-06
Epoch 83/512
448/448 - 0s - loss: 5.4570e-04 - val_loss: 5.5353e-06
Epoch 84/512
448/448 - 0s - loss: 5.4195e-04 - val_loss: 5.4248e-06
Epoch 85/512
448/448 - 0s - loss: 5.3416e-04 - val_loss: 5.3328e-06
Epoch 86/512
448/448 - 0s - loss: 5.2954e-04 - val_loss: 5.2140e-06
Epoch 87/512
448/448 - 0s - loss: 5.1337e-04 - val_loss: 5.3748e-06
Epoch 88/512
448/448 - 0s - loss: 5.2767e-04 - val_loss: 5.3444e-06
Epoch 89/512
448/448 - 0s - loss: 5.2037e-04 - val_loss: 4.8983e-06
Epoch 90/512
448/448 - 0s - loss: 4.8709e-04 - val_loss: 4.9780e-06
Epoch 91/512
448/448 - 0s - loss: 5.0221e-04 - val_loss: 5.1366e-06
Epoch 92/512
448/448 - 0s - loss: 5.0375e-04 - val_loss: 4.9189e-06
Epoch 93/512
448/448 - 0s - loss: 4.8303e-04 - val_loss: 4.7775e-06
Epoch 94/512
448/448 - 0s - loss: 4.7541e-04 - val_loss: 4.9180e-06
Epoch 95/512
448/448 - 0s - loss: 4.8808e-04 - val_loss: 4.7205e-06
Epoch 96/512
448/448 - 0s - loss: 4.6379e-04 - val_loss: 4.6786e-06
Epoch 97/512
448/448 - 0s - loss: 4.6413e-04 - val_loss: 4.8343e-06
Epoch 98/512
448/448 - 0s - loss: 4.7361e-04 - val_loss: 4.6123e-06
Epoch 99/512
448/448 - 0s - loss: 4.4993e-04 - val_loss: 4.4708e-06
Epoch 100/512
448/448 - 0s - loss: 4.4863e-04 - val_loss: 4.4390e-06
Epoch 101/512
448/448 - 0s - loss: 4.4479e-04 - val_loss: 4.4379e-06
Epoch 102/512
448/448 - 0s - loss: 4.4342e-04 - val_loss: 4.3414e-06
Epoch 103/512
448/448 - 0s - loss: 4.3051e-04 - val_loss: 4.3797e-06
Epoch 104/512
448/448 - 0s - loss: 4.3634e-04 - val_loss: 4.2530e-06
Epoch 105/512
448/448 - 0s - loss: 4.1835e-04 - val_loss: 4.2908e-06
Epoch 106/512
448/448 - 0s - loss: 4.2781e-04 - val_loss: 4.1574e-06
Epoch 107/512
448/448 - 0s - loss: 4.0834e-04 - val_loss: 4.1681e-06
Epoch 108/512
448/448 - 0s - loss: 4.1583e-04 - val_loss: 4.1025e-06
Epoch 109/512
448/448 - 0s - loss: 4.0302e-04 - val_loss: 4.0609e-06
Epoch 110/512
448/448 - 0s - loss: 4.0060e-04 - val_loss: 4.0812e-06
Epoch 111/512
448/448 - 0s - loss: 4.0040e-04 - val_loss: 3.9487e-06
Epoch 112/512
448/448 - 0s - loss: 3.8812e-04 - val_loss: 3.8805e-06
Epoch 113/512
448/448 - 0s - loss: 3.8500e-04 - val_loss: 3.8873e-06
Epoch 114/512
448/448 - 0s - loss: 3.8460e-04 - val_loss: 3.8286e-06
Epoch 115/512
448/448 - 0s - loss: 3.7732e-04 - val_loss: 3.7309e-06
Epoch 116/512
448/448 - 0s - loss: 3.7134e-04 - val_loss: 3.6672e-06
Epoch 117/512
448/448 - 0s - loss: 3.6842e-04 - val_loss: 3.6324e-06
Epoch 118/512
448/448 - 0s - loss: 3.6451e-04 - val_loss: 3.5752e-06
Epoch 119/512
448/448 - 0s - loss: 3.5427e-04 - val_loss: 3.6546e-06
Epoch 120/512
448/448 - 0s - loss: 3.6171e-04 - val_loss: 3.5817e-06
Epoch 121/512
448/448 - 0s - loss: 3.4984e-04 - val_loss: 3.4574e-06
Epoch 122/512
448/448 - 0s - loss: 3.4313e-04 - val_loss: 3.4247e-06
Epoch 123/512
448/448 - 0s - loss: 3.4069e-04 - val_loss: 3.4272e-06
Epoch 124/512
448/448 - 0s - loss: 3.4048e-04 - val_loss: 3.3677e-06
Epoch 125/512
448/448 - 0s - loss: 3.3074e-04 - val_loss: 3.2964e-06
Epoch 126/512
448/448 - 0s - loss: 3.2731e-04 - val_loss: 3.2892e-06
Epoch 127/512
448/448 - 0s - loss: 3.2389e-04 - val_loss: 3.2789e-06
Epoch 128/512
448/448 - 0s - loss: 3.2260e-04 - val_loss: 3.1803e-06
Epoch 129/512
448/448 - 0s - loss: 3.1237e-04 - val_loss: 3.1692e-06
Epoch 130/512
448/448 - 0s - loss: 3.1412e-04 - val_loss: 3.1294e-06
Epoch 131/512
448/448 - 0s - loss: 3.0775e-04 - val_loss: 3.0391e-06
Epoch 132/512
448/448 - 0s - loss: 3.0228e-04 - val_loss: 2.9639e-06
Epoch 133/512
448/448 - 0s - loss: 2.9386e-04 - val_loss: 3.0659e-06
Epoch 134/512
448/448 - 0s - loss: 3.0213e-04 - val_loss: 2.9909e-06
Epoch 135/512
448/448 - 0s - loss: 2.9305e-04 - val_loss: 2.7741e-06
Epoch 136/512
448/448 - 0s - loss: 2.7718e-04 - val_loss: 2.8305e-06
Epoch 137/512
448/448 - 0s - loss: 2.8673e-04 - val_loss: 2.8849e-06
Epoch 138/512
448/448 - 0s - loss: 2.8272e-04 - val_loss: 2.7832e-06
Epoch 139/512
448/448 - 0s - loss: 2.7397e-04 - val_loss: 2.6586e-06
Epoch 140/512
448/448 - 0s - loss: 2.6385e-04 - val_loss: 2.7586e-06
Epoch 141/512
448/448 - 0s - loss: 2.7611e-04 - val_loss: 2.6532e-06
Epoch 142/512
448/448 - 0s - loss: 2.5984e-04 - val_loss: 2.5789e-06
Epoch 143/512
448/448 - 0s - loss: 2.5902e-04 - val_loss: 2.5613e-06
Epoch 144/512
448/448 - 0s - loss: 2.5559e-04 - val_loss: 2.5302e-06
Epoch 145/512
448/448 - 0s - loss: 2.5245e-04 - val_loss: 2.4904e-06
Epoch 146/512
448/448 - 0s - loss: 2.4776e-04 - val_loss: 2.4371e-06
Epoch 147/512
448/448 - 0s - loss: 2.4446e-04 - val_loss: 2.3794e-06
Epoch 148/512
448/448 - 0s - loss: 2.3785e-04 - val_loss: 2.4186e-06
Epoch 149/512
448/448 - 0s - loss: 2.4135e-04 - val_loss: 2.3685e-06
Epoch 150/512
448/448 - 0s - loss: 2.3331e-04 - val_loss: 2.2817e-06
Epoch 151/512
448/448 - 0s - loss: 2.2893e-04 - val_loss: 2.2429e-06
Epoch 152/512
448/448 - 0s - loss: 2.2486e-04 - val_loss: 2.2567e-06
Epoch 153/512
448/448 - 0s - loss: 2.2484e-04 - val_loss: 2.2351e-06
Epoch 154/512
448/448 - 0s - loss: 2.2115e-04 - val_loss: 2.1337e-06
Epoch 155/512
448/448 - 0s - loss: 2.1199e-04 - val_loss: 2.1239e-06
Epoch 156/512
448/448 - 0s - loss: 2.1298e-04 - val_loss: 2.1521e-06
Epoch 157/512
448/448 - 0s - loss: 2.1233e-04 - val_loss: 2.0807e-06
Epoch 158/512
448/448 - 0s - loss: 2.0431e-04 - val_loss: 2.0074e-06
Epoch 159/512
448/448 - 0s - loss: 2.0020e-04 - val_loss: 2.0110e-06
Epoch 160/512
448/448 - 0s - loss: 2.0052e-04 - val_loss: 2.0119e-06
Epoch 161/512
448/448 - 0s - loss: 1.9659e-04 - val_loss: 1.9686e-06
Epoch 162/512
448/448 - 0s - loss: 1.9423e-04 - val_loss: 1.8597e-06
Epoch 163/512
448/448 - 0s - loss: 1.8429e-04 - val_loss: 1.8993e-06
Epoch 164/512
448/448 - 0s - loss: 1.8958e-04 - val_loss: 1.8870e-06
Epoch 165/512
448/448 - 0s - loss: 1.8512e-04 - val_loss: 1.7619e-06
Epoch 166/512
448/448 - 0s - loss: 1.7463e-04 - val_loss: 1.7790e-06
Epoch 167/512
448/448 - 0s - loss: 1.7841e-04 - val_loss: 1.7998e-06
Epoch 168/512
448/448 - 0s - loss: 1.7826e-04 - val_loss: 1.6647e-06
Epoch 169/512
448/448 - 0s - loss: 1.6468e-04 - val_loss: 1.6731e-06
Epoch 170/512
448/448 - 0s - loss: 1.6953e-04 - val_loss: 1.7019e-06
Epoch 171/512
448/448 - 0s - loss: 1.6808e-04 - val_loss: 1.6097e-06
Epoch 172/512
448/448 - 0s - loss: 1.5924e-04 - val_loss: 1.5629e-06
Epoch 173/512
448/448 - 0s - loss: 1.5684e-04 - val_loss: 1.6104e-06
Epoch 174/512
448/448 - 0s - loss: 1.5987e-04 - val_loss: 1.5550e-06
Epoch 175/512
448/448 - 0s - loss: 1.5233e-04 - val_loss: 1.4920e-06
Epoch 176/512
448/448 - 0s - loss: 1.4950e-04 - val_loss: 1.4844e-06
Epoch 177/512
448/448 - 0s - loss: 1.4824e-04 - val_loss: 1.4813e-06
Epoch 178/512
448/448 - 0s - loss: 1.4589e-04 - val_loss: 1.4485e-06
Epoch 179/512
448/448 - 0s - loss: 1.4312e-04 - val_loss: 1.3878e-06
Epoch 180/512
448/448 - 0s - loss: 1.3706e-04 - val_loss: 1.4030e-06
Epoch 181/512
448/448 - 0s - loss: 1.3950e-04 - val_loss: 1.3649e-06
Epoch 182/512
448/448 - 0s - loss: 1.3534e-04 - val_loss: 1.2928e-06
Epoch 183/512
448/448 - 0s - loss: 1.2866e-04 - val_loss: 1.3155e-06
Epoch 184/512
448/448 - 0s - loss: 1.3190e-04 - val_loss: 1.2973e-06
Epoch 185/512
448/448 - 0s - loss: 1.2718e-04 - val_loss: 1.2467e-06
Epoch 186/512
448/448 - 0s - loss: 1.2320e-04 - val_loss: 1.2376e-06
Epoch 187/512
448/448 - 0s - loss: 1.2356e-04 - val_loss: 1.2013e-06
Epoch 188/512
448/448 - 0s - loss: 1.1884e-04 - val_loss: 1.1616e-06
Epoch 189/512
448/448 - 0s - loss: 1.1643e-04 - val_loss: 1.1643e-06
Epoch 190/512
448/448 - 0s - loss: 1.1665e-04 - val_loss: 1.1345e-06
Epoch 191/512
448/448 - 0s - loss: 1.1305e-04 - val_loss: 1.0803e-06
Epoch 192/512
448/448 - 0s - loss: 1.0826e-04 - val_loss: 1.0982e-06
Epoch 193/512
448/448 - 0s - loss: 1.1015e-04 - val_loss: 1.0793e-06
Epoch 194/512
448/448 - 0s - loss: 1.0638e-04 - val_loss: 1.0393e-06
Epoch 195/512
448/448 - 0s - loss: 1.0298e-04 - val_loss: 1.0357e-06
Epoch 196/512
448/448 - 0s - loss: 1.0234e-04 - val_loss: 1.0228e-06
Epoch 197/512
448/448 - 0s - loss: 1.0100e-04 - val_loss: 9.6411e-07
Epoch 198/512
448/448 - 0s - loss: 9.5367e-05 - val_loss: 9.6201e-07
Epoch 199/512
448/448 - 0s - loss: 9.6731e-05 - val_loss: 9.5653e-07
Epoch 200/512
448/448 - 0s - loss: 9.4733e-05 - val_loss: 9.1417e-07
Epoch 201/512
448/448 - 0s - loss: 9.0439e-05 - val_loss: 9.0494e-07
Epoch 202/512
448/448 - 0s - loss: 9.0166e-05 - val_loss: 9.0351e-07
Epoch 203/512
448/448 - 0s - loss: 8.8734e-05 - val_loss: 8.7287e-07
Epoch 204/512
448/448 - 0s - loss: 8.6069e-05 - val_loss: 8.3292e-07
Epoch 205/512
448/448 - 0s - loss: 8.3072e-05 - val_loss: 8.2233e-07
Epoch 206/512
448/448 - 0s - loss: 8.2589e-05 - val_loss: 8.1271e-07
Epoch 207/512
448/448 - 0s - loss: 8.1115e-05 - val_loss: 7.8376e-07
Epoch 208/512
448/448 - 0s - loss: 7.7798e-05 - val_loss: 7.8315e-07
Epoch 209/512
448/448 - 0s - loss: 7.8003e-05 - val_loss: 7.6464e-07
Epoch 210/512
448/448 - 0s - loss: 7.5070e-05 - val_loss: 7.4488e-07
Epoch 211/512
448/448 - 0s - loss: 7.4117e-05 - val_loss: 7.1905e-07
Epoch 212/512
448/448 - 0s - loss: 7.1620e-05 - val_loss: 6.9819e-07
Epoch 213/512
448/448 - 0s - loss: 7.0069e-05 - val_loss: 7.0174e-07
Epoch 214/512
448/448 - 0s - loss: 6.9527e-05 - val_loss: 6.9485e-07
Epoch 215/512
448/448 - 0s - loss: 6.7703e-05 - val_loss: 6.7586e-07
Epoch 216/512
448/448 - 0s - loss: 6.6381e-05 - val_loss: 6.4385e-07
Epoch 217/512
448/448 - 0s - loss: 6.3767e-05 - val_loss: 6.2787e-07
Epoch 218/512
448/448 - 0s - loss: 6.2813e-05 - val_loss: 6.2780e-07
Epoch 219/512
448/448 - 0s - loss: 6.2217e-05 - val_loss: 6.0838e-07
Epoch 220/512
448/448 - 0s - loss: 5.9716e-05 - val_loss: 5.9196e-07
Epoch 221/512
448/448 - 0s - loss: 5.8869e-05 - val_loss: 5.7826e-07
Epoch 222/512
448/448 - 0s - loss: 5.7553e-05 - val_loss: 5.5602e-07
Epoch 223/512
448/448 - 0s - loss: 5.5465e-05 - val_loss: 5.4532e-07
Epoch 224/512
448/448 - 0s - loss: 5.4746e-05 - val_loss: 5.4263e-07
Epoch 225/512
448/448 - 0s - loss: 5.3864e-05 - val_loss: 5.2840e-07
Epoch 226/512
448/448 - 0s - loss: 5.2083e-05 - val_loss: 5.1773e-07
Epoch 227/512
448/448 - 0s - loss: 5.1261e-05 - val_loss: 5.0157e-07
Epoch 228/512
448/448 - 0s - loss: 5.0067e-05 - val_loss: 4.7249e-07
Epoch 229/512
448/448 - 0s - loss: 4.7202e-05 - val_loss: 4.7878e-07
Epoch 230/512
448/448 - 0s - loss: 4.8299e-05 - val_loss: 4.7333e-07
Epoch 231/512
448/448 - 0s - loss: 4.6572e-05 - val_loss: 4.5283e-07
Epoch 232/512
448/448 - 0s - loss: 4.4532e-05 - val_loss: 4.5401e-07
Epoch 233/512
448/448 - 0s - loss: 4.4920e-05 - val_loss: 4.3369e-07
Epoch 234/512
448/448 - 0s - loss: 4.2703e-05 - val_loss: 4.1135e-07
Epoch 235/512
448/448 - 0s - loss: 4.1401e-05 - val_loss: 4.0972e-07
Epoch 236/512
448/448 - 0s - loss: 4.1102e-05 - val_loss: 4.0813e-07
Epoch 237/512
448/448 - 0s - loss: 4.0415e-05 - val_loss: 3.8338e-07
Epoch 238/512
448/448 - 0s - loss: 3.7997e-05 - val_loss: 3.8173e-07
Epoch 239/512
448/448 - 0s - loss: 3.8100e-05 - val_loss: 3.8825e-07
Epoch 240/512
448/448 - 0s - loss: 3.8053e-05 - val_loss: 3.6227e-07
Epoch 241/512
448/448 - 0s - loss: 3.5633e-05 - val_loss: 3.4086e-07
Epoch 242/512
448/448 - 0s - loss: 3.4255e-05 - val_loss: 3.5273e-07
Epoch 243/512
448/448 - 0s - loss: 3.5249e-05 - val_loss: 3.4700e-07
Epoch 244/512
448/448 - 0s - loss: 3.3910e-05 - val_loss: 3.1572e-07
Epoch 245/512
448/448 - 0s - loss: 3.1113e-05 - val_loss: 3.2555e-07
Epoch 246/512
448/448 - 0s - loss: 3.2748e-05 - val_loss: 3.2411e-07
Epoch 247/512
448/448 - 0s - loss: 3.1418e-05 - val_loss: 2.9831e-07
Epoch 248/512
448/448 - 0s - loss: 2.9495e-05 - val_loss: 2.8870e-07
Epoch 249/512
448/448 - 0s - loss: 2.9264e-05 - val_loss: 2.9376e-07
Epoch 250/512
448/448 - 0s - loss: 2.9090e-05 - val_loss: 2.8731e-07
Epoch 251/512
448/448 - 0s - loss: 2.7978e-05 - val_loss: 2.7367e-07
Epoch 252/512
448/448 - 0s - loss: 2.7068e-05 - val_loss: 2.6511e-07
Epoch 253/512
448/448 - 0s - loss: 2.6462e-05 - val_loss: 2.5913e-07
Epoch 254/512
448/448 - 0s - loss: 2.5858e-05 - val_loss: 2.5088e-07
Epoch 255/512
448/448 - 0s - loss: 2.5106e-05 - val_loss: 2.4330e-07
Epoch 256/512
448/448 - 0s - loss: 2.4302e-05 - val_loss: 2.4110e-07
Epoch 257/512
448/448 - 0s - loss: 2.4045e-05 - val_loss: 2.3612e-07
Epoch 258/512
448/448 - 0s - loss: 2.3447e-05 - val_loss: 2.2080e-07
Epoch 259/512
448/448 - 0s - loss: 2.1951e-05 - val_loss: 2.2297e-07
Epoch 260/512
448/448 - 0s - loss: 2.2469e-05 - val_loss: 2.1760e-07
Epoch 261/512
448/448 - 0s - loss: 2.1461e-05 - val_loss: 2.0590e-07
Epoch 262/512
448/448 - 0s - loss: 2.0388e-05 - val_loss: 2.0864e-07
Epoch 263/512
448/448 - 0s - loss: 2.0619e-05 - val_loss: 2.0887e-07
Epoch 264/512
448/448 - 0s - loss: 2.0306e-05 - val_loss: 1.8804e-07
Epoch 265/512
448/448 - 0s - loss: 1.8425e-05 - val_loss: 1.8539e-07
Epoch 266/512
448/448 - 0s - loss: 1.8881e-05 - val_loss: 1.8763e-07
Epoch 267/512
448/448 - 0s - loss: 1.8457e-05 - val_loss: 1.7982e-07
Epoch 268/512
448/448 - 0s - loss: 1.7697e-05 - val_loss: 1.7124e-07
Epoch 269/512
448/448 - 0s - loss: 1.7017e-05 - val_loss: 1.6981e-07
Epoch 270/512
448/448 - 0s - loss: 1.6882e-05 - val_loss: 1.6887e-07
Epoch 271/512
448/448 - 0s - loss: 1.6594e-05 - val_loss: 1.5868e-07
Epoch 272/512
448/448 - 0s - loss: 1.5576e-05 - val_loss: 1.5468e-07
Epoch 273/512
448/448 - 0s - loss: 1.5523e-05 - val_loss: 1.5062e-07
Epoch 274/512
448/448 - 0s - loss: 1.5036e-05 - val_loss: 1.4569e-07
Epoch 275/512
448/448 - 0s - loss: 1.4557e-05 - val_loss: 1.4254e-07
Epoch 276/512
448/448 - 0s - loss: 1.4180e-05 - val_loss: 1.4084e-07
Epoch 277/512
448/448 - 0s - loss: 1.3965e-05 - val_loss: 1.3494e-07
Epoch 278/512
448/448 - 0s - loss: 1.3171e-05 - val_loss: 1.3569e-07
Epoch 279/512
448/448 - 0s - loss: 1.3444e-05 - val_loss: 1.2749e-07
Epoch 280/512
448/448 - 0s - loss: 1.2487e-05 - val_loss: 1.2196e-07
Epoch 281/512
448/448 - 0s - loss: 1.2222e-05 - val_loss: 1.2210e-07
Epoch 282/512
448/448 - 0s - loss: 1.2112e-05 - val_loss: 1.1962e-07
Epoch 283/512
448/448 - 0s - loss: 1.1686e-05 - val_loss: 1.1594e-07
Epoch 284/512
448/448 - 0s - loss: 1.1404e-05 - val_loss: 1.1015e-07
Epoch 285/512
448/448 - 0s - loss: 1.0912e-05 - val_loss: 1.0619e-07
Epoch 286/512
448/448 - 0s - loss: 1.0601e-05 - val_loss: 1.0709e-07
Epoch 287/512
448/448 - 0s - loss: 1.0583e-05 - val_loss: 1.0412e-07
Epoch 288/512
448/448 - 0s - loss: 1.0139e-05 - val_loss: 9.8753e-08
Epoch 289/512
448/448 - 0s - loss: 9.7063e-06 - val_loss: 9.6627e-08
Epoch 290/512
448/448 - 0s - loss: 9.5829e-06 - val_loss: 9.5590e-08
Epoch 291/512
448/448 - 0s - loss: 9.3911e-06 - val_loss: 9.1617e-08
Epoch 292/512
448/448 - 0s - loss: 8.9594e-06 - val_loss: 8.9555e-08
Epoch 293/512
448/448 - 0s - loss: 8.8197e-06 - val_loss: 8.6772e-08
Epoch 294/512
448/448 - 0s - loss: 8.6239e-06 - val_loss: 7.9312e-08
Epoch 295/512
448/448 - 0s - loss: 7.9610e-06 - val_loss: 8.0494e-08
Epoch 296/512
448/448 - 0s - loss: 8.1328e-06 - val_loss: 8.3268e-08
Epoch 297/512
448/448 - 0s - loss: 8.0752e-06 - val_loss: 7.6466e-08
Epoch 298/512
448/448 - 0s - loss: 7.4719e-06 - val_loss: 7.0727e-08
Epoch 299/512
448/448 - 0s - loss: 7.0744e-06 - val_loss: 7.4860e-08
Epoch 300/512
448/448 - 0s - loss: 7.4728e-06 - val_loss: 7.2905e-08
Epoch 301/512
448/448 - 0s - loss: 7.0349e-06 - val_loss: 6.4499e-08
Epoch 302/512
448/448 - 0s - loss: 6.3675e-06 - val_loss: 6.6661e-08
Epoch 303/512
448/448 - 0s - loss: 6.7876e-06 - val_loss: 6.5643e-08
Epoch 304/512
448/448 - 0s - loss: 6.3844e-06 - val_loss: 6.1615e-08
Epoch 305/512
448/448 - 0s - loss: 6.0953e-06 - val_loss: 6.0259e-08
Epoch 306/512
448/448 - 0s - loss: 5.9942e-06 - val_loss: 6.0128e-08
Epoch 307/512
448/448 - 0s - loss: 5.9082e-06 - val_loss: 5.7622e-08
Epoch 308/512
448/448 - 0s - loss: 5.6572e-06 - val_loss: 5.4022e-08
Epoch 309/512
448/448 - 0s - loss: 5.3624e-06 - val_loss: 5.4390e-08
Epoch 310/512
448/448 - 0s - loss: 5.4467e-06 - val_loss: 5.2832e-08
Epoch 311/512
448/448 - 0s - loss: 5.1513e-06 - val_loss: 5.0932e-08
Epoch 312/512
448/448 - 0s - loss: 5.0489e-06 - val_loss: 4.8483e-08
Epoch 313/512
448/448 - 0s - loss: 4.8134e-06 - val_loss: 4.7709e-08
Epoch 314/512
448/448 - 0s - loss: 4.7654e-06 - val_loss: 4.6938e-08
Epoch 315/512
448/448 - 0s - loss: 4.6012e-06 - val_loss: 4.5628e-08
Epoch 316/512
448/448 - 0s - loss: 4.4954e-06 - val_loss: 4.2921e-08
Epoch 317/512
448/448 - 0s - loss: 4.2207e-06 - val_loss: 4.2914e-08
Epoch 318/512
448/448 - 0s - loss: 4.2943e-06 - val_loss: 4.1302e-08
Epoch 319/512
448/448 - 0s - loss: 4.0595e-06 - val_loss: 3.8981e-08
Epoch 320/512
448/448 - 0s - loss: 3.8710e-06 - val_loss: 3.9113e-08
Epoch 321/512
448/448 - 0s - loss: 3.8932e-06 - val_loss: 3.8574e-08
Epoch 322/512
448/448 - 0s - loss: 3.7815e-06 - val_loss: 3.5451e-08
Epoch 323/512
448/448 - 0s - loss: 3.5142e-06 - val_loss: 3.5045e-08
Epoch 324/512
448/448 - 0s - loss: 3.5327e-06 - val_loss: 3.4965e-08
Epoch 325/512
448/448 - 0s - loss: 3.4551e-06 - val_loss: 3.3151e-08
Epoch 326/512
448/448 - 0s - loss: 3.2419e-06 - val_loss: 3.2412e-08
Epoch 327/512
448/448 - 0s - loss: 3.2086e-06 - val_loss: 3.1877e-08
Epoch 328/512
448/448 - 0s - loss: 3.1588e-06 - val_loss: 2.9902e-08
Epoch 329/512
448/448 - 0s - loss: 2.9436e-06 - val_loss: 2.9184e-08
Epoch 330/512
448/448 - 0s - loss: 2.9285e-06 - val_loss: 2.9302e-08
Epoch 331/512
448/448 - 0s - loss: 2.8783e-06 - val_loss: 2.7739e-08
Epoch 332/512
448/448 - 0s - loss: 2.7265e-06 - val_loss: 2.6545e-08
Epoch 333/512
448/448 - 0s - loss: 2.6414e-06 - val_loss: 2.6664e-08
Epoch 334/512
448/448 - 0s - loss: 2.6205e-06 - val_loss: 2.6146e-08
Epoch 335/512
448/448 - 0s - loss: 2.5525e-06 - val_loss: 2.4019e-08
Epoch 336/512
448/448 - 0s - loss: 2.3598e-06 - val_loss: 2.3693e-08
Epoch 337/512
448/448 - 0s - loss: 2.3708e-06 - val_loss: 2.3643e-08
Epoch 338/512
448/448 - 0s - loss: 2.3302e-06 - val_loss: 2.2400e-08
Epoch 339/512
448/448 - 0s - loss: 2.1989e-06 - val_loss: 2.1475e-08
Epoch 340/512
448/448 - 0s - loss: 2.1353e-06 - val_loss: 2.1230e-08
Epoch 341/512
448/448 - 0s - loss: 2.1113e-06 - val_loss: 2.0542e-08
Epoch 342/512
448/448 - 0s - loss: 2.0342e-06 - val_loss: 1.9449e-08
Epoch 343/512
448/448 - 0s - loss: 1.9293e-06 - val_loss: 1.9402e-08
Epoch 344/512
448/448 - 0s - loss: 1.9363e-06 - val_loss: 1.8526e-08
Epoch 345/512
448/448 - 0s - loss: 1.8360e-06 - val_loss: 1.7524e-08
Epoch 346/512
448/448 - 0s - loss: 1.7588e-06 - val_loss: 1.7413e-08
Epoch 347/512
448/448 - 0s - loss: 1.7311e-06 - val_loss: 1.7842e-08
Epoch 348/512
448/448 - 0s - loss: 1.7502e-06 - val_loss: 1.6282e-08
Epoch 349/512
448/448 - 0s - loss: 1.5862e-06 - val_loss: 1.5268e-08
Epoch 350/512
448/448 - 0s - loss: 1.5546e-06 - val_loss: 1.5765e-08
Epoch 351/512
448/448 - 0s - loss: 1.5783e-06 - val_loss: 1.5202e-08
Epoch 352/512
448/448 - 0s - loss: 1.4797e-06 - val_loss: 1.4284e-08
Epoch 353/512
448/448 - 0s - loss: 1.4133e-06 - val_loss: 1.4503e-08
Epoch 354/512
448/448 - 0s - loss: 1.4385e-06 - val_loss: 1.3915e-08
Epoch 355/512
448/448 - 0s - loss: 1.3645e-06 - val_loss: 1.2644e-08
Epoch 356/512
448/448 - 0s - loss: 1.2663e-06 - val_loss: 1.2586e-08
Epoch 357/512
448/448 - 0s - loss: 1.2719e-06 - val_loss: 1.3195e-08
Epoch 358/512
448/448 - 0s - loss: 1.2901e-06 - val_loss: 1.2212e-08
Epoch 359/512
448/448 - 0s - loss: 1.1751e-06 - val_loss: 1.1283e-08
Epoch 360/512
448/448 - 0s - loss: 1.1353e-06 - val_loss: 1.1395e-08
Epoch 361/512
448/448 - 0s - loss: 1.1420e-06 - val_loss: 1.1241e-08
Epoch 362/512
448/448 - 0s - loss: 1.0984e-06 - val_loss: 1.0664e-08
Epoch 363/512
448/448 - 0s - loss: 1.0508e-06 - val_loss: 1.0208e-08
Epoch 364/512
448/448 - 0s - loss: 1.0124e-06 - val_loss: 1.0049e-08
Epoch 365/512
448/448 - 0s - loss: 9.9669e-07 - val_loss: 9.8580e-09
Epoch 366/512
448/448 - 0s - loss: 9.6818e-07 - val_loss: 9.4563e-09
Epoch 367/512
448/448 - 0s - loss: 9.3053e-07 - val_loss: 9.0805e-09
Epoch 368/512
448/448 - 0s - loss: 9.0097e-07 - val_loss: 8.7846e-09
Epoch 369/512
448/448 - 0s - loss: 8.6765e-07 - val_loss: 8.7841e-09
Epoch 370/512
448/448 - 0s - loss: 8.6544e-07 - val_loss: 8.3375e-09
Epoch 371/512
448/448 - 0s - loss: 8.1951e-07 - val_loss: 7.7861e-09
Epoch 372/512
448/448 - 0s - loss: 7.7959e-07 - val_loss: 7.8362e-09
Epoch 373/512
448/448 - 0s - loss: 7.8699e-07 - val_loss: 7.6947e-09
Epoch 374/512
448/448 - 0s - loss: 7.5485e-07 - val_loss: 7.2951e-09
Epoch 375/512
448/448 - 0s - loss: 7.2012e-07 - val_loss: 6.9436e-09
Epoch 376/512
448/448 - 0s - loss: 6.9577e-07 - val_loss: 6.8488e-09
Epoch 377/512
448/448 - 0s - loss: 6.8242e-07 - val_loss: 6.7642e-09
Epoch 378/512
448/448 - 0s - loss: 6.6395e-07 - val_loss: 6.5379e-09
Epoch 379/512
448/448 - 0s - loss: 6.4593e-07 - val_loss: 6.0737e-09
Epoch 380/512
448/448 - 0s - loss: 5.9930e-07 - val_loss: 6.0610e-09
Epoch 381/512
448/448 - 0s - loss: 6.0946e-07 - val_loss: 6.0321e-09
Epoch 382/512
448/448 - 0s - loss: 5.9475e-07 - val_loss: 5.5135e-09
Epoch 383/512
448/448 - 0s - loss: 5.4508e-07 - val_loss: 5.4960e-09
Epoch 384/512
448/448 - 0s - loss: 5.5339e-07 - val_loss: 5.4469e-09
Epoch 385/512
448/448 - 0s - loss: 5.3411e-07 - val_loss: 5.1626e-09
Epoch 386/512
448/448 - 0s - loss: 5.1214e-07 - val_loss: 4.8666e-09
Epoch 387/512
448/448 - 0s - loss: 4.8148e-07 - val_loss: 5.0094e-09
Epoch 388/512
448/448 - 0s - loss: 4.9887e-07 - val_loss: 4.7906e-09
Epoch 389/512
448/448 - 0s - loss: 4.6814e-07 - val_loss: 4.3630e-09
Epoch 390/512
448/448 - 0s - loss: 4.3577e-07 - val_loss: 4.3932e-09
Epoch 391/512
448/448 - 0s - loss: 4.4145e-07 - val_loss: 4.5209e-09
Epoch 392/512
448/448 - 0s - loss: 4.4207e-07 - val_loss: 4.0931e-09
Epoch 393/512
448/448 - 0s - loss: 3.9514e-07 - val_loss: 3.8913e-09
Epoch 394/512
448/448 - 0s - loss: 3.9459e-07 - val_loss: 3.9898e-09
Epoch 395/512
448/448 - 0s - loss: 3.9490e-07 - val_loss: 3.8366e-09
Epoch 396/512
448/448 - 0s - loss: 3.7669e-07 - val_loss: 3.4823e-09
Epoch 397/512
448/448 - 0s - loss: 3.4675e-07 - val_loss: 3.5739e-09
Epoch 398/512
448/448 - 0s - loss: 3.5882e-07 - val_loss: 3.6087e-09
Epoch 399/512
448/448 - 0s - loss: 3.5096e-07 - val_loss: 3.2391e-09
Epoch 400/512
448/448 - 0s - loss: 3.1679e-07 - val_loss: 3.0955e-09
Epoch 401/512
448/448 - 0s - loss: 3.1241e-07 - val_loss: 3.2689e-09
Epoch 402/512
448/448 - 0s - loss: 3.2447e-07 - val_loss: 3.0526e-09
Epoch 403/512
448/448 - 0s - loss: 2.9343e-07 - val_loss: 2.8434e-09
Epoch 404/512
448/448 - 0s - loss: 2.8423e-07 - val_loss: 2.8619e-09
Epoch 405/512
448/448 - 0s - loss: 2.8636e-07 - val_loss: 2.7894e-09
Epoch 406/512
448/448 - 0s - loss: 2.7371e-07 - val_loss: 2.6292e-09
Epoch 407/512
448/448 - 0s - loss: 2.6055e-07 - val_loss: 2.5385e-09
Epoch 408/512
448/448 - 0s - loss: 2.5427e-07 - val_loss: 2.5018e-09
Epoch 409/512
448/448 - 0s - loss: 2.4936e-07 - val_loss: 2.4236e-09
Epoch 410/512
448/448 - 0s - loss: 2.4019e-07 - val_loss: 2.3288e-09
Epoch 411/512
448/448 - 0s - loss: 2.3229e-07 - val_loss: 2.1892e-09
Epoch 412/512
448/448 - 0s - loss: 2.2092e-07 - val_loss: 2.1607e-09
Epoch 413/512
448/448 - 0s - loss: 2.1704e-07 - val_loss: 2.1723e-09
Epoch 414/512
448/448 - 0s - loss: 2.1456e-07 - val_loss: 2.1016e-09
Epoch 415/512
448/448 - 0s - loss: 2.0658e-07 - val_loss: 1.9152e-09
Epoch 416/512
448/448 - 0s - loss: 1.9173e-07 - val_loss: 1.8761e-09
Epoch 417/512
448/448 - 0s - loss: 1.9008e-07 - val_loss: 1.9314e-09
Epoch 418/512
448/448 - 0s - loss: 1.9134e-07 - val_loss: 1.8511e-09
Epoch 419/512
448/448 - 0s - loss: 1.7998e-07 - val_loss: 1.7075e-09
Epoch 420/512
448/448 - 0s - loss: 1.6957e-07 - val_loss: 1.6994e-09
Epoch 421/512
448/448 - 0s - loss: 1.7072e-07 - val_loss: 1.6556e-09
Epoch 422/512
448/448 - 0s - loss: 1.6287e-07 - val_loss: 1.6093e-09
Epoch 423/512
448/448 - 0s - loss: 1.5952e-07 - val_loss: 1.5300e-09
Epoch 424/512
448/448 - 0s - loss: 1.5137e-07 - val_loss: 1.4973e-09
Epoch 425/512
448/448 - 0s - loss: 1.5015e-07 - val_loss: 1.4457e-09
Epoch 426/512
448/448 - 0s - loss: 1.4393e-07 - val_loss: 1.3697e-09
Epoch 427/512
448/448 - 0s - loss: 1.3759e-07 - val_loss: 1.3341e-09
Epoch 428/512
448/448 - 0s - loss: 1.3418e-07 - val_loss: 1.3460e-09
Epoch 429/512
448/448 - 0s - loss: 1.3395e-07 - val_loss: 1.2780e-09
Epoch 430/512
448/448 - 0s - loss: 1.2522e-07 - val_loss: 1.2150e-09
Epoch 431/512
448/448 - 0s - loss: 1.2123e-07 - val_loss: 1.1915e-09
Epoch 432/512
448/448 - 0s - loss: 1.1972e-07 - val_loss: 1.1590e-09
Epoch 433/512
448/448 - 0s - loss: 1.1554e-07 - val_loss: 1.1080e-09
Epoch 434/512
448/448 - 0s - loss: 1.1040e-07 - val_loss: 1.0982e-09
Epoch 435/512
448/448 - 0s - loss: 1.0878e-07 - val_loss: 1.0678e-09
Epoch 436/512
448/448 - 0s - loss: 1.0470e-07 - val_loss: 1.0275e-09
Epoch 437/512
448/448 - 0s - loss: 1.0152e-07 - val_loss: 9.8956e-10
Epoch 438/512
448/448 - 0s - loss: 9.7602e-08 - val_loss: 9.6732e-10
Epoch 439/512
448/448 - 0s - loss: 9.5796e-08 - val_loss: 9.3499e-10
Epoch 440/512
448/448 - 0s - loss: 9.2829e-08 - val_loss: 8.7396e-10
Epoch 441/512
448/448 - 0s - loss: 8.7527e-08 - val_loss: 8.5147e-10
Epoch 442/512
448/448 - 0s - loss: 8.5568e-08 - val_loss: 8.7352e-10
Epoch 443/512
448/448 - 0s - loss: 8.7066e-08 - val_loss: 8.0660e-10
Epoch 444/512
448/448 - 0s - loss: 7.9337e-08 - val_loss: 7.5796e-10
Epoch 445/512
448/448 - 0s - loss: 7.6792e-08 - val_loss: 7.7754e-10
Epoch 446/512
448/448 - 0s - loss: 7.8570e-08 - val_loss: 7.4243e-10
Epoch 447/512
448/448 - 0s - loss: 7.3662e-08 - val_loss: 6.9542e-10
Epoch 448/512
448/448 - 0s - loss: 6.9305e-08 - val_loss: 7.0266e-10
Epoch 449/512
448/448 - 0s - loss: 7.0556e-08 - val_loss: 6.9269e-10
Epoch 450/512
448/448 - 0s - loss: 6.8324e-08 - val_loss: 6.3602e-10
Epoch 451/512
448/448 - 0s - loss: 6.2744e-08 - val_loss: 6.2542e-10
Epoch 452/512
448/448 - 0s - loss: 6.2668e-08 - val_loss: 6.3389e-10
Epoch 453/512
448/448 - 0s - loss: 6.3041e-08 - val_loss: 5.9478e-10
Epoch 454/512
448/448 - 0s - loss: 5.8328e-08 - val_loss: 5.5834e-10
Epoch 455/512
448/448 - 0s - loss: 5.6106e-08 - val_loss: 5.5487e-10
Epoch 456/512
448/448 - 0s - loss: 5.5690e-08 - val_loss: 5.5748e-10
Epoch 457/512
448/448 - 0s - loss: 5.5097e-08 - val_loss: 5.2752e-10
Epoch 458/512
448/448 - 0s - loss: 5.1733e-08 - val_loss: 4.9175e-10
Epoch 459/512
448/448 - 0s - loss: 4.9005e-08 - val_loss: 4.9427e-10
Epoch 460/512
448/448 - 0s - loss: 4.9604e-08 - val_loss: 4.9203e-10
Epoch 461/512
448/448 - 0s - loss: 4.8434e-08 - val_loss: 4.5391e-10
Epoch 462/512
448/448 - 0s - loss: 4.5074e-08 - val_loss: 4.2977e-10
Epoch 463/512
448/448 - 0s - loss: 4.3383e-08 - val_loss: 4.3774e-10
Epoch 464/512
448/448 - 0s - loss: 4.4222e-08 - val_loss: 4.2906e-10
Epoch 465/512
448/448 - 0s - loss: 4.2162e-08 - val_loss: 3.9930e-10
Epoch 466/512
448/448 - 0s - loss: 3.9859e-08 - val_loss: 3.8411e-10
Epoch 467/512
448/448 - 0s - loss: 3.8887e-08 - val_loss: 3.7900e-10
Epoch 468/512
448/448 - 0s - loss: 3.8115e-08 - val_loss: 3.7867e-10
Epoch 469/512
448/448 - 0s - loss: 3.7568e-08 - val_loss: 3.6257e-10
Epoch 470/512
448/448 - 0s - loss: 3.5630e-08 - val_loss: 3.4271e-10
Epoch 471/512
448/448 - 0s - loss: 3.4406e-08 - val_loss: 3.3356e-10
Epoch 472/512
448/448 - 0s - loss: 3.3712e-08 - val_loss: 3.2474e-10
Epoch 473/512
448/448 - 0s - loss: 3.2598e-08 - val_loss: 3.1661e-10
Epoch 474/512
448/448 - 0s - loss: 3.1701e-08 - val_loss: 3.0660e-10
Epoch 475/512
448/448 - 0s - loss: 3.0396e-08 - val_loss: 3.0189e-10
Epoch 476/512
448/448 - 0s - loss: 2.9975e-08 - val_loss: 2.9360e-10
Epoch 477/512
448/448 - 0s - loss: 2.9174e-08 - val_loss: 2.7884e-10
Epoch 478/512
448/448 - 0s - loss: 2.7703e-08 - val_loss: 2.6655e-10
Epoch 479/512
448/448 - 0s - loss: 2.6618e-08 - val_loss: 2.6854e-10
Epoch 480/512
448/448 - 0s - loss: 2.7127e-08 - val_loss: 2.5552e-10
Epoch 481/512
448/448 - 0s - loss: 2.5125e-08 - val_loss: 2.4498e-10
Epoch 482/512
448/448 - 0s - loss: 2.4604e-08 - val_loss: 2.4261e-10
Epoch 483/512
448/448 - 0s - loss: 2.4115e-08 - val_loss: 2.3932e-10
Epoch 484/512
448/448 - 0s - loss: 2.3922e-08 - val_loss: 2.2279e-10
Epoch 485/512
448/448 - 0s - loss: 2.2082e-08 - val_loss: 2.1393e-10
Epoch 486/512
448/448 - 0s - loss: 2.1689e-08 - val_loss: 2.1241e-10
Epoch 487/512
448/448 - 0s - loss: 2.1308e-08 - val_loss: 2.1472e-10
Epoch 488/512
448/448 - 0s - loss: 2.1401e-08 - val_loss: 2.0196e-10
Epoch 489/512
448/448 - 0s - loss: 1.9715e-08 - val_loss: 1.8983e-10
Epoch 490/512
448/448 - 0s - loss: 1.9094e-08 - val_loss: 1.8847e-10
Epoch 491/512
448/448 - 0s - loss: 1.9171e-08 - val_loss: 1.8475e-10
Epoch 492/512
448/448 - 0s - loss: 1.8487e-08 - val_loss: 1.7586e-10
Epoch 493/512
448/448 - 0s - loss: 1.7664e-08 - val_loss: 1.6704e-10
Epoch 494/512
448/448 - 0s - loss: 1.6782e-08 - val_loss: 1.7137e-10
Epoch 495/512
448/448 - 0s - loss: 1.7316e-08 - val_loss: 1.6472e-10
Epoch 496/512
448/448 - 0s - loss: 1.6303e-08 - val_loss: 1.5534e-10
Epoch 497/512
448/448 - 0s - loss: 1.5505e-08 - val_loss: 1.5389e-10
Epoch 498/512
448/448 - 0s - loss: 1.5536e-08 - val_loss: 1.4959e-10
Epoch 499/512
448/448 - 0s - loss: 1.4936e-08 - val_loss: 1.4427e-10
Epoch 500/512
448/448 - 0s - loss: 1.4449e-08 - val_loss: 1.4141e-10
Epoch 501/512
448/448 - 0s - loss: 1.4231e-08 - val_loss: 1.3590e-10
Epoch 502/512
448/448 - 0s - loss: 1.3537e-08 - val_loss: 1.3171e-10
Epoch 503/512
448/448 - 0s - loss: 1.3218e-08 - val_loss: 1.2910e-10
Epoch 504/512
448/448 - 0s - loss: 1.2988e-08 - val_loss: 1.2675e-10
Epoch 505/512
448/448 - 0s - loss: 1.2685e-08 - val_loss: 1.2152e-10
Epoch 506/512
448/448 - 0s - loss: 1.2161e-08 - val_loss: 1.1734e-10
Epoch 507/512
448/448 - 0s - loss: 1.1772e-08 - val_loss: 1.1381e-10
Epoch 508/512
448/448 - 0s - loss: 1.1445e-08 - val_loss: 1.1167e-10
Epoch 509/512
448/448 - 0s - loss: 1.1252e-08 - val_loss: 1.0917e-10
Epoch 510/512
448/448 - 0s - loss: 1.0932e-08 - val_loss: 1.0694e-10
Epoch 511/512
448/448 - 0s - loss: 1.0731e-08 - val_loss: 1.0191e-10
Epoch 512/512
448/448 - 0s - loss: 1.0216e-08 - val_loss: 9.8471e-11
2024-04-14 21:39:00.789010: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2319e-08 - val_loss: 1.4895e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4266e-08 - val_loss: 1.0985e-08
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7624e-09 - val_loss: 7.8052e-09
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8558e-09 - val_loss: 8.4852e-09
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3793e-09 - val_loss: 1.0873e-08
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1214e-08 - val_loss: 1.0511e-08
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9125e-09 - val_loss: 8.4137e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1685e-09 - val_loss: 7.8229e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1294e-09 - val_loss: 8.6777e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0265e-09 - val_loss: 9.0793e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9848e-09 - val_loss: 8.2595e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0570e-09 - val_loss: 7.4191e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4513e-09 - val_loss: 7.4064e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5958e-09 - val_loss: 7.7219e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8217e-09 - val_loss: 7.5920e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5022e-09 - val_loss: 7.0420e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.9927e-09 - val_loss: 6.7306e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7944e-09 - val_loss: 6.6660e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7505e-09 - val_loss: 6.6901e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7490e-09 - val_loss: 6.5746e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5599e-09 - val_loss: 6.2076e-09
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1883e-09 - val_loss: 5.9544e-09
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0111e-09 - val_loss: 5.9155e-09
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9865e-09 - val_loss: 5.8671e-09
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8945e-09 - val_loss: 5.6900e-09
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7047e-09 - val_loss: 5.5255e-09
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5198e-09 - val_loss: 5.3142e-09
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3621e-09 - val_loss: 5.2431e-09
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2874e-09 - val_loss: 5.1641e-09
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1970e-09 - val_loss: 5.0009e-09
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0105e-09 - val_loss: 4.8305e-09
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8516e-09 - val_loss: 4.7696e-09
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7993e-09 - val_loss: 4.6668e-09
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6834e-09 - val_loss: 4.5283e-09
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5450e-09 - val_loss: 4.3941e-09
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4275e-09 - val_loss: 4.3232e-09
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3457e-09 - val_loss: 4.2313e-09
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2585e-09 - val_loss: 4.1459e-09
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1767e-09 - val_loss: 4.0569e-09
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0636e-09 - val_loss: 3.9123e-09
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9482e-09 - val_loss: 3.8553e-09
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8757e-09 - val_loss: 3.7653e-09
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7956e-09 - val_loss: 3.7105e-09
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7223e-09 - val_loss: 3.6027e-09
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6145e-09 - val_loss: 3.5263e-09
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5549e-09 - val_loss: 3.4436e-09
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4574e-09 - val_loss: 3.3804e-09
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3991e-09 - val_loss: 3.3291e-09
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3508e-09 - val_loss: 3.2425e-09
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2377e-09 - val_loss: 3.1265e-09
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1512e-09 - val_loss: 3.0876e-09
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.1089e-09 - val_loss: 3.0701e-09
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0956e-09 - val_loss: 3.0040e-09
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0129e-09 - val_loss: 2.9000e-09
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9091e-09 - val_loss: 2.8066e-09
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8261e-09 - val_loss: 2.7525e-09
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7666e-09 - val_loss: 2.7176e-09
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7471e-09 - val_loss: 2.6919e-09
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7163e-09 - val_loss: 2.6416e-09
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6594e-09 - val_loss: 2.5752e-09
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5757e-09 - val_loss: 2.4838e-09
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4946e-09 - val_loss: 2.4384e-09
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4614e-09 - val_loss: 2.4185e-09
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4334e-09 - val_loss: 2.3926e-09
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4075e-09 - val_loss: 2.3386e-09
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3419e-09 - val_loss: 2.2676e-09
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2653e-09 - val_loss: 2.2025e-09
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2127e-09 - val_loss: 2.1654e-09
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1794e-09 - val_loss: 2.1371e-09
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1530e-09 - val_loss: 2.1358e-09
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1483e-09 - val_loss: 2.0885e-09
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0881e-09 - val_loss: 2.0325e-09
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0418e-09 - val_loss: 1.9885e-09
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9996e-09 - val_loss: 1.9599e-09
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9637e-09 - val_loss: 1.8970e-09
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9053e-09 - val_loss: 1.8561e-09
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8760e-09 - val_loss: 1.8543e-09
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8683e-09 - val_loss: 1.8254e-09
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8385e-09 - val_loss: 1.7998e-09
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8044e-09 - val_loss: 1.7408e-09
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7397e-09 - val_loss: 1.6942e-09
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7138e-09 - val_loss: 1.6931e-09
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7083e-09 - val_loss: 1.6787e-09
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6865e-09 - val_loss: 1.6283e-09
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6237e-09 - val_loss: 1.5646e-09
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5839e-09 - val_loss: 1.5675e-09
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5873e-09 - val_loss: 1.5616e-09
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5769e-09 - val_loss: 1.5364e-09
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5400e-09 - val_loss: 1.4946e-09
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4943e-09 - val_loss: 1.4660e-09
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4742e-09 - val_loss: 1.4424e-09
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4493e-09 - val_loss: 1.4203e-09
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4242e-09 - val_loss: 1.3957e-09
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4009e-09 - val_loss: 1.3637e-09
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3750e-09 - val_loss: 1.3527e-09
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3516e-09 - val_loss: 1.3125e-09
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3262e-09 - val_loss: 1.2903e-09
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2974e-09 - val_loss: 1.2795e-09
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2898e-09 - val_loss: 1.2751e-09
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2858e-09 - val_loss: 1.2490e-09
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2509e-09 - val_loss: 1.2230e-09
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2214e-09 - val_loss: 1.1951e-09
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2010e-09 - val_loss: 1.1780e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1869e-09 - val_loss: 1.1622e-09
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1696e-09 - val_loss: 1.1625e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1641e-09 - val_loss: 1.1326e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1341e-09 - val_loss: 1.1135e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1200e-09 - val_loss: 1.0962e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1066e-09 - val_loss: 1.0941e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0989e-09 - val_loss: 1.0683e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0682e-09 - val_loss: 1.0369e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0386e-09 - val_loss: 1.0260e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0373e-09 - val_loss: 1.0254e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0285e-09 - val_loss: 1.0137e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0172e-09 - val_loss: 9.8985e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9566e-10 - val_loss: 9.7583e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7861e-10 - val_loss: 9.6235e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.6168e-10 - val_loss: 9.3244e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.3637e-10 - val_loss: 9.1470e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2476e-10 - val_loss: 9.2363e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3565e-10 - val_loss: 9.3090e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3405e-10 - val_loss: 9.1533e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.0755e-10 - val_loss: 8.6973e-10
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.6922e-10 - val_loss: 8.4983e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.5597e-10 - val_loss: 8.4504e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5478e-10 - val_loss: 8.5360e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.5963e-10 - val_loss: 8.4479e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.4519e-10 - val_loss: 8.2522e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.3081e-10 - val_loss: 8.1714e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.1397e-10 - val_loss: 7.9927e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0686e-10 - val_loss: 8.0391e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.0589e-10 - val_loss: 7.8621e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8304e-10 - val_loss: 7.5743e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5867e-10 - val_loss: 7.4292e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4694e-10 - val_loss: 7.3755e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4121e-10 - val_loss: 7.3798e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5189e-10 - val_loss: 7.4570e-10
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.4822e-10 - val_loss: 7.3410e-10
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.3166e-10 - val_loss: 7.0748e-10
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0515e-10 - val_loss: 6.7303e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7435e-10 - val_loss: 6.6730e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7588e-10 - val_loss: 6.7451e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7994e-10 - val_loss: 6.7013e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7674e-10 - val_loss: 6.6595e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.7351e-10 - val_loss: 6.6335e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.6546e-10 - val_loss: 6.5570e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5776e-10 - val_loss: 6.3413e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.3672e-10 - val_loss: 6.2478e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.2855e-10 - val_loss: 6.1629e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2200e-10 - val_loss: 6.1947e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.2238e-10 - val_loss: 6.0822e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0903e-10 - val_loss: 5.9891e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9892e-10 - val_loss: 5.9207e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9508e-10 - val_loss: 5.7864e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8821e-10 - val_loss: 5.8888e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.9201e-10 - val_loss: 5.7462e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8380e-10 - val_loss: 5.7958e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.7922e-10 - val_loss: 5.6602e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6779e-10 - val_loss: 5.5111e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.5379e-10 - val_loss: 5.3564e-10
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3739e-10 - val_loss: 5.2636e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3110e-10 - val_loss: 5.2607e-10
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2891e-10 - val_loss: 5.2722e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.3355e-10 - val_loss: 5.2532e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2829e-10 - val_loss: 5.1846e-10
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.2234e-10 - val_loss: 5.1598e-10
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1590e-10 - val_loss: 5.0455e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.0438e-10 - val_loss: 4.9510e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9749e-10 - val_loss: 4.8553e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8520e-10 - val_loss: 4.8023e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8331e-10 - val_loss: 4.7127e-10
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.7348e-10 - val_loss: 4.6910e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.6787e-10 - val_loss: 4.5633e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5769e-10 - val_loss: 4.5452e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.5534e-10 - val_loss: 4.4423e-10
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4633e-10 - val_loss: 4.4610e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4985e-10 - val_loss: 4.5093e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5339e-10 - val_loss: 4.4715e-10
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5135e-10 - val_loss: 4.4664e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.4755e-10 - val_loss: 4.3662e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.3507e-10 - val_loss: 4.2778e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2808e-10 - val_loss: 4.2395e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.2494e-10 - val_loss: 4.1961e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.1955e-10 - val_loss: 4.0605e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.0499e-10 - val_loss: 3.9530e-10
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9962e-10 - val_loss: 4.0041e-10
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0282e-10 - val_loss: 3.9860e-10
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0208e-10 - val_loss: 3.9870e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.9806e-10 - val_loss: 3.8474e-10
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8521e-10 - val_loss: 3.8827e-10
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9315e-10 - val_loss: 3.9798e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9774e-10 - val_loss: 3.8985e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.8659e-10 - val_loss: 3.8159e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.7933e-10 - val_loss: 3.6631e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6767e-10 - val_loss: 3.6227e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6641e-10 - val_loss: 3.6481e-10
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7104e-10 - val_loss: 3.7426e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7529e-10 - val_loss: 3.7411e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7648e-10 - val_loss: 3.6776e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6834e-10 - val_loss: 3.6140e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.6048e-10 - val_loss: 3.5145e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.5168e-10 - val_loss: 3.4409e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4496e-10 - val_loss: 3.4414e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4711e-10 - val_loss: 3.4616e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4901e-10 - val_loss: 3.4509e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4642e-10 - val_loss: 3.4723e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.4640e-10 - val_loss: 3.3493e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3400e-10 - val_loss: 3.2943e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3233e-10 - val_loss: 3.2945e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.3053e-10 - val_loss: 3.1935e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2085e-10 - val_loss: 3.2265e-10
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2615e-10 - val_loss: 3.2104e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.2032e-10 - val_loss: 3.0599e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0726e-10 - val_loss: 3.0499e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0702e-10 - val_loss: 3.0202e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0741e-10 - val_loss: 3.0713e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0917e-10 - val_loss: 3.0914e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0886e-10 - val_loss: 3.0189e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 3.0052e-10 - val_loss: 2.9804e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9512e-10 - val_loss: 2.8692e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.9002e-10 - val_loss: 2.8554e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8809e-10 - val_loss: 2.8287e-10
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8507e-10 - val_loss: 2.8746e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8819e-10 - val_loss: 2.8616e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8778e-10 - val_loss: 2.8426e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.8669e-10 - val_loss: 2.8250e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.7864e-10 - val_loss: 2.6282e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6143e-10 - val_loss: 2.6039e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6429e-10 - val_loss: 2.6516e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6520e-10 - val_loss: 2.6520e-10
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6721e-10 - val_loss: 2.6722e-10
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7139e-10 - val_loss: 2.7436e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7329e-10 - val_loss: 2.6361e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.6071e-10 - val_loss: 2.5498e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5607e-10 - val_loss: 2.4877e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5041e-10 - val_loss: 2.4637e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4619e-10 - val_loss: 2.4643e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4999e-10 - val_loss: 2.4966e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5220e-10 - val_loss: 2.5922e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5827e-10 - val_loss: 2.5624e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5735e-10 - val_loss: 2.4995e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.5103e-10 - val_loss: 2.4482e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.4407e-10 - val_loss: 2.3517e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3545e-10 - val_loss: 2.3138e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.3001e-10 - val_loss: 2.2554e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2867e-10 - val_loss: 2.2677e-10
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2705e-10 - val_loss: 2.2639e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3164e-10 - val_loss: 2.3455e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3717e-10 - val_loss: 2.4100e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4275e-10 - val_loss: 2.3609e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3729e-10 - val_loss: 2.3472e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3323e-10 - val_loss: 2.2668e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2588e-10 - val_loss: 2.2368e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2703e-10 - val_loss: 2.2524e-10
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.2128e-10 - val_loss: 2.1756e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1718e-10 - val_loss: 2.1297e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1754e-10 - val_loss: 2.2096e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1960e-10 - val_loss: 2.1211e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1466e-10 - val_loss: 2.1230e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1426e-10 - val_loss: 2.1523e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.1363e-10 - val_loss: 2.0874e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0883e-10 - val_loss: 2.0503e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0527e-10 - val_loss: 2.0086e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0398e-10 - val_loss: 2.0112e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0422e-10 - val_loss: 2.0074e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 2.0131e-10 - val_loss: 1.9821e-10
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0151e-10 - val_loss: 2.0212e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0402e-10 - val_loss: 2.0335e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0451e-10 - val_loss: 2.0817e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0715e-10 - val_loss: 2.0152e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9876e-10 - val_loss: 1.9088e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.9055e-10 - val_loss: 1.8157e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7981e-10 - val_loss: 1.7967e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8226e-10 - val_loss: 1.8168e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8384e-10 - val_loss: 1.8585e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8755e-10 - val_loss: 1.8968e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9307e-10 - val_loss: 1.9316e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9219e-10 - val_loss: 1.8870e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8949e-10 - val_loss: 1.8350e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.8307e-10 - val_loss: 1.7775e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7694e-10 - val_loss: 1.7612e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7617e-10 - val_loss: 1.7400e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7708e-10 - val_loss: 1.8074e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8249e-10 - val_loss: 1.8111e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7892e-10 - val_loss: 1.7719e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7899e-10 - val_loss: 1.7603e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7572e-10 - val_loss: 1.7280e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.7208e-10 - val_loss: 1.6958e-10
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6979e-10 - val_loss: 1.6806e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6807e-10 - val_loss: 1.6730e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6747e-10 - val_loss: 1.6858e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6878e-10 - val_loss: 1.6626e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6744e-10 - val_loss: 1.6414e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6436e-10 - val_loss: 1.6154e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6245e-10 - val_loss: 1.6245e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6269e-10 - val_loss: 1.6103e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6243e-10 - val_loss: 1.6326e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6483e-10 - val_loss: 1.6434e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6342e-10 - val_loss: 1.6242e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6524e-10 - val_loss: 1.6753e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6925e-10 - val_loss: 1.6340e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.6338e-10 - val_loss: 1.5955e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5795e-10 - val_loss: 1.5312e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5459e-10 - val_loss: 1.5293e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5304e-10 - val_loss: 1.5035e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5161e-10 - val_loss: 1.5143e-10
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.5255e-10 - val_loss: 1.4994e-10
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4986e-10 - val_loss: 1.4514e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4699e-10 - val_loss: 1.4755e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4693e-10 - val_loss: 1.4367e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4428e-10 - val_loss: 1.4399e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4390e-10 - val_loss: 1.4403e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4378e-10 - val_loss: 1.4562e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4796e-10 - val_loss: 1.4801e-10
Epoch 315/512

Epoch 00315: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4539e-10 - val_loss: 1.4023e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.4038e-10 - val_loss: 1.4022e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3935e-10 - val_loss: 1.3727e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3932e-10 - val_loss: 1.4087e-10
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4250e-10 - val_loss: 1.4287e-10
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4387e-10 - val_loss: 1.4234e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4219e-10 - val_loss: 1.3959e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3783e-10 - val_loss: 1.3736e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3727e-10 - val_loss: 1.3543e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.3327e-10 - val_loss: 1.2727e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2778e-10 - val_loss: 1.2683e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2821e-10 - val_loss: 1.2831e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2928e-10 - val_loss: 1.2809e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2945e-10 - val_loss: 1.3122e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3340e-10 - val_loss: 1.2968e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2791e-10 - val_loss: 1.2780e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2752e-10 - val_loss: 1.2721e-10
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2648e-10 - val_loss: 1.2434e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2492e-10 - val_loss: 1.2562e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2493e-10 - val_loss: 1.2290e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2362e-10 - val_loss: 1.2227e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2272e-10 - val_loss: 1.2228e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.2253e-10 - val_loss: 1.1927e-10
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1951e-10 - val_loss: 1.1838e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1882e-10 - val_loss: 1.2014e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1977e-10 - val_loss: 1.1884e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1842e-10 - val_loss: 1.1735e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1798e-10 - val_loss: 1.1714e-10
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1668e-10 - val_loss: 1.1465e-10
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1388e-10 - val_loss: 1.1373e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1649e-10 - val_loss: 1.1943e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1845e-10 - val_loss: 1.1401e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1423e-10 - val_loss: 1.1214e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1256e-10 - val_loss: 1.0959e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0983e-10 - val_loss: 1.0871e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1087e-10 - val_loss: 1.1393e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1504e-10 - val_loss: 1.1383e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1356e-10 - val_loss: 1.1222e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1154e-10 - val_loss: 1.1017e-10
Epoch 354/512

Epoch 00354: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.1062e-10 - val_loss: 1.0775e-10
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0713e-10 - val_loss: 1.0508e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0646e-10 - val_loss: 1.0803e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0814e-10 - val_loss: 1.0894e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1031e-10 - val_loss: 1.1134e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0950e-10 - val_loss: 1.0510e-10
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0344e-10 - val_loss: 1.0440e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0562e-10 - val_loss: 1.0333e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0105e-10 - val_loss: 9.9493e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0075e-10 - val_loss: 1.0331e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 1.0287e-10 - val_loss: 9.9386e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0018e-10 - val_loss: 1.0207e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0400e-10 - val_loss: 1.0453e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0690e-10 - val_loss: 1.1032e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1069e-10 - val_loss: 1.1087e-10
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0803e-10 - val_loss: 1.0339e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0457e-10 - val_loss: 1.0298e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0341e-10 - val_loss: 1.0237e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0257e-10 - val_loss: 1.0221e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0192e-10 - val_loss: 9.9939e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.9122e-11 - val_loss: 9.7507e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.6332e-11 - val_loss: 9.4625e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4167e-11 - val_loss: 9.7305e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9572e-11 - val_loss: 9.8915e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0024e-10 - val_loss: 1.0225e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0143e-10 - val_loss: 9.9872e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8504e-11 - val_loss: 9.7588e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8700e-11 - val_loss: 9.9427e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8598e-11 - val_loss: 9.4712e-11
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.3260e-11 - val_loss: 9.2091e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2875e-11 - val_loss: 9.3863e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4528e-11 - val_loss: 9.6535e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7246e-11 - val_loss: 9.6617e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6574e-11 - val_loss: 9.6008e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7327e-11 - val_loss: 9.8262e-11
Epoch 389/512

Epoch 00389: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.7334e-11 - val_loss: 9.1441e-11
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.1324e-11 - val_loss: 9.0449e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.9962e-11 - val_loss: 8.9684e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 9.0903e-11 - val_loss: 8.8796e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.7258e-11 - val_loss: 8.4411e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5089e-11 - val_loss: 8.5250e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5202e-11 - val_loss: 8.4429e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4646e-11 - val_loss: 8.6084e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6549e-11 - val_loss: 8.7225e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7252e-11 - val_loss: 8.6883e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8186e-11 - val_loss: 9.0579e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3571e-11 - val_loss: 9.7616e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8409e-11 - val_loss: 9.4596e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2367e-11 - val_loss: 9.0878e-11
Epoch 403/512

Epoch 00403: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.8293e-11 - val_loss: 8.2972e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2088e-11 - val_loss: 7.9378e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.9074e-11 - val_loss: 7.8912e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1001e-11 - val_loss: 8.5472e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7255e-11 - val_loss: 8.9289e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8012e-11 - val_loss: 8.4162e-11
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 8.2537e-11 - val_loss: 7.8463e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0108e-11 - val_loss: 8.3037e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3491e-11 - val_loss: 8.5002e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6181e-11 - val_loss: 8.7091e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4622e-11 - val_loss: 8.0752e-11
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.9401e-11 - val_loss: 7.7551e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7611e-11 - val_loss: 7.8333e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9374e-11 - val_loss: 7.9604e-11
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.8299e-11 - val_loss: 7.6547e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7663e-11 - val_loss: 7.9800e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9409e-11 - val_loss: 7.9210e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.9366e-11 - val_loss: 7.6075e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6264e-11 - val_loss: 7.7259e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8639e-11 - val_loss: 7.7243e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.6399e-11 - val_loss: 7.4832e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8104e-11 - val_loss: 8.2717e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5028e-11 - val_loss: 8.7097e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7400e-11 - val_loss: 8.3624e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2353e-11 - val_loss: 7.7624e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.5877e-11 - val_loss: 7.1927e-11
Epoch 429/512

Epoch 00429: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0554e-11 - val_loss: 6.8851e-11
Epoch 430/512

Epoch 00430: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8599e-11 - val_loss: 6.8798e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0704e-11 - val_loss: 7.3607e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3532e-11 - val_loss: 7.2773e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3835e-11 - val_loss: 7.5077e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3255e-11 - val_loss: 7.1615e-11
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 7.0830e-11 - val_loss: 6.8782e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9091e-11 - val_loss: 6.9868e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1342e-11 - val_loss: 7.3912e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5676e-11 - val_loss: 7.8289e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8334e-11 - val_loss: 7.7502e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7010e-11 - val_loss: 7.2721e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1493e-11 - val_loss: 7.1019e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1454e-11 - val_loss: 7.1638e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2060e-11 - val_loss: 7.3229e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2146e-11 - val_loss: 6.9184e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8864e-11 - val_loss: 7.0587e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1956e-11 - val_loss: 7.4018e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2585e-11 - val_loss: 7.0585e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.8437e-11 - val_loss: 6.4059e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4190e-11 - val_loss: 6.4605e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5927e-11 - val_loss: 6.7000e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7135e-11 - val_loss: 6.6985e-11
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.5376e-11 - val_loss: 6.2380e-11
Epoch 453/512

Epoch 00453: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.1357e-11 - val_loss: 5.7621e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7258e-11 - val_loss: 5.9908e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1706e-11 - val_loss: 6.4543e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5667e-11 - val_loss: 6.7333e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7953e-11 - val_loss: 7.0305e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1141e-11 - val_loss: 7.1902e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3466e-11 - val_loss: 7.1796e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0670e-11 - val_loss: 6.8536e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8262e-11 - val_loss: 6.7658e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6133e-11 - val_loss: 6.3010e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1978e-11 - val_loss: 5.9484e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8850e-11 - val_loss: 5.7789e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9039e-11 - val_loss: 6.2023e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3555e-11 - val_loss: 6.4018e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5372e-11 - val_loss: 6.6268e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6002e-11 - val_loss: 6.2185e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2340e-11 - val_loss: 6.0768e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1103e-11 - val_loss: 5.9502e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9921e-11 - val_loss: 6.0191e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1324e-11 - val_loss: 6.3991e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3278e-11 - val_loss: 6.1108e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 6.0130e-11 - val_loss: 5.6990e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6711e-11 - val_loss: 5.8111e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8727e-11 - val_loss: 5.9646e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1201e-11 - val_loss: 6.5238e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6347e-11 - val_loss: 6.7505e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5740e-11 - val_loss: 6.2055e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0522e-11 - val_loss: 5.9633e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.8563e-11 - val_loss: 5.5888e-11
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.6223e-11 - val_loss: 5.4059e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 5.1887e-11 - val_loss: 4.9402e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9808e-11 - val_loss: 5.2356e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4202e-11 - val_loss: 5.5840e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7257e-11 - val_loss: 5.9260e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9545e-11 - val_loss: 5.9316e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9628e-11 - val_loss: 5.8481e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7326e-11 - val_loss: 5.4883e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4759e-11 - val_loss: 5.2368e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2080e-11 - val_loss: 5.0309e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9855e-11 - val_loss: 5.2904e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5522e-11 - val_loss: 5.6328e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6301e-11 - val_loss: 5.4871e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5725e-11 - val_loss: 5.7159e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6706e-11 - val_loss: 5.6582e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6170e-11 - val_loss: 5.4432e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4981e-11 - val_loss: 5.5722e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6163e-11 - val_loss: 5.7501e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7433e-11 - val_loss: 5.7354e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6888e-11 - val_loss: 5.6973e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6130e-11 - val_loss: 5.3796e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3211e-11 - val_loss: 5.0170e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.9340e-11 - val_loss: 4.7483e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6891e-11 - val_loss: 4.9574e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0742e-11 - val_loss: 5.2058e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2056e-11 - val_loss: 5.0622e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2840e-11 - val_loss: 5.4917e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5587e-11 - val_loss: 5.6446e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6004e-11 - val_loss: 5.3166e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2966e-11 - val_loss: 4.9661e-11
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/addition_weights.h5
448/448 - 0s - loss: 4.8565e-11 - val_loss: 4.6643e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0927 - val_loss: 0.0140
Epoch 2/512
448/448 - 0s - loss: 0.0260 - val_loss: 0.0059
Epoch 3/512
448/448 - 0s - loss: 0.0206 - val_loss: 0.0016
Epoch 4/512
448/448 - 0s - loss: 0.0156 - val_loss: 4.6617e-04
Epoch 5/512
448/448 - 0s - loss: 0.0103 - val_loss: 3.9414e-04
Epoch 6/512
448/448 - 0s - loss: 0.0060 - val_loss: 3.0611e-04
Epoch 7/512
448/448 - 0s - loss: 0.0053 - val_loss: 8.6799e-04
Epoch 8/512
448/448 - 0s - loss: 0.0050 - val_loss: 5.1248e-04
Epoch 9/512
448/448 - 0s - loss: 0.0030 - val_loss: 8.4927e-04
Epoch 10/512
448/448 - 0s - loss: 0.0032 - val_loss: 6.6923e-04
Epoch 11/512
448/448 - 0s - loss: 0.0042 - val_loss: 9.0771e-04
Epoch 12/512
448/448 - 0s - loss: 0.0030 - val_loss: 0.0013
Epoch 13/512
448/448 - 0s - loss: 0.0027 - val_loss: 0.0011
Epoch 14/512
448/448 - 0s - loss: 0.0030 - val_loss: 8.6544e-04
Epoch 15/512
448/448 - 0s - loss: 0.0030 - val_loss: 9.9077e-04
Epoch 16/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.9308e-04
Epoch 17/512
448/448 - 0s - loss: 0.0024 - val_loss: 6.3969e-04
Epoch 18/512
448/448 - 0s - loss: 0.0025 - val_loss: 5.7585e-04
Epoch 19/512
448/448 - 0s - loss: 0.0022 - val_loss: 5.5678e-04
Epoch 20/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1948e-04
Epoch 21/512
448/448 - 0s - loss: 0.0020 - val_loss: 2.9863e-04
Epoch 22/512
448/448 - 0s - loss: 0.0020 - val_loss: 2.3279e-04
Epoch 23/512
448/448 - 0s - loss: 0.0018 - val_loss: 2.0498e-04
Epoch 24/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.5009e-04
Epoch 25/512
448/448 - 0s - loss: 0.0017 - val_loss: 1.1389e-04
Epoch 26/512
448/448 - 0s - loss: 0.0017 - val_loss: 9.0281e-05
Epoch 27/512
448/448 - 0s - loss: 0.0016 - val_loss: 7.0819e-05
Epoch 28/512
448/448 - 0s - loss: 0.0015 - val_loss: 6.6094e-05
Epoch 29/512
448/448 - 0s - loss: 0.0014 - val_loss: 7.5582e-05
Epoch 30/512
448/448 - 0s - loss: 0.0014 - val_loss: 7.3382e-05
Epoch 31/512
448/448 - 0s - loss: 0.0013 - val_loss: 6.2663e-05
Epoch 32/512
448/448 - 0s - loss: 0.0013 - val_loss: 7.6098e-05
Epoch 33/512
448/448 - 0s - loss: 0.0012 - val_loss: 6.5733e-05
Epoch 34/512
448/448 - 0s - loss: 0.0012 - val_loss: 7.0133e-05
Epoch 35/512
448/448 - 0s - loss: 0.0011 - val_loss: 5.8669e-05
Epoch 36/512
448/448 - 0s - loss: 0.0010 - val_loss: 7.3760e-05
Epoch 37/512
448/448 - 0s - loss: 9.4320e-04 - val_loss: 1.2873e-04
Epoch 38/512
448/448 - 0s - loss: 0.0013 - val_loss: 12.6168
Epoch 39/512
448/448 - 0s - loss: 3.6020 - val_loss: 0.0010
Epoch 40/512
448/448 - 0s - loss: 9.1156e-04 - val_loss: 7.8221e-04
Epoch 41/512
448/448 - 0s - loss: 8.9216e-04 - val_loss: 0.0010
Epoch 42/512
448/448 - 0s - loss: 9.9549e-04 - val_loss: 9.9602e-04
Epoch 43/512
448/448 - 0s - loss: 9.5112e-04 - val_loss: 8.0356e-04
Epoch 44/512
448/448 - 0s - loss: 9.1186e-04 - val_loss: 8.1524e-04
Epoch 45/512
448/448 - 0s - loss: 9.2768e-04 - val_loss: 8.1876e-04
Epoch 46/512
448/448 - 0s - loss: 9.1280e-04 - val_loss: 7.4478e-04
Epoch 47/512
448/448 - 0s - loss: 8.7704e-04 - val_loss: 7.1822e-04
Epoch 48/512
448/448 - 0s - loss: 8.5368e-04 - val_loss: 7.0818e-04
Epoch 49/512
448/448 - 0s - loss: 8.2239e-04 - val_loss: 6.7078e-04
Epoch 50/512
448/448 - 0s - loss: 7.7950e-04 - val_loss: 6.4359e-04
Epoch 51/512
448/448 - 0s - loss: 7.3775e-04 - val_loss: 6.1284e-04
Epoch 52/512
448/448 - 0s - loss: 6.9372e-04 - val_loss: 5.7998e-04
Epoch 53/512
448/448 - 0s - loss: 6.5256e-04 - val_loss: 5.3873e-04
Epoch 54/512
448/448 - 0s - loss: 6.1338e-04 - val_loss: 4.9669e-04
Epoch 55/512
448/448 - 0s - loss: 5.8002e-04 - val_loss: 4.5863e-04
Epoch 56/512
448/448 - 0s - loss: 5.4973e-04 - val_loss: 4.1710e-04
Epoch 57/512
448/448 - 0s - loss: 5.2239e-04 - val_loss: 3.7964e-04
Epoch 58/512
448/448 - 0s - loss: 4.9435e-04 - val_loss: 3.4182e-04
Epoch 59/512
448/448 - 0s - loss: 4.6574e-04 - val_loss: 3.0730e-04
Epoch 60/512
448/448 - 0s - loss: 4.3939e-04 - val_loss: 2.8103e-04
Epoch 61/512
448/448 - 0s - loss: 4.1591e-04 - val_loss: 2.5317e-04
Epoch 62/512
448/448 - 0s - loss: 3.8813e-04 - val_loss: 2.2927e-04
Epoch 63/512
448/448 - 0s - loss: 3.6310e-04 - val_loss: 2.0845e-04
Epoch 64/512
448/448 - 0s - loss: 3.3943e-04 - val_loss: 1.9210e-04
Epoch 65/512
448/448 - 0s - loss: 3.1667e-04 - val_loss: 1.7398e-04
Epoch 66/512
448/448 - 0s - loss: 2.9050e-04 - val_loss: 1.5595e-04
Epoch 67/512
448/448 - 0s - loss: 2.6543e-04 - val_loss: 1.4115e-04
Epoch 68/512
448/448 - 0s - loss: 2.4073e-04 - val_loss: 1.2371e-04
Epoch 69/512
448/448 - 0s - loss: 2.1241e-04 - val_loss: 1.0479e-04
Epoch 70/512
448/448 - 0s - loss: 1.8519e-04 - val_loss: 8.9446e-05
Epoch 71/512
448/448 - 0s - loss: 1.6192e-04 - val_loss: 7.6784e-05
Epoch 72/512
448/448 - 0s - loss: 1.3853e-04 - val_loss: 6.2067e-05
Epoch 73/512
448/448 - 0s - loss: 1.1559e-04 - val_loss: 4.8354e-05
Epoch 74/512
448/448 - 0s - loss: 9.5942e-05 - val_loss: 3.8957e-05
Epoch 75/512
448/448 - 0s - loss: 8.0003e-05 - val_loss: 3.1863e-05
Epoch 76/512
448/448 - 0s - loss: 6.6263e-05 - val_loss: 2.4400e-05
Epoch 77/512
448/448 - 0s - loss: 5.3170e-05 - val_loss: 1.7798e-05
Epoch 78/512
448/448 - 0s - loss: 4.2557e-05 - val_loss: 1.3905e-05
Epoch 79/512
448/448 - 0s - loss: 3.4409e-05 - val_loss: 1.1197e-05
Epoch 80/512
448/448 - 0s - loss: 2.7687e-05 - val_loss: 8.8295e-06
Epoch 81/512
448/448 - 0s - loss: 2.1915e-05 - val_loss: 6.8289e-06
Epoch 82/512
448/448 - 0s - loss: 1.7187e-05 - val_loss: 5.5993e-06
Epoch 83/512
448/448 - 0s - loss: 1.3527e-05 - val_loss: 5.1095e-06
Epoch 84/512
448/448 - 0s - loss: 1.0859e-05 - val_loss: 5.0799e-06
Epoch 85/512
448/448 - 0s - loss: 8.7358e-06 - val_loss: 4.9184e-06
Epoch 86/512
448/448 - 0s - loss: 6.7880e-06 - val_loss: 4.8893e-06
Epoch 87/512
448/448 - 0s - loss: 5.2643e-06 - val_loss: 5.1820e-06
Epoch 88/512
448/448 - 0s - loss: 4.1879e-06 - val_loss: 5.6075e-06
Epoch 89/512
448/448 - 0s - loss: 3.2832e-06 - val_loss: 6.0316e-06
Epoch 90/512
448/448 - 0s - loss: 2.3341e-06 - val_loss: 6.6573e-06
Epoch 91/512
448/448 - 0s - loss: 1.3670e-06 - val_loss: 6.8556e-06
Epoch 92/512
448/448 - 0s - loss: 7.3949e-07 - val_loss: 4.3967e-06
Epoch 93/512
448/448 - 0s - loss: 6.4683e-07 - val_loss: 3.4449e-06
Epoch 94/512
448/448 - 0s - loss: 9.4801e-07 - val_loss: 3.2076e-06
Epoch 95/512
448/448 - 0s - loss: 7.5934e-07 - val_loss: 4.8067e-06
Epoch 96/512
448/448 - 0s - loss: 2.8929e-07 - val_loss: 4.8948e-06
Epoch 97/512
448/448 - 0s - loss: 2.8790e-07 - val_loss: 5.7148e-06
Epoch 98/512
448/448 - 0s - loss: 6.3141e-07 - val_loss: 4.0831e-06
Epoch 99/512
448/448 - 0s - loss: 2.3525e-07 - val_loss: 4.0331e-06
Epoch 100/512
448/448 - 0s - loss: 1.5016e-07 - val_loss: 4.1228e-06
Epoch 101/512
448/448 - 0s - loss: 1.4413e-07 - val_loss: 4.3040e-06
Epoch 102/512
448/448 - 0s - loss: 2.8785e-07 - val_loss: 2.7729e-06
Epoch 103/512
448/448 - 0s - loss: 4.5561e-07 - val_loss: 2.1673e-06
Epoch 104/512
448/448 - 0s - loss: 3.2576e-07 - val_loss: 1.8429e-06
Epoch 105/512
448/448 - 0s - loss: 1.4843e-07 - val_loss: 1.9782e-06
Epoch 106/512
448/448 - 0s - loss: 8.4988e-08 - val_loss: 1.8222e-06
Epoch 107/512
448/448 - 0s - loss: 8.0112e-08 - val_loss: 1.3228e-06
Epoch 108/512
448/448 - 0s - loss: 2.0130e-07 - val_loss: 6.0537e-07
Epoch 109/512
448/448 - 0s - loss: 3.2605e-07 - val_loss: 1.4697e-06
Epoch 110/512
448/448 - 0s - loss: 4.4031e-06 - val_loss: 3.7567e-06
Epoch 111/512
448/448 - 0s - loss: 1.1321e-07 - val_loss: 1.9113e-06
Epoch 112/512
448/448 - 0s - loss: 7.0066e-08 - val_loss: 1.2593e-06
Epoch 113/512
448/448 - 0s - loss: 5.7790e-08 - val_loss: 1.0784e-06
Epoch 114/512
448/448 - 0s - loss: 4.9589e-08 - val_loss: 9.5942e-07
Epoch 115/512
448/448 - 0s - loss: 4.3104e-08 - val_loss: 8.8763e-07
Epoch 116/512
448/448 - 0s - loss: 3.7757e-08 - val_loss: 7.8005e-07
Epoch 117/512
448/448 - 0s - loss: 3.3268e-08 - val_loss: 6.7638e-07
Epoch 118/512
448/448 - 0s - loss: 2.9384e-08 - val_loss: 6.0276e-07
Epoch 119/512
448/448 - 0s - loss: 2.5979e-08 - val_loss: 5.8763e-07
Epoch 120/512
448/448 - 0s - loss: 2.3018e-08 - val_loss: 5.1351e-07
Epoch 121/512
448/448 - 0s - loss: 2.1490e-08 - val_loss: 3.2852e-07
Epoch 122/512
448/448 - 0s - loss: 1.4800e-07 - val_loss: 2.5475e-07
Epoch 123/512
448/448 - 0s - loss: 1.2403e-08 - val_loss: 3.1744e-07
Epoch 124/512
448/448 - 0s - loss: 1.1149e-08 - val_loss: 2.8334e-07
Epoch 125/512
448/448 - 0s - loss: 1.0311e-08 - val_loss: 2.5194e-07
Epoch 126/512
448/448 - 0s - loss: 9.5927e-09 - val_loss: 2.4915e-07
Epoch 127/512
448/448 - 0s - loss: 8.9971e-09 - val_loss: 2.1094e-07
Epoch 128/512
448/448 - 0s - loss: 2.0973e-08 - val_loss: 1.1513e-07
Epoch 129/512
448/448 - 0s - loss: 8.6222e-08 - val_loss: 5.4653e-07
Epoch 130/512
448/448 - 0s - loss: 2.8961e-08 - val_loss: 3.1490e-07
Epoch 131/512
448/448 - 0s - loss: 1.0724e-08 - val_loss: 2.4880e-07
Epoch 132/512
448/448 - 0s - loss: 9.2200e-09 - val_loss: 2.1931e-07
Epoch 133/512
448/448 - 0s - loss: 8.9658e-09 - val_loss: 1.8590e-07
Epoch 134/512
448/448 - 0s - loss: 1.0844e-08 - val_loss: 1.5924e-07
Epoch 135/512
448/448 - 0s - loss: 1.0535e-08 - val_loss: 1.2175e-07
Epoch 136/512
448/448 - 0s - loss: 9.4334e-09 - val_loss: 1.1984e-07
Epoch 137/512
448/448 - 0s - loss: 1.3546e-08 - val_loss: 1.0190e-07
Epoch 138/512
448/448 - 0s - loss: 1.2669e-08 - val_loss: 1.1640e-07
Epoch 139/512
448/448 - 0s - loss: 8.3850e-09 - val_loss: 1.0724e-07
Epoch 140/512
448/448 - 0s - loss: 9.4723e-09 - val_loss: 9.9269e-08
Epoch 141/512
448/448 - 0s - loss: 1.1406e-08 - val_loss: 9.3972e-08
Epoch 142/512
448/448 - 0s - loss: 9.1592e-09 - val_loss: 9.6754e-08
Epoch 143/512
448/448 - 0s - loss: 7.8870e-09 - val_loss: 9.3773e-08
Epoch 144/512
448/448 - 0s - loss: 8.7629e-09 - val_loss: 8.6431e-08
Epoch 145/512
448/448 - 0s - loss: 8.8672e-09 - val_loss: 8.7850e-08
Epoch 146/512
448/448 - 0s - loss: 7.5541e-09 - val_loss: 8.7802e-08
Epoch 147/512
448/448 - 0s - loss: 7.4808e-09 - val_loss: 8.2146e-08
Epoch 148/512
448/448 - 0s - loss: 7.7177e-09 - val_loss: 7.8721e-08
Epoch 149/512
448/448 - 0s - loss: 7.1266e-09 - val_loss: 7.9401e-08
Epoch 150/512
448/448 - 0s - loss: 6.7371e-09 - val_loss: 7.7195e-08
Epoch 151/512
448/448 - 0s - loss: 6.5603e-09 - val_loss: 7.1363e-08
Epoch 152/512
448/448 - 0s - loss: 6.7638e-09 - val_loss: 6.9689e-08
Epoch 153/512
448/448 - 0s - loss: 6.1243e-09 - val_loss: 7.3239e-08
Epoch 154/512
448/448 - 0s - loss: 6.0384e-09 - val_loss: 6.7915e-08
Epoch 155/512
448/448 - 0s - loss: 6.0230e-09 - val_loss: 6.7339e-08
Epoch 156/512
448/448 - 0s - loss: 5.6857e-09 - val_loss: 6.3611e-08
Epoch 157/512
448/448 - 0s - loss: 5.4877e-09 - val_loss: 6.4723e-08
Epoch 158/512
448/448 - 0s - loss: 5.3044e-09 - val_loss: 6.2765e-08
Epoch 159/512
448/448 - 0s - loss: 5.3461e-09 - val_loss: 6.1179e-08
Epoch 160/512
448/448 - 0s - loss: 5.1232e-09 - val_loss: 5.8580e-08
Epoch 161/512
448/448 - 0s - loss: 5.1192e-09 - val_loss: 5.7491e-08
Epoch 162/512
448/448 - 0s - loss: 4.7361e-09 - val_loss: 5.5669e-08
Epoch 163/512
448/448 - 0s - loss: 4.6999e-09 - val_loss: 5.3334e-08
Epoch 164/512
448/448 - 0s - loss: 4.8004e-09 - val_loss: 5.1912e-08
Epoch 165/512
448/448 - 0s - loss: 4.5520e-09 - val_loss: 5.1174e-08
Epoch 166/512
448/448 - 0s - loss: 4.2833e-09 - val_loss: 4.9802e-08
Epoch 167/512
448/448 - 0s - loss: 4.4207e-09 - val_loss: 4.8833e-08
Epoch 168/512
448/448 - 0s - loss: 4.2101e-09 - val_loss: 4.8544e-08
Epoch 169/512
448/448 - 0s - loss: 4.1352e-09 - val_loss: 4.7867e-08
Epoch 170/512
448/448 - 0s - loss: 4.0673e-09 - val_loss: 4.6858e-08
Epoch 171/512
448/448 - 0s - loss: 4.0021e-09 - val_loss: 4.4705e-08
Epoch 172/512
448/448 - 0s - loss: 3.8587e-09 - val_loss: 4.3833e-08
Epoch 173/512
448/448 - 0s - loss: 3.7753e-09 - val_loss: 4.2676e-08
Epoch 174/512
448/448 - 0s - loss: 3.7875e-09 - val_loss: 4.2391e-08
Epoch 175/512
448/448 - 0s - loss: 3.6866e-09 - val_loss: 4.1636e-08
Epoch 176/512
448/448 - 0s - loss: 3.4867e-09 - val_loss: 4.0469e-08
Epoch 177/512
448/448 - 0s - loss: 3.5916e-09 - val_loss: 4.0839e-08
Epoch 178/512
448/448 - 0s - loss: 3.5175e-09 - val_loss: 4.0387e-08
Epoch 179/512
448/448 - 0s - loss: 3.3014e-09 - val_loss: 3.9988e-08
Epoch 180/512
448/448 - 0s - loss: 3.2847e-09 - val_loss: 3.9444e-08
Epoch 181/512
448/448 - 0s - loss: 3.3076e-09 - val_loss: 3.7082e-08
Epoch 182/512
448/448 - 0s - loss: 3.3185e-09 - val_loss: 3.7459e-08
Epoch 183/512
448/448 - 0s - loss: 3.1030e-09 - val_loss: 3.6458e-08
Epoch 184/512
448/448 - 0s - loss: 3.1415e-09 - val_loss: 3.6760e-08
Epoch 185/512
448/448 - 0s - loss: 3.0410e-09 - val_loss: 3.5660e-08
Epoch 186/512
448/448 - 0s - loss: 3.0009e-09 - val_loss: 3.5913e-08
Epoch 187/512
448/448 - 0s - loss: 3.0096e-09 - val_loss: 3.4889e-08
Epoch 188/512
448/448 - 0s - loss: 2.9539e-09 - val_loss: 3.3356e-08
Epoch 189/512
448/448 - 0s - loss: 2.8888e-09 - val_loss: 3.3205e-08
Epoch 190/512
448/448 - 0s - loss: 2.8438e-09 - val_loss: 3.2849e-08
Epoch 191/512
448/448 - 0s - loss: 2.8012e-09 - val_loss: 3.2629e-08
Epoch 192/512
448/448 - 0s - loss: 2.7218e-09 - val_loss: 3.2086e-08
Epoch 193/512
448/448 - 0s - loss: 2.7210e-09 - val_loss: 3.1315e-08
Epoch 194/512
448/448 - 0s - loss: 2.6908e-09 - val_loss: 3.1280e-08
Epoch 195/512
448/448 - 0s - loss: 2.6189e-09 - val_loss: 3.1184e-08
Epoch 196/512
448/448 - 0s - loss: 2.5684e-09 - val_loss: 3.0471e-08
Epoch 197/512
448/448 - 0s - loss: 2.5910e-09 - val_loss: 2.9641e-08
Epoch 198/512
448/448 - 0s - loss: 2.5880e-09 - val_loss: 2.9746e-08
Epoch 199/512
448/448 - 0s - loss: 2.4433e-09 - val_loss: 2.9310e-08
Epoch 200/512
448/448 - 0s - loss: 2.4008e-09 - val_loss: 2.8759e-08
Epoch 201/512
448/448 - 0s - loss: 2.4566e-09 - val_loss: 2.8261e-08
Epoch 202/512
448/448 - 0s - loss: 2.4759e-09 - val_loss: 2.8356e-08
Epoch 203/512
448/448 - 0s - loss: 2.3823e-09 - val_loss: 2.7801e-08
Epoch 204/512
448/448 - 0s - loss: 2.2437e-09 - val_loss: 2.7416e-08
Epoch 205/512
448/448 - 0s - loss: 2.3679e-09 - val_loss: 2.6772e-08
Epoch 206/512
448/448 - 0s - loss: 2.3635e-09 - val_loss: 2.6826e-08
Epoch 207/512
448/448 - 0s - loss: 2.2021e-09 - val_loss: 2.6744e-08
Epoch 208/512
448/448 - 0s - loss: 2.2293e-09 - val_loss: 2.5915e-08
Epoch 209/512
448/448 - 0s - loss: 2.2621e-09 - val_loss: 2.6078e-08
Epoch 210/512
448/448 - 0s - loss: 2.1442e-09 - val_loss: 2.6215e-08
Epoch 211/512
448/448 - 0s - loss: 2.1273e-09 - val_loss: 2.5487e-08
Epoch 212/512
448/448 - 0s - loss: 2.1254e-09 - val_loss: 2.5377e-08
Epoch 213/512
448/448 - 0s - loss: 2.1364e-09 - val_loss: 2.5080e-08
Epoch 214/512
448/448 - 0s - loss: 2.0462e-09 - val_loss: 2.4669e-08
Epoch 215/512
448/448 - 0s - loss: 2.0426e-09 - val_loss: 2.4820e-08
Epoch 216/512
448/448 - 0s - loss: 2.0261e-09 - val_loss: 2.4410e-08
Epoch 217/512
448/448 - 0s - loss: 2.0573e-09 - val_loss: 2.3873e-08
Epoch 218/512
448/448 - 0s - loss: 2.0292e-09 - val_loss: 2.3955e-08
Epoch 219/512
448/448 - 0s - loss: 1.9337e-09 - val_loss: 2.3569e-08
Epoch 220/512
448/448 - 0s - loss: 1.9026e-09 - val_loss: 2.3209e-08
Epoch 221/512
448/448 - 0s - loss: 1.9928e-09 - val_loss: 2.3063e-08
Epoch 222/512
448/448 - 0s - loss: 1.9488e-09 - val_loss: 2.2758e-08
Epoch 223/512
448/448 - 0s - loss: 1.8618e-09 - val_loss: 2.2547e-08
Epoch 224/512
448/448 - 0s - loss: 1.8511e-09 - val_loss: 2.1983e-08
Epoch 225/512
448/448 - 0s - loss: 1.8478e-09 - val_loss: 2.1573e-08
Epoch 226/512
448/448 - 0s - loss: 1.8931e-09 - val_loss: 2.1852e-08
Epoch 227/512
448/448 - 0s - loss: 1.8017e-09 - val_loss: 2.1418e-08
Epoch 228/512
448/448 - 0s - loss: 1.8030e-09 - val_loss: 2.1272e-08
Epoch 229/512
448/448 - 0s - loss: 1.8133e-09 - val_loss: 2.0989e-08
Epoch 230/512
448/448 - 0s - loss: 1.7701e-09 - val_loss: 2.0695e-08
Epoch 231/512
448/448 - 0s - loss: 1.7494e-09 - val_loss: 2.0658e-08
Epoch 232/512
448/448 - 0s - loss: 1.7199e-09 - val_loss: 2.0751e-08
Epoch 233/512
448/448 - 0s - loss: 1.7354e-09 - val_loss: 2.0559e-08
Epoch 234/512
448/448 - 0s - loss: 1.7480e-09 - val_loss: 2.0489e-08
Epoch 235/512
448/448 - 0s - loss: 1.6863e-09 - val_loss: 2.0331e-08
Epoch 236/512
448/448 - 0s - loss: 1.6166e-09 - val_loss: 2.0098e-08
Epoch 237/512
448/448 - 0s - loss: 1.6825e-09 - val_loss: 1.9695e-08
Epoch 238/512
448/448 - 0s - loss: 1.7186e-09 - val_loss: 1.9915e-08
Epoch 239/512
448/448 - 0s - loss: 1.5998e-09 - val_loss: 2.0072e-08
Epoch 240/512
448/448 - 0s - loss: 1.5319e-09 - val_loss: 1.9808e-08
Epoch 241/512
448/448 - 0s - loss: 1.5742e-09 - val_loss: 1.9210e-08
Epoch 242/512
448/448 - 0s - loss: 1.6672e-09 - val_loss: 1.9105e-08
Epoch 243/512
448/448 - 0s - loss: 1.6251e-09 - val_loss: 1.9139e-08
Epoch 244/512
448/448 - 0s - loss: 1.5391e-09 - val_loss: 1.9205e-08
Epoch 245/512
448/448 - 0s - loss: 1.4846e-09 - val_loss: 1.8733e-08
Epoch 246/512
448/448 - 0s - loss: 1.5626e-09 - val_loss: 1.8361e-08
Epoch 247/512
448/448 - 0s - loss: 1.5991e-09 - val_loss: 1.8304e-08
Epoch 248/512
448/448 - 0s - loss: 1.5273e-09 - val_loss: 1.8352e-08
Epoch 249/512
448/448 - 0s - loss: 1.4389e-09 - val_loss: 1.8202e-08
Epoch 250/512
448/448 - 0s - loss: 1.4351e-09 - val_loss: 1.7700e-08
Epoch 251/512
448/448 - 0s - loss: 1.5343e-09 - val_loss: 1.7537e-08
Epoch 252/512
448/448 - 0s - loss: 1.5205e-09 - val_loss: 1.7425e-08
Epoch 253/512
448/448 - 0s - loss: 1.4391e-09 - val_loss: 1.7371e-08
Epoch 254/512
448/448 - 0s - loss: 1.4004e-09 - val_loss: 1.7239e-08
Epoch 255/512
448/448 - 0s - loss: 1.4001e-09 - val_loss: 1.7160e-08
Epoch 256/512
448/448 - 0s - loss: 1.4298e-09 - val_loss: 1.7016e-08
Epoch 257/512
448/448 - 0s - loss: 1.4488e-09 - val_loss: 1.7029e-08
Epoch 258/512
448/448 - 0s - loss: 1.4336e-09 - val_loss: 1.7088e-08
Epoch 259/512
448/448 - 0s - loss: 1.3697e-09 - val_loss: 1.6918e-08
Epoch 260/512
448/448 - 0s - loss: 1.3459e-09 - val_loss: 1.6740e-08
Epoch 261/512
448/448 - 0s - loss: 1.3732e-09 - val_loss: 1.6345e-08
Epoch 262/512
448/448 - 0s - loss: 1.3894e-09 - val_loss: 1.6325e-08
Epoch 263/512
448/448 - 0s - loss: 1.3548e-09 - val_loss: 1.6283e-08
Epoch 264/512
448/448 - 0s - loss: 1.3297e-09 - val_loss: 1.6265e-08
Epoch 265/512
448/448 - 0s - loss: 1.3165e-09 - val_loss: 1.6069e-08
Epoch 266/512
448/448 - 0s - loss: 1.3441e-09 - val_loss: 1.5941e-08
Epoch 267/512
448/448 - 0s - loss: 1.3383e-09 - val_loss: 1.5820e-08
Epoch 268/512
448/448 - 0s - loss: 1.3060e-09 - val_loss: 1.5784e-08
Epoch 269/512
448/448 - 0s - loss: 1.2694e-09 - val_loss: 1.5619e-08
Epoch 270/512
448/448 - 0s - loss: 1.2714e-09 - val_loss: 1.5459e-08
Epoch 271/512
448/448 - 0s - loss: 1.2975e-09 - val_loss: 1.5335e-08
Epoch 272/512
448/448 - 0s - loss: 1.3004e-09 - val_loss: 1.5409e-08
Epoch 273/512
448/448 - 0s - loss: 1.2518e-09 - val_loss: 1.5297e-08
Epoch 274/512
448/448 - 0s - loss: 1.2464e-09 - val_loss: 1.5155e-08
Epoch 275/512
448/448 - 0s - loss: 1.2633e-09 - val_loss: 1.5075e-08
Epoch 276/512
448/448 - 0s - loss: 1.2594e-09 - val_loss: 1.4988e-08
Epoch 277/512
448/448 - 0s - loss: 1.2219e-09 - val_loss: 1.4979e-08
Epoch 278/512
448/448 - 0s - loss: 1.1888e-09 - val_loss: 1.4920e-08
Epoch 279/512
448/448 - 0s - loss: 1.2030e-09 - val_loss: 1.4665e-08
Epoch 280/512
448/448 - 0s - loss: 1.2353e-09 - val_loss: 1.4563e-08
Epoch 281/512
448/448 - 0s - loss: 1.2318e-09 - val_loss: 1.4469e-08
Epoch 282/512
448/448 - 0s - loss: 1.2217e-09 - val_loss: 1.4457e-08
Epoch 283/512
448/448 - 0s - loss: 1.1637e-09 - val_loss: 1.4536e-08
Epoch 284/512
448/448 - 0s - loss: 1.1244e-09 - val_loss: 1.4186e-08
Epoch 285/512
448/448 - 0s - loss: 1.1849e-09 - val_loss: 1.4041e-08
Epoch 286/512
448/448 - 0s - loss: 1.1875e-09 - val_loss: 1.3962e-08
Epoch 287/512
448/448 - 0s - loss: 1.1799e-09 - val_loss: 1.4017e-08
Epoch 288/512
448/448 - 0s - loss: 1.1506e-09 - val_loss: 1.3944e-08
Epoch 289/512
448/448 - 0s - loss: 1.1432e-09 - val_loss: 1.3892e-08
Epoch 290/512
448/448 - 0s - loss: 1.1443e-09 - val_loss: 1.3753e-08
Epoch 291/512
448/448 - 0s - loss: 1.1215e-09 - val_loss: 1.3718e-08
Epoch 292/512
448/448 - 0s - loss: 1.1081e-09 - val_loss: 1.3618e-08
Epoch 293/512
448/448 - 0s - loss: 1.1224e-09 - val_loss: 1.3525e-08
Epoch 294/512
448/448 - 0s - loss: 1.1381e-09 - val_loss: 1.3461e-08
Epoch 295/512
448/448 - 0s - loss: 1.1248e-09 - val_loss: 1.3525e-08
Epoch 296/512
448/448 - 0s - loss: 1.0725e-09 - val_loss: 1.3386e-08
Epoch 297/512
448/448 - 0s - loss: 1.0718e-09 - val_loss: 1.3241e-08
Epoch 298/512
448/448 - 0s - loss: 1.1005e-09 - val_loss: 1.3161e-08
Epoch 299/512
448/448 - 0s - loss: 1.0734e-09 - val_loss: 1.3166e-08
Epoch 300/512
448/448 - 0s - loss: 1.0892e-09 - val_loss: 1.3056e-08
Epoch 301/512
448/448 - 0s - loss: 1.0862e-09 - val_loss: 1.2977e-08
Epoch 302/512
448/448 - 0s - loss: 1.0494e-09 - val_loss: 1.3020e-08
Epoch 303/512
448/448 - 0s - loss: 1.0300e-09 - val_loss: 1.2883e-08
Epoch 304/512
448/448 - 0s - loss: 1.0715e-09 - val_loss: 1.2701e-08
Epoch 305/512
448/448 - 0s - loss: 1.0812e-09 - val_loss: 1.2703e-08
Epoch 306/512
448/448 - 0s - loss: 1.0359e-09 - val_loss: 1.2737e-08
Epoch 307/512
448/448 - 0s - loss: 1.0090e-09 - val_loss: 1.2693e-08
Epoch 308/512
448/448 - 0s - loss: 1.0183e-09 - val_loss: 1.2550e-08
Epoch 309/512
448/448 - 0s - loss: 1.0321e-09 - val_loss: 1.2476e-08
Epoch 310/512
448/448 - 0s - loss: 1.0150e-09 - val_loss: 1.2374e-08
Epoch 311/512
448/448 - 0s - loss: 1.0174e-09 - val_loss: 1.2270e-08
Epoch 312/512
448/448 - 0s - loss: 1.0047e-09 - val_loss: 1.2195e-08
Epoch 313/512
448/448 - 0s - loss: 9.8998e-10 - val_loss: 1.2159e-08
Epoch 314/512
448/448 - 0s - loss: 9.8416e-10 - val_loss: 1.2002e-08
Epoch 315/512
448/448 - 0s - loss: 1.0012e-09 - val_loss: 1.1967e-08
Epoch 316/512
448/448 - 0s - loss: 1.0046e-09 - val_loss: 1.1877e-08
Epoch 317/512
448/448 - 0s - loss: 1.0078e-09 - val_loss: 1.1897e-08
Epoch 318/512
448/448 - 0s - loss: 9.7854e-10 - val_loss: 1.1857e-08
Epoch 319/512
448/448 - 0s - loss: 9.4858e-10 - val_loss: 1.1898e-08
Epoch 320/512
448/448 - 0s - loss: 9.4771e-10 - val_loss: 1.1865e-08
Epoch 321/512
448/448 - 0s - loss: 9.6601e-10 - val_loss: 1.1722e-08
Epoch 322/512
448/448 - 0s - loss: 9.8423e-10 - val_loss: 1.1638e-08
Epoch 323/512
448/448 - 0s - loss: 9.5806e-10 - val_loss: 1.1667e-08
Epoch 324/512
448/448 - 0s - loss: 9.2801e-10 - val_loss: 1.1517e-08
Epoch 325/512
448/448 - 0s - loss: 9.4658e-10 - val_loss: 1.1423e-08
Epoch 326/512
448/448 - 0s - loss: 9.4527e-10 - val_loss: 1.1395e-08
Epoch 327/512
448/448 - 0s - loss: 9.2951e-10 - val_loss: 1.1341e-08
Epoch 328/512
448/448 - 0s - loss: 9.4689e-10 - val_loss: 1.1310e-08
Epoch 329/512
448/448 - 0s - loss: 9.1837e-10 - val_loss: 1.1282e-08
Epoch 330/512
448/448 - 0s - loss: 9.0059e-10 - val_loss: 1.1313e-08
Epoch 331/512
448/448 - 0s - loss: 9.0370e-10 - val_loss: 1.1120e-08
Epoch 332/512
448/448 - 0s - loss: 9.4182e-10 - val_loss: 1.0989e-08
Epoch 333/512
448/448 - 0s - loss: 9.2682e-10 - val_loss: 1.1102e-08
Epoch 334/512
448/448 - 0s - loss: 8.8457e-10 - val_loss: 1.1078e-08
Epoch 335/512
448/448 - 0s - loss: 8.8469e-10 - val_loss: 1.0945e-08
Epoch 336/512
448/448 - 0s - loss: 9.0440e-10 - val_loss: 1.0892e-08
Epoch 337/512
448/448 - 0s - loss: 8.9598e-10 - val_loss: 1.0765e-08
Epoch 338/512
448/448 - 0s - loss: 9.0761e-10 - val_loss: 1.0728e-08
Epoch 339/512
448/448 - 0s - loss: 8.9282e-10 - val_loss: 1.0761e-08
Epoch 340/512
448/448 - 0s - loss: 8.6655e-10 - val_loss: 1.0795e-08
Epoch 341/512
448/448 - 0s - loss: 8.4710e-10 - val_loss: 1.0662e-08
Epoch 342/512
448/448 - 0s - loss: 8.6953e-10 - val_loss: 1.0462e-08
Epoch 343/512
448/448 - 0s - loss: 9.0893e-10 - val_loss: 1.0431e-08
Epoch 344/512
448/448 - 0s - loss: 9.0861e-10 - val_loss: 1.0431e-08
Epoch 345/512
448/448 - 0s - loss: 8.5519e-10 - val_loss: 1.0500e-08
Epoch 346/512
448/448 - 0s - loss: 8.3018e-10 - val_loss: 1.0435e-08
Epoch 347/512
448/448 - 0s - loss: 8.2256e-10 - val_loss: 1.0409e-08
Epoch 348/512
448/448 - 0s - loss: 8.3580e-10 - val_loss: 1.0299e-08
Epoch 349/512
448/448 - 0s - loss: 8.6789e-10 - val_loss: 1.0155e-08
Epoch 350/512
448/448 - 0s - loss: 8.7441e-10 - val_loss: 1.0216e-08
Epoch 351/512
448/448 - 0s - loss: 8.4662e-10 - val_loss: 1.0232e-08
Epoch 352/512
448/448 - 0s - loss: 8.1911e-10 - val_loss: 1.0182e-08
Epoch 353/512
448/448 - 0s - loss: 8.0483e-10 - val_loss: 1.0185e-08
Epoch 354/512
448/448 - 0s - loss: 8.1679e-10 - val_loss: 1.0071e-08
Epoch 355/512
448/448 - 0s - loss: 8.3382e-10 - val_loss: 1.0008e-08
Epoch 356/512
448/448 - 0s - loss: 8.2794e-10 - val_loss: 9.9641e-09
Epoch 357/512
448/448 - 0s - loss: 8.2455e-10 - val_loss: 9.8966e-09
Epoch 358/512
448/448 - 0s - loss: 8.2506e-10 - val_loss: 9.8646e-09
Epoch 359/512
448/448 - 0s - loss: 8.1112e-10 - val_loss: 9.8472e-09
Epoch 360/512
448/448 - 0s - loss: 8.0704e-10 - val_loss: 9.8335e-09
Epoch 361/512
448/448 - 0s - loss: 7.9670e-10 - val_loss: 9.8235e-09
Epoch 362/512
448/448 - 0s - loss: 7.9504e-10 - val_loss: 9.7509e-09
Epoch 363/512
448/448 - 0s - loss: 8.1500e-10 - val_loss: 9.6743e-09
Epoch 364/512
448/448 - 0s - loss: 8.0274e-10 - val_loss: 9.7127e-09
Epoch 365/512
448/448 - 0s - loss: 7.8599e-10 - val_loss: 9.5974e-09
Epoch 366/512
448/448 - 0s - loss: 7.9458e-10 - val_loss: 9.5671e-09
Epoch 367/512
448/448 - 0s - loss: 7.8053e-10 - val_loss: 9.5349e-09
Epoch 368/512
448/448 - 0s - loss: 7.7846e-10 - val_loss: 9.4494e-09
Epoch 369/512
448/448 - 0s - loss: 7.8534e-10 - val_loss: 9.3961e-09
Epoch 370/512
448/448 - 0s - loss: 7.6943e-10 - val_loss: 9.3638e-09
Epoch 371/512
448/448 - 0s - loss: 7.7014e-10 - val_loss: 9.2930e-09
Epoch 372/512
448/448 - 0s - loss: 7.7730e-10 - val_loss: 9.2926e-09
Epoch 373/512
448/448 - 0s - loss: 7.5894e-10 - val_loss: 9.3130e-09
Epoch 374/512
448/448 - 0s - loss: 7.4556e-10 - val_loss: 9.2722e-09
Epoch 375/512
448/448 - 0s - loss: 7.6351e-10 - val_loss: 9.1713e-09
Epoch 376/512
448/448 - 0s - loss: 7.6191e-10 - val_loss: 9.1176e-09
Epoch 377/512
448/448 - 0s - loss: 7.6425e-10 - val_loss: 9.0692e-09
Epoch 378/512
448/448 - 0s - loss: 7.8051e-10 - val_loss: 9.0481e-09
Epoch 379/512
448/448 - 0s - loss: 7.6648e-10 - val_loss: 9.1136e-09
Epoch 380/512
448/448 - 0s - loss: 7.2748e-10 - val_loss: 9.1249e-09
Epoch 381/512
448/448 - 0s - loss: 7.2444e-10 - val_loss: 9.0135e-09
Epoch 382/512
448/448 - 0s - loss: 7.4996e-10 - val_loss: 8.9606e-09
Epoch 383/512
448/448 - 0s - loss: 7.4364e-10 - val_loss: 8.9597e-09
Epoch 384/512
448/448 - 0s - loss: 7.4398e-10 - val_loss: 8.8172e-09
Epoch 385/512
448/448 - 0s - loss: 7.6628e-10 - val_loss: 8.8046e-09
Epoch 386/512
448/448 - 0s - loss: 7.3666e-10 - val_loss: 8.7986e-09
Epoch 387/512
448/448 - 0s - loss: 7.1903e-10 - val_loss: 8.7911e-09
Epoch 388/512
448/448 - 0s - loss: 7.0925e-10 - val_loss: 8.7516e-09
Epoch 389/512
448/448 - 0s - loss: 7.1117e-10 - val_loss: 8.7618e-09
Epoch 390/512
448/448 - 0s - loss: 7.2595e-10 - val_loss: 8.6580e-09
Epoch 391/512
448/448 - 0s - loss: 7.3949e-10 - val_loss: 8.6508e-09
Epoch 392/512
448/448 - 0s - loss: 7.1685e-10 - val_loss: 8.6942e-09
Epoch 393/512
448/448 - 0s - loss: 7.0205e-10 - val_loss: 8.6552e-09
Epoch 394/512
448/448 - 0s - loss: 7.0605e-10 - val_loss: 8.6217e-09
Epoch 395/512
448/448 - 0s - loss: 7.0402e-10 - val_loss: 8.5479e-09
Epoch 396/512
448/448 - 0s - loss: 7.1967e-10 - val_loss: 8.5086e-09
Epoch 397/512
448/448 - 0s - loss: 7.3023e-10 - val_loss: 8.4804e-09
Epoch 398/512
448/448 - 0s - loss: 7.0970e-10 - val_loss: 8.5122e-09
Epoch 399/512
448/448 - 0s - loss: 6.8381e-10 - val_loss: 8.4844e-09
Epoch 400/512
448/448 - 0s - loss: 6.7646e-10 - val_loss: 8.4593e-09
Epoch 401/512
448/448 - 0s - loss: 6.8524e-10 - val_loss: 8.3236e-09
Epoch 402/512
448/448 - 0s - loss: 7.0763e-10 - val_loss: 8.3063e-09
Epoch 403/512
448/448 - 0s - loss: 7.0436e-10 - val_loss: 8.2750e-09
Epoch 404/512
448/448 - 0s - loss: 6.8806e-10 - val_loss: 8.2746e-09
Epoch 405/512
448/448 - 0s - loss: 6.8532e-10 - val_loss: 8.1885e-09
Epoch 406/512
448/448 - 0s - loss: 6.9362e-10 - val_loss: 8.1358e-09
Epoch 407/512
448/448 - 0s - loss: 6.8809e-10 - val_loss: 8.1626e-09
Epoch 408/512
448/448 - 0s - loss: 6.7299e-10 - val_loss: 8.1664e-09
Epoch 409/512
448/448 - 0s - loss: 6.6834e-10 - val_loss: 8.1350e-09
Epoch 410/512
448/448 - 0s - loss: 6.6642e-10 - val_loss: 8.0701e-09
Epoch 411/512
448/448 - 0s - loss: 6.6712e-10 - val_loss: 8.0733e-09
Epoch 412/512
448/448 - 0s - loss: 6.7420e-10 - val_loss: 7.9773e-09
Epoch 413/512
448/448 - 0s - loss: 6.9392e-10 - val_loss: 7.9292e-09
Epoch 414/512
448/448 - 0s - loss: 6.8361e-10 - val_loss: 7.9756e-09
Epoch 415/512
448/448 - 0s - loss: 6.5483e-10 - val_loss: 7.9566e-09
Epoch 416/512
448/448 - 0s - loss: 6.5361e-10 - val_loss: 7.8972e-09
Epoch 417/512
448/448 - 0s - loss: 6.5606e-10 - val_loss: 7.8501e-09
Epoch 418/512
448/448 - 0s - loss: 6.5118e-10 - val_loss: 7.8122e-09
Epoch 419/512
448/448 - 0s - loss: 6.5854e-10 - val_loss: 7.7569e-09
Epoch 420/512
448/448 - 0s - loss: 6.6484e-10 - val_loss: 7.7590e-09
Epoch 421/512
448/448 - 0s - loss: 6.5410e-10 - val_loss: 7.7571e-09
Epoch 422/512
448/448 - 0s - loss: 6.4652e-10 - val_loss: 7.7732e-09
Epoch 423/512
448/448 - 0s - loss: 6.3588e-10 - val_loss: 7.7578e-09
Epoch 424/512
448/448 - 0s - loss: 6.3039e-10 - val_loss: 7.6932e-09
Epoch 425/512
448/448 - 0s - loss: 6.4287e-10 - val_loss: 7.6647e-09
Epoch 426/512
448/448 - 0s - loss: 6.4219e-10 - val_loss: 7.6067e-09
Epoch 427/512
448/448 - 0s - loss: 6.5462e-10 - val_loss: 7.5913e-09
Epoch 428/512
448/448 - 0s - loss: 6.4675e-10 - val_loss: 7.5840e-09
Epoch 429/512
448/448 - 0s - loss: 6.3539e-10 - val_loss: 7.6056e-09
Epoch 430/512
448/448 - 0s - loss: 6.2326e-10 - val_loss: 7.6102e-09
Epoch 431/512
448/448 - 0s - loss: 6.1211e-10 - val_loss: 7.5491e-09
Epoch 432/512
448/448 - 0s - loss: 6.3910e-10 - val_loss: 7.4926e-09
Epoch 433/512
448/448 - 0s - loss: 6.4133e-10 - val_loss: 7.4965e-09
Epoch 434/512
448/448 - 0s - loss: 6.1745e-10 - val_loss: 7.5010e-09
Epoch 435/512
448/448 - 0s - loss: 6.0533e-10 - val_loss: 7.4753e-09
Epoch 436/512
448/448 - 0s - loss: 6.0414e-10 - val_loss: 7.4769e-09
Epoch 437/512
448/448 - 0s - loss: 6.0250e-10 - val_loss: 7.3753e-09
Epoch 438/512
448/448 - 0s - loss: 6.2961e-10 - val_loss: 7.3240e-09
Epoch 439/512
448/448 - 0s - loss: 6.4379e-10 - val_loss: 7.3241e-09
Epoch 440/512
448/448 - 0s - loss: 6.1611e-10 - val_loss: 7.3533e-09
Epoch 441/512
448/448 - 0s - loss: 5.8441e-10 - val_loss: 7.3373e-09
Epoch 442/512
448/448 - 0s - loss: 5.8519e-10 - val_loss: 7.2931e-09
Epoch 443/512
448/448 - 0s - loss: 6.0277e-10 - val_loss: 7.2415e-09
Epoch 444/512
448/448 - 0s - loss: 6.1600e-10 - val_loss: 7.2426e-09
Epoch 445/512
448/448 - 0s - loss: 6.0130e-10 - val_loss: 7.2002e-09
Epoch 446/512
448/448 - 0s - loss: 6.1179e-10 - val_loss: 7.1901e-09
Epoch 447/512
448/448 - 0s - loss: 6.0387e-10 - val_loss: 7.1846e-09
Epoch 448/512
448/448 - 0s - loss: 5.9367e-10 - val_loss: 7.2082e-09
Epoch 449/512
448/448 - 0s - loss: 5.7857e-10 - val_loss: 7.1602e-09
Epoch 450/512
448/448 - 0s - loss: 5.9328e-10 - val_loss: 7.0957e-09
Epoch 451/512
448/448 - 0s - loss: 6.0641e-10 - val_loss: 7.0638e-09
Epoch 452/512
448/448 - 0s - loss: 6.0356e-10 - val_loss: 7.0705e-09
Epoch 453/512
448/448 - 0s - loss: 5.8828e-10 - val_loss: 7.0654e-09
Epoch 454/512
448/448 - 0s - loss: 5.7500e-10 - val_loss: 7.0593e-09
Epoch 455/512
448/448 - 0s - loss: 5.7265e-10 - val_loss: 6.9875e-09
Epoch 456/512
448/448 - 0s - loss: 5.7987e-10 - val_loss: 6.9032e-09
Epoch 457/512
448/448 - 0s - loss: 5.9056e-10 - val_loss: 6.8792e-09
Epoch 458/512
448/448 - 0s - loss: 5.8312e-10 - val_loss: 6.8388e-09
Epoch 459/512
448/448 - 0s - loss: 5.8825e-10 - val_loss: 6.8344e-09
Epoch 460/512
448/448 - 0s - loss: 5.7371e-10 - val_loss: 6.8682e-09
Epoch 461/512
448/448 - 0s - loss: 5.5610e-10 - val_loss: 6.8402e-09
Epoch 462/512
448/448 - 0s - loss: 5.7013e-10 - val_loss: 6.7521e-09
Epoch 463/512
448/448 - 0s - loss: 5.7667e-10 - val_loss: 6.7419e-09
Epoch 464/512
448/448 - 0s - loss: 5.7008e-10 - val_loss: 6.6924e-09
Epoch 465/512
448/448 - 0s - loss: 5.8576e-10 - val_loss: 6.6583e-09
Epoch 466/512
448/448 - 0s - loss: 5.9337e-10 - val_loss: 6.6653e-09
Epoch 467/512
448/448 - 0s - loss: 5.6854e-10 - val_loss: 6.6945e-09
Epoch 468/512
448/448 - 0s - loss: 5.4285e-10 - val_loss: 6.7096e-09
Epoch 469/512
448/448 - 0s - loss: 5.2993e-10 - val_loss: 6.7082e-09
Epoch 470/512
448/448 - 0s - loss: 5.3515e-10 - val_loss: 6.6283e-09
Epoch 471/512
448/448 - 0s - loss: 5.6651e-10 - val_loss: 6.5402e-09
Epoch 472/512
448/448 - 0s - loss: 5.8518e-10 - val_loss: 6.5199e-09
Epoch 473/512
448/448 - 0s - loss: 5.7285e-10 - val_loss: 6.5394e-09
Epoch 474/512
448/448 - 0s - loss: 5.6134e-10 - val_loss: 6.5491e-09
Epoch 475/512
448/448 - 0s - loss: 5.4745e-10 - val_loss: 6.5325e-09
Epoch 476/512
448/448 - 0s - loss: 5.4649e-10 - val_loss: 6.5101e-09
Epoch 477/512
448/448 - 0s - loss: 5.5410e-10 - val_loss: 6.5017e-09
Epoch 478/512
448/448 - 0s - loss: 5.5336e-10 - val_loss: 6.4614e-09
Epoch 479/512
448/448 - 0s - loss: 5.4720e-10 - val_loss: 6.4811e-09
Epoch 480/512
448/448 - 0s - loss: 5.4192e-10 - val_loss: 6.4344e-09
Epoch 481/512
448/448 - 0s - loss: 5.5163e-10 - val_loss: 6.4045e-09
Epoch 482/512
448/448 - 0s - loss: 5.4952e-10 - val_loss: 6.3875e-09
Epoch 483/512
448/448 - 0s - loss: 5.4934e-10 - val_loss: 6.3791e-09
Epoch 484/512
448/448 - 0s - loss: 5.3961e-10 - val_loss: 6.3855e-09
Epoch 485/512
448/448 - 0s - loss: 5.2341e-10 - val_loss: 6.3890e-09
Epoch 486/512
448/448 - 0s - loss: 5.2675e-10 - val_loss: 6.3297e-09
Epoch 487/512
448/448 - 0s - loss: 5.4202e-10 - val_loss: 6.3076e-09
Epoch 488/512
448/448 - 0s - loss: 5.5006e-10 - val_loss: 6.2984e-09
Epoch 489/512
448/448 - 0s - loss: 5.3435e-10 - val_loss: 6.3105e-09
Epoch 490/512
448/448 - 0s - loss: 5.2812e-10 - val_loss: 6.3294e-09
Epoch 491/512
448/448 - 0s - loss: 5.1473e-10 - val_loss: 6.3075e-09
Epoch 492/512
448/448 - 0s - loss: 5.1469e-10 - val_loss: 6.2706e-09
Epoch 493/512
448/448 - 0s - loss: 5.3024e-10 - val_loss: 6.2268e-09
Epoch 494/512
448/448 - 0s - loss: 5.3872e-10 - val_loss: 6.1907e-09
Epoch 495/512
448/448 - 0s - loss: 5.4376e-10 - val_loss: 6.1812e-09
Epoch 496/512
448/448 - 0s - loss: 5.2009e-10 - val_loss: 6.2045e-09
Epoch 497/512
448/448 - 0s - loss: 5.1327e-10 - val_loss: 6.1665e-09
Epoch 498/512
448/448 - 0s - loss: 5.1601e-10 - val_loss: 6.1355e-09
Epoch 499/512
448/448 - 0s - loss: 5.1794e-10 - val_loss: 6.1549e-09
Epoch 500/512
448/448 - 0s - loss: 5.0925e-10 - val_loss: 6.1342e-09
Epoch 501/512
448/448 - 0s - loss: 5.1347e-10 - val_loss: 6.1093e-09
Epoch 502/512
448/448 - 0s - loss: 5.2314e-10 - val_loss: 6.0819e-09
Epoch 503/512
448/448 - 0s - loss: 5.2208e-10 - val_loss: 6.0558e-09
Epoch 504/512
448/448 - 0s - loss: 5.2574e-10 - val_loss: 6.0553e-09
Epoch 505/512
448/448 - 0s - loss: 5.1640e-10 - val_loss: 6.0655e-09
Epoch 506/512
448/448 - 0s - loss: 5.1493e-10 - val_loss: 6.0283e-09
Epoch 507/512
448/448 - 0s - loss: 5.1332e-10 - val_loss: 6.0361e-09
Epoch 508/512
448/448 - 0s - loss: 5.0144e-10 - val_loss: 6.0385e-09
Epoch 509/512
448/448 - 0s - loss: 4.9197e-10 - val_loss: 6.0444e-09
Epoch 510/512
448/448 - 0s - loss: 4.8689e-10 - val_loss: 6.0114e-09
Epoch 511/512
448/448 - 0s - loss: 4.9509e-10 - val_loss: 5.9771e-09
Epoch 512/512
448/448 - 0s - loss: 5.0448e-10 - val_loss: 5.9399e-09
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6000e-10 - val_loss: 4.5993e-11
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.4724e-11 - val_loss: 4.2100e-11
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 4.1163e-11 - val_loss: 3.9015e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.8221e-11 - val_loss: 3.6361e-11
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.5654e-11 - val_loss: 3.4050e-11
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.3439e-11 - val_loss: 3.2016e-11
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 3.1479e-11 - val_loss: 3.0171e-11
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.9689e-11 - val_loss: 2.8516e-11
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.8106e-11 - val_loss: 2.7100e-11
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.6738e-11 - val_loss: 2.5840e-11
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.5506e-11 - val_loss: 2.4848e-11
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4741e-11 - val_loss: 2.4725e-11
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5158e-11 - val_loss: 2.7512e-11
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0772e-11 - val_loss: 4.5422e-11
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1742e-11 - val_loss: 1.1241e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3148e-10 - val_loss: 1.5849e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3762e-10 - val_loss: 9.9144e-11
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0818e-11 - val_loss: 5.6865e-11
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0648e-11 - val_loss: 4.2360e-11
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0612e-11 - val_loss: 3.9838e-11
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0147e-11 - val_loss: 4.4091e-11
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7749e-11 - val_loss: 5.8615e-11
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4081e-11 - val_loss: 7.8993e-11
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2730e-11 - val_loss: 9.0187e-11
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7251e-11 - val_loss: 8.3219e-11
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7414e-11 - val_loss: 6.9487e-11
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4296e-11 - val_loss: 5.9955e-11
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7082e-11 - val_loss: 5.5606e-11
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4071e-11 - val_loss: 5.5919e-11
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6081e-11 - val_loss: 6.0222e-11
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1411e-11 - val_loss: 6.5873e-11
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6122e-11 - val_loss: 6.9244e-11
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8528e-11 - val_loss: 6.8359e-11
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6192e-11 - val_loss: 6.5915e-11
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3777e-11 - val_loss: 6.1737e-11
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9724e-11 - val_loss: 5.8454e-11
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7282e-11 - val_loss: 5.9907e-11
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8635e-11 - val_loss: 6.0288e-11
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9056e-11 - val_loss: 5.9583e-11
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8591e-11 - val_loss: 6.0582e-11
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0145e-11 - val_loss: 6.1537e-11
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0540e-11 - val_loss: 6.1861e-11
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0558e-11 - val_loss: 6.0605e-11
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9010e-11 - val_loss: 5.9039e-11
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7662e-11 - val_loss: 5.8110e-11
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7125e-11 - val_loss: 5.7103e-11
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5887e-11 - val_loss: 5.7440e-11
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7084e-11 - val_loss: 5.8958e-11
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7189e-11 - val_loss: 5.7760e-11
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6613e-11 - val_loss: 5.7669e-11
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6687e-11 - val_loss: 5.7261e-11
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6558e-11 - val_loss: 5.7458e-11
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6198e-11 - val_loss: 5.6449e-11
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5037e-11 - val_loss: 5.4788e-11
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3828e-11 - val_loss: 5.3471e-11
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2543e-11 - val_loss: 5.2673e-11
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2110e-11 - val_loss: 5.3166e-11
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2430e-11 - val_loss: 5.3784e-11
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2914e-11 - val_loss: 5.4159e-11
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3770e-11 - val_loss: 5.4935e-11
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4375e-11 - val_loss: 5.5173e-11
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3927e-11 - val_loss: 5.4786e-11
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3564e-11 - val_loss: 5.3299e-11
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2575e-11 - val_loss: 5.2353e-11
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0933e-11 - val_loss: 5.0451e-11
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9822e-11 - val_loss: 4.9920e-11
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9459e-11 - val_loss: 5.0503e-11
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9926e-11 - val_loss: 5.1145e-11
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0622e-11 - val_loss: 5.1019e-11
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0389e-11 - val_loss: 5.1373e-11
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0623e-11 - val_loss: 5.1801e-11
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0313e-11 - val_loss: 4.9842e-11
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8473e-11 - val_loss: 4.8460e-11
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7662e-11 - val_loss: 4.7857e-11
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7045e-11 - val_loss: 4.7486e-11
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6637e-11 - val_loss: 4.6534e-11
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6304e-11 - val_loss: 4.7459e-11
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7591e-11 - val_loss: 4.9849e-11
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8938e-11 - val_loss: 4.9293e-11
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8329e-11 - val_loss: 4.8471e-11
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7351e-11 - val_loss: 4.6448e-11
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5708e-11 - val_loss: 4.5610e-11
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4813e-11 - val_loss: 4.4905e-11
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4200e-11 - val_loss: 4.4998e-11
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4490e-11 - val_loss: 4.5493e-11
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5290e-11 - val_loss: 4.6081e-11
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4954e-11 - val_loss: 4.5145e-11
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4468e-11 - val_loss: 4.5287e-11
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4481e-11 - val_loss: 4.4395e-11
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3544e-11 - val_loss: 4.3912e-11
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3471e-11 - val_loss: 4.4717e-11
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4096e-11 - val_loss: 4.4614e-11
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3918e-11 - val_loss: 4.4518e-11
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3502e-11 - val_loss: 4.3764e-11
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3613e-11 - val_loss: 4.4624e-11
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4088e-11 - val_loss: 4.4517e-11
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3333e-11 - val_loss: 4.2810e-11
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1990e-11 - val_loss: 4.3052e-11
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2599e-11 - val_loss: 4.2543e-11
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1744e-11 - val_loss: 4.1833e-11
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0821e-11 - val_loss: 4.1268e-11
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0554e-11 - val_loss: 4.0512e-11
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0588e-11 - val_loss: 4.1593e-11
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1344e-11 - val_loss: 4.2200e-11
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1943e-11 - val_loss: 4.2261e-11
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1489e-11 - val_loss: 4.1951e-11
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0764e-11 - val_loss: 4.0206e-11
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9167e-11 - val_loss: 3.9514e-11
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9250e-11 - val_loss: 4.0400e-11
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9222e-11 - val_loss: 4.0107e-11
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9759e-11 - val_loss: 4.0569e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9701e-11 - val_loss: 3.9901e-11
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9370e-11 - val_loss: 3.9405e-11
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8323e-11 - val_loss: 3.8412e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7779e-11 - val_loss: 3.8404e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7773e-11 - val_loss: 3.7892e-11
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7621e-11 - val_loss: 3.8908e-11
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8302e-11 - val_loss: 3.8999e-11
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8462e-11 - val_loss: 3.8182e-11
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7314e-11 - val_loss: 3.7961e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7530e-11 - val_loss: 3.8263e-11
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7805e-11 - val_loss: 3.7688e-11
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7499e-11 - val_loss: 3.8155e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7909e-11 - val_loss: 3.7643e-11
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6662e-11 - val_loss: 3.5911e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5871e-11 - val_loss: 3.6366e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5736e-11 - val_loss: 3.5879e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5309e-11 - val_loss: 3.5674e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4977e-11 - val_loss: 3.5655e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5584e-11 - val_loss: 3.6285e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5724e-11 - val_loss: 3.7031e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6669e-11 - val_loss: 3.8157e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7626e-11 - val_loss: 3.7858e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7221e-11 - val_loss: 3.6670e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5944e-11 - val_loss: 3.5924e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4710e-11 - val_loss: 3.3899e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2871e-11 - val_loss: 3.2849e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2505e-11 - val_loss: 3.2298e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2131e-11 - val_loss: 3.3270e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3552e-11 - val_loss: 3.4776e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4470e-11 - val_loss: 3.5416e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5055e-11 - val_loss: 3.4910e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4145e-11 - val_loss: 3.3786e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2989e-11 - val_loss: 3.2778e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2189e-11 - val_loss: 3.2231e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1931e-11 - val_loss: 3.2185e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1688e-11 - val_loss: 3.2727e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2066e-11 - val_loss: 3.2774e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2329e-11 - val_loss: 3.2640e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2552e-11 - val_loss: 3.2719e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2482e-11 - val_loss: 3.3977e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3585e-11 - val_loss: 3.3684e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2696e-11 - val_loss: 3.2172e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1450e-11 - val_loss: 3.1465e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0926e-11 - val_loss: 3.0431e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9937e-11 - val_loss: 3.0266e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9907e-11 - val_loss: 3.0513e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0159e-11 - val_loss: 3.0098e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9656e-11 - val_loss: 3.0528e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0554e-11 - val_loss: 3.1535e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1673e-11 - val_loss: 3.2263e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1602e-11 - val_loss: 3.1349e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0534e-11 - val_loss: 3.0194e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9511e-11 - val_loss: 2.9637e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9034e-11 - val_loss: 2.8642e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8253e-11 - val_loss: 2.8565e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8347e-11 - val_loss: 2.9140e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9234e-11 - val_loss: 2.9724e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9777e-11 - val_loss: 3.0690e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0415e-11 - val_loss: 3.1363e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0841e-11 - val_loss: 3.1135e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0750e-11 - val_loss: 3.1317e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0616e-11 - val_loss: 3.0356e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9248e-11 - val_loss: 2.8095e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7250e-11 - val_loss: 2.6894e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6484e-11 - val_loss: 2.6354e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6118e-11 - val_loss: 2.6635e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6623e-11 - val_loss: 2.7520e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7531e-11 - val_loss: 2.8579e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8388e-11 - val_loss: 2.9078e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9174e-11 - val_loss: 2.9955e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9494e-11 - val_loss: 2.9843e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9458e-11 - val_loss: 2.9432e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8991e-11 - val_loss: 2.8365e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7804e-11 - val_loss: 2.7817e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7096e-11 - val_loss: 2.6301e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5770e-11 - val_loss: 2.5824e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5616e-11 - val_loss: 2.6317e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6385e-11 - val_loss: 2.7418e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7007e-11 - val_loss: 2.7268e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7183e-11 - val_loss: 2.7254e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6891e-11 - val_loss: 2.6679e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6280e-11 - val_loss: 2.6131e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6145e-11 - val_loss: 2.6276e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5768e-11 - val_loss: 2.5974e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5741e-11 - val_loss: 2.5824e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5618e-11 - val_loss: 2.6188e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5795e-11 - val_loss: 2.6468e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6242e-11 - val_loss: 2.6597e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6331e-11 - val_loss: 2.6903e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6461e-11 - val_loss: 2.6595e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6330e-11 - val_loss: 2.6562e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6271e-11 - val_loss: 2.6218e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5547e-11 - val_loss: 2.5332e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5204e-11 - val_loss: 2.5770e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5550e-11 - val_loss: 2.5512e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5132e-11 - val_loss: 2.4911e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4736e-11 - val_loss: 2.4938e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.4697e-11 - val_loss: 2.4295e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3842e-11 - val_loss: 2.3740e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3466e-11 - val_loss: 2.3518e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3132e-11 - val_loss: 2.3403e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.3132e-11 - val_loss: 2.3242e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2963e-11 - val_loss: 2.3457e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3333e-11 - val_loss: 2.3675e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3582e-11 - val_loss: 2.3924e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3873e-11 - val_loss: 2.4369e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4403e-11 - val_loss: 2.5087e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4786e-11 - val_loss: 2.5158e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4678e-11 - val_loss: 2.4328e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3898e-11 - val_loss: 2.3853e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3445e-11 - val_loss: 2.3290e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2940e-11 - val_loss: 2.2766e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.2177e-11 - val_loss: 2.1777e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1496e-11 - val_loss: 2.1968e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1852e-11 - val_loss: 2.2265e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2122e-11 - val_loss: 2.2590e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2566e-11 - val_loss: 2.3164e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3359e-11 - val_loss: 2.4075e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3922e-11 - val_loss: 2.3695e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3313e-11 - val_loss: 2.3005e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2425e-11 - val_loss: 2.1934e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1594e-11 - val_loss: 2.1482e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1557e-11 - val_loss: 2.2013e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1941e-11 - val_loss: 2.2097e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1934e-11 - val_loss: 2.2016e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1699e-11 - val_loss: 2.1920e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2072e-11 - val_loss: 2.2527e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2434e-11 - val_loss: 2.2505e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2297e-11 - val_loss: 2.2176e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1933e-11 - val_loss: 2.1997e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1805e-11 - val_loss: 2.1798e-11
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1508e-11 - val_loss: 2.1154e-11
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0876e-11 - val_loss: 2.0735e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0556e-11 - val_loss: 2.1051e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1151e-11 - val_loss: 2.1410e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1275e-11 - val_loss: 2.1571e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1408e-11 - val_loss: 2.1472e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1274e-11 - val_loss: 2.1431e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.1018e-11 - val_loss: 2.0593e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0424e-11 - val_loss: 2.0800e-11
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0484e-11 - val_loss: 2.0485e-11
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 2.0075e-11 - val_loss: 1.9818e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9513e-11 - val_loss: 1.9585e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9582e-11 - val_loss: 1.9748e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9683e-11 - val_loss: 1.9933e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9887e-11 - val_loss: 2.0271e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0108e-11 - val_loss: 1.9916e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9819e-11 - val_loss: 2.0404e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0552e-11 - val_loss: 2.0821e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0677e-11 - val_loss: 2.1176e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0939e-11 - val_loss: 2.0648e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0443e-11 - val_loss: 2.0242e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9854e-11 - val_loss: 1.9755e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9867e-11 - val_loss: 1.9858e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9719e-11 - val_loss: 1.9691e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9562e-11 - val_loss: 1.9893e-11
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9677e-11 - val_loss: 1.9514e-11
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9353e-11 - val_loss: 1.9338e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9239e-11 - val_loss: 1.9453e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9360e-11 - val_loss: 1.9609e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9596e-11 - val_loss: 1.9881e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9753e-11 - val_loss: 1.9946e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9617e-11 - val_loss: 1.9368e-11
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.9113e-11 - val_loss: 1.9083e-11
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8808e-11 - val_loss: 1.8674e-11
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8554e-11 - val_loss: 1.8515e-11
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8301e-11 - val_loss: 1.8105e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8230e-11 - val_loss: 1.8578e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8572e-11 - val_loss: 1.9009e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8735e-11 - val_loss: 1.8459e-11
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.8179e-11 - val_loss: 1.8100e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7973e-11 - val_loss: 1.8086e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8065e-11 - val_loss: 1.8696e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8683e-11 - val_loss: 1.8889e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8752e-11 - val_loss: 1.9165e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9123e-11 - val_loss: 1.9336e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9147e-11 - val_loss: 1.9349e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8852e-11 - val_loss: 1.8357e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7937e-11 - val_loss: 1.7813e-11
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7562e-11 - val_loss: 1.7443e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7377e-11 - val_loss: 1.7693e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7623e-11 - val_loss: 1.7978e-11
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.7679e-11 - val_loss: 1.7121e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7005e-11 - val_loss: 1.7188e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6890e-11 - val_loss: 1.6927e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7017e-11 - val_loss: 1.7604e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7546e-11 - val_loss: 1.8029e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7922e-11 - val_loss: 1.8130e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8069e-11 - val_loss: 1.8040e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8009e-11 - val_loss: 1.8243e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7932e-11 - val_loss: 1.7525e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7406e-11 - val_loss: 1.7138e-11
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6895e-11 - val_loss: 1.6628e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6611e-11 - val_loss: 1.6656e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6433e-11 - val_loss: 1.6280e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6284e-11 - val_loss: 1.6412e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6473e-11 - val_loss: 1.6685e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6767e-11 - val_loss: 1.7013e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6915e-11 - val_loss: 1.7061e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6996e-11 - val_loss: 1.7355e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7210e-11 - val_loss: 1.7624e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7581e-11 - val_loss: 1.7678e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7605e-11 - val_loss: 1.7611e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7373e-11 - val_loss: 1.7117e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6723e-11 - val_loss: 1.6416e-11
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.6046e-11 - val_loss: 1.5697e-11
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5709e-11 - val_loss: 1.5527e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5353e-11 - val_loss: 1.5493e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5475e-11 - val_loss: 1.5885e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5893e-11 - val_loss: 1.5886e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5848e-11 - val_loss: 1.5752e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5834e-11 - val_loss: 1.6095e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5964e-11 - val_loss: 1.5931e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5814e-11 - val_loss: 1.5919e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5868e-11 - val_loss: 1.6254e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5896e-11 - val_loss: 1.5708e-11
Epoch 328/512

Epoch 00328: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5293e-11 - val_loss: 1.5266e-11
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5172e-11 - val_loss: 1.5048e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5025e-11 - val_loss: 1.5391e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5369e-11 - val_loss: 1.5489e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5358e-11 - val_loss: 1.5553e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5610e-11 - val_loss: 1.5716e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5592e-11 - val_loss: 1.5643e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5550e-11 - val_loss: 1.5762e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5632e-11 - val_loss: 1.5722e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5647e-11 - val_loss: 1.5758e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5560e-11 - val_loss: 1.5803e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5695e-11 - val_loss: 1.5651e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5411e-11 - val_loss: 1.5235e-11
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4974e-11 - val_loss: 1.4971e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4820e-11 - val_loss: 1.5036e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5038e-11 - val_loss: 1.5264e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5074e-11 - val_loss: 1.5024e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4965e-11 - val_loss: 1.5068e-11
Epoch 346/512

Epoch 00346: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4932e-11 - val_loss: 1.4908e-11
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4739e-11 - val_loss: 1.4769e-11
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4670e-11 - val_loss: 1.4712e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4662e-11 - val_loss: 1.4546e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4620e-11 - val_loss: 1.5103e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5290e-11 - val_loss: 1.6109e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6069e-11 - val_loss: 1.6636e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6381e-11 - val_loss: 1.6201e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5821e-11 - val_loss: 1.5472e-11
Epoch 355/512

Epoch 00355: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.5052e-11 - val_loss: 1.4475e-11
Epoch 356/512

Epoch 00356: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.4061e-11 - val_loss: 1.3628e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3619e-11 - val_loss: 1.3682e-11
Epoch 358/512

Epoch 00358: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3560e-11 - val_loss: 1.3498e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3446e-11 - val_loss: 1.3623e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3606e-11 - val_loss: 1.3756e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3681e-11 - val_loss: 1.3728e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3804e-11 - val_loss: 1.3984e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4020e-11 - val_loss: 1.4242e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4193e-11 - val_loss: 1.4007e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3783e-11 - val_loss: 1.3775e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3802e-11 - val_loss: 1.3961e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3945e-11 - val_loss: 1.3996e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3979e-11 - val_loss: 1.4359e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4065e-11 - val_loss: 1.4032e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4174e-11 - val_loss: 1.4475e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4499e-11 - val_loss: 1.4610e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4362e-11 - val_loss: 1.3866e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3618e-11 - val_loss: 1.3521e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3355e-11 - val_loss: 1.3263e-11
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3141e-11 - val_loss: 1.3012e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2905e-11 - val_loss: 1.3212e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3135e-11 - val_loss: 1.3261e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3216e-11 - val_loss: 1.3291e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3554e-11 - val_loss: 1.4185e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4027e-11 - val_loss: 1.3965e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3722e-11 - val_loss: 1.3772e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3636e-11 - val_loss: 1.3347e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3262e-11 - val_loss: 1.3119e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3175e-11 - val_loss: 1.3427e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3420e-11 - val_loss: 1.3747e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3748e-11 - val_loss: 1.4112e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3973e-11 - val_loss: 1.4004e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3701e-11 - val_loss: 1.3706e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3614e-11 - val_loss: 1.3635e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3326e-11 - val_loss: 1.3149e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.3050e-11 - val_loss: 1.2803e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2732e-11 - val_loss: 1.2526e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2629e-11 - val_loss: 1.3058e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3014e-11 - val_loss: 1.3097e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2950e-11 - val_loss: 1.3315e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3270e-11 - val_loss: 1.3161e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3146e-11 - val_loss: 1.3392e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2992e-11 - val_loss: 1.2753e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2789e-11 - val_loss: 1.2809e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2626e-11 - val_loss: 1.2566e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2627e-11 - val_loss: 1.2823e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2720e-11 - val_loss: 1.2790e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2801e-11 - val_loss: 1.2808e-11
Epoch 404/512

Epoch 00404: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2546e-11 - val_loss: 1.2343e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2326e-11 - val_loss: 1.2380e-11
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.2157e-11 - val_loss: 1.1819e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1785e-11 - val_loss: 1.1898e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1881e-11 - val_loss: 1.2075e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2111e-11 - val_loss: 1.2255e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2203e-11 - val_loss: 1.2371e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2430e-11 - val_loss: 1.2633e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2658e-11 - val_loss: 1.2672e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2543e-11 - val_loss: 1.2465e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2393e-11 - val_loss: 1.2627e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2523e-11 - val_loss: 1.2379e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2360e-11 - val_loss: 1.2556e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2606e-11 - val_loss: 1.2547e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2423e-11 - val_loss: 1.2299e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2085e-11 - val_loss: 1.1891e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2004e-11 - val_loss: 1.2115e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1984e-11 - val_loss: 1.1959e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1869e-11 - val_loss: 1.1745e-11
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1602e-11 - val_loss: 1.1700e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1769e-11 - val_loss: 1.1884e-11
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1666e-11 - val_loss: 1.1607e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1750e-11 - val_loss: 1.1862e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2001e-11 - val_loss: 1.1969e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1796e-11 - val_loss: 1.1698e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1830e-11 - val_loss: 1.2131e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2085e-11 - val_loss: 1.2061e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2038e-11 - val_loss: 1.2327e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2401e-11 - val_loss: 1.2658e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2747e-11 - val_loss: 1.2808e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2665e-11 - val_loss: 1.2416e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2300e-11 - val_loss: 1.2140e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1981e-11 - val_loss: 1.1865e-11
Epoch 437/512

Epoch 00437: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1720e-11 - val_loss: 1.1593e-11
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1510e-11 - val_loss: 1.1266e-11
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.1138e-11 - val_loss: 1.0960e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0832e-11 - val_loss: 1.0801e-11
Epoch 441/512

Epoch 00441: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0702e-11 - val_loss: 1.0801e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0906e-11 - val_loss: 1.1286e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1173e-11 - val_loss: 1.1169e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1250e-11 - val_loss: 1.1506e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1525e-11 - val_loss: 1.1553e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1591e-11 - val_loss: 1.1669e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1466e-11 - val_loss: 1.1411e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1549e-11 - val_loss: 1.1840e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1756e-11 - val_loss: 1.1954e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2150e-11 - val_loss: 1.2457e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2361e-11 - val_loss: 1.2183e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1921e-11 - val_loss: 1.1795e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1771e-11 - val_loss: 1.1788e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1504e-11 - val_loss: 1.1030e-11
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0945e-11 - val_loss: 1.0742e-11
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0611e-11 - val_loss: 1.0635e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0664e-11 - val_loss: 1.0646e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0613e-11 - val_loss: 1.0675e-11
Epoch 459/512

Epoch 00459: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0594e-11 - val_loss: 1.0393e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0391e-11 - val_loss: 1.0542e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0440e-11 - val_loss: 1.0299e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0348e-11 - val_loss: 1.0393e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0392e-11 - val_loss: 1.0414e-11
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0319e-11 - val_loss: 1.0248e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0328e-11 - val_loss: 1.0475e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0375e-11 - val_loss: 1.0381e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0407e-11 - val_loss: 1.0459e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0371e-11 - val_loss: 1.0364e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0340e-11 - val_loss: 1.0393e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0421e-11 - val_loss: 1.0649e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0722e-11 - val_loss: 1.0830e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0741e-11 - val_loss: 1.0691e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0660e-11 - val_loss: 1.0615e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0637e-11 - val_loss: 1.0558e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0514e-11 - val_loss: 1.0477e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0367e-11 - val_loss: 1.0309e-11
Epoch 477/512

Epoch 00477: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0209e-11 - val_loss: 1.0196e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0157e-11 - val_loss: 1.0342e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0309e-11 - val_loss: 1.0364e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0359e-11 - val_loss: 1.0366e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0290e-11 - val_loss: 1.0365e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0319e-11 - val_loss: 1.0264e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0266e-11 - val_loss: 1.0361e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0347e-11 - val_loss: 1.0438e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0458e-11 - val_loss: 1.0557e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0533e-11 - val_loss: 1.0624e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0515e-11 - val_loss: 1.0390e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0393e-11 - val_loss: 1.0522e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0443e-11 - val_loss: 1.0427e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0643e-11 - val_loss: 1.0906e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0778e-11 - val_loss: 1.0607e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0494e-11 - val_loss: 1.0475e-11
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0430e-11 - val_loss: 1.0131e-11
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0094e-11 - val_loss: 1.0000e-11
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 1.0017e-11 - val_loss: 9.9208e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9760e-12 - val_loss: 1.0174e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0094e-11 - val_loss: 9.9904e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9748e-12 - val_loss: 1.0030e-11
Epoch 499/512

Epoch 00499: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.9690e-12 - val_loss: 9.8444e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8861e-12 - val_loss: 9.9273e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8307e-12 - val_loss: 9.9325e-12
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.8567e-12 - val_loss: 9.8419e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0048e-11 - val_loss: 1.0156e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0046e-11 - val_loss: 9.9589e-12
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-cuvre-secp224r1-batch-448-0.00005-random-59/multiplication_weights.h5
448/448 - 0s - loss: 9.9651e-12 - val_loss: 9.7799e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7818e-12 - val_loss: 9.9621e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8736e-12 - val_loss: 9.7919e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7869e-12 - val_loss: 9.8979e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9468e-12 - val_loss: 9.8413e-12
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8847e-12 - val_loss: 9.9830e-12
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9591e-12 - val_loss: 1.0015e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9888e-12 - val_loss: 9.7966e-12
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.758 | eve: 9.363 | bob: 9.583Epoch   0:   0% | abe: 9.717 | eve: 9.366 | bob: 9.550Epoch   0:   1% | abe: 9.700 | eve: 9.359 | bob: 9.541Epoch   0:   2% | abe: 9.643 | eve: 9.353 | bob: 9.493Epoch   0:   2% | abe: 9.617 | eve: 9.346 | bob: 9.476Epoch   0:   3% | abe: 9.589 | eve: 9.345 | bob: 9.456Epoch   0:   4% | abe: 9.562 | eve: 9.340 | bob: 9.437Epoch   0:   4% | abe: 9.540 | eve: 9.344 | bob: 9.421Epoch   0:   5% | abe: 9.510 | eve: 9.350 | bob: 9.398Epoch   0:   6% | abe: 9.483 | eve: 9.348 | bob: 9.375Epoch   0:   6% | abe: 9.463 | eve: 9.348 | bob: 9.359Epoch   0:   7% | abe: 9.449 | eve: 9.351 | bob: 9.349Epoch   0:   8% | abe: 9.439 | eve: 9.349 | bob: 9.342Epoch   0:   8% | abe: 9.427 | eve: 9.350 | bob: 9.333Epoch   0:   9% | abe: 9.417 | eve: 9.355 | bob: 9.325Epoch   0:  10% | abe: 9.403 | eve: 9.352 | bob: 9.313Epoch   0:  10% | abe: 9.394 | eve: 9.350 | bob: 9.306Epoch   0:  11% | abe: 9.386 | eve: 9.350 | bob: 9.299Epoch   0:  12% | abe: 9.374 | eve: 9.349 | bob: 9.288Epoch   0:  13% | abe: 9.367 | eve: 9.352 | bob: 9.282Epoch   0:  13% | abe: 9.361 | eve: 9.353 | bob: 9.277Epoch   0:  14% | abe: 9.354 | eve: 9.356 | bob: 9.271Epoch   0:  15% | abe: 9.347 | eve: 9.354 | bob: 9.264Epoch   0:  15% | abe: 9.339 | eve: 9.358 | bob: 9.257Epoch   0:  16% | abe: 9.332 | eve: 9.358 | bob: 9.250Epoch   0:  17% | abe: 9.324 | eve: 9.357 | bob: 9.241Epoch   0:  17% | abe: 9.318 | eve: 9.357 | bob: 9.236Epoch   0:  18% | abe: 9.313 | eve: 9.353 | bob: 9.231Epoch   0:  19% | abe: 9.308 | eve: 9.351 | bob: 9.226Epoch   0:  19% | abe: 9.302 | eve: 9.352 | bob: 9.220Epoch   0:  20% | abe: 9.293 | eve: 9.352 | bob: 9.212Epoch   0:  21% | abe: 9.286 | eve: 9.351 | bob: 9.204Epoch   0:  21% | abe: 9.280 | eve: 9.352 | bob: 9.198Epoch   0:  22% | abe: 9.270 | eve: 9.355 | bob: 9.189Epoch   0:  23% | abe: 9.262 | eve: 9.355 | bob: 9.181Epoch   0:  23% | abe: 9.254 | eve: 9.356 | bob: 9.174Epoch   0:  24% | abe: 9.247 | eve: 9.355 | bob: 9.167Epoch   0:  25% | abe: 9.240 | eve: 9.356 | bob: 9.161Epoch   0:  26% | abe: 9.232 | eve: 9.357 | bob: 9.153Epoch   0:  26% | abe: 9.228 | eve: 9.357 | bob: 9.149Epoch   0:  27% | abe: 9.221 | eve: 9.355 | bob: 9.142Epoch   0:  28% | abe: 9.215 | eve: 9.355 | bob: 9.137Epoch   0:  28% | abe: 9.209 | eve: 9.357 | bob: 9.131Epoch   0:  29% | abe: 9.203 | eve: 9.355 | bob: 9.126Epoch   0:  30% | abe: 9.197 | eve: 9.357 | bob: 9.119Epoch   0:  30% | abe: 9.191 | eve: 9.356 | bob: 9.113Epoch   0:  31% | abe: 9.186 | eve: 9.357 | bob: 9.108Epoch   0:  32% | abe: 9.181 | eve: 9.357 | bob: 9.104Epoch   0:  32% | abe: 9.175 | eve: 9.357 | bob: 9.098Epoch   0:  33% | abe: 9.170 | eve: 9.358 | bob: 9.093Epoch   0:  34% | abe: 9.165 | eve: 9.357 | bob: 9.088Epoch   0:  34% | abe: 9.161 | eve: 9.358 | bob: 9.084Epoch   0:  35% | abe: 9.155 | eve: 9.359 | bob: 9.079Epoch   0:  36% | abe: 9.151 | eve: 9.359 | bob: 9.074Epoch   0:  36% | abe: 9.146 | eve: 9.358 | bob: 9.070Epoch   0:  37% | abe: 9.142 | eve: 9.359 | bob: 9.066Epoch   0:  38% | abe: 9.138 | eve: 9.357 | bob: 9.062Epoch   0:  39% | abe: 9.134 | eve: 9.357 | bob: 9.058Epoch   0:  39% | abe: 9.131 | eve: 9.357 | bob: 9.055Epoch   0:  40% | abe: 9.127 | eve: 9.357 | bob: 9.052Epoch   0:  41% | abe: 9.124 | eve: 9.357 | bob: 9.048Epoch   0:  41% | abe: 9.120 | eve: 9.357 | bob: 9.045Epoch   0:  42% | abe: 9.116 | eve: 9.356 | bob: 9.041Epoch   0:  43% | abe: 9.113 | eve: 9.356 | bob: 9.038Epoch   0:  43% | abe: 9.110 | eve: 9.357 | bob: 9.035Epoch   0:  44% | abe: 9.107 | eve: 9.357 | bob: 9.032Epoch   0:  45% | abe: 9.104 | eve: 9.357 | bob: 9.029Epoch   0:  45% | abe: 9.100 | eve: 9.356 | bob: 9.025Epoch   0:  46% | abe: 9.097 | eve: 9.357 | bob: 9.022Epoch   0:  47% | abe: 9.093 | eve: 9.357 | bob: 9.019Epoch   0:  47% | abe: 9.090 | eve: 9.357 | bob: 9.015Epoch   0:  48% | abe: 9.087 | eve: 9.358 | bob: 9.013Epoch   0:  49% | abe: 9.084 | eve: 9.358 | bob: 9.009Epoch   0:  50% | abe: 9.081 | eve: 9.358 | bob: 9.007Epoch   0:  50% | abe: 9.078 | eve: 9.359 | bob: 9.004Epoch   0:  51% | abe: 9.076 | eve: 9.359 | bob: 9.001Epoch   0:  52% | abe: 9.072 | eve: 9.358 | bob: 8.998Epoch   0:  52% | abe: 9.069 | eve: 9.359 | bob: 8.995Epoch   0:  53% | abe: 9.067 | eve: 9.359 | bob: 8.993Epoch   0:  54% | abe: 9.064 | eve: 9.358 | bob: 8.990Epoch   0:  54% | abe: 9.062 | eve: 9.359 | bob: 8.989Epoch   0:  55% | abe: 9.060 | eve: 9.359 | bob: 8.986Epoch   0:  56% | abe: 9.058 | eve: 9.359 | bob: 8.984Epoch   0:  56% | abe: 9.055 | eve: 9.359 | bob: 8.982Epoch   0:  57% | abe: 9.053 | eve: 9.359 | bob: 8.979Epoch   0:  58% | abe: 9.051 | eve: 9.358 | bob: 8.978Epoch   0:  58% | abe: 9.049 | eve: 9.359 | bob: 8.976Epoch   0:  59% | abe: 9.046 | eve: 9.359 | bob: 8.973Epoch   0:  60% | abe: 9.044 | eve: 9.359 | bob: 8.971Epoch   0:  60% | abe: 9.043 | eve: 9.358 | bob: 8.970Epoch   0:  61% | abe: 9.040 | eve: 9.358 | bob: 8.967Epoch   0:  62% | abe: 9.037 | eve: 9.358 | bob: 8.965Epoch   0:  63% | abe: 9.035 | eve: 9.357 | bob: 8.963Epoch   0:  63% | abe: 9.033 | eve: 9.358 | bob: 8.961Epoch   0:  64% | abe: 9.031 | eve: 9.359 | bob: 8.958Epoch   0:  65% | abe: 9.028 | eve: 9.359 | bob: 8.956Epoch   0:  65% | abe: 9.027 | eve: 9.359 | bob: 8.954Epoch   0:  66% | abe: 9.025 | eve: 9.358 | bob: 8.953Epoch   0:  67% | abe: 9.023 | eve: 9.359 | bob: 8.951Epoch   0:  67% | abe: 9.021 | eve: 9.359 | bob: 8.949Epoch   0:  68% | abe: 9.019 | eve: 9.358 | bob: 8.948Epoch   0:  69% | abe: 9.017 | eve: 9.358 | bob: 8.945Epoch   0:  69% | abe: 9.015 | eve: 9.358 | bob: 8.943Epoch   0:  70% | abe: 9.013 | eve: 9.358 | bob: 8.941Epoch   0:  71% | abe: 9.011 | eve: 9.359 | bob: 8.940Epoch   0:  71% | abe: 9.009 | eve: 9.358 | bob: 8.938Epoch   0:  72% | abe: 9.007 | eve: 9.358 | bob: 8.936Epoch   0:  73% | abe: 9.005 | eve: 9.359 | bob: 8.934Epoch   0:  73% | abe: 9.003 | eve: 9.359 | bob: 8.932Epoch   0:  74% | abe: 9.002 | eve: 9.359 | bob: 8.930Epoch   0:  75% | abe: 9.001 | eve: 9.359 | bob: 8.929Epoch   0:  76% | abe: 8.999 | eve: 9.359 | bob: 8.928Epoch   0:  76% | abe: 8.997 | eve: 9.358 | bob: 8.926Epoch   0:  77% | abe: 8.995 | eve: 9.358 | bob: 8.924Epoch   0:  78% | abe: 8.994 | eve: 9.359 | bob: 8.923Epoch   0:  78% | abe: 8.992 | eve: 9.359 | bob: 8.921Epoch   0:  79% | abe: 8.990 | eve: 9.359 | bob: 8.919Epoch   0:  80% | abe: 8.989 | eve: 9.360 | bob: 8.918Epoch   0:  80% | abe: 8.987 | eve: 9.360 | bob: 8.916Epoch   0:  81% | abe: 8.986 | eve: 9.359 | bob: 8.915Epoch   0:  82% | abe: 8.984 | eve: 9.360 | bob: 8.914Epoch   0:  82% | abe: 8.983 | eve: 9.360 | bob: 8.912Epoch   0:  83% | abe: 8.981 | eve: 9.359 | bob: 8.910Epoch   0:  84% | abe: 8.979 | eve: 9.359 | bob: 8.909Epoch   0:  84% | abe: 8.978 | eve: 9.360 | bob: 8.908Epoch   0:  85% | abe: 8.977 | eve: 9.360 | bob: 8.906Epoch   0:  86% | abe: 8.975 | eve: 9.360 | bob: 8.905Epoch   0:  86% | abe: 8.974 | eve: 9.360 | bob: 8.904Epoch   0:  87% | abe: 8.973 | eve: 9.360 | bob: 8.903Epoch   0:  88% | abe: 8.972 | eve: 9.360 | bob: 8.902Epoch   0:  89% | abe: 8.971 | eve: 9.360 | bob: 8.901Epoch   0:  89% | abe: 8.969 | eve: 9.360 | bob: 8.899Epoch   0:  90% | abe: 8.968 | eve: 9.360 | bob: 8.898Epoch   0:  91% | abe: 8.967 | eve: 9.361 | bob: 8.897Epoch   0:  91% | abe: 8.965 | eve: 9.361 | bob: 8.895Epoch   0:  92% | abe: 8.964 | eve: 9.362 | bob: 8.894Epoch   0:  93% | abe: 8.962 | eve: 9.362 | bob: 8.892Epoch   0:  93% | abe: 8.961 | eve: 9.362 | bob: 8.892Epoch   0:  94% | abe: 8.960 | eve: 9.363 | bob: 8.890Epoch   0:  95% | abe: 8.959 | eve: 9.364 | bob: 8.889Epoch   0:  95% | abe: 8.958 | eve: 9.364 | bob: 8.888Epoch   0:  96% | abe: 8.957 | eve: 9.364 | bob: 8.887Epoch   0:  97% | abe: 8.955 | eve: 9.364 | bob: 8.886Epoch   0:  97% | abe: 8.955 | eve: 9.364 | bob: 8.885Epoch   0:  98% | abe: 8.953 | eve: 9.364 | bob: 8.884Epoch   0:  99% | abe: 8.952 | eve: 9.364 | bob: 8.883
New best Bob loss 8.882968650613648 at epoch 0
Epoch   1:   0% | abe: 8.785 | eve: 9.339 | bob: 8.722Epoch   1:   0% | abe: 8.778 | eve: 9.372 | bob: 8.714Epoch   1:   1% | abe: 8.807 | eve: 9.365 | bob: 8.744Epoch   1:   2% | abe: 8.807 | eve: 9.352 | bob: 8.744Epoch   1:   2% | abe: 8.798 | eve: 9.353 | bob: 8.735Epoch   1:   3% | abe: 8.790 | eve: 9.353 | bob: 8.727Epoch   1:   4% | abe: 8.786 | eve: 9.346 | bob: 8.723Epoch   1:   4% | abe: 8.775 | eve: 9.360 | bob: 8.712Epoch   1:   5% | abe: 8.775 | eve: 9.369 | bob: 8.711Epoch   1:   6% | abe: 8.779 | eve: 9.364 | bob: 8.716Epoch   1:   6% | abe: 8.779 | eve: 9.362 | bob: 8.715Epoch   1:   7% | abe: 8.774 | eve: 9.368 | bob: 8.711Epoch   1:   8% | abe: 8.773 | eve: 9.369 | bob: 8.710Epoch   1:   8% | abe: 8.777 | eve: 9.367 | bob: 8.714Epoch   1:   9% | abe: 8.779 | eve: 9.368 | bob: 8.716Epoch   1:  10% | abe: 8.780 | eve: 9.366 | bob: 8.717Epoch   1:  10% | abe: 8.778 | eve: 9.368 | bob: 8.715Epoch   1:  11% | abe: 8.779 | eve: 9.369 | bob: 8.716Epoch   1:  12% | abe: 8.779 | eve: 9.369 | bob: 8.716Epoch   1:  13% | abe: 8.776 | eve: 9.368 | bob: 8.713Epoch   1:  13% | abe: 8.774 | eve: 9.367 | bob: 8.711Epoch   1:  14% | abe: 8.772 | eve: 9.366 | bob: 8.709Epoch   1:  15% | abe: 8.770 | eve: 9.367 | bob: 8.707Epoch   1:  15% | abe: 8.769 | eve: 9.371 | bob: 8.706Epoch   1:  16% | abe: 8.769 | eve: 9.371 | bob: 8.706Epoch   1:  17% | abe: 8.768 | eve: 9.369 | bob: 8.705Epoch   1:  17% | abe: 8.767 | eve: 9.370 | bob: 8.704Epoch   1:  18% | abe: 8.767 | eve: 9.370 | bob: 8.704Epoch   1:  19% | abe: 8.766 | eve: 9.375 | bob: 8.703Epoch   1:  19% | abe: 8.766 | eve: 9.373 | bob: 8.703Epoch   1:  20% | abe: 8.765 | eve: 9.370 | bob: 8.702Epoch   1:  21% | abe: 8.765 | eve: 9.370 | bob: 8.702Epoch   1:  21% | abe: 8.766 | eve: 9.370 | bob: 8.703Epoch   1:  22% | abe: 8.764 | eve: 9.372 | bob: 8.702Epoch   1:  23% | abe: 8.763 | eve: 9.373 | bob: 8.701Epoch   1:  23% | abe: 8.763 | eve: 9.374 | bob: 8.700Epoch   1:  24% | abe: 8.763 | eve: 9.374 | bob: 8.701Epoch   1:  25% | abe: 8.762 | eve: 9.375 | bob: 8.699Epoch   1:  26% | abe: 8.761 | eve: 9.375 | bob: 8.698Epoch   1:  26% | abe: 8.760 | eve: 9.376 | bob: 8.697Epoch   1:  27% | abe: 8.760 | eve: 9.377 | bob: 8.697Epoch   1:  28% | abe: 8.759 | eve: 9.379 | bob: 8.696Epoch   1:  28% | abe: 8.759 | eve: 9.380 | bob: 8.696Epoch   1:  29% | abe: 8.759 | eve: 9.379 | bob: 8.697Epoch   1:  30% | abe: 8.760 | eve: 9.381 | bob: 8.697Epoch   1:  30% | abe: 8.759 | eve: 9.381 | bob: 8.697Epoch   1:  31% | abe: 8.758 | eve: 9.380 | bob: 8.696Epoch   1:  32% | abe: 8.760 | eve: 9.381 | bob: 8.697Epoch   1:  32% | abe: 8.761 | eve: 9.382 | bob: 8.698Epoch   1:  33% | abe: 8.761 | eve: 9.382 | bob: 8.699Epoch   1:  34% | abe: 8.761 | eve: 9.382 | bob: 8.699Epoch   1:  34% | abe: 8.761 | eve: 9.381 | bob: 8.699Epoch   1:  35% | abe: 8.761 | eve: 9.381 | bob: 8.699Epoch   1:  36% | abe: 8.761 | eve: 9.381 | bob: 8.698Epoch   1:  36% | abe: 8.760 | eve: 9.380 | bob: 8.698Epoch   1:  37% | abe: 8.760 | eve: 9.382 | bob: 8.698Epoch   1:  38% | abe: 8.760 | eve: 9.383 | bob: 8.697Epoch   1:  39% | abe: 8.759 | eve: 9.383 | bob: 8.697Epoch   1:  39% | abe: 8.759 | eve: 9.385 | bob: 8.697Epoch   1:  40% | abe: 8.759 | eve: 9.383 | bob: 8.697Epoch   1:  41% | abe: 8.759 | eve: 9.384 | bob: 8.696Epoch   1:  41% | abe: 8.758 | eve: 9.384 | bob: 8.696Epoch   1:  42% | abe: 8.757 | eve: 9.384 | bob: 8.695Epoch   1:  43% | abe: 8.757 | eve: 9.385 | bob: 8.694Epoch   1:  43% | abe: 8.756 | eve: 9.385 | bob: 8.694Epoch   1:  44% | abe: 8.755 | eve: 9.385 | bob: 8.693Epoch   1:  45% | abe: 8.755 | eve: 9.386 | bob: 8.693Epoch   1:  45% | abe: 8.754 | eve: 9.386 | bob: 8.692Epoch   1:  46% | abe: 8.753 | eve: 9.386 | bob: 8.691Epoch   1:  47% | abe: 8.752 | eve: 9.385 | bob: 8.690Epoch   1:  47% | abe: 8.752 | eve: 9.385 | bob: 8.689Epoch   1:  48% | abe: 8.751 | eve: 9.385 | bob: 8.688Epoch   1:  49% | abe: 8.751 | eve: 9.385 | bob: 8.688Epoch   1:  50% | abe: 8.750 | eve: 9.384 | bob: 8.688Epoch   1:  50% | abe: 8.749 | eve: 9.386 | bob: 8.687Epoch   1:  51% | abe: 8.749 | eve: 9.387 | bob: 8.687Epoch   1:  52% | abe: 8.748 | eve: 9.388 | bob: 8.686Epoch   1:  52% | abe: 8.748 | eve: 9.389 | bob: 8.686Epoch   1:  53% | abe: 8.748 | eve: 9.389 | bob: 8.685Epoch   1:  54% | abe: 8.747 | eve: 9.389 | bob: 8.685Epoch   1:  54% | abe: 8.747 | eve: 9.390 | bob: 8.685Epoch   1:  55% | abe: 8.747 | eve: 9.391 | bob: 8.684Epoch   1:  56% | abe: 8.746 | eve: 9.392 | bob: 8.683Epoch   1:  56% | abe: 8.746 | eve: 9.393 | bob: 8.683Epoch   1:  57% | abe: 8.746 | eve: 9.395 | bob: 8.683Epoch   1:  58% | abe: 8.745 | eve: 9.395 | bob: 8.683Epoch   1:  58% | abe: 8.745 | eve: 9.395 | bob: 8.683Epoch   1:  59% | abe: 8.745 | eve: 9.395 | bob: 8.682Epoch   1:  60% | abe: 8.743 | eve: 9.395 | bob: 8.681Epoch   1:  60% | abe: 8.743 | eve: 9.395 | bob: 8.680Epoch   1:  61% | abe: 8.742 | eve: 9.395 | bob: 8.680Epoch   1:  62% | abe: 8.742 | eve: 9.395 | bob: 8.680Epoch   1:  63% | abe: 8.741 | eve: 9.396 | bob: 8.679Epoch   1:  63% | abe: 8.741 | eve: 9.395 | bob: 8.679Epoch   1:  64% | abe: 8.741 | eve: 9.395 | bob: 8.679Epoch   1:  65% | abe: 8.741 | eve: 9.396 | bob: 8.678Epoch   1:  65% | abe: 8.740 | eve: 9.397 | bob: 8.677Epoch   1:  66% | abe: 8.739 | eve: 9.397 | bob: 8.676Epoch   1:  67% | abe: 8.739 | eve: 9.397 | bob: 8.676Epoch   1:  67% | abe: 8.738 | eve: 9.397 | bob: 8.676Epoch   1:  68% | abe: 8.738 | eve: 9.398 | bob: 8.676Epoch   1:  69% | abe: 8.738 | eve: 9.399 | bob: 8.675Epoch   1:  69% | abe: 8.738 | eve: 9.398 | bob: 8.675Epoch   1:  70% | abe: 8.737 | eve: 9.399 | bob: 8.675Epoch   1:  71% | abe: 8.737 | eve: 9.399 | bob: 8.674Epoch   1:  71% | abe: 8.737 | eve: 9.399 | bob: 8.674Epoch   1:  72% | abe: 8.737 | eve: 9.400 | bob: 8.674Epoch   1:  73% | abe: 8.736 | eve: 9.400 | bob: 8.674Epoch   1:  73% | abe: 8.736 | eve: 9.401 | bob: 8.673Epoch   1:  74% | abe: 8.735 | eve: 9.401 | bob: 8.673Epoch   1:  75% | abe: 8.735 | eve: 9.402 | bob: 8.673Epoch   1:  76% | abe: 8.735 | eve: 9.402 | bob: 8.673Epoch   1:  76% | abe: 8.734 | eve: 9.402 | bob: 8.672Epoch   1:  77% | abe: 8.734 | eve: 9.402 | bob: 8.672Epoch   1:  78% | abe: 8.734 | eve: 9.403 | bob: 8.671Epoch   1:  78% | abe: 8.733 | eve: 9.403 | bob: 8.670Epoch   1:  79% | abe: 8.732 | eve: 9.403 | bob: 8.670Epoch   1:  80% | abe: 8.732 | eve: 9.404 | bob: 8.669Epoch   1:  80% | abe: 8.732 | eve: 9.405 | bob: 8.669Epoch   1:  81% | abe: 8.732 | eve: 9.405 | bob: 8.669Epoch   1:  82% | abe: 8.732 | eve: 9.405 | bob: 8.669Epoch   1:  82% | abe: 8.731 | eve: 9.406 | bob: 8.668Epoch   1:  83% | abe: 8.730 | eve: 9.406 | bob: 8.668Epoch   1:  84% | abe: 8.730 | eve: 9.406 | bob: 8.667Epoch   1:  84% | abe: 8.730 | eve: 9.406 | bob: 8.667Epoch   1:  85% | abe: 8.729 | eve: 9.406 | bob: 8.667Epoch   1:  86% | abe: 8.729 | eve: 9.407 | bob: 8.666Epoch   1:  86% | abe: 8.728 | eve: 9.407 | bob: 8.665Epoch   1:  87% | abe: 8.728 | eve: 9.407 | bob: 8.665Epoch   1:  88% | abe: 8.727 | eve: 9.407 | bob: 8.664Epoch   1:  89% | abe: 8.727 | eve: 9.407 | bob: 8.664Epoch   1:  89% | abe: 8.726 | eve: 9.407 | bob: 8.663Epoch   1:  90% | abe: 8.726 | eve: 9.407 | bob: 8.663Epoch   1:  91% | abe: 8.725 | eve: 9.408 | bob: 8.663Epoch   1:  91% | abe: 8.725 | eve: 9.408 | bob: 8.662Epoch   1:  92% | abe: 8.724 | eve: 9.408 | bob: 8.661Epoch   1:  93% | abe: 8.724 | eve: 9.408 | bob: 8.661Epoch   1:  93% | abe: 8.723 | eve: 9.408 | bob: 8.661Epoch   1:  94% | abe: 8.723 | eve: 9.409 | bob: 8.660Epoch   1:  95% | abe: 8.722 | eve: 9.409 | bob: 8.660Epoch   1:  95% | abe: 8.722 | eve: 9.409 | bob: 8.659Epoch   1:  96% | abe: 8.722 | eve: 9.409 | bob: 8.659Epoch   1:  97% | abe: 8.721 | eve: 9.409 | bob: 8.659Epoch   1:  97% | abe: 8.722 | eve: 9.410 | bob: 8.659Epoch   1:  98% | abe: 8.721 | eve: 9.411 | bob: 8.659Epoch   1:  99% | abe: 8.721 | eve: 9.411 | bob: 8.658
New best Bob loss 8.658297561151668 at epoch 1
Training complete.
cipher1 + cipher2
[[0.94801605 1.1592784  0.6120428  ... 1.2014699  0.6632293  1.1345882 ]
 [1.0555013  0.96336937 0.60719895 ... 1.1838129  0.6673621  1.1521404 ]
 [1.1339653  1.1043243  0.64348465 ... 0.94537127 0.7819419  1.0539958 ]
 ...
 [0.999241   1.1372851  0.6020362  ... 1.2680279  0.6595612  1.1831797 ]
 [1.0086515  1.0698237  0.6093987  ... 1.1461003  0.7460295  0.96971506]
 [0.8176973  1.054386   0.63318956 ... 0.9760491  0.8149117  0.99960226]]
HO addition:
[[0.9479951  1.1592529  0.6120292  ... 1.2014434  0.66321456 1.1345633 ]
 [1.0554782  0.96334815 0.6071855  ... 1.1837869  0.6673473  1.1521149 ]
 [1.1339405  1.1043     0.6434704  ... 0.9453504  0.7819246  1.0539725 ]
 ...
 [0.9992188  1.1372601  0.6020229  ... 1.2679999  0.6595467  1.1831536 ]
 [1.0086294  1.0698004  0.60938525 ... 1.146075   0.7460131  0.96969366]
 [0.81767917 1.054363   0.6331755  ... 0.9760278  0.8148938  0.99958026]]
cipher1 * cipher2
[[0.22461744 0.32373136 0.09307928 ... 0.36084777 0.10959145 0.3150099 ]
 [0.2659069  0.23000298 0.09150242 ... 0.34594747 0.11094535 0.33144298]
 [0.32116246 0.30366233 0.10194031 ... 0.22300504 0.15235479 0.27771938]
 ...
 [0.24819268 0.3192483  0.09039671 ... 0.40163997 0.10842418 0.34986332]
 [0.24422035 0.28159112 0.09235356 ... 0.3262232  0.13698874 0.23430625]
 [0.16710158 0.2651866  0.09861822 ... 0.2258475  0.1648452  0.24531312]]
HO multiplication
[[0.2246214  0.32373148 0.09307423 ... 0.3608489  0.10958751 0.31501156]
 [0.2659083  0.23000634 0.09149729 ... 0.3459482  0.11094154 0.33144552]
 [0.32116544 0.3036657  0.10193548 ... 0.22300863 0.15235427 0.27772382]
 ...
 [0.24819721 0.3192504  0.09039168 ... 0.4016376  0.10842001 0.34986502]
 [0.24422213 0.28159416 0.09234852 ... 0.32622543 0.13698642 0.23431006]
 [0.16710243 0.26518804 0.09861315 ... 0.2258484  0.16484547 0.24531612]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 100.00%
Bob decrypted addition: [[1.0020686  0.9467012  0.9693088  ... 0.9790177  0.98897654 1.0006992 ]
 [1.0199096  0.95719963 1.0004199  ... 1.0022314  1.0184516  0.9260233 ]
 [1.0336175  0.9766181  0.947795   ... 1.0179     0.9519252  1.0962209 ]
 ...
 [1.0422794  0.9756928  0.9404312  ... 0.9775639  0.99333847 0.9884484 ]
 [0.97990227 1.0311229  0.82577497 ... 0.9865552  0.96163017 1.0394785 ]
 [1.0143279  0.94214344 0.9982883  ... 0.9988541  0.97642475 1.0408293 ]]
Bob decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits addition: 3551
Total number of bits addition: 7168
Decryption accuracy addition: 49.539620535714285%
Bob decrypted multiplication: [[0.75520927 0.7483026  0.77258015 ... 0.7611573  0.77159005 0.74775237]
 [0.7532306  0.75094193 0.82064295 ... 0.77643895 0.7888538  0.74309546]
 [0.75405425 0.76218444 0.76176584 ... 0.8166164  0.759087   0.7604917 ]
 ...
 [0.75748193 0.7569815  0.7597424  ... 0.76050985 0.78154534 0.7447071 ]
 [0.7531881  0.75998336 0.7520351  ... 0.76390046 0.7531998  0.7842236 ]
 [0.7541802  0.7462222  0.8426218  ... 0.76900977 0.76171803 0.7601229 ]]
Bob decrypted bits multiplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits multiplication: 1818
Total number of bits multiplication: 7168
Decryption accuracy multiplication: 25.362723214285715%
Eve decrypted addition: [[0.90657413 1.0855242  0.97204554 ... 0.94131976 1.1032488  0.8896221 ]
 [0.93321323 1.1035247  0.9553214  ... 0.9464974  1.1036116  0.89555997]
 [0.9418205  1.0585518  0.9572436  ... 0.9523349  1.085992   0.91730726]
 ...
 [0.9092461  1.1094284  0.97023326 ... 0.94245416 1.1022029  0.89126974]
 [0.9245499  1.0818714  0.9782354  ... 0.93051076 1.0995672  0.8980203 ]
 [0.9575303  1.0825872  0.9458292  ... 0.9149006  1.0959743  0.9024675 ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 3551
Total number of bits addition: 7168
Decryption accuracy by Eve addition: 49.539620535714285%
Eve decrypted mulitplication: [[0.89328426 1.084846   1.0238276  ... 0.98398703 1.0779175  0.90770364]
 [0.926987   1.1194974  1.0028192  ... 0.96609133 1.0853668  0.9174186 ]
 [0.9344549  1.0306795  1.0312731  ... 1.0202429  1.0323464  0.9501265 ]
 ...
 [0.8913668  1.13002    0.99227405 ... 0.98977995 1.0666915  0.91774505]
 [0.89116126 1.1005057  1.0418831  ... 0.9832885  1.0651072  0.9208107 ]
 [0.933731   1.0796809  1.0217469  ... 0.95825326 1.0759546  0.9150872 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 1818
Total number of bits mulitplication: 7168
Decryption accuracy by Eve mulitplication: 25.362723214285715%
Bob decrypted P1: [[0.75790405 0.76194435 0.8375985  ... 0.8447938  0.900456   0.8007407 ]
 [0.76339424 0.7845237  0.9227871  ... 0.8981174  0.94804966 0.77337545]
 [0.78508425 0.8411491  0.8128449  ... 0.95598763 0.84841174 0.8847431 ]
 ...
 [0.7962944  0.81173754 0.7896039  ... 0.830602   0.91514164 0.77937996]
 [0.75676304 0.8093284  0.76154417 ... 0.83060265 0.80146486 0.9085709 ]
 [0.76097697 0.76501817 0.94442487 ... 0.869354   0.85747105 0.8610156 ]]
Bob decrypted bits P1: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P1: 3593
Total number of bits P1: 7168
Decryption accuracy P1: 50.12555803571429%
Bob decrypted P2: [[0.7616709  0.765506   0.82333446 ... 0.8261596  0.9087626  0.80300796]
 [0.77001655 0.78602606 0.90917957 ... 0.8842884  0.9543756  0.7708611 ]
 [0.7912513  0.83143073 0.8102792  ... 0.95058244 0.8535183  0.8830954 ]
 ...
 [0.8174185  0.8273558  0.7858578  ... 0.8367367  0.93296444 0.782748  ]
 [0.75847816 0.8081313  0.759918   ... 0.81655097 0.8109925  0.92407036]
 [0.76471496 0.76619995 0.93930584 ... 0.8609151  0.885981   0.8785857 ]]
Bob decrypted bits P2: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P2: 3594
Total number of bits P2: 7168
Decryption accuracy P2: 50.13950892857143%
