WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-17 11:30:01.906468: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-17 11:30:02.035822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-17 11:30:02.036656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:02.039861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-17 11:30:02.042668: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-17 11:30:02.044702: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-17 11:30:02.047608: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-17 11:30:02.050727: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-17 11:30:02.057998: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-17 11:30:02.065890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-17 11:30:02.066326: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-17 11:30:02.086620: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-17 11:30:02.089844: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3c667b0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-17 11:30:02.089908: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-17 11:30:02.356075: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x16ef9e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-17 11:30:02.356185: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-17 11:30:02.361162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-17 11:30:02.361286: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:02.361325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-17 11:30:02.361381: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-17 11:30:02.361413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-17 11:30:02.361456: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-17 11:30:02.361487: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-17 11:30:02.361523: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-17 11:30:02.373138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-17 11:30:02.373298: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:02.376602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-17 11:30:02.376625: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-17 11:30:02.376634: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-17 11:30:02.382381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-17 11:30:05.954869: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9229 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4586 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1466 - val_loss: 3.9179e-04
Epoch 4/512
448/448 - 0s - loss: 0.0224 - val_loss: 4.0177e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5982e-06
Epoch 6/512
448/448 - 0s - loss: 7.4248e-04 - val_loss: 5.8806e-06
Epoch 7/512
448/448 - 0s - loss: 5.4002e-04 - val_loss: 4.4272e-06
Epoch 8/512
448/448 - 0s - loss: 3.9958e-04 - val_loss: 3.1254e-06
Epoch 9/512
448/448 - 0s - loss: 2.7541e-04 - val_loss: 2.0253e-06
Epoch 10/512
448/448 - 0s - loss: 1.7333e-04 - val_loss: 1.1801e-06
Epoch 11/512
448/448 - 0s - loss: 9.7418e-05 - val_loss: 6.0066e-07
Epoch 12/512
448/448 - 0s - loss: 4.7450e-05 - val_loss: 2.5781e-07
Epoch 13/512
448/448 - 0s - loss: 1.9310e-05 - val_loss: 8.9180e-08
Epoch 14/512
448/448 - 0s - loss: 6.2672e-06 - val_loss: 2.3621e-08
Epoch 15/512
448/448 - 0s - loss: 1.5985e-06 - val_loss: 1.2279e-08
Epoch 16/512
448/448 - 0s - loss: 1.6245e-05 - val_loss: 4.5125e-06
Epoch 17/512
448/448 - 0s - loss: 0.0035 - val_loss: 2.7922e-05
Epoch 18/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.4468e-06
Epoch 19/512
448/448 - 0s - loss: 1.2374e-04 - val_loss: 1.7471e-06
Epoch 20/512
448/448 - 0s - loss: 4.1824e-04 - val_loss: 1.9251e-05
Epoch 21/512
448/448 - 0s - loss: 0.0026 - val_loss: 1.5529e-05
Epoch 22/512
448/448 - 0s - loss: 9.5004e-04 - val_loss: 4.3119e-06
Epoch 23/512
448/448 - 0s - loss: 4.6574e-04 - val_loss: 8.2900e-06
Epoch 24/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.9556e-05
Epoch 25/512
448/448 - 0s - loss: 0.0016 - val_loss: 8.5438e-06
Epoch 26/512
448/448 - 0s - loss: 7.2597e-04 - val_loss: 7.2754e-06
Epoch 27/512
448/448 - 0s - loss: 8.7560e-04 - val_loss: 1.3615e-05
Epoch 28/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.1369e-05
Epoch 29/512
448/448 - 0s - loss: 9.5472e-04 - val_loss: 8.2460e-06
Epoch 30/512
448/448 - 0s - loss: 8.5293e-04 - val_loss: 1.0286e-05
Epoch 31/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1748e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 9.2363e-06
Epoch 33/512
448/448 - 0s - loss: 8.7803e-04 - val_loss: 8.9202e-06
Epoch 34/512
448/448 - 0s - loss: 9.2518e-04 - val_loss: 1.0578e-05
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 9.6823e-06
Epoch 36/512
448/448 - 0s - loss: 9.0617e-04 - val_loss: 8.8603e-06
Epoch 37/512
448/448 - 0s - loss: 8.8126e-04 - val_loss: 9.5189e-06
Epoch 38/512
448/448 - 0s - loss: 9.4261e-04 - val_loss: 9.5663e-06
Epoch 39/512
448/448 - 0s - loss: 9.0979e-04 - val_loss: 8.8396e-06
Epoch 40/512
448/448 - 0s - loss: 8.5872e-04 - val_loss: 8.8528e-06
Epoch 41/512
448/448 - 0s - loss: 8.8065e-04 - val_loss: 9.0573e-06
Epoch 42/512
448/448 - 0s - loss: 8.8076e-04 - val_loss: 8.6445e-06
Epoch 43/512
448/448 - 0s - loss: 8.3316e-04 - val_loss: 8.7598e-06
Epoch 44/512
448/448 - 0s - loss: 8.5938e-04 - val_loss: 8.5423e-06
Epoch 45/512
448/448 - 0s - loss: 8.2989e-04 - val_loss: 8.2576e-06
Epoch 46/512
448/448 - 0s - loss: 8.1014e-04 - val_loss: 8.4690e-06
Epoch 47/512
448/448 - 0s - loss: 8.3240e-04 - val_loss: 8.2439e-06
Epoch 48/512
448/448 - 0s - loss: 8.0030e-04 - val_loss: 7.9873e-06
Epoch 49/512
448/448 - 0s - loss: 7.8977e-04 - val_loss: 7.9357e-06
Epoch 50/512
448/448 - 0s - loss: 7.8223e-04 - val_loss: 8.0209e-06
Epoch 51/512
448/448 - 0s - loss: 7.8736e-04 - val_loss: 7.8075e-06
Epoch 52/512
448/448 - 0s - loss: 7.5967e-04 - val_loss: 7.7217e-06
Epoch 53/512
448/448 - 0s - loss: 7.5889e-04 - val_loss: 7.7240e-06
Epoch 54/512
448/448 - 0s - loss: 7.5898e-04 - val_loss: 7.4597e-06
Epoch 55/512
448/448 - 0s - loss: 7.3090e-04 - val_loss: 7.5106e-06
Epoch 56/512
448/448 - 0s - loss: 7.4108e-04 - val_loss: 7.4410e-06
Epoch 57/512
448/448 - 0s - loss: 7.2500e-04 - val_loss: 7.3656e-06
Epoch 58/512
448/448 - 0s - loss: 7.2868e-04 - val_loss: 6.9929e-06
Epoch 59/512
448/448 - 0s - loss: 6.9365e-04 - val_loss: 7.0199e-06
Epoch 60/512
448/448 - 0s - loss: 7.0221e-04 - val_loss: 7.2760e-06
Epoch 61/512
448/448 - 0s - loss: 7.1456e-04 - val_loss: 6.9252e-06
Epoch 62/512
448/448 - 0s - loss: 6.7607e-04 - val_loss: 6.7762e-06
Epoch 63/512
448/448 - 0s - loss: 6.7299e-04 - val_loss: 7.0431e-06
Epoch 64/512
448/448 - 0s - loss: 6.9242e-04 - val_loss: 6.8267e-06
Epoch 65/512
448/448 - 0s - loss: 6.6480e-04 - val_loss: 6.4752e-06
Epoch 66/512
448/448 - 0s - loss: 6.4639e-04 - val_loss: 6.5704e-06
Epoch 67/512
448/448 - 0s - loss: 6.5143e-04 - val_loss: 6.8802e-06
Epoch 68/512
448/448 - 0s - loss: 6.7250e-04 - val_loss: 6.3638e-06
Epoch 69/512
448/448 - 0s - loss: 6.2059e-04 - val_loss: 6.2547e-06
Epoch 70/512
448/448 - 0s - loss: 6.2896e-04 - val_loss: 6.5570e-06
Epoch 71/512
448/448 - 0s - loss: 6.4732e-04 - val_loss: 6.2697e-06
Epoch 72/512
448/448 - 0s - loss: 6.0930e-04 - val_loss: 6.1531e-06
Epoch 73/512
448/448 - 0s - loss: 6.1238e-04 - val_loss: 6.3004e-06
Epoch 74/512
448/448 - 0s - loss: 6.1997e-04 - val_loss: 6.0897e-06
Epoch 75/512
448/448 - 0s - loss: 5.9740e-04 - val_loss: 5.9562e-06
Epoch 76/512
448/448 - 0s - loss: 5.9486e-04 - val_loss: 5.9851e-06
Epoch 77/512
448/448 - 0s - loss: 5.9125e-04 - val_loss: 6.0646e-06
Epoch 78/512
448/448 - 0s - loss: 5.9805e-04 - val_loss: 5.8083e-06
Epoch 79/512
448/448 - 0s - loss: 5.7235e-04 - val_loss: 5.7190e-06
Epoch 80/512
448/448 - 0s - loss: 5.7108e-04 - val_loss: 5.8830e-06
Epoch 81/512
448/448 - 0s - loss: 5.8142e-04 - val_loss: 5.7354e-06
Epoch 82/512
448/448 - 0s - loss: 5.5895e-04 - val_loss: 5.6562e-06
Epoch 83/512
448/448 - 0s - loss: 5.5982e-04 - val_loss: 5.6161e-06
Epoch 84/512
448/448 - 0s - loss: 5.5438e-04 - val_loss: 5.5986e-06
Epoch 85/512
448/448 - 0s - loss: 5.5500e-04 - val_loss: 5.3703e-06
Epoch 86/512
448/448 - 0s - loss: 5.3272e-04 - val_loss: 5.4406e-06
Epoch 87/512
448/448 - 0s - loss: 5.4382e-04 - val_loss: 5.5401e-06
Epoch 88/512
448/448 - 0s - loss: 5.4449e-04 - val_loss: 5.2750e-06
Epoch 89/512
448/448 - 0s - loss: 5.2126e-04 - val_loss: 5.1537e-06
Epoch 90/512
448/448 - 0s - loss: 5.2119e-04 - val_loss: 5.2496e-06
Epoch 91/512
448/448 - 0s - loss: 5.2363e-04 - val_loss: 5.2860e-06
Epoch 92/512
448/448 - 0s - loss: 5.2020e-04 - val_loss: 5.1048e-06
Epoch 93/512
448/448 - 0s - loss: 5.0491e-04 - val_loss: 5.0181e-06
Epoch 94/512
448/448 - 0s - loss: 5.0361e-04 - val_loss: 5.0644e-06
Epoch 95/512
448/448 - 0s - loss: 5.0669e-04 - val_loss: 4.9766e-06
Epoch 96/512
448/448 - 0s - loss: 4.9116e-04 - val_loss: 4.9871e-06
Epoch 97/512
448/448 - 0s - loss: 4.9488e-04 - val_loss: 4.9698e-06
Epoch 98/512
448/448 - 0s - loss: 4.9002e-04 - val_loss: 4.8530e-06
Epoch 99/512
448/448 - 0s - loss: 4.8304e-04 - val_loss: 4.7672e-06
Epoch 100/512
448/448 - 0s - loss: 4.7655e-04 - val_loss: 4.7383e-06
Epoch 101/512
448/448 - 0s - loss: 4.7433e-04 - val_loss: 4.7957e-06
Epoch 102/512
448/448 - 0s - loss: 4.7636e-04 - val_loss: 4.6939e-06
Epoch 103/512
448/448 - 0s - loss: 4.6368e-04 - val_loss: 4.6266e-06
Epoch 104/512
448/448 - 0s - loss: 4.6026e-04 - val_loss: 4.6579e-06
Epoch 105/512
448/448 - 0s - loss: 4.6354e-04 - val_loss: 4.5325e-06
Epoch 106/512
448/448 - 0s - loss: 4.5024e-04 - val_loss: 4.4824e-06
Epoch 107/512
448/448 - 0s - loss: 4.4883e-04 - val_loss: 4.5299e-06
Epoch 108/512
448/448 - 0s - loss: 4.4898e-04 - val_loss: 4.4742e-06
Epoch 109/512
448/448 - 0s - loss: 4.4355e-04 - val_loss: 4.3432e-06
Epoch 110/512
448/448 - 0s - loss: 4.3402e-04 - val_loss: 4.3367e-06
Epoch 111/512
448/448 - 0s - loss: 4.3689e-04 - val_loss: 4.2938e-06
Epoch 112/512
448/448 - 0s - loss: 4.2886e-04 - val_loss: 4.2617e-06
Epoch 113/512
448/448 - 0s - loss: 4.2455e-04 - val_loss: 4.3062e-06
Epoch 114/512
448/448 - 0s - loss: 4.2925e-04 - val_loss: 4.1744e-06
Epoch 115/512
448/448 - 0s - loss: 4.1260e-04 - val_loss: 4.1385e-06
Epoch 116/512
448/448 - 0s - loss: 4.1377e-04 - val_loss: 4.2140e-06
Epoch 117/512
448/448 - 0s - loss: 4.1723e-04 - val_loss: 4.0934e-06
Epoch 118/512
448/448 - 0s - loss: 4.0357e-04 - val_loss: 4.0649e-06
Epoch 119/512
448/448 - 0s - loss: 4.0437e-04 - val_loss: 4.0600e-06
Epoch 120/512
448/448 - 0s - loss: 4.0388e-04 - val_loss: 3.9144e-06
Epoch 121/512
448/448 - 0s - loss: 3.9100e-04 - val_loss: 3.9084e-06
Epoch 122/512
448/448 - 0s - loss: 3.9410e-04 - val_loss: 3.9236e-06
Epoch 123/512
448/448 - 0s - loss: 3.8857e-04 - val_loss: 3.9564e-06
Epoch 124/512
448/448 - 0s - loss: 3.9216e-04 - val_loss: 3.8013e-06
Epoch 125/512
448/448 - 0s - loss: 3.7637e-04 - val_loss: 3.8070e-06
Epoch 126/512
448/448 - 0s - loss: 3.8129e-04 - val_loss: 3.7822e-06
Epoch 127/512
448/448 - 0s - loss: 3.7666e-04 - val_loss: 3.6973e-06
Epoch 128/512
448/448 - 0s - loss: 3.6951e-04 - val_loss: 3.7034e-06
Epoch 129/512
448/448 - 0s - loss: 3.7037e-04 - val_loss: 3.6855e-06
Epoch 130/512
448/448 - 0s - loss: 3.6516e-04 - val_loss: 3.6582e-06
Epoch 131/512
448/448 - 0s - loss: 3.6293e-04 - val_loss: 3.6257e-06
Epoch 132/512
448/448 - 0s - loss: 3.6188e-04 - val_loss: 3.4657e-06
Epoch 133/512
448/448 - 0s - loss: 3.4539e-04 - val_loss: 3.5741e-06
Epoch 134/512
448/448 - 0s - loss: 3.5900e-04 - val_loss: 3.5735e-06
Epoch 135/512
448/448 - 0s - loss: 3.5482e-04 - val_loss: 3.2662e-06
Epoch 136/512
448/448 - 0s - loss: 3.2786e-04 - val_loss: 3.4752e-06
Epoch 137/512
448/448 - 0s - loss: 3.5298e-04 - val_loss: 3.5109e-06
Epoch 138/512
448/448 - 0s - loss: 3.4436e-04 - val_loss: 3.2565e-06
Epoch 139/512
448/448 - 0s - loss: 3.2384e-04 - val_loss: 3.3236e-06
Epoch 140/512
448/448 - 0s - loss: 3.3701e-04 - val_loss: 3.3814e-06
Epoch 141/512
448/448 - 0s - loss: 3.3151e-04 - val_loss: 3.3146e-06
Epoch 142/512
448/448 - 0s - loss: 3.2922e-04 - val_loss: 3.1116e-06
Epoch 143/512
448/448 - 0s - loss: 3.1204e-04 - val_loss: 3.2389e-06
Epoch 144/512
448/448 - 0s - loss: 3.2484e-04 - val_loss: 3.3403e-06
Epoch 145/512
448/448 - 0s - loss: 3.2698e-04 - val_loss: 3.0659e-06
Epoch 146/512
448/448 - 0s - loss: 3.0274e-04 - val_loss: 3.0315e-06
Epoch 147/512
448/448 - 0s - loss: 3.0847e-04 - val_loss: 3.2011e-06
Epoch 148/512
448/448 - 0s - loss: 3.1821e-04 - val_loss: 3.0749e-06
Epoch 149/512
448/448 - 0s - loss: 3.0089e-04 - val_loss: 2.9451e-06
Epoch 150/512
448/448 - 0s - loss: 2.9580e-04 - val_loss: 3.0541e-06
Epoch 151/512
448/448 - 0s - loss: 3.0716e-04 - val_loss: 2.9618e-06
Epoch 152/512
448/448 - 0s - loss: 2.9338e-04 - val_loss: 2.8502e-06
Epoch 153/512
448/448 - 0s - loss: 2.8684e-04 - val_loss: 2.9759e-06
Epoch 154/512
448/448 - 0s - loss: 2.9761e-04 - val_loss: 2.9401e-06
Epoch 155/512
448/448 - 0s - loss: 2.8818e-04 - val_loss: 2.7999e-06
Epoch 156/512
448/448 - 0s - loss: 2.8157e-04 - val_loss: 2.7389e-06
Epoch 157/512
448/448 - 0s - loss: 2.7701e-04 - val_loss: 2.8452e-06
Epoch 158/512
448/448 - 0s - loss: 2.8454e-04 - val_loss: 2.7810e-06
Epoch 159/512
448/448 - 0s - loss: 2.7640e-04 - val_loss: 2.6755e-06
Epoch 160/512
448/448 - 0s - loss: 2.7013e-04 - val_loss: 2.6456e-06
Epoch 161/512
448/448 - 0s - loss: 2.6655e-04 - val_loss: 2.7550e-06
Epoch 162/512
448/448 - 0s - loss: 2.7501e-04 - val_loss: 2.6886e-06
Epoch 163/512
448/448 - 0s - loss: 2.6396e-04 - val_loss: 2.5760e-06
Epoch 164/512
448/448 - 0s - loss: 2.5757e-04 - val_loss: 2.6236e-06
Epoch 165/512
448/448 - 0s - loss: 2.6309e-04 - val_loss: 2.6082e-06
Epoch 166/512
448/448 - 0s - loss: 2.5942e-04 - val_loss: 2.4599e-06
Epoch 167/512
448/448 - 0s - loss: 2.4624e-04 - val_loss: 2.5272e-06
Epoch 168/512
448/448 - 0s - loss: 2.5345e-04 - val_loss: 2.5990e-06
Epoch 169/512
448/448 - 0s - loss: 2.5475e-04 - val_loss: 2.4484e-06
Epoch 170/512
448/448 - 0s - loss: 2.4240e-04 - val_loss: 2.3501e-06
Epoch 171/512
448/448 - 0s - loss: 2.3790e-04 - val_loss: 2.4565e-06
Epoch 172/512
448/448 - 0s - loss: 2.4571e-04 - val_loss: 2.4615e-06
Epoch 173/512
448/448 - 0s - loss: 2.4188e-04 - val_loss: 2.3176e-06
Epoch 174/512
448/448 - 0s - loss: 2.2910e-04 - val_loss: 2.3380e-06
Epoch 175/512
448/448 - 0s - loss: 2.3459e-04 - val_loss: 2.3603e-06
Epoch 176/512
448/448 - 0s - loss: 2.3437e-04 - val_loss: 2.2389e-06
Epoch 177/512
448/448 - 0s - loss: 2.2289e-04 - val_loss: 2.2346e-06
Epoch 178/512
448/448 - 0s - loss: 2.2460e-04 - val_loss: 2.2972e-06
Epoch 179/512
448/448 - 0s - loss: 2.2871e-04 - val_loss: 2.1917e-06
Epoch 180/512
448/448 - 0s - loss: 2.1613e-04 - val_loss: 2.1369e-06
Epoch 181/512
448/448 - 0s - loss: 2.1587e-04 - val_loss: 2.1828e-06
Epoch 182/512
448/448 - 0s - loss: 2.1841e-04 - val_loss: 2.1377e-06
Epoch 183/512
448/448 - 0s - loss: 2.1107e-04 - val_loss: 2.1254e-06
Epoch 184/512
448/448 - 0s - loss: 2.1085e-04 - val_loss: 2.1256e-06
Epoch 185/512
448/448 - 0s - loss: 2.1055e-04 - val_loss: 2.0387e-06
Epoch 186/512
448/448 - 0s - loss: 2.0475e-04 - val_loss: 1.9688e-06
Epoch 187/512
448/448 - 0s - loss: 1.9912e-04 - val_loss: 2.0445e-06
Epoch 188/512
448/448 - 0s - loss: 2.0639e-04 - val_loss: 1.9753e-06
Epoch 189/512
448/448 - 0s - loss: 1.9524e-04 - val_loss: 1.9618e-06
Epoch 190/512
448/448 - 0s - loss: 1.9717e-04 - val_loss: 1.9276e-06
Epoch 191/512
448/448 - 0s - loss: 1.9300e-04 - val_loss: 1.9029e-06
Epoch 192/512
448/448 - 0s - loss: 1.9112e-04 - val_loss: 1.8846e-06
Epoch 193/512
448/448 - 0s - loss: 1.8667e-04 - val_loss: 1.9310e-06
Epoch 194/512
448/448 - 0s - loss: 1.9168e-04 - val_loss: 1.8276e-06
Epoch 195/512
448/448 - 0s - loss: 1.8166e-04 - val_loss: 1.7665e-06
Epoch 196/512
448/448 - 0s - loss: 1.7895e-04 - val_loss: 1.8317e-06
Epoch 197/512
448/448 - 0s - loss: 1.8347e-04 - val_loss: 1.8018e-06
Epoch 198/512
448/448 - 0s - loss: 1.7888e-04 - val_loss: 1.6844e-06
Epoch 199/512
448/448 - 0s - loss: 1.6870e-04 - val_loss: 1.7452e-06
Epoch 200/512
448/448 - 0s - loss: 1.7597e-04 - val_loss: 1.7923e-06
Epoch 201/512
448/448 - 0s - loss: 1.7634e-04 - val_loss: 1.6172e-06
Epoch 202/512
448/448 - 0s - loss: 1.6053e-04 - val_loss: 1.6282e-06
Epoch 203/512
448/448 - 0s - loss: 1.6736e-04 - val_loss: 1.6822e-06
Epoch 204/512
448/448 - 0s - loss: 1.6782e-04 - val_loss: 1.5983e-06
Epoch 205/512
448/448 - 0s - loss: 1.5923e-04 - val_loss: 1.5571e-06
Epoch 206/512
448/448 - 0s - loss: 1.5778e-04 - val_loss: 1.5837e-06
Epoch 207/512
448/448 - 0s - loss: 1.5930e-04 - val_loss: 1.5768e-06
Epoch 208/512
448/448 - 0s - loss: 1.5761e-04 - val_loss: 1.4790e-06
Epoch 209/512
448/448 - 0s - loss: 1.4897e-04 - val_loss: 1.4813e-06
Epoch 210/512
448/448 - 0s - loss: 1.5072e-04 - val_loss: 1.5266e-06
Epoch 211/512
448/448 - 0s - loss: 1.5258e-04 - val_loss: 1.4464e-06
Epoch 212/512
448/448 - 0s - loss: 1.4350e-04 - val_loss: 1.4193e-06
Epoch 213/512
448/448 - 0s - loss: 1.4285e-04 - val_loss: 1.4806e-06
Epoch 214/512
448/448 - 0s - loss: 1.4801e-04 - val_loss: 1.3829e-06
Epoch 215/512
448/448 - 0s - loss: 1.3643e-04 - val_loss: 1.3456e-06
Epoch 216/512
448/448 - 0s - loss: 1.3764e-04 - val_loss: 1.3686e-06
Epoch 217/512
448/448 - 0s - loss: 1.3733e-04 - val_loss: 1.3559e-06
Epoch 218/512
448/448 - 0s - loss: 1.3540e-04 - val_loss: 1.2821e-06
Epoch 219/512
448/448 - 0s - loss: 1.2869e-04 - val_loss: 1.3091e-06
Epoch 220/512
448/448 - 0s - loss: 1.3241e-04 - val_loss: 1.2767e-06
Epoch 221/512
448/448 - 0s - loss: 1.2702e-04 - val_loss: 1.2501e-06
Epoch 222/512
448/448 - 0s - loss: 1.2549e-04 - val_loss: 1.2535e-06
Epoch 223/512
448/448 - 0s - loss: 1.2606e-04 - val_loss: 1.1920e-06
Epoch 224/512
448/448 - 0s - loss: 1.1916e-04 - val_loss: 1.1894e-06
Epoch 225/512
448/448 - 0s - loss: 1.2024e-04 - val_loss: 1.2236e-06
Epoch 226/512
448/448 - 0s - loss: 1.2166e-04 - val_loss: 1.1341e-06
Epoch 227/512
448/448 - 0s - loss: 1.1210e-04 - val_loss: 1.1195e-06
Epoch 228/512
448/448 - 0s - loss: 1.1409e-04 - val_loss: 1.1421e-06
Epoch 229/512
448/448 - 0s - loss: 1.1449e-04 - val_loss: 1.0884e-06
Epoch 230/512
448/448 - 0s - loss: 1.0816e-04 - val_loss: 1.0834e-06
Epoch 231/512
448/448 - 0s - loss: 1.0903e-04 - val_loss: 1.0831e-06
Epoch 232/512
448/448 - 0s - loss: 1.0840e-04 - val_loss: 1.0201e-06
Epoch 233/512
448/448 - 0s - loss: 1.0195e-04 - val_loss: 1.0199e-06
Epoch 234/512
448/448 - 0s - loss: 1.0328e-04 - val_loss: 1.0390e-06
Epoch 235/512
448/448 - 0s - loss: 1.0245e-04 - val_loss: 1.0083e-06
Epoch 236/512
448/448 - 0s - loss: 9.9666e-05 - val_loss: 9.4418e-07
Epoch 237/512
448/448 - 0s - loss: 9.4852e-05 - val_loss: 9.4635e-07
Epoch 238/512
448/448 - 0s - loss: 9.6567e-05 - val_loss: 9.4004e-07
Epoch 239/512
448/448 - 0s - loss: 9.4395e-05 - val_loss: 9.0499e-07
Epoch 240/512
448/448 - 0s - loss: 9.0653e-05 - val_loss: 9.0414e-07
Epoch 241/512
448/448 - 0s - loss: 9.0860e-05 - val_loss: 9.0978e-07
Epoch 242/512
448/448 - 0s - loss: 8.9999e-05 - val_loss: 8.5904e-07
Epoch 243/512
448/448 - 0s - loss: 8.5767e-05 - val_loss: 8.2966e-07
Epoch 244/512
448/448 - 0s - loss: 8.4463e-05 - val_loss: 8.3096e-07
Epoch 245/512
448/448 - 0s - loss: 8.4167e-05 - val_loss: 8.1830e-07
Epoch 246/512
448/448 - 0s - loss: 8.2583e-05 - val_loss: 7.7504e-07
Epoch 247/512
448/448 - 0s - loss: 7.8871e-05 - val_loss: 7.6242e-07
Epoch 248/512
448/448 - 0s - loss: 7.7716e-05 - val_loss: 7.8847e-07
Epoch 249/512
448/448 - 0s - loss: 7.9025e-05 - val_loss: 7.5386e-07
Epoch 250/512
448/448 - 0s - loss: 7.4074e-05 - val_loss: 7.4080e-07
Epoch 251/512
448/448 - 0s - loss: 7.4406e-05 - val_loss: 7.1728e-07
Epoch 252/512
448/448 - 0s - loss: 7.1835e-05 - val_loss: 6.9652e-07
Epoch 253/512
448/448 - 0s - loss: 7.0058e-05 - val_loss: 7.0086e-07
Epoch 254/512
448/448 - 0s - loss: 7.0292e-05 - val_loss: 6.7805e-07
Epoch 255/512
448/448 - 0s - loss: 6.7499e-05 - val_loss: 6.4977e-07
Epoch 256/512
448/448 - 0s - loss: 6.5557e-05 - val_loss: 6.4078e-07
Epoch 257/512
448/448 - 0s - loss: 6.4827e-05 - val_loss: 6.3603e-07
Epoch 258/512
448/448 - 0s - loss: 6.3977e-05 - val_loss: 6.0494e-07
Epoch 259/512
448/448 - 0s - loss: 6.0620e-05 - val_loss: 6.0508e-07
Epoch 260/512
448/448 - 0s - loss: 6.1106e-05 - val_loss: 5.9225e-07
Epoch 261/512
448/448 - 0s - loss: 5.9442e-05 - val_loss: 5.6184e-07
Epoch 262/512
448/448 - 0s - loss: 5.6772e-05 - val_loss: 5.5483e-07
Epoch 263/512
448/448 - 0s - loss: 5.6450e-05 - val_loss: 5.5699e-07
Epoch 264/512
448/448 - 0s - loss: 5.6193e-05 - val_loss: 5.2071e-07
Epoch 265/512
448/448 - 0s - loss: 5.2489e-05 - val_loss: 5.0782e-07
Epoch 266/512
448/448 - 0s - loss: 5.1906e-05 - val_loss: 5.2688e-07
Epoch 267/512
448/448 - 0s - loss: 5.2884e-05 - val_loss: 5.0068e-07
Epoch 268/512
448/448 - 0s - loss: 4.9390e-05 - val_loss: 4.6765e-07
Epoch 269/512
448/448 - 0s - loss: 4.7886e-05 - val_loss: 4.6177e-07
Epoch 270/512
448/448 - 0s - loss: 4.7196e-05 - val_loss: 4.7597e-07
Epoch 271/512
448/448 - 0s - loss: 4.7485e-05 - val_loss: 4.5679e-07
Epoch 272/512
448/448 - 0s - loss: 4.5274e-05 - val_loss: 4.2026e-07
Epoch 273/512
448/448 - 0s - loss: 4.2706e-05 - val_loss: 4.1867e-07
Epoch 274/512
448/448 - 0s - loss: 4.2838e-05 - val_loss: 4.3392e-07
Epoch 275/512
448/448 - 0s - loss: 4.3297e-05 - val_loss: 4.0062e-07
Epoch 276/512
448/448 - 0s - loss: 3.9719e-05 - val_loss: 3.7798e-07
Epoch 277/512
448/448 - 0s - loss: 3.8860e-05 - val_loss: 3.8462e-07
Epoch 278/512
448/448 - 0s - loss: 3.9303e-05 - val_loss: 3.7734e-07
Epoch 279/512
448/448 - 0s - loss: 3.7776e-05 - val_loss: 3.5449e-07
Epoch 280/512
448/448 - 0s - loss: 3.5685e-05 - val_loss: 3.4920e-07
Epoch 281/512
448/448 - 0s - loss: 3.5496e-05 - val_loss: 3.4623e-07
Epoch 282/512
448/448 - 0s - loss: 3.4875e-05 - val_loss: 3.2963e-07
Epoch 283/512
448/448 - 0s - loss: 3.3120e-05 - val_loss: 3.2272e-07
Epoch 284/512
448/448 - 0s - loss: 3.2633e-05 - val_loss: 3.1973e-07
Epoch 285/512
448/448 - 0s - loss: 3.2054e-05 - val_loss: 3.0708e-07
Epoch 286/512
448/448 - 0s - loss: 3.0955e-05 - val_loss: 2.8380e-07
Epoch 287/512
448/448 - 0s - loss: 2.8758e-05 - val_loss: 2.9133e-07
Epoch 288/512
448/448 - 0s - loss: 2.9912e-05 - val_loss: 2.8401e-07
Epoch 289/512
448/448 - 0s - loss: 2.8067e-05 - val_loss: 2.7153e-07
Epoch 290/512
448/448 - 0s - loss: 2.7473e-05 - val_loss: 2.6069e-07
Epoch 291/512
448/448 - 0s - loss: 2.6476e-05 - val_loss: 2.4892e-07
Epoch 292/512
448/448 - 0s - loss: 2.5367e-05 - val_loss: 2.5122e-07
Epoch 293/512
448/448 - 0s - loss: 2.5764e-05 - val_loss: 2.3867e-07
Epoch 294/512
448/448 - 0s - loss: 2.3958e-05 - val_loss: 2.2697e-07
Epoch 295/512
448/448 - 0s - loss: 2.3284e-05 - val_loss: 2.2443e-07
Epoch 296/512
448/448 - 0s - loss: 2.2902e-05 - val_loss: 2.2137e-07
Epoch 297/512
448/448 - 0s - loss: 2.2370e-05 - val_loss: 2.0839e-07
Epoch 298/512
448/448 - 0s - loss: 2.1184e-05 - val_loss: 1.9953e-07
Epoch 299/512
448/448 - 0s - loss: 2.0561e-05 - val_loss: 1.9915e-07
Epoch 300/512
448/448 - 0s - loss: 2.0162e-05 - val_loss: 1.9881e-07
Epoch 301/512
448/448 - 0s - loss: 2.0000e-05 - val_loss: 1.8356e-07
Epoch 302/512
448/448 - 0s - loss: 1.8451e-05 - val_loss: 1.7638e-07
Epoch 303/512
448/448 - 0s - loss: 1.7975e-05 - val_loss: 1.8274e-07
Epoch 304/512
448/448 - 0s - loss: 1.8587e-05 - val_loss: 1.6509e-07
Epoch 305/512
448/448 - 0s - loss: 1.6591e-05 - val_loss: 1.5265e-07
Epoch 306/512
448/448 - 0s - loss: 1.6018e-05 - val_loss: 1.6161e-07
Epoch 307/512
448/448 - 0s - loss: 1.6718e-05 - val_loss: 1.5672e-07
Epoch 308/512
448/448 - 0s - loss: 1.5652e-05 - val_loss: 1.3831e-07
Epoch 309/512
448/448 - 0s - loss: 1.4088e-05 - val_loss: 1.4342e-07
Epoch 310/512
448/448 - 0s - loss: 1.5041e-05 - val_loss: 1.4264e-07
Epoch 311/512
448/448 - 0s - loss: 1.4168e-05 - val_loss: 1.3139e-07
Epoch 312/512
448/448 - 0s - loss: 1.3263e-05 - val_loss: 1.2519e-07
Epoch 313/512
448/448 - 0s - loss: 1.2970e-05 - val_loss: 1.2443e-07
Epoch 314/512
448/448 - 0s - loss: 1.2753e-05 - val_loss: 1.1913e-07
Epoch 315/512
448/448 - 0s - loss: 1.2031e-05 - val_loss: 1.1636e-07
Epoch 316/512
448/448 - 0s - loss: 1.1906e-05 - val_loss: 1.1085e-07
Epoch 317/512
448/448 - 0s - loss: 1.1227e-05 - val_loss: 1.0731e-07
Epoch 318/512
448/448 - 0s - loss: 1.1029e-05 - val_loss: 1.0269e-07
Epoch 319/512
448/448 - 0s - loss: 1.0527e-05 - val_loss: 9.9793e-08
Epoch 320/512
448/448 - 0s - loss: 1.0193e-05 - val_loss: 9.5661e-08
Epoch 321/512
448/448 - 0s - loss: 9.7914e-06 - val_loss: 9.3033e-08
Epoch 322/512
448/448 - 0s - loss: 9.5452e-06 - val_loss: 8.9202e-08
Epoch 323/512
448/448 - 0s - loss: 9.0786e-06 - val_loss: 8.7411e-08
Epoch 324/512
448/448 - 0s - loss: 8.9214e-06 - val_loss: 8.3596e-08
Epoch 325/512
448/448 - 0s - loss: 8.5023e-06 - val_loss: 7.8757e-08
Epoch 326/512
448/448 - 0s - loss: 8.1042e-06 - val_loss: 7.6664e-08
Epoch 327/512
448/448 - 0s - loss: 7.8764e-06 - val_loss: 7.6580e-08
Epoch 328/512
448/448 - 0s - loss: 7.7976e-06 - val_loss: 7.0449e-08
Epoch 329/512
448/448 - 0s - loss: 7.2081e-06 - val_loss: 6.5724e-08
Epoch 330/512
448/448 - 0s - loss: 6.8711e-06 - val_loss: 6.7102e-08
Epoch 331/512
448/448 - 0s - loss: 6.9877e-06 - val_loss: 6.4351e-08
Epoch 332/512
448/448 - 0s - loss: 6.5495e-06 - val_loss: 5.9344e-08
Epoch 333/512
448/448 - 0s - loss: 6.0581e-06 - val_loss: 6.1655e-08
Epoch 334/512
448/448 - 0s - loss: 6.3894e-06 - val_loss: 5.6488e-08
Epoch 335/512
448/448 - 0s - loss: 5.6806e-06 - val_loss: 5.2827e-08
Epoch 336/512
448/448 - 0s - loss: 5.4836e-06 - val_loss: 5.4097e-08
Epoch 337/512
448/448 - 0s - loss: 5.5615e-06 - val_loss: 5.2483e-08
Epoch 338/512
448/448 - 0s - loss: 5.2956e-06 - val_loss: 4.6043e-08
Epoch 339/512
448/448 - 0s - loss: 4.7264e-06 - val_loss: 4.5985e-08
Epoch 340/512
448/448 - 0s - loss: 4.8162e-06 - val_loss: 4.8172e-08
Epoch 341/512
448/448 - 0s - loss: 4.8554e-06 - val_loss: 4.2995e-08
Epoch 342/512
448/448 - 0s - loss: 4.2854e-06 - val_loss: 3.9196e-08
Epoch 343/512
448/448 - 0s - loss: 4.1009e-06 - val_loss: 4.0686e-08
Epoch 344/512
448/448 - 0s - loss: 4.2278e-06 - val_loss: 4.0541e-08
Epoch 345/512
448/448 - 0s - loss: 4.0564e-06 - val_loss: 3.4778e-08
Epoch 346/512
448/448 - 0s - loss: 3.5363e-06 - val_loss: 3.4777e-08
Epoch 347/512
448/448 - 0s - loss: 3.6558e-06 - val_loss: 3.6433e-08
Epoch 348/512
448/448 - 0s - loss: 3.6740e-06 - val_loss: 3.1533e-08
Epoch 349/512
448/448 - 0s - loss: 3.1775e-06 - val_loss: 2.9484e-08
Epoch 350/512
448/448 - 0s - loss: 3.1267e-06 - val_loss: 3.1413e-08
Epoch 351/512
448/448 - 0s - loss: 3.2168e-06 - val_loss: 3.0248e-08
Epoch 352/512
448/448 - 0s - loss: 3.0171e-06 - val_loss: 2.5496e-08
Epoch 353/512
448/448 - 0s - loss: 2.6028e-06 - val_loss: 2.6210e-08
Epoch 354/512
448/448 - 0s - loss: 2.7895e-06 - val_loss: 2.7692e-08
Epoch 355/512
448/448 - 0s - loss: 2.7815e-06 - val_loss: 2.3206e-08
Epoch 356/512
448/448 - 0s - loss: 2.3169e-06 - val_loss: 2.2077e-08
Epoch 357/512
448/448 - 0s - loss: 2.3523e-06 - val_loss: 2.3687e-08
Epoch 358/512
448/448 - 0s - loss: 2.4416e-06 - val_loss: 2.1437e-08
Epoch 359/512
448/448 - 0s - loss: 2.1470e-06 - val_loss: 1.9230e-08
Epoch 360/512
448/448 - 0s - loss: 2.0178e-06 - val_loss: 1.9955e-08
Epoch 361/512
448/448 - 0s - loss: 2.0871e-06 - val_loss: 1.9886e-08
Epoch 362/512
448/448 - 0s - loss: 1.9925e-06 - val_loss: 1.7525e-08
Epoch 363/512
448/448 - 0s - loss: 1.7824e-06 - val_loss: 1.6810e-08
Epoch 364/512
448/448 - 0s - loss: 1.7674e-06 - val_loss: 1.7033e-08
Epoch 365/512
448/448 - 0s - loss: 1.7370e-06 - val_loss: 1.6239e-08
Epoch 366/512
448/448 - 0s - loss: 1.6396e-06 - val_loss: 1.4803e-08
Epoch 367/512
448/448 - 0s - loss: 1.5302e-06 - val_loss: 1.4018e-08
Epoch 368/512
448/448 - 0s - loss: 1.4702e-06 - val_loss: 1.4326e-08
Epoch 369/512
448/448 - 0s - loss: 1.4830e-06 - val_loss: 1.3166e-08
Epoch 370/512
448/448 - 0s - loss: 1.3440e-06 - val_loss: 1.1974e-08
Epoch 371/512
448/448 - 0s - loss: 1.2499e-06 - val_loss: 1.2530e-08
Epoch 372/512
448/448 - 0s - loss: 1.3074e-06 - val_loss: 1.2112e-08
Epoch 373/512
448/448 - 0s - loss: 1.2090e-06 - val_loss: 1.0676e-08
Epoch 374/512
448/448 - 0s - loss: 1.0979e-06 - val_loss: 1.0324e-08
Epoch 375/512
448/448 - 0s - loss: 1.1003e-06 - val_loss: 1.0342e-08
Epoch 376/512
448/448 - 0s - loss: 1.0662e-06 - val_loss: 9.6706e-09
Epoch 377/512
448/448 - 0s - loss: 9.8976e-07 - val_loss: 9.1649e-09
Epoch 378/512
448/448 - 0s - loss: 9.4897e-07 - val_loss: 8.9613e-09
Epoch 379/512
448/448 - 0s - loss: 9.2596e-07 - val_loss: 8.5262e-09
Epoch 380/512
448/448 - 0s - loss: 8.6919e-07 - val_loss: 7.8777e-09
Epoch 381/512
448/448 - 0s - loss: 8.2257e-07 - val_loss: 7.7183e-09
Epoch 382/512
448/448 - 0s - loss: 8.0294e-07 - val_loss: 7.5960e-09
Epoch 383/512
448/448 - 0s - loss: 7.7802e-07 - val_loss: 6.9456e-09
Epoch 384/512
448/448 - 0s - loss: 7.0869e-07 - val_loss: 6.6895e-09
Epoch 385/512
448/448 - 0s - loss: 7.0175e-07 - val_loss: 6.4209e-09
Epoch 386/512
448/448 - 0s - loss: 6.5879e-07 - val_loss: 6.2767e-09
Epoch 387/512
448/448 - 0s - loss: 6.4381e-07 - val_loss: 5.9876e-09
Epoch 388/512
448/448 - 0s - loss: 6.0837e-07 - val_loss: 5.5086e-09
Epoch 389/512
448/448 - 0s - loss: 5.6928e-07 - val_loss: 5.3292e-09
Epoch 390/512
448/448 - 0s - loss: 5.5690e-07 - val_loss: 5.1396e-09
Epoch 391/512
448/448 - 0s - loss: 5.2772e-07 - val_loss: 4.8460e-09
Epoch 392/512
448/448 - 0s - loss: 5.0245e-07 - val_loss: 4.6400e-09
Epoch 393/512
448/448 - 0s - loss: 4.8273e-07 - val_loss: 4.3823e-09
Epoch 394/512
448/448 - 0s - loss: 4.5526e-07 - val_loss: 4.2595e-09
Epoch 395/512
448/448 - 0s - loss: 4.4270e-07 - val_loss: 4.0713e-09
Epoch 396/512
448/448 - 0s - loss: 4.1828e-07 - val_loss: 3.8193e-09
Epoch 397/512
448/448 - 0s - loss: 3.9372e-07 - val_loss: 3.6450e-09
Epoch 398/512
448/448 - 0s - loss: 3.7952e-07 - val_loss: 3.5461e-09
Epoch 399/512
448/448 - 0s - loss: 3.6533e-07 - val_loss: 3.3625e-09
Epoch 400/512
448/448 - 0s - loss: 3.4764e-07 - val_loss: 3.1169e-09
Epoch 401/512
448/448 - 0s - loss: 3.2309e-07 - val_loss: 3.0858e-09
Epoch 402/512
448/448 - 0s - loss: 3.2231e-07 - val_loss: 2.8878e-09
Epoch 403/512
448/448 - 0s - loss: 2.9634e-07 - val_loss: 2.7131e-09
Epoch 404/512
448/448 - 0s - loss: 2.8446e-07 - val_loss: 2.5833e-09
Epoch 405/512
448/448 - 0s - loss: 2.7110e-07 - val_loss: 2.5165e-09
Epoch 406/512
448/448 - 0s - loss: 2.6249e-07 - val_loss: 2.4567e-09
Epoch 407/512
448/448 - 0s - loss: 2.5193e-07 - val_loss: 2.2344e-09
Epoch 408/512
448/448 - 0s - loss: 2.2988e-07 - val_loss: 2.1523e-09
Epoch 409/512
448/448 - 0s - loss: 2.2715e-07 - val_loss: 2.0753e-09
Epoch 410/512
448/448 - 0s - loss: 2.1519e-07 - val_loss: 1.9529e-09
Epoch 411/512
448/448 - 0s - loss: 2.0362e-07 - val_loss: 1.8627e-09
Epoch 412/512
448/448 - 0s - loss: 1.9645e-07 - val_loss: 1.7446e-09
Epoch 413/512
448/448 - 0s - loss: 1.8170e-07 - val_loss: 1.7370e-09
Epoch 414/512
448/448 - 0s - loss: 1.8131e-07 - val_loss: 1.6641e-09
Epoch 415/512
448/448 - 0s - loss: 1.6878e-07 - val_loss: 1.5435e-09
Epoch 416/512
448/448 - 0s - loss: 1.6048e-07 - val_loss: 1.4364e-09
Epoch 417/512
448/448 - 0s - loss: 1.4898e-07 - val_loss: 1.4338e-09
Epoch 418/512
448/448 - 0s - loss: 1.5010e-07 - val_loss: 1.3635e-09
Epoch 419/512
448/448 - 0s - loss: 1.3969e-07 - val_loss: 1.2111e-09
Epoch 420/512
448/448 - 0s - loss: 1.2748e-07 - val_loss: 1.1711e-09
Epoch 421/512
448/448 - 0s - loss: 1.2515e-07 - val_loss: 1.2066e-09
Epoch 422/512
448/448 - 0s - loss: 1.2641e-07 - val_loss: 1.0855e-09
Epoch 423/512
448/448 - 0s - loss: 1.1003e-07 - val_loss: 1.0122e-09
Epoch 424/512
448/448 - 0s - loss: 1.0743e-07 - val_loss: 1.0182e-09
Epoch 425/512
448/448 - 0s - loss: 1.0728e-07 - val_loss: 9.4512e-10
Epoch 426/512
448/448 - 0s - loss: 9.6938e-08 - val_loss: 8.8892e-10
Epoch 427/512
448/448 - 0s - loss: 9.3312e-08 - val_loss: 8.6643e-10
Epoch 428/512
448/448 - 0s - loss: 9.0983e-08 - val_loss: 8.2031e-10
Epoch 429/512
448/448 - 0s - loss: 8.5551e-08 - val_loss: 7.4549e-10
Epoch 430/512
448/448 - 0s - loss: 7.7872e-08 - val_loss: 7.5387e-10
Epoch 431/512
448/448 - 0s - loss: 7.8989e-08 - val_loss: 7.4665e-10
Epoch 432/512
448/448 - 0s - loss: 7.6131e-08 - val_loss: 6.5507e-10
Epoch 433/512
448/448 - 0s - loss: 6.7075e-08 - val_loss: 6.1929e-10
Epoch 434/512
448/448 - 0s - loss: 6.5909e-08 - val_loss: 6.3028e-10
Epoch 435/512
448/448 - 0s - loss: 6.5772e-08 - val_loss: 5.9007e-10
Epoch 436/512
448/448 - 0s - loss: 6.0438e-08 - val_loss: 5.2433e-10
Epoch 437/512
448/448 - 0s - loss: 5.4930e-08 - val_loss: 5.2887e-10
Epoch 438/512
448/448 - 0s - loss: 5.6170e-08 - val_loss: 5.1681e-10
Epoch 439/512
448/448 - 0s - loss: 5.3075e-08 - val_loss: 4.6836e-10
Epoch 440/512
448/448 - 0s - loss: 4.8515e-08 - val_loss: 4.3996e-10
Epoch 441/512
448/448 - 0s - loss: 4.6209e-08 - val_loss: 4.4672e-10
Epoch 442/512
448/448 - 0s - loss: 4.6832e-08 - val_loss: 4.1270e-10
Epoch 443/512
448/448 - 0s - loss: 4.2315e-08 - val_loss: 3.7289e-10
Epoch 444/512
448/448 - 0s - loss: 3.8999e-08 - val_loss: 3.7461e-10
Epoch 445/512
448/448 - 0s - loss: 3.9747e-08 - val_loss: 3.7005e-10
Epoch 446/512
448/448 - 0s - loss: 3.7888e-08 - val_loss: 3.3508e-10
Epoch 447/512
448/448 - 0s - loss: 3.4557e-08 - val_loss: 3.0452e-10
Epoch 448/512
448/448 - 0s - loss: 3.2102e-08 - val_loss: 3.1541e-10
Epoch 449/512
448/448 - 0s - loss: 3.3122e-08 - val_loss: 3.1093e-10
Epoch 450/512
448/448 - 0s - loss: 3.1337e-08 - val_loss: 2.6968e-10
Epoch 451/512
448/448 - 0s - loss: 2.7347e-08 - val_loss: 2.5773e-10
Epoch 452/512
448/448 - 0s - loss: 2.7495e-08 - val_loss: 2.5718e-10
Epoch 453/512
448/448 - 0s - loss: 2.6956e-08 - val_loss: 2.3802e-10
Epoch 454/512
448/448 - 0s - loss: 2.4684e-08 - val_loss: 2.2049e-10
Epoch 455/512
448/448 - 0s - loss: 2.3118e-08 - val_loss: 2.2081e-10
Epoch 456/512
448/448 - 0s - loss: 2.3150e-08 - val_loss: 2.1478e-10
Epoch 457/512
448/448 - 0s - loss: 2.2203e-08 - val_loss: 1.9181e-10
Epoch 458/512
448/448 - 0s - loss: 1.9988e-08 - val_loss: 1.7739e-10
Epoch 459/512
448/448 - 0s - loss: 1.8689e-08 - val_loss: 1.8462e-10
Epoch 460/512
448/448 - 0s - loss: 1.9520e-08 - val_loss: 1.7642e-10
Epoch 461/512
448/448 - 0s - loss: 1.8058e-08 - val_loss: 1.5649e-10
Epoch 462/512
448/448 - 0s - loss: 1.6234e-08 - val_loss: 1.5255e-10
Epoch 463/512
448/448 - 0s - loss: 1.6244e-08 - val_loss: 1.5031e-10
Epoch 464/512
448/448 - 0s - loss: 1.5583e-08 - val_loss: 1.4617e-10
Epoch 465/512
448/448 - 0s - loss: 1.5038e-08 - val_loss: 1.3345e-10
Epoch 466/512
448/448 - 0s - loss: 1.3667e-08 - val_loss: 1.2565e-10
Epoch 467/512
448/448 - 0s - loss: 1.3182e-08 - val_loss: 1.2394e-10
Epoch 468/512
448/448 - 0s - loss: 1.2967e-08 - val_loss: 1.1921e-10
Epoch 469/512
448/448 - 0s - loss: 1.2163e-08 - val_loss: 1.1398e-10
Epoch 470/512
448/448 - 0s - loss: 1.1791e-08 - val_loss: 1.0552e-10
Epoch 471/512
448/448 - 0s - loss: 1.0920e-08 - val_loss: 9.9385e-11
Epoch 472/512
448/448 - 0s - loss: 1.0375e-08 - val_loss: 9.7205e-11
Epoch 473/512
448/448 - 0s - loss: 1.0250e-08 - val_loss: 9.3477e-11
Epoch 474/512
448/448 - 0s - loss: 9.6376e-09 - val_loss: 8.8972e-11
Epoch 475/512
448/448 - 0s - loss: 9.1837e-09 - val_loss: 8.4193e-11
Epoch 476/512
448/448 - 0s - loss: 8.7812e-09 - val_loss: 8.0063e-11
Epoch 477/512
448/448 - 0s - loss: 8.3264e-09 - val_loss: 7.7774e-11
Epoch 478/512
448/448 - 0s - loss: 8.1609e-09 - val_loss: 7.3606e-11
Epoch 479/512
448/448 - 0s - loss: 7.5765e-09 - val_loss: 7.1219e-11
Epoch 480/512
448/448 - 0s - loss: 7.3599e-09 - val_loss: 6.9637e-11
Epoch 481/512
448/448 - 0s - loss: 7.1586e-09 - val_loss: 6.5643e-11
Epoch 482/512
448/448 - 0s - loss: 6.7426e-09 - val_loss: 6.1098e-11
Epoch 483/512
448/448 - 0s - loss: 6.3135e-09 - val_loss: 5.7205e-11
Epoch 484/512
448/448 - 0s - loss: 5.9748e-09 - val_loss: 5.7623e-11
Epoch 485/512
448/448 - 0s - loss: 6.0117e-09 - val_loss: 5.6722e-11
Epoch 486/512
448/448 - 0s - loss: 5.8122e-09 - val_loss: 5.3103e-11
Epoch 487/512
448/448 - 0s - loss: 5.3911e-09 - val_loss: 4.9749e-11
Epoch 488/512
448/448 - 0s - loss: 5.0530e-09 - val_loss: 4.8019e-11
Epoch 489/512
448/448 - 0s - loss: 4.9794e-09 - val_loss: 4.7108e-11
Epoch 490/512
448/448 - 0s - loss: 4.8147e-09 - val_loss: 4.5337e-11
Epoch 491/512
448/448 - 0s - loss: 4.6479e-09 - val_loss: 4.1664e-11
Epoch 492/512
448/448 - 0s - loss: 4.2513e-09 - val_loss: 4.0033e-11
Epoch 493/512
448/448 - 0s - loss: 4.1556e-09 - val_loss: 3.9240e-11
Epoch 494/512
448/448 - 0s - loss: 4.0484e-09 - val_loss: 3.8769e-11
Epoch 495/512
448/448 - 0s - loss: 4.0042e-09 - val_loss: 3.7028e-11
Epoch 496/512
448/448 - 0s - loss: 3.7697e-09 - val_loss: 3.4101e-11
Epoch 497/512
448/448 - 0s - loss: 3.5051e-09 - val_loss: 3.1645e-11
Epoch 498/512
448/448 - 0s - loss: 3.2889e-09 - val_loss: 3.2257e-11
Epoch 499/512
448/448 - 0s - loss: 3.3713e-09 - val_loss: 3.2190e-11
Epoch 500/512
448/448 - 0s - loss: 3.2902e-09 - val_loss: 3.0786e-11
Epoch 501/512
448/448 - 0s - loss: 3.1283e-09 - val_loss: 2.8162e-11
Epoch 502/512
448/448 - 0s - loss: 2.8850e-09 - val_loss: 2.6379e-11
Epoch 503/512
448/448 - 0s - loss: 2.7223e-09 - val_loss: 2.6490e-11
Epoch 504/512
448/448 - 0s - loss: 2.7483e-09 - val_loss: 2.6445e-11
Epoch 505/512
448/448 - 0s - loss: 2.7170e-09 - val_loss: 2.5347e-11
Epoch 506/512
448/448 - 0s - loss: 2.5754e-09 - val_loss: 2.3796e-11
Epoch 507/512
448/448 - 0s - loss: 2.4227e-09 - val_loss: 2.2748e-11
Epoch 508/512
448/448 - 0s - loss: 2.3441e-09 - val_loss: 2.1924e-11
Epoch 509/512
448/448 - 0s - loss: 2.2569e-09 - val_loss: 2.1577e-11
Epoch 510/512
448/448 - 0s - loss: 2.2132e-09 - val_loss: 2.0843e-11
Epoch 511/512
448/448 - 0s - loss: 2.1366e-09 - val_loss: 2.0293e-11
Epoch 512/512
448/448 - 0s - loss: 2.0540e-09 - val_loss: 1.9540e-11
2024-04-17 11:30:23.062176: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8587e-09 - val_loss: 1.7934e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8699e-09 - val_loss: 1.8287e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8798e-09 - val_loss: 1.7914e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8184e-09 - val_loss: 1.6713e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.7041e-09 - val_loss: 1.6094e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.6382e-09 - val_loss: 1.5486e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5815e-09 - val_loss: 1.5115e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5361e-09 - val_loss: 1.4561e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4898e-09 - val_loss: 1.4091e-09
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4346e-09 - val_loss: 1.3597e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3829e-09 - val_loss: 1.3297e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3534e-09 - val_loss: 1.2982e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3207e-09 - val_loss: 1.2529e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2702e-09 - val_loss: 1.2041e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2250e-09 - val_loss: 1.1543e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1800e-09 - val_loss: 1.1366e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1632e-09 - val_loss: 1.1048e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1259e-09 - val_loss: 1.0708e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0852e-09 - val_loss: 1.0404e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0568e-09 - val_loss: 1.0007e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0233e-09 - val_loss: 9.8293e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0034e-09 - val_loss: 9.5848e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.7740e-10 - val_loss: 9.2193e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.3337e-10 - val_loss: 8.8245e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.0060e-10 - val_loss: 8.7835e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.9549e-10 - val_loss: 8.6507e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.7485e-10 - val_loss: 8.4603e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.5562e-10 - val_loss: 8.2211e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.3633e-10 - val_loss: 7.9549e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.0491e-10 - val_loss: 7.6810e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.7341e-10 - val_loss: 7.3020e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.4250e-10 - val_loss: 7.1825e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.3559e-10 - val_loss: 7.1790e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.3168e-10 - val_loss: 7.0698e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.1268e-10 - val_loss: 6.7484e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.8752e-10 - val_loss: 6.6666e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.7870e-10 - val_loss: 6.3939e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.4345e-10 - val_loss: 6.2592e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.3449e-10 - val_loss: 6.1143e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.1850e-10 - val_loss: 5.9694e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.0989e-10 - val_loss: 5.8445e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.8885e-10 - val_loss: 5.7531e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.8357e-10 - val_loss: 5.6327e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.6493e-10 - val_loss: 5.4054e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.4538e-10 - val_loss: 5.2273e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.3373e-10 - val_loss: 5.1811e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.2560e-10 - val_loss: 5.1172e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1953e-10 - val_loss: 5.0203e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1232e-10 - val_loss: 4.9862e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.9928e-10 - val_loss: 4.8291e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.9148e-10 - val_loss: 4.6830e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.7251e-10 - val_loss: 4.6038e-10
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.6272e-10 - val_loss: 4.4587e-10
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.5326e-10 - val_loss: 4.3158e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.3413e-10 - val_loss: 4.2412e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.2797e-10 - val_loss: 4.1302e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2201e-10 - val_loss: 4.2293e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.2401e-10 - val_loss: 4.0630e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.0644e-10 - val_loss: 3.9100e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9520e-10 - val_loss: 3.8912e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9698e-10 - val_loss: 3.8166e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.8572e-10 - val_loss: 3.7210e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.7471e-10 - val_loss: 3.6265e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.6790e-10 - val_loss: 3.5403e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6003e-10 - val_loss: 3.5514e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5931e-10 - val_loss: 3.4876e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5344e-10 - val_loss: 3.4222e-10
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.4162e-10 - val_loss: 3.2994e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.3489e-10 - val_loss: 3.2678e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.2599e-10 - val_loss: 3.1147e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1864e-10 - val_loss: 3.1961e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2120e-10 - val_loss: 3.1483e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2047e-10 - val_loss: 3.1543e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.1532e-10 - val_loss: 3.0341e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.0826e-10 - val_loss: 2.9543e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.9616e-10 - val_loss: 2.8631e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9266e-10 - val_loss: 2.8650e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8566e-10 - val_loss: 2.7196e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7811e-10 - val_loss: 2.7975e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8130e-10 - val_loss: 2.7043e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.7277e-10 - val_loss: 2.6683e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.6655e-10 - val_loss: 2.5620e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5761e-10 - val_loss: 2.5674e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.5969e-10 - val_loss: 2.4903e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4841e-10 - val_loss: 2.4228e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4470e-10 - val_loss: 2.3639e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4076e-10 - val_loss: 2.4173e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5019e-10 - val_loss: 2.4437e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4529e-10 - val_loss: 2.3419e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.3599e-10 - val_loss: 2.2657e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.2621e-10 - val_loss: 2.1855e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.2008e-10 - val_loss: 2.1708e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.2039e-10 - val_loss: 2.1547e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1935e-10 - val_loss: 2.1993e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2092e-10 - val_loss: 2.1837e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.2034e-10 - val_loss: 2.1391e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.1594e-10 - val_loss: 2.0733e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0829e-10 - val_loss: 2.0253e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0546e-10 - val_loss: 2.0594e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0674e-10 - val_loss: 2.0250e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0338e-10 - val_loss: 2.0072e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0366e-10 - val_loss: 1.9225e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.9094e-10 - val_loss: 1.8748e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8906e-10 - val_loss: 1.8554e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8406e-10 - val_loss: 1.7743e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8165e-10 - val_loss: 1.8116e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8544e-10 - val_loss: 1.8826e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8666e-10 - val_loss: 1.7913e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8073e-10 - val_loss: 1.7757e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8085e-10 - val_loss: 1.7418e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.7202e-10 - val_loss: 1.6592e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7033e-10 - val_loss: 1.7026e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7378e-10 - val_loss: 1.6823e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.6698e-10 - val_loss: 1.6117e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6311e-10 - val_loss: 1.6129e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5809e-10 - val_loss: 1.5120e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5067e-10 - val_loss: 1.4949e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5414e-10 - val_loss: 1.5921e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5882e-10 - val_loss: 1.5176e-10
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5113e-10 - val_loss: 1.4770e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5267e-10 - val_loss: 1.5602e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5293e-10 - val_loss: 1.4334e-10
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4324e-10 - val_loss: 1.3912e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4271e-10 - val_loss: 1.4653e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4727e-10 - val_loss: 1.4169e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4076e-10 - val_loss: 1.3702e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3764e-10 - val_loss: 1.3745e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4093e-10 - val_loss: 1.3850e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3717e-10 - val_loss: 1.2977e-10
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2755e-10 - val_loss: 1.2621e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3103e-10 - val_loss: 1.3369e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3598e-10 - val_loss: 1.3334e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3272e-10 - val_loss: 1.3085e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3373e-10 - val_loss: 1.3544e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3400e-10 - val_loss: 1.2575e-10
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2253e-10 - val_loss: 1.1515e-10
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1347e-10 - val_loss: 1.1063e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1116e-10 - val_loss: 1.1141e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1301e-10 - val_loss: 1.1364e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1563e-10 - val_loss: 1.1401e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1723e-10 - val_loss: 1.2478e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2748e-10 - val_loss: 1.2681e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2584e-10 - val_loss: 1.2324e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2230e-10 - val_loss: 1.1462e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1304e-10 - val_loss: 1.0759e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0744e-10 - val_loss: 1.0631e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0699e-10 - val_loss: 1.0553e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0764e-10 - val_loss: 1.0930e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1085e-10 - val_loss: 1.1086e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1124e-10 - val_loss: 1.0779e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0821e-10 - val_loss: 1.0518e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0514e-10 - val_loss: 1.0089e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0005e-10 - val_loss: 9.8181e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9378e-11 - val_loss: 9.9591e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0194e-10 - val_loss: 1.0284e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0567e-10 - val_loss: 1.0794e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1095e-10 - val_loss: 1.0941e-10
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0756e-10 - val_loss: 1.0199e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0079e-10 - val_loss: 9.6224e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7360e-11 - val_loss: 9.8163e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.7583e-11 - val_loss: 9.3200e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4089e-11 - val_loss: 9.3878e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.5089e-11 - val_loss: 9.3039e-11
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.4288e-11 - val_loss: 9.2380e-11
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.3238e-11 - val_loss: 8.7428e-11
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.7055e-11 - val_loss: 8.6831e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8870e-11 - val_loss: 8.9147e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0491e-11 - val_loss: 8.9549e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9909e-11 - val_loss: 9.0898e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.9592e-11 - val_loss: 8.4674e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.4395e-11 - val_loss: 8.4361e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5432e-11 - val_loss: 8.6438e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8348e-11 - val_loss: 8.8279e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0032e-11 - val_loss: 9.0022e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.7975e-11 - val_loss: 8.2636e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.2889e-11 - val_loss: 8.2283e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3462e-11 - val_loss: 8.2996e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3754e-11 - val_loss: 8.4304e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5797e-11 - val_loss: 8.5866e-11
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.4314e-11 - val_loss: 8.0610e-11
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.0303e-11 - val_loss: 7.9622e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0668e-11 - val_loss: 8.0196e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1279e-11 - val_loss: 8.0563e-11
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.9996e-11 - val_loss: 7.5247e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.4191e-11 - val_loss: 7.0462e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.0755e-11 - val_loss: 6.9992e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2106e-11 - val_loss: 7.5676e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8969e-11 - val_loss: 8.1852e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3079e-11 - val_loss: 8.4268e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2147e-11 - val_loss: 7.2997e-11
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.0596e-11 - val_loss: 6.7235e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8538e-11 - val_loss: 6.9432e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0824e-11 - val_loss: 7.0602e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1414e-11 - val_loss: 7.2282e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3922e-11 - val_loss: 6.9191e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.6388e-11 - val_loss: 5.9679e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.8211e-11 - val_loss: 5.4871e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4392e-11 - val_loss: 5.5611e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8030e-11 - val_loss: 6.3253e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7116e-11 - val_loss: 7.0375e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0600e-11 - val_loss: 6.5386e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4528e-11 - val_loss: 5.9806e-11
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.8253e-11 - val_loss: 5.4433e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4191e-11 - val_loss: 5.6700e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8744e-11 - val_loss: 6.2891e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5191e-11 - val_loss: 6.6661e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6824e-11 - val_loss: 6.2572e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1185e-11 - val_loss: 5.7503e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.6196e-11 - val_loss: 5.2104e-11
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1679e-11 - val_loss: 5.1753e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3777e-11 - val_loss: 5.7490e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8959e-11 - val_loss: 6.1122e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3382e-11 - val_loss: 6.6361e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4350e-11 - val_loss: 5.9330e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7478e-11 - val_loss: 5.2835e-11
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.2521e-11 - val_loss: 5.1121e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0866e-11 - val_loss: 5.2740e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5665e-11 - val_loss: 5.8308e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0197e-11 - val_loss: 6.1277e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2341e-11 - val_loss: 5.9207e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7638e-11 - val_loss: 5.3030e-11
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1684e-11 - val_loss: 4.8830e-11
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.8455e-11 - val_loss: 4.7292e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.7182e-11 - val_loss: 4.7207e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9293e-11 - val_loss: 5.3478e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5287e-11 - val_loss: 5.7352e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8931e-11 - val_loss: 5.9451e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7466e-11 - val_loss: 5.1780e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0467e-11 - val_loss: 4.8480e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8203e-11 - val_loss: 4.7509e-11
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.6998e-11 - val_loss: 4.4557e-11
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.4138e-11 - val_loss: 4.3782e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5990e-11 - val_loss: 5.0172e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1668e-11 - val_loss: 5.5003e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5508e-11 - val_loss: 5.2698e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2255e-11 - val_loss: 4.8964e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8486e-11 - val_loss: 4.6487e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6215e-11 - val_loss: 4.4646e-11
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.3718e-11 - val_loss: 4.0244e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0871e-11 - val_loss: 4.3541e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5720e-11 - val_loss: 4.8719e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0135e-11 - val_loss: 5.1008e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3192e-11 - val_loss: 5.2663e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1448e-11 - val_loss: 4.7795e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7421e-11 - val_loss: 4.6354e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5567e-11 - val_loss: 4.3286e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3521e-11 - val_loss: 4.4631e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4121e-11 - val_loss: 4.3422e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5519e-11 - val_loss: 5.0315e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2058e-11 - val_loss: 5.3230e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4628e-11 - val_loss: 5.6277e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4097e-11 - val_loss: 4.8590e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6395e-11 - val_loss: 4.1394e-11
Epoch 254/512

Epoch 00254: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.0234e-11 - val_loss: 3.7847e-11
Epoch 255/512

Epoch 00255: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.7215e-11 - val_loss: 3.5441e-11
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.4911e-11 - val_loss: 3.3508e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4300e-11 - val_loss: 3.6071e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7157e-11 - val_loss: 4.0281e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3142e-11 - val_loss: 4.8348e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0516e-11 - val_loss: 5.5264e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7100e-11 - val_loss: 5.5177e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3913e-11 - val_loss: 4.8066e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6435e-11 - val_loss: 4.2691e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2069e-11 - val_loss: 4.0030e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9419e-11 - val_loss: 3.7037e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6207e-11 - val_loss: 3.4423e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4933e-11 - val_loss: 3.4411e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6104e-11 - val_loss: 4.0526e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3152e-11 - val_loss: 4.6664e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7591e-11 - val_loss: 4.6527e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5926e-11 - val_loss: 4.1436e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0099e-11 - val_loss: 3.7514e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7041e-11 - val_loss: 3.5341e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4800e-11 - val_loss: 3.3908e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4476e-11 - val_loss: 3.4186e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4981e-11 - val_loss: 3.5393e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5499e-11 - val_loss: 3.6691e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7558e-11 - val_loss: 3.8789e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0814e-11 - val_loss: 4.3528e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4477e-11 - val_loss: 4.6774e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8286e-11 - val_loss: 4.6560e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4827e-11 - val_loss: 4.0263e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8954e-11 - val_loss: 3.6747e-11
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5698e-11 - val_loss: 3.2016e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1635e-11 - val_loss: 3.2166e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3097e-11 - val_loss: 3.4600e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5180e-11 - val_loss: 3.4200e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3966e-11 - val_loss: 3.3740e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4691e-11 - val_loss: 3.3299e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4497e-11 - val_loss: 3.7995e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9011e-11 - val_loss: 3.9717e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0344e-11 - val_loss: 3.8602e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8012e-11 - val_loss: 3.6082e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4907e-11 - val_loss: 3.2782e-11
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.2744e-11 - val_loss: 3.0622e-11
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.0142e-11 - val_loss: 2.9499e-11
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.9016e-11 - val_loss: 2.8226e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8985e-11 - val_loss: 3.1313e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2263e-11 - val_loss: 3.2801e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2265e-11 - val_loss: 3.2481e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2968e-11 - val_loss: 3.1917e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1951e-11 - val_loss: 3.1477e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2914e-11 - val_loss: 3.6756e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7284e-11 - val_loss: 3.6314e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5801e-11 - val_loss: 3.4572e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3326e-11 - val_loss: 3.1313e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1081e-11 - val_loss: 2.8646e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8295e-11 - val_loss: 2.8048e-11
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8308e-11 - val_loss: 2.8024e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7933e-11 - val_loss: 2.8176e-11
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.7296e-11 - val_loss: 2.5513e-11
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.5546e-11 - val_loss: 2.4572e-11
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4163e-11 - val_loss: 2.4073e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5690e-11 - val_loss: 2.7497e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8141e-11 - val_loss: 2.9163e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0023e-11 - val_loss: 3.2680e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4846e-11 - val_loss: 3.9263e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0992e-11 - val_loss: 4.1657e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0232e-11 - val_loss: 3.7691e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6057e-11 - val_loss: 3.1403e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9734e-11 - val_loss: 2.8172e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7972e-11 - val_loss: 2.6895e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6863e-11 - val_loss: 2.5169e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5503e-11 - val_loss: 2.5636e-11
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.5149e-11 - val_loss: 2.4073e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5063e-11 - val_loss: 2.5911e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7064e-11 - val_loss: 2.7287e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7766e-11 - val_loss: 2.8818e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9400e-11 - val_loss: 2.9134e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9441e-11 - val_loss: 2.9636e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9511e-11 - val_loss: 2.8062e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8066e-11 - val_loss: 2.7116e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6425e-11 - val_loss: 2.4833e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5563e-11 - val_loss: 2.6032e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5634e-11 - val_loss: 2.5310e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5228e-11 - val_loss: 2.4926e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4598e-11 - val_loss: 2.4416e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4823e-11 - val_loss: 2.4879e-11
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4819e-11 - val_loss: 2.4054e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4399e-11 - val_loss: 2.4891e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5265e-11 - val_loss: 2.5073e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5609e-11 - val_loss: 2.7031e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7167e-11 - val_loss: 2.6089e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6452e-11 - val_loss: 2.5951e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6059e-11 - val_loss: 2.4884e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5043e-11 - val_loss: 2.4910e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5420e-11 - val_loss: 2.5672e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5984e-11 - val_loss: 2.5854e-11
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4322e-11 - val_loss: 2.1729e-11
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0657e-11 - val_loss: 1.8737e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8388e-11 - val_loss: 1.9197e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9752e-11 - val_loss: 2.0301e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0781e-11 - val_loss: 2.1750e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2270e-11 - val_loss: 2.3695e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4718e-11 - val_loss: 2.5356e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5257e-11 - val_loss: 2.4643e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4208e-11 - val_loss: 2.3712e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3992e-11 - val_loss: 2.4496e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5428e-11 - val_loss: 2.5766e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5326e-11 - val_loss: 2.3962e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3954e-11 - val_loss: 2.2890e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2457e-11 - val_loss: 2.2458e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2647e-11 - val_loss: 2.1816e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2298e-11 - val_loss: 2.2932e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2564e-11 - val_loss: 2.2472e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2783e-11 - val_loss: 2.2136e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2457e-11 - val_loss: 2.3518e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2579e-11 - val_loss: 2.0522e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0018e-11 - val_loss: 1.8278e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.7562e-11 - val_loss: 1.6294e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6376e-11 - val_loss: 1.6639e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7805e-11 - val_loss: 1.9425e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0630e-11 - val_loss: 2.1950e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2265e-11 - val_loss: 2.2306e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3408e-11 - val_loss: 2.4948e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5119e-11 - val_loss: 2.4222e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4894e-11 - val_loss: 2.5718e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5834e-11 - val_loss: 2.4223e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3812e-11 - val_loss: 2.2848e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2492e-11 - val_loss: 2.1560e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1364e-11 - val_loss: 2.1289e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1724e-11 - val_loss: 2.1290e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1625e-11 - val_loss: 2.1519e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0998e-11 - val_loss: 2.0206e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9977e-11 - val_loss: 1.9309e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9727e-11 - val_loss: 2.0247e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0864e-11 - val_loss: 2.1331e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1629e-11 - val_loss: 2.0899e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1140e-11 - val_loss: 2.1352e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0032e-11 - val_loss: 1.7692e-11
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.6562e-11 - val_loss: 1.3536e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2955e-11 - val_loss: 1.1514e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1531e-11 - val_loss: 1.1490e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1370e-11 - val_loss: 1.0978e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0873e-11 - val_loss: 1.0311e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0293e-11 - val_loss: 1.1034e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2735e-11 - val_loss: 1.5578e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6321e-11 - val_loss: 1.7686e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8786e-11 - val_loss: 1.9768e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0262e-11 - val_loss: 2.0938e-11
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1208e-11 - val_loss: 2.1251e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1255e-11 - val_loss: 2.1618e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2077e-11 - val_loss: 2.2311e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2166e-11 - val_loss: 2.2179e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2401e-11 - val_loss: 2.1578e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1687e-11 - val_loss: 2.0977e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0455e-11 - val_loss: 2.0590e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0668e-11 - val_loss: 1.9590e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9365e-11 - val_loss: 1.9550e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9901e-11 - val_loss: 1.9759e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9376e-11 - val_loss: 1.8460e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8232e-11 - val_loss: 1.7022e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6948e-11 - val_loss: 1.6180e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6567e-11 - val_loss: 1.7272e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6665e-11 - val_loss: 1.4963e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4172e-11 - val_loss: 1.2528e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1791e-11 - val_loss: 1.0465e-11
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0418e-11 - val_loss: 9.5281e-12
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.2894e-12 - val_loss: 8.5910e-12
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8429e-12 - val_loss: 9.2450e-12
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1752e-12 - val_loss: 9.0318e-12
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.0443e-12 - val_loss: 8.4044e-12
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5625e-12 - val_loss: 9.1171e-12
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1639e-12 - val_loss: 9.1089e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2443e-12 - val_loss: 1.0026e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0836e-11 - val_loss: 1.3220e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4239e-11 - val_loss: 1.5741e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6806e-11 - val_loss: 1.8214e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9469e-11 - val_loss: 2.0770e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1093e-11 - val_loss: 2.1827e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2012e-11 - val_loss: 2.1922e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1718e-11 - val_loss: 2.0391e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0077e-11 - val_loss: 1.9366e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9139e-11 - val_loss: 1.9164e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8610e-11 - val_loss: 1.7614e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7306e-11 - val_loss: 1.6299e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6854e-11 - val_loss: 1.7170e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7509e-11 - val_loss: 1.7988e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7747e-11 - val_loss: 1.8026e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8237e-11 - val_loss: 1.7896e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7511e-11 - val_loss: 1.6477e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6594e-11 - val_loss: 1.6909e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7074e-11 - val_loss: 1.6651e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6936e-11 - val_loss: 1.7076e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7521e-11 - val_loss: 1.8788e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8678e-11 - val_loss: 1.8028e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6611e-11 - val_loss: 1.3765e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2696e-11 - val_loss: 1.0289e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9523e-12 - val_loss: 8.8155e-12
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.5430e-12 - val_loss: 8.2953e-12
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.0697e-12 - val_loss: 7.1408e-12
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.8635e-12 - val_loss: 6.6165e-12
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6750e-12 - val_loss: 7.3565e-12
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6348e-12 - val_loss: 7.1116e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2966e-12 - val_loss: 7.5021e-12
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5501e-12 - val_loss: 7.9317e-12
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0002e-12 - val_loss: 8.0234e-12
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2641e-12 - val_loss: 8.5215e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6628e-12 - val_loss: 8.5635e-12
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5291e-12 - val_loss: 8.9123e-12
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5035e-12 - val_loss: 1.1345e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2313e-11 - val_loss: 1.4247e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5008e-11 - val_loss: 1.7244e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8583e-11 - val_loss: 1.9919e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0188e-11 - val_loss: 2.0501e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0611e-11 - val_loss: 1.9711e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9500e-11 - val_loss: 1.9559e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9503e-11 - val_loss: 1.8559e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8053e-11 - val_loss: 1.7565e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7323e-11 - val_loss: 1.6993e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6337e-11 - val_loss: 1.5348e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5390e-11 - val_loss: 1.5045e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4630e-11 - val_loss: 1.3577e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4071e-11 - val_loss: 1.4252e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4640e-11 - val_loss: 1.5227e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5562e-11 - val_loss: 1.5979e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5957e-11 - val_loss: 1.5437e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5712e-11 - val_loss: 1.5495e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5567e-11 - val_loss: 1.5558e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5414e-11 - val_loss: 1.4821e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4658e-11 - val_loss: 1.4208e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4715e-11 - val_loss: 1.4784e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5019e-11 - val_loss: 1.5256e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6083e-11 - val_loss: 1.6294e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5122e-11 - val_loss: 1.2242e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1034e-11 - val_loss: 8.9328e-12
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6016e-12 - val_loss: 8.3010e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2641e-12 - val_loss: 7.9927e-12
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7875e-12 - val_loss: 7.5929e-12
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7137e-12 - val_loss: 7.6900e-12
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.0498e-12 - val_loss: 5.9205e-12
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9557e-12 - val_loss: 6.1401e-12
Epoch 493/512

Epoch 00493: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.9592e-12 - val_loss: 5.2892e-12
Epoch 494/512

Epoch 00494: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1080e-12 - val_loss: 4.5515e-12
Epoch 495/512

Epoch 00495: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.6107e-12 - val_loss: 4.5030e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6015e-12 - val_loss: 4.5241e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6200e-12 - val_loss: 4.6643e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8863e-12 - val_loss: 5.1254e-12
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0588e-12 - val_loss: 5.0491e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1022e-12 - val_loss: 5.1962e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1780e-12 - val_loss: 5.3833e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4367e-12 - val_loss: 5.8520e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3004e-12 - val_loss: 6.9145e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7847e-12 - val_loss: 6.7040e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8973e-12 - val_loss: 7.9214e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0380e-12 - val_loss: 1.0528e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1966e-11 - val_loss: 1.4052e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4452e-11 - val_loss: 1.6341e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6400e-11 - val_loss: 1.6822e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7547e-11 - val_loss: 1.7898e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7886e-11 - val_loss: 1.6803e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6210e-11 - val_loss: 1.5225e-11
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0457 - val_loss: 7.6897e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1079e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.1965e-04
Epoch 4/512
448/448 - 0s - loss: 9.6788e-04 - val_loss: 2.1726e-04
Epoch 5/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.4083e-04
Epoch 6/512
448/448 - 0s - loss: 9.5798e-04 - val_loss: 1.3593e-04
Epoch 7/512
448/448 - 0s - loss: 5.5040e-04 - val_loss: 1.0962e-04
Epoch 8/512
448/448 - 0s - loss: 8.1422e-04 - val_loss: 4.6229e-05
Epoch 9/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.1992e-05
Epoch 10/512
448/448 - 0s - loss: 6.1776e-04 - val_loss: 2.0476e-05
Epoch 11/512
448/448 - 0s - loss: 5.2761e-04 - val_loss: 1.1026e-05
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0515e-05
Epoch 13/512
448/448 - 0s - loss: 6.7696e-04 - val_loss: 1.2263e-05
Epoch 14/512
448/448 - 0s - loss: 3.9092e-04 - val_loss: 1.4281e-05
Epoch 15/512
448/448 - 0s - loss: 7.8053e-04 - val_loss: 1.9028e-05
Epoch 16/512
448/448 - 0s - loss: 6.5719e-04 - val_loss: 2.0454e-05
Epoch 17/512
448/448 - 0s - loss: 3.1836e-04 - val_loss: 2.0662e-05
Epoch 18/512
448/448 - 0s - loss: 5.1599e-04 - val_loss: 2.2056e-05
Epoch 19/512
448/448 - 0s - loss: 5.5314e-04 - val_loss: 2.2919e-05
Epoch 20/512
448/448 - 0s - loss: 2.7031e-04 - val_loss: 2.1262e-05
Epoch 21/512
448/448 - 0s - loss: 3.5870e-04 - val_loss: 2.1063e-05
Epoch 22/512
448/448 - 0s - loss: 4.2729e-04 - val_loss: 2.1236e-05
Epoch 23/512
448/448 - 0s - loss: 2.4033e-04 - val_loss: 1.8615e-05
Epoch 24/512
448/448 - 0s - loss: 2.5993e-04 - val_loss: 1.7435e-05
Epoch 25/512
448/448 - 0s - loss: 3.1158e-04 - val_loss: 1.7047e-05
Epoch 26/512
448/448 - 0s - loss: 2.1296e-04 - val_loss: 1.4516e-05
Epoch 27/512
448/448 - 0s - loss: 1.9558e-04 - val_loss: 1.2952e-05
Epoch 28/512
448/448 - 0s - loss: 2.2815e-04 - val_loss: 1.1902e-05
Epoch 29/512
448/448 - 0s - loss: 1.8740e-04 - val_loss: 9.9836e-06
Epoch 30/512
448/448 - 0s - loss: 1.5921e-04 - val_loss: 8.6045e-06
Epoch 31/512
448/448 - 0s - loss: 1.7610e-04 - val_loss: 7.7016e-06
Epoch 32/512
448/448 - 0s - loss: 1.5571e-04 - val_loss: 6.9214e-06
Epoch 33/512
448/448 - 0s - loss: 1.3641e-04 - val_loss: 6.3539e-06
Epoch 34/512
448/448 - 0s - loss: 1.4218e-04 - val_loss: 6.0877e-06
Epoch 35/512
448/448 - 0s - loss: 1.3005e-04 - val_loss: 5.9838e-06
Epoch 36/512
448/448 - 0s - loss: 1.2156e-04 - val_loss: 5.9052e-06
Epoch 37/512
448/448 - 0s - loss: 1.2067e-04 - val_loss: 5.8556e-06
Epoch 38/512
448/448 - 0s - loss: 1.1010e-04 - val_loss: 5.7552e-06
Epoch 39/512
448/448 - 0s - loss: 1.0927e-04 - val_loss: 5.5708e-06
Epoch 40/512
448/448 - 0s - loss: 1.0398e-04 - val_loss: 5.3173e-06
Epoch 41/512
448/448 - 0s - loss: 1.0061e-04 - val_loss: 4.9382e-06
Epoch 42/512
448/448 - 0s - loss: 9.5128e-05 - val_loss: 4.5145e-06
Epoch 43/512
448/448 - 0s - loss: 9.1941e-05 - val_loss: 4.2236e-06
Epoch 44/512
448/448 - 0s - loss: 9.2039e-05 - val_loss: 3.6557e-06
Epoch 45/512
448/448 - 0s - loss: 8.5752e-05 - val_loss: 3.4353e-06
Epoch 46/512
448/448 - 0s - loss: 8.3972e-05 - val_loss: 3.3246e-06
Epoch 47/512
448/448 - 0s - loss: 8.1835e-05 - val_loss: 3.2940e-06
Epoch 48/512
448/448 - 0s - loss: 7.9112e-05 - val_loss: 3.3359e-06
Epoch 49/512
448/448 - 0s - loss: 7.7862e-05 - val_loss: 2.9721e-06
Epoch 50/512
448/448 - 0s - loss: 7.4770e-05 - val_loss: 2.7473e-06
Epoch 51/512
448/448 - 0s - loss: 7.1907e-05 - val_loss: 2.5329e-06
Epoch 52/512
448/448 - 0s - loss: 7.1757e-05 - val_loss: 2.3935e-06
Epoch 53/512
448/448 - 0s - loss: 7.1016e-05 - val_loss: 2.2198e-06
Epoch 54/512
448/448 - 0s - loss: 6.5650e-05 - val_loss: 2.0496e-06
Epoch 55/512
448/448 - 0s - loss: 6.6667e-05 - val_loss: 1.9810e-06
Epoch 56/512
448/448 - 0s - loss: 6.5837e-05 - val_loss: 1.8205e-06
Epoch 57/512
448/448 - 0s - loss: 6.1826e-05 - val_loss: 1.6678e-06
Epoch 58/512
448/448 - 0s - loss: 6.1094e-05 - val_loss: 1.5683e-06
Epoch 59/512
448/448 - 0s - loss: 6.1621e-05 - val_loss: 1.4559e-06
Epoch 60/512
448/448 - 0s - loss: 5.9288e-05 - val_loss: 1.3360e-06
Epoch 61/512
448/448 - 0s - loss: 5.6542e-05 - val_loss: 1.2636e-06
Epoch 62/512
448/448 - 0s - loss: 5.7514e-05 - val_loss: 1.1939e-06
Epoch 63/512
448/448 - 0s - loss: 5.5230e-05 - val_loss: 1.0986e-06
Epoch 64/512
448/448 - 0s - loss: 5.3221e-05 - val_loss: 1.0503e-06
Epoch 65/512
448/448 - 0s - loss: 5.3778e-05 - val_loss: 9.8939e-07
Epoch 66/512
448/448 - 0s - loss: 5.2224e-05 - val_loss: 9.1699e-07
Epoch 67/512
448/448 - 0s - loss: 5.0644e-05 - val_loss: 8.5352e-07
Epoch 68/512
448/448 - 0s - loss: 5.0233e-05 - val_loss: 8.1476e-07
Epoch 69/512
448/448 - 0s - loss: 4.9427e-05 - val_loss: 7.6136e-07
Epoch 70/512
448/448 - 0s - loss: 4.7832e-05 - val_loss: 7.2798e-07
Epoch 71/512
448/448 - 0s - loss: 4.7702e-05 - val_loss: 6.8555e-07
Epoch 72/512
448/448 - 0s - loss: 4.6447e-05 - val_loss: 6.3793e-07
Epoch 73/512
448/448 - 0s - loss: 4.5941e-05 - val_loss: 6.0142e-07
Epoch 74/512
448/448 - 0s - loss: 4.4813e-05 - val_loss: 5.6567e-07
Epoch 75/512
448/448 - 0s - loss: 4.3509e-05 - val_loss: 5.3756e-07
Epoch 76/512
448/448 - 0s - loss: 4.3095e-05 - val_loss: 5.1861e-07
Epoch 77/512
448/448 - 0s - loss: 4.3792e-05 - val_loss: 4.8180e-07
Epoch 78/512
448/448 - 0s - loss: 4.0596e-05 - val_loss: 4.5342e-07
Epoch 79/512
448/448 - 0s - loss: 4.1166e-05 - val_loss: 4.4531e-07
Epoch 80/512
448/448 - 0s - loss: 4.0791e-05 - val_loss: 4.2514e-07
Epoch 81/512
448/448 - 0s - loss: 3.9440e-05 - val_loss: 3.9899e-07
Epoch 82/512
448/448 - 0s - loss: 3.9152e-05 - val_loss: 3.8178e-07
Epoch 83/512
448/448 - 0s - loss: 3.8588e-05 - val_loss: 3.5690e-07
Epoch 84/512
448/448 - 0s - loss: 3.7072e-05 - val_loss: 3.5272e-07
Epoch 85/512
448/448 - 0s - loss: 3.7215e-05 - val_loss: 3.4243e-07
Epoch 86/512
448/448 - 0s - loss: 3.7395e-05 - val_loss: 3.1650e-07
Epoch 87/512
448/448 - 0s - loss: 3.4990e-05 - val_loss: 3.0418e-07
Epoch 88/512
448/448 - 0s - loss: 3.4818e-05 - val_loss: 3.0530e-07
Epoch 89/512
448/448 - 0s - loss: 3.5425e-05 - val_loss: 2.9222e-07
Epoch 90/512
448/448 - 0s - loss: 3.4238e-05 - val_loss: 2.7513e-07
Epoch 91/512
448/448 - 0s - loss: 3.3026e-05 - val_loss: 2.6704e-07
Epoch 92/512
448/448 - 0s - loss: 3.3022e-05 - val_loss: 2.6308e-07
Epoch 93/512
448/448 - 0s - loss: 3.2627e-05 - val_loss: 2.4840e-07
Epoch 94/512
448/448 - 0s - loss: 3.1459e-05 - val_loss: 2.4192e-07
Epoch 95/512
448/448 - 0s - loss: 3.1677e-05 - val_loss: 2.3392e-07
Epoch 96/512
448/448 - 0s - loss: 3.0795e-05 - val_loss: 2.2560e-07
Epoch 97/512
448/448 - 0s - loss: 3.0376e-05 - val_loss: 2.1561e-07
Epoch 98/512
448/448 - 0s - loss: 2.9563e-05 - val_loss: 2.1275e-07
Epoch 99/512
448/448 - 0s - loss: 2.9670e-05 - val_loss: 2.0408e-07
Epoch 100/512
448/448 - 0s - loss: 2.8633e-05 - val_loss: 1.9909e-07
Epoch 101/512
448/448 - 0s - loss: 2.7999e-05 - val_loss: 1.9902e-07
Epoch 102/512
448/448 - 0s - loss: 2.8204e-05 - val_loss: 1.9173e-07
Epoch 103/512
448/448 - 0s - loss: 2.7144e-05 - val_loss: 1.8363e-07
Epoch 104/512
448/448 - 0s - loss: 2.6305e-05 - val_loss: 1.8512e-07
Epoch 105/512
448/448 - 0s - loss: 2.7068e-05 - val_loss: 1.8056e-07
Epoch 106/512
448/448 - 0s - loss: 2.5777e-05 - val_loss: 1.7300e-07
Epoch 107/512
448/448 - 0s - loss: 2.5117e-05 - val_loss: 1.7091e-07
Epoch 108/512
448/448 - 0s - loss: 2.5040e-05 - val_loss: 1.7171e-07
Epoch 109/512
448/448 - 0s - loss: 2.4799e-05 - val_loss: 1.6636e-07
Epoch 110/512
448/448 - 0s - loss: 2.4073e-05 - val_loss: 1.5983e-07
Epoch 111/512
448/448 - 0s - loss: 2.3494e-05 - val_loss: 1.5658e-07
Epoch 112/512
448/448 - 0s - loss: 2.2936e-05 - val_loss: 1.5737e-07
Epoch 113/512
448/448 - 0s - loss: 2.3191e-05 - val_loss: 1.5160e-07
Epoch 114/512
448/448 - 0s - loss: 2.2055e-05 - val_loss: 1.4828e-07
Epoch 115/512
448/448 - 0s - loss: 2.1989e-05 - val_loss: 1.4606e-07
Epoch 116/512
448/448 - 0s - loss: 2.1578e-05 - val_loss: 1.4134e-07
Epoch 117/512
448/448 - 0s - loss: 2.0952e-05 - val_loss: 1.3960e-07
Epoch 118/512
448/448 - 0s - loss: 2.0953e-05 - val_loss: 1.3652e-07
Epoch 119/512
448/448 - 0s - loss: 2.0428e-05 - val_loss: 1.2940e-07
Epoch 120/512
448/448 - 0s - loss: 1.9503e-05 - val_loss: 1.2805e-07
Epoch 121/512
448/448 - 0s - loss: 1.9640e-05 - val_loss: 1.2765e-07
Epoch 122/512
448/448 - 0s - loss: 1.9266e-05 - val_loss: 1.2382e-07
Epoch 123/512
448/448 - 0s - loss: 1.8591e-05 - val_loss: 1.2170e-07
Epoch 124/512
448/448 - 0s - loss: 1.8462e-05 - val_loss: 1.1871e-07
Epoch 125/512
448/448 - 0s - loss: 1.8072e-05 - val_loss: 1.1493e-07
Epoch 126/512
448/448 - 0s - loss: 1.7549e-05 - val_loss: 1.1176e-07
Epoch 127/512
448/448 - 0s - loss: 1.7425e-05 - val_loss: 1.0778e-07
Epoch 128/512
448/448 - 0s - loss: 1.6611e-05 - val_loss: 1.0673e-07
Epoch 129/512
448/448 - 0s - loss: 1.6778e-05 - val_loss: 1.0461e-07
Epoch 130/512
448/448 - 0s - loss: 1.6248e-05 - val_loss: 1.0074e-07
Epoch 131/512
448/448 - 0s - loss: 1.5726e-05 - val_loss: 9.8389e-08
Epoch 132/512
448/448 - 0s - loss: 1.5661e-05 - val_loss: 9.5702e-08
Epoch 133/512
448/448 - 0s - loss: 1.5049e-05 - val_loss: 9.4765e-08
Epoch 134/512
448/448 - 0s - loss: 1.5055e-05 - val_loss: 9.0751e-08
Epoch 135/512
448/448 - 0s - loss: 1.4366e-05 - val_loss: 8.8873e-08
Epoch 136/512
448/448 - 0s - loss: 1.4342e-05 - val_loss: 8.6353e-08
Epoch 137/512
448/448 - 0s - loss: 1.3886e-05 - val_loss: 8.2539e-08
Epoch 138/512
448/448 - 0s - loss: 1.3297e-05 - val_loss: 8.2889e-08
Epoch 139/512
448/448 - 0s - loss: 1.3594e-05 - val_loss: 8.0268e-08
Epoch 140/512
448/448 - 0s - loss: 1.3056e-05 - val_loss: 7.5030e-08
Epoch 141/512
448/448 - 0s - loss: 1.2299e-05 - val_loss: 7.4563e-08
Epoch 142/512
448/448 - 0s - loss: 1.2489e-05 - val_loss: 7.4707e-08
Epoch 143/512
448/448 - 0s - loss: 1.2312e-05 - val_loss: 7.0677e-08
Epoch 144/512
448/448 - 0s - loss: 1.1604e-05 - val_loss: 6.8662e-08
Epoch 145/512
448/448 - 0s - loss: 1.1477e-05 - val_loss: 6.7439e-08
Epoch 146/512
448/448 - 0s - loss: 1.1176e-05 - val_loss: 6.6868e-08
Epoch 147/512
448/448 - 0s - loss: 1.1151e-05 - val_loss: 6.3976e-08
Epoch 148/512
448/448 - 0s - loss: 1.0632e-05 - val_loss: 6.0890e-08
Epoch 149/512
448/448 - 0s - loss: 1.0234e-05 - val_loss: 6.0419e-08
Epoch 150/512
448/448 - 0s - loss: 1.0241e-05 - val_loss: 5.9775e-08
Epoch 151/512
448/448 - 0s - loss: 1.0074e-05 - val_loss: 5.6744e-08
Epoch 152/512
448/448 - 0s - loss: 9.4809e-06 - val_loss: 5.5211e-08
Epoch 153/512
448/448 - 0s - loss: 9.3262e-06 - val_loss: 5.5314e-08
Epoch 154/512
448/448 - 0s - loss: 9.3241e-06 - val_loss: 5.3751e-08
Epoch 155/512
448/448 - 0s - loss: 8.9752e-06 - val_loss: 5.0513e-08
Epoch 156/512
448/448 - 0s - loss: 8.4489e-06 - val_loss: 5.0253e-08
Epoch 157/512
448/448 - 0s - loss: 8.5038e-06 - val_loss: 5.0216e-08
Epoch 158/512
448/448 - 0s - loss: 8.4404e-06 - val_loss: 4.6602e-08
Epoch 159/512
448/448 - 0s - loss: 7.7037e-06 - val_loss: 4.6178e-08
Epoch 160/512
448/448 - 0s - loss: 7.8523e-06 - val_loss: 4.6049e-08
Epoch 161/512
448/448 - 0s - loss: 7.6959e-06 - val_loss: 4.3641e-08
Epoch 162/512
448/448 - 0s - loss: 7.2192e-06 - val_loss: 4.2554e-08
Epoch 163/512
448/448 - 0s - loss: 7.1136e-06 - val_loss: 4.2665e-08
Epoch 164/512
448/448 - 0s - loss: 7.1041e-06 - val_loss: 4.0781e-08
Epoch 165/512
448/448 - 0s - loss: 6.6850e-06 - val_loss: 3.8823e-08
Epoch 166/512
448/448 - 0s - loss: 6.4483e-06 - val_loss: 3.8888e-08
Epoch 167/512
448/448 - 0s - loss: 6.4407e-06 - val_loss: 3.8312e-08
Epoch 168/512
448/448 - 0s - loss: 6.2422e-06 - val_loss: 3.6510e-08
Epoch 169/512
448/448 - 0s - loss: 5.9093e-06 - val_loss: 3.5763e-08
Epoch 170/512
448/448 - 0s - loss: 5.8457e-06 - val_loss: 3.5265e-08
Epoch 171/512
448/448 - 0s - loss: 5.6832e-06 - val_loss: 3.3972e-08
Epoch 172/512
448/448 - 0s - loss: 5.4448e-06 - val_loss: 3.2926e-08
Epoch 173/512
448/448 - 0s - loss: 5.2801e-06 - val_loss: 3.2695e-08
Epoch 174/512
448/448 - 0s - loss: 5.2702e-06 - val_loss: 3.0916e-08
Epoch 175/512
448/448 - 0s - loss: 4.8654e-06 - val_loss: 3.0230e-08
Epoch 176/512
448/448 - 0s - loss: 4.8217e-06 - val_loss: 3.0477e-08
Epoch 177/512
448/448 - 0s - loss: 4.8119e-06 - val_loss: 2.9135e-08
Epoch 178/512
448/448 - 0s - loss: 4.4956e-06 - val_loss: 2.7830e-08
Epoch 179/512
448/448 - 0s - loss: 4.3599e-06 - val_loss: 2.7357e-08
Epoch 180/512
448/448 - 0s - loss: 4.2420e-06 - val_loss: 2.7366e-08
Epoch 181/512
448/448 - 0s - loss: 4.1932e-06 - val_loss: 2.6469e-08
Epoch 182/512
448/448 - 0s - loss: 3.9930e-06 - val_loss: 2.5274e-08
Epoch 183/512
448/448 - 0s - loss: 3.8133e-06 - val_loss: 2.4848e-08
Epoch 184/512
448/448 - 0s - loss: 3.7634e-06 - val_loss: 2.4212e-08
Epoch 185/512
448/448 - 0s - loss: 3.5765e-06 - val_loss: 2.3991e-08
Epoch 186/512
448/448 - 0s - loss: 3.5500e-06 - val_loss: 2.3199e-08
Epoch 187/512
448/448 - 0s - loss: 3.3642e-06 - val_loss: 2.2411e-08
Epoch 188/512
448/448 - 0s - loss: 3.2337e-06 - val_loss: 2.2205e-08
Epoch 189/512
448/448 - 0s - loss: 3.1961e-06 - val_loss: 2.1469e-08
Epoch 190/512
448/448 - 0s - loss: 3.0404e-06 - val_loss: 2.0563e-08
Epoch 191/512
448/448 - 0s - loss: 2.8972e-06 - val_loss: 2.0445e-08
Epoch 192/512
448/448 - 0s - loss: 2.8746e-06 - val_loss: 2.0157e-08
Epoch 193/512
448/448 - 0s - loss: 2.7752e-06 - val_loss: 1.9113e-08
Epoch 194/512
448/448 - 0s - loss: 2.5986e-06 - val_loss: 1.8587e-08
Epoch 195/512
448/448 - 0s - loss: 2.5390e-06 - val_loss: 1.8623e-08
Epoch 196/512
448/448 - 0s - loss: 2.4838e-06 - val_loss: 1.8398e-08
Epoch 197/512
448/448 - 0s - loss: 2.4101e-06 - val_loss: 1.7517e-08
Epoch 198/512
448/448 - 0s - loss: 2.2534e-06 - val_loss: 1.7133e-08
Epoch 199/512
448/448 - 0s - loss: 2.2282e-06 - val_loss: 1.6704e-08
Epoch 200/512
448/448 - 0s - loss: 2.1205e-06 - val_loss: 1.6319e-08
Epoch 201/512
448/448 - 0s - loss: 2.0544e-06 - val_loss: 1.6063e-08
Epoch 202/512
448/448 - 0s - loss: 1.9841e-06 - val_loss: 1.5688e-08
Epoch 203/512
448/448 - 0s - loss: 1.9252e-06 - val_loss: 1.5088e-08
Epoch 204/512
448/448 - 0s - loss: 1.8273e-06 - val_loss: 1.4587e-08
Epoch 205/512
448/448 - 0s - loss: 1.7552e-06 - val_loss: 1.4437e-08
Epoch 206/512
448/448 - 0s - loss: 1.7288e-06 - val_loss: 1.3944e-08
Epoch 207/512
448/448 - 0s - loss: 1.6124e-06 - val_loss: 1.3873e-08
Epoch 208/512
448/448 - 0s - loss: 1.6101e-06 - val_loss: 1.3420e-08
Epoch 209/512
448/448 - 0s - loss: 1.5104e-06 - val_loss: 1.2994e-08
Epoch 210/512
448/448 - 0s - loss: 1.4459e-06 - val_loss: 1.3044e-08
Epoch 211/512
448/448 - 0s - loss: 1.4500e-06 - val_loss: 1.2366e-08
Epoch 212/512
448/448 - 0s - loss: 1.3220e-06 - val_loss: 1.1965e-08
Epoch 213/512
448/448 - 0s - loss: 1.2794e-06 - val_loss: 1.2119e-08
Epoch 214/512
448/448 - 0s - loss: 1.2841e-06 - val_loss: 1.1955e-08
Epoch 215/512
448/448 - 0s - loss: 1.2360e-06 - val_loss: 1.1005e-08
Epoch 216/512
448/448 - 0s - loss: 1.0879e-06 - val_loss: 1.1059e-08
Epoch 217/512
448/448 - 0s - loss: 1.1334e-06 - val_loss: 1.1248e-08
Epoch 218/512
448/448 - 0s - loss: 1.1087e-06 - val_loss: 1.0524e-08
Epoch 219/512
448/448 - 0s - loss: 9.8831e-07 - val_loss: 1.0188e-08
Epoch 220/512
448/448 - 0s - loss: 9.7258e-07 - val_loss: 1.0360e-08
Epoch 221/512
448/448 - 0s - loss: 9.8589e-07 - val_loss: 9.9958e-09
Epoch 222/512
448/448 - 0s - loss: 9.0627e-07 - val_loss: 9.3983e-09
Epoch 223/512
448/448 - 0s - loss: 8.4071e-07 - val_loss: 9.3734e-09
Epoch 224/512
448/448 - 0s - loss: 8.4981e-07 - val_loss: 9.3804e-09
Epoch 225/512
448/448 - 0s - loss: 8.1894e-07 - val_loss: 9.0288e-09
Epoch 226/512
448/448 - 0s - loss: 7.6432e-07 - val_loss: 8.6562e-09
Epoch 227/512
448/448 - 0s - loss: 7.2290e-07 - val_loss: 8.6306e-09
Epoch 228/512
448/448 - 0s - loss: 7.2072e-07 - val_loss: 8.5466e-09
Epoch 229/512
448/448 - 0s - loss: 6.8405e-07 - val_loss: 8.3480e-09
Epoch 230/512
448/448 - 0s - loss: 6.6102e-07 - val_loss: 7.9561e-09
Epoch 231/512
448/448 - 0s - loss: 6.0653e-07 - val_loss: 7.8761e-09
Epoch 232/512
448/448 - 0s - loss: 6.0839e-07 - val_loss: 7.7866e-09
Epoch 233/512
448/448 - 0s - loss: 5.8441e-07 - val_loss: 7.4801e-09
Epoch 234/512
448/448 - 0s - loss: 5.4030e-07 - val_loss: 7.3303e-09
Epoch 235/512
448/448 - 0s - loss: 5.2936e-07 - val_loss: 7.2327e-09
Epoch 236/512
448/448 - 0s - loss: 5.1171e-07 - val_loss: 7.0215e-09
Epoch 237/512
448/448 - 0s - loss: 4.7992e-07 - val_loss: 6.8249e-09
Epoch 238/512
448/448 - 0s - loss: 4.5999e-07 - val_loss: 6.7221e-09
Epoch 239/512
448/448 - 0s - loss: 4.4468e-07 - val_loss: 6.6669e-09
Epoch 240/512
448/448 - 0s - loss: 4.3423e-07 - val_loss: 6.4455e-09
Epoch 241/512
448/448 - 0s - loss: 4.0017e-07 - val_loss: 6.3284e-09
Epoch 242/512
448/448 - 0s - loss: 3.9428e-07 - val_loss: 6.1682e-09
Epoch 243/512
448/448 - 0s - loss: 3.6950e-07 - val_loss: 6.0625e-09
Epoch 244/512
448/448 - 0s - loss: 3.5939e-07 - val_loss: 5.9316e-09
Epoch 245/512
448/448 - 0s - loss: 3.4150e-07 - val_loss: 5.7723e-09
Epoch 246/512
448/448 - 0s - loss: 3.2278e-07 - val_loss: 5.7236e-09
Epoch 247/512
448/448 - 0s - loss: 3.1852e-07 - val_loss: 5.5772e-09
Epoch 248/512
448/448 - 0s - loss: 2.9934e-07 - val_loss: 5.4116e-09
Epoch 249/512
448/448 - 0s - loss: 2.8240e-07 - val_loss: 5.3177e-09
Epoch 250/512
448/448 - 0s - loss: 2.7385e-07 - val_loss: 5.2787e-09
Epoch 251/512
448/448 - 0s - loss: 2.6656e-07 - val_loss: 5.1608e-09
Epoch 252/512
448/448 - 0s - loss: 2.5206e-07 - val_loss: 4.9819e-09
Epoch 253/512
448/448 - 0s - loss: 2.3262e-07 - val_loss: 4.9524e-09
Epoch 254/512
448/448 - 0s - loss: 2.3419e-07 - val_loss: 4.8704e-09
Epoch 255/512
448/448 - 0s - loss: 2.2178e-07 - val_loss: 4.6895e-09
Epoch 256/512
448/448 - 0s - loss: 2.0395e-07 - val_loss: 4.6179e-09
Epoch 257/512
448/448 - 0s - loss: 2.0196e-07 - val_loss: 4.5406e-09
Epoch 258/512
448/448 - 0s - loss: 1.9233e-07 - val_loss: 4.4327e-09
Epoch 259/512
448/448 - 0s - loss: 1.8113e-07 - val_loss: 4.3662e-09
Epoch 260/512
448/448 - 0s - loss: 1.7508e-07 - val_loss: 4.3041e-09
Epoch 261/512
448/448 - 0s - loss: 1.6977e-07 - val_loss: 4.1777e-09
Epoch 262/512
448/448 - 0s - loss: 1.5624e-07 - val_loss: 4.1171e-09
Epoch 263/512
448/448 - 0s - loss: 1.5355e-07 - val_loss: 4.0587e-09
Epoch 264/512
448/448 - 0s - loss: 1.4783e-07 - val_loss: 3.9575e-09
Epoch 265/512
448/448 - 0s - loss: 1.3767e-07 - val_loss: 3.8713e-09
Epoch 266/512
448/448 - 0s - loss: 1.3206e-07 - val_loss: 3.8303e-09
Epoch 267/512
448/448 - 0s - loss: 1.2879e-07 - val_loss: 3.7627e-09
Epoch 268/512
448/448 - 0s - loss: 1.2237e-07 - val_loss: 3.6597e-09
Epoch 269/512
448/448 - 0s - loss: 1.1492e-07 - val_loss: 3.5726e-09
Epoch 270/512
448/448 - 0s - loss: 1.0792e-07 - val_loss: 3.5807e-09
Epoch 271/512
448/448 - 0s - loss: 1.1018e-07 - val_loss: 3.5017e-09
Epoch 272/512
448/448 - 0s - loss: 1.0138e-07 - val_loss: 3.3814e-09
Epoch 273/512
448/448 - 0s - loss: 9.2679e-08 - val_loss: 3.3641e-09
Epoch 274/512
448/448 - 0s - loss: 9.4167e-08 - val_loss: 3.3477e-09
Epoch 275/512
448/448 - 0s - loss: 9.1251e-08 - val_loss: 3.2367e-09
Epoch 276/512
448/448 - 0s - loss: 8.2575e-08 - val_loss: 3.1439e-09
Epoch 277/512
448/448 - 0s - loss: 7.7587e-08 - val_loss: 3.1396e-09
Epoch 278/512
448/448 - 0s - loss: 7.8834e-08 - val_loss: 3.1204e-09
Epoch 279/512
448/448 - 0s - loss: 7.6141e-08 - val_loss: 3.0138e-09
Epoch 280/512
448/448 - 0s - loss: 6.7789e-08 - val_loss: 2.9556e-09
Epoch 281/512
448/448 - 0s - loss: 6.6323e-08 - val_loss: 2.9389e-09
Epoch 282/512
448/448 - 0s - loss: 6.5804e-08 - val_loss: 2.8816e-09
Epoch 283/512
448/448 - 0s - loss: 6.1010e-08 - val_loss: 2.8236e-09
Epoch 284/512
448/448 - 0s - loss: 5.8041e-08 - val_loss: 2.7734e-09
Epoch 285/512
448/448 - 0s - loss: 5.5513e-08 - val_loss: 2.7506e-09
Epoch 286/512
448/448 - 0s - loss: 5.4284e-08 - val_loss: 2.7060e-09
Epoch 287/512
448/448 - 0s - loss: 5.1082e-08 - val_loss: 2.6441e-09
Epoch 288/512
448/448 - 0s - loss: 4.8025e-08 - val_loss: 2.6101e-09
Epoch 289/512
448/448 - 0s - loss: 4.7015e-08 - val_loss: 2.5750e-09
Epoch 290/512
448/448 - 0s - loss: 4.4612e-08 - val_loss: 2.5432e-09
Epoch 291/512
448/448 - 0s - loss: 4.3206e-08 - val_loss: 2.4898e-09
Epoch 292/512
448/448 - 0s - loss: 3.9931e-08 - val_loss: 2.4599e-09
Epoch 293/512
448/448 - 0s - loss: 3.9437e-08 - val_loss: 2.4228e-09
Epoch 294/512
448/448 - 0s - loss: 3.7326e-08 - val_loss: 2.3768e-09
Epoch 295/512
448/448 - 0s - loss: 3.4756e-08 - val_loss: 2.3596e-09
Epoch 296/512
448/448 - 0s - loss: 3.4858e-08 - val_loss: 2.3323e-09
Epoch 297/512
448/448 - 0s - loss: 3.3098e-08 - val_loss: 2.2814e-09
Epoch 298/512
448/448 - 0s - loss: 3.0716e-08 - val_loss: 2.2467e-09
Epoch 299/512
448/448 - 0s - loss: 2.9465e-08 - val_loss: 2.2337e-09
Epoch 300/512
448/448 - 0s - loss: 2.9488e-08 - val_loss: 2.2002e-09
Epoch 301/512
448/448 - 0s - loss: 2.7645e-08 - val_loss: 2.1528e-09
Epoch 302/512
448/448 - 0s - loss: 2.5516e-08 - val_loss: 2.1258e-09
Epoch 303/512
448/448 - 0s - loss: 2.4921e-08 - val_loss: 2.1149e-09
Epoch 304/512
448/448 - 0s - loss: 2.4925e-08 - val_loss: 2.0806e-09
Epoch 305/512
448/448 - 0s - loss: 2.2999e-08 - val_loss: 2.0430e-09
Epoch 306/512
448/448 - 0s - loss: 2.1515e-08 - val_loss: 2.0246e-09
Epoch 307/512
448/448 - 0s - loss: 2.1512e-08 - val_loss: 2.0043e-09
Epoch 308/512
448/448 - 0s - loss: 2.0536e-08 - val_loss: 1.9792e-09
Epoch 309/512
448/448 - 0s - loss: 1.9646e-08 - val_loss: 1.9429e-09
Epoch 310/512
448/448 - 0s - loss: 1.8215e-08 - val_loss: 1.9247e-09
Epoch 311/512
448/448 - 0s - loss: 1.8080e-08 - val_loss: 1.9104e-09
Epoch 312/512
448/448 - 0s - loss: 1.7501e-08 - val_loss: 1.8852e-09
Epoch 313/512
448/448 - 0s - loss: 1.6671e-08 - val_loss: 1.8598e-09
Epoch 314/512
448/448 - 0s - loss: 1.5875e-08 - val_loss: 1.8318e-09
Epoch 315/512
448/448 - 0s - loss: 1.5174e-08 - val_loss: 1.8119e-09
Epoch 316/512
448/448 - 0s - loss: 1.4580e-08 - val_loss: 1.7947e-09
Epoch 317/512
448/448 - 0s - loss: 1.4350e-08 - val_loss: 1.7738e-09
Epoch 318/512
448/448 - 0s - loss: 1.3672e-08 - val_loss: 1.7483e-09
Epoch 319/512
448/448 - 0s - loss: 1.2937e-08 - val_loss: 1.7210e-09
Epoch 320/512
448/448 - 0s - loss: 1.2173e-08 - val_loss: 1.7053e-09
Epoch 321/512
448/448 - 0s - loss: 1.2014e-08 - val_loss: 1.6958e-09
Epoch 322/512
448/448 - 0s - loss: 1.1941e-08 - val_loss: 1.6773e-09
Epoch 323/512
448/448 - 0s - loss: 1.1365e-08 - val_loss: 1.6541e-09
Epoch 324/512
448/448 - 0s - loss: 1.0606e-08 - val_loss: 1.6291e-09
Epoch 325/512
448/448 - 0s - loss: 1.0045e-08 - val_loss: 1.6165e-09
Epoch 326/512
448/448 - 0s - loss: 1.0012e-08 - val_loss: 1.6108e-09
Epoch 327/512
448/448 - 0s - loss: 1.0111e-08 - val_loss: 1.5853e-09
Epoch 328/512
448/448 - 0s - loss: 9.1755e-09 - val_loss: 1.5630e-09
Epoch 329/512
448/448 - 0s - loss: 8.7846e-09 - val_loss: 1.5510e-09
Epoch 330/512
448/448 - 0s - loss: 8.6957e-09 - val_loss: 1.5367e-09
Epoch 331/512
448/448 - 0s - loss: 8.4277e-09 - val_loss: 1.5226e-09
Epoch 332/512
448/448 - 0s - loss: 8.2028e-09 - val_loss: 1.5070e-09
Epoch 333/512
448/448 - 0s - loss: 7.8434e-09 - val_loss: 1.4853e-09
Epoch 334/512
448/448 - 0s - loss: 7.3760e-09 - val_loss: 1.4656e-09
Epoch 335/512
448/448 - 0s - loss: 7.0711e-09 - val_loss: 1.4587e-09
Epoch 336/512
448/448 - 0s - loss: 7.1450e-09 - val_loss: 1.4508e-09
Epoch 337/512
448/448 - 0s - loss: 7.0884e-09 - val_loss: 1.4313e-09
Epoch 338/512
448/448 - 0s - loss: 6.5519e-09 - val_loss: 1.4151e-09
Epoch 339/512
448/448 - 0s - loss: 6.3561e-09 - val_loss: 1.3986e-09
Epoch 340/512
448/448 - 0s - loss: 6.0889e-09 - val_loss: 1.3885e-09
Epoch 341/512
448/448 - 0s - loss: 5.9529e-09 - val_loss: 1.3766e-09
Epoch 342/512
448/448 - 0s - loss: 5.8930e-09 - val_loss: 1.3626e-09
Epoch 343/512
448/448 - 0s - loss: 5.5641e-09 - val_loss: 1.3497e-09
Epoch 344/512
448/448 - 0s - loss: 5.4806e-09 - val_loss: 1.3398e-09
Epoch 345/512
448/448 - 0s - loss: 5.3821e-09 - val_loss: 1.3235e-09
Epoch 346/512
448/448 - 0s - loss: 5.0603e-09 - val_loss: 1.3089e-09
Epoch 347/512
448/448 - 0s - loss: 4.8813e-09 - val_loss: 1.2995e-09
Epoch 348/512
448/448 - 0s - loss: 4.8127e-09 - val_loss: 1.2898e-09
Epoch 349/512
448/448 - 0s - loss: 4.7302e-09 - val_loss: 1.2794e-09
Epoch 350/512
448/448 - 0s - loss: 4.6613e-09 - val_loss: 1.2666e-09
Epoch 351/512
448/448 - 0s - loss: 4.4011e-09 - val_loss: 1.2548e-09
Epoch 352/512
448/448 - 0s - loss: 4.3053e-09 - val_loss: 1.2443e-09
Epoch 353/512
448/448 - 0s - loss: 4.2429e-09 - val_loss: 1.2338e-09
Epoch 354/512
448/448 - 0s - loss: 4.0889e-09 - val_loss: 1.2240e-09
Epoch 355/512
448/448 - 0s - loss: 3.9989e-09 - val_loss: 1.2108e-09
Epoch 356/512
448/448 - 0s - loss: 3.8066e-09 - val_loss: 1.1991e-09
Epoch 357/512
448/448 - 0s - loss: 3.7403e-09 - val_loss: 1.1914e-09
Epoch 358/512
448/448 - 0s - loss: 3.7068e-09 - val_loss: 1.1828e-09
Epoch 359/512
448/448 - 0s - loss: 3.6733e-09 - val_loss: 1.1716e-09
Epoch 360/512
448/448 - 0s - loss: 3.4959e-09 - val_loss: 1.1605e-09
Epoch 361/512
448/448 - 0s - loss: 3.3455e-09 - val_loss: 1.1512e-09
Epoch 362/512
448/448 - 0s - loss: 3.2973e-09 - val_loss: 1.1425e-09
Epoch 363/512
448/448 - 0s - loss: 3.2632e-09 - val_loss: 1.1352e-09
Epoch 364/512
448/448 - 0s - loss: 3.1840e-09 - val_loss: 1.1251e-09
Epoch 365/512
448/448 - 0s - loss: 3.0908e-09 - val_loss: 1.1138e-09
Epoch 366/512
448/448 - 0s - loss: 2.9688e-09 - val_loss: 1.1062e-09
Epoch 367/512
448/448 - 0s - loss: 2.9355e-09 - val_loss: 1.0977e-09
Epoch 368/512
448/448 - 0s - loss: 2.9045e-09 - val_loss: 1.0869e-09
Epoch 369/512
448/448 - 0s - loss: 2.7597e-09 - val_loss: 1.0777e-09
Epoch 370/512
448/448 - 0s - loss: 2.7263e-09 - val_loss: 1.0737e-09
Epoch 371/512
448/448 - 0s - loss: 2.7446e-09 - val_loss: 1.0623e-09
Epoch 372/512
448/448 - 0s - loss: 2.5953e-09 - val_loss: 1.0552e-09
Epoch 373/512
448/448 - 0s - loss: 2.5418e-09 - val_loss: 1.0479e-09
Epoch 374/512
448/448 - 0s - loss: 2.5062e-09 - val_loss: 1.0394e-09
Epoch 375/512
448/448 - 0s - loss: 2.4558e-09 - val_loss: 1.0325e-09
Epoch 376/512
448/448 - 0s - loss: 2.4411e-09 - val_loss: 1.0234e-09
Epoch 377/512
448/448 - 0s - loss: 2.3327e-09 - val_loss: 1.0145e-09
Epoch 378/512
448/448 - 0s - loss: 2.2705e-09 - val_loss: 1.0089e-09
Epoch 379/512
448/448 - 0s - loss: 2.2701e-09 - val_loss: 1.0006e-09
Epoch 380/512
448/448 - 0s - loss: 2.1717e-09 - val_loss: 9.9214e-10
Epoch 381/512
448/448 - 0s - loss: 2.1292e-09 - val_loss: 9.8725e-10
Epoch 382/512
448/448 - 0s - loss: 2.1482e-09 - val_loss: 9.8228e-10
Epoch 383/512
448/448 - 0s - loss: 2.1512e-09 - val_loss: 9.7236e-10
Epoch 384/512
448/448 - 0s - loss: 2.0425e-09 - val_loss: 9.6380e-10
Epoch 385/512
448/448 - 0s - loss: 1.9283e-09 - val_loss: 9.5470e-10
Epoch 386/512
448/448 - 0s - loss: 1.8780e-09 - val_loss: 9.5088e-10
Epoch 387/512
448/448 - 0s - loss: 1.9466e-09 - val_loss: 9.4863e-10
Epoch 388/512
448/448 - 0s - loss: 1.9943e-09 - val_loss: 9.4099e-10
Epoch 389/512
448/448 - 0s - loss: 1.8846e-09 - val_loss: 9.3188e-10
Epoch 390/512
448/448 - 0s - loss: 1.7809e-09 - val_loss: 9.2536e-10
Epoch 391/512
448/448 - 0s - loss: 1.7622e-09 - val_loss: 9.2018e-10
Epoch 392/512
448/448 - 0s - loss: 1.7728e-09 - val_loss: 9.1414e-10
Epoch 393/512
448/448 - 0s - loss: 1.7188e-09 - val_loss: 9.0738e-10
Epoch 394/512
448/448 - 0s - loss: 1.6900e-09 - val_loss: 8.9970e-10
Epoch 395/512
448/448 - 0s - loss: 1.6423e-09 - val_loss: 8.9482e-10
Epoch 396/512
448/448 - 0s - loss: 1.6390e-09 - val_loss: 8.9002e-10
Epoch 397/512
448/448 - 0s - loss: 1.6195e-09 - val_loss: 8.8416e-10
Epoch 398/512
448/448 - 0s - loss: 1.5934e-09 - val_loss: 8.7813e-10
Epoch 399/512
448/448 - 0s - loss: 1.5559e-09 - val_loss: 8.7209e-10
Epoch 400/512
448/448 - 0s - loss: 1.5174e-09 - val_loss: 8.6829e-10
Epoch 401/512
448/448 - 0s - loss: 1.5306e-09 - val_loss: 8.6172e-10
Epoch 402/512
448/448 - 0s - loss: 1.4945e-09 - val_loss: 8.5614e-10
Epoch 403/512
448/448 - 0s - loss: 1.4599e-09 - val_loss: 8.4948e-10
Epoch 404/512
448/448 - 0s - loss: 1.4057e-09 - val_loss: 8.4470e-10
Epoch 405/512
448/448 - 0s - loss: 1.4015e-09 - val_loss: 8.4094e-10
Epoch 406/512
448/448 - 0s - loss: 1.3989e-09 - val_loss: 8.3519e-10
Epoch 407/512
448/448 - 0s - loss: 1.3844e-09 - val_loss: 8.3076e-10
Epoch 408/512
448/448 - 0s - loss: 1.3607e-09 - val_loss: 8.2471e-10
Epoch 409/512
448/448 - 0s - loss: 1.3283e-09 - val_loss: 8.2008e-10
Epoch 410/512
448/448 - 0s - loss: 1.3216e-09 - val_loss: 8.1463e-10
Epoch 411/512
448/448 - 0s - loss: 1.2873e-09 - val_loss: 8.0937e-10
Epoch 412/512
448/448 - 0s - loss: 1.2616e-09 - val_loss: 8.0332e-10
Epoch 413/512
448/448 - 0s - loss: 1.2199e-09 - val_loss: 7.9792e-10
Epoch 414/512
448/448 - 0s - loss: 1.2012e-09 - val_loss: 7.9483e-10
Epoch 415/512
448/448 - 0s - loss: 1.2235e-09 - val_loss: 7.9182e-10
Epoch 416/512
448/448 - 0s - loss: 1.2227e-09 - val_loss: 7.8775e-10
Epoch 417/512
448/448 - 0s - loss: 1.2064e-09 - val_loss: 7.8251e-10
Epoch 418/512
448/448 - 0s - loss: 1.1890e-09 - val_loss: 7.7697e-10
Epoch 419/512
448/448 - 0s - loss: 1.1612e-09 - val_loss: 7.7232e-10
Epoch 420/512
448/448 - 0s - loss: 1.1534e-09 - val_loss: 7.6687e-10
Epoch 421/512
448/448 - 0s - loss: 1.0928e-09 - val_loss: 7.6317e-10
Epoch 422/512
448/448 - 0s - loss: 1.0979e-09 - val_loss: 7.5952e-10
Epoch 423/512
448/448 - 0s - loss: 1.1050e-09 - val_loss: 7.5541e-10
Epoch 424/512
448/448 - 0s - loss: 1.0843e-09 - val_loss: 7.5116e-10
Epoch 425/512
448/448 - 0s - loss: 1.0653e-09 - val_loss: 7.4622e-10
Epoch 426/512
448/448 - 0s - loss: 1.0490e-09 - val_loss: 7.4257e-10
Epoch 427/512
448/448 - 0s - loss: 1.0457e-09 - val_loss: 7.3947e-10
Epoch 428/512
448/448 - 0s - loss: 1.0416e-09 - val_loss: 7.3394e-10
Epoch 429/512
448/448 - 0s - loss: 9.9785e-10 - val_loss: 7.2986e-10
Epoch 430/512
448/448 - 0s - loss: 9.8723e-10 - val_loss: 7.2564e-10
Epoch 431/512
448/448 - 0s - loss: 9.6366e-10 - val_loss: 7.2325e-10
Epoch 432/512
448/448 - 0s - loss: 9.8883e-10 - val_loss: 7.2084e-10
Epoch 433/512
448/448 - 0s - loss: 1.0094e-09 - val_loss: 7.1598e-10
Epoch 434/512
448/448 - 0s - loss: 9.7203e-10 - val_loss: 7.1217e-10
Epoch 435/512
448/448 - 0s - loss: 9.5150e-10 - val_loss: 7.0648e-10
Epoch 436/512
448/448 - 0s - loss: 9.1089e-10 - val_loss: 7.0203e-10
Epoch 437/512
448/448 - 0s - loss: 8.8835e-10 - val_loss: 6.9946e-10
Epoch 438/512
448/448 - 0s - loss: 9.0925e-10 - val_loss: 6.9702e-10
Epoch 439/512
448/448 - 0s - loss: 9.1815e-10 - val_loss: 6.9445e-10
Epoch 440/512
448/448 - 0s - loss: 9.3004e-10 - val_loss: 6.9105e-10
Epoch 441/512
448/448 - 0s - loss: 9.1104e-10 - val_loss: 6.8657e-10
Epoch 442/512
448/448 - 0s - loss: 8.7407e-10 - val_loss: 6.8144e-10
Epoch 443/512
448/448 - 0s - loss: 8.4019e-10 - val_loss: 6.7752e-10
Epoch 444/512
448/448 - 0s - loss: 8.2115e-10 - val_loss: 6.7518e-10
Epoch 445/512
448/448 - 0s - loss: 8.3279e-10 - val_loss: 6.7247e-10
Epoch 446/512
448/448 - 0s - loss: 8.3925e-10 - val_loss: 6.6888e-10
Epoch 447/512
448/448 - 0s - loss: 8.2987e-10 - val_loss: 6.6595e-10
Epoch 448/512
448/448 - 0s - loss: 8.2996e-10 - val_loss: 6.6312e-10
Epoch 449/512
448/448 - 0s - loss: 8.1741e-10 - val_loss: 6.5981e-10
Epoch 450/512
448/448 - 0s - loss: 8.0767e-10 - val_loss: 6.5600e-10
Epoch 451/512
448/448 - 0s - loss: 7.8223e-10 - val_loss: 6.5273e-10
Epoch 452/512
448/448 - 0s - loss: 7.8416e-10 - val_loss: 6.4941e-10
Epoch 453/512
448/448 - 0s - loss: 7.7848e-10 - val_loss: 6.4655e-10
Epoch 454/512
448/448 - 0s - loss: 7.6389e-10 - val_loss: 6.4381e-10
Epoch 455/512
448/448 - 0s - loss: 7.6608e-10 - val_loss: 6.4096e-10
Epoch 456/512
448/448 - 0s - loss: 7.6034e-10 - val_loss: 6.3762e-10
Epoch 457/512
448/448 - 0s - loss: 7.3928e-10 - val_loss: 6.3303e-10
Epoch 458/512
448/448 - 0s - loss: 7.2001e-10 - val_loss: 6.3072e-10
Epoch 459/512
448/448 - 0s - loss: 7.1194e-10 - val_loss: 6.2816e-10
Epoch 460/512
448/448 - 0s - loss: 7.3116e-10 - val_loss: 6.2597e-10
Epoch 461/512
448/448 - 0s - loss: 7.3559e-10 - val_loss: 6.2349e-10
Epoch 462/512
448/448 - 0s - loss: 7.2119e-10 - val_loss: 6.2053e-10
Epoch 463/512
448/448 - 0s - loss: 7.1975e-10 - val_loss: 6.1704e-10
Epoch 464/512
448/448 - 0s - loss: 6.9373e-10 - val_loss: 6.1428e-10
Epoch 465/512
448/448 - 0s - loss: 7.0356e-10 - val_loss: 6.1178e-10
Epoch 466/512
448/448 - 0s - loss: 6.9693e-10 - val_loss: 6.0908e-10
Epoch 467/512
448/448 - 0s - loss: 6.8331e-10 - val_loss: 6.0521e-10
Epoch 468/512
448/448 - 0s - loss: 6.6038e-10 - val_loss: 6.0247e-10
Epoch 469/512
448/448 - 0s - loss: 6.5133e-10 - val_loss: 5.9886e-10
Epoch 470/512
448/448 - 0s - loss: 6.4880e-10 - val_loss: 5.9853e-10
Epoch 471/512
448/448 - 0s - loss: 6.6602e-10 - val_loss: 5.9581e-10
Epoch 472/512
448/448 - 0s - loss: 6.6122e-10 - val_loss: 5.9338e-10
Epoch 473/512
448/448 - 0s - loss: 6.4667e-10 - val_loss: 5.9015e-10
Epoch 474/512
448/448 - 0s - loss: 6.4954e-10 - val_loss: 5.8688e-10
Epoch 475/512
448/448 - 0s - loss: 6.1487e-10 - val_loss: 5.8386e-10
Epoch 476/512
448/448 - 0s - loss: 6.0788e-10 - val_loss: 5.8206e-10
Epoch 477/512
448/448 - 0s - loss: 6.1799e-10 - val_loss: 5.7942e-10
Epoch 478/512
448/448 - 0s - loss: 6.1892e-10 - val_loss: 5.7826e-10
Epoch 479/512
448/448 - 0s - loss: 6.3122e-10 - val_loss: 5.7567e-10
Epoch 480/512
448/448 - 0s - loss: 6.3784e-10 - val_loss: 5.7346e-10
Epoch 481/512
448/448 - 0s - loss: 6.1164e-10 - val_loss: 5.6995e-10
Epoch 482/512
448/448 - 0s - loss: 5.9190e-10 - val_loss: 5.6680e-10
Epoch 483/512
448/448 - 0s - loss: 5.7906e-10 - val_loss: 5.6455e-10
Epoch 484/512
448/448 - 0s - loss: 5.8012e-10 - val_loss: 5.6249e-10
Epoch 485/512
448/448 - 0s - loss: 5.7700e-10 - val_loss: 5.6094e-10
Epoch 486/512
448/448 - 0s - loss: 5.8033e-10 - val_loss: 5.5833e-10
Epoch 487/512
448/448 - 0s - loss: 5.8152e-10 - val_loss: 5.5686e-10
Epoch 488/512
448/448 - 0s - loss: 5.8113e-10 - val_loss: 5.5383e-10
Epoch 489/512
448/448 - 0s - loss: 5.6371e-10 - val_loss: 5.5154e-10
Epoch 490/512
448/448 - 0s - loss: 5.6142e-10 - val_loss: 5.4913e-10
Epoch 491/512
448/448 - 0s - loss: 5.5116e-10 - val_loss: 5.4689e-10
Epoch 492/512
448/448 - 0s - loss: 5.5039e-10 - val_loss: 5.4492e-10
Epoch 493/512
448/448 - 0s - loss: 5.5086e-10 - val_loss: 5.4283e-10
Epoch 494/512
448/448 - 0s - loss: 5.4991e-10 - val_loss: 5.4089e-10
Epoch 495/512
448/448 - 0s - loss: 5.4191e-10 - val_loss: 5.3784e-10
Epoch 496/512
448/448 - 0s - loss: 5.3015e-10 - val_loss: 5.3602e-10
Epoch 497/512
448/448 - 0s - loss: 5.2834e-10 - val_loss: 5.3397e-10
Epoch 498/512
448/448 - 0s - loss: 5.2703e-10 - val_loss: 5.3261e-10
Epoch 499/512
448/448 - 0s - loss: 5.2930e-10 - val_loss: 5.3035e-10
Epoch 500/512
448/448 - 0s - loss: 5.2661e-10 - val_loss: 5.2791e-10
Epoch 501/512
448/448 - 0s - loss: 5.1874e-10 - val_loss: 5.2523e-10
Epoch 502/512
448/448 - 0s - loss: 5.0391e-10 - val_loss: 5.2295e-10
Epoch 503/512
448/448 - 0s - loss: 4.9959e-10 - val_loss: 5.2188e-10
Epoch 504/512
448/448 - 0s - loss: 5.0436e-10 - val_loss: 5.2013e-10
Epoch 505/512
448/448 - 0s - loss: 5.0771e-10 - val_loss: 5.1865e-10
Epoch 506/512
448/448 - 0s - loss: 5.1064e-10 - val_loss: 5.1624e-10
Epoch 507/512
448/448 - 0s - loss: 5.0398e-10 - val_loss: 5.1394e-10
Epoch 508/512
448/448 - 0s - loss: 5.0032e-10 - val_loss: 5.1164e-10
Epoch 509/512
448/448 - 0s - loss: 4.8520e-10 - val_loss: 5.0975e-10
Epoch 510/512
448/448 - 0s - loss: 4.7939e-10 - val_loss: 5.0753e-10
Epoch 511/512
448/448 - 0s - loss: 4.7496e-10 - val_loss: 5.0518e-10
Epoch 512/512
448/448 - 0s - loss: 4.6225e-10 - val_loss: 5.0350e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3651e-08 - val_loss: 2.7774e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6401e-08 - val_loss: 1.6559e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.9340e-10 - val_loss: 3.4370e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1417e-10 - val_loss: 3.3975e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8341e-10 - val_loss: 1.1439e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4391e-09 - val_loss: 7.0994e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6880e-09 - val_loss: 7.9557e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1445e-09 - val_loss: 2.6334e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1952e-09 - val_loss: 1.5825e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7084e-09 - val_loss: 2.1668e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8480e-09 - val_loss: 4.2410e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0174e-09 - val_loss: 4.9374e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6421e-09 - val_loss: 3.2606e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0061e-09 - val_loss: 2.3736e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4286e-09 - val_loss: 2.4649e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7563e-09 - val_loss: 3.1060e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4192e-09 - val_loss: 3.4245e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4764e-09 - val_loss: 3.0068e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9528e-09 - val_loss: 2.5277e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5574e-09 - val_loss: 2.4232e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5533e-09 - val_loss: 2.5486e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6928e-09 - val_loss: 2.6639e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7557e-09 - val_loss: 2.5610e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5957e-09 - val_loss: 2.3606e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4062e-09 - val_loss: 2.2469e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3220e-09 - val_loss: 2.2417e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3267e-09 - val_loss: 2.2341e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3127e-09 - val_loss: 2.1973e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2479e-09 - val_loss: 2.1061e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1506e-09 - val_loss: 2.0224e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0758e-09 - val_loss: 1.9572e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0203e-09 - val_loss: 1.9282e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9988e-09 - val_loss: 1.9272e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9844e-09 - val_loss: 1.9066e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9495e-09 - val_loss: 1.8232e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8519e-09 - val_loss: 1.7338e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7776e-09 - val_loss: 1.7131e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7668e-09 - val_loss: 1.7104e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7598e-09 - val_loss: 1.6623e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6978e-09 - val_loss: 1.6003e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6418e-09 - val_loss: 1.5581e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5971e-09 - val_loss: 1.5205e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5671e-09 - val_loss: 1.5082e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5522e-09 - val_loss: 1.4856e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5298e-09 - val_loss: 1.4629e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4953e-09 - val_loss: 1.4206e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4520e-09 - val_loss: 1.3711e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4013e-09 - val_loss: 1.3337e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3699e-09 - val_loss: 1.3200e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3581e-09 - val_loss: 1.3018e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3345e-09 - val_loss: 1.2686e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3000e-09 - val_loss: 1.2420e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2724e-09 - val_loss: 1.2124e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2455e-09 - val_loss: 1.2023e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2354e-09 - val_loss: 1.1753e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2011e-09 - val_loss: 1.1381e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1649e-09 - val_loss: 1.1213e-09
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1508e-09 - val_loss: 1.1139e-09
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1429e-09 - val_loss: 1.0968e-09
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1221e-09 - val_loss: 1.0629e-09
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0855e-09 - val_loss: 1.0400e-09
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0646e-09 - val_loss: 1.0185e-09
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0436e-09 - val_loss: 1.0098e-09
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0325e-09 - val_loss: 9.8003e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0015e-09 - val_loss: 9.5951e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8348e-10 - val_loss: 9.5099e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7340e-10 - val_loss: 9.3623e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5611e-10 - val_loss: 9.1658e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3868e-10 - val_loss: 9.0484e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2584e-10 - val_loss: 8.9265e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1189e-10 - val_loss: 8.8728e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0446e-10 - val_loss: 8.7571e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8759e-10 - val_loss: 8.4707e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5942e-10 - val_loss: 8.3080e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4562e-10 - val_loss: 8.0553e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2044e-10 - val_loss: 7.9233e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1416e-10 - val_loss: 7.9488e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1486e-10 - val_loss: 7.9611e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0871e-10 - val_loss: 7.7960e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9469e-10 - val_loss: 7.5983e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7436e-10 - val_loss: 7.4705e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5779e-10 - val_loss: 7.2303e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3693e-10 - val_loss: 7.1777e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3137e-10 - val_loss: 7.0594e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1989e-10 - val_loss: 6.9545e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0884e-10 - val_loss: 6.8778e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9988e-10 - val_loss: 6.7280e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8674e-10 - val_loss: 6.6557e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7746e-10 - val_loss: 6.5955e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7431e-10 - val_loss: 6.6134e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7558e-10 - val_loss: 6.5157e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6207e-10 - val_loss: 6.3110e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4053e-10 - val_loss: 6.1890e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2861e-10 - val_loss: 6.0378e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1567e-10 - val_loss: 6.0186e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1243e-10 - val_loss: 5.9531e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0808e-10 - val_loss: 5.8504e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9538e-10 - val_loss: 5.7419e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8690e-10 - val_loss: 5.7783e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8865e-10 - val_loss: 5.6803e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7913e-10 - val_loss: 5.5877e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6853e-10 - val_loss: 5.5208e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5922e-10 - val_loss: 5.4348e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5493e-10 - val_loss: 5.3631e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4525e-10 - val_loss: 5.3483e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4310e-10 - val_loss: 5.2291e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2947e-10 - val_loss: 5.1038e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1665e-10 - val_loss: 5.0015e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0801e-10 - val_loss: 4.9764e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0778e-10 - val_loss: 4.9527e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0716e-10 - val_loss: 4.9710e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0800e-10 - val_loss: 4.9527e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0056e-10 - val_loss: 4.7784e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8497e-10 - val_loss: 4.6970e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7880e-10 - val_loss: 4.6159e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7105e-10 - val_loss: 4.6595e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7241e-10 - val_loss: 4.5498e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6130e-10 - val_loss: 4.5071e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5560e-10 - val_loss: 4.4626e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5187e-10 - val_loss: 4.3627e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4369e-10 - val_loss: 4.3809e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4446e-10 - val_loss: 4.3369e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3999e-10 - val_loss: 4.1977e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2588e-10 - val_loss: 4.1271e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1962e-10 - val_loss: 4.1050e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1724e-10 - val_loss: 4.0504e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1309e-10 - val_loss: 4.0493e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1149e-10 - val_loss: 3.9684e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0349e-10 - val_loss: 3.9616e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0451e-10 - val_loss: 4.0076e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0550e-10 - val_loss: 3.8928e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9414e-10 - val_loss: 3.8480e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9152e-10 - val_loss: 3.8283e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8576e-10 - val_loss: 3.7354e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7986e-10 - val_loss: 3.6957e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7313e-10 - val_loss: 3.6710e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7269e-10 - val_loss: 3.6299e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7034e-10 - val_loss: 3.6395e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7109e-10 - val_loss: 3.6457e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6844e-10 - val_loss: 3.5739e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5921e-10 - val_loss: 3.4887e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5363e-10 - val_loss: 3.4668e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5054e-10 - val_loss: 3.4176e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4749e-10 - val_loss: 3.4140e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.4722e-10 - val_loss: 3.3704e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.4093e-10 - val_loss: 3.3356e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3747e-10 - val_loss: 3.3334e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3922e-10 - val_loss: 3.2948e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3319e-10 - val_loss: 3.2375e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.2797e-10 - val_loss: 3.2153e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2614e-10 - val_loss: 3.2260e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.2531e-10 - val_loss: 3.1201e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1671e-10 - val_loss: 3.1092e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1472e-10 - val_loss: 3.0696e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1193e-10 - val_loss: 3.0847e-10
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1239e-10 - val_loss: 3.0456e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1014e-10 - val_loss: 3.0583e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0866e-10 - val_loss: 2.9993e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0387e-10 - val_loss: 2.9789e-10
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0245e-10 - val_loss: 2.9543e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0039e-10 - val_loss: 2.9681e-10
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0069e-10 - val_loss: 2.9160e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.9117e-10 - val_loss: 2.7814e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.8177e-10 - val_loss: 2.7548e-10
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7890e-10 - val_loss: 2.7819e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8283e-10 - val_loss: 2.7749e-10
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8090e-10 - val_loss: 2.7572e-10
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.7736e-10 - val_loss: 2.6755e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.7072e-10 - val_loss: 2.6409e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6830e-10 - val_loss: 2.6364e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6599e-10 - val_loss: 2.6141e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6623e-10 - val_loss: 2.6397e-10
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6707e-10 - val_loss: 2.6630e-10
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7082e-10 - val_loss: 2.6616e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6724e-10 - val_loss: 2.6123e-10
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6358e-10 - val_loss: 2.5743e-10
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5934e-10 - val_loss: 2.5298e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5687e-10 - val_loss: 2.5264e-10
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5538e-10 - val_loss: 2.5195e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5553e-10 - val_loss: 2.4945e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5203e-10 - val_loss: 2.4844e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5222e-10 - val_loss: 2.4348e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4485e-10 - val_loss: 2.4235e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4532e-10 - val_loss: 2.4434e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4650e-10 - val_loss: 2.4021e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4260e-10 - val_loss: 2.3712e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.3951e-10 - val_loss: 2.3565e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.3915e-10 - val_loss: 2.3426e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.3610e-10 - val_loss: 2.2838e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2978e-10 - val_loss: 2.2536e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2759e-10 - val_loss: 2.2454e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2622e-10 - val_loss: 2.2082e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2425e-10 - val_loss: 2.2490e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2855e-10 - val_loss: 2.2718e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3071e-10 - val_loss: 2.2807e-10
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2984e-10 - val_loss: 2.2443e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2579e-10 - val_loss: 2.1926e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2217e-10 - val_loss: 2.1700e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1937e-10 - val_loss: 2.1259e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1383e-10 - val_loss: 2.0844e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1109e-10 - val_loss: 2.1023e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1246e-10 - val_loss: 2.0778e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0897e-10 - val_loss: 2.0681e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1067e-10 - val_loss: 2.0875e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1398e-10 - val_loss: 2.1535e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1630e-10 - val_loss: 2.1209e-10
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1356e-10 - val_loss: 2.0920e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1202e-10 - val_loss: 2.0949e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1288e-10 - val_loss: 2.0876e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0904e-10 - val_loss: 2.0266e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0583e-10 - val_loss: 2.0018e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0120e-10 - val_loss: 1.9609e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9779e-10 - val_loss: 1.9572e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9817e-10 - val_loss: 1.9512e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9614e-10 - val_loss: 1.9477e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9806e-10 - val_loss: 1.9435e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9628e-10 - val_loss: 1.9343e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9511e-10 - val_loss: 1.9173e-10
Epoch 219/512

Epoch 00219: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9338e-10 - val_loss: 1.9065e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9516e-10 - val_loss: 1.9495e-10
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9716e-10 - val_loss: 1.9245e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9475e-10 - val_loss: 1.9026e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9037e-10 - val_loss: 1.8621e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8728e-10 - val_loss: 1.8135e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8261e-10 - val_loss: 1.7898e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8113e-10 - val_loss: 1.7974e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8176e-10 - val_loss: 1.8083e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8371e-10 - val_loss: 1.8161e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8267e-10 - val_loss: 1.8034e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8248e-10 - val_loss: 1.7724e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7856e-10 - val_loss: 1.7414e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7525e-10 - val_loss: 1.7300e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7397e-10 - val_loss: 1.6982e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7049e-10 - val_loss: 1.6631e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6833e-10 - val_loss: 1.6785e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6881e-10 - val_loss: 1.6567e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6670e-10 - val_loss: 1.6624e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6981e-10 - val_loss: 1.6864e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7032e-10 - val_loss: 1.6820e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6956e-10 - val_loss: 1.6580e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6795e-10 - val_loss: 1.6841e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6920e-10 - val_loss: 1.6487e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6658e-10 - val_loss: 1.6362e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6444e-10 - val_loss: 1.6029e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5942e-10 - val_loss: 1.5752e-10
Epoch 246/512

Epoch 00246: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5912e-10 - val_loss: 1.5652e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5658e-10 - val_loss: 1.5497e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5696e-10 - val_loss: 1.5436e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5606e-10 - val_loss: 1.5608e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5736e-10 - val_loss: 1.5579e-10
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5734e-10 - val_loss: 1.5630e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5742e-10 - val_loss: 1.5541e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5804e-10 - val_loss: 1.5660e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5756e-10 - val_loss: 1.5564e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5862e-10 - val_loss: 1.5600e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5714e-10 - val_loss: 1.5400e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5531e-10 - val_loss: 1.5180e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5150e-10 - val_loss: 1.4790e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4952e-10 - val_loss: 1.4735e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4827e-10 - val_loss: 1.4611e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4774e-10 - val_loss: 1.4674e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4861e-10 - val_loss: 1.4416e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4602e-10 - val_loss: 1.4637e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4860e-10 - val_loss: 1.4699e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4814e-10 - val_loss: 1.4559e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4667e-10 - val_loss: 1.4208e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4186e-10 - val_loss: 1.3855e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3895e-10 - val_loss: 1.3739e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4011e-10 - val_loss: 1.4259e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4537e-10 - val_loss: 1.4514e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4594e-10 - val_loss: 1.4258e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4420e-10 - val_loss: 1.4127e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4287e-10 - val_loss: 1.4070e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4287e-10 - val_loss: 1.3928e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4063e-10 - val_loss: 1.3802e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3879e-10 - val_loss: 1.3607e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3741e-10 - val_loss: 1.3590e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3599e-10 - val_loss: 1.3244e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3445e-10 - val_loss: 1.3526e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3703e-10 - val_loss: 1.3784e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3968e-10 - val_loss: 1.3687e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3794e-10 - val_loss: 1.3574e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3709e-10 - val_loss: 1.3730e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3752e-10 - val_loss: 1.3356e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3435e-10 - val_loss: 1.3219e-10
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3276e-10 - val_loss: 1.3004e-10
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2972e-10 - val_loss: 1.2689e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2854e-10 - val_loss: 1.2964e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3111e-10 - val_loss: 1.2997e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3091e-10 - val_loss: 1.3023e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3207e-10 - val_loss: 1.3106e-10
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3231e-10 - val_loss: 1.3193e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3259e-10 - val_loss: 1.2735e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2820e-10 - val_loss: 1.2440e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2603e-10 - val_loss: 1.2424e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2438e-10 - val_loss: 1.2322e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2441e-10 - val_loss: 1.2319e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2412e-10 - val_loss: 1.2376e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2489e-10 - val_loss: 1.2568e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2689e-10 - val_loss: 1.2481e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2548e-10 - val_loss: 1.2326e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2378e-10 - val_loss: 1.1934e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2037e-10 - val_loss: 1.1969e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1984e-10 - val_loss: 1.1846e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1867e-10 - val_loss: 1.1659e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1803e-10 - val_loss: 1.1826e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1932e-10 - val_loss: 1.1713e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1771e-10 - val_loss: 1.1779e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1953e-10 - val_loss: 1.1862e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2087e-10 - val_loss: 1.1946e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2001e-10 - val_loss: 1.1904e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1966e-10 - val_loss: 1.1893e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1836e-10 - val_loss: 1.1706e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1858e-10 - val_loss: 1.1919e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1982e-10 - val_loss: 1.1805e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1868e-10 - val_loss: 1.1752e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1856e-10 - val_loss: 1.1731e-10
Epoch 318/512

Epoch 00318: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1720e-10 - val_loss: 1.1348e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1348e-10 - val_loss: 1.1182e-10
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1328e-10 - val_loss: 1.1198e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1289e-10 - val_loss: 1.1204e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1270e-10 - val_loss: 1.1105e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1149e-10 - val_loss: 1.1078e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1212e-10 - val_loss: 1.1191e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1275e-10 - val_loss: 1.1240e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1373e-10 - val_loss: 1.1126e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1217e-10 - val_loss: 1.1111e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1262e-10 - val_loss: 1.1217e-10
Epoch 329/512

Epoch 00329: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1201e-10 - val_loss: 1.0981e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1092e-10 - val_loss: 1.0879e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1084e-10 - val_loss: 1.1114e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1236e-10 - val_loss: 1.1109e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1220e-10 - val_loss: 1.1107e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1120e-10 - val_loss: 1.0808e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0850e-10 - val_loss: 1.0734e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0837e-10 - val_loss: 1.0806e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0974e-10 - val_loss: 1.1127e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1174e-10 - val_loss: 1.0938e-10
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0964e-10 - val_loss: 1.0709e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0786e-10 - val_loss: 1.0585e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0532e-10 - val_loss: 1.0343e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0446e-10 - val_loss: 1.0460e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0623e-10 - val_loss: 1.0605e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0666e-10 - val_loss: 1.0442e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0520e-10 - val_loss: 1.0527e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0623e-10 - val_loss: 1.0529e-10
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0551e-10 - val_loss: 1.0287e-10
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0315e-10 - val_loss: 1.0226e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0323e-10 - val_loss: 1.0045e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0129e-10 - val_loss: 1.0105e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0167e-10 - val_loss: 1.0027e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0211e-10 - val_loss: 1.0370e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0530e-10 - val_loss: 1.0581e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0632e-10 - val_loss: 1.0650e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0784e-10 - val_loss: 1.0746e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0795e-10 - val_loss: 1.0643e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0593e-10 - val_loss: 1.0308e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0383e-10 - val_loss: 1.0342e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0396e-10 - val_loss: 1.0223e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0213e-10 - val_loss: 1.0085e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0134e-10 - val_loss: 9.9460e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0036e-10 - val_loss: 9.9476e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0062e-10 - val_loss: 1.0033e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0102e-10 - val_loss: 1.0012e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0191e-10 - val_loss: 1.0242e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0405e-10 - val_loss: 1.0270e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0296e-10 - val_loss: 1.0071e-10
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0035e-10 - val_loss: 9.9198e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.8894e-11 - val_loss: 9.7471e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.8618e-11 - val_loss: 9.7291e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.7421e-11 - val_loss: 9.7061e-11
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.7472e-11 - val_loss: 9.5834e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.4620e-11 - val_loss: 9.2364e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3981e-11 - val_loss: 9.3659e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4973e-11 - val_loss: 9.6204e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6585e-11 - val_loss: 9.5110e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5419e-11 - val_loss: 9.5030e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5425e-11 - val_loss: 9.4054e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4553e-11 - val_loss: 9.2744e-11
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.2850e-11 - val_loss: 9.2332e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3070e-11 - val_loss: 9.3106e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4531e-11 - val_loss: 9.5391e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6906e-11 - val_loss: 9.7289e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8212e-11 - val_loss: 9.7217e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8395e-11 - val_loss: 9.5927e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6339e-11 - val_loss: 9.5296e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5368e-11 - val_loss: 9.3174e-11
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.2522e-11 - val_loss: 9.0262e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1280e-11 - val_loss: 9.1186e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2550e-11 - val_loss: 9.2981e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2866e-11 - val_loss: 9.1878e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1812e-11 - val_loss: 9.0532e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0984e-11 - val_loss: 9.1065e-11
Epoch 394/512

Epoch 00394: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.1728e-11 - val_loss: 8.9988e-11
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.0024e-11 - val_loss: 8.8145e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8794e-11 - val_loss: 8.8443e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9307e-11 - val_loss: 8.9031e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0100e-11 - val_loss: 8.9069e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9580e-11 - val_loss: 8.9121e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9487e-11 - val_loss: 8.8310e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.7947e-11 - val_loss: 8.5341e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.5196e-11 - val_loss: 8.2570e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3284e-11 - val_loss: 8.4069e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5832e-11 - val_loss: 8.6689e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7240e-11 - val_loss: 8.6319e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6394e-11 - val_loss: 8.4910e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5642e-11 - val_loss: 8.5156e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5977e-11 - val_loss: 8.5226e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6097e-11 - val_loss: 8.6380e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6672e-11 - val_loss: 8.5063e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5187e-11 - val_loss: 8.3110e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3132e-11 - val_loss: 8.3571e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4745e-11 - val_loss: 8.5785e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6860e-11 - val_loss: 8.6203e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6759e-11 - val_loss: 8.4286e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.4410e-11 - val_loss: 8.2536e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2907e-11 - val_loss: 8.2727e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3369e-11 - val_loss: 8.2661e-11
Epoch 419/512

Epoch 00419: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.2942e-11 - val_loss: 8.1770e-11
Epoch 420/512

Epoch 00420: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.1616e-11 - val_loss: 8.0373e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.0921e-11 - val_loss: 7.9272e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.9916e-11 - val_loss: 7.8865e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0103e-11 - val_loss: 8.1493e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2594e-11 - val_loss: 8.2522e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2504e-11 - val_loss: 8.1669e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1684e-11 - val_loss: 8.0040e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0527e-11 - val_loss: 8.0977e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1730e-11 - val_loss: 8.1184e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1404e-11 - val_loss: 8.0332e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0147e-11 - val_loss: 7.9000e-11
Epoch 431/512

Epoch 00431: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.9183e-11 - val_loss: 7.8101e-11
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.8280e-11 - val_loss: 7.6918e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.6559e-11 - val_loss: 7.5359e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5308e-11 - val_loss: 7.5489e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6432e-11 - val_loss: 7.7250e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8187e-11 - val_loss: 7.8316e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8599e-11 - val_loss: 7.7776e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8065e-11 - val_loss: 7.6706e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7596e-11 - val_loss: 7.7136e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8180e-11 - val_loss: 7.7343e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8015e-11 - val_loss: 7.7135e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8340e-11 - val_loss: 7.8518e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8784e-11 - val_loss: 7.7828e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7651e-11 - val_loss: 7.6343e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6997e-11 - val_loss: 7.7386e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8037e-11 - val_loss: 7.7436e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7701e-11 - val_loss: 7.7141e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7150e-11 - val_loss: 7.5892e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.5826e-11 - val_loss: 7.4519e-11
Epoch 450/512

Epoch 00450: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.4459e-11 - val_loss: 7.2092e-11
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.2077e-11 - val_loss: 7.1327e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2180e-11 - val_loss: 7.2805e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3378e-11 - val_loss: 7.3335e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4320e-11 - val_loss: 7.2981e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2646e-11 - val_loss: 7.1441e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2738e-11 - val_loss: 7.3407e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4601e-11 - val_loss: 7.5997e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6803e-11 - val_loss: 7.5769e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6242e-11 - val_loss: 7.4862e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4690e-11 - val_loss: 7.2177e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2350e-11 - val_loss: 7.1421e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.1258e-11 - val_loss: 7.0040e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0545e-11 - val_loss: 7.0279e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1404e-11 - val_loss: 7.1318e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2506e-11 - val_loss: 7.3921e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4554e-11 - val_loss: 7.5437e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5395e-11 - val_loss: 7.3965e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4440e-11 - val_loss: 7.3014e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2822e-11 - val_loss: 7.2693e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3338e-11 - val_loss: 7.4273e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5291e-11 - val_loss: 7.5263e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5534e-11 - val_loss: 7.4431e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4684e-11 - val_loss: 7.2973e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2922e-11 - val_loss: 7.1468e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1815e-11 - val_loss: 7.1911e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2942e-11 - val_loss: 7.2739e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2930e-11 - val_loss: 7.2732e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3101e-11 - val_loss: 7.2242e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1957e-11 - val_loss: 7.0895e-11
Epoch 480/512

Epoch 00480: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.1076e-11 - val_loss: 6.9709e-11
Epoch 481/512

Epoch 00481: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.9755e-11 - val_loss: 6.9552e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0316e-11 - val_loss: 7.1218e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2751e-11 - val_loss: 7.3715e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3865e-11 - val_loss: 7.1774e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1975e-11 - val_loss: 7.0609e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1074e-11 - val_loss: 7.1682e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.1460e-11 - val_loss: 6.9468e-11
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.8866e-11 - val_loss: 6.7318e-11
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.7096e-11 - val_loss: 6.6192e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6719e-11 - val_loss: 6.6491e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7283e-11 - val_loss: 6.8312e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9075e-11 - val_loss: 6.8989e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9310e-11 - val_loss: 6.8460e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8978e-11 - val_loss: 6.9022e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9721e-11 - val_loss: 7.0123e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0832e-11 - val_loss: 7.0732e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1383e-11 - val_loss: 7.0465e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0355e-11 - val_loss: 6.9858e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9904e-11 - val_loss: 6.8235e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8354e-11 - val_loss: 6.7371e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7953e-11 - val_loss: 6.7885e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8267e-11 - val_loss: 6.8227e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8696e-11 - val_loss: 6.6843e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7107e-11 - val_loss: 6.7431e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7457e-11 - val_loss: 6.6993e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7416e-11 - val_loss: 6.7676e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8219e-11 - val_loss: 6.7600e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7854e-11 - val_loss: 6.6876e-11
Epoch 509/512

Epoch 00509: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.2-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.7306e-11 - val_loss: 6.5474e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5516e-11 - val_loss: 6.5662e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6427e-11 - val_loss: 6.7154e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7528e-11 - val_loss: 6.7307e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.304 | eve: 9.768 | bob: 9.195Epoch   0:   0% | abe: 9.276 | eve: 9.781 | bob: 9.174Epoch   0:   1% | abe: 9.270 | eve: 9.775 | bob: 9.175Epoch   0:   2% | abe: 9.231 | eve: 9.771 | bob: 9.141Epoch   0:   2% | abe: 9.225 | eve: 9.765 | bob: 9.139Epoch   0:   3% | abe: 9.217 | eve: 9.765 | bob: 9.137Epoch   0:   4% | abe: 9.209 | eve: 9.763 | bob: 9.131Epoch   0:   4% | abe: 9.201 | eve: 9.768 | bob: 9.127Epoch   0:   5% | abe: 9.186 | eve: 9.774 | bob: 9.115