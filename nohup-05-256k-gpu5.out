WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-03-07 17:34:18.302804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-03-07 17:34:18.530809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-03-07 17:34:18.531395: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-07 17:34:18.534133: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-07 17:34:18.536361: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-07 17:34:18.537001: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-07 17:34:18.540069: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-07 17:34:18.542533: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-07 17:34:18.549559: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-07 17:34:18.557487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-07 17:34:18.558152: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-03-07 17:34:18.575471: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-03-07 17:34:18.577876: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3ac1b10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-03-07 17:34:18.577924: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-03-07 17:34:19.047624: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x14d8a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-03-07 17:34:19.047675: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-03-07 17:34:19.050767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:86:00.0
2024-03-07 17:34:19.050870: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-07 17:34:19.050899: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-03-07 17:34:19.050924: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-03-07 17:34:19.050948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-03-07 17:34:19.050971: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-03-07 17:34:19.050993: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-03-07 17:34:19.051017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-03-07 17:34:19.058913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-03-07 17:34:19.059036: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-03-07 17:34:19.066168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-03-07 17:34:19.066231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-03-07 17:34:19.066245: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-03-07 17:34:19.076274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:86:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
2024-03-07 17:34:30.187676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Generating dataset
Generating dataset
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6678 - val_loss: 0.1979
Epoch 2/512
512/512 - 0s - loss: 0.1037 - val_loss: 0.0201
Epoch 3/512
512/512 - 0s - loss: 0.0109 - val_loss: 0.0037
Epoch 4/512
512/512 - 0s - loss: 0.0031 - val_loss: 0.0024
Epoch 5/512
512/512 - 0s - loss: 0.0022 - val_loss: 0.0019
Epoch 6/512
512/512 - 0s - loss: 0.0017 - val_loss: 0.0013
Epoch 7/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.0183e-04
Epoch 8/512
512/512 - 0s - loss: 7.6160e-04 - val_loss: 5.4822e-04
Epoch 9/512
512/512 - 0s - loss: 4.4520e-04 - val_loss: 2.9438e-04
Epoch 10/512
512/512 - 0s - loss: 2.2782e-04 - val_loss: 1.3497e-04
Epoch 11/512
512/512 - 0s - loss: 9.8463e-05 - val_loss: 5.0558e-05
Epoch 12/512
512/512 - 0s - loss: 3.4336e-05 - val_loss: 1.4622e-05
Epoch 13/512
512/512 - 0s - loss: 9.1219e-06 - val_loss: 3.0686e-06
Epoch 14/512
512/512 - 0s - loss: 3.1298e-06 - val_loss: 3.2225e-05
Epoch 15/512
512/512 - 0s - loss: 0.0030 - val_loss: 0.0110
Epoch 16/512
512/512 - 0s - loss: 0.0038 - val_loss: 2.0885e-04
Epoch 17/512
512/512 - 0s - loss: 1.2988e-04 - val_loss: 1.0347e-04
Epoch 18/512
512/512 - 0s - loss: 2.2698e-04 - val_loss: 9.2416e-04
Epoch 19/512
512/512 - 0s - loss: 0.0027 - val_loss: 0.0042
Epoch 20/512
512/512 - 0s - loss: 0.0022 - val_loss: 6.5793e-04
Epoch 21/512
512/512 - 0s - loss: 5.4354e-04 - val_loss: 6.6202e-04
Epoch 22/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0025
Epoch 23/512
512/512 - 0s - loss: 0.0024 - val_loss: 0.0017
Epoch 24/512
512/512 - 0s - loss: 0.0012 - val_loss: 8.8402e-04
Epoch 25/512
512/512 - 0s - loss: 9.9482e-04 - val_loss: 0.0015
Epoch 26/512
512/512 - 0s - loss: 0.0017 - val_loss: 0.0019
Epoch 27/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0012
Epoch 28/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 29/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0016
Epoch 30/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0014
Epoch 31/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0012
Epoch 32/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0014
Epoch 33/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0014
Epoch 34/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0012
Epoch 35/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0013
Epoch 36/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0014
Epoch 37/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0013
Epoch 38/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 39/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0013
Epoch 40/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0013
Epoch 41/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 42/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0013
Epoch 43/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 44/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 45/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 46/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 47/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0012
Epoch 48/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 49/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0012
Epoch 50/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 51/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 52/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 53/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 54/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 55/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 56/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 57/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 58/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 59/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 60/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 61/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0011
Epoch 62/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 63/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 64/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0011
Epoch 65/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 66/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 67/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 68/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 69/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 70/512
512/512 - 0s - loss: 9.9071e-04 - val_loss: 0.0010
Epoch 71/512
512/512 - 0s - loss: 9.9516e-04 - val_loss: 0.0010
Epoch 72/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0010
Epoch 73/512
512/512 - 0s - loss: 9.7895e-04 - val_loss: 9.8351e-04
Epoch 74/512
512/512 - 0s - loss: 9.7665e-04 - val_loss: 0.0010
Epoch 75/512
512/512 - 0s - loss: 0.0010 - val_loss: 9.9745e-04
Epoch 76/512
512/512 - 0s - loss: 9.7291e-04 - val_loss: 9.7522e-04
Epoch 77/512
512/512 - 0s - loss: 9.6875e-04 - val_loss: 9.9200e-04
Epoch 78/512
512/512 - 0s - loss: 9.7620e-04 - val_loss: 9.8934e-04
Epoch 79/512
512/512 - 0s - loss: 9.6390e-04 - val_loss: 9.8990e-04
Epoch 80/512
512/512 - 0s - loss: 9.7618e-04 - val_loss: 9.6974e-04
Epoch 81/512
512/512 - 0s - loss: 9.5599e-04 - val_loss: 9.5592e-04
Epoch 82/512
512/512 - 0s - loss: 9.4663e-04 - val_loss: 9.6885e-04
Epoch 83/512
512/512 - 0s - loss: 9.5792e-04 - val_loss: 9.8816e-04
Epoch 84/512
512/512 - 0s - loss: 9.6295e-04 - val_loss: 9.6409e-04
Epoch 85/512
512/512 - 0s - loss: 9.3014e-04 - val_loss: 9.7267e-04
Epoch 86/512
512/512 - 0s - loss: 9.6536e-04 - val_loss: 9.5326e-04
Epoch 87/512
512/512 - 0s - loss: 9.3593e-04 - val_loss: 9.2764e-04
Epoch 88/512
512/512 - 0s - loss: 9.2634e-04 - val_loss: 9.4964e-04
Epoch 89/512
512/512 - 0s - loss: 9.4136e-04 - val_loss: 9.6333e-04
Epoch 90/512
512/512 - 0s - loss: 9.4248e-04 - val_loss: 9.3975e-04
Epoch 91/512
512/512 - 0s - loss: 9.1782e-04 - val_loss: 9.4511e-04
Epoch 92/512
512/512 - 0s - loss: 9.3341e-04 - val_loss: 9.5300e-04
Epoch 93/512
512/512 - 0s - loss: 9.3015e-04 - val_loss: 9.3844e-04
Epoch 94/512
512/512 - 0s - loss: 9.1948e-04 - val_loss: 9.3097e-04
Epoch 95/512
512/512 - 0s - loss: 9.2303e-04 - val_loss: 9.2905e-04
Epoch 96/512
512/512 - 0s - loss: 9.1574e-04 - val_loss: 9.2667e-04
Epoch 97/512
512/512 - 0s - loss: 9.1594e-04 - val_loss: 9.2467e-04
Epoch 98/512
512/512 - 0s - loss: 9.1172e-04 - val_loss: 9.2698e-04
Epoch 99/512
512/512 - 0s - loss: 9.1558e-04 - val_loss: 9.1943e-04
Epoch 100/512
512/512 - 0s - loss: 9.0104e-04 - val_loss: 9.2871e-04
Epoch 101/512
512/512 - 0s - loss: 9.1939e-04 - val_loss: 9.1437e-04
Epoch 102/512
512/512 - 0s - loss: 9.0274e-04 - val_loss: 8.9108e-04
Epoch 103/512
512/512 - 0s - loss: 8.8012e-04 - val_loss: 9.3015e-04
Epoch 104/512
512/512 - 0s - loss: 9.2620e-04 - val_loss: 9.2122e-04
Epoch 105/512
512/512 - 0s - loss: 8.9641e-04 - val_loss: 8.8420e-04
Epoch 106/512
512/512 - 0s - loss: 8.7398e-04 - val_loss: 9.1478e-04
Epoch 107/512
512/512 - 0s - loss: 9.1056e-04 - val_loss: 9.2288e-04
Epoch 108/512
512/512 - 0s - loss: 9.0113e-04 - val_loss: 8.8308e-04
Epoch 109/512
512/512 - 0s - loss: 8.7189e-04 - val_loss: 8.8785e-04
Epoch 110/512
512/512 - 0s - loss: 8.9236e-04 - val_loss: 9.0491e-04
Epoch 111/512
512/512 - 0s - loss: 8.8846e-04 - val_loss: 9.0017e-04
Epoch 112/512
512/512 - 0s - loss: 8.8035e-04 - val_loss: 8.9926e-04
Epoch 113/512
512/512 - 0s - loss: 8.8905e-04 - val_loss: 8.8998e-04
Epoch 114/512
512/512 - 0s - loss: 8.7564e-04 - val_loss: 8.8710e-04
Epoch 115/512
512/512 - 0s - loss: 8.8112e-04 - val_loss: 8.8671e-04
Epoch 116/512
512/512 - 0s - loss: 8.7835e-04 - val_loss: 8.7600e-04
Epoch 117/512
512/512 - 0s - loss: 8.6712e-04 - val_loss: 8.8711e-04
Epoch 118/512
512/512 - 0s - loss: 8.7471e-04 - val_loss: 8.9570e-04
Epoch 119/512
512/512 - 0s - loss: 8.7861e-04 - val_loss: 8.7709e-04
Epoch 120/512
512/512 - 0s - loss: 8.6524e-04 - val_loss: 8.6636e-04
Epoch 121/512
512/512 - 0s - loss: 8.5981e-04 - val_loss: 8.7908e-04
Epoch 122/512
512/512 - 0s - loss: 8.6936e-04 - val_loss: 8.7934e-04
Epoch 123/512
512/512 - 0s - loss: 8.6438e-04 - val_loss: 8.7917e-04
Epoch 124/512
512/512 - 0s - loss: 8.6738e-04 - val_loss: 8.6306e-04
Epoch 125/512
512/512 - 0s - loss: 8.5536e-04 - val_loss: 8.5694e-04
Epoch 126/512
512/512 - 0s - loss: 8.4907e-04 - val_loss: 8.7227e-04
Epoch 127/512
512/512 - 0s - loss: 8.6690e-04 - val_loss: 8.7295e-04
Epoch 128/512
512/512 - 0s - loss: 8.5334e-04 - val_loss: 8.5672e-04
Epoch 129/512
512/512 - 0s - loss: 8.4760e-04 - val_loss: 8.5877e-04
Epoch 130/512
512/512 - 0s - loss: 8.5516e-04 - val_loss: 8.5871e-04
Epoch 131/512
512/512 - 0s - loss: 8.5235e-04 - val_loss: 8.4247e-04
Epoch 132/512
512/512 - 0s - loss: 8.3428e-04 - val_loss: 8.5554e-04
Epoch 133/512
512/512 - 0s - loss: 8.5704e-04 - val_loss: 8.4700e-04
Epoch 134/512
512/512 - 0s - loss: 8.3835e-04 - val_loss: 8.3698e-04
Epoch 135/512
512/512 - 0s - loss: 8.3134e-04 - val_loss: 8.5369e-04
Epoch 136/512
512/512 - 0s - loss: 8.4504e-04 - val_loss: 8.6418e-04
Epoch 137/512
512/512 - 0s - loss: 8.5194e-04 - val_loss: 8.2610e-04
Epoch 138/512
512/512 - 0s - loss: 8.1422e-04 - val_loss: 8.3311e-04
Epoch 139/512
512/512 - 0s - loss: 8.3852e-04 - val_loss: 8.5118e-04
Epoch 140/512
512/512 - 0s - loss: 8.3968e-04 - val_loss: 8.3675e-04
Epoch 141/512
512/512 - 0s - loss: 8.2654e-04 - val_loss: 8.1812e-04
Epoch 142/512
512/512 - 0s - loss: 8.1222e-04 - val_loss: 8.4064e-04
Epoch 143/512
512/512 - 0s - loss: 8.3663e-04 - val_loss: 8.4881e-04
Epoch 144/512
512/512 - 0s - loss: 8.3009e-04 - val_loss: 8.2420e-04
Epoch 145/512
512/512 - 0s - loss: 8.0974e-04 - val_loss: 8.2586e-04
Epoch 146/512
512/512 - 0s - loss: 8.2415e-04 - val_loss: 8.2746e-04
Epoch 147/512
512/512 - 0s - loss: 8.1955e-04 - val_loss: 8.1286e-04
Epoch 148/512
512/512 - 0s - loss: 7.9962e-04 - val_loss: 8.3109e-04
Epoch 149/512
512/512 - 0s - loss: 8.2307e-04 - val_loss: 8.3755e-04
Epoch 150/512
512/512 - 0s - loss: 8.2020e-04 - val_loss: 8.0199e-04
Epoch 151/512
512/512 - 0s - loss: 7.9032e-04 - val_loss: 8.1336e-04
Epoch 152/512
512/512 - 0s - loss: 8.1212e-04 - val_loss: 8.2251e-04
Epoch 153/512
512/512 - 0s - loss: 8.0809e-04 - val_loss: 8.0962e-04
Epoch 154/512
512/512 - 0s - loss: 7.9720e-04 - val_loss: 8.0402e-04
Epoch 155/512
512/512 - 0s - loss: 8.0005e-04 - val_loss: 8.0274e-04
Epoch 156/512
512/512 - 0s - loss: 7.9553e-04 - val_loss: 8.0528e-04
Epoch 157/512
512/512 - 0s - loss: 7.9245e-04 - val_loss: 8.0871e-04
Epoch 158/512
512/512 - 0s - loss: 8.0112e-04 - val_loss: 7.9135e-04
Epoch 159/512
512/512 - 0s - loss: 7.7847e-04 - val_loss: 7.9302e-04
Epoch 160/512
512/512 - 0s - loss: 7.9621e-04 - val_loss: 7.8574e-04
Epoch 161/512
512/512 - 0s - loss: 7.7883e-04 - val_loss: 7.8323e-04
Epoch 162/512
512/512 - 0s - loss: 7.8172e-04 - val_loss: 7.8781e-04
Epoch 163/512
512/512 - 0s - loss: 7.8375e-04 - val_loss: 7.8046e-04
Epoch 164/512
512/512 - 0s - loss: 7.6973e-04 - val_loss: 7.8783e-04
Epoch 165/512
512/512 - 0s - loss: 7.8027e-04 - val_loss: 7.8168e-04
Epoch 166/512
512/512 - 0s - loss: 7.6878e-04 - val_loss: 7.7680e-04
Epoch 167/512
512/512 - 0s - loss: 7.6599e-04 - val_loss: 7.8534e-04
Epoch 168/512
512/512 - 0s - loss: 7.7748e-04 - val_loss: 7.6599e-04
Epoch 169/512
512/512 - 0s - loss: 7.4907e-04 - val_loss: 7.7023e-04
Epoch 170/512
512/512 - 0s - loss: 7.6791e-04 - val_loss: 7.7712e-04
Epoch 171/512
512/512 - 0s - loss: 7.5648e-04 - val_loss: 7.7425e-04
Epoch 172/512
512/512 - 0s - loss: 7.5883e-04 - val_loss: 7.6044e-04
Epoch 173/512
512/512 - 0s - loss: 7.4918e-04 - val_loss: 7.5498e-04
Epoch 174/512
512/512 - 0s - loss: 7.4841e-04 - val_loss: 7.5908e-04
Epoch 175/512
512/512 - 0s - loss: 7.4896e-04 - val_loss: 7.5434e-04
Epoch 176/512
512/512 - 0s - loss: 7.4603e-04 - val_loss: 7.4087e-04
Epoch 177/512
512/512 - 0s - loss: 7.2955e-04 - val_loss: 7.5326e-04
Epoch 178/512
512/512 - 0s - loss: 7.4902e-04 - val_loss: 7.4825e-04
Epoch 179/512
512/512 - 0s - loss: 7.2926e-04 - val_loss: 7.3701e-04
Epoch 180/512
512/512 - 0s - loss: 7.3291e-04 - val_loss: 7.3427e-04
Epoch 181/512
512/512 - 0s - loss: 7.2522e-04 - val_loss: 7.3657e-04
Epoch 182/512
512/512 - 0s - loss: 7.2852e-04 - val_loss: 7.3073e-04
Epoch 183/512
512/512 - 0s - loss: 7.2165e-04 - val_loss: 7.1926e-04
Epoch 184/512
512/512 - 0s - loss: 7.1308e-04 - val_loss: 7.2187e-04
Epoch 185/512
512/512 - 0s - loss: 7.1463e-04 - val_loss: 7.2896e-04
Epoch 186/512
512/512 - 0s - loss: 7.1871e-04 - val_loss: 7.1223e-04
Epoch 187/512
512/512 - 0s - loss: 7.0651e-04 - val_loss: 7.0110e-04
Epoch 188/512
512/512 - 0s - loss: 6.9793e-04 - val_loss: 7.1128e-04
Epoch 189/512
512/512 - 0s - loss: 7.0668e-04 - val_loss: 7.1221e-04
Epoch 190/512
512/512 - 0s - loss: 6.9782e-04 - val_loss: 7.0172e-04
Epoch 191/512
512/512 - 0s - loss: 6.9555e-04 - val_loss: 6.9044e-04
Epoch 192/512
512/512 - 0s - loss: 6.8297e-04 - val_loss: 6.9790e-04
Epoch 193/512
512/512 - 0s - loss: 6.9618e-04 - val_loss: 6.9216e-04
Epoch 194/512
512/512 - 0s - loss: 6.8053e-04 - val_loss: 6.7923e-04
Epoch 195/512
512/512 - 0s - loss: 6.7904e-04 - val_loss: 6.7664e-04
Epoch 196/512
512/512 - 0s - loss: 6.7536e-04 - val_loss: 6.7435e-04
Epoch 197/512
512/512 - 0s - loss: 6.7443e-04 - val_loss: 6.6814e-04
Epoch 198/512
512/512 - 0s - loss: 6.6667e-04 - val_loss: 6.5864e-04
Epoch 199/512
512/512 - 0s - loss: 6.5775e-04 - val_loss: 6.7005e-04
Epoch 200/512
512/512 - 0s - loss: 6.6475e-04 - val_loss: 6.7230e-04
Epoch 201/512
512/512 - 0s - loss: 6.6334e-04 - val_loss: 6.4878e-04
Epoch 202/512
512/512 - 0s - loss: 6.4825e-04 - val_loss: 6.3555e-04
Epoch 203/512
512/512 - 0s - loss: 6.3628e-04 - val_loss: 6.5526e-04
Epoch 204/512
512/512 - 0s - loss: 6.5790e-04 - val_loss: 6.5138e-04
Epoch 205/512
512/512 - 0s - loss: 6.3891e-04 - val_loss: 6.3314e-04
Epoch 206/512
512/512 - 0s - loss: 6.2799e-04 - val_loss: 6.3849e-04
Epoch 207/512
512/512 - 0s - loss: 6.3879e-04 - val_loss: 6.3451e-04
Epoch 208/512
512/512 - 0s - loss: 6.2544e-04 - val_loss: 6.2347e-04
Epoch 209/512
512/512 - 0s - loss: 6.1831e-04 - val_loss: 6.3003e-04
Epoch 210/512
512/512 - 0s - loss: 6.2700e-04 - val_loss: 6.1926e-04
Epoch 211/512
512/512 - 0s - loss: 6.0848e-04 - val_loss: 6.1347e-04
Epoch 212/512
512/512 - 0s - loss: 6.0745e-04 - val_loss: 6.1997e-04
Epoch 213/512
512/512 - 0s - loss: 6.1594e-04 - val_loss: 6.0480e-04
Epoch 214/512
512/512 - 0s - loss: 5.9602e-04 - val_loss: 5.9014e-04
Epoch 215/512
512/512 - 0s - loss: 5.8932e-04 - val_loss: 6.0312e-04
Epoch 216/512
512/512 - 0s - loss: 6.0127e-04 - val_loss: 6.0069e-04
Epoch 217/512
512/512 - 0s - loss: 5.8502e-04 - val_loss: 5.8967e-04
Epoch 218/512
512/512 - 0s - loss: 5.8550e-04 - val_loss: 5.7694e-04
Epoch 219/512
512/512 - 0s - loss: 5.7003e-04 - val_loss: 5.8331e-04
Epoch 220/512
512/512 - 0s - loss: 5.8116e-04 - val_loss: 5.7822e-04
Epoch 221/512
512/512 - 0s - loss: 5.6676e-04 - val_loss: 5.7108e-04
Epoch 222/512
512/512 - 0s - loss: 5.6281e-04 - val_loss: 5.6947e-04
Epoch 223/512
512/512 - 0s - loss: 5.6260e-04 - val_loss: 5.5938e-04
Epoch 224/512
512/512 - 0s - loss: 5.5225e-04 - val_loss: 5.5214e-04
Epoch 225/512
512/512 - 0s - loss: 5.4777e-04 - val_loss: 5.5514e-04
Epoch 226/512
512/512 - 0s - loss: 5.5010e-04 - val_loss: 5.4377e-04
Epoch 227/512
512/512 - 0s - loss: 5.3741e-04 - val_loss: 5.3663e-04
Epoch 228/512
512/512 - 0s - loss: 5.3471e-04 - val_loss: 5.3286e-04
Epoch 229/512
512/512 - 0s - loss: 5.2925e-04 - val_loss: 5.3280e-04
Epoch 230/512
512/512 - 0s - loss: 5.2738e-04 - val_loss: 5.2562e-04
Epoch 231/512
512/512 - 0s - loss: 5.2259e-04 - val_loss: 5.1244e-04
Epoch 232/512
512/512 - 0s - loss: 5.1278e-04 - val_loss: 5.0573e-04
Epoch 233/512
512/512 - 0s - loss: 5.0614e-04 - val_loss: 5.1313e-04
Epoch 234/512
512/512 - 0s - loss: 5.1045e-04 - val_loss: 5.0663e-04
Epoch 235/512
512/512 - 0s - loss: 5.0101e-04 - val_loss: 4.9393e-04
Epoch 236/512
512/512 - 0s - loss: 4.9379e-04 - val_loss: 4.8448e-04
Epoch 237/512
512/512 - 0s - loss: 4.8410e-04 - val_loss: 4.8996e-04
Epoch 238/512
512/512 - 0s - loss: 4.9215e-04 - val_loss: 4.8390e-04
Epoch 239/512
512/512 - 0s - loss: 4.7753e-04 - val_loss: 4.7412e-04
Epoch 240/512
512/512 - 0s - loss: 4.7043e-04 - val_loss: 4.7653e-04
Epoch 241/512
512/512 - 0s - loss: 4.7074e-04 - val_loss: 4.7694e-04
Epoch 242/512
512/512 - 0s - loss: 4.6902e-04 - val_loss: 4.6324e-04
Epoch 243/512
512/512 - 0s - loss: 4.5857e-04 - val_loss: 4.4696e-04
Epoch 244/512
512/512 - 0s - loss: 4.4515e-04 - val_loss: 4.5659e-04
Epoch 245/512
512/512 - 0s - loss: 4.5642e-04 - val_loss: 4.5404e-04
Epoch 246/512
512/512 - 0s - loss: 4.4670e-04 - val_loss: 4.3335e-04
Epoch 247/512
512/512 - 0s - loss: 4.2995e-04 - val_loss: 4.3252e-04
Epoch 248/512
512/512 - 0s - loss: 4.3431e-04 - val_loss: 4.3818e-04
Epoch 249/512
512/512 - 0s - loss: 4.3290e-04 - val_loss: 4.2550e-04
Epoch 250/512
512/512 - 0s - loss: 4.2151e-04 - val_loss: 4.1126e-04
Epoch 251/512
512/512 - 0s - loss: 4.1145e-04 - val_loss: 4.1395e-04
Epoch 252/512
512/512 - 0s - loss: 4.1647e-04 - val_loss: 4.1058e-04
Epoch 253/512
512/512 - 0s - loss: 4.0438e-04 - val_loss: 4.0753e-04
Epoch 254/512
512/512 - 0s - loss: 4.0478e-04 - val_loss: 3.9852e-04
Epoch 255/512
512/512 - 0s - loss: 3.9302e-04 - val_loss: 3.9383e-04
Epoch 256/512
512/512 - 0s - loss: 3.9389e-04 - val_loss: 3.8899e-04
Epoch 257/512
512/512 - 0s - loss: 3.8411e-04 - val_loss: 3.8359e-04
Epoch 258/512
512/512 - 0s - loss: 3.8077e-04 - val_loss: 3.7908e-04
Epoch 259/512
512/512 - 0s - loss: 3.7638e-04 - val_loss: 3.7255e-04
Epoch 260/512
512/512 - 0s - loss: 3.6858e-04 - val_loss: 3.6989e-04
Epoch 261/512
512/512 - 0s - loss: 3.6786e-04 - val_loss: 3.6292e-04
Epoch 262/512
512/512 - 0s - loss: 3.5876e-04 - val_loss: 3.5473e-04
Epoch 263/512
512/512 - 0s - loss: 3.5368e-04 - val_loss: 3.5096e-04
Epoch 264/512
512/512 - 0s - loss: 3.5001e-04 - val_loss: 3.4979e-04
Epoch 265/512
512/512 - 0s - loss: 3.4758e-04 - val_loss: 3.4018e-04
Epoch 266/512
512/512 - 0s - loss: 3.3696e-04 - val_loss: 3.3467e-04
Epoch 267/512
512/512 - 0s - loss: 3.3506e-04 - val_loss: 3.3176e-04
Epoch 268/512
512/512 - 0s - loss: 3.2879e-04 - val_loss: 3.2847e-04
Epoch 269/512
512/512 - 0s - loss: 3.2655e-04 - val_loss: 3.1995e-04
Epoch 270/512
512/512 - 0s - loss: 3.1778e-04 - val_loss: 3.1470e-04
Epoch 271/512
512/512 - 0s - loss: 3.1439e-04 - val_loss: 3.1171e-04
Epoch 272/512
512/512 - 0s - loss: 3.0847e-04 - val_loss: 3.0947e-04
Epoch 273/512
512/512 - 0s - loss: 3.0759e-04 - val_loss: 3.0141e-04
Epoch 274/512
512/512 - 0s - loss: 2.9934e-04 - val_loss: 2.9104e-04
Epoch 275/512
512/512 - 0s - loss: 2.9068e-04 - val_loss: 2.9297e-04
Epoch 276/512
512/512 - 0s - loss: 2.9238e-04 - val_loss: 2.9292e-04
Epoch 277/512
512/512 - 0s - loss: 2.8966e-04 - val_loss: 2.7642e-04
Epoch 278/512
512/512 - 0s - loss: 2.7309e-04 - val_loss: 2.7605e-04
Epoch 279/512
512/512 - 0s - loss: 2.7898e-04 - val_loss: 2.7775e-04
Epoch 280/512
512/512 - 0s - loss: 2.7428e-04 - val_loss: 2.6540e-04
Epoch 281/512
512/512 - 0s - loss: 2.6187e-04 - val_loss: 2.6082e-04
Epoch 282/512
512/512 - 0s - loss: 2.6163e-04 - val_loss: 2.6119e-04
Epoch 283/512
512/512 - 0s - loss: 2.5934e-04 - val_loss: 2.5592e-04
Epoch 284/512
512/512 - 0s - loss: 2.5092e-04 - val_loss: 2.5027e-04
Epoch 285/512
512/512 - 0s - loss: 2.4908e-04 - val_loss: 2.4565e-04
Epoch 286/512
512/512 - 0s - loss: 2.4493e-04 - val_loss: 2.3763e-04
Epoch 287/512
512/512 - 0s - loss: 2.3698e-04 - val_loss: 2.3397e-04
Epoch 288/512
512/512 - 0s - loss: 2.3409e-04 - val_loss: 2.3309e-04
Epoch 289/512
512/512 - 0s - loss: 2.3337e-04 - val_loss: 2.2610e-04
Epoch 290/512
512/512 - 0s - loss: 2.2273e-04 - val_loss: 2.2344e-04
Epoch 291/512
512/512 - 0s - loss: 2.2269e-04 - val_loss: 2.2297e-04
Epoch 292/512
512/512 - 0s - loss: 2.1953e-04 - val_loss: 2.1434e-04
Epoch 293/512
512/512 - 0s - loss: 2.1289e-04 - val_loss: 2.0751e-04
Epoch 294/512
512/512 - 0s - loss: 2.0812e-04 - val_loss: 2.0464e-04
Epoch 295/512
512/512 - 0s - loss: 2.0526e-04 - val_loss: 2.0382e-04
Epoch 296/512
512/512 - 0s - loss: 2.0224e-04 - val_loss: 1.9946e-04
Epoch 297/512
512/512 - 0s - loss: 1.9635e-04 - val_loss: 1.9630e-04
Epoch 298/512
512/512 - 0s - loss: 1.9494e-04 - val_loss: 1.8939e-04
Epoch 299/512
512/512 - 0s - loss: 1.8717e-04 - val_loss: 1.8579e-04
Epoch 300/512
512/512 - 0s - loss: 1.8571e-04 - val_loss: 1.8582e-04
Epoch 301/512
512/512 - 0s - loss: 1.8426e-04 - val_loss: 1.7818e-04
Epoch 302/512
512/512 - 0s - loss: 1.7504e-04 - val_loss: 1.7534e-04
Epoch 303/512
512/512 - 0s - loss: 1.7659e-04 - val_loss: 1.7243e-04
Epoch 304/512
512/512 - 0s - loss: 1.6942e-04 - val_loss: 1.6744e-04
Epoch 305/512
512/512 - 0s - loss: 1.6772e-04 - val_loss: 1.6359e-04
Epoch 306/512
512/512 - 0s - loss: 1.6287e-04 - val_loss: 1.6034e-04
Epoch 307/512
512/512 - 0s - loss: 1.5950e-04 - val_loss: 1.5908e-04
Epoch 308/512
512/512 - 0s - loss: 1.5777e-04 - val_loss: 1.5526e-04
Epoch 309/512
512/512 - 0s - loss: 1.5410e-04 - val_loss: 1.4788e-04
Epoch 310/512
512/512 - 0s - loss: 1.4720e-04 - val_loss: 1.4659e-04
Epoch 311/512
512/512 - 0s - loss: 1.4739e-04 - val_loss: 1.4642e-04
Epoch 312/512
512/512 - 0s - loss: 1.4524e-04 - val_loss: 1.3977e-04
Epoch 313/512
512/512 - 0s - loss: 1.3898e-04 - val_loss: 1.3491e-04
Epoch 314/512
512/512 - 0s - loss: 1.3563e-04 - val_loss: 1.3445e-04
Epoch 315/512
512/512 - 0s - loss: 1.3446e-04 - val_loss: 1.3379e-04
Epoch 316/512
512/512 - 0s - loss: 1.3211e-04 - val_loss: 1.2840e-04
Epoch 317/512
512/512 - 0s - loss: 1.2693e-04 - val_loss: 1.2437e-04
Epoch 318/512
512/512 - 0s - loss: 1.2391e-04 - val_loss: 1.2420e-04
Epoch 319/512
512/512 - 0s - loss: 1.2365e-04 - val_loss: 1.2063e-04
Epoch 320/512
512/512 - 0s - loss: 1.1967e-04 - val_loss: 1.1433e-04
Epoch 321/512
512/512 - 0s - loss: 1.1325e-04 - val_loss: 1.1477e-04
Epoch 322/512
512/512 - 0s - loss: 1.1504e-04 - val_loss: 1.1593e-04
Epoch 323/512
512/512 - 0s - loss: 1.1395e-04 - val_loss: 1.0675e-04
Epoch 324/512
512/512 - 0s - loss: 1.0490e-04 - val_loss: 1.0382e-04
Epoch 325/512
512/512 - 0s - loss: 1.0541e-04 - val_loss: 1.0646e-04
Epoch 326/512
512/512 - 0s - loss: 1.0528e-04 - val_loss: 1.0232e-04
Epoch 327/512
512/512 - 0s - loss: 1.0066e-04 - val_loss: 9.5764e-05
Epoch 328/512
512/512 - 0s - loss: 9.5681e-05 - val_loss: 9.6725e-05
Epoch 329/512
512/512 - 0s - loss: 9.6645e-05 - val_loss: 9.7401e-05
Epoch 330/512
512/512 - 0s - loss: 9.5210e-05 - val_loss: 9.1406e-05
Epoch 331/512
512/512 - 0s - loss: 9.0252e-05 - val_loss: 8.6728e-05
Epoch 332/512
512/512 - 0s - loss: 8.7169e-05 - val_loss: 8.8065e-05
Epoch 333/512
512/512 - 0s - loss: 8.8217e-05 - val_loss: 8.6359e-05
Epoch 334/512
512/512 - 0s - loss: 8.4949e-05 - val_loss: 8.0947e-05
Epoch 335/512
512/512 - 0s - loss: 8.0680e-05 - val_loss: 8.0132e-05
Epoch 336/512
512/512 - 0s - loss: 8.0517e-05 - val_loss: 8.0113e-05
Epoch 337/512
512/512 - 0s - loss: 7.9304e-05 - val_loss: 7.6151e-05
Epoch 338/512
512/512 - 0s - loss: 7.4994e-05 - val_loss: 7.3608e-05
Epoch 339/512
512/512 - 0s - loss: 7.4418e-05 - val_loss: 7.2729e-05
Epoch 340/512
512/512 - 0s - loss: 7.2387e-05 - val_loss: 7.0756e-05
Epoch 341/512
512/512 - 0s - loss: 7.0808e-05 - val_loss: 6.7415e-05
Epoch 342/512
512/512 - 0s - loss: 6.7407e-05 - val_loss: 6.6276e-05
Epoch 343/512
512/512 - 0s - loss: 6.6816e-05 - val_loss: 6.6267e-05
Epoch 344/512
512/512 - 0s - loss: 6.5707e-05 - val_loss: 6.3706e-05
Epoch 345/512
512/512 - 0s - loss: 6.3028e-05 - val_loss: 6.0577e-05
Epoch 346/512
512/512 - 0s - loss: 6.0398e-05 - val_loss: 6.0892e-05
Epoch 347/512
512/512 - 0s - loss: 6.1400e-05 - val_loss: 5.9144e-05
Epoch 348/512
512/512 - 0s - loss: 5.7922e-05 - val_loss: 5.5887e-05
Epoch 349/512
512/512 - 0s - loss: 5.6127e-05 - val_loss: 5.5152e-05
Epoch 350/512
512/512 - 0s - loss: 5.5471e-05 - val_loss: 5.3945e-05
Epoch 351/512
512/512 - 0s - loss: 5.3423e-05 - val_loss: 5.2434e-05
Epoch 352/512
512/512 - 0s - loss: 5.2523e-05 - val_loss: 5.1076e-05
Epoch 353/512
512/512 - 0s - loss: 5.0679e-05 - val_loss: 4.9562e-05
Epoch 354/512
512/512 - 0s - loss: 4.9520e-05 - val_loss: 4.7571e-05
Epoch 355/512
512/512 - 0s - loss: 4.7514e-05 - val_loss: 4.6789e-05
Epoch 356/512
512/512 - 0s - loss: 4.6804e-05 - val_loss: 4.6046e-05
Epoch 357/512
512/512 - 0s - loss: 4.5661e-05 - val_loss: 4.4244e-05
Epoch 358/512
512/512 - 0s - loss: 4.4064e-05 - val_loss: 4.2501e-05
Epoch 359/512
512/512 - 0s - loss: 4.2448e-05 - val_loss: 4.1970e-05
Epoch 360/512
512/512 - 0s - loss: 4.2053e-05 - val_loss: 4.0739e-05
Epoch 361/512
512/512 - 0s - loss: 4.0614e-05 - val_loss: 3.8939e-05
Epoch 362/512
512/512 - 0s - loss: 3.8818e-05 - val_loss: 3.8193e-05
Epoch 363/512
512/512 - 0s - loss: 3.8404e-05 - val_loss: 3.7717e-05
Epoch 364/512
512/512 - 0s - loss: 3.7371e-05 - val_loss: 3.6062e-05
Epoch 365/512
512/512 - 0s - loss: 3.5682e-05 - val_loss: 3.5042e-05
Epoch 366/512
512/512 - 0s - loss: 3.5027e-05 - val_loss: 3.4583e-05
Epoch 367/512
512/512 - 0s - loss: 3.4316e-05 - val_loss: 3.3309e-05
Epoch 368/512
512/512 - 0s - loss: 3.3027e-05 - val_loss: 3.1588e-05
Epoch 369/512
512/512 - 0s - loss: 3.1596e-05 - val_loss: 3.1384e-05
Epoch 370/512
512/512 - 0s - loss: 3.1526e-05 - val_loss: 3.0529e-05
Epoch 371/512
512/512 - 0s - loss: 3.0174e-05 - val_loss: 2.9371e-05
Epoch 372/512
512/512 - 0s - loss: 2.9341e-05 - val_loss: 2.8355e-05
Epoch 373/512
512/512 - 0s - loss: 2.8377e-05 - val_loss: 2.7699e-05
Epoch 374/512
512/512 - 0s - loss: 2.7644e-05 - val_loss: 2.7081e-05
Epoch 375/512
512/512 - 0s - loss: 2.6924e-05 - val_loss: 2.6099e-05
Epoch 376/512
512/512 - 0s - loss: 2.5970e-05 - val_loss: 2.5123e-05
Epoch 377/512
512/512 - 0s - loss: 2.4968e-05 - val_loss: 2.5107e-05
Epoch 378/512
512/512 - 0s - loss: 2.5139e-05 - val_loss: 2.3772e-05
Epoch 379/512
512/512 - 0s - loss: 2.3472e-05 - val_loss: 2.2460e-05
Epoch 380/512
512/512 - 0s - loss: 2.2702e-05 - val_loss: 2.2466e-05
Epoch 381/512
512/512 - 0s - loss: 2.2660e-05 - val_loss: 2.2134e-05
Epoch 382/512
512/512 - 0s - loss: 2.1867e-05 - val_loss: 2.0821e-05
Epoch 383/512
512/512 - 0s - loss: 2.0719e-05 - val_loss: 2.0233e-05
Epoch 384/512
512/512 - 0s - loss: 2.0538e-05 - val_loss: 1.9776e-05
Epoch 385/512
512/512 - 0s - loss: 1.9782e-05 - val_loss: 1.9186e-05
Epoch 386/512
512/512 - 0s - loss: 1.9247e-05 - val_loss: 1.8448e-05
Epoch 387/512
512/512 - 0s - loss: 1.8472e-05 - val_loss: 1.7898e-05
Epoch 388/512
512/512 - 0s - loss: 1.7949e-05 - val_loss: 1.7677e-05
Epoch 389/512
512/512 - 0s - loss: 1.7653e-05 - val_loss: 1.6961e-05
Epoch 390/512
512/512 - 0s - loss: 1.6926e-05 - val_loss: 1.6120e-05
Epoch 391/512
512/512 - 0s - loss: 1.6155e-05 - val_loss: 1.6031e-05
Epoch 392/512
512/512 - 0s - loss: 1.6135e-05 - val_loss: 1.5663e-05
Epoch 393/512
512/512 - 0s - loss: 1.5554e-05 - val_loss: 1.4587e-05
Epoch 394/512
512/512 - 0s - loss: 1.4555e-05 - val_loss: 1.4433e-05
Epoch 395/512
512/512 - 0s - loss: 1.4637e-05 - val_loss: 1.4363e-05
Epoch 396/512
512/512 - 0s - loss: 1.4270e-05 - val_loss: 1.3410e-05
Epoch 397/512
512/512 - 0s - loss: 1.3252e-05 - val_loss: 1.2998e-05
Epoch 398/512
512/512 - 0s - loss: 1.3091e-05 - val_loss: 1.3192e-05
Epoch 399/512
512/512 - 0s - loss: 1.3031e-05 - val_loss: 1.2586e-05
Epoch 400/512
512/512 - 0s - loss: 1.2407e-05 - val_loss: 1.1579e-05
Epoch 401/512
512/512 - 0s - loss: 1.1585e-05 - val_loss: 1.1576e-05
Epoch 402/512
512/512 - 0s - loss: 1.1773e-05 - val_loss: 1.1497e-05
Epoch 403/512
512/512 - 0s - loss: 1.1371e-05 - val_loss: 1.0769e-05
Epoch 404/512
512/512 - 0s - loss: 1.0726e-05 - val_loss: 1.0285e-05
Epoch 405/512
512/512 - 0s - loss: 1.0342e-05 - val_loss: 1.0383e-05
Epoch 406/512
512/512 - 0s - loss: 1.0506e-05 - val_loss: 9.9154e-06
Epoch 407/512
512/512 - 0s - loss: 9.7284e-06 - val_loss: 9.3787e-06
Epoch 408/512
512/512 - 0s - loss: 9.4629e-06 - val_loss: 9.2788e-06
Epoch 409/512
512/512 - 0s - loss: 9.3379e-06 - val_loss: 8.9921e-06
Epoch 410/512
512/512 - 0s - loss: 8.9205e-06 - val_loss: 8.6436e-06
Epoch 411/512
512/512 - 0s - loss: 8.6624e-06 - val_loss: 8.3988e-06
Epoch 412/512
512/512 - 0s - loss: 8.3663e-06 - val_loss: 8.1684e-06
Epoch 413/512
512/512 - 0s - loss: 8.1767e-06 - val_loss: 7.8159e-06
Epoch 414/512
512/512 - 0s - loss: 7.8210e-06 - val_loss: 7.4806e-06
Epoch 415/512
512/512 - 0s - loss: 7.5390e-06 - val_loss: 7.4037e-06
Epoch 416/512
512/512 - 0s - loss: 7.3825e-06 - val_loss: 7.2475e-06
Epoch 417/512
512/512 - 0s - loss: 7.2041e-06 - val_loss: 6.8644e-06
Epoch 418/512
512/512 - 0s - loss: 6.8402e-06 - val_loss: 6.5180e-06
Epoch 419/512
512/512 - 0s - loss: 6.5542e-06 - val_loss: 6.5263e-06
Epoch 420/512
512/512 - 0s - loss: 6.5917e-06 - val_loss: 6.2887e-06
Epoch 421/512
512/512 - 0s - loss: 6.1886e-06 - val_loss: 5.9749e-06
Epoch 422/512
512/512 - 0s - loss: 6.0029e-06 - val_loss: 5.8238e-06
Epoch 423/512
512/512 - 0s - loss: 5.8450e-06 - val_loss: 5.7013e-06
Epoch 424/512
512/512 - 0s - loss: 5.6810e-06 - val_loss: 5.4789e-06
Epoch 425/512
512/512 - 0s - loss: 5.4408e-06 - val_loss: 5.2692e-06
Epoch 426/512
512/512 - 0s - loss: 5.3018e-06 - val_loss: 5.0695e-06
Epoch 427/512
512/512 - 0s - loss: 5.0754e-06 - val_loss: 4.9736e-06
Epoch 428/512
512/512 - 0s - loss: 4.9752e-06 - val_loss: 4.8043e-06
Epoch 429/512
512/512 - 0s - loss: 4.7910e-06 - val_loss: 4.5378e-06
Epoch 430/512
512/512 - 0s - loss: 4.5533e-06 - val_loss: 4.4651e-06
Epoch 431/512
512/512 - 0s - loss: 4.4876e-06 - val_loss: 4.4086e-06
Epoch 432/512
512/512 - 0s - loss: 4.3862e-06 - val_loss: 4.1659e-06
Epoch 433/512
512/512 - 0s - loss: 4.1142e-06 - val_loss: 4.0134e-06
Epoch 434/512
512/512 - 0s - loss: 4.0477e-06 - val_loss: 3.9188e-06
Epoch 435/512
512/512 - 0s - loss: 3.9024e-06 - val_loss: 3.8241e-06
Epoch 436/512
512/512 - 0s - loss: 3.8088e-06 - val_loss: 3.6541e-06
Epoch 437/512
512/512 - 0s - loss: 3.6203e-06 - val_loss: 3.5317e-06
Epoch 438/512
512/512 - 0s - loss: 3.5339e-06 - val_loss: 3.4407e-06
Epoch 439/512
512/512 - 0s - loss: 3.4454e-06 - val_loss: 3.2843e-06
Epoch 440/512
512/512 - 0s - loss: 3.2681e-06 - val_loss: 3.1646e-06
Epoch 441/512
512/512 - 0s - loss: 3.1890e-06 - val_loss: 3.0944e-06
Epoch 442/512
512/512 - 0s - loss: 3.0999e-06 - val_loss: 2.9905e-06
Epoch 443/512
512/512 - 0s - loss: 2.9835e-06 - val_loss: 2.8431e-06
Epoch 444/512
512/512 - 0s - loss: 2.8728e-06 - val_loss: 2.7475e-06
Epoch 445/512
512/512 - 0s - loss: 2.7660e-06 - val_loss: 2.6726e-06
Epoch 446/512
512/512 - 0s - loss: 2.6847e-06 - val_loss: 2.6298e-06
Epoch 447/512
512/512 - 0s - loss: 2.6362e-06 - val_loss: 2.5184e-06
Epoch 448/512
512/512 - 0s - loss: 2.5041e-06 - val_loss: 2.3853e-06
Epoch 449/512
512/512 - 0s - loss: 2.3916e-06 - val_loss: 2.3548e-06
Epoch 450/512
512/512 - 0s - loss: 2.3732e-06 - val_loss: 2.2912e-06
Epoch 451/512
512/512 - 0s - loss: 2.2889e-06 - val_loss: 2.1423e-06
Epoch 452/512
512/512 - 0s - loss: 2.1376e-06 - val_loss: 2.1031e-06
Epoch 453/512
512/512 - 0s - loss: 2.1501e-06 - val_loss: 2.0535e-06
Epoch 454/512
512/512 - 0s - loss: 2.0398e-06 - val_loss: 1.9724e-06
Epoch 455/512
512/512 - 0s - loss: 1.9810e-06 - val_loss: 1.9067e-06
Epoch 456/512
512/512 - 0s - loss: 1.9100e-06 - val_loss: 1.8326e-06
Epoch 457/512
512/512 - 0s - loss: 1.8489e-06 - val_loss: 1.7463e-06
Epoch 458/512
512/512 - 0s - loss: 1.7595e-06 - val_loss: 1.7229e-06
Epoch 459/512
512/512 - 0s - loss: 1.7413e-06 - val_loss: 1.6811e-06
Epoch 460/512
512/512 - 0s - loss: 1.6559e-06 - val_loss: 1.6115e-06
Epoch 461/512
512/512 - 0s - loss: 1.5967e-06 - val_loss: 1.5761e-06
Epoch 462/512
512/512 - 0s - loss: 1.5722e-06 - val_loss: 1.5093e-06
Epoch 463/512
512/512 - 0s - loss: 1.5022e-06 - val_loss: 1.4123e-06
Epoch 464/512
512/512 - 0s - loss: 1.4243e-06 - val_loss: 1.3868e-06
Epoch 465/512
512/512 - 0s - loss: 1.3901e-06 - val_loss: 1.4011e-06
Epoch 466/512
512/512 - 0s - loss: 1.3921e-06 - val_loss: 1.3185e-06
Epoch 467/512
512/512 - 0s - loss: 1.2910e-06 - val_loss: 1.2228e-06
Epoch 468/512
512/512 - 0s - loss: 1.2202e-06 - val_loss: 1.2534e-06
Epoch 469/512
512/512 - 0s - loss: 1.2768e-06 - val_loss: 1.1973e-06
Epoch 470/512
512/512 - 0s - loss: 1.1575e-06 - val_loss: 1.1049e-06
Epoch 471/512
512/512 - 0s - loss: 1.1117e-06 - val_loss: 1.1090e-06
Epoch 472/512
512/512 - 0s - loss: 1.1192e-06 - val_loss: 1.0836e-06
Epoch 473/512
512/512 - 0s - loss: 1.0754e-06 - val_loss: 9.9327e-07
Epoch 474/512
512/512 - 0s - loss: 9.9345e-07 - val_loss: 9.6509e-07
Epoch 475/512
512/512 - 0s - loss: 9.8292e-07 - val_loss: 9.9225e-07
Epoch 476/512
512/512 - 0s - loss: 9.9227e-07 - val_loss: 9.1717e-07
Epoch 477/512
512/512 - 0s - loss: 8.9778e-07 - val_loss: 8.5753e-07
Epoch 478/512
512/512 - 0s - loss: 8.6954e-07 - val_loss: 8.7866e-07
Epoch 479/512
512/512 - 0s - loss: 8.8731e-07 - val_loss: 8.5188e-07
Epoch 480/512
512/512 - 0s - loss: 8.4067e-07 - val_loss: 7.6388e-07
Epoch 481/512
512/512 - 0s - loss: 7.6656e-07 - val_loss: 7.5490e-07
Epoch 482/512
512/512 - 0s - loss: 7.8338e-07 - val_loss: 7.6916e-07
Epoch 483/512
512/512 - 0s - loss: 7.6649e-07 - val_loss: 7.1038e-07
Epoch 484/512
512/512 - 0s - loss: 7.0408e-07 - val_loss: 6.7668e-07
Epoch 485/512
512/512 - 0s - loss: 6.8855e-07 - val_loss: 6.8238e-07
Epoch 486/512
512/512 - 0s - loss: 6.9037e-07 - val_loss: 6.5443e-07
Epoch 487/512
512/512 - 0s - loss: 6.4551e-07 - val_loss: 6.1075e-07
Epoch 488/512
512/512 - 0s - loss: 6.1739e-07 - val_loss: 6.0041e-07
Epoch 489/512
512/512 - 0s - loss: 6.0735e-07 - val_loss: 5.9588e-07
Epoch 490/512
512/512 - 0s - loss: 5.9477e-07 - val_loss: 5.6077e-07
Epoch 491/512
512/512 - 0s - loss: 5.5741e-07 - val_loss: 5.3075e-07
Epoch 492/512
512/512 - 0s - loss: 5.4020e-07 - val_loss: 5.2616e-07
Epoch 493/512
512/512 - 0s - loss: 5.3073e-07 - val_loss: 5.0866e-07
Epoch 494/512
512/512 - 0s - loss: 5.0865e-07 - val_loss: 4.8253e-07
Epoch 495/512
512/512 - 0s - loss: 4.8544e-07 - val_loss: 4.7044e-07
Epoch 496/512
512/512 - 0s - loss: 4.7593e-07 - val_loss: 4.5616e-07
Epoch 497/512
512/512 - 0s - loss: 4.5657e-07 - val_loss: 4.3540e-07
Epoch 498/512
512/512 - 0s - loss: 4.3771e-07 - val_loss: 4.2441e-07
Epoch 499/512
512/512 - 0s - loss: 4.2781e-07 - val_loss: 4.1005e-07
Epoch 500/512
512/512 - 0s - loss: 4.1423e-07 - val_loss: 3.8973e-07
Epoch 501/512
512/512 - 0s - loss: 3.9063e-07 - val_loss: 3.7791e-07
Epoch 502/512
512/512 - 0s - loss: 3.8373e-07 - val_loss: 3.7039e-07
Epoch 503/512
512/512 - 0s - loss: 3.7247e-07 - val_loss: 3.5565e-07
Epoch 504/512
512/512 - 0s - loss: 3.5578e-07 - val_loss: 3.3898e-07
Epoch 505/512
512/512 - 0s - loss: 3.4250e-07 - val_loss: 3.2962e-07
Epoch 506/512
512/512 - 0s - loss: 3.3162e-07 - val_loss: 3.2508e-07
Epoch 507/512
512/512 - 0s - loss: 3.2626e-07 - val_loss: 3.0689e-07
Epoch 508/512
512/512 - 0s - loss: 3.0521e-07 - val_loss: 2.9513e-07
Epoch 509/512
512/512 - 0s - loss: 2.9899e-07 - val_loss: 2.9093e-07
Epoch 510/512
512/512 - 0s - loss: 2.9304e-07 - val_loss: 2.7701e-07
Epoch 511/512
512/512 - 0s - loss: 2.7563e-07 - val_loss: 2.6355e-07
Epoch 512/512
512/512 - 0s - loss: 2.6882e-07 - val_loss: 2.5858e-07
2024-03-07 17:34:45.262504: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3536e-07 - val_loss: 6.4941e-08
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9306e-08 - val_loss: 1.4902e-07
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7888e-07 - val_loss: 4.2102e-07
Epoch 4/512

Epoch 00004: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1817e-07 - val_loss: 1.4130e-07
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1113e-07 - val_loss: 9.7707e-08
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3680e-07 - val_loss: 2.3448e-07
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7965e-07 - val_loss: 2.3975e-07
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8191e-07 - val_loss: 1.1273e-07
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1331e-07 - val_loss: 1.3664e-07
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7761e-07 - val_loss: 2.2650e-07
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0982e-07 - val_loss: 1.4903e-07
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2831e-07 - val_loss: 1.1120e-07
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2643e-07 - val_loss: 1.5853e-07
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7458e-07 - val_loss: 1.6720e-07
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4775e-07 - val_loss: 1.1501e-07
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1207e-07 - val_loss: 1.1746e-07
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3224e-07 - val_loss: 1.4724e-07
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4400e-07 - val_loss: 1.2336e-07
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1398e-07 - val_loss: 1.0265e-07
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0754e-07 - val_loss: 1.1646e-07
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2300e-07 - val_loss: 1.1889e-07
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1318e-07 - val_loss: 9.9249e-08
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7988e-08 - val_loss: 9.6858e-08
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0203e-07 - val_loss: 1.0457e-07
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0408e-07 - val_loss: 9.6659e-08
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3736e-08 - val_loss: 8.6541e-08
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7712e-08 - val_loss: 8.8327e-08
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1091e-08 - val_loss: 8.9010e-08
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7704e-08 - val_loss: 8.0726e-08
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0128e-08 - val_loss: 7.6898e-08
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8635e-08 - val_loss: 7.8668e-08
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9322e-08 - val_loss: 7.5688e-08
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4849e-08 - val_loss: 6.9977e-08
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9821e-08 - val_loss: 6.8266e-08
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9449e-08 - val_loss: 6.8307e-08
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.8314e-08 - val_loss: 6.4611e-08
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.3902e-08 - val_loss: 6.0967e-08
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.1696e-08 - val_loss: 6.0399e-08
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.1092e-08 - val_loss: 5.8410e-08
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.8247e-08 - val_loss: 5.5344e-08
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.5425e-08 - val_loss: 5.4018e-08
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.4529e-08 - val_loss: 5.2731e-08
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.2870e-08 - val_loss: 5.0173e-08
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.0034e-08 - val_loss: 4.8161e-08
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.8619e-08 - val_loss: 4.7490e-08
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.7488e-08 - val_loss: 4.6056e-08
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.5999e-08 - val_loss: 4.3916e-08
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.3735e-08 - val_loss: 4.1961e-08
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.2384e-08 - val_loss: 4.1012e-08
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.1340e-08 - val_loss: 4.0094e-08
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.9992e-08 - val_loss: 3.8080e-08
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.8035e-08 - val_loss: 3.6987e-08
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.7330e-08 - val_loss: 3.5946e-08
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.5914e-08 - val_loss: 3.4399e-08
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.4544e-08 - val_loss: 3.3311e-08
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.3369e-08 - val_loss: 3.2403e-08
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.2546e-08 - val_loss: 3.1495e-08
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1558e-08 - val_loss: 2.9982e-08
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.9980e-08 - val_loss: 2.8965e-08
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.9231e-08 - val_loss: 2.8316e-08
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.8507e-08 - val_loss: 2.7321e-08
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7192e-08 - val_loss: 2.6157e-08
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.6437e-08 - val_loss: 2.5712e-08
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.5706e-08 - val_loss: 2.4794e-08
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.4826e-08 - val_loss: 2.3775e-08
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.3885e-08 - val_loss: 2.3078e-08
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.3222e-08 - val_loss: 2.2289e-08
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.2418e-08 - val_loss: 2.1450e-08
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.1625e-08 - val_loss: 2.0948e-08
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.1184e-08 - val_loss: 2.0341e-08
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0339e-08 - val_loss: 1.9396e-08
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9442e-08 - val_loss: 1.8901e-08
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9170e-08 - val_loss: 1.8511e-08
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8561e-08 - val_loss: 1.7824e-08
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7871e-08 - val_loss: 1.7041e-08
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7077e-08 - val_loss: 1.6681e-08
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6933e-08 - val_loss: 1.6348e-08
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6327e-08 - val_loss: 1.5654e-08
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5566e-08 - val_loss: 1.5064e-08
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5241e-08 - val_loss: 1.4708e-08
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4860e-08 - val_loss: 1.4309e-08
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4381e-08 - val_loss: 1.3774e-08
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3907e-08 - val_loss: 1.3247e-08
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3266e-08 - val_loss: 1.2931e-08
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3131e-08 - val_loss: 1.2761e-08
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2812e-08 - val_loss: 1.2163e-08
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2152e-08 - val_loss: 1.1729e-08
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1929e-08 - val_loss: 1.1508e-08
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1600e-08 - val_loss: 1.1117e-08
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1117e-08 - val_loss: 1.0746e-08
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0887e-08 - val_loss: 1.0532e-08
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0635e-08 - val_loss: 1.0135e-08
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0104e-08 - val_loss: 9.8280e-09
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.9385e-09 - val_loss: 9.6959e-09
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.7150e-09 - val_loss: 9.3584e-09
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.4455e-09 - val_loss: 9.0529e-09
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.0671e-09 - val_loss: 8.7173e-09
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.8270e-09 - val_loss: 8.5232e-09
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.6366e-09 - val_loss: 8.3585e-09
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.4222e-09 - val_loss: 8.0100e-09
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.0329e-09 - val_loss: 7.7042e-09
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.8363e-09 - val_loss: 7.6657e-09
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.7661e-09 - val_loss: 7.4160e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.4438e-09 - val_loss: 7.1695e-09
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.2515e-09 - val_loss: 6.9704e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.0713e-09 - val_loss: 6.8142e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.8853e-09 - val_loss: 6.6488e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.6723e-09 - val_loss: 6.4007e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.4248e-09 - val_loss: 6.2920e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.3978e-09 - val_loss: 6.1677e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.1787e-09 - val_loss: 5.9590e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.9865e-09 - val_loss: 5.7756e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.8339e-09 - val_loss: 5.6282e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.6778e-09 - val_loss: 5.4908e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.5560e-09 - val_loss: 5.3661e-09
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.4131e-09 - val_loss: 5.2219e-09
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.2732e-09 - val_loss: 5.0974e-09
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.1398e-09 - val_loss: 4.9124e-09
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.9668e-09 - val_loss: 4.7888e-09
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.8370e-09 - val_loss: 4.6984e-09
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.7402e-09 - val_loss: 4.6495e-09
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.6933e-09 - val_loss: 4.4738e-09
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.5196e-09 - val_loss: 4.3153e-09
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.3659e-09 - val_loss: 4.2357e-09
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.3259e-09 - val_loss: 4.1870e-09
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.2200e-09 - val_loss: 4.0860e-09
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.1235e-09 - val_loss: 3.9476e-09
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.9637e-09 - val_loss: 3.8224e-09
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.8613e-09 - val_loss: 3.7477e-09
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.8174e-09 - val_loss: 3.7248e-09
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.7784e-09 - val_loss: 3.6498e-09
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.6725e-09 - val_loss: 3.5196e-09
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.5374e-09 - val_loss: 3.4243e-09
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.4631e-09 - val_loss: 3.3377e-09
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.4045e-09 - val_loss: 3.2645e-09
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.2969e-09 - val_loss: 3.1928e-09
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.2446e-09 - val_loss: 3.1282e-09
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1580e-09 - val_loss: 3.0634e-09
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1292e-09 - val_loss: 3.0086e-09
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.0340e-09 - val_loss: 2.9457e-09
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.9532e-09 - val_loss: 2.8690e-09
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.8924e-09 - val_loss: 2.7974e-09
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.8431e-09 - val_loss: 2.7512e-09
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7701e-09 - val_loss: 2.6996e-09
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7312e-09 - val_loss: 2.6703e-09
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7069e-09 - val_loss: 2.5988e-09
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.5921e-09 - val_loss: 2.4919e-09
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.5297e-09 - val_loss: 2.4651e-09
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.4920e-09 - val_loss: 2.4193e-09
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.4675e-09 - val_loss: 2.4072e-09
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.4300e-09 - val_loss: 2.3343e-09
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.3059e-09 - val_loss: 2.2193e-09
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.2464e-09 - val_loss: 2.2085e-09
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2506e-09 - val_loss: 2.2181e-09
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.2475e-09 - val_loss: 2.1760e-09
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.1831e-09 - val_loss: 2.0793e-09
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0950e-09 - val_loss: 2.0373e-09
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0682e-09 - val_loss: 2.0153e-09
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0384e-09 - val_loss: 1.9972e-09
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0233e-09 - val_loss: 1.9738e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9868e-09 - val_loss: 1.9110e-09
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9178e-09 - val_loss: 1.8370e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8548e-09 - val_loss: 1.8147e-09
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8462e-09 - val_loss: 1.8008e-09
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8416e-09 - val_loss: 1.8063e-09
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8307e-09 - val_loss: 1.7363e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7565e-09 - val_loss: 1.6922e-09
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6882e-09 - val_loss: 1.6344e-09
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6601e-09 - val_loss: 1.6362e-09
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6695e-09 - val_loss: 1.6234e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6403e-09 - val_loss: 1.5984e-09
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6207e-09 - val_loss: 1.5740e-09
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5747e-09 - val_loss: 1.5191e-09
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5462e-09 - val_loss: 1.5027e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4935e-09 - val_loss: 1.4509e-09
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4801e-09 - val_loss: 1.4589e-09
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4834e-09 - val_loss: 1.4625e-09
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4794e-09 - val_loss: 1.4136e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4198e-09 - val_loss: 1.3678e-09
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3721e-09 - val_loss: 1.3329e-09
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3499e-09 - val_loss: 1.3282e-09
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3481e-09 - val_loss: 1.3345e-09
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3418e-09 - val_loss: 1.2858e-09
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2883e-09 - val_loss: 1.2528e-09
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2678e-09 - val_loss: 1.2514e-09
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2644e-09 - val_loss: 1.2300e-09
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2484e-09 - val_loss: 1.2229e-09
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2348e-09 - val_loss: 1.1950e-09
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1924e-09 - val_loss: 1.1557e-09
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1700e-09 - val_loss: 1.1412e-09
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1594e-09 - val_loss: 1.1317e-09
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1551e-09 - val_loss: 1.1373e-09
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1384e-09 - val_loss: 1.1028e-09
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1014e-09 - val_loss: 1.0637e-09
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0734e-09 - val_loss: 1.0457e-09
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0651e-09 - val_loss: 1.0504e-09
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0740e-09 - val_loss: 1.0539e-09
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0604e-09 - val_loss: 1.0211e-09
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0155e-09 - val_loss: 9.8351e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.9436e-10 - val_loss: 9.8042e-10
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0037e-09 - val_loss: 9.8974e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.9858e-10 - val_loss: 9.6592e-10
Epoch 203/512

Epoch 00203: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.6918e-10 - val_loss: 9.3740e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.4699e-10 - val_loss: 9.1398e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2501e-10 - val_loss: 9.2234e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.3023e-10 - val_loss: 9.0635e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.1181e-10 - val_loss: 8.9551e-10
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.9987e-10 - val_loss: 8.7471e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.7620e-10 - val_loss: 8.6007e-10
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7368e-10 - val_loss: 8.6264e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.6939e-10 - val_loss: 8.3766e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.4792e-10 - val_loss: 8.2496e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.3479e-10 - val_loss: 8.1390e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.2477e-10 - val_loss: 8.0682e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.1975e-10 - val_loss: 8.0181e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.0868e-10 - val_loss: 7.8048e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.8171e-10 - val_loss: 7.5301e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6257e-10 - val_loss: 7.6273e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7745e-10 - val_loss: 7.6343e-10
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7699e-10 - val_loss: 7.5773e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.5901e-10 - val_loss: 7.3457e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.4720e-10 - val_loss: 7.2867e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.3147e-10 - val_loss: 7.0927e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.1259e-10 - val_loss: 6.9310e-10
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.0178e-10 - val_loss: 6.9122e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0387e-10 - val_loss: 6.9553e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.0455e-10 - val_loss: 6.8954e-10
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.9524e-10 - val_loss: 6.7643e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.8777e-10 - val_loss: 6.7264e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.7271e-10 - val_loss: 6.4536e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.4704e-10 - val_loss: 6.2576e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.2989e-10 - val_loss: 6.2148e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3217e-10 - val_loss: 6.3192e-10
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4433e-10 - val_loss: 6.3867e-10
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4671e-10 - val_loss: 6.2320e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.2130e-10 - val_loss: 5.9367e-10
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.9278e-10 - val_loss: 5.7227e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7613e-10 - val_loss: 5.8206e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9830e-10 - val_loss: 6.0139e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0686e-10 - val_loss: 5.9113e-10
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9297e-10 - val_loss: 5.7453e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.7635e-10 - val_loss: 5.6825e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.6838e-10 - val_loss: 5.4734e-10
Epoch 244/512

Epoch 00244: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.4984e-10 - val_loss: 5.4065e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.4533e-10 - val_loss: 5.3607e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3929e-10 - val_loss: 5.3803e-10
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4806e-10 - val_loss: 5.4468e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5745e-10 - val_loss: 5.4798e-10
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.4797e-10 - val_loss: 5.2747e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.2039e-10 - val_loss: 4.9807e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.9507e-10 - val_loss: 4.8687e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9283e-10 - val_loss: 4.8794e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9343e-10 - val_loss: 4.9783e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1310e-10 - val_loss: 5.0820e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1560e-10 - val_loss: 5.0292e-10
Epoch 256/512

Epoch 00256: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 5.0071e-10 - val_loss: 4.7876e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.8304e-10 - val_loss: 4.6951e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.6297e-10 - val_loss: 4.5857e-10
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6638e-10 - val_loss: 4.5883e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.6123e-10 - val_loss: 4.5182e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5679e-10 - val_loss: 4.5254e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.5913e-10 - val_loss: 4.5172e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.5798e-10 - val_loss: 4.4676e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.5022e-10 - val_loss: 4.4625e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.4943e-10 - val_loss: 4.3689e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.3655e-10 - val_loss: 4.2669e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.2553e-10 - val_loss: 4.1050e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1353e-10 - val_loss: 4.1843e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2557e-10 - val_loss: 4.2115e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2039e-10 - val_loss: 4.1848e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2630e-10 - val_loss: 4.1864e-10
Epoch 272/512

Epoch 00272: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.1332e-10 - val_loss: 4.0032e-10
Epoch 273/512

Epoch 00273: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 4.0074e-10 - val_loss: 3.8659e-10
Epoch 274/512

Epoch 00274: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.8851e-10 - val_loss: 3.8109e-10
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9083e-10 - val_loss: 3.9616e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0223e-10 - val_loss: 3.9928e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9873e-10 - val_loss: 3.8224e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.8378e-10 - val_loss: 3.7581e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.7254e-10 - val_loss: 3.6287e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.6681e-10 - val_loss: 3.6279e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6933e-10 - val_loss: 3.7762e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8222e-10 - val_loss: 3.7644e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7671e-10 - val_loss: 3.6920e-10
Epoch 284/512

Epoch 00284: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.6963e-10 - val_loss: 3.5275e-10
Epoch 285/512

Epoch 00285: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.5347e-10 - val_loss: 3.4985e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5411e-10 - val_loss: 3.5249e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5691e-10 - val_loss: 3.5043e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5419e-10 - val_loss: 3.5177e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5630e-10 - val_loss: 3.4995e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.5288e-10 - val_loss: 3.4776e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.4996e-10 - val_loss: 3.4623e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.4544e-10 - val_loss: 3.3250e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.2848e-10 - val_loss: 3.1417e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1587e-10 - val_loss: 3.1759e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2028e-10 - val_loss: 3.1574e-10
Epoch 296/512

Epoch 00296: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1936e-10 - val_loss: 3.1214e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1214e-10 - val_loss: 3.0409e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1262e-10 - val_loss: 3.1383e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1898e-10 - val_loss: 3.1589e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1720e-10 - val_loss: 3.1229e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1571e-10 - val_loss: 3.1068e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 3.1098e-10 - val_loss: 2.9760e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.9702e-10 - val_loss: 2.8960e-10
Epoch 304/512

Epoch 00304: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.9084e-10 - val_loss: 2.8011e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7685e-10 - val_loss: 2.7107e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8297e-10 - val_loss: 2.8925e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9493e-10 - val_loss: 3.0590e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0975e-10 - val_loss: 2.9706e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9144e-10 - val_loss: 2.8211e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7929e-10 - val_loss: 2.6691e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.6824e-10 - val_loss: 2.6128e-10
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6085e-10 - val_loss: 2.6137e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6729e-10 - val_loss: 2.7035e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7910e-10 - val_loss: 2.7940e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7824e-10 - val_loss: 2.7444e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.7010e-10 - val_loss: 2.5613e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6203e-10 - val_loss: 2.6065e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6302e-10 - val_loss: 2.5988e-10
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6807e-10 - val_loss: 2.6872e-10
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6516e-10 - val_loss: 2.5806e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5865e-10 - val_loss: 2.6026e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.6200e-10 - val_loss: 2.5385e-10
Epoch 323/512

Epoch 00323: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.5429e-10 - val_loss: 2.5152e-10
Epoch 324/512

Epoch 00324: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.5058e-10 - val_loss: 2.4818e-10
Epoch 325/512

Epoch 00325: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.4607e-10 - val_loss: 2.3596e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4232e-10 - val_loss: 2.4147e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4242e-10 - val_loss: 2.4411e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4389e-10 - val_loss: 2.3871e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4647e-10 - val_loss: 2.4880e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4977e-10 - val_loss: 2.4736e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4647e-10 - val_loss: 2.3875e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3795e-10 - val_loss: 2.3898e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.3779e-10 - val_loss: 2.2758e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.2709e-10 - val_loss: 2.2538e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3131e-10 - val_loss: 2.3414e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3400e-10 - val_loss: 2.3086e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3345e-10 - val_loss: 2.3302e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3316e-10 - val_loss: 2.3008e-10
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.2983e-10 - val_loss: 2.2036e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.1650e-10 - val_loss: 2.0842e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0750e-10 - val_loss: 2.0202e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0277e-10 - val_loss: 2.0617e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0874e-10 - val_loss: 2.1436e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1992e-10 - val_loss: 2.2215e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2083e-10 - val_loss: 2.2134e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2130e-10 - val_loss: 2.1358e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1479e-10 - val_loss: 2.0907e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0735e-10 - val_loss: 2.0588e-10
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 2.0419e-10 - val_loss: 1.9602e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9642e-10 - val_loss: 1.9339e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9341e-10 - val_loss: 1.9042e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9239e-10 - val_loss: 1.9558e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0148e-10 - val_loss: 2.0398e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0413e-10 - val_loss: 2.0415e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0528e-10 - val_loss: 2.0382e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0271e-10 - val_loss: 1.9425e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9503e-10 - val_loss: 1.9177e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9435e-10 - val_loss: 1.9302e-10
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.9186e-10 - val_loss: 1.8810e-10
Epoch 360/512

Epoch 00360: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8726e-10 - val_loss: 1.8458e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8613e-10 - val_loss: 1.8957e-10
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8570e-10 - val_loss: 1.7990e-10
Epoch 363/512

Epoch 00363: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8262e-10 - val_loss: 1.7958e-10
Epoch 364/512

Epoch 00364: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7984e-10 - val_loss: 1.7941e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8153e-10 - val_loss: 1.8165e-10
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8411e-10 - val_loss: 1.7839e-10
Epoch 367/512

Epoch 00367: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7892e-10 - val_loss: 1.7839e-10
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8182e-10 - val_loss: 1.7973e-10
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.8072e-10 - val_loss: 1.7614e-10
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7793e-10 - val_loss: 1.7578e-10
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7344e-10 - val_loss: 1.6950e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7047e-10 - val_loss: 1.6986e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7206e-10 - val_loss: 1.7225e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7310e-10 - val_loss: 1.7296e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7661e-10 - val_loss: 1.7668e-10
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7841e-10 - val_loss: 1.7291e-10
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.7383e-10 - val_loss: 1.6670e-10
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6629e-10 - val_loss: 1.6139e-10
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6085e-10 - val_loss: 1.5702e-10
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5593e-10 - val_loss: 1.5523e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5742e-10 - val_loss: 1.5901e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6114e-10 - val_loss: 1.6153e-10
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6364e-10 - val_loss: 1.6215e-10
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.6202e-10 - val_loss: 1.5327e-10
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5500e-10 - val_loss: 1.5106e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5182e-10 - val_loss: 1.5439e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6054e-10 - val_loss: 1.6327e-10
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6208e-10 - val_loss: 1.6055e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5739e-10 - val_loss: 1.5331e-10
Epoch 390/512

Epoch 00390: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.5142e-10 - val_loss: 1.4678e-10
Epoch 391/512

Epoch 00391: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4474e-10 - val_loss: 1.4181e-10
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3996e-10 - val_loss: 1.4191e-10
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4976e-10 - val_loss: 1.5467e-10
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5455e-10 - val_loss: 1.4907e-10
Epoch 395/512

Epoch 00395: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4855e-10 - val_loss: 1.3978e-10
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4280e-10 - val_loss: 1.4534e-10
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4717e-10 - val_loss: 1.4681e-10
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4734e-10 - val_loss: 1.4486e-10
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.4148e-10 - val_loss: 1.3829e-10
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3990e-10 - val_loss: 1.3936e-10
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3983e-10 - val_loss: 1.3615e-10
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3925e-10 - val_loss: 1.4139e-10
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4228e-10 - val_loss: 1.4333e-10
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4528e-10 - val_loss: 1.4103e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4130e-10 - val_loss: 1.3788e-10
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3874e-10 - val_loss: 1.3630e-10
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3491e-10 - val_loss: 1.2909e-10
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3116e-10 - val_loss: 1.3127e-10
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3113e-10 - val_loss: 1.3238e-10
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3550e-10 - val_loss: 1.3274e-10
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.3340e-10 - val_loss: 1.2549e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2509e-10 - val_loss: 1.2517e-10
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2900e-10 - val_loss: 1.3282e-10
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3612e-10 - val_loss: 1.3565e-10
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3489e-10 - val_loss: 1.2735e-10
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2629e-10 - val_loss: 1.2605e-10
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2449e-10 - val_loss: 1.2512e-10
Epoch 418/512

Epoch 00418: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2822e-10 - val_loss: 1.2488e-10
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2713e-10 - val_loss: 1.2883e-10
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2918e-10 - val_loss: 1.2929e-10
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2987e-10 - val_loss: 1.2676e-10
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2745e-10 - val_loss: 1.2402e-10
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2381e-10 - val_loss: 1.2423e-10
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.2167e-10 - val_loss: 1.1572e-10
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1479e-10 - val_loss: 1.1511e-10
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1592e-10 - val_loss: 1.1361e-10
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1589e-10 - val_loss: 1.1551e-10
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1664e-10 - val_loss: 1.1695e-10
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2251e-10 - val_loss: 1.2891e-10
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2761e-10 - val_loss: 1.2370e-10
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2407e-10 - val_loss: 1.1812e-10
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1602e-10 - val_loss: 1.1174e-10
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.1220e-10 - val_loss: 1.1087e-10
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0844e-10 - val_loss: 1.0780e-10
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1138e-10 - val_loss: 1.1184e-10
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1280e-10 - val_loss: 1.1695e-10
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2070e-10 - val_loss: 1.2071e-10
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1799e-10 - val_loss: 1.1025e-10
Epoch 439/512

Epoch 00439: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0715e-10 - val_loss: 1.0408e-10
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0491e-10 - val_loss: 1.0505e-10
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0750e-10 - val_loss: 1.1145e-10
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1004e-10 - val_loss: 1.0960e-10
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1088e-10 - val_loss: 1.0691e-10
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0878e-10 - val_loss: 1.0879e-10
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0992e-10 - val_loss: 1.1072e-10
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1224e-10 - val_loss: 1.1017e-10
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0875e-10 - val_loss: 1.0756e-10
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0917e-10 - val_loss: 1.0762e-10
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 1.0496e-10 - val_loss: 1.0086e-10
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0178e-10 - val_loss: 1.0114e-10
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.7767e-11 - val_loss: 9.5794e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9311e-11 - val_loss: 1.0256e-10
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8990e-11 - val_loss: 9.6686e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8722e-11 - val_loss: 9.5904e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7339e-11 - val_loss: 9.7895e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9244e-11 - val_loss: 9.6512e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7059e-11 - val_loss: 9.9678e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9472e-11 - val_loss: 9.9655e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0243e-10 - val_loss: 1.0433e-10
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0745e-10 - val_loss: 1.0487e-10
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0417e-10 - val_loss: 9.9759e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.7349e-11 - val_loss: 9.3884e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.4546e-11 - val_loss: 9.2057e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2462e-11 - val_loss: 9.4612e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7105e-11 - val_loss: 9.6011e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5321e-11 - val_loss: 9.4110e-11
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.2716e-11 - val_loss: 9.0865e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2310e-11 - val_loss: 9.4545e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3080e-11 - val_loss: 9.1417e-11
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2600e-11 - val_loss: 9.3300e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.2421e-11 - val_loss: 9.0599e-11
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1158e-11 - val_loss: 9.3240e-11
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2807e-11 - val_loss: 9.1963e-11
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 9.0701e-11 - val_loss: 8.7850e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8455e-11 - val_loss: 8.8213e-11
Epoch 476/512

Epoch 00476: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.8718e-11 - val_loss: 8.4899e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5331e-11 - val_loss: 8.6291e-11
Epoch 478/512

Epoch 00478: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.7566e-11 - val_loss: 8.3844e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5879e-11 - val_loss: 8.8210e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9501e-11 - val_loss: 9.1958e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1652e-11 - val_loss: 8.9645e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9523e-11 - val_loss: 8.9300e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8754e-11 - val_loss: 8.7602e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7813e-11 - val_loss: 8.8029e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7206e-11 - val_loss: 8.6310e-11
Epoch 486/512

Epoch 00486: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 8.6228e-11 - val_loss: 8.0801e-11
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.8746e-11 - val_loss: 7.7073e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9326e-11 - val_loss: 7.9662e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9624e-11 - val_loss: 7.9896e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3050e-11 - val_loss: 8.5143e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4197e-11 - val_loss: 8.1959e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2244e-11 - val_loss: 8.1967e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0444e-11 - val_loss: 7.7950e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8021e-11 - val_loss: 7.9715e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2816e-11 - val_loss: 7.9729e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8580e-11 - val_loss: 7.9543e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2663e-11 - val_loss: 8.4935e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4185e-11 - val_loss: 8.3321e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4935e-11 - val_loss: 8.5469e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4208e-11 - val_loss: 7.8724e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9348e-11 - val_loss: 7.9666e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.7624e-11 - val_loss: 7.5022e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 7.4374e-11 - val_loss: 6.9806e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-rate-0.5-curve-secp256k1/addition_weights.h5
512/512 - 0s - loss: 6.8755e-11 - val_loss: 6.8394e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1116e-11 - val_loss: 7.4849e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7880e-11 - val_loss: 7.9595e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0782e-11 - val_loss: 7.9526e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1657e-11 - val_loss: 8.2425e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2883e-11 - val_loss: 7.8869e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8340e-11 - val_loss: 7.4032e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1563e-11 - val_loss: 7.0353e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1908e-11 - val_loss: 7.2271e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.070 | eve: 8.918 | bob: 8.790Epoch   0:   0% | abe: 8.983 | eve: 8.966 | bob: 8.738Epoch   0:   1% | abe: 8.922 | eve: 8.971 | bob: 8.704Epoch   0:   2% | abe: 8.873 | eve: 8.978 | bob: 8.672Epoch   0:   3% | abe: 8.835 | eve: 8.945 | bob: 8.645Epoch   0:   3% | abe: 8.783 | eve: 8.971 | bob: 8.603Epoch   0:   4% | abe: 8.748 | eve: 8.988 | bob: 8.580Epoch   0:   5% | abe: 8.710 | eve: 8.987 | bob: 8.552Epoch   0:   6% | abe: 8.691 | eve: 8.994 | bob: 8.541Epoch   0:   7% | abe: 8.670 | eve: 9.004 | bob: 8.525Epoch   0:   7% | abe: 8.655 | eve: 9.007 | bob: 8.514Epoch   0:   8% | abe: 8.645 | eve: 9.005 | bob: 8.507Epoch   0:   9% | abe: 8.632 | eve: 9.001 | bob: 8.496Epoch   0:  10% | abe: 8.625 | eve: 9.000 | bob: 8.491Epoch   0:  10% | abe: 8.620 | eve: 9.002 | bob: 8.488Epoch   0:  11% | abe: 8.609 | eve: 8.999 | bob: 8.478Epoch   0:  12% | abe: 8.598 | eve: 9.001 | bob: 8.469Epoch   0:  13% | abe: 8.585 | eve: 9.005 | bob: 8.457Epoch   0:  14% | abe: 8.573 | eve: 9.010 | bob: 8.446Epoch   0:  14% | abe: 8.564 | eve: 9.009 | bob: 8.438Epoch   0:  15% | abe: 8.550 | eve: 9.009 | bob: 8.426Epoch   0:  16% | abe: 8.537 | eve: 9.012 | bob: 8.414Epoch   0:  17% | abe: 8.526 | eve: 9.017 | bob: 8.404Epoch   0:  17% | abe: 8.517 | eve: 9.018 | bob: 8.397Epoch   0:  18% | abe: 8.505 | eve: 9.014 | bob: 8.386Epoch   0:  19% | abe: 8.499 | eve: 9.013 | bob: 8.381Epoch   0:  20% | abe: 8.490 | eve: 9.015 | bob: 8.373Epoch   0:  21% | abe: 8.487 | eve: 9.019 | bob: 8.371Epoch   0:  21% | abe: 8.482 | eve: 9.019 | bob: 8.366Epoch   0:  22% | abe: 8.475 | eve: 9.018 | bob: 8.360Epoch   0:  23% | abe: 8.466 | eve: 9.023 | bob: 8.352Epoch   0:  24% | abe: 8.460 | eve: 9.024 | bob: 8.347Epoch   0:  25% | abe: 8.454 | eve: 9.022 | bob: 8.342Epoch   0:  25% | abe: 8.447 | eve: 9.021 | bob: 8.335Epoch   0:  26% | abe: 8.440 | eve: 9.024 | bob: 8.329Epoch   0:  27% | abe: 8.436 | eve: 9.027 | bob: 8.326Epoch   0:  28% | abe: 8.431 | eve: 9.026 | bob: 8.321Epoch   0:  28% | abe: 8.426 | eve: 9.029 | bob: 8.317Epoch   0:  29% | abe: 8.422 | eve: 9.030 | bob: 8.314Epoch   0:  30% | abe: 8.417 | eve: 9.033 | bob: 8.309Epoch   0:  31% | abe: 8.413 | eve: 9.036 | bob: 8.305Epoch   0:  32% | abe: 8.407 | eve: 9.037 | bob: 8.300Epoch   0:  32% | abe: 8.402 | eve: 9.038 | bob: 8.296Epoch   0:  33% | abe: 8.399 | eve: 9.039 | bob: 8.293Epoch   0:  34% | abe: 8.395 | eve: 9.041 | bob: 8.289Epoch   0:  35% | abe: 8.392 | eve: 9.043 | bob: 8.286Epoch   0:  35% | abe: 8.389 | eve: 9.046 | bob: 8.285Epoch   0:  36% | abe: 8.385 | eve: 9.048 | bob: 8.281Epoch   0:  37% | abe: 8.381 | eve: 9.048 | bob: 8.277Epoch   0:  38% | abe: 8.379 | eve: 9.049 | bob: 8.275Epoch   0:  39% | abe: 8.375 | eve: 9.051 | bob: 8.271Epoch   0:  39% | abe: 8.372 | eve: 9.052 | bob: 8.269Epoch   0:  40% | abe: 8.369 | eve: 9.054 | bob: 8.266Epoch   0:  41% | abe: 8.365 | eve: 9.053 | bob: 8.263Epoch   0:  42% | abe: 8.362 | eve: 9.053 | bob: 8.260Epoch   0:  42% | abe: 8.359 | eve: 9.053 | bob: 8.257Epoch   0:  43% | abe: 8.355 | eve: 9.052 | bob: 8.253Epoch   0:  44% | abe: 8.352 | eve: 9.053 | bob: 8.251Epoch   0:  45% | abe: 8.350 | eve: 9.054 | bob: 8.249Epoch   0:  46% | abe: 8.347 | eve: 9.055 | bob: 8.246Epoch   0:  46% | abe: 8.345 | eve: 9.057 | bob: 8.244Epoch   0:  47% | abe: 8.342 | eve: 9.059 | bob: 8.242Epoch   0:  48% | abe: 8.340 | eve: 9.059 | bob: 8.240Epoch   0:  49% | abe: 8.337 | eve: 9.059 | bob: 8.238Epoch   0:  50% | abe: 8.335 | eve: 9.060 | bob: 8.236Epoch   0:  50% | abe: 8.332 | eve: 9.060 | bob: 8.233Epoch   0:  51% | abe: 8.329 | eve: 9.060 | bob: 8.230Epoch   0:  52% | abe: 8.327 | eve: 9.062 | bob: 8.228Epoch   0:  53% | abe: 8.324 | eve: 9.062 | bob: 8.225Epoch   0:  53% | abe: 8.321 | eve: 9.063 | bob: 8.222Epoch   0:  54% | abe: 8.318 | eve: 9.065 | bob: 8.219Epoch   0:  55% | abe: 8.315 | eve: 9.068 | bob: 8.217Epoch   0:  56% | abe: 8.315 | eve: 9.068 | bob: 8.217Epoch   0:  57% | abe: 8.313 | eve: 9.070 | bob: 8.215Epoch   0:  57% | abe: 8.309 | eve: 9.070 | bob: 8.211Epoch   0:  58% | abe: 8.307 | eve: 9.071 | bob: 8.209Epoch   0:  59% | abe: 8.305 | eve: 9.071 | bob: 8.207Epoch   0:  60% | abe: 8.304 | eve: 9.073 | bob: 8.206Epoch   0:  60% | abe: 8.302 | eve: 9.073 | bob: 8.205Epoch   0:  61% | abe: 8.300 | eve: 9.074 | bob: 8.203Epoch   0:  62% | abe: 8.298 | eve: 9.075 | bob: 8.201Epoch   0:  63% | abe: 8.296 | eve: 9.076 | bob: 8.199Epoch   0:  64% | abe: 8.295 | eve: 9.077 | bob: 8.198Epoch   0:  64% | abe: 8.294 | eve: 9.077 | bob: 8.197Epoch   0:  65% | abe: 8.293 | eve: 9.077 | bob: 8.196Epoch   0:  66% | abe: 8.290 | eve: 9.077 | bob: 8.193Epoch   0:  67% | abe: 8.287 | eve: 9.078 | bob: 8.191Epoch   0:  67% | abe: 8.286 | eve: 9.078 | bob: 8.190Epoch   0:  68% | abe: 8.285 | eve: 9.076 | bob: 8.189Epoch   0:  69% | abe: 8.284 | eve: 9.076 | bob: 8.188Epoch   0:  70% | abe: 8.283 | eve: 9.077 | bob: 8.187Epoch   0:  71% | abe: 8.282 | eve: 9.077 | bob: 8.186Epoch   0:  71% | abe: 8.280 | eve: 9.078 | bob: 8.185Epoch   0:  72% | abe: 8.279 | eve: 9.078 | bob: 8.183Epoch   0:  73% | abe: 8.277 | eve: 9.079 | bob: 8.182Epoch   0:  74% | abe: 8.275 | eve: 9.078 | bob: 8.180Epoch   0:  75% | abe: 8.274 | eve: 9.079 | bob: 8.179Epoch   0:  75% | abe: 8.272 | eve: 9.078 | bob: 8.177Epoch   0:  76% | abe: 8.271 | eve: 9.078 | bob: 8.176Epoch   0:  77% | abe: 8.270 | eve: 9.077 | bob: 8.176Epoch   0:  78% | abe: 8.270 | eve: 9.079 | bob: 8.175Epoch   0:  78% | abe: 8.269 | eve: 9.079 | bob: 8.175Epoch   0:  79% | abe: 8.267 | eve: 9.078 | bob: 8.173Epoch   0:  80% | abe: 8.266 | eve: 9.077 | bob: 8.172Epoch   0:  81% | abe: 8.266 | eve: 9.079 | bob: 8.172Epoch   0:  82% | abe: 8.265 | eve: 9.078 | bob: 8.171Epoch   0:  82% | abe: 8.263 | eve: 9.078 | bob: 8.169Epoch   0:  83% | abe: 8.262 | eve: 9.079 | bob: 8.168Epoch   0:  84% | abe: 8.261 | eve: 9.079 | bob: 8.167Epoch   0:  85% | abe: 8.260 | eve: 9.080 | bob: 8.167Epoch   0:  85% | abe: 8.260 | eve: 9.080 | bob: 8.166Epoch   0:  86% | abe: 8.258 | eve: 9.080 | bob: 8.165Epoch   0:  87% | abe: 8.257 | eve: 9.080 | bob: 8.164Epoch   0:  88% | abe: 8.256 | eve: 9.080 | bob: 8.163Epoch   0:  89% | abe: 8.255 | eve: 9.080 | bob: 8.162Epoch   0:  89% | abe: 8.254 | eve: 9.080 | bob: 8.161Epoch   0:  90% | abe: 8.252 | eve: 9.080 | bob: 8.159Epoch   0:  91% | abe: 8.252 | eve: 9.081 | bob: 8.159Epoch   0:  92% | abe: 8.250 | eve: 9.082 | bob: 8.158Epoch   0:  92% | abe: 8.249 | eve: 9.082 | bob: 8.157Epoch   0:  93% | abe: 8.249 | eve: 9.082 | bob: 8.156Epoch   0:  94% | abe: 8.248 | eve: 9.083 | bob: 8.155Epoch   0:  95% | abe: 8.245 | eve: 9.083 | bob: 8.153Epoch   0:  96% | abe: 8.245 | eve: 9.084 | bob: 8.153Epoch   0:  96% | abe: 8.245 | eve: 9.084 | bob: 8.152Epoch   0:  97% | abe: 8.243 | eve: 9.084 | bob: 8.151Epoch   0:  98% | abe: 8.242 | eve: 9.084 | bob: 8.150Epoch   0:  99% | abe: 8.241 | eve: 9.083 | bob: 8.149Epoch   1:   0% | abe: 8.224 | eve: 9.135 | bob: 8.145Epoch   1:   0% | abe: 8.175 | eve: 9.166 | bob: 8.094Epoch   1:   1% | abe: 8.165 | eve: 9.131 | bob: 8.084Epoch   1:   2% | abe: 8.161 | eve: 9.120 | bob: 8.079Epoch   1:   3% | abe: 8.155 | eve: 9.110 | bob: 8.073Epoch   1:   3% | abe: 8.145 | eve: 9.109 | bob: 8.063Epoch   1:   4% | abe: 8.136 | eve: 9.116 | bob: 8.053Epoch   1:   5% | abe: 8.138 | eve: 9.099 | bob: 8.055Epoch   1:   6% | abe: 8.120 | eve: 9.097 | bob: 8.037Epoch   1:   7% | abe: 8.132 | eve: 9.091 | bob: 8.049Epoch   1:   7% | abe: 8.131 | eve: 9.101 | bob: 8.049Epoch   1:   8% | abe: 8.137 | eve: 9.093 | bob: 8.054Epoch   1:   9% | abe: 8.138 | eve: 9.087 | bob: 8.055Epoch   1:  10% | abe: 8.129 | eve: 9.087 | bob: 8.046Epoch   1:  10% | abe: 8.125 | eve: 9.095 | bob: 8.042Epoch   1:  11% | abe: 8.115 | eve: 9.095 | bob: 8.032Epoch   1:  12% | abe: 8.107 | eve: 9.099 | bob: 8.024Epoch   1:  13% | abe: 8.104 | eve: 9.101 | bob: 8.021Epoch   1:  14% | abe: 8.102 | eve: 9.099 | bob: 8.019Epoch   1:  14% | abe: 8.097 | eve: 9.100 | bob: 8.013Epoch   1:  15% | abe: 8.099 | eve: 9.103 | bob: 8.016Epoch   1:  16% | abe: 8.101 | eve: 9.106 | bob: 8.017Epoch   1:  17% | abe: 8.098 | eve: 9.102 | bob: 8.015Epoch   1:  17% | abe: 8.101 | eve: 9.106 | bob: 8.017Epoch   1:  18% | abe: 8.101 | eve: 9.109 | bob: 8.018Epoch   1:  19% | abe: 8.100 | eve: 9.107 | bob: 8.017Epoch   1:  20% | abe: 8.102 | eve: 9.107 | bob: 8.019Epoch   1:  21% | abe: 8.103 | eve: 9.107 | bob: 8.020Epoch   1:  21% | abe: 8.103 | eve: 9.108 | bob: 8.019Epoch   1:  22% | abe: 8.103 | eve: 9.108 | bob: 8.019Epoch   1:  23% | abe: 8.105 | eve: 9.109 | bob: 8.021Epoch   1:  24% | abe: 8.106 | eve: 9.109 | bob: 8.023Epoch   1:  25% | abe: 8.110 | eve: 9.113 | bob: 8.026Epoch   1:  25% | abe: 8.109 | eve: 9.113 | bob: 8.026Epoch   1:  26% | abe: 8.109 | eve: 9.114 | bob: 8.025Epoch   1:  27% | abe: 8.109 | eve: 9.115 | bob: 8.025Epoch   1:  28% | abe: 8.110 | eve: 9.113 | bob: 8.027Epoch   1:  28% | abe: 8.110 | eve: 9.113 | bob: 8.027Epoch   1:  29% | abe: 8.111 | eve: 9.116 | bob: 8.028Epoch   1:  30% | abe: 8.114 | eve: 9.115 | bob: 8.030Epoch   1:  31% | abe: 8.113 | eve: 9.115 | bob: 8.030Epoch   1:  32% | abe: 8.113 | eve: 9.115 | bob: 8.030Epoch   1:  32% | abe: 8.113 | eve: 9.116 | bob: 8.030Epoch   1:  33% | abe: 8.111 | eve: 9.117 | bob: 8.028Epoch   1:  34% | abe: 8.111 | eve: 9.116 | bob: 8.028Epoch   1:  35% | abe: 8.109 | eve: 9.116 | bob: 8.026