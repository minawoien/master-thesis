WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-11 21:19:19.144849: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-11 21:19:19.244157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-11 21:19:19.245112: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 21:19:19.248554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 21:19:19.251529: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 21:19:19.252519: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 21:19:19.256421: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 21:19:19.259884: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 21:19:19.266209: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 21:19:19.273233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 21:19:19.273774: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-11 21:19:19.289624: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-11 21:19:19.293144: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52360d0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-11 21:19:19.293217: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-11 21:19:19.567736: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4bff0e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-11 21:19:19.567827: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-11 21:19:19.574366: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-11 21:19:19.574489: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 21:19:19.574528: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 21:19:19.574560: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 21:19:19.574591: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 21:19:19.574622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 21:19:19.574652: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 21:19:19.574685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 21:19:19.584751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 21:19:19.584832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 21:19:19.588127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-11 21:19:19.588148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-11 21:19:19.588159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-11 21:19:19.595713: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-11 21:19:22.776497: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.4500 - val_loss: 0.0012
Epoch 2/512
512/512 - 0s - loss: 0.0659 - val_loss: 1.9271e-04
Epoch 3/512
512/512 - 0s - loss: 0.0135 - val_loss: 8.5051e-05
Epoch 4/512
512/512 - 0s - loss: 0.0075 - val_loss: 6.1207e-05
Epoch 5/512
512/512 - 0s - loss: 0.0054 - val_loss: 4.2189e-05
Epoch 6/512
512/512 - 0s - loss: 0.0036 - val_loss: 2.6134e-05
Epoch 7/512
512/512 - 0s - loss: 0.0021 - val_loss: 1.4093e-05
Epoch 8/512
512/512 - 0s - loss: 0.0011 - val_loss: 6.3459e-06
Epoch 9/512
512/512 - 0s - loss: 4.5727e-04 - val_loss: 2.2628e-06
Epoch 10/512
512/512 - 0s - loss: 1.5024e-04 - val_loss: 5.9716e-07
Epoch 11/512
512/512 - 0s - loss: 3.6023e-05 - val_loss: 1.0741e-07
Epoch 12/512
512/512 - 0s - loss: 5.9015e-06 - val_loss: 2.2420e-08
Epoch 13/512
512/512 - 0s - loss: 4.4626e-05 - val_loss: 9.6877e-06
Epoch 14/512
512/512 - 0s - loss: 0.0053 - val_loss: 2.5498e-05
Epoch 15/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.9069e-06
Epoch 16/512
512/512 - 0s - loss: 1.8463e-04 - val_loss: 3.2586e-06
Epoch 17/512
512/512 - 0s - loss: 9.4347e-04 - val_loss: 3.3320e-05
Epoch 18/512
512/512 - 0s - loss: 0.0032 - val_loss: 1.5355e-05
Epoch 19/512
512/512 - 0s - loss: 9.4550e-04 - val_loss: 5.8317e-06
Epoch 20/512
512/512 - 0s - loss: 7.2492e-04 - val_loss: 1.3931e-05
Epoch 21/512
512/512 - 0s - loss: 0.0019 - val_loss: 2.3504e-05
Epoch 22/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.0513e-05
Epoch 23/512
512/512 - 0s - loss: 9.5758e-04 - val_loss: 1.0851e-05
Epoch 24/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.8166e-05
Epoch 25/512
512/512 - 0s - loss: 0.0017 - val_loss: 1.4281e-05
Epoch 26/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1174e-05
Epoch 27/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.4186e-05
Epoch 28/512
512/512 - 0s - loss: 0.0014 - val_loss: 1.5062e-05
Epoch 29/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2019e-05
Epoch 30/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.2465e-05
Epoch 31/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.4082e-05
Epoch 32/512
512/512 - 0s - loss: 0.0013 - val_loss: 1.2673e-05
Epoch 33/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1865e-05
Epoch 34/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.2627e-05
Epoch 35/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.2559e-05
Epoch 36/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1687e-05
Epoch 37/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1814e-05
Epoch 38/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.2302e-05
Epoch 39/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1426e-05
Epoch 40/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1168e-05
Epoch 41/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1643e-05
Epoch 42/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1473e-05
Epoch 43/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0933e-05
Epoch 44/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0967e-05
Epoch 45/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.1010e-05
Epoch 46/512
512/512 - 0s - loss: 0.0011 - val_loss: 1.0641e-05
Epoch 47/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0522e-05
Epoch 48/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0696e-05
Epoch 49/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0360e-05
Epoch 50/512
512/512 - 0s - loss: 9.9388e-04 - val_loss: 1.0165e-05
Epoch 51/512
512/512 - 0s - loss: 9.9254e-04 - val_loss: 1.0479e-05
Epoch 52/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0155e-05
Epoch 53/512
512/512 - 0s - loss: 9.7367e-04 - val_loss: 9.8625e-06
Epoch 54/512
512/512 - 0s - loss: 9.6182e-04 - val_loss: 9.8871e-06
Epoch 55/512
512/512 - 0s - loss: 9.5817e-04 - val_loss: 1.0089e-05
Epoch 56/512
512/512 - 0s - loss: 9.6750e-04 - val_loss: 9.8296e-06
Epoch 57/512
512/512 - 0s - loss: 9.4370e-04 - val_loss: 9.4490e-06
Epoch 58/512
512/512 - 0s - loss: 9.1994e-04 - val_loss: 9.5664e-06
Epoch 59/512
512/512 - 0s - loss: 9.3884e-04 - val_loss: 9.5215e-06
Epoch 60/512
512/512 - 0s - loss: 9.1575e-04 - val_loss: 9.3264e-06
Epoch 61/512
512/512 - 0s - loss: 9.0734e-04 - val_loss: 9.3364e-06
Epoch 62/512
512/512 - 0s - loss: 9.0770e-04 - val_loss: 9.1520e-06
Epoch 63/512
512/512 - 0s - loss: 8.9239e-04 - val_loss: 9.0568e-06
Epoch 64/512
512/512 - 0s - loss: 8.8738e-04 - val_loss: 8.9985e-06
Epoch 65/512
512/512 - 0s - loss: 8.7919e-04 - val_loss: 8.9768e-06
Epoch 66/512
512/512 - 0s - loss: 8.7143e-04 - val_loss: 8.9539e-06
Epoch 67/512
512/512 - 0s - loss: 8.7180e-04 - val_loss: 8.8191e-06
Epoch 68/512
512/512 - 0s - loss: 8.5897e-04 - val_loss: 8.6597e-06
Epoch 69/512
512/512 - 0s - loss: 8.4840e-04 - val_loss: 8.6492e-06
Epoch 70/512
512/512 - 0s - loss: 8.4714e-04 - val_loss: 8.6935e-06
Epoch 71/512
512/512 - 0s - loss: 8.4944e-04 - val_loss: 8.4116e-06
Epoch 72/512
512/512 - 0s - loss: 8.2162e-04 - val_loss: 8.4043e-06
Epoch 73/512
512/512 - 0s - loss: 8.2354e-04 - val_loss: 8.5842e-06
Epoch 74/512
512/512 - 0s - loss: 8.4483e-04 - val_loss: 8.1716e-06
Epoch 75/512
512/512 - 0s - loss: 7.9637e-04 - val_loss: 8.1045e-06
Epoch 76/512
512/512 - 0s - loss: 8.0435e-04 - val_loss: 8.3856e-06
Epoch 77/512
512/512 - 0s - loss: 8.2076e-04 - val_loss: 8.2434e-06
Epoch 78/512
512/512 - 0s - loss: 8.0319e-04 - val_loss: 7.8342e-06
Epoch 79/512
512/512 - 0s - loss: 7.7546e-04 - val_loss: 7.9190e-06
Epoch 80/512
512/512 - 0s - loss: 7.8654e-04 - val_loss: 8.3034e-06
Epoch 81/512
512/512 - 0s - loss: 8.0504e-04 - val_loss: 7.9893e-06
Epoch 82/512
512/512 - 0s - loss: 7.6971e-04 - val_loss: 7.6917e-06
Epoch 83/512
512/512 - 0s - loss: 7.6405e-04 - val_loss: 7.8314e-06
Epoch 84/512
512/512 - 0s - loss: 7.7147e-04 - val_loss: 7.9485e-06
Epoch 85/512
512/512 - 0s - loss: 7.7658e-04 - val_loss: 7.6782e-06
Epoch 86/512
512/512 - 0s - loss: 7.4915e-04 - val_loss: 7.5561e-06
Epoch 87/512
512/512 - 0s - loss: 7.5091e-04 - val_loss: 7.7230e-06
Epoch 88/512
512/512 - 0s - loss: 7.5973e-04 - val_loss: 7.6155e-06
Epoch 89/512
512/512 - 0s - loss: 7.3869e-04 - val_loss: 7.5685e-06
Epoch 90/512
512/512 - 0s - loss: 7.4133e-04 - val_loss: 7.6061e-06
Epoch 91/512
512/512 - 0s - loss: 7.4266e-04 - val_loss: 7.4868e-06
Epoch 92/512
512/512 - 0s - loss: 7.3264e-04 - val_loss: 7.3225e-06
Epoch 93/512
512/512 - 0s - loss: 7.2742e-04 - val_loss: 7.2885e-06
Epoch 94/512
512/512 - 0s - loss: 7.1848e-04 - val_loss: 7.3548e-06
Epoch 95/512
512/512 - 0s - loss: 7.2894e-04 - val_loss: 7.2712e-06
Epoch 96/512
512/512 - 0s - loss: 7.1720e-04 - val_loss: 7.0802e-06
Epoch 97/512
512/512 - 0s - loss: 6.9650e-04 - val_loss: 7.3176e-06
Epoch 98/512
512/512 - 0s - loss: 7.2475e-04 - val_loss: 7.2929e-06
Epoch 99/512
512/512 - 0s - loss: 7.0827e-04 - val_loss: 6.9520e-06
Epoch 100/512
512/512 - 0s - loss: 6.8785e-04 - val_loss: 6.9845e-06
Epoch 101/512
512/512 - 0s - loss: 6.9966e-04 - val_loss: 7.0593e-06
Epoch 102/512
512/512 - 0s - loss: 6.9382e-04 - val_loss: 7.0317e-06
Epoch 103/512
512/512 - 0s - loss: 6.8533e-04 - val_loss: 7.0742e-06
Epoch 104/512
512/512 - 0s - loss: 6.9239e-04 - val_loss: 6.9766e-06
Epoch 105/512
512/512 - 0s - loss: 6.8562e-04 - val_loss: 6.7991e-06
Epoch 106/512
512/512 - 0s - loss: 6.6688e-04 - val_loss: 6.8952e-06
Epoch 107/512
512/512 - 0s - loss: 6.8130e-04 - val_loss: 6.9124e-06
Epoch 108/512
512/512 - 0s - loss: 6.8253e-04 - val_loss: 6.5614e-06
Epoch 109/512
512/512 - 0s - loss: 6.4434e-04 - val_loss: 6.7476e-06
Epoch 110/512
512/512 - 0s - loss: 6.7572e-04 - val_loss: 6.9055e-06
Epoch 111/512
512/512 - 0s - loss: 6.7404e-04 - val_loss: 6.5048e-06
Epoch 112/512
512/512 - 0s - loss: 6.4567e-04 - val_loss: 6.3473e-06
Epoch 113/512
512/512 - 0s - loss: 6.4462e-04 - val_loss: 6.5951e-06
Epoch 114/512
512/512 - 0s - loss: 6.6396e-04 - val_loss: 6.5538e-06
Epoch 115/512
512/512 - 0s - loss: 6.4310e-04 - val_loss: 6.3824e-06
Epoch 116/512
512/512 - 0s - loss: 6.3570e-04 - val_loss: 6.4930e-06
Epoch 117/512
512/512 - 0s - loss: 6.4447e-04 - val_loss: 6.5156e-06
Epoch 118/512
512/512 - 0s - loss: 6.4128e-04 - val_loss: 6.3911e-06
Epoch 119/512
512/512 - 0s - loss: 6.2769e-04 - val_loss: 6.3339e-06
Epoch 120/512
512/512 - 0s - loss: 6.3013e-04 - val_loss: 6.3420e-06
Epoch 121/512
512/512 - 0s - loss: 6.2819e-04 - val_loss: 6.2763e-06
Epoch 122/512
512/512 - 0s - loss: 6.2540e-04 - val_loss: 6.1726e-06
Epoch 123/512
512/512 - 0s - loss: 6.1182e-04 - val_loss: 6.2197e-06
Epoch 124/512
512/512 - 0s - loss: 6.1876e-04 - val_loss: 6.2423e-06
Epoch 125/512
512/512 - 0s - loss: 6.1912e-04 - val_loss: 6.0423e-06
Epoch 126/512
512/512 - 0s - loss: 6.0009e-04 - val_loss: 6.0479e-06
Epoch 127/512
512/512 - 0s - loss: 6.0245e-04 - val_loss: 6.2317e-06
Epoch 128/512
512/512 - 0s - loss: 6.1707e-04 - val_loss: 6.0578e-06
Epoch 129/512
512/512 - 0s - loss: 5.9234e-04 - val_loss: 5.8752e-06
Epoch 130/512
512/512 - 0s - loss: 5.8914e-04 - val_loss: 6.0069e-06
Epoch 131/512
512/512 - 0s - loss: 5.9955e-04 - val_loss: 5.9487e-06
Epoch 132/512
512/512 - 0s - loss: 5.8536e-04 - val_loss: 5.8714e-06
Epoch 133/512
512/512 - 0s - loss: 5.8563e-04 - val_loss: 5.8858e-06
Epoch 134/512
512/512 - 0s - loss: 5.8215e-04 - val_loss: 5.8488e-06
Epoch 135/512
512/512 - 0s - loss: 5.7895e-04 - val_loss: 5.8003e-06
Epoch 136/512
512/512 - 0s - loss: 5.7535e-04 - val_loss: 5.7466e-06
Epoch 137/512
512/512 - 0s - loss: 5.7244e-04 - val_loss: 5.6554e-06
Epoch 138/512
512/512 - 0s - loss: 5.6427e-04 - val_loss: 5.6792e-06
Epoch 139/512
512/512 - 0s - loss: 5.6636e-04 - val_loss: 5.7043e-06
Epoch 140/512
512/512 - 0s - loss: 5.6115e-04 - val_loss: 5.6596e-06
Epoch 141/512
512/512 - 0s - loss: 5.5971e-04 - val_loss: 5.5688e-06
Epoch 142/512
512/512 - 0s - loss: 5.5490e-04 - val_loss: 5.4494e-06
Epoch 143/512
512/512 - 0s - loss: 5.4467e-04 - val_loss: 5.5152e-06
Epoch 144/512
512/512 - 0s - loss: 5.5218e-04 - val_loss: 5.5257e-06
Epoch 145/512
512/512 - 0s - loss: 5.4467e-04 - val_loss: 5.4123e-06
Epoch 146/512
512/512 - 0s - loss: 5.3882e-04 - val_loss: 5.3392e-06
Epoch 147/512
512/512 - 0s - loss: 5.3276e-04 - val_loss: 5.4065e-06
Epoch 148/512
512/512 - 0s - loss: 5.3632e-04 - val_loss: 5.4420e-06
Epoch 149/512
512/512 - 0s - loss: 5.3799e-04 - val_loss: 5.2348e-06
Epoch 150/512
512/512 - 0s - loss: 5.1618e-04 - val_loss: 5.2135e-06
Epoch 151/512
512/512 - 0s - loss: 5.2625e-04 - val_loss: 5.2482e-06
Epoch 152/512
512/512 - 0s - loss: 5.2159e-04 - val_loss: 5.1582e-06
Epoch 153/512
512/512 - 0s - loss: 5.1257e-04 - val_loss: 5.1141e-06
Epoch 154/512
512/512 - 0s - loss: 5.1225e-04 - val_loss: 5.1182e-06
Epoch 155/512
512/512 - 0s - loss: 5.0888e-04 - val_loss: 5.1442e-06
Epoch 156/512
512/512 - 0s - loss: 5.1130e-04 - val_loss: 4.9950e-06
Epoch 157/512
512/512 - 0s - loss: 4.9608e-04 - val_loss: 4.9088e-06
Epoch 158/512
512/512 - 0s - loss: 4.9416e-04 - val_loss: 5.0179e-06
Epoch 159/512
512/512 - 0s - loss: 5.0088e-04 - val_loss: 5.0069e-06
Epoch 160/512
512/512 - 0s - loss: 4.9593e-04 - val_loss: 4.7708e-06
Epoch 161/512
512/512 - 0s - loss: 4.7673e-04 - val_loss: 4.8031e-06
Epoch 162/512
512/512 - 0s - loss: 4.8852e-04 - val_loss: 4.8483e-06
Epoch 163/512
512/512 - 0s - loss: 4.8211e-04 - val_loss: 4.7748e-06
Epoch 164/512
512/512 - 0s - loss: 4.7175e-04 - val_loss: 4.7923e-06
Epoch 165/512
512/512 - 0s - loss: 4.7852e-04 - val_loss: 4.7054e-06
Epoch 166/512
512/512 - 0s - loss: 4.6986e-04 - val_loss: 4.6071e-06
Epoch 167/512
512/512 - 0s - loss: 4.6121e-04 - val_loss: 4.6407e-06
Epoch 168/512
512/512 - 0s - loss: 4.6358e-04 - val_loss: 4.7048e-06
Epoch 169/512
512/512 - 0s - loss: 4.6028e-04 - val_loss: 4.6597e-06
Epoch 170/512
512/512 - 0s - loss: 4.6058e-04 - val_loss: 4.5024e-06
Epoch 171/512
512/512 - 0s - loss: 4.4718e-04 - val_loss: 4.4535e-06
Epoch 172/512
512/512 - 0s - loss: 4.4842e-04 - val_loss: 4.4665e-06
Epoch 173/512
512/512 - 0s - loss: 4.4666e-04 - val_loss: 4.4251e-06
Epoch 174/512
512/512 - 0s - loss: 4.4169e-04 - val_loss: 4.3711e-06
Epoch 175/512
512/512 - 0s - loss: 4.3840e-04 - val_loss: 4.3137e-06
Epoch 176/512
512/512 - 0s - loss: 4.3345e-04 - val_loss: 4.2850e-06
Epoch 177/512
512/512 - 0s - loss: 4.2659e-04 - val_loss: 4.3389e-06
Epoch 178/512
512/512 - 0s - loss: 4.3660e-04 - val_loss: 4.2078e-06
Epoch 179/512
512/512 - 0s - loss: 4.1754e-04 - val_loss: 4.1296e-06
Epoch 180/512
512/512 - 0s - loss: 4.1762e-04 - val_loss: 4.2327e-06
Epoch 181/512
512/512 - 0s - loss: 4.2129e-04 - val_loss: 4.2133e-06
Epoch 182/512
512/512 - 0s - loss: 4.1330e-04 - val_loss: 4.1372e-06
Epoch 183/512
512/512 - 0s - loss: 4.0976e-04 - val_loss: 4.0788e-06
Epoch 184/512
512/512 - 0s - loss: 4.0758e-04 - val_loss: 4.0224e-06
Epoch 185/512
512/512 - 0s - loss: 4.0168e-04 - val_loss: 3.9996e-06
Epoch 186/512
512/512 - 0s - loss: 3.9857e-04 - val_loss: 4.0052e-06
Epoch 187/512
512/512 - 0s - loss: 4.0099e-04 - val_loss: 3.9216e-06
Epoch 188/512
512/512 - 0s - loss: 3.8937e-04 - val_loss: 3.8601e-06
Epoch 189/512
512/512 - 0s - loss: 3.8778e-04 - val_loss: 3.8933e-06
Epoch 190/512
512/512 - 0s - loss: 3.9027e-04 - val_loss: 3.8277e-06
Epoch 191/512
512/512 - 0s - loss: 3.8193e-04 - val_loss: 3.7632e-06
Epoch 192/512
512/512 - 0s - loss: 3.7407e-04 - val_loss: 3.8234e-06
Epoch 193/512
512/512 - 0s - loss: 3.8166e-04 - val_loss: 3.7902e-06
Epoch 194/512
512/512 - 0s - loss: 3.7517e-04 - val_loss: 3.6279e-06
Epoch 195/512
512/512 - 0s - loss: 3.6434e-04 - val_loss: 3.6039e-06
Epoch 196/512
512/512 - 0s - loss: 3.6595e-04 - val_loss: 3.6773e-06
Epoch 197/512
512/512 - 0s - loss: 3.6438e-04 - val_loss: 3.6525e-06
Epoch 198/512
512/512 - 0s - loss: 3.6289e-04 - val_loss: 3.5393e-06
Epoch 199/512
512/512 - 0s - loss: 3.5602e-04 - val_loss: 3.4354e-06
Epoch 200/512
512/512 - 0s - loss: 3.4981e-04 - val_loss: 3.4389e-06
Epoch 201/512
512/512 - 0s - loss: 3.4700e-04 - val_loss: 3.5393e-06
Epoch 202/512
512/512 - 0s - loss: 3.5361e-04 - val_loss: 3.4715e-06
Epoch 203/512
512/512 - 0s - loss: 3.4393e-04 - val_loss: 3.3320e-06
Epoch 204/512
512/512 - 0s - loss: 3.3481e-04 - val_loss: 3.3799e-06
Epoch 205/512
512/512 - 0s - loss: 3.4215e-04 - val_loss: 3.3614e-06
Epoch 206/512
512/512 - 0s - loss: 3.3311e-04 - val_loss: 3.2792e-06
Epoch 207/512
512/512 - 0s - loss: 3.2860e-04 - val_loss: 3.3043e-06
Epoch 208/512
512/512 - 0s - loss: 3.3201e-04 - val_loss: 3.2629e-06
Epoch 209/512
512/512 - 0s - loss: 3.2232e-04 - val_loss: 3.2503e-06
Epoch 210/512
512/512 - 0s - loss: 3.2538e-04 - val_loss: 3.1840e-06
Epoch 211/512
512/512 - 0s - loss: 3.1807e-04 - val_loss: 3.1212e-06
Epoch 212/512
512/512 - 0s - loss: 3.1314e-04 - val_loss: 3.1560e-06
Epoch 213/512
512/512 - 0s - loss: 3.1744e-04 - val_loss: 3.1265e-06
Epoch 214/512
512/512 - 0s - loss: 3.1212e-04 - val_loss: 3.0135e-06
Epoch 215/512
512/512 - 0s - loss: 3.0310e-04 - val_loss: 3.0130e-06
Epoch 216/512
512/512 - 0s - loss: 3.0460e-04 - val_loss: 3.0715e-06
Epoch 217/512
512/512 - 0s - loss: 3.0516e-04 - val_loss: 2.9990e-06
Epoch 218/512
512/512 - 0s - loss: 2.9725e-04 - val_loss: 2.9308e-06
Epoch 219/512
512/512 - 0s - loss: 2.9596e-04 - val_loss: 2.8932e-06
Epoch 220/512
512/512 - 0s - loss: 2.8937e-04 - val_loss: 2.9716e-06
Epoch 221/512
512/512 - 0s - loss: 2.9468e-04 - val_loss: 2.9552e-06
Epoch 222/512
512/512 - 0s - loss: 2.8956e-04 - val_loss: 2.8241e-06
Epoch 223/512
512/512 - 0s - loss: 2.8063e-04 - val_loss: 2.8267e-06
Epoch 224/512
512/512 - 0s - loss: 2.8461e-04 - val_loss: 2.8251e-06
Epoch 225/512
512/512 - 0s - loss: 2.8317e-04 - val_loss: 2.7006e-06
Epoch 226/512
512/512 - 0s - loss: 2.7084e-04 - val_loss: 2.7221e-06
Epoch 227/512
512/512 - 0s - loss: 2.7595e-04 - val_loss: 2.7429e-06
Epoch 228/512
512/512 - 0s - loss: 2.7412e-04 - val_loss: 2.6684e-06
Epoch 229/512
512/512 - 0s - loss: 2.6673e-04 - val_loss: 2.6617e-06
Epoch 230/512
512/512 - 0s - loss: 2.6623e-04 - val_loss: 2.6792e-06
Epoch 231/512
512/512 - 0s - loss: 2.6832e-04 - val_loss: 2.5863e-06
Epoch 232/512
512/512 - 0s - loss: 2.5731e-04 - val_loss: 2.5527e-06
Epoch 233/512
512/512 - 0s - loss: 2.6081e-04 - val_loss: 2.5498e-06
Epoch 234/512
512/512 - 0s - loss: 2.5509e-04 - val_loss: 2.5408e-06
Epoch 235/512
512/512 - 0s - loss: 2.5357e-04 - val_loss: 2.5407e-06
Epoch 236/512
512/512 - 0s - loss: 2.5462e-04 - val_loss: 2.4664e-06
Epoch 237/512
512/512 - 0s - loss: 2.4648e-04 - val_loss: 2.4507e-06
Epoch 238/512
512/512 - 0s - loss: 2.4638e-04 - val_loss: 2.4684e-06
Epoch 239/512
512/512 - 0s - loss: 2.4721e-04 - val_loss: 2.4129e-06
Epoch 240/512
512/512 - 0s - loss: 2.4048e-04 - val_loss: 2.3622e-06
Epoch 241/512
512/512 - 0s - loss: 2.3786e-04 - val_loss: 2.3737e-06
Epoch 242/512
512/512 - 0s - loss: 2.3849e-04 - val_loss: 2.3717e-06
Epoch 243/512
512/512 - 0s - loss: 2.3635e-04 - val_loss: 2.3198e-06
Epoch 244/512
512/512 - 0s - loss: 2.3171e-04 - val_loss: 2.2882e-06
Epoch 245/512
512/512 - 0s - loss: 2.2983e-04 - val_loss: 2.2861e-06
Epoch 246/512
512/512 - 0s - loss: 2.2856e-04 - val_loss: 2.2689e-06
Epoch 247/512
512/512 - 0s - loss: 2.2630e-04 - val_loss: 2.2416e-06
Epoch 248/512
512/512 - 0s - loss: 2.2176e-04 - val_loss: 2.2411e-06
Epoch 249/512
512/512 - 0s - loss: 2.2460e-04 - val_loss: 2.2013e-06
Epoch 250/512
512/512 - 0s - loss: 2.1844e-04 - val_loss: 2.1583e-06
Epoch 251/512
512/512 - 0s - loss: 2.1760e-04 - val_loss: 2.1240e-06
Epoch 252/512
512/512 - 0s - loss: 2.1273e-04 - val_loss: 2.1369e-06
Epoch 253/512
512/512 - 0s - loss: 2.1650e-04 - val_loss: 2.0968e-06
Epoch 254/512
512/512 - 0s - loss: 2.0709e-04 - val_loss: 2.0912e-06
Epoch 255/512
512/512 - 0s - loss: 2.1062e-04 - val_loss: 2.0990e-06
Epoch 256/512
512/512 - 0s - loss: 2.0857e-04 - val_loss: 2.0202e-06
Epoch 257/512
512/512 - 0s - loss: 2.0310e-04 - val_loss: 1.9748e-06
Epoch 258/512
512/512 - 0s - loss: 1.9784e-04 - val_loss: 2.0594e-06
Epoch 259/512
512/512 - 0s - loss: 2.0762e-04 - val_loss: 2.0235e-06
Epoch 260/512
512/512 - 0s - loss: 1.9676e-04 - val_loss: 1.9317e-06
Epoch 261/512
512/512 - 0s - loss: 1.9386e-04 - val_loss: 1.9447e-06
Epoch 262/512
512/512 - 0s - loss: 1.9765e-04 - val_loss: 1.9302e-06
Epoch 263/512
512/512 - 0s - loss: 1.9149e-04 - val_loss: 1.8889e-06
Epoch 264/512
512/512 - 0s - loss: 1.9151e-04 - val_loss: 1.8363e-06
Epoch 265/512
512/512 - 0s - loss: 1.8509e-04 - val_loss: 1.8787e-06
Epoch 266/512
512/512 - 0s - loss: 1.8841e-04 - val_loss: 1.9147e-06
Epoch 267/512
512/512 - 0s - loss: 1.8953e-04 - val_loss: 1.7864e-06
Epoch 268/512
512/512 - 0s - loss: 1.7732e-04 - val_loss: 1.7620e-06
Epoch 269/512
512/512 - 0s - loss: 1.7927e-04 - val_loss: 1.8614e-06
Epoch 270/512
512/512 - 0s - loss: 1.8513e-04 - val_loss: 1.7922e-06
Epoch 271/512
512/512 - 0s - loss: 1.7500e-04 - val_loss: 1.7177e-06
Epoch 272/512
512/512 - 0s - loss: 1.7375e-04 - val_loss: 1.7429e-06
Epoch 273/512
512/512 - 0s - loss: 1.7506e-04 - val_loss: 1.7373e-06
Epoch 274/512
512/512 - 0s - loss: 1.7306e-04 - val_loss: 1.6838e-06
Epoch 275/512
512/512 - 0s - loss: 1.6827e-04 - val_loss: 1.6488e-06
Epoch 276/512
512/512 - 0s - loss: 1.6543e-04 - val_loss: 1.7100e-06
Epoch 277/512
512/512 - 0s - loss: 1.7123e-04 - val_loss: 1.6688e-06
Epoch 278/512
512/512 - 0s - loss: 1.6306e-04 - val_loss: 1.6107e-06
Epoch 279/512
512/512 - 0s - loss: 1.6301e-04 - val_loss: 1.5950e-06
Epoch 280/512
512/512 - 0s - loss: 1.5988e-04 - val_loss: 1.5958e-06
Epoch 281/512
512/512 - 0s - loss: 1.5908e-04 - val_loss: 1.6074e-06
Epoch 282/512
512/512 - 0s - loss: 1.5972e-04 - val_loss: 1.5480e-06
Epoch 283/512
512/512 - 0s - loss: 1.5460e-04 - val_loss: 1.5200e-06
Epoch 284/512
512/512 - 0s - loss: 1.5220e-04 - val_loss: 1.5443e-06
Epoch 285/512
512/512 - 0s - loss: 1.5420e-04 - val_loss: 1.5337e-06
Epoch 286/512
512/512 - 0s - loss: 1.5157e-04 - val_loss: 1.4885e-06
Epoch 287/512
512/512 - 0s - loss: 1.4689e-04 - val_loss: 1.4821e-06
Epoch 288/512
512/512 - 0s - loss: 1.4855e-04 - val_loss: 1.4559e-06
Epoch 289/512
512/512 - 0s - loss: 1.4619e-04 - val_loss: 1.4121e-06
Epoch 290/512
512/512 - 0s - loss: 1.4208e-04 - val_loss: 1.4156e-06
Epoch 291/512
512/512 - 0s - loss: 1.4339e-04 - val_loss: 1.4076e-06
Epoch 292/512
512/512 - 0s - loss: 1.4053e-04 - val_loss: 1.3839e-06
Epoch 293/512
512/512 - 0s - loss: 1.3830e-04 - val_loss: 1.3667e-06
Epoch 294/512
512/512 - 0s - loss: 1.3802e-04 - val_loss: 1.3498e-06
Epoch 295/512
512/512 - 0s - loss: 1.3553e-04 - val_loss: 1.3280e-06
Epoch 296/512
512/512 - 0s - loss: 1.3324e-04 - val_loss: 1.3238e-06
Epoch 297/512
512/512 - 0s - loss: 1.3257e-04 - val_loss: 1.3204e-06
Epoch 298/512
512/512 - 0s - loss: 1.3138e-04 - val_loss: 1.2974e-06
Epoch 299/512
512/512 - 0s - loss: 1.2839e-04 - val_loss: 1.2889e-06
Epoch 300/512
512/512 - 0s - loss: 1.2832e-04 - val_loss: 1.2802e-06
Epoch 301/512
512/512 - 0s - loss: 1.2676e-04 - val_loss: 1.2441e-06
Epoch 302/512
512/512 - 0s - loss: 1.2394e-04 - val_loss: 1.2258e-06
Epoch 303/512
512/512 - 0s - loss: 1.2207e-04 - val_loss: 1.2258e-06
Epoch 304/512
512/512 - 0s - loss: 1.2192e-04 - val_loss: 1.2207e-06
Epoch 305/512
512/512 - 0s - loss: 1.2108e-04 - val_loss: 1.1731e-06
Epoch 306/512
512/512 - 0s - loss: 1.1622e-04 - val_loss: 1.1765e-06
Epoch 307/512
512/512 - 0s - loss: 1.1842e-04 - val_loss: 1.1639e-06
Epoch 308/512
512/512 - 0s - loss: 1.1555e-04 - val_loss: 1.1107e-06
Epoch 309/512
512/512 - 0s - loss: 1.1103e-04 - val_loss: 1.1330e-06
Epoch 310/512
512/512 - 0s - loss: 1.1434e-04 - val_loss: 1.1248e-06
Epoch 311/512
512/512 - 0s - loss: 1.1215e-04 - val_loss: 1.0478e-06
Epoch 312/512
512/512 - 0s - loss: 1.0552e-04 - val_loss: 1.0668e-06
Epoch 313/512
512/512 - 0s - loss: 1.0851e-04 - val_loss: 1.1000e-06
Epoch 314/512
512/512 - 0s - loss: 1.1028e-04 - val_loss: 1.0082e-06
Epoch 315/512
512/512 - 0s - loss: 9.9770e-05 - val_loss: 1.0080e-06
Epoch 316/512
512/512 - 0s - loss: 1.0335e-04 - val_loss: 1.0621e-06
Epoch 317/512
512/512 - 0s - loss: 1.0585e-04 - val_loss: 1.0035e-06
Epoch 318/512
512/512 - 0s - loss: 9.8446e-05 - val_loss: 9.4928e-07
Epoch 319/512
512/512 - 0s - loss: 9.6207e-05 - val_loss: 1.0014e-06
Epoch 320/512
512/512 - 0s - loss: 1.0133e-04 - val_loss: 9.7665e-07
Epoch 321/512
512/512 - 0s - loss: 9.5156e-05 - val_loss: 9.2822e-07
Epoch 322/512
512/512 - 0s - loss: 9.2743e-05 - val_loss: 9.5064e-07
Epoch 323/512
512/512 - 0s - loss: 9.4835e-05 - val_loss: 9.4231e-07
Epoch 324/512
512/512 - 0s - loss: 9.3120e-05 - val_loss: 8.8620e-07
Epoch 325/512
512/512 - 0s - loss: 8.8603e-05 - val_loss: 8.7333e-07
Epoch 326/512
512/512 - 0s - loss: 8.9654e-05 - val_loss: 8.7960e-07
Epoch 327/512
512/512 - 0s - loss: 8.7358e-05 - val_loss: 8.7618e-07
Epoch 328/512
512/512 - 0s - loss: 8.7049e-05 - val_loss: 8.5983e-07
Epoch 329/512
512/512 - 0s - loss: 8.5632e-05 - val_loss: 8.2932e-07
Epoch 330/512
512/512 - 0s - loss: 8.2962e-05 - val_loss: 8.1005e-07
Epoch 331/512
512/512 - 0s - loss: 8.1921e-05 - val_loss: 8.1775e-07
Epoch 332/512
512/512 - 0s - loss: 8.1515e-05 - val_loss: 8.1502e-07
Epoch 333/512
512/512 - 0s - loss: 8.0718e-05 - val_loss: 7.7549e-07
Epoch 334/512
512/512 - 0s - loss: 7.7460e-05 - val_loss: 7.5963e-07
Epoch 335/512
512/512 - 0s - loss: 7.7208e-05 - val_loss: 7.6240e-07
Epoch 336/512
512/512 - 0s - loss: 7.6108e-05 - val_loss: 7.5192e-07
Epoch 337/512
512/512 - 0s - loss: 7.5255e-05 - val_loss: 7.2701e-07
Epoch 338/512
512/512 - 0s - loss: 7.2091e-05 - val_loss: 7.2328e-07
Epoch 339/512
512/512 - 0s - loss: 7.2960e-05 - val_loss: 7.1987e-07
Epoch 340/512
512/512 - 0s - loss: 7.1787e-05 - val_loss: 6.8597e-07
Epoch 341/512
512/512 - 0s - loss: 6.8500e-05 - val_loss: 6.7422e-07
Epoch 342/512
512/512 - 0s - loss: 6.7966e-05 - val_loss: 6.8941e-07
Epoch 343/512
512/512 - 0s - loss: 6.8452e-05 - val_loss: 6.7084e-07
Epoch 344/512
512/512 - 0s - loss: 6.5962e-05 - val_loss: 6.4512e-07
Epoch 345/512
512/512 - 0s - loss: 6.4400e-05 - val_loss: 6.3877e-07
Epoch 346/512
512/512 - 0s - loss: 6.3938e-05 - val_loss: 6.3196e-07
Epoch 347/512
512/512 - 0s - loss: 6.3048e-05 - val_loss: 6.0705e-07
Epoch 348/512
512/512 - 0s - loss: 6.0630e-05 - val_loss: 6.0069e-07
Epoch 349/512
512/512 - 0s - loss: 6.0691e-05 - val_loss: 5.9529e-07
Epoch 350/512
512/512 - 0s - loss: 5.9055e-05 - val_loss: 5.8497e-07
Epoch 351/512
512/512 - 0s - loss: 5.7969e-05 - val_loss: 5.7736e-07
Epoch 352/512
512/512 - 0s - loss: 5.7524e-05 - val_loss: 5.6011e-07
Epoch 353/512
512/512 - 0s - loss: 5.5513e-05 - val_loss: 5.4810e-07
Epoch 354/512
512/512 - 0s - loss: 5.4784e-05 - val_loss: 5.4382e-07
Epoch 355/512
512/512 - 0s - loss: 5.4255e-05 - val_loss: 5.2351e-07
Epoch 356/512
512/512 - 0s - loss: 5.2297e-05 - val_loss: 5.1322e-07
Epoch 357/512
512/512 - 0s - loss: 5.1517e-05 - val_loss: 5.1274e-07
Epoch 358/512
512/512 - 0s - loss: 5.1146e-05 - val_loss: 4.9765e-07
Epoch 359/512
512/512 - 0s - loss: 4.9359e-05 - val_loss: 4.8503e-07
Epoch 360/512
512/512 - 0s - loss: 4.8398e-05 - val_loss: 4.8345e-07
Epoch 361/512
512/512 - 0s - loss: 4.8562e-05 - val_loss: 4.6150e-07
Epoch 362/512
512/512 - 0s - loss: 4.5814e-05 - val_loss: 4.5170e-07
Epoch 363/512
512/512 - 0s - loss: 4.5284e-05 - val_loss: 4.6493e-07
Epoch 364/512
512/512 - 0s - loss: 4.6184e-05 - val_loss: 4.4423e-07
Epoch 365/512
512/512 - 0s - loss: 4.3483e-05 - val_loss: 4.1781e-07
Epoch 366/512
512/512 - 0s - loss: 4.2675e-05 - val_loss: 4.1243e-07
Epoch 367/512
512/512 - 0s - loss: 4.1752e-05 - val_loss: 4.1291e-07
Epoch 368/512
512/512 - 0s - loss: 4.1271e-05 - val_loss: 4.1219e-07
Epoch 369/512
512/512 - 0s - loss: 4.0609e-05 - val_loss: 3.9727e-07
Epoch 370/512
512/512 - 0s - loss: 3.9320e-05 - val_loss: 3.8172e-07
Epoch 371/512
512/512 - 0s - loss: 3.8619e-05 - val_loss: 3.6779e-07
Epoch 372/512
512/512 - 0s - loss: 3.6730e-05 - val_loss: 3.7365e-07
Epoch 373/512
512/512 - 0s - loss: 3.7374e-05 - val_loss: 3.7561e-07
Epoch 374/512
512/512 - 0s - loss: 3.6889e-05 - val_loss: 3.5055e-07
Epoch 375/512
512/512 - 0s - loss: 3.4542e-05 - val_loss: 3.3534e-07
Epoch 376/512
512/512 - 0s - loss: 3.4012e-05 - val_loss: 3.3983e-07
Epoch 377/512
512/512 - 0s - loss: 3.4194e-05 - val_loss: 3.3500e-07
Epoch 378/512
512/512 - 0s - loss: 3.2839e-05 - val_loss: 3.2267e-07
Epoch 379/512
512/512 - 0s - loss: 3.2211e-05 - val_loss: 3.1215e-07
Epoch 380/512
512/512 - 0s - loss: 3.1284e-05 - val_loss: 3.0463e-07
Epoch 381/512
512/512 - 0s - loss: 3.0545e-05 - val_loss: 2.9892e-07
Epoch 382/512
512/512 - 0s - loss: 3.0058e-05 - val_loss: 2.9184e-07
Epoch 383/512
512/512 - 0s - loss: 2.9255e-05 - val_loss: 2.8453e-07
Epoch 384/512
512/512 - 0s - loss: 2.8513e-05 - val_loss: 2.7782e-07
Epoch 385/512
512/512 - 0s - loss: 2.7782e-05 - val_loss: 2.7312e-07
Epoch 386/512
512/512 - 0s - loss: 2.7331e-05 - val_loss: 2.6638e-07
Epoch 387/512
512/512 - 0s - loss: 2.6412e-05 - val_loss: 2.5950e-07
Epoch 388/512
512/512 - 0s - loss: 2.5978e-05 - val_loss: 2.5469e-07
Epoch 389/512
512/512 - 0s - loss: 2.5508e-05 - val_loss: 2.4367e-07
Epoch 390/512
512/512 - 0s - loss: 2.4388e-05 - val_loss: 2.3758e-07
Epoch 391/512
512/512 - 0s - loss: 2.4118e-05 - val_loss: 2.3475e-07
Epoch 392/512
512/512 - 0s - loss: 2.3411e-05 - val_loss: 2.3093e-07
Epoch 393/512
512/512 - 0s - loss: 2.2977e-05 - val_loss: 2.2237e-07
Epoch 394/512
512/512 - 0s - loss: 2.2101e-05 - val_loss: 2.1644e-07
Epoch 395/512
512/512 - 0s - loss: 2.1745e-05 - val_loss: 2.1341e-07
Epoch 396/512
512/512 - 0s - loss: 2.1343e-05 - val_loss: 2.0614e-07
Epoch 397/512
512/512 - 0s - loss: 2.0435e-05 - val_loss: 2.0175e-07
Epoch 398/512
512/512 - 0s - loss: 2.0210e-05 - val_loss: 1.9702e-07
Epoch 399/512
512/512 - 0s - loss: 1.9556e-05 - val_loss: 1.9111e-07
Epoch 400/512
512/512 - 0s - loss: 1.9149e-05 - val_loss: 1.8711e-07
Epoch 401/512
512/512 - 0s - loss: 1.8649e-05 - val_loss: 1.7973e-07
Epoch 402/512
512/512 - 0s - loss: 1.8016e-05 - val_loss: 1.7483e-07
Epoch 403/512
512/512 - 0s - loss: 1.7502e-05 - val_loss: 1.7338e-07
Epoch 404/512
512/512 - 0s - loss: 1.7333e-05 - val_loss: 1.6915e-07
Epoch 405/512
512/512 - 0s - loss: 1.6829e-05 - val_loss: 1.5948e-07
Epoch 406/512
512/512 - 0s - loss: 1.5962e-05 - val_loss: 1.5701e-07
Epoch 407/512
512/512 - 0s - loss: 1.5814e-05 - val_loss: 1.5814e-07
Epoch 408/512
512/512 - 0s - loss: 1.5621e-05 - val_loss: 1.5370e-07
Epoch 409/512
512/512 - 0s - loss: 1.5099e-05 - val_loss: 1.4263e-07
Epoch 410/512
512/512 - 0s - loss: 1.4186e-05 - val_loss: 1.4095e-07
Epoch 411/512
512/512 - 0s - loss: 1.4225e-05 - val_loss: 1.4281e-07
Epoch 412/512
512/512 - 0s - loss: 1.4178e-05 - val_loss: 1.3449e-07
Epoch 413/512
512/512 - 0s - loss: 1.3193e-05 - val_loss: 1.2702e-07
Epoch 414/512
512/512 - 0s - loss: 1.2727e-05 - val_loss: 1.3006e-07
Epoch 415/512
512/512 - 0s - loss: 1.3318e-05 - val_loss: 1.2260e-07
Epoch 416/512
512/512 - 0s - loss: 1.2001e-05 - val_loss: 1.1412e-07
Epoch 417/512
512/512 - 0s - loss: 1.1535e-05 - val_loss: 1.2207e-07
Epoch 418/512
512/512 - 0s - loss: 1.2272e-05 - val_loss: 1.1705e-07
Epoch 419/512
512/512 - 0s - loss: 1.1304e-05 - val_loss: 1.0494e-07
Epoch 420/512
512/512 - 0s - loss: 1.0574e-05 - val_loss: 1.0612e-07
Epoch 421/512
512/512 - 0s - loss: 1.0771e-05 - val_loss: 1.0986e-07
Epoch 422/512
512/512 - 0s - loss: 1.0751e-05 - val_loss: 1.0090e-07
Epoch 423/512
512/512 - 0s - loss: 9.7799e-06 - val_loss: 9.5690e-08
Epoch 424/512
512/512 - 0s - loss: 9.6508e-06 - val_loss: 9.9344e-08
Epoch 425/512
512/512 - 0s - loss: 9.8822e-06 - val_loss: 9.4054e-08
Epoch 426/512
512/512 - 0s - loss: 9.2216e-06 - val_loss: 8.5711e-08
Epoch 427/512
512/512 - 0s - loss: 8.7078e-06 - val_loss: 8.5450e-08
Epoch 428/512
512/512 - 0s - loss: 8.6149e-06 - val_loss: 8.8945e-08
Epoch 429/512
512/512 - 0s - loss: 8.9320e-06 - val_loss: 8.1325e-08
Epoch 430/512
512/512 - 0s - loss: 7.9294e-06 - val_loss: 7.4704e-08
Epoch 431/512
512/512 - 0s - loss: 7.6789e-06 - val_loss: 7.9252e-08
Epoch 432/512
512/512 - 0s - loss: 8.0717e-06 - val_loss: 7.8381e-08
Epoch 433/512
512/512 - 0s - loss: 7.5913e-06 - val_loss: 6.9264e-08
Epoch 434/512
512/512 - 0s - loss: 6.9443e-06 - val_loss: 6.8368e-08
Epoch 435/512
512/512 - 0s - loss: 7.0336e-06 - val_loss: 7.1781e-08
Epoch 436/512
512/512 - 0s - loss: 7.1530e-06 - val_loss: 6.6904e-08
Epoch 437/512
512/512 - 0s - loss: 6.4684e-06 - val_loss: 6.2422e-08
Epoch 438/512
512/512 - 0s - loss: 6.2552e-06 - val_loss: 6.5416e-08
Epoch 439/512
512/512 - 0s - loss: 6.5098e-06 - val_loss: 6.2755e-08
Epoch 440/512
512/512 - 0s - loss: 6.0885e-06 - val_loss: 5.5714e-08
Epoch 441/512
512/512 - 0s - loss: 5.6342e-06 - val_loss: 5.5376e-08
Epoch 442/512
512/512 - 0s - loss: 5.6754e-06 - val_loss: 5.8283e-08
Epoch 443/512
512/512 - 0s - loss: 5.8109e-06 - val_loss: 5.2936e-08
Epoch 444/512
512/512 - 0s - loss: 5.1649e-06 - val_loss: 4.8799e-08
Epoch 445/512
512/512 - 0s - loss: 5.0497e-06 - val_loss: 5.0844e-08
Epoch 446/512
512/512 - 0s - loss: 5.2240e-06 - val_loss: 4.8850e-08
Epoch 447/512
512/512 - 0s - loss: 4.7651e-06 - val_loss: 4.6052e-08
Epoch 448/512
512/512 - 0s - loss: 4.6736e-06 - val_loss: 4.6196e-08
Epoch 449/512
512/512 - 0s - loss: 4.6336e-06 - val_loss: 4.4512e-08
Epoch 450/512
512/512 - 0s - loss: 4.4257e-06 - val_loss: 4.1835e-08
Epoch 451/512
512/512 - 0s - loss: 4.2373e-06 - val_loss: 4.0752e-08
Epoch 452/512
512/512 - 0s - loss: 4.1296e-06 - val_loss: 4.0518e-08
Epoch 453/512
512/512 - 0s - loss: 4.0821e-06 - val_loss: 3.8830e-08
Epoch 454/512
512/512 - 0s - loss: 3.8512e-06 - val_loss: 3.7300e-08
Epoch 455/512
512/512 - 0s - loss: 3.7673e-06 - val_loss: 3.6392e-08
Epoch 456/512
512/512 - 0s - loss: 3.6462e-06 - val_loss: 3.5242e-08
Epoch 457/512
512/512 - 0s - loss: 3.5676e-06 - val_loss: 3.3471e-08
Epoch 458/512
512/512 - 0s - loss: 3.3904e-06 - val_loss: 3.2080e-08
Epoch 459/512
512/512 - 0s - loss: 3.2577e-06 - val_loss: 3.2091e-08
Epoch 460/512
512/512 - 0s - loss: 3.2523e-06 - val_loss: 3.1202e-08
Epoch 461/512
512/512 - 0s - loss: 3.0748e-06 - val_loss: 2.9964e-08
Epoch 462/512
512/512 - 0s - loss: 3.0084e-06 - val_loss: 2.9327e-08
Epoch 463/512
512/512 - 0s - loss: 2.8945e-06 - val_loss: 2.8620e-08
Epoch 464/512
512/512 - 0s - loss: 2.8602e-06 - val_loss: 2.6709e-08
Epoch 465/512
512/512 - 0s - loss: 2.6553e-06 - val_loss: 2.5887e-08
Epoch 466/512
512/512 - 0s - loss: 2.6648e-06 - val_loss: 2.4941e-08
Epoch 467/512
512/512 - 0s - loss: 2.5148e-06 - val_loss: 2.4285e-08
Epoch 468/512
512/512 - 0s - loss: 2.4612e-06 - val_loss: 2.3800e-08
Epoch 469/512
512/512 - 0s - loss: 2.3695e-06 - val_loss: 2.3376e-08
Epoch 470/512
512/512 - 0s - loss: 2.3423e-06 - val_loss: 2.2050e-08
Epoch 471/512
512/512 - 0s - loss: 2.2031e-06 - val_loss: 2.0865e-08
Epoch 472/512
512/512 - 0s - loss: 2.0991e-06 - val_loss: 2.1429e-08
Epoch 473/512
512/512 - 0s - loss: 2.1544e-06 - val_loss: 2.0508e-08
Epoch 474/512
512/512 - 0s - loss: 1.9881e-06 - val_loss: 1.9133e-08
Epoch 475/512
512/512 - 0s - loss: 1.9297e-06 - val_loss: 1.8732e-08
Epoch 476/512
512/512 - 0s - loss: 1.8797e-06 - val_loss: 1.8477e-08
Epoch 477/512
512/512 - 0s - loss: 1.8375e-06 - val_loss: 1.7572e-08
Epoch 478/512
512/512 - 0s - loss: 1.7554e-06 - val_loss: 1.6670e-08
Epoch 479/512
512/512 - 0s - loss: 1.6706e-06 - val_loss: 1.6441e-08
Epoch 480/512
512/512 - 0s - loss: 1.6620e-06 - val_loss: 1.5988e-08
Epoch 481/512
512/512 - 0s - loss: 1.5928e-06 - val_loss: 1.5091e-08
Epoch 482/512
512/512 - 0s - loss: 1.5084e-06 - val_loss: 1.4707e-08
Epoch 483/512
512/512 - 0s - loss: 1.4643e-06 - val_loss: 1.4964e-08
Epoch 484/512
512/512 - 0s - loss: 1.4813e-06 - val_loss: 1.4072e-08
Epoch 485/512
512/512 - 0s - loss: 1.3689e-06 - val_loss: 1.2960e-08
Epoch 486/512
512/512 - 0s - loss: 1.2982e-06 - val_loss: 1.3110e-08
Epoch 487/512
512/512 - 0s - loss: 1.3281e-06 - val_loss: 1.2693e-08
Epoch 488/512
512/512 - 0s - loss: 1.2695e-06 - val_loss: 1.1415e-08
Epoch 489/512
512/512 - 0s - loss: 1.1416e-06 - val_loss: 1.1362e-08
Epoch 490/512
512/512 - 0s - loss: 1.1837e-06 - val_loss: 1.1584e-08
Epoch 491/512
512/512 - 0s - loss: 1.1556e-06 - val_loss: 1.0549e-08
Epoch 492/512
512/512 - 0s - loss: 1.0599e-06 - val_loss: 9.8601e-09
Epoch 493/512
512/512 - 0s - loss: 1.0162e-06 - val_loss: 1.0309e-08
Epoch 494/512
512/512 - 0s - loss: 1.0548e-06 - val_loss: 9.9702e-09
Epoch 495/512
512/512 - 0s - loss: 9.6892e-07 - val_loss: 9.1535e-09
Epoch 496/512
512/512 - 0s - loss: 9.1870e-07 - val_loss: 9.1657e-09
Epoch 497/512
512/512 - 0s - loss: 9.3588e-07 - val_loss: 8.8548e-09
Epoch 498/512
512/512 - 0s - loss: 8.7004e-07 - val_loss: 8.3228e-09
Epoch 499/512
512/512 - 0s - loss: 8.4371e-07 - val_loss: 8.1651e-09
Epoch 500/512
512/512 - 0s - loss: 8.2491e-07 - val_loss: 7.8575e-09
Epoch 501/512
512/512 - 0s - loss: 7.8694e-07 - val_loss: 7.4639e-09
Epoch 502/512
512/512 - 0s - loss: 7.6030e-07 - val_loss: 7.2559e-09
Epoch 503/512
512/512 - 0s - loss: 7.3736e-07 - val_loss: 7.0977e-09
Epoch 504/512
512/512 - 0s - loss: 7.1580e-07 - val_loss: 6.7443e-09
Epoch 505/512
512/512 - 0s - loss: 6.7634e-07 - val_loss: 6.4745e-09
Epoch 506/512
512/512 - 0s - loss: 6.5479e-07 - val_loss: 6.5658e-09
Epoch 507/512
512/512 - 0s - loss: 6.6194e-07 - val_loss: 6.0926e-09
Epoch 508/512
512/512 - 0s - loss: 6.0180e-07 - val_loss: 5.7230e-09
Epoch 509/512
512/512 - 0s - loss: 5.8567e-07 - val_loss: 5.7926e-09
Epoch 510/512
512/512 - 0s - loss: 5.8024e-07 - val_loss: 5.6437e-09
Epoch 511/512
512/512 - 0s - loss: 5.6098e-07 - val_loss: 5.2441e-09
Epoch 512/512
512/512 - 0s - loss: 5.2855e-07 - val_loss: 4.9213e-09
2024-04-11 21:19:38.517697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2514e-07 - val_loss: 3.9859e-07
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4819e-07 - val_loss: 5.2083e-07
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4149e-07 - val_loss: 4.8574e-07
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4397e-07 - val_loss: 3.7437e-07
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8426e-07 - val_loss: 4.0759e-07
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4224e-07 - val_loss: 4.4793e-07
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3153e-07 - val_loss: 3.7117e-07
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6063e-07 - val_loss: 3.4569e-07
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6668e-07 - val_loss: 3.8067e-07
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8625e-07 - val_loss: 3.5773e-07
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4582e-07 - val_loss: 3.1409e-07
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1938e-07 - val_loss: 3.2135e-07
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3375e-07 - val_loss: 3.2510e-07
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2063e-07 - val_loss: 2.9265e-07
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9115e-07 - val_loss: 2.7936e-07
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8790e-07 - val_loss: 2.8473e-07
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8778e-07 - val_loss: 2.7067e-07
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6787e-07 - val_loss: 2.5081e-07
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5442e-07 - val_loss: 2.4856e-07
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5372e-07 - val_loss: 2.4391e-07
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4353e-07 - val_loss: 2.2791e-07
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2881e-07 - val_loss: 2.1887e-07
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2299e-07 - val_loss: 2.1622e-07
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1835e-07 - val_loss: 2.0668e-07
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0698e-07 - val_loss: 1.9601e-07
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9838e-07 - val_loss: 1.9131e-07
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9370e-07 - val_loss: 1.8544e-07
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8629e-07 - val_loss: 1.7655e-07
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7772e-07 - val_loss: 1.6963e-07
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7193e-07 - val_loss: 1.6510e-07
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6651e-07 - val_loss: 1.5836e-07
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5936e-07 - val_loss: 1.5116e-07
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5283e-07 - val_loss: 1.4689e-07
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4876e-07 - val_loss: 1.4175e-07
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4265e-07 - val_loss: 1.3515e-07
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3661e-07 - val_loss: 1.3018e-07
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3175e-07 - val_loss: 1.2658e-07
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2794e-07 - val_loss: 1.2110e-07
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2205e-07 - val_loss: 1.1592e-07
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1734e-07 - val_loss: 1.1237e-07
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1378e-07 - val_loss: 1.0847e-07
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0927e-07 - val_loss: 1.0348e-07
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0455e-07 - val_loss: 9.9943e-08
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0133e-07 - val_loss: 9.6412e-08
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7208e-08 - val_loss: 9.2613e-08
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3634e-08 - val_loss: 8.8981e-08
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9975e-08 - val_loss: 8.5611e-08
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6658e-08 - val_loss: 8.2572e-08
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3438e-08 - val_loss: 7.9392e-08
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0220e-08 - val_loss: 7.6443e-08
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7223e-08 - val_loss: 7.3419e-08
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4264e-08 - val_loss: 7.0445e-08
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.1271e-08 - val_loss: 6.7814e-08
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8788e-08 - val_loss: 6.5457e-08
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6192e-08 - val_loss: 6.2799e-08
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3516e-08 - val_loss: 6.0399e-08
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1031e-08 - val_loss: 5.8213e-08
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8968e-08 - val_loss: 5.5967e-08
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6608e-08 - val_loss: 5.3701e-08
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4304e-08 - val_loss: 5.1668e-08
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2366e-08 - val_loss: 4.9899e-08
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0395e-08 - val_loss: 4.7815e-08
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8340e-08 - val_loss: 4.5913e-08
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6511e-08 - val_loss: 4.4340e-08
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4803e-08 - val_loss: 4.2535e-08
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3029e-08 - val_loss: 4.0793e-08
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1353e-08 - val_loss: 3.9398e-08
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9885e-08 - val_loss: 3.7746e-08
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8180e-08 - val_loss: 3.6308e-08
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6820e-08 - val_loss: 3.4970e-08
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5395e-08 - val_loss: 3.3586e-08
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3991e-08 - val_loss: 3.2260e-08
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2629e-08 - val_loss: 3.1092e-08
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1493e-08 - val_loss: 2.9931e-08
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0203e-08 - val_loss: 2.8663e-08
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9005e-08 - val_loss: 2.7654e-08
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8023e-08 - val_loss: 2.6576e-08
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6837e-08 - val_loss: 2.5416e-08
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5721e-08 - val_loss: 2.4563e-08
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4952e-08 - val_loss: 2.3695e-08
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3888e-08 - val_loss: 2.2660e-08
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2889e-08 - val_loss: 2.1815e-08
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2071e-08 - val_loss: 2.0997e-08
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1228e-08 - val_loss: 2.0172e-08
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0412e-08 - val_loss: 1.9364e-08
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9549e-08 - val_loss: 1.8651e-08
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8903e-08 - val_loss: 1.7937e-08
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8165e-08 - val_loss: 1.7276e-08
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7441e-08 - val_loss: 1.6536e-08
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6728e-08 - val_loss: 1.5949e-08
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6181e-08 - val_loss: 1.5386e-08
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5506e-08 - val_loss: 1.4773e-08
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4884e-08 - val_loss: 1.4148e-08
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4362e-08 - val_loss: 1.3714e-08
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3819e-08 - val_loss: 1.3155e-08
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3297e-08 - val_loss: 1.2663e-08
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2789e-08 - val_loss: 1.2178e-08
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2308e-08 - val_loss: 1.1665e-08
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1792e-08 - val_loss: 1.1241e-08
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1381e-08 - val_loss: 1.0889e-08
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0990e-08 - val_loss: 1.0453e-08
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0573e-08 - val_loss: 1.0047e-08
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0146e-08 - val_loss: 9.6487e-09
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7168e-09 - val_loss: 9.2907e-09
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4428e-09 - val_loss: 9.0560e-09
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1432e-09 - val_loss: 8.6539e-09
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.7304e-09 - val_loss: 8.2607e-09
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3241e-09 - val_loss: 7.9593e-09
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1076e-09 - val_loss: 7.7923e-09
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.8567e-09 - val_loss: 7.4645e-09
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.5239e-09 - val_loss: 7.1814e-09
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2375e-09 - val_loss: 6.8740e-09
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9484e-09 - val_loss: 6.6945e-09
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.7360e-09 - val_loss: 6.4449e-09
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5131e-09 - val_loss: 6.2692e-09
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2997e-09 - val_loss: 5.9778e-09
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0218e-09 - val_loss: 5.7563e-09
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8106e-09 - val_loss: 5.5495e-09
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6033e-09 - val_loss: 5.3955e-09
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4552e-09 - val_loss: 5.2296e-09
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2457e-09 - val_loss: 4.9847e-09
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0266e-09 - val_loss: 4.8106e-09
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8602e-09 - val_loss: 4.6898e-09
Epoch 124/512

Epoch 00124: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7342e-09 - val_loss: 4.5528e-09
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6034e-09 - val_loss: 4.3959e-09
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3948e-09 - val_loss: 4.1666e-09
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1952e-09 - val_loss: 4.0452e-09
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0849e-09 - val_loss: 3.9684e-09
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0177e-09 - val_loss: 3.8447e-09
Epoch 130/512

Epoch 00130: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8573e-09 - val_loss: 3.6905e-09
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6928e-09 - val_loss: 3.5325e-09
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5607e-09 - val_loss: 3.4364e-09
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4824e-09 - val_loss: 3.3930e-09
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4053e-09 - val_loss: 3.2519e-09
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2559e-09 - val_loss: 3.1113e-09
Epoch 136/512

Epoch 00136: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1321e-09 - val_loss: 3.0336e-09
Epoch 137/512

Epoch 00137: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0770e-09 - val_loss: 2.9568e-09
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9596e-09 - val_loss: 2.8495e-09
Epoch 139/512

Epoch 00139: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8719e-09 - val_loss: 2.7657e-09
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7904e-09 - val_loss: 2.6812e-09
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6992e-09 - val_loss: 2.6095e-09
Epoch 142/512

Epoch 00142: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6263e-09 - val_loss: 2.5187e-09
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5357e-09 - val_loss: 2.4414e-09
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4433e-09 - val_loss: 2.3549e-09
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3856e-09 - val_loss: 2.3183e-09
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3231e-09 - val_loss: 2.2480e-09
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2675e-09 - val_loss: 2.1910e-09
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1881e-09 - val_loss: 2.1025e-09
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1161e-09 - val_loss: 2.0484e-09
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0641e-09 - val_loss: 2.0088e-09
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0225e-09 - val_loss: 1.9397e-09
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9459e-09 - val_loss: 1.8957e-09
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9170e-09 - val_loss: 1.8595e-09
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8573e-09 - val_loss: 1.7814e-09
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7984e-09 - val_loss: 1.7402e-09
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7406e-09 - val_loss: 1.6776e-09
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6765e-09 - val_loss: 1.6307e-09
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6464e-09 - val_loss: 1.6133e-09
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6395e-09 - val_loss: 1.6031e-09
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6033e-09 - val_loss: 1.5437e-09
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5345e-09 - val_loss: 1.4726e-09
Epoch 162/512

Epoch 00162: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4730e-09 - val_loss: 1.4342e-09
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4412e-09 - val_loss: 1.4110e-09
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4274e-09 - val_loss: 1.4005e-09
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4095e-09 - val_loss: 1.3551e-09
Epoch 166/512

Epoch 00166: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3485e-09 - val_loss: 1.3017e-09
Epoch 167/512

Epoch 00167: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3056e-09 - val_loss: 1.2605e-09
Epoch 168/512

Epoch 00168: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2661e-09 - val_loss: 1.2443e-09
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2670e-09 - val_loss: 1.2535e-09
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2471e-09 - val_loss: 1.1931e-09
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1853e-09 - val_loss: 1.1533e-09
Epoch 172/512

Epoch 00172: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1535e-09 - val_loss: 1.1239e-09
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1365e-09 - val_loss: 1.1289e-09
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1418e-09 - val_loss: 1.1159e-09
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1131e-09 - val_loss: 1.0762e-09
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0735e-09 - val_loss: 1.0339e-09
Epoch 177/512

Epoch 00177: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0321e-09 - val_loss: 9.9899e-10
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0066e-09 - val_loss: 1.0006e-09
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0092e-09 - val_loss: 9.8703e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8776e-10 - val_loss: 9.6529e-10
Epoch 181/512

Epoch 00181: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.7523e-10 - val_loss: 9.5022e-10
Epoch 182/512

Epoch 00182: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4506e-10 - val_loss: 9.1004e-10
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1397e-10 - val_loss: 8.8829e-10
Epoch 184/512

Epoch 00184: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9304e-10 - val_loss: 8.7851e-10
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.8052e-10 - val_loss: 8.6291e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6356e-10 - val_loss: 8.5026e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.5038e-10 - val_loss: 8.3129e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.3732e-10 - val_loss: 8.1152e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.1166e-10 - val_loss: 8.0021e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.0274e-10 - val_loss: 7.8876e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9031e-10 - val_loss: 7.7084e-10
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.7071e-10 - val_loss: 7.5881e-10
Epoch 193/512

Epoch 00193: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.6107e-10 - val_loss: 7.4460e-10
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4665e-10 - val_loss: 7.3442e-10
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.3462e-10 - val_loss: 7.1140e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0447e-10 - val_loss: 6.8530e-10
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9569e-10 - val_loss: 6.9424e-10
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.0004e-10 - val_loss: 6.8445e-10
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.8083e-10 - val_loss: 6.6249e-10
Epoch 200/512

Epoch 00200: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6768e-10 - val_loss: 6.6208e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.6595e-10 - val_loss: 6.4876e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.4147e-10 - val_loss: 6.2483e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2803e-10 - val_loss: 6.2736e-10
Epoch 204/512

Epoch 00204: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2736e-10 - val_loss: 6.1623e-10
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2008e-10 - val_loss: 6.0755e-10
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0671e-10 - val_loss: 5.9227e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.9077e-10 - val_loss: 5.8014e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8550e-10 - val_loss: 5.8559e-10
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8491e-10 - val_loss: 5.6565e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6162e-10 - val_loss: 5.5297e-10
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5828e-10 - val_loss: 5.5471e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5152e-10 - val_loss: 5.3615e-10
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3775e-10 - val_loss: 5.3651e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4326e-10 - val_loss: 5.3363e-10
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.2991e-10 - val_loss: 5.1398e-10
Epoch 216/512

Epoch 00216: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1165e-10 - val_loss: 5.0916e-10
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1187e-10 - val_loss: 5.0289e-10
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.0400e-10 - val_loss: 4.9634e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0044e-10 - val_loss: 4.9717e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.9999e-10 - val_loss: 4.8941e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8409e-10 - val_loss: 4.7350e-10
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7724e-10 - val_loss: 4.7649e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.7626e-10 - val_loss: 4.6435e-10
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.6296e-10 - val_loss: 4.5194e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5199e-10 - val_loss: 4.5525e-10
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.5862e-10 - val_loss: 4.5193e-10
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4865e-10 - val_loss: 4.3935e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4043e-10 - val_loss: 4.3999e-10
Epoch 229/512

Epoch 00229: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3961e-10 - val_loss: 4.3502e-10
Epoch 230/512

Epoch 00230: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3292e-10 - val_loss: 4.2434e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2478e-10 - val_loss: 4.1939e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1771e-10 - val_loss: 4.1104e-10
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1333e-10 - val_loss: 4.1150e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1326e-10 - val_loss: 4.0851e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0688e-10 - val_loss: 3.9611e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9551e-10 - val_loss: 3.9340e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9752e-10 - val_loss: 3.9925e-10
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.0002e-10 - val_loss: 3.9300e-10
Epoch 239/512

Epoch 00239: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9048e-10 - val_loss: 3.8004e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8347e-10 - val_loss: 3.8160e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.8278e-10 - val_loss: 3.7859e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.7730e-10 - val_loss: 3.6590e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6413e-10 - val_loss: 3.5612e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5962e-10 - val_loss: 3.6178e-10
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6579e-10 - val_loss: 3.6797e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6655e-10 - val_loss: 3.5965e-10
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5870e-10 - val_loss: 3.5290e-10
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.5262e-10 - val_loss: 3.5149e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5468e-10 - val_loss: 3.5186e-10
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4827e-10 - val_loss: 3.3777e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3487e-10 - val_loss: 3.2930e-10
Epoch 252/512

Epoch 00252: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2994e-10 - val_loss: 3.2651e-10
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2955e-10 - val_loss: 3.3052e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3074e-10 - val_loss: 3.2897e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2833e-10 - val_loss: 3.2887e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2971e-10 - val_loss: 3.2823e-10
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2894e-10 - val_loss: 3.2449e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2336e-10 - val_loss: 3.1622e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1023e-10 - val_loss: 3.0383e-10
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0638e-10 - val_loss: 3.0841e-10
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1002e-10 - val_loss: 3.0838e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0667e-10 - val_loss: 3.0329e-10
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0311e-10 - val_loss: 3.0138e-10
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0182e-10 - val_loss: 3.0431e-10
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0599e-10 - val_loss: 3.0307e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0374e-10 - val_loss: 2.9836e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9768e-10 - val_loss: 2.9433e-10
Epoch 268/512

Epoch 00268: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9259e-10 - val_loss: 2.8919e-10
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8909e-10 - val_loss: 2.8547e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8771e-10 - val_loss: 2.8812e-10
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8615e-10 - val_loss: 2.7566e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7509e-10 - val_loss: 2.7756e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8061e-10 - val_loss: 2.7979e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7845e-10 - val_loss: 2.7659e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7571e-10 - val_loss: 2.7448e-10
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7474e-10 - val_loss: 2.7546e-10
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7906e-10 - val_loss: 2.7776e-10
Epoch 278/512

Epoch 00278: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7808e-10 - val_loss: 2.7232e-10
Epoch 279/512

Epoch 00279: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7190e-10 - val_loss: 2.6689e-10
Epoch 280/512

Epoch 00280: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6612e-10 - val_loss: 2.6524e-10
Epoch 281/512

Epoch 00281: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6509e-10 - val_loss: 2.5987e-10
Epoch 282/512

Epoch 00282: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5831e-10 - val_loss: 2.5680e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5870e-10 - val_loss: 2.5916e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5991e-10 - val_loss: 2.6195e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6278e-10 - val_loss: 2.6462e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6491e-10 - val_loss: 2.6214e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6169e-10 - val_loss: 2.5706e-10
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4975e-10 - val_loss: 2.4117e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4381e-10 - val_loss: 2.4607e-10
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4777e-10 - val_loss: 2.4847e-10
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4800e-10 - val_loss: 2.4127e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4096e-10 - val_loss: 2.4005e-10
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4418e-10 - val_loss: 2.4858e-10
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4903e-10 - val_loss: 2.4592e-10
Epoch 295/512

Epoch 00295: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4054e-10 - val_loss: 2.3404e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3408e-10 - val_loss: 2.3502e-10
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3897e-10 - val_loss: 2.4229e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4357e-10 - val_loss: 2.3711e-10
Epoch 299/512

Epoch 00299: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3374e-10 - val_loss: 2.3236e-10
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3418e-10 - val_loss: 2.3450e-10
Epoch 301/512

Epoch 00301: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3469e-10 - val_loss: 2.3105e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2812e-10 - val_loss: 2.2240e-10
Epoch 303/512

Epoch 00303: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2111e-10 - val_loss: 2.2039e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2227e-10 - val_loss: 2.2384e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2234e-10 - val_loss: 2.1969e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2015e-10 - val_loss: 2.2492e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2748e-10 - val_loss: 2.2870e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2720e-10 - val_loss: 2.2167e-10
Epoch 309/512

Epoch 00309: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1900e-10 - val_loss: 2.1574e-10
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1844e-10 - val_loss: 2.2242e-10
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2404e-10 - val_loss: 2.2443e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2090e-10 - val_loss: 2.1333e-10
Epoch 313/512

Epoch 00313: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1201e-10 - val_loss: 2.1199e-10
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1287e-10 - val_loss: 2.1430e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1761e-10 - val_loss: 2.1545e-10
Epoch 316/512

Epoch 00316: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1171e-10 - val_loss: 2.0854e-10
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0898e-10 - val_loss: 2.0945e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1003e-10 - val_loss: 2.0972e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0771e-10 - val_loss: 2.0467e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0146e-10 - val_loss: 1.9949e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0209e-10 - val_loss: 2.0520e-10
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0729e-10 - val_loss: 2.1036e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0881e-10 - val_loss: 2.0705e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0891e-10 - val_loss: 2.1178e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1189e-10 - val_loss: 2.1274e-10
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1052e-10 - val_loss: 2.0328e-10
Epoch 327/512

Epoch 00327: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9915e-10 - val_loss: 1.9573e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9780e-10 - val_loss: 2.0041e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0217e-10 - val_loss: 2.0336e-10
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9995e-10 - val_loss: 1.9490e-10
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9234e-10 - val_loss: 1.9137e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9347e-10 - val_loss: 1.9727e-10
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9860e-10 - val_loss: 1.9733e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9508e-10 - val_loss: 1.9105e-10
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8999e-10 - val_loss: 1.9184e-10
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9183e-10 - val_loss: 1.9345e-10
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9605e-10 - val_loss: 1.9575e-10
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9382e-10 - val_loss: 1.9324e-10
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9400e-10 - val_loss: 1.9502e-10
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9561e-10 - val_loss: 1.9507e-10
Epoch 341/512

Epoch 00341: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9249e-10 - val_loss: 1.8970e-10
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8772e-10 - val_loss: 1.8579e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8632e-10 - val_loss: 1.8742e-10
Epoch 344/512

Epoch 00344: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8672e-10 - val_loss: 1.8303e-10
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8050e-10 - val_loss: 1.7754e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8033e-10 - val_loss: 1.8563e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8717e-10 - val_loss: 1.8868e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8871e-10 - val_loss: 1.8465e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8191e-10 - val_loss: 1.7800e-10
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7836e-10 - val_loss: 1.8115e-10
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8405e-10 - val_loss: 1.8688e-10
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8419e-10 - val_loss: 1.8058e-10
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7763e-10 - val_loss: 1.7275e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7264e-10 - val_loss: 1.7470e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7878e-10 - val_loss: 1.8407e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8252e-10 - val_loss: 1.7827e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7539e-10 - val_loss: 1.7315e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7347e-10 - val_loss: 1.7411e-10
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7485e-10 - val_loss: 1.7884e-10
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7671e-10 - val_loss: 1.7371e-10
Epoch 361/512

Epoch 00361: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7123e-10 - val_loss: 1.7015e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7081e-10 - val_loss: 1.7169e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7347e-10 - val_loss: 1.7393e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7266e-10 - val_loss: 1.7061e-10
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7158e-10 - val_loss: 1.7206e-10
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7404e-10 - val_loss: 1.7578e-10
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7571e-10 - val_loss: 1.7269e-10
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7076e-10 - val_loss: 1.6664e-10
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6517e-10 - val_loss: 1.6542e-10
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6605e-10 - val_loss: 1.6809e-10
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6971e-10 - val_loss: 1.6932e-10
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6840e-10 - val_loss: 1.6620e-10
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6623e-10 - val_loss: 1.6784e-10
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6898e-10 - val_loss: 1.7067e-10
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7105e-10 - val_loss: 1.6800e-10
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6517e-10 - val_loss: 1.6106e-10
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6006e-10 - val_loss: 1.6010e-10
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6042e-10 - val_loss: 1.6151e-10
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6181e-10 - val_loss: 1.6152e-10
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5973e-10 - val_loss: 1.5739e-10
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5760e-10 - val_loss: 1.5920e-10
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6089e-10 - val_loss: 1.6374e-10
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6263e-10 - val_loss: 1.6178e-10
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6129e-10 - val_loss: 1.6007e-10
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5997e-10 - val_loss: 1.6108e-10
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6258e-10 - val_loss: 1.6502e-10
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6261e-10 - val_loss: 1.5971e-10
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5722e-10 - val_loss: 1.5325e-10
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5409e-10 - val_loss: 1.5667e-10
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5789e-10 - val_loss: 1.5932e-10
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5767e-10 - val_loss: 1.5535e-10
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5401e-10 - val_loss: 1.5274e-10
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5392e-10 - val_loss: 1.5723e-10
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5860e-10 - val_loss: 1.6004e-10
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5836e-10 - val_loss: 1.5564e-10
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5439e-10 - val_loss: 1.5185e-10
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5066e-10 - val_loss: 1.4932e-10
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5138e-10 - val_loss: 1.5585e-10
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5543e-10 - val_loss: 1.5409e-10
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5281e-10 - val_loss: 1.5085e-10
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4955e-10 - val_loss: 1.4910e-10
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5013e-10 - val_loss: 1.5227e-10
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5403e-10 - val_loss: 1.5560e-10
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5331e-10 - val_loss: 1.5062e-10
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4985e-10 - val_loss: 1.4965e-10
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5144e-10 - val_loss: 1.5425e-10
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5478e-10 - val_loss: 1.5401e-10
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5296e-10 - val_loss: 1.5102e-10
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5098e-10 - val_loss: 1.5171e-10
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5284e-10 - val_loss: 1.5388e-10
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5343e-10 - val_loss: 1.5047e-10
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4859e-10 - val_loss: 1.4632e-10
Epoch 413/512

Epoch 00413: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4622e-10 - val_loss: 1.4615e-10
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4624e-10 - val_loss: 1.4754e-10
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4772e-10 - val_loss: 1.4626e-10
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4533e-10 - val_loss: 1.4320e-10
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4202e-10 - val_loss: 1.4161e-10
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4264e-10 - val_loss: 1.4593e-10
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4949e-10 - val_loss: 1.5280e-10
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5201e-10 - val_loss: 1.4954e-10
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4775e-10 - val_loss: 1.4491e-10
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4243e-10 - val_loss: 1.4090e-10
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4150e-10 - val_loss: 1.4338e-10
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4326e-10 - val_loss: 1.4150e-10
Epoch 425/512

Epoch 00425: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3981e-10 - val_loss: 1.3849e-10
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3767e-10 - val_loss: 1.3843e-10
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4097e-10 - val_loss: 1.4365e-10
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4444e-10 - val_loss: 1.4431e-10
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4528e-10 - val_loss: 1.4553e-10
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4454e-10 - val_loss: 1.4161e-10
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4121e-10 - val_loss: 1.4196e-10
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4237e-10 - val_loss: 1.4109e-10
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4021e-10 - val_loss: 1.3888e-10
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3967e-10 - val_loss: 1.3980e-10
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3999e-10 - val_loss: 1.4239e-10
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4341e-10 - val_loss: 1.4384e-10
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4157e-10 - val_loss: 1.3887e-10
Epoch 438/512

Epoch 00438: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3727e-10 - val_loss: 1.3511e-10
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3508e-10 - val_loss: 1.3728e-10
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3900e-10 - val_loss: 1.4142e-10
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4255e-10 - val_loss: 1.4211e-10
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4039e-10 - val_loss: 1.3748e-10
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3572e-10 - val_loss: 1.3434e-10
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3472e-10 - val_loss: 1.3745e-10
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3842e-10 - val_loss: 1.3794e-10
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3665e-10 - val_loss: 1.3570e-10
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3537e-10 - val_loss: 1.3424e-10
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3368e-10 - val_loss: 1.3364e-10
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3438e-10 - val_loss: 1.3600e-10
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3611e-10 - val_loss: 1.3562e-10
Epoch 451/512

Epoch 00451: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3493e-10 - val_loss: 1.3363e-10
Epoch 452/512

Epoch 00452: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3293e-10 - val_loss: 1.3329e-10
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3448e-10 - val_loss: 1.3516e-10
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3635e-10 - val_loss: 1.3655e-10
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3568e-10 - val_loss: 1.3486e-10
Epoch 456/512

Epoch 00456: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3377e-10 - val_loss: 1.3215e-10
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3244e-10 - val_loss: 1.3382e-10
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3451e-10 - val_loss: 1.3689e-10
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3610e-10 - val_loss: 1.3479e-10
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3429e-10 - val_loss: 1.3355e-10
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3266e-10 - val_loss: 1.3265e-10
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3343e-10 - val_loss: 1.3550e-10
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3660e-10 - val_loss: 1.3518e-10
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3391e-10 - val_loss: 1.3141e-10
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3053e-10 - val_loss: 1.2868e-10
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2995e-10 - val_loss: 1.3227e-10
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3277e-10 - val_loss: 1.3511e-10
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3413e-10 - val_loss: 1.3274e-10
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3156e-10 - val_loss: 1.2954e-10
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2904e-10 - val_loss: 1.2958e-10
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3088e-10 - val_loss: 1.3428e-10
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3494e-10 - val_loss: 1.3341e-10
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3151e-10 - val_loss: 1.2897e-10
Epoch 474/512

Epoch 00474: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2817e-10 - val_loss: 1.2668e-10
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2694e-10 - val_loss: 1.2827e-10
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2929e-10 - val_loss: 1.3145e-10
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3135e-10 - val_loss: 1.2975e-10
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2889e-10 - val_loss: 1.2766e-10
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2701e-10 - val_loss: 1.2609e-10
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2614e-10 - val_loss: 1.2741e-10
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2824e-10 - val_loss: 1.2683e-10
Epoch 482/512

Epoch 00482: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2610e-10 - val_loss: 1.2589e-10
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2560e-10 - val_loss: 1.2532e-10
Epoch 484/512

Epoch 00484: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2376e-10 - val_loss: 1.2351e-10
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2500e-10 - val_loss: 1.2658e-10
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2514e-10 - val_loss: 1.2391e-10
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2406e-10 - val_loss: 1.2411e-10
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2437e-10 - val_loss: 1.2379e-10
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2375e-10 - val_loss: 1.2504e-10
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2581e-10 - val_loss: 1.2539e-10
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2570e-10 - val_loss: 1.2564e-10
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2524e-10 - val_loss: 1.2466e-10
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2408e-10 - val_loss: 1.2472e-10
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2624e-10 - val_loss: 1.2825e-10
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2812e-10 - val_loss: 1.2687e-10
Epoch 496/512

Epoch 00496: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2550e-10 - val_loss: 1.2285e-10
Epoch 497/512

Epoch 00497: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2139e-10 - val_loss: 1.1958e-10
Epoch 498/512

Epoch 00498: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1859e-10 - val_loss: 1.1851e-10
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2062e-10 - val_loss: 1.2383e-10
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2338e-10 - val_loss: 1.2289e-10
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2232e-10 - val_loss: 1.2182e-10
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2051e-10 - val_loss: 1.1906e-10
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1979e-10 - val_loss: 1.2203e-10
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2270e-10 - val_loss: 1.2293e-10
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2279e-10 - val_loss: 1.2262e-10
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2228e-10 - val_loss: 1.2180e-10
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2121e-10 - val_loss: 1.2037e-10
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2090e-10 - val_loss: 1.2336e-10
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2383e-10 - val_loss: 1.2307e-10
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2177e-10 - val_loss: 1.1991e-10
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1895e-10 - val_loss: 1.1793e-10
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1737e-10 - val_loss: 1.1724e-10
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.1458 - val_loss: 0.0238
Epoch 2/512
512/512 - 0s - loss: 0.0265 - val_loss: 0.0097
Epoch 3/512
512/512 - 0s - loss: 0.0218 - val_loss: 0.0064
Epoch 4/512
512/512 - 0s - loss: 0.0185 - val_loss: 0.0042
Epoch 5/512
512/512 - 0s - loss: 0.0156 - val_loss: 0.0040
Epoch 6/512
512/512 - 0s - loss: 0.0131 - val_loss: 0.0048
Epoch 7/512
512/512 - 0s - loss: 0.0111 - val_loss: 0.0052
Epoch 8/512
512/512 - 0s - loss: 0.0101 - val_loss: 0.0048
Epoch 9/512
512/512 - 0s - loss: 0.0107 - val_loss: 0.0028
Epoch 10/512
512/512 - 0s - loss: 0.0077 - val_loss: 0.0023
Epoch 11/512
512/512 - 0s - loss: 0.0071 - val_loss: 0.0016
Epoch 12/512
512/512 - 0s - loss: 0.0076 - val_loss: 9.3993e-04
Epoch 13/512
512/512 - 0s - loss: 0.0068 - val_loss: 6.2293e-04
Epoch 14/512
512/512 - 0s - loss: 0.0057 - val_loss: 4.6923e-04
Epoch 15/512
512/512 - 0s - loss: 0.0054 - val_loss: 4.4234e-04
Epoch 16/512
512/512 - 0s - loss: 0.0053 - val_loss: 3.3104e-04
Epoch 17/512
512/512 - 0s - loss: 0.0046 - val_loss: 2.5026e-04
Epoch 18/512
512/512 - 0s - loss: 0.0037 - val_loss: 2.0091e-04
Epoch 19/512
512/512 - 0s - loss: 0.0033 - val_loss: 1.5240e-04
Epoch 20/512
512/512 - 0s - loss: 0.0030 - val_loss: 9.4496e-05
Epoch 21/512
512/512 - 0s - loss: 0.0025 - val_loss: 5.9633e-05
Epoch 22/512
512/512 - 0s - loss: 0.0019 - val_loss: 1.3103e-05
Epoch 23/512
512/512 - 0s - loss: 0.0016 - val_loss: 2.1606e-05
Epoch 24/512
512/512 - 0s - loss: 0.0022 - val_loss: 1.9982e-05
Epoch 25/512
512/512 - 0s - loss: 0.0015 - val_loss: 2.5629e-05
Epoch 26/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1223e-04
Epoch 27/512
512/512 - 0s - loss: 0.0015 - val_loss: 8.3684e-05
Epoch 28/512
512/512 - 0s - loss: 0.0014 - val_loss: 2.5383e-05
Epoch 29/512
512/512 - 0s - loss: 0.0011 - val_loss: 8.0384e-05
Epoch 30/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.2635e-04
Epoch 31/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1610e-04
Epoch 32/512
512/512 - 0s - loss: 0.0012 - val_loss: 2.3519e-04
Epoch 33/512
512/512 - 0s - loss: 9.7965e-04 - val_loss: 1.4446e-04
Epoch 34/512
512/512 - 0s - loss: 9.2388e-04 - val_loss: 5.6820e-05
Epoch 35/512
512/512 - 0s - loss: 0.0011 - val_loss: 3.6138e-04
Epoch 36/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0500e-04
Epoch 37/512
512/512 - 0s - loss: 7.9825e-04 - val_loss: 2.4895e-05
Epoch 38/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.3703e-05
Epoch 39/512
512/512 - 0s - loss: 8.5466e-04 - val_loss: 3.5994e-04
Epoch 40/512
512/512 - 0s - loss: 8.7632e-04 - val_loss: 1.1063e-04
Epoch 41/512
512/512 - 0s - loss: 8.3414e-04 - val_loss: 2.0665e-05
Epoch 42/512
512/512 - 0s - loss: 8.2960e-04 - val_loss: 1.1371e-05
Epoch 43/512
512/512 - 0s - loss: 6.9247e-04 - val_loss: 3.8386e-04
Epoch 44/512
512/512 - 0s - loss: 8.4278e-04 - val_loss: 3.9373e-05
Epoch 45/512
512/512 - 0s - loss: 6.8384e-04 - val_loss: 1.8622e-05
Epoch 46/512
512/512 - 0s - loss: 6.4144e-04 - val_loss: 2.6373e-05
Epoch 47/512
512/512 - 0s - loss: 6.5384e-04 - val_loss: 3.0915e-04
Epoch 48/512
512/512 - 0s - loss: 5.8589e-04 - val_loss: 8.6479e-05
Epoch 49/512
512/512 - 0s - loss: 5.1611e-04 - val_loss: 2.3411e-05
Epoch 50/512
512/512 - 0s - loss: 5.3895e-04 - val_loss: 2.1517e-05
Epoch 51/512
512/512 - 0s - loss: 4.1152e-04 - val_loss: 2.6793e-04
Epoch 52/512
512/512 - 0s - loss: 4.4968e-04 - val_loss: 1.6786e-04
Epoch 53/512
512/512 - 0s - loss: 3.5374e-04 - val_loss: 6.2009e-05
Epoch 54/512
512/512 - 0s - loss: 3.4677e-04 - val_loss: 4.6103e-05
Epoch 55/512
512/512 - 0s - loss: 2.9237e-04 - val_loss: 3.1058e-05
Epoch 56/512
512/512 - 0s - loss: 2.2741e-04 - val_loss: 5.5265e-05
Epoch 57/512
512/512 - 0s - loss: 1.9867e-04 - val_loss: 1.6674e-04
Epoch 58/512
512/512 - 0s - loss: 1.9741e-04 - val_loss: 1.3969e-04
Epoch 59/512
512/512 - 0s - loss: 2.2649e-04 - val_loss: 1.1277e-04
Epoch 60/512
512/512 - 0s - loss: 1.1198e-04 - val_loss: 1.0392e-04
Epoch 61/512
512/512 - 0s - loss: 8.8149e-05 - val_loss: 4.4039e-05
Epoch 62/512
512/512 - 0s - loss: 1.2428e-04 - val_loss: 7.6955e-05
Epoch 63/512
512/512 - 0s - loss: 9.3959e-05 - val_loss: 5.3676e-05
Epoch 64/512
512/512 - 0s - loss: 4.9376e-05 - val_loss: 4.8119e-05
Epoch 65/512
512/512 - 0s - loss: 4.5579e-05 - val_loss: 2.4911e-05
Epoch 66/512
512/512 - 0s - loss: 5.8238e-05 - val_loss: 2.2364e-05
Epoch 67/512
512/512 - 0s - loss: 2.9815e-05 - val_loss: 3.6578e-05
Epoch 68/512
512/512 - 0s - loss: 2.2111e-05 - val_loss: 3.4754e-05
Epoch 69/512
512/512 - 0s - loss: 1.8153e-05 - val_loss: 3.3120e-05
Epoch 70/512
512/512 - 0s - loss: 1.4466e-05 - val_loss: 2.5730e-05
Epoch 71/512
512/512 - 0s - loss: 1.8736e-05 - val_loss: 7.8732e-06
Epoch 72/512
512/512 - 0s - loss: 2.0545e-05 - val_loss: 2.0851e-05
Epoch 73/512
512/512 - 0s - loss: 8.0413e-06 - val_loss: 2.1233e-05
Epoch 74/512
512/512 - 0s - loss: 6.5289e-06 - val_loss: 2.0792e-05
Epoch 75/512
512/512 - 0s - loss: 1.4302e-05 - val_loss: 3.9110e-05
Epoch 76/512
512/512 - 0s - loss: 8.6559e-06 - val_loss: 1.2472e-05
Epoch 77/512
512/512 - 0s - loss: 3.5654e-06 - val_loss: 1.1816e-05
Epoch 78/512
512/512 - 0s - loss: 3.1931e-06 - val_loss: 1.0170e-05
Epoch 79/512
512/512 - 0s - loss: 2.6105e-06 - val_loss: 8.2949e-06
Epoch 80/512
512/512 - 0s - loss: 2.2474e-06 - val_loss: 9.4107e-06
Epoch 81/512
512/512 - 0s - loss: 4.8738e-06 - val_loss: 1.5972e-05
Epoch 82/512
512/512 - 0s - loss: 3.0293e-06 - val_loss: 5.5040e-06
Epoch 83/512
512/512 - 0s - loss: 1.2507e-06 - val_loss: 4.6880e-06
Epoch 84/512
512/512 - 0s - loss: 1.0524e-06 - val_loss: 4.1991e-06
Epoch 85/512
512/512 - 0s - loss: 9.0184e-07 - val_loss: 3.7419e-06
Epoch 86/512
512/512 - 0s - loss: 8.9653e-07 - val_loss: 6.8713e-06
Epoch 87/512
512/512 - 0s - loss: 3.0483e-06 - val_loss: 2.5615e-06
Epoch 88/512
512/512 - 0s - loss: 5.7835e-07 - val_loss: 2.1798e-06
Epoch 89/512
512/512 - 0s - loss: 4.6053e-07 - val_loss: 2.0000e-06
Epoch 90/512
512/512 - 0s - loss: 4.8436e-07 - val_loss: 3.2384e-06
Epoch 91/512
512/512 - 0s - loss: 9.8727e-07 - val_loss: 1.3840e-06
Epoch 92/512
512/512 - 0s - loss: 3.2610e-07 - val_loss: 1.0492e-06
Epoch 93/512
512/512 - 0s - loss: 3.8582e-07 - val_loss: 8.3829e-07
Epoch 94/512
512/512 - 0s - loss: 2.8562e-07 - val_loss: 8.4399e-07
Epoch 95/512
512/512 - 0s - loss: 3.6160e-07 - val_loss: 1.7013e-06
Epoch 96/512
512/512 - 0s - loss: 4.4345e-07 - val_loss: 5.5439e-07
Epoch 97/512
512/512 - 0s - loss: 1.6671e-07 - val_loss: 5.0233e-07
Epoch 98/512
512/512 - 0s - loss: 1.3579e-07 - val_loss: 4.5201e-07
Epoch 99/512
512/512 - 0s - loss: 1.1207e-07 - val_loss: 4.3650e-07
Epoch 100/512
512/512 - 0s - loss: 1.1552e-07 - val_loss: 8.8484e-07
Epoch 101/512
512/512 - 0s - loss: 3.0574e-07 - val_loss: 3.4906e-07
Epoch 102/512
512/512 - 0s - loss: 7.3311e-08 - val_loss: 3.1756e-07
Epoch 103/512
512/512 - 0s - loss: 6.1902e-08 - val_loss: 2.8791e-07
Epoch 104/512
512/512 - 0s - loss: 5.3117e-08 - val_loss: 2.7000e-07
Epoch 105/512
512/512 - 0s - loss: 4.6145e-08 - val_loss: 2.5211e-07
Epoch 106/512
512/512 - 0s - loss: 4.8028e-08 - val_loss: 4.6147e-07
Epoch 107/512
512/512 - 0s - loss: 1.5369e-07 - val_loss: 1.8561e-07
Epoch 108/512
512/512 - 0s - loss: 4.0758e-08 - val_loss: 1.7378e-07
Epoch 109/512
512/512 - 0s - loss: 3.4607e-08 - val_loss: 1.6650e-07
Epoch 110/512
512/512 - 0s - loss: 3.0198e-08 - val_loss: 1.5899e-07
Epoch 111/512
512/512 - 0s - loss: 2.6677e-08 - val_loss: 1.5329e-07
Epoch 112/512
512/512 - 0s - loss: 2.8321e-08 - val_loss: 2.8605e-07
Epoch 113/512
512/512 - 0s - loss: 1.1905e-07 - val_loss: 1.5725e-07
Epoch 114/512
512/512 - 0s - loss: 1.7654e-08 - val_loss: 1.3679e-07
Epoch 115/512
512/512 - 0s - loss: 1.6248e-08 - val_loss: 1.2945e-07
Epoch 116/512
512/512 - 0s - loss: 1.5105e-08 - val_loss: 1.2153e-07
Epoch 117/512
512/512 - 0s - loss: 1.4596e-08 - val_loss: 1.1020e-07
Epoch 118/512
512/512 - 0s - loss: 1.6082e-08 - val_loss: 3.9361e-08
Epoch 119/512
512/512 - 0s - loss: 6.8904e-08 - val_loss: 1.0371e-07
Epoch 120/512
512/512 - 0s - loss: 1.2410e-08 - val_loss: 1.0090e-07
Epoch 121/512
512/512 - 0s - loss: 1.1506e-08 - val_loss: 9.8059e-08
Epoch 122/512
512/512 - 0s - loss: 1.0442e-08 - val_loss: 9.2253e-08
Epoch 123/512
512/512 - 0s - loss: 1.0099e-08 - val_loss: 7.2629e-08
Epoch 124/512
512/512 - 0s - loss: 2.5155e-08 - val_loss: 3.1921e-08
Epoch 125/512
512/512 - 0s - loss: 2.0996e-08 - val_loss: 6.4654e-08
Epoch 126/512
512/512 - 0s - loss: 9.1725e-08 - val_loss: 5.1374e-08
Epoch 127/512
512/512 - 0s - loss: 9.9633e-09 - val_loss: 7.4891e-08
Epoch 128/512
512/512 - 0s - loss: 7.1161e-09 - val_loss: 7.1080e-08
Epoch 129/512
512/512 - 0s - loss: 6.9194e-09 - val_loss: 7.0267e-08
Epoch 130/512
512/512 - 0s - loss: 6.7619e-09 - val_loss: 6.9738e-08
Epoch 131/512
512/512 - 0s - loss: 9.9665e-09 - val_loss: 3.5690e-08
Epoch 132/512
512/512 - 0s - loss: 3.9272e-08 - val_loss: 2.6396e-08
Epoch 133/512
512/512 - 0s - loss: 5.1546e-08 - val_loss: 4.3171e-08
Epoch 134/512
512/512 - 0s - loss: 8.1754e-09 - val_loss: 5.1661e-08
Epoch 135/512
512/512 - 0s - loss: 6.1546e-09 - val_loss: 5.0811e-08
Epoch 136/512
512/512 - 0s - loss: 5.9886e-09 - val_loss: 4.8350e-08
Epoch 137/512
512/512 - 0s - loss: 8.8138e-09 - val_loss: 2.2694e-08
Epoch 138/512
512/512 - 0s - loss: 2.4731e-08 - val_loss: 4.0594e-08
Epoch 139/512
512/512 - 0s - loss: 5.9241e-09 - val_loss: 4.2143e-08
Epoch 140/512
512/512 - 0s - loss: 5.7176e-09 - val_loss: 3.8344e-08
Epoch 141/512
512/512 - 0s - loss: 7.0110e-09 - val_loss: 2.6209e-08
Epoch 142/512
512/512 - 0s - loss: 2.3198e-08 - val_loss: 2.5812e-08
Epoch 143/512
512/512 - 0s - loss: 7.4151e-09 - val_loss: 3.7993e-08
Epoch 144/512
512/512 - 0s - loss: 4.5360e-09 - val_loss: 3.8725e-08
Epoch 145/512
512/512 - 0s - loss: 4.2009e-09 - val_loss: 3.9478e-08
Epoch 146/512
512/512 - 0s - loss: 4.0603e-09 - val_loss: 3.8851e-08
Epoch 147/512
512/512 - 0s - loss: 3.9780e-09 - val_loss: 3.8889e-08
Epoch 148/512
512/512 - 0s - loss: 4.3031e-09 - val_loss: 3.8571e-08
Epoch 149/512
512/512 - 0s - loss: 4.1115e-09 - val_loss: 3.7830e-08
Epoch 150/512
512/512 - 0s - loss: 3.9680e-09 - val_loss: 3.6954e-08
Epoch 151/512
512/512 - 0s - loss: 3.8330e-09 - val_loss: 3.7412e-08
Epoch 152/512
512/512 - 0s - loss: 4.0159e-09 - val_loss: 4.1570e-08
Epoch 153/512
512/512 - 0s - loss: 4.1655e-09 - val_loss: 3.8177e-08
Epoch 154/512
512/512 - 0s - loss: 3.8408e-09 - val_loss: 3.8935e-08
Epoch 155/512
512/512 - 0s - loss: 6.0687e-09 - val_loss: 5.1140e-08
Epoch 156/512
512/512 - 0s - loss: 6.9668e-09 - val_loss: 3.3585e-08
Epoch 157/512
512/512 - 0s - loss: 4.1159e-09 - val_loss: 3.2989e-08
Epoch 158/512
512/512 - 0s - loss: 4.1467e-09 - val_loss: 3.2414e-08
Epoch 159/512
512/512 - 0s - loss: 4.1744e-09 - val_loss: 3.1808e-08
Epoch 160/512
512/512 - 0s - loss: 4.0432e-09 - val_loss: 3.1568e-08
Epoch 161/512
512/512 - 0s - loss: 3.9276e-09 - val_loss: 3.1456e-08
Epoch 162/512
512/512 - 0s - loss: 3.8265e-09 - val_loss: 3.1236e-08
Epoch 163/512
512/512 - 0s - loss: 3.6662e-09 - val_loss: 3.1068e-08
Epoch 164/512
512/512 - 0s - loss: 3.6932e-09 - val_loss: 3.0204e-08
Epoch 165/512
512/512 - 0s - loss: 4.0984e-09 - val_loss: 2.8924e-08
Epoch 166/512
512/512 - 0s - loss: 4.0976e-09 - val_loss: 2.8216e-08
Epoch 167/512
512/512 - 0s - loss: 4.0173e-09 - val_loss: 2.8131e-08
Epoch 168/512
512/512 - 0s - loss: 3.6331e-09 - val_loss: 2.8164e-08
Epoch 169/512
512/512 - 0s - loss: 3.4205e-09 - val_loss: 2.7377e-08
Epoch 170/512
512/512 - 0s - loss: 3.3758e-09 - val_loss: 2.6590e-08
Epoch 171/512
512/512 - 0s - loss: 3.3397e-09 - val_loss: 2.5828e-08
Epoch 172/512
512/512 - 0s - loss: 3.3200e-09 - val_loss: 2.4818e-08
Epoch 173/512
512/512 - 0s - loss: 3.7236e-09 - val_loss: 2.5669e-08
Epoch 174/512
512/512 - 0s - loss: 3.6203e-09 - val_loss: 2.5757e-08
Epoch 175/512
512/512 - 0s - loss: 3.3743e-09 - val_loss: 2.5403e-08
Epoch 176/512
512/512 - 0s - loss: 3.4625e-09 - val_loss: 2.4887e-08
Epoch 177/512
512/512 - 0s - loss: 3.4668e-09 - val_loss: 2.4840e-08
Epoch 178/512
512/512 - 0s - loss: 3.5502e-09 - val_loss: 2.4566e-08
Epoch 179/512
512/512 - 0s - loss: 3.6975e-09 - val_loss: 2.1726e-08
Epoch 180/512
512/512 - 0s - loss: 4.1336e-09 - val_loss: 2.2041e-08
Epoch 181/512
512/512 - 0s - loss: 3.8192e-09 - val_loss: 2.2933e-08
Epoch 182/512
512/512 - 0s - loss: 3.6969e-09 - val_loss: 2.2617e-08
Epoch 183/512
512/512 - 0s - loss: 3.6226e-09 - val_loss: 2.2480e-08
Epoch 184/512
512/512 - 0s - loss: 3.5825e-09 - val_loss: 2.1976e-08
Epoch 185/512
512/512 - 0s - loss: 3.5549e-09 - val_loss: 2.1861e-08
Epoch 186/512
512/512 - 0s - loss: 3.5389e-09 - val_loss: 2.1006e-08
Epoch 187/512
512/512 - 0s - loss: 3.6262e-09 - val_loss: 2.0595e-08
Epoch 188/512
512/512 - 0s - loss: 3.6073e-09 - val_loss: 2.0358e-08
Epoch 189/512
512/512 - 0s - loss: 3.5824e-09 - val_loss: 2.0433e-08
Epoch 190/512
512/512 - 0s - loss: 3.5192e-09 - val_loss: 2.0103e-08
Epoch 191/512
512/512 - 0s - loss: 3.4456e-09 - val_loss: 2.0167e-08
Epoch 192/512
512/512 - 0s - loss: 3.3923e-09 - val_loss: 1.9778e-08
Epoch 193/512
512/512 - 0s - loss: 3.4454e-09 - val_loss: 1.9321e-08
Epoch 194/512
512/512 - 0s - loss: 3.4776e-09 - val_loss: 1.8913e-08
Epoch 195/512
512/512 - 0s - loss: 3.4286e-09 - val_loss: 1.8604e-08
Epoch 196/512
512/512 - 0s - loss: 3.3415e-09 - val_loss: 1.8570e-08
Epoch 197/512
512/512 - 0s - loss: 3.3173e-09 - val_loss: 1.7737e-08
Epoch 198/512
512/512 - 0s - loss: 3.3302e-09 - val_loss: 1.8239e-08
Epoch 199/512
512/512 - 0s - loss: 3.1924e-09 - val_loss: 1.8250e-08
Epoch 200/512
512/512 - 0s - loss: 3.1294e-09 - val_loss: 1.8072e-08
Epoch 201/512
512/512 - 0s - loss: 3.0905e-09 - val_loss: 1.8180e-08
Epoch 202/512
512/512 - 0s - loss: 2.9790e-09 - val_loss: 1.8197e-08
Epoch 203/512
512/512 - 0s - loss: 2.8988e-09 - val_loss: 1.8358e-08
Epoch 204/512
512/512 - 0s - loss: 2.8438e-09 - val_loss: 1.8046e-08
Epoch 205/512
512/512 - 0s - loss: 2.8160e-09 - val_loss: 1.7896e-08
Epoch 206/512
512/512 - 0s - loss: 2.7692e-09 - val_loss: 1.7850e-08
Epoch 207/512
512/512 - 0s - loss: 2.6855e-09 - val_loss: 1.7918e-08
Epoch 208/512
512/512 - 0s - loss: 2.6011e-09 - val_loss: 1.7747e-08
Epoch 209/512
512/512 - 0s - loss: 2.5907e-09 - val_loss: 1.7589e-08
Epoch 210/512
512/512 - 0s - loss: 2.5720e-09 - val_loss: 1.7351e-08
Epoch 211/512
512/512 - 0s - loss: 2.5812e-09 - val_loss: 1.7230e-08
Epoch 212/512
512/512 - 0s - loss: 2.5549e-09 - val_loss: 1.7026e-08
Epoch 213/512
512/512 - 0s - loss: 2.5737e-09 - val_loss: 1.7038e-08
Epoch 214/512
512/512 - 0s - loss: 2.5240e-09 - val_loss: 1.6856e-08
Epoch 215/512
512/512 - 0s - loss: 2.5834e-09 - val_loss: 1.6514e-08
Epoch 216/512
512/512 - 0s - loss: 2.4169e-09 - val_loss: 1.6707e-08
Epoch 217/512
512/512 - 0s - loss: 2.3585e-09 - val_loss: 1.6546e-08
Epoch 218/512
512/512 - 0s - loss: 2.3241e-09 - val_loss: 1.6594e-08
Epoch 219/512
512/512 - 0s - loss: 2.3376e-09 - val_loss: 1.6226e-08
Epoch 220/512
512/512 - 0s - loss: 2.3410e-09 - val_loss: 1.6243e-08
Epoch 221/512
512/512 - 0s - loss: 2.3271e-09 - val_loss: 1.6036e-08
Epoch 222/512
512/512 - 0s - loss: 2.3646e-09 - val_loss: 1.5833e-08
Epoch 223/512
512/512 - 0s - loss: 2.3326e-09 - val_loss: 1.5765e-08
Epoch 224/512
512/512 - 0s - loss: 2.2508e-09 - val_loss: 1.5662e-08
Epoch 225/512
512/512 - 0s - loss: 2.2119e-09 - val_loss: 1.5921e-08
Epoch 226/512
512/512 - 0s - loss: 2.1817e-09 - val_loss: 1.5667e-08
Epoch 227/512
512/512 - 0s - loss: 2.2363e-09 - val_loss: 1.5187e-08
Epoch 228/512
512/512 - 0s - loss: 2.2221e-09 - val_loss: 1.5146e-08
Epoch 229/512
512/512 - 0s - loss: 2.1800e-09 - val_loss: 1.5264e-08
Epoch 230/512
512/512 - 0s - loss: 2.1323e-09 - val_loss: 1.5192e-08
Epoch 231/512
512/512 - 0s - loss: 2.0999e-09 - val_loss: 1.5193e-08
Epoch 232/512
512/512 - 0s - loss: 2.0562e-09 - val_loss: 1.5436e-08
Epoch 233/512
512/512 - 0s - loss: 1.9592e-09 - val_loss: 1.5319e-08
Epoch 234/512
512/512 - 0s - loss: 1.8941e-09 - val_loss: 1.5273e-08
Epoch 235/512
512/512 - 0s - loss: 1.9057e-09 - val_loss: 1.5129e-08
Epoch 236/512
512/512 - 0s - loss: 1.8899e-09 - val_loss: 1.5010e-08
Epoch 237/512
512/512 - 0s - loss: 1.8736e-09 - val_loss: 1.4909e-08
Epoch 238/512
512/512 - 0s - loss: 1.9008e-09 - val_loss: 1.4805e-08
Epoch 239/512
512/512 - 0s - loss: 1.8939e-09 - val_loss: 1.4618e-08
Epoch 240/512
512/512 - 0s - loss: 1.8771e-09 - val_loss: 1.4493e-08
Epoch 241/512
512/512 - 0s - loss: 1.9089e-09 - val_loss: 1.4236e-08
Epoch 242/512
512/512 - 0s - loss: 1.9659e-09 - val_loss: 1.3931e-08
Epoch 243/512
512/512 - 0s - loss: 1.9685e-09 - val_loss: 1.3706e-08
Epoch 244/512
512/512 - 0s - loss: 1.9427e-09 - val_loss: 1.3746e-08
Epoch 245/512
512/512 - 0s - loss: 1.9376e-09 - val_loss: 1.3531e-08
Epoch 246/512
512/512 - 0s - loss: 1.9226e-09 - val_loss: 1.3419e-08
Epoch 247/512
512/512 - 0s - loss: 1.8934e-09 - val_loss: 1.3347e-08
Epoch 248/512
512/512 - 0s - loss: 1.8880e-09 - val_loss: 1.3175e-08
Epoch 249/512
512/512 - 0s - loss: 1.8663e-09 - val_loss: 1.3351e-08
Epoch 250/512
512/512 - 0s - loss: 1.8039e-09 - val_loss: 1.3346e-08
Epoch 251/512
512/512 - 0s - loss: 1.7634e-09 - val_loss: 1.3498e-08
Epoch 252/512
512/512 - 0s - loss: 1.7278e-09 - val_loss: 1.3515e-08
Epoch 253/512
512/512 - 0s - loss: 1.7118e-09 - val_loss: 1.3564e-08
Epoch 254/512
512/512 - 0s - loss: 1.6710e-09 - val_loss: 1.3532e-08
Epoch 255/512
512/512 - 0s - loss: 1.6739e-09 - val_loss: 1.3380e-08
Epoch 256/512
512/512 - 0s - loss: 1.6516e-09 - val_loss: 1.3373e-08
Epoch 257/512
512/512 - 0s - loss: 1.6223e-09 - val_loss: 1.3314e-08
Epoch 258/512
512/512 - 0s - loss: 1.6143e-09 - val_loss: 1.3141e-08
Epoch 259/512
512/512 - 0s - loss: 1.5967e-09 - val_loss: 1.3076e-08
Epoch 260/512
512/512 - 0s - loss: 1.5472e-09 - val_loss: 1.3027e-08
Epoch 261/512
512/512 - 0s - loss: 1.5318e-09 - val_loss: 1.3040e-08
Epoch 262/512
512/512 - 0s - loss: 1.5095e-09 - val_loss: 1.3021e-08
Epoch 263/512
512/512 - 0s - loss: 1.4981e-09 - val_loss: 1.3003e-08
Epoch 264/512
512/512 - 0s - loss: 1.4881e-09 - val_loss: 1.2800e-08
Epoch 265/512
512/512 - 0s - loss: 1.5261e-09 - val_loss: 1.2736e-08
Epoch 266/512
512/512 - 0s - loss: 1.5077e-09 - val_loss: 1.2716e-08
Epoch 267/512
512/512 - 0s - loss: 1.4930e-09 - val_loss: 1.2508e-08
Epoch 268/512
512/512 - 0s - loss: 1.4766e-09 - val_loss: 1.2315e-08
Epoch 269/512
512/512 - 0s - loss: 1.4938e-09 - val_loss: 1.2341e-08
Epoch 270/512
512/512 - 0s - loss: 1.4818e-09 - val_loss: 1.2172e-08
Epoch 271/512
512/512 - 0s - loss: 1.4706e-09 - val_loss: 1.2138e-08
Epoch 272/512
512/512 - 0s - loss: 1.4660e-09 - val_loss: 1.2249e-08
Epoch 273/512
512/512 - 0s - loss: 1.4372e-09 - val_loss: 1.2292e-08
Epoch 274/512
512/512 - 0s - loss: 1.3954e-09 - val_loss: 1.2180e-08
Epoch 275/512
512/512 - 0s - loss: 1.3822e-09 - val_loss: 1.2089e-08
Epoch 276/512
512/512 - 0s - loss: 1.3718e-09 - val_loss: 1.1828e-08
Epoch 277/512
512/512 - 0s - loss: 1.3640e-09 - val_loss: 1.1819e-08
Epoch 278/512
512/512 - 0s - loss: 1.3460e-09 - val_loss: 1.1730e-08
Epoch 279/512
512/512 - 0s - loss: 1.3377e-09 - val_loss: 1.1592e-08
Epoch 280/512
512/512 - 0s - loss: 1.3518e-09 - val_loss: 1.1450e-08
Epoch 281/512
512/512 - 0s - loss: 1.3580e-09 - val_loss: 1.1464e-08
Epoch 282/512
512/512 - 0s - loss: 1.3426e-09 - val_loss: 1.1499e-08
Epoch 283/512
512/512 - 0s - loss: 1.3436e-09 - val_loss: 1.1376e-08
Epoch 284/512
512/512 - 0s - loss: 1.3660e-09 - val_loss: 1.1260e-08
Epoch 285/512
512/512 - 0s - loss: 1.3602e-09 - val_loss: 1.1399e-08
Epoch 286/512
512/512 - 0s - loss: 1.3531e-09 - val_loss: 1.1338e-08
Epoch 287/512
512/512 - 0s - loss: 1.3629e-09 - val_loss: 1.1216e-08
Epoch 288/512
512/512 - 0s - loss: 1.3628e-09 - val_loss: 1.1245e-08
Epoch 289/512
512/512 - 0s - loss: 1.3370e-09 - val_loss: 1.1417e-08
Epoch 290/512
512/512 - 0s - loss: 1.3107e-09 - val_loss: 1.1401e-08
Epoch 291/512
512/512 - 0s - loss: 1.3005e-09 - val_loss: 1.1246e-08
Epoch 292/512
512/512 - 0s - loss: 1.2965e-09 - val_loss: 1.1241e-08
Epoch 293/512
512/512 - 0s - loss: 1.3047e-09 - val_loss: 1.1172e-08
Epoch 294/512
512/512 - 0s - loss: 1.3272e-09 - val_loss: 1.1054e-08
Epoch 295/512
512/512 - 0s - loss: 1.3142e-09 - val_loss: 1.0981e-08
Epoch 296/512
512/512 - 0s - loss: 1.3039e-09 - val_loss: 1.0866e-08
Epoch 297/512
512/512 - 0s - loss: 1.2934e-09 - val_loss: 1.0750e-08
Epoch 298/512
512/512 - 0s - loss: 1.3012e-09 - val_loss: 1.0670e-08
Epoch 299/512
512/512 - 0s - loss: 1.2855e-09 - val_loss: 1.0827e-08
Epoch 300/512
512/512 - 0s - loss: 1.3601e-09 - val_loss: 1.0142e-08
Epoch 301/512
512/512 - 0s - loss: 1.4052e-09 - val_loss: 1.0093e-08
Epoch 302/512
512/512 - 0s - loss: 1.3930e-09 - val_loss: 1.0123e-08
Epoch 303/512
512/512 - 0s - loss: 1.3775e-09 - val_loss: 1.0046e-08
Epoch 304/512
512/512 - 0s - loss: 1.3546e-09 - val_loss: 1.0032e-08
Epoch 305/512
512/512 - 0s - loss: 1.3492e-09 - val_loss: 1.0035e-08
Epoch 306/512
512/512 - 0s - loss: 1.3413e-09 - val_loss: 1.0014e-08
Epoch 307/512
512/512 - 0s - loss: 1.3096e-09 - val_loss: 1.0128e-08
Epoch 308/512
512/512 - 0s - loss: 1.2656e-09 - val_loss: 1.0054e-08
Epoch 309/512
512/512 - 0s - loss: 1.2611e-09 - val_loss: 1.0054e-08
Epoch 310/512
512/512 - 0s - loss: 1.2594e-09 - val_loss: 1.0088e-08
Epoch 311/512
512/512 - 0s - loss: 1.2366e-09 - val_loss: 1.0228e-08
Epoch 312/512
512/512 - 0s - loss: 1.2109e-09 - val_loss: 1.0190e-08
Epoch 313/512
512/512 - 0s - loss: 1.2092e-09 - val_loss: 1.0398e-08
Epoch 314/512
512/512 - 0s - loss: 1.1387e-09 - val_loss: 1.0276e-08
Epoch 315/512
512/512 - 0s - loss: 1.1388e-09 - val_loss: 1.0163e-08
Epoch 316/512
512/512 - 0s - loss: 1.1404e-09 - val_loss: 1.0045e-08
Epoch 317/512
512/512 - 0s - loss: 1.1515e-09 - val_loss: 9.8501e-09
Epoch 318/512
512/512 - 0s - loss: 1.1734e-09 - val_loss: 9.7414e-09
Epoch 319/512
512/512 - 0s - loss: 1.1940e-09 - val_loss: 9.6750e-09
Epoch 320/512
512/512 - 0s - loss: 1.1667e-09 - val_loss: 9.7399e-09
Epoch 321/512
512/512 - 0s - loss: 1.1706e-09 - val_loss: 9.4980e-09
Epoch 322/512
512/512 - 0s - loss: 1.1958e-09 - val_loss: 9.4595e-09
Epoch 323/512
512/512 - 0s - loss: 1.1905e-09 - val_loss: 9.3890e-09
Epoch 324/512
512/512 - 0s - loss: 1.1859e-09 - val_loss: 9.4099e-09
Epoch 325/512
512/512 - 0s - loss: 1.1664e-09 - val_loss: 9.4512e-09
Epoch 326/512
512/512 - 0s - loss: 1.1152e-09 - val_loss: 9.3948e-09
Epoch 327/512
512/512 - 0s - loss: 1.0882e-09 - val_loss: 9.3223e-09
Epoch 328/512
512/512 - 0s - loss: 1.0668e-09 - val_loss: 9.3320e-09
Epoch 329/512
512/512 - 0s - loss: 1.0701e-09 - val_loss: 9.3220e-09
Epoch 330/512
512/512 - 0s - loss: 1.0712e-09 - val_loss: 9.2909e-09
Epoch 331/512
512/512 - 0s - loss: 1.0794e-09 - val_loss: 9.1585e-09
Epoch 332/512
512/512 - 0s - loss: 1.0719e-09 - val_loss: 9.1677e-09
Epoch 333/512
512/512 - 0s - loss: 1.0666e-09 - val_loss: 9.0493e-09
Epoch 334/512
512/512 - 0s - loss: 1.0660e-09 - val_loss: 8.9903e-09
Epoch 335/512
512/512 - 0s - loss: 1.0528e-09 - val_loss: 8.9719e-09
Epoch 336/512
512/512 - 0s - loss: 1.0488e-09 - val_loss: 8.9179e-09
Epoch 337/512
512/512 - 0s - loss: 1.0390e-09 - val_loss: 8.8997e-09
Epoch 338/512
512/512 - 0s - loss: 1.0375e-09 - val_loss: 8.8708e-09
Epoch 339/512
512/512 - 0s - loss: 1.0355e-09 - val_loss: 8.8887e-09
Epoch 340/512
512/512 - 0s - loss: 1.0268e-09 - val_loss: 8.8341e-09
Epoch 341/512
512/512 - 0s - loss: 1.0155e-09 - val_loss: 8.8549e-09
Epoch 342/512
512/512 - 0s - loss: 1.0113e-09 - val_loss: 8.7848e-09
Epoch 343/512
512/512 - 0s - loss: 1.0068e-09 - val_loss: 8.7707e-09
Epoch 344/512
512/512 - 0s - loss: 9.9864e-10 - val_loss: 8.7241e-09
Epoch 345/512
512/512 - 0s - loss: 1.0060e-09 - val_loss: 8.6294e-09
Epoch 346/512
512/512 - 0s - loss: 1.0071e-09 - val_loss: 8.7159e-09
Epoch 347/512
512/512 - 0s - loss: 9.9929e-10 - val_loss: 8.7181e-09
Epoch 348/512
512/512 - 0s - loss: 9.9078e-10 - val_loss: 8.7012e-09
Epoch 349/512
512/512 - 0s - loss: 9.9510e-10 - val_loss: 8.5611e-09
Epoch 350/512
512/512 - 0s - loss: 1.0056e-09 - val_loss: 8.5412e-09
Epoch 351/512
512/512 - 0s - loss: 9.7382e-10 - val_loss: 8.4880e-09
Epoch 352/512
512/512 - 0s - loss: 9.6574e-10 - val_loss: 8.6229e-09
Epoch 353/512
512/512 - 0s - loss: 9.4206e-10 - val_loss: 8.4825e-09
Epoch 354/512
512/512 - 0s - loss: 9.4586e-10 - val_loss: 8.4799e-09
Epoch 355/512
512/512 - 0s - loss: 9.4681e-10 - val_loss: 8.3996e-09
Epoch 356/512
512/512 - 0s - loss: 9.3519e-10 - val_loss: 8.2486e-09
Epoch 357/512
512/512 - 0s - loss: 9.3351e-10 - val_loss: 8.1087e-09
Epoch 358/512
512/512 - 0s - loss: 9.3253e-10 - val_loss: 8.0109e-09
Epoch 359/512
512/512 - 0s - loss: 9.4521e-10 - val_loss: 7.3335e-09
Epoch 360/512
512/512 - 0s - loss: 1.0812e-09 - val_loss: 6.9965e-09
Epoch 361/512
512/512 - 0s - loss: 1.0914e-09 - val_loss: 7.1686e-09
Epoch 362/512
512/512 - 0s - loss: 1.0055e-09 - val_loss: 8.1759e-09
Epoch 363/512
512/512 - 0s - loss: 9.4967e-10 - val_loss: 8.2478e-09
Epoch 364/512
512/512 - 0s - loss: 9.8301e-10 - val_loss: 8.1142e-09
Epoch 365/512
512/512 - 0s - loss: 1.0041e-09 - val_loss: 8.0784e-09
Epoch 366/512
512/512 - 0s - loss: 1.0052e-09 - val_loss: 7.9365e-09
Epoch 367/512
512/512 - 0s - loss: 1.0179e-09 - val_loss: 7.9144e-09
Epoch 368/512
512/512 - 0s - loss: 1.0132e-09 - val_loss: 7.9349e-09
Epoch 369/512
512/512 - 0s - loss: 1.0071e-09 - val_loss: 7.9075e-09
Epoch 370/512
512/512 - 0s - loss: 1.0335e-09 - val_loss: 7.7471e-09
Epoch 371/512
512/512 - 0s - loss: 1.0448e-09 - val_loss: 7.6745e-09
Epoch 372/512
512/512 - 0s - loss: 1.0409e-09 - val_loss: 7.7030e-09
Epoch 373/512
512/512 - 0s - loss: 1.0357e-09 - val_loss: 7.6691e-09
Epoch 374/512
512/512 - 0s - loss: 1.0066e-09 - val_loss: 7.8731e-09
Epoch 375/512
512/512 - 0s - loss: 9.6989e-10 - val_loss: 7.8063e-09
Epoch 376/512
512/512 - 0s - loss: 9.8524e-10 - val_loss: 7.7634e-09
Epoch 377/512
512/512 - 0s - loss: 9.8175e-10 - val_loss: 7.7459e-09
Epoch 378/512
512/512 - 0s - loss: 9.7610e-10 - val_loss: 7.7101e-09
Epoch 379/512
512/512 - 0s - loss: 9.6169e-10 - val_loss: 7.6894e-09
Epoch 380/512
512/512 - 0s - loss: 9.5018e-10 - val_loss: 7.7019e-09
Epoch 381/512
512/512 - 0s - loss: 9.4159e-10 - val_loss: 7.6799e-09
Epoch 382/512
512/512 - 0s - loss: 9.4022e-10 - val_loss: 7.6047e-09
Epoch 383/512
512/512 - 0s - loss: 9.4424e-10 - val_loss: 7.6788e-09
Epoch 384/512
512/512 - 0s - loss: 9.2668e-10 - val_loss: 7.6199e-09
Epoch 385/512
512/512 - 0s - loss: 9.1819e-10 - val_loss: 7.6137e-09
Epoch 386/512
512/512 - 0s - loss: 9.0884e-10 - val_loss: 7.4959e-09
Epoch 387/512
512/512 - 0s - loss: 8.9789e-10 - val_loss: 7.5312e-09
Epoch 388/512
512/512 - 0s - loss: 8.8550e-10 - val_loss: 7.5063e-09
Epoch 389/512
512/512 - 0s - loss: 8.8384e-10 - val_loss: 7.4542e-09
Epoch 390/512
512/512 - 0s - loss: 8.7388e-10 - val_loss: 7.4740e-09
Epoch 391/512
512/512 - 0s - loss: 8.7228e-10 - val_loss: 7.4885e-09
Epoch 392/512
512/512 - 0s - loss: 8.7446e-10 - val_loss: 7.4539e-09
Epoch 393/512
512/512 - 0s - loss: 8.6728e-10 - val_loss: 7.4624e-09
Epoch 394/512
512/512 - 0s - loss: 8.5379e-10 - val_loss: 7.4988e-09
Epoch 395/512
512/512 - 0s - loss: 8.4844e-10 - val_loss: 7.5244e-09
Epoch 396/512
512/512 - 0s - loss: 8.4107e-10 - val_loss: 7.4487e-09
Epoch 397/512
512/512 - 0s - loss: 8.3653e-10 - val_loss: 7.3674e-09
Epoch 398/512
512/512 - 0s - loss: 8.3535e-10 - val_loss: 7.2722e-09
Epoch 399/512
512/512 - 0s - loss: 8.2342e-10 - val_loss: 7.1791e-09
Epoch 400/512
512/512 - 0s - loss: 8.1908e-10 - val_loss: 7.2259e-09
Epoch 401/512
512/512 - 0s - loss: 8.2279e-10 - val_loss: 7.1921e-09
Epoch 402/512
512/512 - 0s - loss: 8.0396e-10 - val_loss: 7.1778e-09
Epoch 403/512
512/512 - 0s - loss: 7.9731e-10 - val_loss: 7.0945e-09
Epoch 404/512
512/512 - 0s - loss: 8.0144e-10 - val_loss: 6.9441e-09
Epoch 405/512
512/512 - 0s - loss: 8.1114e-10 - val_loss: 6.9392e-09
Epoch 406/512
512/512 - 0s - loss: 8.0788e-10 - val_loss: 7.0236e-09
Epoch 407/512
512/512 - 0s - loss: 7.9709e-10 - val_loss: 7.0770e-09
Epoch 408/512
512/512 - 0s - loss: 7.9265e-10 - val_loss: 7.0350e-09
Epoch 409/512
512/512 - 0s - loss: 8.1027e-10 - val_loss: 7.0540e-09
Epoch 410/512
512/512 - 0s - loss: 8.0128e-10 - val_loss: 7.1109e-09
Epoch 411/512
512/512 - 0s - loss: 7.9820e-10 - val_loss: 7.0827e-09
Epoch 412/512
512/512 - 0s - loss: 7.9406e-10 - val_loss: 7.0683e-09
Epoch 413/512
512/512 - 0s - loss: 7.9958e-10 - val_loss: 6.9551e-09
Epoch 414/512
512/512 - 0s - loss: 7.9800e-10 - val_loss: 6.9133e-09
Epoch 415/512
512/512 - 0s - loss: 7.9653e-10 - val_loss: 6.9864e-09
Epoch 416/512
512/512 - 0s - loss: 7.7891e-10 - val_loss: 6.8559e-09
Epoch 417/512
512/512 - 0s - loss: 7.9710e-10 - val_loss: 6.8833e-09
Epoch 418/512
512/512 - 0s - loss: 7.7767e-10 - val_loss: 6.9323e-09
Epoch 419/512
512/512 - 0s - loss: 7.6741e-10 - val_loss: 7.0269e-09
Epoch 420/512
512/512 - 0s - loss: 7.7045e-10 - val_loss: 7.0146e-09
Epoch 421/512
512/512 - 0s - loss: 7.8326e-10 - val_loss: 6.9259e-09
Epoch 422/512
512/512 - 0s - loss: 7.8163e-10 - val_loss: 6.9979e-09
Epoch 423/512
512/512 - 0s - loss: 7.7805e-10 - val_loss: 7.0855e-09
Epoch 424/512
512/512 - 0s - loss: 7.6443e-10 - val_loss: 7.0875e-09
Epoch 425/512
512/512 - 0s - loss: 7.5557e-10 - val_loss: 7.0493e-09
Epoch 426/512
512/512 - 0s - loss: 7.3731e-10 - val_loss: 7.2269e-09
Epoch 427/512
512/512 - 0s - loss: 7.3228e-10 - val_loss: 7.2735e-09
Epoch 428/512
512/512 - 0s - loss: 7.3027e-10 - val_loss: 7.2490e-09
Epoch 429/512
512/512 - 0s - loss: 7.2956e-10 - val_loss: 7.2031e-09
Epoch 430/512
512/512 - 0s - loss: 7.3524e-10 - val_loss: 7.2770e-09
Epoch 431/512
512/512 - 0s - loss: 7.1984e-10 - val_loss: 7.3326e-09
Epoch 432/512
512/512 - 0s - loss: 7.6341e-10 - val_loss: 7.0206e-09
Epoch 433/512
512/512 - 0s - loss: 7.7122e-10 - val_loss: 6.8510e-09
Epoch 434/512
512/512 - 0s - loss: 7.7254e-10 - val_loss: 6.7784e-09
Epoch 435/512
512/512 - 0s - loss: 7.6163e-10 - val_loss: 6.6840e-09
Epoch 436/512
512/512 - 0s - loss: 7.5666e-10 - val_loss: 6.5069e-09
Epoch 437/512
512/512 - 0s - loss: 7.7766e-10 - val_loss: 6.4604e-09
Epoch 438/512
512/512 - 0s - loss: 7.7154e-10 - val_loss: 6.4701e-09
Epoch 439/512
512/512 - 0s - loss: 7.6663e-10 - val_loss: 6.4692e-09
Epoch 440/512
512/512 - 0s - loss: 7.6938e-10 - val_loss: 6.4444e-09
Epoch 441/512
512/512 - 0s - loss: 7.7269e-10 - val_loss: 6.4083e-09
Epoch 442/512
512/512 - 0s - loss: 7.7293e-10 - val_loss: 6.4119e-09
Epoch 443/512
512/512 - 0s - loss: 7.7164e-10 - val_loss: 6.3797e-09
Epoch 444/512
512/512 - 0s - loss: 7.6913e-10 - val_loss: 6.3377e-09
Epoch 445/512
512/512 - 0s - loss: 7.7021e-10 - val_loss: 6.3955e-09
Epoch 446/512
512/512 - 0s - loss: 7.6520e-10 - val_loss: 6.3988e-09
Epoch 447/512
512/512 - 0s - loss: 7.5248e-10 - val_loss: 6.3867e-09
Epoch 448/512
512/512 - 0s - loss: 7.5450e-10 - val_loss: 6.3299e-09
Epoch 449/512
512/512 - 0s - loss: 7.5864e-10 - val_loss: 6.3639e-09
Epoch 450/512
512/512 - 0s - loss: 7.4900e-10 - val_loss: 6.3013e-09
Epoch 451/512
512/512 - 0s - loss: 7.5986e-10 - val_loss: 6.3658e-09
Epoch 452/512
512/512 - 0s - loss: 7.5689e-10 - val_loss: 6.3587e-09
Epoch 453/512
512/512 - 0s - loss: 7.6367e-10 - val_loss: 6.2544e-09
Epoch 454/512
512/512 - 0s - loss: 7.6272e-10 - val_loss: 6.2260e-09
Epoch 455/512
512/512 - 0s - loss: 7.6609e-10 - val_loss: 6.1744e-09
Epoch 456/512
512/512 - 0s - loss: 7.6508e-10 - val_loss: 6.1619e-09
Epoch 457/512
512/512 - 0s - loss: 7.5843e-10 - val_loss: 6.1047e-09
Epoch 458/512
512/512 - 0s - loss: 7.5535e-10 - val_loss: 6.1230e-09
Epoch 459/512
512/512 - 0s - loss: 7.5560e-10 - val_loss: 6.0851e-09
Epoch 460/512
512/512 - 0s - loss: 7.5210e-10 - val_loss: 6.0645e-09
Epoch 461/512
512/512 - 0s - loss: 7.6077e-10 - val_loss: 5.9994e-09
Epoch 462/512
512/512 - 0s - loss: 7.6126e-10 - val_loss: 5.9577e-09
Epoch 463/512
512/512 - 0s - loss: 7.5712e-10 - val_loss: 6.0230e-09
Epoch 464/512
512/512 - 0s - loss: 7.2371e-10 - val_loss: 6.1541e-09
Epoch 465/512
512/512 - 0s - loss: 7.0514e-10 - val_loss: 6.1804e-09
Epoch 466/512
512/512 - 0s - loss: 7.0858e-10 - val_loss: 6.1105e-09
Epoch 467/512
512/512 - 0s - loss: 7.1507e-10 - val_loss: 6.1070e-09
Epoch 468/512
512/512 - 0s - loss: 7.0787e-10 - val_loss: 6.0916e-09
Epoch 469/512
512/512 - 0s - loss: 7.0611e-10 - val_loss: 6.1104e-09
Epoch 470/512
512/512 - 0s - loss: 7.0068e-10 - val_loss: 6.1242e-09
Epoch 471/512
512/512 - 0s - loss: 6.9438e-10 - val_loss: 6.0888e-09
Epoch 472/512
512/512 - 0s - loss: 6.9642e-10 - val_loss: 6.0806e-09
Epoch 473/512
512/512 - 0s - loss: 6.9375e-10 - val_loss: 6.0167e-09
Epoch 474/512
512/512 - 0s - loss: 6.8979e-10 - val_loss: 5.9488e-09
Epoch 475/512
512/512 - 0s - loss: 6.9648e-10 - val_loss: 5.9103e-09
Epoch 476/512
512/512 - 0s - loss: 6.9572e-10 - val_loss: 5.8342e-09
Epoch 477/512
512/512 - 0s - loss: 7.1182e-10 - val_loss: 5.7871e-09
Epoch 478/512
512/512 - 0s - loss: 7.1105e-10 - val_loss: 5.7388e-09
Epoch 479/512
512/512 - 0s - loss: 7.0912e-10 - val_loss: 5.7373e-09
Epoch 480/512
512/512 - 0s - loss: 7.0120e-10 - val_loss: 5.7804e-09
Epoch 481/512
512/512 - 0s - loss: 6.9284e-10 - val_loss: 5.7452e-09
Epoch 482/512
512/512 - 0s - loss: 7.0319e-10 - val_loss: 5.6363e-09
Epoch 483/512
512/512 - 0s - loss: 7.1954e-10 - val_loss: 5.5808e-09
Epoch 484/512
512/512 - 0s - loss: 7.1385e-10 - val_loss: 5.4489e-09
Epoch 485/512
512/512 - 0s - loss: 7.3974e-10 - val_loss: 5.4921e-09
Epoch 486/512
512/512 - 0s - loss: 7.3717e-10 - val_loss: 5.4694e-09
Epoch 487/512
512/512 - 0s - loss: 7.3346e-10 - val_loss: 5.4117e-09
Epoch 488/512
512/512 - 0s - loss: 7.3033e-10 - val_loss: 5.4494e-09
Epoch 489/512
512/512 - 0s - loss: 7.2596e-10 - val_loss: 5.4747e-09
Epoch 490/512
512/512 - 0s - loss: 7.2418e-10 - val_loss: 5.4323e-09
Epoch 491/512
512/512 - 0s - loss: 7.2784e-10 - val_loss: 5.4782e-09
Epoch 492/512
512/512 - 0s - loss: 7.2078e-10 - val_loss: 5.5242e-09
Epoch 493/512
512/512 - 0s - loss: 7.1349e-10 - val_loss: 5.5364e-09
Epoch 494/512
512/512 - 0s - loss: 7.1114e-10 - val_loss: 5.4382e-09
Epoch 495/512
512/512 - 0s - loss: 7.3267e-10 - val_loss: 5.4279e-09
Epoch 496/512
512/512 - 0s - loss: 7.2967e-10 - val_loss: 5.4301e-09
Epoch 497/512
512/512 - 0s - loss: 7.2636e-10 - val_loss: 5.3779e-09
Epoch 498/512
512/512 - 0s - loss: 7.2030e-10 - val_loss: 5.4298e-09
Epoch 499/512
512/512 - 0s - loss: 7.0977e-10 - val_loss: 5.3645e-09
Epoch 500/512
512/512 - 0s - loss: 7.0760e-10 - val_loss: 5.3412e-09
Epoch 501/512
512/512 - 0s - loss: 7.0433e-10 - val_loss: 5.3378e-09
Epoch 502/512
512/512 - 0s - loss: 7.0164e-10 - val_loss: 5.3368e-09
Epoch 503/512
512/512 - 0s - loss: 6.9801e-10 - val_loss: 5.3287e-09
Epoch 504/512
512/512 - 0s - loss: 6.8983e-10 - val_loss: 5.3286e-09
Epoch 505/512
512/512 - 0s - loss: 6.9075e-10 - val_loss: 5.3155e-09
Epoch 506/512
512/512 - 0s - loss: 6.8516e-10 - val_loss: 5.4470e-09
Epoch 507/512
512/512 - 0s - loss: 6.6263e-10 - val_loss: 5.4688e-09
Epoch 508/512
512/512 - 0s - loss: 6.5508e-10 - val_loss: 5.4824e-09
Epoch 509/512
512/512 - 0s - loss: 6.5016e-10 - val_loss: 5.4766e-09
Epoch 510/512
512/512 - 0s - loss: 6.4505e-10 - val_loss: 5.4375e-09
Epoch 511/512
512/512 - 0s - loss: 6.4073e-10 - val_loss: 5.3629e-09
Epoch 512/512
512/512 - 0s - loss: 6.4001e-10 - val_loss: 5.3658e-09
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 2.6499e-10 - val_loss: 1.3887e-10
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3808e-10 - val_loss: 1.3731e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3656e-10 - val_loss: 1.3588e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3508e-10 - val_loss: 1.3434e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3369e-10 - val_loss: 1.3305e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3225e-10 - val_loss: 1.3156e-10
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.3095e-10 - val_loss: 1.3037e-10
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2961e-10 - val_loss: 1.2895e-10
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2840e-10 - val_loss: 1.2776e-10
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2705e-10 - val_loss: 1.2651e-10
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2590e-10 - val_loss: 1.2537e-10
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2467e-10 - val_loss: 1.2412e-10
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2355e-10 - val_loss: 1.2299e-10
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2227e-10 - val_loss: 1.2179e-10
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2122e-10 - val_loss: 1.2075e-10
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.2011e-10 - val_loss: 1.1954e-10
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1902e-10 - val_loss: 1.1860e-10
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1804e-10 - val_loss: 1.1749e-10
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1699e-10 - val_loss: 1.1648e-10
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1601e-10 - val_loss: 1.1541e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1495e-10 - val_loss: 1.1461e-10
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1412e-10 - val_loss: 1.1344e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1294e-10 - val_loss: 1.1256e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1210e-10 - val_loss: 1.1169e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1119e-10 - val_loss: 1.1078e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.1032e-10 - val_loss: 1.0990e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0943e-10 - val_loss: 1.0896e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0853e-10 - val_loss: 1.0822e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0780e-10 - val_loss: 1.0732e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0690e-10 - val_loss: 1.0660e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0616e-10 - val_loss: 1.0585e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0533e-10 - val_loss: 1.0495e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0452e-10 - val_loss: 1.0423e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0379e-10 - val_loss: 1.0334e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0295e-10 - val_loss: 1.0270e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0231e-10 - val_loss: 1.0205e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0159e-10 - val_loss: 1.0126e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0089e-10 - val_loss: 1.0065e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 1.0026e-10 - val_loss: 9.9781e-11
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.9411e-11 - val_loss: 9.9286e-11
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.8848e-11 - val_loss: 9.8599e-11
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.8139e-11 - val_loss: 9.7784e-11
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.7444e-11 - val_loss: 9.7224e-11
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.6846e-11 - val_loss: 9.6663e-11
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.6188e-11 - val_loss: 9.5918e-11
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.5586e-11 - val_loss: 9.5423e-11
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.5039e-11 - val_loss: 9.4696e-11
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.4343e-11 - val_loss: 9.4097e-11
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.3765e-11 - val_loss: 9.3577e-11
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.3172e-11 - val_loss: 9.2920e-11
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.2599e-11 - val_loss: 9.2420e-11
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.2105e-11 - val_loss: 9.1938e-11
Epoch 53/512

Epoch 00053: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.1508e-11 - val_loss: 9.1189e-11
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.0855e-11 - val_loss: 9.0716e-11
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 9.0406e-11 - val_loss: 9.0213e-11
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.9792e-11 - val_loss: 8.9598e-11
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.9319e-11 - val_loss: 8.9196e-11
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.8866e-11 - val_loss: 8.8563e-11
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.8266e-11 - val_loss: 8.8122e-11
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.7818e-11 - val_loss: 8.7699e-11
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.7354e-11 - val_loss: 8.7128e-11
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.6795e-11 - val_loss: 8.6624e-11
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.6340e-11 - val_loss: 8.6214e-11
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.5931e-11 - val_loss: 8.5658e-11
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.5344e-11 - val_loss: 8.5190e-11
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.4914e-11 - val_loss: 8.4799e-11
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.4513e-11 - val_loss: 8.4222e-11
Epoch 68/512

Epoch 00068: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.3982e-11 - val_loss: 8.3802e-11
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.3577e-11 - val_loss: 8.3460e-11
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.3183e-11 - val_loss: 8.2908e-11
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.2658e-11 - val_loss: 8.2591e-11
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.2316e-11 - val_loss: 8.2217e-11
Epoch 73/512

Epoch 00073: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.1929e-11 - val_loss: 8.1682e-11
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.1418e-11 - val_loss: 8.1328e-11
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.1082e-11 - val_loss: 8.1017e-11
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.0715e-11 - val_loss: 8.0557e-11
Epoch 77/512

Epoch 00077: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 8.0224e-11 - val_loss: 8.0049e-11
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.9750e-11 - val_loss: 7.9680e-11
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.9413e-11 - val_loss: 7.9311e-11
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.8963e-11 - val_loss: 7.8843e-11
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.8603e-11 - val_loss: 7.8526e-11
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.8287e-11 - val_loss: 7.8210e-11
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.7919e-11 - val_loss: 7.7724e-11
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.7515e-11 - val_loss: 7.7451e-11
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.7181e-11 - val_loss: 7.7110e-11
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.6891e-11 - val_loss: 7.6670e-11
Epoch 87/512

Epoch 00087: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.6440e-11 - val_loss: 7.6412e-11
Epoch 88/512

Epoch 00088: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.6136e-11 - val_loss: 7.6077e-11
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.5825e-11 - val_loss: 7.5751e-11
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.5427e-11 - val_loss: 7.5272e-11
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.5042e-11 - val_loss: 7.4977e-11
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.4750e-11 - val_loss: 7.4703e-11
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.4460e-11 - val_loss: 7.4225e-11
Epoch 94/512

Epoch 00094: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.4051e-11 - val_loss: 7.3983e-11
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.3773e-11 - val_loss: 7.3747e-11
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.3535e-11 - val_loss: 7.3295e-11
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.3111e-11 - val_loss: 7.3061e-11
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.2848e-11 - val_loss: 7.2760e-11
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.2554e-11 - val_loss: 7.2493e-11
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.2224e-11 - val_loss: 7.2113e-11
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1904e-11 - val_loss: 7.1859e-11
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1642e-11 - val_loss: 7.1604e-11
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1394e-11 - val_loss: 7.1362e-11
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.1074e-11 - val_loss: 7.0954e-11
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.0720e-11 - val_loss: 7.0653e-11
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.0468e-11 - val_loss: 7.0378e-11
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 7.0144e-11 - val_loss: 7.0016e-11
Epoch 108/512

Epoch 00108: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.9814e-11 - val_loss: 6.9784e-11
Epoch 109/512

Epoch 00109: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.9581e-11 - val_loss: 6.9547e-11
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.9359e-11 - val_loss: 6.9318e-11
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.9039e-11 - val_loss: 6.8935e-11
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.8737e-11 - val_loss: 6.8644e-11
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.8487e-11 - val_loss: 6.8454e-11
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.8271e-11 - val_loss: 6.8118e-11
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.7933e-11 - val_loss: 6.7940e-11
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.7732e-11 - val_loss: 6.7681e-11
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.7503e-11 - val_loss: 6.7473e-11
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.7272e-11 - val_loss: 6.7171e-11
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6970e-11 - val_loss: 6.6899e-11
Epoch 120/512

Epoch 00120: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6716e-11 - val_loss: 6.6725e-11
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6577e-11 - val_loss: 6.6639e-11
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6515e-11 - val_loss: 6.6482e-11
Epoch 123/512

Epoch 00123: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6298e-11 - val_loss: 6.6399e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6345e-11 - val_loss: 6.6525e-11
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6493e-11 - val_loss: 6.6698e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6608e-11 - val_loss: 6.6945e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6929e-11 - val_loss: 6.7274e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7361e-11 - val_loss: 6.7970e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8218e-11 - val_loss: 6.8955e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8845e-11 - val_loss: 6.9286e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9803e-11 - val_loss: 7.0904e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1258e-11 - val_loss: 7.2166e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2462e-11 - val_loss: 7.3269e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3770e-11 - val_loss: 7.4743e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4741e-11 - val_loss: 7.4618e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4593e-11 - val_loss: 7.4662e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4863e-11 - val_loss: 7.5275e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5475e-11 - val_loss: 7.5660e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6045e-11 - val_loss: 7.6700e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6926e-11 - val_loss: 7.6854e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6753e-11 - val_loss: 7.6568e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6760e-11 - val_loss: 7.6687e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6955e-11 - val_loss: 7.7277e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6839e-11 - val_loss: 7.6455e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6085e-11 - val_loss: 7.6120e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6143e-11 - val_loss: 7.5907e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6095e-11 - val_loss: 7.6638e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6785e-11 - val_loss: 7.7185e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7751e-11 - val_loss: 7.8475e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8621e-11 - val_loss: 7.9789e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9927e-11 - val_loss: 7.9916e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0092e-11 - val_loss: 8.0503e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0659e-11 - val_loss: 8.0785e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0831e-11 - val_loss: 8.1005e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0673e-11 - val_loss: 8.0488e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0717e-11 - val_loss: 8.0907e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1001e-11 - val_loss: 8.1529e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1482e-11 - val_loss: 8.2102e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2562e-11 - val_loss: 8.2716e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2078e-11 - val_loss: 8.1972e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1566e-11 - val_loss: 8.1655e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1209e-11 - val_loss: 8.1266e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1433e-11 - val_loss: 8.1984e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2599e-11 - val_loss: 8.3546e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3833e-11 - val_loss: 8.4636e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4758e-11 - val_loss: 8.5195e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5234e-11 - val_loss: 8.4736e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4227e-11 - val_loss: 8.4408e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3743e-11 - val_loss: 8.2954e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2225e-11 - val_loss: 8.2075e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1802e-11 - val_loss: 8.1659e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2107e-11 - val_loss: 8.3309e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3186e-11 - val_loss: 8.4041e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4360e-11 - val_loss: 8.5328e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5478e-11 - val_loss: 8.5880e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6362e-11 - val_loss: 8.6826e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6224e-11 - val_loss: 8.5247e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4945e-11 - val_loss: 8.4312e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4093e-11 - val_loss: 8.4019e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3369e-11 - val_loss: 8.3109e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3347e-11 - val_loss: 8.3632e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3437e-11 - val_loss: 8.3587e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4352e-11 - val_loss: 8.5255e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5076e-11 - val_loss: 8.5334e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5985e-11 - val_loss: 8.6647e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7203e-11 - val_loss: 8.7049e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6670e-11 - val_loss: 8.6039e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5473e-11 - val_loss: 8.4779e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4554e-11 - val_loss: 8.4190e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3842e-11 - val_loss: 8.3681e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3801e-11 - val_loss: 8.4168e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3973e-11 - val_loss: 8.4453e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4620e-11 - val_loss: 8.5265e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6184e-11 - val_loss: 8.7351e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8035e-11 - val_loss: 8.8393e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8291e-11 - val_loss: 8.7841e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7368e-11 - val_loss: 8.7097e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6290e-11 - val_loss: 8.5208e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4914e-11 - val_loss: 8.4710e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4309e-11 - val_loss: 8.4059e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3493e-11 - val_loss: 8.3676e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3768e-11 - val_loss: 8.4066e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4153e-11 - val_loss: 8.4340e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4681e-11 - val_loss: 8.5460e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5251e-11 - val_loss: 8.5122e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4697e-11 - val_loss: 8.4886e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4954e-11 - val_loss: 8.4824e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5195e-11 - val_loss: 8.5057e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4696e-11 - val_loss: 8.4789e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4461e-11 - val_loss: 8.4038e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4234e-11 - val_loss: 8.4313e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4363e-11 - val_loss: 8.4052e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3435e-11 - val_loss: 8.3876e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4209e-11 - val_loss: 8.4561e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4407e-11 - val_loss: 8.4153e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3809e-11 - val_loss: 8.3390e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3081e-11 - val_loss: 8.2636e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2258e-11 - val_loss: 8.2251e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2983e-11 - val_loss: 8.4113e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4155e-11 - val_loss: 8.4089e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3676e-11 - val_loss: 8.3506e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3566e-11 - val_loss: 8.4017e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3950e-11 - val_loss: 8.4420e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4877e-11 - val_loss: 8.4978e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4971e-11 - val_loss: 8.4499e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4303e-11 - val_loss: 8.3912e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3142e-11 - val_loss: 8.2891e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2864e-11 - val_loss: 8.2711e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2663e-11 - val_loss: 8.2842e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2417e-11 - val_loss: 8.2657e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2513e-11 - val_loss: 8.2490e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1971e-11 - val_loss: 8.2066e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1451e-11 - val_loss: 8.1568e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1059e-11 - val_loss: 8.0800e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0896e-11 - val_loss: 8.1471e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1013e-11 - val_loss: 8.0762e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0882e-11 - val_loss: 8.1128e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1670e-11 - val_loss: 8.2502e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2527e-11 - val_loss: 8.3145e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2789e-11 - val_loss: 8.2881e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2883e-11 - val_loss: 8.2698e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1660e-11 - val_loss: 8.1187e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0847e-11 - val_loss: 8.0751e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0598e-11 - val_loss: 8.0018e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9640e-11 - val_loss: 7.9127e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9088e-11 - val_loss: 7.9215e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9549e-11 - val_loss: 8.0605e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0801e-11 - val_loss: 8.1206e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2076e-11 - val_loss: 8.2758e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2535e-11 - val_loss: 8.2280e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2502e-11 - val_loss: 8.1749e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1613e-11 - val_loss: 8.1323e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1196e-11 - val_loss: 8.0633e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9571e-11 - val_loss: 7.8983e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8927e-11 - val_loss: 7.9171e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9175e-11 - val_loss: 7.8991e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8624e-11 - val_loss: 7.8820e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8411e-11 - val_loss: 7.7998e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7785e-11 - val_loss: 7.8515e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8834e-11 - val_loss: 7.9292e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9264e-11 - val_loss: 7.9753e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9667e-11 - val_loss: 7.9648e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9463e-11 - val_loss: 7.9652e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9377e-11 - val_loss: 7.9751e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9415e-11 - val_loss: 7.8874e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8612e-11 - val_loss: 7.8329e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8308e-11 - val_loss: 7.8306e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8194e-11 - val_loss: 7.8446e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8013e-11 - val_loss: 7.7954e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8730e-11 - val_loss: 7.9442e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8801e-11 - val_loss: 7.9189e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9363e-11 - val_loss: 7.9191e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9164e-11 - val_loss: 7.8988e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9097e-11 - val_loss: 7.9576e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9641e-11 - val_loss: 7.9952e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9095e-11 - val_loss: 7.7613e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6829e-11 - val_loss: 7.5984e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6003e-11 - val_loss: 7.6227e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6450e-11 - val_loss: 7.7093e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6871e-11 - val_loss: 7.7106e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8046e-11 - val_loss: 7.8952e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8578e-11 - val_loss: 7.8149e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8210e-11 - val_loss: 7.8210e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7653e-11 - val_loss: 7.6940e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6180e-11 - val_loss: 7.6095e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6077e-11 - val_loss: 7.6992e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7109e-11 - val_loss: 7.7254e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6729e-11 - val_loss: 7.5969e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5876e-11 - val_loss: 7.5427e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5111e-11 - val_loss: 7.5099e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4883e-11 - val_loss: 7.5195e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5282e-11 - val_loss: 7.5679e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6095e-11 - val_loss: 7.6563e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6547e-11 - val_loss: 7.5823e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5900e-11 - val_loss: 7.5833e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4933e-11 - val_loss: 7.4269e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3688e-11 - val_loss: 7.3388e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3189e-11 - val_loss: 7.3199e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3310e-11 - val_loss: 7.3381e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2712e-11 - val_loss: 7.2530e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2531e-11 - val_loss: 7.3469e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4600e-11 - val_loss: 7.5678e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5490e-11 - val_loss: 7.5281e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5427e-11 - val_loss: 7.4918e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5299e-11 - val_loss: 7.5576e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4703e-11 - val_loss: 7.3962e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3434e-11 - val_loss: 7.2706e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2629e-11 - val_loss: 7.2625e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2882e-11 - val_loss: 7.3364e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3620e-11 - val_loss: 7.4168e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4728e-11 - val_loss: 7.4174e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3557e-11 - val_loss: 7.3439e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3661e-11 - val_loss: 7.3819e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3452e-11 - val_loss: 7.2946e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2440e-11 - val_loss: 7.1894e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1419e-11 - val_loss: 7.0640e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0883e-11 - val_loss: 7.1608e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1400e-11 - val_loss: 7.1328e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1514e-11 - val_loss: 7.1900e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1936e-11 - val_loss: 7.2903e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3079e-11 - val_loss: 7.3251e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3276e-11 - val_loss: 7.3118e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2518e-11 - val_loss: 7.1889e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1537e-11 - val_loss: 7.1278e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0823e-11 - val_loss: 7.0533e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0487e-11 - val_loss: 7.1209e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1311e-11 - val_loss: 7.1420e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0991e-11 - val_loss: 7.1105e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1728e-11 - val_loss: 7.1825e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2229e-11 - val_loss: 7.2275e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1978e-11 - val_loss: 7.1863e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1773e-11 - val_loss: 7.1794e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2138e-11 - val_loss: 7.2119e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1547e-11 - val_loss: 7.1177e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0896e-11 - val_loss: 7.0780e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0420e-11 - val_loss: 7.0260e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0351e-11 - val_loss: 7.1075e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0678e-11 - val_loss: 7.0982e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0687e-11 - val_loss: 7.0530e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0571e-11 - val_loss: 7.0959e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0531e-11 - val_loss: 7.0191e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9857e-11 - val_loss: 6.9939e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9626e-11 - val_loss: 6.9690e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0247e-11 - val_loss: 7.1315e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1006e-11 - val_loss: 7.0535e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0448e-11 - val_loss: 7.0129e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0422e-11 - val_loss: 7.0512e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0398e-11 - val_loss: 7.0192e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0316e-11 - val_loss: 7.0197e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9319e-11 - val_loss: 6.8777e-11
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8872e-11 - val_loss: 6.8527e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8057e-11 - val_loss: 6.7998e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7730e-11 - val_loss: 6.7532e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7819e-11 - val_loss: 6.8353e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8606e-11 - val_loss: 6.8963e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8689e-11 - val_loss: 6.8352e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8442e-11 - val_loss: 6.8871e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9317e-11 - val_loss: 6.9426e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8792e-11 - val_loss: 6.8198e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7799e-11 - val_loss: 6.7712e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7391e-11 - val_loss: 6.7203e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7592e-11 - val_loss: 6.8066e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7889e-11 - val_loss: 6.8582e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7549e-11 - val_loss: 6.7069e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7037e-11 - val_loss: 6.7742e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7241e-11 - val_loss: 6.7305e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7504e-11 - val_loss: 6.7962e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7692e-11 - val_loss: 6.7671e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7173e-11 - val_loss: 6.6795e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6698e-11 - val_loss: 6.7238e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6840e-11 - val_loss: 6.7057e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7186e-11 - val_loss: 6.7660e-11
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7269e-11 - val_loss: 6.6944e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6907e-11 - val_loss: 6.6606e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6687e-11 - val_loss: 6.7254e-11
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.6904e-11 - val_loss: 6.6351e-11
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.5858e-11 - val_loss: 6.5860e-11
Epoch 378/512

Epoch 00378: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.5632e-11 - val_loss: 6.5501e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.5138e-11 - val_loss: 6.5137e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4887e-11 - val_loss: 6.5211e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5656e-11 - val_loss: 6.6499e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6625e-11 - val_loss: 6.7114e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7603e-11 - val_loss: 6.8035e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7581e-11 - val_loss: 6.7243e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6490e-11 - val_loss: 6.6531e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6346e-11 - val_loss: 6.6093e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5985e-11 - val_loss: 6.6445e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6174e-11 - val_loss: 6.6268e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6412e-11 - val_loss: 6.6047e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5756e-11 - val_loss: 6.5332e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5172e-11 - val_loss: 6.5253e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5413e-11 - val_loss: 6.5416e-11
Epoch 393/512

Epoch 00393: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.5131e-11 - val_loss: 6.4750e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4815e-11 - val_loss: 6.5267e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5133e-11 - val_loss: 6.5457e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.4907e-11 - val_loss: 6.4371e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4370e-11 - val_loss: 6.4793e-11
Epoch 398/512

Epoch 00398: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.4545e-11 - val_loss: 6.4026e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4375e-11 - val_loss: 6.4643e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4796e-11 - val_loss: 6.4959e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.4316e-11 - val_loss: 6.3682e-11
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3979e-11 - val_loss: 6.4418e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4380e-11 - val_loss: 6.4216e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4331e-11 - val_loss: 6.4213e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4265e-11 - val_loss: 6.3970e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3747e-11 - val_loss: 6.3696e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3768e-11 - val_loss: 6.3901e-11
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3385e-11 - val_loss: 6.3462e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3769e-11 - val_loss: 6.4006e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4199e-11 - val_loss: 6.4222e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3817e-11 - val_loss: 6.3422e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.3222e-11 - val_loss: 6.3022e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3220e-11 - val_loss: 6.3148e-11
Epoch 414/512

Epoch 00414: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2858e-11 - val_loss: 6.2679e-11
Epoch 415/512

Epoch 00415: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2578e-11 - val_loss: 6.2539e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2371e-11 - val_loss: 6.2393e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3002e-11 - val_loss: 6.3382e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3582e-11 - val_loss: 6.3752e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3341e-11 - val_loss: 6.2910e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2538e-11 - val_loss: 6.3014e-11
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.2464e-11 - val_loss: 6.1784e-11
Epoch 422/512

Epoch 00422: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.1454e-11 - val_loss: 6.1567e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2052e-11 - val_loss: 6.2642e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2941e-11 - val_loss: 6.3184e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2974e-11 - val_loss: 6.2586e-11
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2298e-11 - val_loss: 6.1826e-11
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1862e-11 - val_loss: 6.1718e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.1696e-11 - val_loss: 6.1546e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1805e-11 - val_loss: 6.2237e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2070e-11 - val_loss: 6.2138e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2194e-11 - val_loss: 6.2143e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1879e-11 - val_loss: 6.1916e-11
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.1614e-11 - val_loss: 6.1282e-11
Epoch 434/512

Epoch 00434: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0576e-11 - val_loss: 6.0482e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0746e-11 - val_loss: 6.0942e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1080e-11 - val_loss: 6.1495e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1588e-11 - val_loss: 6.1946e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1561e-11 - val_loss: 6.1156e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0708e-11 - val_loss: 6.0623e-11
Epoch 440/512

Epoch 00440: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0469e-11 - val_loss: 6.0048e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0248e-11 - val_loss: 6.0543e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0700e-11 - val_loss: 6.0904e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1016e-11 - val_loss: 6.1158e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1553e-11 - val_loss: 6.1629e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1962e-11 - val_loss: 6.1727e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1239e-11 - val_loss: 6.0673e-11
Epoch 447/512

Epoch 00447: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 6.0146e-11 - val_loss: 5.9519e-11
Epoch 448/512

Epoch 00448: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.9233e-11 - val_loss: 5.9009e-11
Epoch 449/512

Epoch 00449: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8759e-11 - val_loss: 5.8715e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8631e-11 - val_loss: 5.8761e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8963e-11 - val_loss: 5.9403e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9542e-11 - val_loss: 5.9832e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9764e-11 - val_loss: 5.9832e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9833e-11 - val_loss: 6.0133e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0354e-11 - val_loss: 6.0755e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0572e-11 - val_loss: 5.9953e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9836e-11 - val_loss: 5.9947e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9545e-11 - val_loss: 5.9444e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9323e-11 - val_loss: 5.8877e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8884e-11 - val_loss: 5.9247e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9197e-11 - val_loss: 5.8930e-11
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8441e-11 - val_loss: 5.8439e-11
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8197e-11 - val_loss: 5.8182e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8462e-11 - val_loss: 5.8406e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8761e-11 - val_loss: 5.9328e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9347e-11 - val_loss: 5.9299e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8973e-11 - val_loss: 5.8825e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8416e-11 - val_loss: 5.8354e-11
Epoch 469/512

Epoch 00469: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.8014e-11 - val_loss: 5.7707e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7335e-11 - val_loss: 5.7093e-11
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7170e-11 - val_loss: 5.7004e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6622e-11 - val_loss: 5.6910e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6685e-11 - val_loss: 5.6657e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7004e-11 - val_loss: 5.7609e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7980e-11 - val_loss: 5.8564e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8436e-11 - val_loss: 5.8521e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8459e-11 - val_loss: 5.8308e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8099e-11 - val_loss: 5.8256e-11
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8222e-11 - val_loss: 5.8490e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8330e-11 - val_loss: 5.7757e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7557e-11 - val_loss: 5.7299e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7149e-11 - val_loss: 5.7286e-11
Epoch 483/512

Epoch 00483: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.7217e-11 - val_loss: 5.6636e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6638e-11 - val_loss: 5.6746e-11
Epoch 485/512

Epoch 00485: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6565e-11 - val_loss: 5.6592e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6673e-11 - val_loss: 5.7453e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7370e-11 - val_loss: 5.7440e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7552e-11 - val_loss: 5.7558e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7388e-11 - val_loss: 5.7340e-11
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6836e-11 - val_loss: 5.6541e-11
Epoch 491/512

Epoch 00491: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.6109e-11 - val_loss: 5.6146e-11
Epoch 492/512

Epoch 00492: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5603e-11 - val_loss: 5.5387e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5729e-11 - val_loss: 5.6596e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6526e-11 - val_loss: 5.6869e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6995e-11 - val_loss: 5.7298e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6920e-11 - val_loss: 5.6624e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6713e-11 - val_loss: 5.6756e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6514e-11 - val_loss: 5.6269e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6233e-11 - val_loss: 5.6135e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6196e-11 - val_loss: 5.6295e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6293e-11 - val_loss: 5.6395e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6448e-11 - val_loss: 5.6722e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6835e-11 - val_loss: 5.6509e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5959e-11 - val_loss: 5.5794e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5378e-11 - val_loss: 5.5224e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5152e-11 - val_loss: 5.5091e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5069e-11 - val_loss: 5.5359e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-00005-sigmoid-RMS-33/multiplication_multiplication_weights.h5
512/512 - 0s - loss: 5.5059e-11 - val_loss: 5.4727e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4796e-11 - val_loss: 5.5356e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5580e-11 - val_loss: 5.6266e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6205e-11 - val_loss: 5.5925e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6105e-11 - val_loss: 5.5952e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.670 | eve: 10.070 | bob: 9.440Epoch   0:   0% | abe: 9.542 | eve: 10.023 | bob: 9.365Epoch   0:   1% | abe: 9.460 | eve: 9.988 | bob: 9.316Epoch   0:   2% | abe: 9.413 | eve: 9.981 | bob: 9.294Epoch   0:   3% | abe: 9.357 | eve: 9.963 | bob: 9.256Epoch   0:   3% | abe: 9.326 | eve: 9.952 | bob: 9.240Epoch   0:   4% | abe: 9.285 | eve: 9.945 | bob: 9.212Epoch   0:   5% | abe: 9.267 | eve: 9.937 | bob: 9.202Epoch   0:   6% | abe: 9.254 | eve: 9.938 | bob: 9.197Epoch   0:   7% | abe: 9.240 | eve: 9.942 | bob: 9.189Epoch   0:   7% | abe: 9.231 | eve: 9.940 | bob: 9.186Epoch   0:   8% | abe: 9.218 | eve: 9.938 | bob: 9.177Epoch   0:   9% | abe: 9.203 | eve: 9.943 | bob: 9.166Epoch   0:  10% | abe: 9.194 | eve: 9.940 | bob: 9.160Epoch   0:  10% | abe: 9.186 | eve: 9.945 | bob: 9.155Epoch   0:  11% | abe: 9.178 | eve: 9.937 | bob: 9.150Epoch   0:  12% | abe: 9.170 | eve: 9.935 | bob: 9.145Epoch   0:  13% | abe: 9.168 | eve: 9.934 | bob: 9.145Epoch   0:  14% | abe: 9.165 | eve: 9.933 | bob: 9.145Epoch   0:  14% | abe: 9.159 | eve: 9.930 | bob: 9.140Epoch   0:  15% | abe: 9.157 | eve: 9.929 | bob: 9.141Epoch   0:  16% | abe: 9.150 | eve: 9.930 | bob: 9.136Epoch   0:  17% | abe: 9.145 | eve: 9.927 | bob: 9.132Epoch   0:  17% | abe: 9.141 | eve: 9.926 | bob: 9.129Epoch   0:  18% | abe: 9.139 | eve: 9.924 | bob: 9.129Epoch   0:  19% | abe: 9.136 | eve: 9.928 | bob: 9.129Epoch   0:  20% | abe: 9.135 | eve: 9.929 | bob: 9.128Epoch   0:  21% | abe: 9.131 | eve: 9.930 | bob: 9.126Epoch   0:  21% | abe: 9.130 | eve: 9.930 | bob: 9.127Epoch   0:  22% | abe: 9.128 | eve: 9.930 | bob: 9.126Epoch   0:  23% | abe: 9.126 | eve: 9.935 | bob: 9.125Epoch   0:  24% | abe: 9.124 | eve: 9.935 | bob: 9.124Epoch   0:  25% | abe: 9.124 | eve: 9.938 | bob: 9.125Epoch   0:  25% | abe: 9.123 | eve: 9.942 | bob: 9.126Epoch   0:  26% | abe: 9.119 | eve: 9.944 | bob: 9.123Epoch   0:  27% | abe: 9.117 | eve: 9.945 | bob: 9.122Epoch   0:  28% | abe: 9.115 | eve: 9.947 | bob: 9.121Epoch   0:  28% | abe: 9.114 | eve: 9.949 | bob: 9.121Epoch   0:  29% | abe: 9.112 | eve: 9.952 | bob: 9.120Epoch   0:  30% | abe: 9.110 | eve: 9.954 | bob: 9.119Epoch   0:  31% | abe: 9.109 | eve: 9.957 | bob: 9.118Epoch   0:  32% | abe: 9.107 | eve: 9.958 | bob: 9.117Epoch   0:  32% | abe: 9.107 | eve: 9.962 | bob: 9.118Epoch   0:  33% | abe: 9.106 | eve: 9.965 | bob: 9.118Epoch   0:  34% | abe: 9.104 | eve: 9.967 | bob: 9.116Epoch   0:  35% | abe: 9.102 | eve: 9.970 | bob: 9.115Epoch   0:  35% | abe: 9.101 | eve: 9.973 | bob: 9.115Epoch   0:  36% | abe: 9.099 | eve: 9.974 | bob: 9.114Epoch   0:  37% | abe: 9.098 | eve: 9.979 | bob: 9.114Epoch   0:  38% | abe: 9.098 | eve: 9.980 | bob: 9.114Epoch   0:  39% | abe: 9.096 | eve: 9.982 | bob: 9.113Epoch   0:  39% | abe: 9.097 | eve: 9.985 | bob: 9.114Epoch   0:  40% | abe: 9.096 | eve: 9.988 | bob: 9.114Epoch   0:  41% | abe: 9.096 | eve: 9.990 | bob: 9.115Epoch   0:  42% | abe: 9.096 | eve: 9.992 | bob: 9.116Epoch   0:  42% | abe: 9.096 | eve: 9.995 | bob: 9.116Epoch   0:  43% | abe: 9.094 | eve: 9.997 | bob: 9.115Epoch   0:  44% | abe: 9.093 | eve: 9.999 | bob: 9.115Epoch   0:  45% | abe: 9.092 | eve: 10.002 | bob: 9.114Epoch   0:  46% | abe: 9.091 | eve: 10.005 | bob: 9.114Epoch   0:  46% | abe: 9.091 | eve: 10.008 | bob: 9.114Epoch   0:  47% | abe: 9.090 | eve: 10.011 | bob: 9.114Epoch   0:  48% | abe: 9.089 | eve: 10.013 | bob: 9.114Epoch   0:  49% | abe: 9.089 | eve: 10.016 | bob: 9.114Epoch   0:  50% | abe: 9.088 | eve: 10.019 | bob: 9.113Epoch   0:  50% | abe: 9.087 | eve: 10.021 | bob: 9.113Epoch   0:  51% | abe: 9.086 | eve: 10.024 | bob: 9.113Epoch   0:  52% | abe: 9.085 | eve: 10.025 | bob: 9.113Epoch   0:  53% | abe: 9.085 | eve: 10.026 | bob: 9.113Epoch   0:  53% | abe: 9.084 | eve: 10.030 | bob: 9.113Epoch   0:  54% | abe: 9.085 | eve: 10.032 | bob: 9.114Epoch   0:  55% | abe: 9.084 | eve: 10.036 | bob: 9.114Epoch   0:  56% | abe: 9.083 | eve: 10.038 | bob: 9.113Epoch   0:  57% | abe: 9.082 | eve: 10.041 | bob: 9.113Epoch   0:  57% | abe: 9.081 | eve: 10.044 | bob: 9.113Epoch   0:  58% | abe: 9.080 | eve: 10.046 | bob: 9.113Epoch   0:  59% | abe: 9.079 | eve: 10.048 | bob: 9.113Epoch   0:  60% | abe: 9.078 | eve: 10.051 | bob: 9.112Epoch   0:  60% | abe: 9.079 | eve: 10.053 | bob: 9.114Epoch   0:  61% | abe: 9.078 | eve: 10.056 | bob: 9.113Epoch   0:  62% | abe: 9.078 | eve: 10.058 | bob: 9.114Epoch   0:  63% | abe: 9.077 | eve: 10.059 | bob: 9.114Epoch   0:  64% | abe: 9.077 | eve: 10.061 | bob: 9.114Epoch   0:  64% | abe: 9.076 | eve: 10.062 | bob: 9.114Epoch   0:  65% | abe: 9.076 | eve: 10.065 | bob: 9.114Epoch   0:  66% | abe: 9.075 | eve: 10.067 | bob: 9.114Epoch   0:  67% | abe: 9.075 | eve: 10.069 | bob: 9.114Epoch   0:  67% | abe: 9.074 | eve: 10.072 | bob: 9.114Epoch   0:  68% | abe: 9.074 | eve: 10.074 | bob: 9.115Epoch   0:  69% | abe: 9.073 | eve: 10.076 | bob: 9.115Epoch   0:  70% | abe: 9.073 | eve: 10.078 | bob: 9.115Epoch   0:  71% | abe: 9.072 | eve: 10.080 | bob: 9.116Epoch   0:  71% | abe: 9.072 | eve: 10.082 | bob: 9.116Epoch   0:  72% | abe: 9.071 | eve: 10.085 | bob: 9.115Epoch   0:  73% | abe: 9.070 | eve: 10.087 | bob: 9.115Epoch   0:  74% | abe: 9.070 | eve: 10.089 | bob: 9.115Epoch   0:  75% | abe: 9.070 | eve: 10.091 | bob: 9.116Epoch   0:  75% | abe: 9.069 | eve: 10.093 | bob: 9.116Epoch   0:  76% | abe: 9.068 | eve: 10.094 | bob: 9.116Epoch   0:  77% | abe: 9.068 | eve: 10.096 | bob: 9.116Epoch   0:  78% | abe: 9.067 | eve: 10.099 | bob: 9.116Epoch   0:  78% | abe: 9.067 | eve: 10.100 | bob: 9.117Epoch   0:  79% | abe: 9.066 | eve: 10.102 | bob: 9.117Epoch   0:  80% | abe: 9.066 | eve: 10.103 | bob: 9.116Epoch   0:  81% | abe: 9.065 | eve: 10.104 | bob: 9.117Epoch   0:  82% | abe: 9.065 | eve: 10.106 | bob: 9.117Epoch   0:  82% | abe: 9.065 | eve: 10.108 | bob: 9.118Epoch   0:  83% | abe: 9.064 | eve: 10.110 | bob: 9.118Epoch   0:  84% | abe: 9.064 | eve: 10.111 | bob: 9.118Epoch   0:  85% | abe: 9.064 | eve: 10.112 | bob: 9.119Epoch   0:  85% | abe: 9.064 | eve: 10.114 | bob: 9.119Epoch   0:  86% | abe: 9.063 | eve: 10.116 | bob: 9.119Epoch   0:  87% | abe: 9.063 | eve: 10.118 | bob: 9.119Epoch   0:  88% | abe: 9.063 | eve: 10.120 | bob: 9.120Epoch   0:  89% | abe: 9.063 | eve: 10.122 | bob: 9.121Epoch   0:  89% | abe: 9.063 | eve: 10.123 | bob: 9.122Epoch   0:  90% | abe: 9.062 | eve: 10.125 | bob: 9.122Epoch   0:  91% | abe: 9.062 | eve: 10.126 | bob: 9.122Epoch   0:  92% | abe: 9.062 | eve: 10.127 | bob: 9.123Epoch   0:  92% | abe: 9.061 | eve: 10.128 | bob: 9.123Epoch   0:  93% | abe: 9.061 | eve: 10.130 | bob: 9.123Epoch   0:  94% | abe: 9.061 | eve: 10.132 | bob: 9.124Epoch   0:  95% | abe: 9.061 | eve: 10.133 | bob: 9.125Epoch   0:  96% | abe: 9.061 | eve: 10.134 | bob: 9.125Epoch   0:  96% | abe: 9.060 | eve: 10.135 | bob: 9.125Epoch   0:  97% | abe: 9.060 | eve: 10.136 | bob: 9.126Epoch   0:  98% | abe: 9.059 | eve: 10.138 | bob: 9.126Epoch   0:  99% | abe: 9.059 | eve: 10.139 | bob: 9.126
New best Bob loss 9.12603507112749 at epoch 0
Epoch   1:   0% | abe: 8.994 | eve: 10.353 | bob: 9.132Epoch   1:   0% | abe: 9.019 | eve: 10.311 | bob: 9.161Epoch   1:   1% | abe: 9.024 | eve: 10.324 | bob: 9.169Epoch   1:   2% | abe: 9.032 | eve: 10.319 | bob: 9.182Epoch   1:   3% | abe: 9.029 | eve: 10.307 | bob: 9.182Epoch   1:   3% | abe: 9.023 | eve: 10.308 | bob: 9.176Epoch   1:   4% | abe: 9.025 | eve: 10.297 | bob: 9.179Epoch   1:   5% | abe: 9.016 | eve: 10.291 | bob: 9.170Epoch   1:   6% | abe: 9.011 | eve: 10.284 | bob: 9.164Epoch   1:   7% | abe: 9.011 | eve: 10.274 | bob: 9.165Epoch   1:   7% | abe: 9.012 | eve: 10.273 | bob: 9.166Epoch   1:   8% | abe: 9.011 | eve: 10.280 | bob: 9.166Epoch   1:   9% | abe: 9.009 | eve: 10.279 | bob: 9.164Epoch   1:  10% | abe: 9.007 | eve: 10.279 | bob: 9.163Epoch   1:  10% | abe: 9.011 | eve: 10.279 | bob: 9.168Epoch   1:  11% | abe: 9.010 | eve: 10.280 | bob: 9.169Epoch   1:  12% | abe: 9.013 | eve: 10.282 | bob: 9.173Epoch   1:  13% | abe: 9.010 | eve: 10.282 | bob: 9.170Epoch   1:  14% | abe: 9.007 | eve: 10.281 | bob: 9.166Epoch   1:  14% | abe: 9.006 | eve: 10.282 | bob: 9.165Epoch   1:  15% | abe: 9.007 | eve: 10.282 | bob: 9.167Epoch   1:  16% | abe: 9.006 | eve: 10.281 | bob: 9.166Epoch   1:  17% | abe: 9.008 | eve: 10.282 | bob: 9.167Epoch   1:  17% | abe: 9.010 | eve: 10.283 | bob: 9.170Epoch   1:  18% | abe: 9.010 | eve: 10.285 | bob: 9.170Epoch   1:  19% | abe: 9.007 | eve: 10.285 | bob: 9.168Epoch   1:  20% | abe: 9.007 | eve: 10.284 | bob: 9.167Epoch   1:  21% | abe: 9.005 | eve: 10.284 | bob: 9.166Epoch   1:  21% | abe: 9.008 | eve: 10.284 | bob: 9.169Epoch   1:  22% | abe: 9.007 | eve: 10.286 | bob: 9.168Epoch   1:  23% | abe: 9.006 | eve: 10.286 | bob: 9.168Epoch   1:  24% | abe: 9.007 | eve: 10.284 | bob: 9.169Epoch   1:  25% | abe: 9.005 | eve: 10.285 | bob: 9.167Epoch   1:  25% | abe: 9.003 | eve: 10.289 | bob: 9.166Epoch   1:  26% | abe: 9.003 | eve: 10.289 | bob: 9.166Epoch   1:  27% | abe: 9.003 | eve: 10.289 | bob: 9.166Epoch   1:  28% | abe: 9.004 | eve: 10.291 | bob: 9.168Epoch   1:  28% | abe: 9.005 | eve: 10.292 | bob: 9.169Epoch   1:  29% | abe: 9.005 | eve: 10.293 | bob: 9.169Epoch   1:  30% | abe: 9.004 | eve: 10.294 | bob: 9.169Epoch   1:  31% | abe: 9.002 | eve: 10.296 | bob: 9.168Epoch   1:  32% | abe: 9.002 | eve: 10.295 | bob: 9.168Epoch   1:  32% | abe: 9.002 | eve: 10.297 | bob: 9.169Epoch   1:  33% | abe: 9.004 | eve: 10.295 | bob: 9.172Epoch   1:  34% | abe: 9.003 | eve: 10.296 | bob: 9.171Epoch   1:  35% | abe: 9.002 | eve: 10.296 | bob: 9.171Epoch   1:  35% | abe: 9.003 | eve: 10.297 | bob: 9.172Epoch   1:  36% | abe: 9.004 | eve: 10.296 | bob: 9.174Epoch   1:  37% | abe: 9.003 | eve: 10.296 | bob: 9.174Epoch   1:  38% | abe: 9.004 | eve: 10.296 | bob: 9.176Epoch   1:  39% | abe: 9.004 | eve: 10.298 | bob: 9.177Epoch   1:  39% | abe: 9.005 | eve: 10.298 | bob: 9.178Epoch   1:  40% | abe: 9.005 | eve: 10.299 | bob: 9.179Epoch   1:  41% | abe: 9.003 | eve: 10.298 | bob: 9.178Epoch   1:  42% | abe: 9.003 | eve: 10.300 | bob: 9.177Epoch   1:  42% | abe: 9.003 | eve: 10.299 | bob: 9.178Epoch   1:  43% | abe: 9.003 | eve: 10.299 | bob: 9.178Epoch   1:  44% | abe: 9.002 | eve: 10.299 | bob: 9.178Epoch   1:  45% | abe: 9.003 | eve: 10.298 | bob: 9.179Epoch   1:  46% | abe: 9.003 | eve: 10.299 | bob: 9.181Epoch   1:  46% | abe: 9.003 | eve: 10.299 | bob: 9.181Epoch   1:  47% | abe: 9.003 | eve: 10.301 | bob: 9.181Epoch   1:  48% | abe: 9.004 | eve: 10.301 | bob: 9.183Epoch   1:  49% | abe: 9.003 | eve: 10.301 | bob: 9.183Epoch   1:  50% | abe: 9.003 | eve: 10.301 | bob: 9.183Epoch   1:  50% | abe: 9.002 | eve: 10.303 | bob: 9.182Epoch   1:  51% | abe: 9.001 | eve: 10.303 | bob: 9.182Epoch   1:  52% | abe: 9.002 | eve: 10.302 | bob: 9.184Epoch   1:  53% | abe: 9.001 | eve: 10.302 | bob: 9.184Epoch   1:  53% | abe: 9.001 | eve: 10.302 | bob: 9.184Epoch   1:  54% | abe: 9.000 | eve: 10.302 | bob: 9.184Epoch   1:  55% | abe: 9.000 | eve: 10.303 | bob: 9.184Epoch   1:  56% | abe: 8.999 | eve: 10.302 | bob: 9.183Epoch   1:  57% | abe: 8.999 | eve: 10.303 | bob: 9.184Epoch   1:  57% | abe: 9.000 | eve: 10.302 | bob: 9.185Epoch   1:  58% | abe: 8.999 | eve: 10.302 | bob: 9.185Epoch   1:  59% | abe: 9.000 | eve: 10.302 | bob: 9.186Epoch   1:  60% | abe: 8.999 | eve: 10.301 | bob: 9.187Epoch   1:  60% | abe: 8.999 | eve: 10.300 | bob: 9.187Epoch   1:  61% | abe: 8.998 | eve: 10.300 | bob: 9.187Epoch   1:  62% | abe: 8.998 | eve: 10.300 | bob: 9.187Epoch   1:  63% | abe: 8.997 | eve: 10.301 | bob: 9.186Epoch   1:  64% | abe: 8.997 | eve: 10.301 | bob: 9.187Epoch   1:  64% | abe: 8.997 | eve: 10.301 | bob: 9.187Epoch   1:  65% | abe: 8.997 | eve: 10.301 | bob: 9.187Epoch   1:  66% | abe: 8.997 | eve: 10.300 | bob: 9.188Epoch   1:  67% | abe: 8.997 | eve: 10.300 | bob: 9.188Epoch   1:  67% | abe: 8.996 | eve: 10.302 | bob: 9.188Epoch   1:  68% | abe: 8.996 | eve: 10.302 | bob: 9.188Epoch   1:  69% | abe: 8.995 | eve: 10.302 | bob: 9.188Epoch   1:  70% | abe: 8.995 | eve: 10.302 | bob: 9.188Epoch   1:  71% | abe: 8.994 | eve: 10.301 | bob: 9.188Epoch   1:  71% | abe: 8.994 | eve: 10.302 | bob: 9.188Epoch   1:  72% | abe: 8.993 | eve: 10.302 | bob: 9.187Epoch   1:  73% | abe: 8.993 | eve: 10.302 | bob: 9.188Epoch   1:  74% | abe: 8.993 | eve: 10.303 | bob: 9.189Epoch   1:  75% | abe: 8.993 | eve: 10.303 | bob: 9.188Epoch   1:  75% | abe: 8.993 | eve: 10.303 | bob: 9.189Epoch   1:  76% | abe: 8.992 | eve: 10.303 | bob: 9.189Epoch   1:  77% | abe: 8.992 | eve: 10.304 | bob: 9.189Epoch   1:  78% | abe: 8.992 | eve: 10.303 | bob: 9.189Epoch   1:  78% | abe: 8.992 | eve: 10.303 | bob: 9.190Epoch   1:  79% | abe: 8.993 | eve: 10.303 | bob: 9.191Epoch   1:  80% | abe: 8.993 | eve: 10.302 | bob: 9.192Epoch   1:  81% | abe: 8.992 | eve: 10.303 | bob: 9.192Epoch   1:  82% | abe: 8.992 | eve: 10.303 | bob: 9.192Epoch   1:  82% | abe: 8.991 | eve: 10.303 | bob: 9.192Epoch   1:  83% | abe: 8.991 | eve: 10.302 | bob: 9.192Epoch   1:  84% | abe: 8.990 | eve: 10.302 | bob: 9.192Epoch   1:  85% | abe: 8.990 | eve: 10.302 | bob: 9.191Epoch   1:  85% | abe: 8.989 | eve: 10.302 | bob: 9.191Epoch   1:  86% | abe: 8.989 | eve: 10.302 | bob: 9.191Epoch   1:  87% | abe: 8.989 | eve: 10.303 | bob: 9.191Epoch   1:  88% | abe: 8.988 | eve: 10.302 | bob: 9.191Epoch   1:  89% | abe: 8.988 | eve: 10.302 | bob: 9.192Epoch   1:  89% | abe: 8.988 | eve: 10.301 | bob: 9.192Epoch   1:  90% | abe: 8.988 | eve: 10.301 | bob: 9.193Epoch   1:  91% | abe: 8.988 | eve: 10.301 | bob: 9.193Epoch   1:  92% | abe: 8.988 | eve: 10.300 | bob: 9.194Epoch   1:  92% | abe: 8.988 | eve: 10.300 | bob: 9.194Epoch   1:  93% | abe: 8.988 | eve: 10.301 | bob: 9.194Epoch   1:  94% | abe: 8.987 | eve: 10.300 | bob: 9.194Epoch   1:  95% | abe: 8.987 | eve: 10.300 | bob: 9.194Epoch   1:  96% | abe: 8.987 | eve: 10.301 | bob: 9.195Epoch   1:  96% | abe: 8.986 | eve: 10.301 | bob: 9.195Epoch   1:  97% | abe: 8.986 | eve: 10.300 | bob: 9.195Epoch   1:  98% | abe: 8.986 | eve: 10.300 | bob: 9.195Epoch   1:  99% | abe: 8.986 | eve: 10.300 | bob: 9.195Epoch   2:   0% | abe: 8.898 | eve: 10.282 | bob: 9.157Epoch   2:   0% | abe: 8.933 | eve: 10.312 | bob: 9.196Epoch   2:   1% | abe: 8.929 | eve: 10.285 | bob: 9.193Epoch   2:   2% | abe: 8.933 | eve: 10.283 | bob: 9.198Epoch   2:   3% | abe: 8.931 | eve: 10.292 | bob: 9.200Epoch   2:   3% | abe: 8.936 | eve: 10.293 | bob: 9.205Epoch   2:   4% | abe: 8.940 | eve: 10.295 | bob: 9.209Epoch   2:   5% | abe: 8.940 | eve: 10.290 | bob: 9.209Epoch   2:   6% | abe: 8.946 | eve: 10.294 | bob: 9.215Epoch   2:   7% | abe: 8.947 | eve: 10.293 | bob: 9.217Epoch   2:   7% | abe: 8.952 | eve: 10.298 | bob: 9.222Epoch   2:   8% | abe: 8.954 | eve: 10.304 | bob: 9.225Epoch   2:   9% | abe: 8.950 | eve: 10.299 | bob: 9.221Epoch   2:  10% | abe: 8.949 | eve: 10.296 | bob: 9.221Epoch   2:  10% | abe: 8.950 | eve: 10.298 | bob: 9.223Epoch   2:  11% | abe: 8.954 | eve: 10.299 | bob: 9.227Epoch   2:  12% | abe: 8.951 | eve: 10.302 | bob: 9.223Epoch   2:  13% | abe: 8.947 | eve: 10.301 | bob: 9.220Epoch   2:  14% | abe: 8.950 | eve: 10.304 | bob: 9.224Epoch   2:  14% | abe: 8.951 | eve: 10.304 | bob: 9.225Epoch   2:  15% | abe: 8.948 | eve: 10.302 | bob: 9.221Epoch   2:  16% | abe: 8.946 | eve: 10.299 | bob: 9.218Epoch   2:  17% | abe: 8.945 | eve: 10.299 | bob: 9.217Epoch   2:  17% | abe: 8.946 | eve: 10.296 | bob: 9.218Epoch   2:  18% | abe: 8.946 | eve: 10.296 | bob: 9.219Epoch   2:  19% | abe: 8.947 | eve: 10.294 | bob: 9.220Epoch   2:  20% | abe: 8.945 | eve: 10.294 | bob: 9.219Epoch   2:  21% | abe: 8.945 | eve: 10.294 | bob: 9.219Epoch   2:  21% | abe: 8.945 | eve: 10.294 | bob: 9.219Epoch   2:  22% | abe: 8.945 | eve: 10.289 | bob: 9.219Epoch   2:  23% | abe: 8.945 | eve: 10.289 | bob: 9.219Epoch   2:  24% | abe: 8.945 | eve: 10.289 | bob: 9.220Epoch   2:  25% | abe: 8.944 | eve: 10.290 | bob: 9.220Epoch   2:  25% | abe: 8.944 | eve: 10.289 | bob: 9.221Epoch   2:  26% | abe: 8.943 | eve: 10.290 | bob: 9.221Epoch   2:  27% | abe: 8.944 | eve: 10.289 | bob: 9.221Epoch   2:  28% | abe: 8.944 | eve: 10.288 | bob: 9.222Epoch   2:  28% | abe: 8.944 | eve: 10.290 | bob: 9.222Epoch   2:  29% | abe: 8.943 | eve: 10.291 | bob: 9.222Epoch   2:  30% | abe: 8.943 | eve: 10.290 | bob: 9.222Epoch   2:  31% | abe: 8.942 | eve: 10.288 | bob: 9.221Epoch   2:  32% | abe: 8.942 | eve: 10.289 | bob: 9.221Epoch   2:  32% | abe: 8.941 | eve: 10.289 | bob: 9.221Epoch   2:  33% | abe: 8.940 | eve: 10.289 | bob: 9.220Epoch   2:  34% | abe: 8.940 | eve: 10.288 | bob: 9.220Epoch   2:  35% | abe: 8.940 | eve: 10.288 | bob: 9.220Epoch   2:  35% | abe: 8.940 | eve: 10.287 | bob: 9.221Epoch   2:  36% | abe: 8.941 | eve: 10.287 | bob: 9.223Epoch   2:  37% | abe: 8.941 | eve: 10.287 | bob: 9.224Epoch   2:  38% | abe: 8.941 | eve: 10.288 | bob: 9.224Epoch   2:  39% | abe: 8.941 | eve: 10.288 | bob: 9.224Epoch   2:  39% | abe: 8.939 | eve: 10.289 | bob: 9.223Epoch   2:  40% | abe: 8.939 | eve: 10.291 | bob: 9.223Epoch   2:  41% | abe: 8.939 | eve: 10.291 | bob: 9.224Epoch   2:  42% | abe: 8.939 | eve: 10.290 | bob: 9.225Epoch   2:  42% | abe: 8.940 | eve: 10.290 | bob: 9.226Epoch   2:  43% | abe: 8.940 | eve: 10.291 | bob: 9.226Epoch   2:  44% | abe: 8.938 | eve: 10.290 | bob: 9.225Epoch   2:  45% | abe: 8.938 | eve: 10.289 | bob: 9.225Epoch   2:  46% | abe: 8.937 | eve: 10.288 | bob: 9.224Epoch   2:  46% | abe: 8.936 | eve: 10.288 | bob: 9.224Epoch   2:  47% | abe: 8.936 | eve: 10.288 | bob: 9.224Epoch   2:  48% | abe: 8.936 | eve: 10.288 | bob: 9.224Epoch   2:  49% | abe: 8.936 | eve: 10.288 | bob: 9.224Epoch   2:  50% | abe: 8.936 | eve: 10.288 | bob: 9.224Epoch   2:  50% | abe: 8.935 | eve: 10.288 | bob: 9.224Epoch   2:  51% | abe: 8.935 | eve: 10.287 | bob: 9.225Epoch   2:  52% | abe: 8.935 | eve: 10.287 | bob: 9.225Epoch   2:  53% | abe: 8.935 | eve: 10.287 | bob: 9.225Epoch   2:  53% | abe: 8.935 | eve: 10.288 | bob: 9.226Epoch   2:  54% | abe: 8.935 | eve: 10.288 | bob: 9.227Epoch   2:  55% | abe: 8.935 | eve: 10.287 | bob: 9.227Epoch   2:  56% | abe: 8.934 | eve: 10.288 | bob: 9.227Epoch   2:  57% | abe: 8.935 | eve: 10.289 | bob: 9.228Epoch   2:  57% | abe: 8.934 | eve: 10.289 | bob: 9.227Epoch   2:  58% | abe: 8.934 | eve: 10.288 | bob: 9.227Epoch   2:  59% | abe: 8.934 | eve: 10.288 | bob: 9.227Epoch   2:  60% | abe: 8.934 | eve: 10.288 | bob: 9.228Epoch   2:  60% | abe: 8.934 | eve: 10.288 | bob: 9.229Epoch   2:  61% | abe: 8.933 | eve: 10.288 | bob: 9.229Epoch   2:  62% | abe: 8.933 | eve: 10.287 | bob: 9.229Epoch   2:  63% | abe: 8.933 | eve: 10.287 | bob: 9.229Epoch   2:  64% | abe: 8.932 | eve: 10.287 | bob: 9.228Epoch   2:  64% | abe: 8.931 | eve: 10.286 | bob: 9.228Epoch   2:  65% | abe: 8.932 | eve: 10.285 | bob: 9.229Epoch   2:  66% | abe: 8.932 | eve: 10.285 | bob: 9.229Epoch   2:  67% | abe: 8.931 | eve: 10.285 | bob: 9.229Epoch   2:  67% | abe: 8.930 | eve: 10.285 | bob: 9.228Epoch   2:  68% | abe: 8.931 | eve: 10.286 | bob: 9.229Epoch   2:  69% | abe: 8.930 | eve: 10.287 | bob: 9.229Epoch   2:  70% | abe: 8.931 | eve: 10.286 | bob: 9.230Epoch   2:  71% | abe: 8.931 | eve: 10.286 | bob: 9.231Epoch   2:  71% | abe: 8.931 | eve: 10.286 | bob: 9.232Epoch   2:  72% | abe: 8.930 | eve: 10.286 | bob: 9.231Epoch   2:  73% | abe: 8.931 | eve: 10.285 | bob: 9.232Epoch   2:  74% | abe: 8.931 | eve: 10.285 | bob: 9.232Epoch   2:  75% | abe: 8.931 | eve: 10.285 | bob: 9.233Epoch   2:  75% | abe: 8.930 | eve: 10.285 | bob: 9.233Epoch   2:  76% | abe: 8.930 | eve: 10.285 | bob: 9.233Epoch   2:  77% | abe: 8.930 | eve: 10.285 | bob: 9.233Epoch   2:  78% | abe: 8.930 | eve: 10.284 | bob: 9.234Epoch   2:  78% | abe: 8.929 | eve: 10.284 | bob: 9.234Epoch   2:  79% | abe: 8.930 | eve: 10.283 | bob: 9.235Epoch   2:  80% | abe: 8.929 | eve: 10.282 | bob: 9.235Epoch   2:  81% | abe: 8.929 | eve: 10.281 | bob: 9.235Epoch   2:  82% | abe: 8.929 | eve: 10.282 | bob: 9.235Epoch   2:  82% | abe: 8.929 | eve: 10.282 | bob: 9.235Epoch   2:  83% | abe: 8.928 | eve: 10.281 | bob: 9.235Epoch   2:  84% | abe: 8.928 | eve: 10.282 | bob: 9.235Epoch   2:  85% | abe: 8.927 | eve: 10.282 | bob: 9.235Epoch   2:  85% | abe: 8.927 | eve: 10.281 | bob: 9.235Epoch   2:  86% | abe: 8.927 | eve: 10.280 | bob: 9.235Epoch   2:  87% | abe: 8.926 | eve: 10.281 | bob: 9.235Epoch   2:  88% | abe: 8.926 | eve: 10.280 | bob: 9.235Epoch   2:  89% | abe: 8.926 | eve: 10.280 | bob: 9.235Epoch   2:  89% | abe: 8.926 | eve: 10.280 | bob: 9.236Epoch   2:  90% | abe: 8.926 | eve: 10.279 | bob: 9.236Epoch   2:  91% | abe: 8.926 | eve: 10.280 | bob: 9.236Epoch   2:  92% | abe: 8.925 | eve: 10.280 | bob: 9.236Epoch   2:  92% | abe: 8.925 | eve: 10.279 | bob: 9.236Epoch   2:  93% | abe: 8.924 | eve: 10.279 | bob: 9.236Epoch   2:  94% | abe: 8.924 | eve: 10.278 | bob: 9.236Epoch   2:  95% | abe: 8.925 | eve: 10.277 | bob: 9.237Epoch   2:  96% | abe: 8.925 | eve: 10.277 | bob: 9.238Epoch   2:  96% | abe: 8.924 | eve: 10.277 | bob: 9.238Epoch   2:  97% | abe: 8.924 | eve: 10.277 | bob: 9.237Epoch   2:  98% | abe: 8.923 | eve: 10.277 | bob: 9.237Epoch   2:  99% | abe: 8.923 | eve: 10.277 | bob: 9.238Epoch   3:   0% | abe: 8.873 | eve: 10.242 | bob: 9.245Epoch   3:   0% | abe: 8.886 | eve: 10.251 | bob: 9.253Epoch   3:   1% | abe: 8.862 | eve: 10.265 | bob: 9.222Epoch   3:   2% | abe: 8.875 | eve: 10.277 | bob: 9.237Epoch   3:   3% | abe: 8.890 | eve: 10.266 | bob: 9.256Epoch   3:   3% | abe: 8.890 | eve: 10.267 | bob: 9.258Epoch   3:   4% | abe: 8.898 | eve: 10.265 | bob: 9.267Epoch   3:   5% | abe: 8.893 | eve: 10.269 | bob: 9.258Epoch   3:   6% | abe: 8.895 | eve: 10.268 | bob: 9.262Epoch   3:   7% | abe: 8.893 | eve: 10.268 | bob: 9.259Epoch   3:   7% | abe: 8.892 | eve: 10.271 | bob: 9.258Epoch   3:   8% | abe: 8.885 | eve: 10.276 | bob: 9.248Epoch   3:   9% | abe: 8.887 | eve: 10.273 | bob: 9.252Epoch   3:  10% | abe: 8.888 | eve: 10.268 | bob: 9.253Epoch   3:  10% | abe: 8.888 | eve: 10.262 | bob: 9.255Epoch   3:  11% | abe: 8.886 | eve: 10.258 | bob: 9.252Epoch   3:  12% | abe: 8.886 | eve: 10.259 | bob: 9.252Epoch   3:  13% | abe: 8.889 | eve: 10.257 | bob: 9.255Epoch   3:  14% | abe: 8.886 | eve: 10.253 | bob: 9.252Epoch   3:  14% | abe: 8.887 | eve: 10.253 | bob: 9.254Epoch   3:  15% | abe: 8.887 | eve: 10.252 | bob: 9.255Epoch   3:  16% | abe: 8.887 | eve: 10.250 | bob: 9.256Epoch   3:  17% | abe: 8.890 | eve: 10.251 | bob: 9.261Epoch   3:  17% | abe: 8.890 | eve: 10.252 | bob: 9.262Epoch   3:  18% | abe: 8.888 | eve: 10.250 | bob: 9.260Epoch   3:  19% | abe: 8.885 | eve: 10.249 | bob: 9.256Epoch   3:  20% | abe: 8.884 | eve: 10.248 | bob: 9.256Epoch   3:  21% | abe: 8.883 | eve: 10.248 | bob: 9.256Epoch   3:  21% | abe: 8.882 | eve: 10.250 | bob: 9.256Epoch   3:  22% | abe: 8.883 | eve: 10.250 | bob: 9.257Epoch   3:  23% | abe: 8.881 | eve: 10.252 | bob: 9.254Epoch   3:  24% | abe: 8.880 | eve: 10.249 | bob: 9.254Epoch   3:  25% | abe: 8.879 | eve: 10.250 | bob: 9.253Epoch   3:  25% | abe: 8.877 | eve: 10.251 | bob: 9.250Epoch   3:  26% | abe: 8.876 | eve: 10.249 | bob: 9.250Epoch   3:  27% | abe: 8.874 | eve: 10.248 | bob: 9.247Epoch   3:  28% | abe: 8.875 | eve: 10.249 | bob: 9.249Epoch   3:  28% | abe: 8.874 | eve: 10.250 | bob: 9.249Epoch   3:  29% | abe: 8.873 | eve: 10.251 | bob: 9.248Epoch   3:  30% | abe: 8.872 | eve: 10.250 | bob: 9.246Epoch   3:  31% | abe: 8.871 | eve: 10.250 | bob: 9.245Epoch   3:  32% | abe: 8.870 | eve: 10.251 | bob: 9.244Epoch   3:  32% | abe: 8.867 | eve: 10.251 | bob: 9.242Epoch   3:  33% | abe: 8.866 | eve: 10.250 | bob: 9.241Epoch   3:  34% | abe: 8.867 | eve: 10.251 | bob: 9.242Epoch   3:  35% | abe: 8.866 | eve: 10.250 | bob: 9.241Epoch   3:  35% | abe: 8.865 | eve: 10.250 | bob: 9.240Epoch   3:  36% | abe: 8.864 | eve: 10.250 | bob: 9.240Epoch   3:  37% | abe: 8.864 | eve: 10.251 | bob: 9.240Epoch   3:  38% | abe: 8.864 | eve: 10.250 | bob: 9.241Epoch   3:  39% | abe: 8.864 | eve: 10.250 | bob: 9.241Epoch   3:  39% | abe: 8.863 | eve: 10.249 | bob: 9.241Epoch   3:  40% | abe: 8.863 | eve: 10.249 | bob: 9.241Epoch   3:  41% | abe: 8.863 | eve: 10.247 | bob: 9.241Epoch   3:  42% | abe: 8.863 | eve: 10.246 | bob: 9.241Epoch   3:  42% | abe: 8.862 | eve: 10.245 | bob: 9.241Epoch   3:  43% | abe: 8.862 | eve: 10.245 | bob: 9.242Epoch   3:  44% | abe: 8.862 | eve: 10.245 | bob: 9.241Epoch   3:  45% | abe: 8.862 | eve: 10.245 | bob: 9.243Epoch   3:  46% | abe: 8.861 | eve: 10.246 | bob: 9.242Epoch   3:  46% | abe: 8.861 | eve: 10.247 | bob: 9.242Epoch   3:  47% | abe: 8.861 | eve: 10.247 | bob: 9.243Epoch   3:  48% | abe: 8.861 | eve: 10.247 | bob: 9.245Epoch   3:  49% | abe: 8.860 | eve: 10.248 | bob: 9.243Epoch   3:  50% | abe: 8.861 | eve: 10.249 | bob: 9.245Epoch   3:  50% | abe: 8.860 | eve: 10.250 | bob: 9.244Epoch   3:  51% | abe: 8.859 | eve: 10.249 | bob: 9.244Epoch   3:  52% | abe: 8.858 | eve: 10.248 | bob: 9.243Epoch   3:  53% | abe: 8.858 | eve: 10.248 | bob: 9.243Epoch   3:  53% | abe: 8.858 | eve: 10.248 | bob: 9.244Epoch   3:  54% | abe: 8.859 | eve: 10.247 | bob: 9.244Epoch   3:  55% | abe: 8.858 | eve: 10.246 | bob: 9.244Epoch   3:  56% | abe: 8.858 | eve: 10.246 | bob: 9.244Epoch   3:  57% | abe: 8.857 | eve: 10.245 | bob: 9.244Epoch   3:  57% | abe: 8.857 | eve: 10.245 | bob: 9.244Epoch   3:  58% | abe: 8.856 | eve: 10.245 | bob: 9.244Epoch   3:  59% | abe: 8.856 | eve: 10.245 | bob: 9.244Epoch   3:  60% | abe: 8.856 | eve: 10.245 | bob: 9.244Epoch   3:  60% | abe: 8.856 | eve: 10.245 | bob: 9.244Epoch   3:  61% | abe: 8.855 | eve: 10.246 | bob: 9.244Epoch   3:  62% | abe: 8.855 | eve: 10.245 | bob: 9.244Epoch   3:  63% | abe: 8.854 | eve: 10.246 | bob: 9.244Epoch   3:  64% | abe: 8.855 | eve: 10.247 | bob: 9.245Epoch   3:  64% | abe: 8.854 | eve: 10.246 | bob: 9.245Epoch   3:  65% | abe: 8.855 | eve: 10.246 | bob: 9.246Epoch   3:  66% | abe: 8.855 | eve: 10.246 | bob: 9.247Epoch   3:  67% | abe: 8.855 | eve: 10.245 | bob: 9.247Epoch   3:  67% | abe: 8.855 | eve: 10.245 | bob: 9.247Epoch   3:  68% | abe: 8.854 | eve: 10.244 | bob: 9.247Epoch   3:  69% | abe: 8.853 | eve: 10.245 | bob: 9.247Epoch   3:  70% | abe: 8.853 | eve: 10.244 | bob: 9.247Epoch   3:  71% | abe: 8.853 | eve: 10.244 | bob: 9.247Epoch   3:  71% | abe: 8.852 | eve: 10.244 | bob: 9.246Epoch   3:  72% | abe: 8.852 | eve: 10.244 | bob: 9.247Epoch   3:  73% | abe: 8.851 | eve: 10.244 | bob: 9.247Epoch   3:  74% | abe: 8.851 | eve: 10.244 | bob: 9.247Epoch   3:  75% | abe: 8.851 | eve: 10.244 | bob: 9.247Epoch   3:  75% | abe: 8.851 | eve: 10.244 | bob: 9.247Epoch   3:  76% | abe: 8.850 | eve: 10.244 | bob: 9.247Epoch   3:  77% | abe: 8.850 | eve: 10.244 | bob: 9.247Epoch   3:  78% | abe: 8.849 | eve: 10.244 | bob: 9.247Epoch   3:  78% | abe: 8.849 | eve: 10.243 | bob: 9.247Epoch   3:  79% | abe: 8.848 | eve: 10.243 | bob: 9.247Epoch   3:  80% | abe: 8.848 | eve: 10.244 | bob: 9.247Epoch   3:  81% | abe: 8.847 | eve: 10.244 | bob: 9.247Epoch   3:  82% | abe: 8.847 | eve: 10.244 | bob: 9.247Epoch   3:  82% | abe: 8.847 | eve: 10.244 | bob: 9.247Epoch   3:  83% | abe: 8.847 | eve: 10.245 | bob: 9.247Epoch   3:  84% | abe: 8.847 | eve: 10.245 | bob: 9.248Epoch   3:  85% | abe: 8.847 | eve: 10.245 | bob: 9.248Epoch   3:  85% | abe: 8.846 | eve: 10.245 | bob: 9.248Epoch   3:  86% | abe: 8.846 | eve: 10.246 | bob: 9.248Epoch   3:  87% | abe: 8.845 | eve: 10.246 | bob: 9.248Epoch   3:  88% | abe: 8.845 | eve: 10.245 | bob: 9.248Epoch   3:  89% | abe: 8.844 | eve: 10.246 | bob: 9.248Epoch   3:  89% | abe: 8.844 | eve: 10.245 | bob: 9.247Epoch   3:  90% | abe: 8.843 | eve: 10.245 | bob: 9.247Epoch   3:  91% | abe: 8.843 | eve: 10.245 | bob: 9.248Epoch   3:  92% | abe: 8.842 | eve: 10.244 | bob: 9.247Epoch   3:  92% | abe: 8.842 | eve: 10.244 | bob: 9.247Epoch   3:  93% | abe: 8.842 | eve: 10.245 | bob: 9.248Epoch   3:  94% | abe: 8.841 | eve: 10.245 | bob: 9.248Epoch   3:  95% | abe: 8.841 | eve: 10.245 | bob: 9.248Epoch   3:  96% | abe: 8.841 | eve: 10.245 | bob: 9.248Epoch   3:  96% | abe: 8.841 | eve: 10.245 | bob: 9.248Epoch   3:  97% | abe: 8.841 | eve: 10.244 | bob: 9.248Epoch   3:  98% | abe: 8.840 | eve: 10.245 | bob: 9.249Epoch   3:  99% | abe: 8.840 | eve: 10.245 | bob: 9.248Epoch   4:   0% | abe: 8.767 | eve: 10.298 | bob: 9.214Epoch   4:   0% | abe: 8.748 | eve: 10.316 | bob: 9.192Epoch   4:   1% | abe: 8.745 | eve: 10.293 | bob: 9.187Epoch   4:   2% | abe: 8.764 | eve: 10.266 | bob: 9.208Epoch   4:   3% | abe: 8.766 | eve: 10.258 | bob: 9.210Epoch   4:   3% | abe: 8.768 | eve: 10.262 | bob: 9.214Epoch   4:   4% | abe: 8.768 | eve: 10.264 | bob: 9.214Epoch   4:   5% | abe: 8.775 | eve: 10.256 | bob: 9.224Epoch   4:   6% | abe: 8.776 | eve: 10.260 | bob: 9.226Epoch   4:   7% | abe: 8.779 | eve: 10.264 | bob: 9.230Epoch   4:   7% | abe: 8.778 | eve: 10.251 | bob: 9.232Epoch   4:   8% | abe: 8.778 | eve: 10.251 | bob: 9.231Epoch   4:   9% | abe: 8.768 | eve: 10.244 | bob: 9.218Epoch   4:  10% | abe: 8.770 | eve: 10.243 | bob: 9.221Epoch   4:  10% | abe: 8.773 | eve: 10.243 | bob: 9.226Epoch   4:  11% | abe: 8.772 | eve: 10.242 | bob: 9.225Epoch   4:  12% | abe: 8.772 | eve: 10.238 | bob: 9.225Epoch   4:  13% | abe: 8.771 | eve: 10.242 | bob: 9.224Epoch   4:  14% | abe: 8.773 | eve: 10.246 | bob: 9.227Epoch   4:  14% | abe: 8.773 | eve: 10.245 | bob: 9.228Epoch   4:  15% | abe: 8.772 | eve: 10.245 | bob: 9.228Epoch   4:  16% | abe: 8.774 | eve: 10.247 | bob: 9.230Epoch   4:  17% | abe: 8.773 | eve: 10.246 | bob: 9.230Epoch   4:  17% | abe: 8.773 | eve: 10.247 | bob: 9.230Epoch   4:  18% | abe: 8.772 | eve: 10.249 | bob: 9.229Epoch   4:  19% | abe: 8.770 | eve: 10.247 | bob: 9.227Epoch   4:  20% | abe: 8.769 | eve: 10.248 | bob: 9.228Epoch   4:  21% | abe: 8.769 | eve: 10.247 | bob: 9.228Epoch   4:  21% | abe: 8.769 | eve: 10.246 | bob: 9.230Epoch   4:  22% | abe: 8.768 | eve: 10.248 | bob: 9.229Epoch   4:  23% | abe: 8.768 | eve: 10.247 | bob: 9.230Epoch   4:  24% | abe: 8.768 | eve: 10.250 | bob: 9.229Epoch   4:  25% | abe: 8.767 | eve: 10.251 | bob: 9.228Epoch   4:  25% | abe: 8.768 | eve: 10.250 | bob: 9.231Epoch   4:  26% | abe: 8.768 | eve: 10.249 | bob: 9.232Epoch   4:  27% | abe: 8.769 | eve: 10.248 | bob: 9.233Epoch   4:  28% | abe: 8.767 | eve: 10.250 | bob: 9.231Epoch   4:  28% | abe: 8.765 | eve: 10.250 | bob: 9.229Epoch   4:  29% | abe: 8.765 | eve: 10.248 | bob: 9.230Epoch   4:  30% | abe: 8.765 | eve: 10.247 | bob: 9.229Epoch   4:  31% | abe: 8.764 | eve: 10.247 | bob: 9.229Epoch   4:  32% | abe: 8.763 | eve: 10.247 | bob: 9.228Epoch   4:  32% | abe: 8.762 | eve: 10.246 | bob: 9.228Epoch   4:  33% | abe: 8.762 | eve: 10.246 | bob: 9.227Epoch   4:  34% | abe: 8.761 | eve: 10.246 | bob: 9.227Epoch   4:  35% | abe: 8.761 | eve: 10.247 | bob: 9.227Epoch   4:  35% | abe: 8.760 | eve: 10.247 | bob: 9.227Epoch   4:  36% | abe: 8.759 | eve: 10.246 | bob: 9.227Epoch   4:  37% | abe: 8.759 | eve: 10.246 | bob: 9.227Epoch   4:  38% | abe: 8.759 | eve: 10.245 | bob: 9.227Epoch   4:  39% | abe: 8.757 | eve: 10.246 | bob: 9.225Epoch   4:  39% | abe: 8.755 | eve: 10.246 | bob: 9.223Epoch   4:  40% | abe: 8.755 | eve: 10.244 | bob: 9.224Epoch   4:  41% | abe: 8.755 | eve: 10.243 | bob: 9.225Epoch   4:  42% | abe: 8.755 | eve: 10.243 | bob: 9.225Epoch   4:  42% | abe: 8.757 | eve: 10.242 | bob: 9.227Epoch   4:  43% | abe: 8.757 | eve: 10.241 | bob: 9.228Epoch   4:  44% | abe: 8.756 | eve: 10.241 | bob: 9.228Epoch   4:  45% | abe: 8.756 | eve: 10.241 | bob: 9.228Epoch   4:  46% | abe: 8.755 | eve: 10.241 | bob: 9.226Epoch   4:  46% | abe: 8.755 | eve: 10.240 | bob: 9.227Epoch   4:  47% | abe: 8.755 | eve: 10.240 | bob: 9.227Epoch   4:  48% | abe: 8.754 | eve: 10.239 | bob: 9.226Epoch   4:  49% | abe: 8.753 | eve: 10.238 | bob: 9.226Epoch   4:  50% | abe: 8.753 | eve: 10.237 | bob: 9.226Epoch   4:  50% | abe: 8.753 | eve: 10.237 | bob: 9.228Epoch   4:  51% | abe: 8.753 | eve: 10.237 | bob: 9.228Epoch   4:  52% | abe: 8.753 | eve: 10.237 | bob: 9.228Epoch   4:  53% | abe: 8.752 | eve: 10.236 | bob: 9.228Epoch   4:  53% | abe: 8.753 | eve: 10.235 | bob: 9.229Epoch   4:  54% | abe: 8.753 | eve: 10.234 | bob: 9.229Epoch   4:  55% | abe: 8.751 | eve: 10.234 | bob: 9.227Epoch   4:  56% | abe: 8.751 | eve: 10.234 | bob: 9.228Epoch   4:  57% | abe: 8.751 | eve: 10.233 | bob: 9.228Epoch   4:  57% | abe: 8.750 | eve: 10.233 | bob: 9.228Epoch   4:  58% | abe: 8.750 | eve: 10.232 | bob: 9.228Epoch   4:  59% | abe: 8.750 | eve: 10.230 | bob: 9.228Epoch   4:  60% | abe: 8.749 | eve: 10.230 | bob: 9.227Epoch   4:  60% | abe: 8.748 | eve: 10.229 | bob: 9.227Epoch   4:  61% | abe: 8.748 | eve: 10.228 | bob: 9.228Epoch   4:  62% | abe: 8.748 | eve: 10.228 | bob: 9.228Epoch   4:  63% | abe: 8.749 | eve: 10.228 | bob: 9.229Epoch   4:  64% | abe: 8.748 | eve: 10.228 | bob: 9.229Epoch   4:  64% | abe: 8.748 | eve: 10.227 | bob: 9.229Epoch   4:  65% | abe: 8.748 | eve: 10.226 | bob: 9.229Epoch   4:  66% | abe: 8.747 | eve: 10.225 | bob: 9.230Epoch   4:  67% | abe: 8.747 | eve: 10.226 | bob: 9.230Epoch   4:  67% | abe: 8.747 | eve: 10.225 | bob: 9.230Epoch   4:  68% | abe: 8.747 | eve: 10.224 | bob: 9.231Epoch   4:  69% | abe: 8.747 | eve: 10.224 | bob: 9.232Epoch   4:  70% | abe: 8.746 | eve: 10.224 | bob: 9.230Epoch   4:  71% | abe: 8.745 | eve: 10.224 | bob: 9.230Epoch   4:  71% | abe: 8.745 | eve: 10.224 | bob: 9.230Epoch   4:  72% | abe: 8.744 | eve: 10.224 | bob: 9.230Epoch   4:  73% | abe: 8.743 | eve: 10.224 | bob: 9.229Epoch   4:  74% | abe: 8.743 | eve: 10.224 | bob: 9.229Epoch   4:  75% | abe: 8.742 | eve: 10.224 | bob: 9.229Epoch   4:  75% | abe: 8.742 | eve: 10.225 | bob: 9.229Epoch   4:  76% | abe: 8.742 | eve: 10.224 | bob: 9.229Epoch   4:  77% | abe: 8.741 | eve: 10.224 | bob: 9.228Epoch   4:  78% | abe: 8.740 | eve: 10.224 | bob: 9.228Epoch   4:  78% | abe: 8.740 | eve: 10.224 | bob: 9.228Epoch   4:  79% | abe: 8.739 | eve: 10.224 | bob: 9.228Epoch   4:  80% | abe: 8.740 | eve: 10.224 | bob: 9.229Epoch   4:  81% | abe: 8.740 | eve: 10.224 | bob: 9.230Epoch   4:  82% | abe: 8.740 | eve: 10.223 | bob: 9.230Epoch   4:  82% | abe: 8.740 | eve: 10.223 | bob: 9.230Epoch   4:  83% | abe: 8.739 | eve: 10.223 | bob: 9.230Epoch   4:  84% | abe: 8.739 | eve: 10.222 | bob: 9.230Epoch   4:  85% | abe: 8.738 | eve: 10.222 | bob: 9.230Epoch   4:  85% | abe: 8.738 | eve: 10.222 | bob: 9.230Epoch   4:  86% | abe: 8.738 | eve: 10.220 | bob: 9.230Epoch   4:  87% | abe: 8.737 | eve: 10.220 | bob: 9.230Epoch   4:  88% | abe: 8.737 | eve: 10.219 | bob: 9.230Epoch   4:  89% | abe: 8.736 | eve: 10.219 | bob: 9.230Epoch   4:  89% | abe: 8.736 | eve: 10.219 | bob: 9.230Epoch   4:  90% | abe: 8.735 | eve: 10.218 | bob: 9.229Epoch   4:  91% | abe: 8.735 | eve: 10.218 | bob: 9.229Epoch   4:  92% | abe: 8.735 | eve: 10.219 | bob: 9.229Epoch   4:  92% | abe: 8.734 | eve: 10.219 | bob: 9.229Epoch   4:  93% | abe: 8.734 | eve: 10.219 | bob: 9.229Epoch   4:  94% | abe: 8.733 | eve: 10.219 | bob: 9.229Epoch   4:  95% | abe: 8.733 | eve: 10.219 | bob: 9.229Epoch   4:  96% | abe: 8.732 | eve: 10.219 | bob: 9.228Epoch   4:  96% | abe: 8.732 | eve: 10.219 | bob: 9.229Epoch   4:  97% | abe: 8.732 | eve: 10.219 | bob: 9.229Epoch   4:  98% | abe: 8.732 | eve: 10.219 | bob: 9.229Epoch   4:  99% | abe: 8.731 | eve: 10.219 | bob: 9.229Epoch   5:   0% | abe: 8.667 | eve: 10.252 | bob: 9.228Epoch   5:   0% | abe: 8.686 | eve: 10.240 | bob: 9.250Epoch   5:   1% | abe: 8.683 | eve: 10.239 | bob: 9.238Epoch   5:   2% | abe: 8.680 | eve: 10.226 | bob: 9.233Epoch   5:   3% | abe: 8.676 | eve: 10.226 | bob: 9.227Epoch   5:   3% | abe: 8.679 | eve: 10.236 | bob: 9.229Epoch   5:   4% | abe: 8.686 | eve: 10.223 | bob: 9.237Epoch   5:   5% | abe: 8.675 | eve: 10.223 | bob: 9.221Epoch   5:   6% | abe: 8.671 | eve: 10.221 | bob: 9.216Epoch   5:   7% | abe: 8.667 | eve: 10.209 | bob: 9.209Epoch   5:   7% | abe: 8.667 | eve: 10.212 | bob: 9.209Epoch   5:   8% | abe: 8.666 | eve: 10.214 | bob: 9.208Epoch   5:   9% | abe: 8.663 | eve: 10.215 | bob: 9.207Epoch   5:  10% | abe: 8.661 | eve: 10.215 | bob: 9.205Epoch   5:  10% | abe: 8.663 | eve: 10.212 | bob: 9.209Epoch   5:  11% | abe: 8.667 | eve: 10.210 | bob: 9.216Epoch   5:  12% | abe: 8.666 | eve: 10.215 | bob: 9.214Epoch   5:  13% | abe: 8.667 | eve: 10.213 | bob: 9.216Epoch   5:  14% | abe: 8.663 | eve: 10.212 | bob: 9.211Epoch   5:  14% | abe: 8.667 | eve: 10.213 | bob: 9.217Epoch   5:  15% | abe: 8.669 | eve: 10.212 | bob: 9.220Epoch   5:  16% | abe: 8.668 | eve: 10.214 | bob: 9.220Epoch   5:  17% | abe: 8.669 | eve: 10.213 | bob: 9.222Epoch   5:  17% | abe: 8.668 | eve: 10.214 | bob: 9.221Epoch   5:  18% | abe: 8.668 | eve: 10.214 | bob: 9.223Epoch   5:  19% | abe: 8.666 | eve: 10.212 | bob: 9.220Epoch   5:  20% | abe: 8.663 | eve: 10.212 | bob: 9.216Epoch   5:  21% | abe: 8.663 | eve: 10.212 | bob: 9.216Epoch   5:  21% | abe: 8.662 | eve: 10.211 | bob: 9.216Epoch   5:  22% | abe: 8.663 | eve: 10.212 | bob: 9.218Epoch   5:  23% | abe: 8.662 | eve: 10.209 | bob: 9.217Epoch   5:  24% | abe: 8.662 | eve: 10.207 | bob: 9.218Epoch   5:  25% | abe: 8.660 | eve: 10.207 | bob: 9.216Epoch   5:  25% | abe: 8.658 | eve: 10.206 | bob: 9.214Epoch   5:  26% | abe: 8.657 | eve: 10.205 | bob: 9.213Epoch   5:  27% | abe: 8.659 | eve: 10.207 | bob: 9.215Epoch   5:  28% | abe: 8.659 | eve: 10.205 | bob: 9.216Epoch   5:  28% | abe: 8.658 | eve: 10.205 | bob: 9.216Epoch   5:  29% | abe: 8.657 | eve: 10.206 | bob: 9.215Epoch   5:  30% | abe: 8.657 | eve: 10.206 | bob: 9.216Epoch   5:  31% | abe: 8.656 | eve: 10.205 | bob: 9.214Epoch   5:  32% | abe: 8.656 | eve: 10.205 | bob: 9.215Epoch   5:  32% | abe: 8.656 | eve: 10.207 | bob: 9.216Epoch   5:  33% | abe: 8.655 | eve: 10.206 | bob: 9.215Epoch   5:  34% | abe: 8.654 | eve: 10.205 | bob: 9.215Epoch   5:  35% | abe: 8.654 | eve: 10.206 | bob: 9.215Epoch   5:  35% | abe: 8.654 | eve: 10.205 | bob: 9.215Epoch   5:  36% | abe: 8.654 | eve: 10.205 | bob: 9.215Epoch   5:  37% | abe: 8.653 | eve: 10.204 | bob: 9.214Epoch   5:  38% | abe: 8.652 | eve: 10.202 | bob: 9.213Epoch   5:  39% | abe: 8.652 | eve: 10.203 | bob: 9.214Epoch   5:  39% | abe: 8.652 | eve: 10.203 | bob: 9.213Epoch   5:  40% | abe: 8.650 | eve: 10.203 | bob: 9.212Epoch   5:  41% | abe: 8.651 | eve: 10.202 | bob: 9.214Epoch   5:  42% | abe: 8.651 | eve: 10.203 | bob: 9.214Epoch   5:  42% | abe: 8.649 | eve: 10.202 | bob: 9.213Epoch   5:  43% | abe: 8.648 | eve: 10.200 | bob: 9.212Epoch   5:  44% | abe: 8.649 | eve: 10.201 | bob: 9.213Epoch   5:  45% | abe: 8.649 | eve: 10.201 | bob: 9.214Epoch   5:  46% | abe: 8.649 | eve: 10.202 | bob: 9.215Epoch   5:  46% | abe: 8.649 | eve: 10.202 | bob: 9.215Epoch   5:  47% | abe: 8.648 | eve: 10.201 | bob: 9.215Epoch   5:  48% | abe: 8.647 | eve: 10.202 | bob: 9.214Epoch   5:  49% | abe: 8.647 | eve: 10.202 | bob: 9.215Epoch   5:  50% | abe: 8.647 | eve: 10.201 | bob: 9.216Epoch   5:  50% | abe: 8.646 | eve: 10.200 | bob: 9.215Epoch   5:  51% | abe: 8.645 | eve: 10.199 | bob: 9.214Epoch   5:  52% | abe: 8.645 | eve: 10.199 | bob: 9.214Epoch   5:  53% | abe: 8.645 | eve: 10.198 | bob: 9.214Epoch   5:  53% | abe: 8.645 | eve: 10.198 | bob: 9.214Epoch   5:  54% | abe: 8.643 | eve: 10.198 | bob: 9.212Epoch   5:  55% | abe: 8.643 | eve: 10.199 | bob: 9.212Epoch   5:  56% | abe: 8.642 | eve: 10.198 | bob: 9.211Epoch   5:  57% | abe: 8.641 | eve: 10.199 | bob: 9.211Epoch   5:  57% | abe: 8.640 | eve: 10.199 | bob: 9.210Epoch   5:  58% | abe: 8.640 | eve: 10.200 | bob: 9.211Epoch   5:  59% | abe: 8.640 | eve: 10.199 | bob: 9.210Epoch   5:  60% | abe: 8.639 | eve: 10.198 | bob: 9.210Epoch   5:  60% | abe: 8.638 | eve: 10.198 | bob: 9.209Epoch   5:  61% | abe: 8.638 | eve: 10.198 | bob: 9.210Epoch   5:  62% | abe: 8.638 | eve: 10.199 | bob: 9.210Epoch   5:  63% | abe: 8.637 | eve: 10.198 | bob: 9.210Epoch   5:  64% | abe: 8.637 | eve: 10.198 | bob: 9.210Epoch   5:  64% | abe: 8.637 | eve: 10.198 | bob: 9.210Epoch   5:  65% | abe: 8.636 | eve: 10.198 | bob: 9.209Epoch   5:  66% | abe: 8.635 | eve: 10.198 | bob: 9.209Epoch   5:  67% | abe: 8.635 | eve: 10.198 | bob: 9.209Epoch   5:  67% | abe: 8.634 | eve: 10.197 | bob: 9.209Epoch   5:  68% | abe: 8.633 | eve: 10.197 | bob: 9.208Epoch   5:  69% | abe: 8.633 | eve: 10.197 | bob: 9.208Epoch   5:  70% | abe: 8.631 | eve: 10.197 | bob: 9.206Epoch   5:  71% | abe: 8.632 | eve: 10.196 | bob: 9.207Epoch   5:  71% | abe: 8.631 | eve: 10.196 | bob: 9.207Epoch   5:  72% | abe: 8.630 | eve: 10.196 | bob: 9.206Epoch   5:  73% | abe: 8.630 | eve: 10.196 | bob: 9.206Epoch   5:  74% | abe: 8.629 | eve: 10.197 | bob: 9.206Epoch   5:  75% | abe: 8.628 | eve: 10.197 | bob: 9.205Epoch   5:  75% | abe: 8.627 | eve: 10.197 | bob: 9.204Epoch   5:  76% | abe: 8.627 | eve: 10.197 | bob: 9.204Epoch   5:  77% | abe: 8.626 | eve: 10.197 | bob: 9.204Epoch   5:  78% | abe: 8.626 | eve: 10.197 | bob: 9.203Epoch   5:  78% | abe: 8.626 | eve: 10.197 | bob: 9.204Epoch   5:  79% | abe: 8.625 | eve: 10.196 | bob: 9.203Epoch   5:  80% | abe: 8.625 | eve: 10.196 | bob: 9.204Epoch   5:  81% | abe: 8.624 | eve: 10.197 | bob: 9.203Epoch   5:  82% | abe: 8.624 | eve: 10.197 | bob: 9.203Epoch   5:  82% | abe: 8.623 | eve: 10.196 | bob: 9.203Epoch   5:  83% | abe: 8.623 | eve: 10.196 | bob: 9.203Epoch   5:  84% | abe: 8.623 | eve: 10.196 | bob: 9.203Epoch   5:  85% | abe: 8.622 | eve: 10.196 | bob: 9.202Epoch   5:  85% | abe: 8.622 | eve: 10.196 | bob: 9.203Epoch   5:  86% | abe: 8.621 | eve: 10.196 | bob: 9.203Epoch   5:  87% | abe: 8.621 | eve: 10.196 | bob: 9.203Epoch   5:  88% | abe: 8.620 | eve: 10.196 | bob: 9.202Epoch   5:  89% | abe: 8.620 | eve: 10.197 | bob: 9.203Epoch   5:  89% | abe: 8.620 | eve: 10.197 | bob: 9.204Epoch   5:  90% | abe: 8.620 | eve: 10.197 | bob: 9.204Epoch   5:  91% | abe: 8.619 | eve: 10.197 | bob: 9.203Epoch   5:  92% | abe: 8.618 | eve: 10.196 | bob: 9.203Epoch   5:  92% | abe: 8.618 | eve: 10.196 | bob: 9.203Epoch   5:  93% | abe: 8.617 | eve: 10.196 | bob: 9.202Epoch   5:  94% | abe: 8.616 | eve: 10.196 | bob: 9.202Epoch   5:  95% | abe: 8.616 | eve: 10.196 | bob: 9.202Epoch   5:  96% | abe: 8.616 | eve: 10.196 | bob: 9.202Epoch   5:  96% | abe: 8.615 | eve: 10.196 | bob: 9.202Epoch   5:  97% | abe: 8.615 | eve: 10.196 | bob: 9.202Epoch   5:  98% | abe: 8.614 | eve: 10.196 | bob: 9.202Epoch   5:  99% | abe: 8.614 | eve: 10.196 | bob: 9.202Epoch   6:   0% | abe: 8.557 | eve: 10.219 | bob: 9.207Epoch   6:   0% | abe: 8.545 | eve: 10.174 | bob: 9.171Epoch   6:   1% | abe: 8.550 | eve: 10.204 | bob: 9.181Epoch   6:   2% | abe: 8.552 | eve: 10.179 | bob: 9.188Epoch   6:   3% | abe: 8.552 | eve: 10.173 | bob: 9.191Epoch   6:   3% | abe: 8.544 | eve: 10.173 | bob: 9.182Epoch   6:   4% | abe: 8.550 | eve: 10.188 | bob: 9.191Epoch   6:   5% | abe: 8.555 | eve: 10.182 | bob: 9.194Epoch   6:   6% | abe: 8.548 | eve: 10.186 | bob: 9.189Epoch   6:   7% | abe: 8.546 | eve: 10.187 | bob: 9.187Epoch   6:   7% | abe: 8.546 | eve: 10.194 | bob: 9.186Epoch   6:   8% | abe: 8.546 | eve: 10.190 | bob: 9.188Epoch   6:   9% | abe: 8.543 | eve: 10.192 | bob: 9.184Epoch   6:  10% | abe: 8.546 | eve: 10.190 | bob: 9.187Epoch   6:  10% | abe: 8.546 | eve: 10.188 | bob: 9.189Epoch   6:  11% | abe: 8.548 | eve: 10.189 | bob: 9.193Epoch   6:  12% | abe: 8.547 | eve: 10.187 | bob: 9.192Epoch   6:  13% | abe: 8.547 | eve: 10.188 | bob: 9.194Epoch   6:  14% | abe: 8.546 | eve: 10.181 | bob: 9.193Epoch   6:  14% | abe: 8.543 | eve: 10.182 | bob: 9.189Epoch   6:  15% | abe: 8.543 | eve: 10.185 | bob: 9.188Epoch   6:  16% | abe: 8.543 | eve: 10.188 | bob: 9.188Epoch   6:  17% | abe: 8.545 | eve: 10.188 | bob: 9.190Epoch   6:  17% | abe: 8.543 | eve: 10.182 | bob: 9.187Epoch   6:  18% | abe: 8.545 | eve: 10.182 | bob: 9.190Epoch   6:  19% | abe: 8.543 | eve: 10.186 | bob: 9.186Epoch   6:  20% | abe: 8.544 | eve: 10.191 | bob: 9.189Epoch   6:  21% | abe: 8.542 | eve: 10.190 | bob: 9.186Epoch   6:  21% | abe: 8.541 | eve: 10.189 | bob: 9.186Epoch   6:  22% | abe: 8.540 | eve: 10.188 | bob: 9.185Epoch   6:  23% | abe: 8.537 | eve: 10.189 | bob: 9.182Epoch   6:  24% | abe: 8.537 | eve: 10.188 | bob: 9.182Epoch   6:  25% | abe: 8.536 | eve: 10.190 | bob: 9.181Epoch   6:  25% | abe: 8.534 | eve: 10.190 | bob: 9.180Epoch   6:  26% | abe: 8.533 | eve: 10.190 | bob: 9.178Epoch   6:  27% | abe: 8.534 | eve: 10.190 | bob: 9.180Epoch   6:  28% | abe: 8.533 | eve: 10.190 | bob: 9.179Epoch   6:  28% | abe: 8.534 | eve: 10.189 | bob: 9.181Epoch   6:  29% | abe: 8.534 | eve: 10.189 | bob: 9.181Epoch   6:  30% | abe: 8.533 | eve: 10.189 | bob: 9.181Epoch   6:  31% | abe: 8.532 | eve: 10.190 | bob: 9.180Epoch   6:  32% | abe: 8.532 | eve: 10.190 | bob: 9.180Epoch   6:  32% | abe: 8.531 | eve: 10.191 | bob: 9.179Epoch   6:  33% | abe: 8.529 | eve: 10.190 | bob: 9.177Epoch   6:  34% | abe: 8.528 | eve: 10.190 | bob: 9.177Epoch   6:  35% | abe: 8.527 | eve: 10.189 | bob: 9.176Epoch   6:  35% | abe: 8.529 | eve: 10.189 | bob: 9.179Epoch   6:  36% | abe: 8.529 | eve: 10.189 | bob: 9.179Epoch   6:  37% | abe: 8.529 | eve: 10.189 | bob: 9.181Epoch   6:  38% | abe: 8.528 | eve: 10.190 | bob: 9.180Epoch   6:  39% | abe: 8.528 | eve: 10.189 | bob: 9.181Epoch   6:  39% | abe: 8.527 | eve: 10.189 | bob: 9.180Epoch   6:  40% | abe: 8.527 | eve: 10.189 | bob: 9.181Epoch   6:  41% | abe: 8.527 | eve: 10.189 | bob: 9.181Epoch   6:  42% | abe: 8.526 | eve: 10.189 | bob: 9.181Epoch   6:  42% | abe: 8.526 | eve: 10.189 | bob: 9.181Epoch   6:  43% | abe: 8.525 | eve: 10.190 | bob: 9.180Epoch   6:  44% | abe: 8.525 | eve: 10.189 | bob: 9.181Epoch   6:  45% | abe: 8.526 | eve: 10.190 | bob: 9.183Epoch   6:  46% | abe: 8.525 | eve: 10.188 | bob: 9.182Epoch   6:  46% | abe: 8.525 | eve: 10.189 | bob: 9.183Epoch   6:  47% | abe: 8.525 | eve: 10.188 | bob: 9.183Epoch   6:  48% | abe: 8.525 | eve: 10.188 | bob: 9.183Epoch   6:  49% | abe: 8.524 | eve: 10.187 | bob: 9.183Epoch   6:  50% | abe: 8.524 | eve: 10.187 | bob: 9.183Epoch   6:  50% | abe: 8.523 | eve: 10.188 | bob: 9.183Epoch   6:  51% | abe: 8.524 | eve: 10.188 | bob: 9.183Epoch   6:  52% | abe: 8.524 | eve: 10.187 | bob: 9.184Epoch   6:  53% | abe: 8.523 | eve: 10.188 | bob: 9.184Epoch   6:  53% | abe: 8.524 | eve: 10.187 | bob: 9.185Epoch   6:  54% | abe: 8.523 | eve: 10.186 | bob: 9.185Epoch   6:  55% | abe: 8.523 | eve: 10.185 | bob: 9.184Epoch   6:  56% | abe: 8.522 | eve: 10.184 | bob: 9.183Epoch   6:  57% | abe: 8.522 | eve: 10.183 | bob: 9.184Epoch   6:  57% | abe: 8.521 | eve: 10.183 | bob: 9.183Epoch   6:  58% | abe: 8.521 | eve: 10.180 | bob: 9.184Epoch   6:  59% | abe: 8.521 | eve: 10.181 | bob: 9.184Epoch   6:  60% | abe: 8.520 | eve: 10.179 | bob: 9.183Epoch   6:  60% | abe: 8.519 | eve: 10.179 | bob: 9.183Epoch   6:  61% | abe: 8.519 | eve: 10.178 | bob: 9.183Epoch   6:  62% | abe: 8.518 | eve: 10.178 | bob: 9.182Epoch   6:  63% | abe: 8.518 | eve: 10.179 | bob: 9.183Epoch   6:  64% | abe: 8.517 | eve: 10.179 | bob: 9.182Epoch   6:  64% | abe: 8.517 | eve: 10.178 | bob: 9.183Epoch   6:  65% | abe: 8.516 | eve: 10.178 | bob: 9.182Epoch   6:  66% | abe: 8.517 | eve: 10.179 | bob: 9.183Epoch   6:  67% | abe: 8.516 | eve: 10.179 | bob: 9.183Epoch   6:  67% | abe: 8.516 | eve: 10.178 | bob: 9.183Epoch   6:  68% | abe: 8.515 | eve: 10.178 | bob: 9.183Epoch   6:  69% | abe: 8.515 | eve: 10.177 | bob: 9.183Epoch   6:  70% | abe: 8.514 | eve: 10.176 | bob: 9.183Epoch   6:  71% | abe: 8.514 | eve: 10.176 | bob: 9.183Epoch   6:  71% | abe: 8.513 | eve: 10.175 | bob: 9.182Epoch   6:  72% | abe: 8.513 | eve: 10.175 | bob: 9.183Epoch   6:  73% | abe: 8.512 | eve: 10.175 | bob: 9.182Epoch   6:  74% | abe: 8.511 | eve: 10.175 | bob: 9.182Epoch   6:  75% | abe: 8.510 | eve: 10.175 | bob: 9.181Epoch   6:  75% | abe: 8.510 | eve: 10.175 | bob: 9.181Epoch   6:  76% | abe: 8.509 | eve: 10.175 | bob: 9.180Epoch   6:  77% | abe: 8.508 | eve: 10.175 | bob: 9.180Epoch   6:  78% | abe: 8.508 | eve: 10.175 | bob: 9.180Epoch   6:  78% | abe: 8.507 | eve: 10.175 | bob: 9.179Epoch   6:  79% | abe: 8.507 | eve: 10.176 | bob: 9.180Epoch   6:  80% | abe: 8.507 | eve: 10.175 | bob: 9.180Epoch   6:  81% | abe: 8.506 | eve: 10.175 | bob: 9.179Epoch   6:  82% | abe: 8.506 | eve: 10.175 | bob: 9.179Epoch   6:  82% | abe: 8.505 | eve: 10.175 | bob: 9.179Epoch   6:  83% | abe: 8.504 | eve: 10.174 | bob: 9.178Epoch   6:  84% | abe: 8.504 | eve: 10.174 | bob: 9.178Epoch   6:  85% | abe: 8.504 | eve: 10.174 | bob: 9.179Epoch   6:  85% | abe: 8.504 | eve: 10.174 | bob: 9.179Epoch   6:  86% | abe: 8.504 | eve: 10.173 | bob: 9.179Epoch   6:  87% | abe: 8.503 | eve: 10.173 | bob: 9.179Epoch   6:  88% | abe: 8.503 | eve: 10.173 | bob: 9.178Epoch   6:  89% | abe: 8.502 | eve: 10.172 | bob: 9.178Epoch   6:  89% | abe: 8.501 | eve: 10.172 | bob: 9.178Epoch   6:  90% | abe: 8.501 | eve: 10.172 | bob: 9.178Epoch   6:  91% | abe: 8.499 | eve: 10.171 | bob: 9.176Epoch   6:  92% | abe: 8.499 | eve: 10.171 | bob: 9.176Epoch   6:  92% | abe: 8.498 | eve: 10.171 | bob: 9.176Epoch   6:  93% | abe: 8.498 | eve: 10.170 | bob: 9.176Epoch   6:  94% | abe: 8.497 | eve: 10.170 | bob: 9.176Epoch   6:  95% | abe: 8.497 | eve: 10.170 | bob: 9.175Epoch   6:  96% | abe: 8.497 | eve: 10.170 | bob: 9.176Epoch   6:  96% | abe: 8.496 | eve: 10.170 | bob: 9.175Epoch   6:  97% | abe: 8.496 | eve: 10.170 | bob: 9.175Epoch   6:  98% | abe: 8.495 | eve: 10.170 | bob: 9.175Epoch   6:  99% | abe: 8.495 | eve: 10.169 | bob: 9.175
Early stopping: No improvement after 5 epochs since epoch 0. Best Bob loss: 9.12603507112749
Training complete.
cipher1 + cipher2
[[0.7348282  1.0471793  0.8302971  ... 0.9213111  0.85400176 0.8222742 ]
 [0.7239145  1.2512989  0.75632817 ... 0.81703466 0.9197116  0.79739213]
 [0.7511113  1.0228646  0.96853876 ... 0.96981865 0.9005933  0.8937944 ]
 ...
 [0.8100265  0.9755906  0.9421386  ... 0.85690296 0.99484307 0.80966353]
 [0.7779436  0.84529525 1.0230241  ... 0.8656852  0.7498299  0.97141874]
 [0.6787128  1.1021299  0.83023214 ... 0.76802135 0.93923056 0.87329346]]
HO addition:
[[0.7347374  1.0470468  0.83019036 ... 0.9211934  0.85389394 0.8221713 ]
 [0.7238266  1.2511402  0.75623447 ... 0.81693053 0.9195937  0.7972941 ]
 [0.75102174 1.0227319  0.9684166  ... 0.9696958  0.9004829  0.89368206]
 ...
 [0.80992526 0.97546685 0.94201946 ... 0.85679406 0.9947163  0.80956334]
 [0.7778449  0.84518903 1.0228983  ... 0.8655752  0.7497344  0.97129804]
 [0.6786294  1.1019893  0.83012956 ... 0.76792395 0.9391105  0.873186  ]]
cipher1 * cipher2
[[0.13385993 0.27279326 0.16715404 ... 0.208614   0.1818725  0.16903186]
 [0.12537017 0.38910148 0.14222626 ... 0.1647017  0.20640789 0.1565331 ]
 [0.12645121 0.2474369  0.23393075 ... 0.23384978 0.19866498 0.19962728]
 ...
 [0.16399065 0.23637855 0.22102816 ... 0.1818306  0.24406578 0.16277374]
 [0.15028484 0.17855917 0.25724548 ... 0.18557878 0.13883528 0.23522498]
 [0.11313172 0.29866558 0.17084399 ... 0.14648034 0.21641488 0.18792295]]
HO multiplication
[[0.13383678 0.2728008  0.16714084 ... 0.20861223 0.18186551 0.16902205]
 [0.12534738 0.38911167 0.14220732 ... 0.16468805 0.20640549 0.15652272]
 [0.12643698 0.24744111 0.23393428 ... 0.23385292 0.19866773 0.19962525]
 ...
 [0.16397949 0.23638195 0.22102948 ... 0.18182278 0.24406974 0.16276419]
 [0.15026554 0.17855182 0.25725695 ... 0.18557203 0.13880885 0.23523122]
 [0.11309099 0.2986742  0.1708376  ... 0.14645907 0.21641448 0.18792236]]
HO model Accuracy Percentage Addition: 100.00%
HO model Accuracy Percentage Multiplication: 100.00%
Bob decrypted addition: [[0.95162445 0.94608355 0.8458815  ... 0.92918634 0.9191418  0.8476565 ]
 [0.85992676 0.7472199  0.7867919  ... 0.967674   0.9339958  0.86978644]
 [0.9300161  0.85555637 0.60639393 ... 0.96232975 0.98054874 0.7941655 ]
 ...
 [0.94814694 1.0204525  0.81680083 ... 0.90027946 0.80671954 0.7716261 ]
 [0.8868528  0.8999415  0.7914115  ... 0.95396984 0.9449082  0.90895987]
 [0.8605254  0.8003963  0.7658781  ... 0.91188264 0.83276635 0.78325236]]
Bob decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits addition: 4138
Total number of bits addition: 8192
Decryption accuracy addition: 50.5126953125%
Bob decrypted multiplication: [[1.0083162  0.944385   0.96553725 ... 0.98038816 0.9669068  0.9629028 ]
 [0.99622345 0.93607986 0.9562628  ... 0.98768973 0.9736521  0.96502835]
 [1.027076   0.9025345  0.95820904 ... 0.9920845  1.0143906  0.91425544]
 ...
 [1.0002269  1.000989   0.96542764 ... 0.9924546  0.96491057 0.8406017 ]
 [0.9976322  0.94622433 0.96018445 ... 0.97649753 0.9774792  1.0037502 ]
 [0.99185026 0.9290823  0.9606249  ... 0.99348706 0.97045225 0.8662919 ]]
Bob decrypted bits multiplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits multiplication: 1967
Total number of bits multiplication: 8192
Decryption accuracy multiplication: 24.01123046875%
Eve decrypted addition: [[1.2172961  1.0914205  1.1844602  ... 1.2055217  0.8742429  0.97031087]
 [1.1920367  1.0667284  1.1803561  ... 1.2047775  0.9537293  1.0204413 ]
 [1.2114023  1.1337395  1.2154584  ... 1.1881623  0.8739252  1.0449094 ]
 ...
 [1.2245901  1.129179   1.2007161  ... 1.248275   0.8800232  1.0435203 ]
 [1.2127088  1.1011145  1.2089882  ... 1.2045763  0.9522963  0.9890074 ]
 [1.2179799  1.1502602  1.2029206  ... 1.2359383  0.86643046 1.0859414 ]]
Eve decrypted bits addition: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve addition: 4138
Total number of bits addition: 8192
Decryption accuracy by Eve addition: 50.5126953125%
Eve decrypted mulitplication: [[1.2151698  0.94255966 1.1684194  ... 0.9892522  0.97097033 0.92314494]
 [1.148572   0.87779236 1.1366277  ... 1.0458962  1.1016548  0.98557585]
 [1.2014349  1.0024445  1.1916683  ... 0.98698634 0.9761157  1.051067  ]
 ...
 [1.2344285  1.0203241  1.1795928  ... 1.1605058  0.97369516 1.0287553 ]
 [1.2111273  0.99538237 1.1738286  ... 1.0544125  1.0126615  0.93692845]
 [1.2237983  1.0943456  1.2096926  ... 1.049205   0.9923298  1.1020216 ]]
Eve decrypted bits mulitplication: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits by Eve mulitplication: 1967
Total number of bits mulitplication: 8192
Decryption accuracy by Eve mulitplication: 24.01123046875%
Bob decrypted P1: [[0.98043483 0.97335476 0.9741575  ... 0.98176336 0.95697194 0.87054896]
 [0.97860634 0.946321   0.96171856 ... 0.9857365  0.965687   0.90194714]
 [0.9857265  0.9379443  0.9557302  ... 0.9914528  1.0316931  0.84113556]
 ...
 [0.9788159  1.0118796  0.969483   ... 0.9915039  0.9601665  0.80640376]
 [0.97875327 0.95532143 0.9631393  ... 0.9769321  0.97703964 0.9426745 ]
 [0.9781821  0.93755543 0.9672311  ... 0.9953104  0.9664334  0.821194  ]]
Bob decrypted bits P1: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P1: 4029
Total number of bits P1: 8192
Decryption accuracy P1: 49.18212890625%
Bob decrypted P2: [[0.9810465  0.9552343  0.9677617  ... 0.9795305  0.9619247  0.9232651 ]
 [0.97904414 0.94392234 0.9603691  ... 0.98859596 0.9669262  0.8972972 ]
 [0.98594415 0.93572724 0.95706344 ... 0.992492   1.0113913  0.84373736]
 ...
 [0.979899   1.0128261  0.96908724 ... 0.993288   0.9557202  0.7795111 ]
 [0.97908324 0.95913273 0.9658794  ... 0.97617763 0.97357345 0.9366583 ]
 [0.97805846 0.93618715 0.9639434  ... 0.9912177  0.9590454  0.79750234]]
Bob decrypted bits P2: [[1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 ...
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]
 [1 1 1 ... 1 1 1]]
Number of correctly decrypted bits P2: 4043
Total number of bits P2: 8192
Decryption accuracy P2: 49.35302734375%
