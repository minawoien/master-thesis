WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-17 11:30:16.218409: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-17 11:30:16.331768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-17 11:30:16.332446: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:16.335616: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-17 11:30:16.338273: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-17 11:30:16.339175: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-17 11:30:16.341583: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-17 11:30:16.343918: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-17 11:30:16.351683: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-17 11:30:16.364765: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-17 11:30:16.365203: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-17 11:30:16.377772: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-17 11:30:16.382497: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5b54e80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-17 11:30:16.382568: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-17 11:30:16.731120: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x52ad670 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-17 11:30:16.731180: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-17 11:30:16.734556: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:8a:00.0
2024-04-17 11:30:16.734631: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:16.734658: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-17 11:30:16.734679: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-17 11:30:16.734701: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-17 11:30:16.734720: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-17 11:30:16.734741: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-17 11:30:16.734762: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-17 11:30:16.745962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-17 11:30:16.746058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-17 11:30:16.749457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-17 11:30:16.749486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-17 11:30:16.749494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-17 11:30:16.756497: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:8a:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-17 11:30:20.621588: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.9229 - val_loss: 0.0058
Epoch 2/512
448/448 - 0s - loss: 0.4586 - val_loss: 0.0022
Epoch 3/512
448/448 - 0s - loss: 0.1466 - val_loss: 3.9179e-04
Epoch 4/512
448/448 - 0s - loss: 0.0224 - val_loss: 4.0177e-05
Epoch 5/512
448/448 - 0s - loss: 0.0024 - val_loss: 8.5982e-06
Epoch 6/512
448/448 - 0s - loss: 7.4248e-04 - val_loss: 5.8806e-06
Epoch 7/512
448/448 - 0s - loss: 5.4002e-04 - val_loss: 4.4272e-06
Epoch 8/512
448/448 - 0s - loss: 3.9958e-04 - val_loss: 3.1254e-06
Epoch 9/512
448/448 - 0s - loss: 2.7541e-04 - val_loss: 2.0253e-06
Epoch 10/512
448/448 - 0s - loss: 1.7333e-04 - val_loss: 1.1801e-06
Epoch 11/512
448/448 - 0s - loss: 9.7418e-05 - val_loss: 6.0066e-07
Epoch 12/512
448/448 - 0s - loss: 4.7450e-05 - val_loss: 2.5781e-07
Epoch 13/512
448/448 - 0s - loss: 1.9310e-05 - val_loss: 8.9180e-08
Epoch 14/512
448/448 - 0s - loss: 6.2672e-06 - val_loss: 2.3621e-08
Epoch 15/512
448/448 - 0s - loss: 1.5985e-06 - val_loss: 1.2279e-08
Epoch 16/512
448/448 - 0s - loss: 1.6245e-05 - val_loss: 4.5125e-06
Epoch 17/512
448/448 - 0s - loss: 0.0035 - val_loss: 2.7922e-05
Epoch 18/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.4468e-06
Epoch 19/512
448/448 - 0s - loss: 1.2374e-04 - val_loss: 1.7471e-06
Epoch 20/512
448/448 - 0s - loss: 4.1824e-04 - val_loss: 1.9251e-05
Epoch 21/512
448/448 - 0s - loss: 0.0026 - val_loss: 1.5529e-05
Epoch 22/512
448/448 - 0s - loss: 9.5004e-04 - val_loss: 4.3119e-06
Epoch 23/512
448/448 - 0s - loss: 4.6574e-04 - val_loss: 8.2900e-06
Epoch 24/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.9556e-05
Epoch 25/512
448/448 - 0s - loss: 0.0016 - val_loss: 8.5438e-06
Epoch 26/512
448/448 - 0s - loss: 7.2597e-04 - val_loss: 7.2754e-06
Epoch 27/512
448/448 - 0s - loss: 8.7560e-04 - val_loss: 1.3615e-05
Epoch 28/512
448/448 - 0s - loss: 0.0014 - val_loss: 1.1369e-05
Epoch 29/512
448/448 - 0s - loss: 9.5472e-04 - val_loss: 8.2460e-06
Epoch 30/512
448/448 - 0s - loss: 8.5293e-04 - val_loss: 1.0286e-05
Epoch 31/512
448/448 - 0s - loss: 0.0011 - val_loss: 1.1748e-05
Epoch 32/512
448/448 - 0s - loss: 0.0011 - val_loss: 9.2363e-06
Epoch 33/512
448/448 - 0s - loss: 8.7803e-04 - val_loss: 8.9202e-06
Epoch 34/512
448/448 - 0s - loss: 9.2518e-04 - val_loss: 1.0578e-05
Epoch 35/512
448/448 - 0s - loss: 0.0010 - val_loss: 9.6823e-06
Epoch 36/512
448/448 - 0s - loss: 9.0617e-04 - val_loss: 8.8603e-06
Epoch 37/512
448/448 - 0s - loss: 8.8126e-04 - val_loss: 9.5189e-06
Epoch 38/512
448/448 - 0s - loss: 9.4261e-04 - val_loss: 9.5663e-06
Epoch 39/512
448/448 - 0s - loss: 9.0979e-04 - val_loss: 8.8396e-06
Epoch 40/512
448/448 - 0s - loss: 8.5872e-04 - val_loss: 8.8528e-06
Epoch 41/512
448/448 - 0s - loss: 8.8065e-04 - val_loss: 9.0573e-06
Epoch 42/512
448/448 - 0s - loss: 8.8076e-04 - val_loss: 8.6445e-06
Epoch 43/512
448/448 - 0s - loss: 8.3316e-04 - val_loss: 8.7598e-06
Epoch 44/512
448/448 - 0s - loss: 8.5938e-04 - val_loss: 8.5423e-06
Epoch 45/512
448/448 - 0s - loss: 8.2989e-04 - val_loss: 8.2576e-06
Epoch 46/512
448/448 - 0s - loss: 8.1014e-04 - val_loss: 8.4690e-06
Epoch 47/512
448/448 - 0s - loss: 8.3240e-04 - val_loss: 8.2439e-06
Epoch 48/512
448/448 - 0s - loss: 8.0030e-04 - val_loss: 7.9873e-06
Epoch 49/512
448/448 - 0s - loss: 7.8977e-04 - val_loss: 7.9357e-06
Epoch 50/512
448/448 - 0s - loss: 7.8223e-04 - val_loss: 8.0209e-06
Epoch 51/512
448/448 - 0s - loss: 7.8736e-04 - val_loss: 7.8075e-06
Epoch 52/512
448/448 - 0s - loss: 7.5967e-04 - val_loss: 7.7217e-06
Epoch 53/512
448/448 - 0s - loss: 7.5889e-04 - val_loss: 7.7240e-06
Epoch 54/512
448/448 - 0s - loss: 7.5898e-04 - val_loss: 7.4597e-06
Epoch 55/512
448/448 - 0s - loss: 7.3090e-04 - val_loss: 7.5106e-06
Epoch 56/512
448/448 - 0s - loss: 7.4108e-04 - val_loss: 7.4410e-06
Epoch 57/512
448/448 - 0s - loss: 7.2500e-04 - val_loss: 7.3656e-06
Epoch 58/512
448/448 - 0s - loss: 7.2868e-04 - val_loss: 6.9929e-06
Epoch 59/512
448/448 - 0s - loss: 6.9365e-04 - val_loss: 7.0199e-06
Epoch 60/512
448/448 - 0s - loss: 7.0221e-04 - val_loss: 7.2760e-06
Epoch 61/512
448/448 - 0s - loss: 7.1456e-04 - val_loss: 6.9252e-06
Epoch 62/512
448/448 - 0s - loss: 6.7607e-04 - val_loss: 6.7762e-06
Epoch 63/512
448/448 - 0s - loss: 6.7299e-04 - val_loss: 7.0431e-06
Epoch 64/512
448/448 - 0s - loss: 6.9242e-04 - val_loss: 6.8267e-06
Epoch 65/512
448/448 - 0s - loss: 6.6480e-04 - val_loss: 6.4752e-06
Epoch 66/512
448/448 - 0s - loss: 6.4639e-04 - val_loss: 6.5704e-06
Epoch 67/512
448/448 - 0s - loss: 6.5143e-04 - val_loss: 6.8802e-06
Epoch 68/512
448/448 - 0s - loss: 6.7250e-04 - val_loss: 6.3638e-06
Epoch 69/512
448/448 - 0s - loss: 6.2059e-04 - val_loss: 6.2547e-06
Epoch 70/512
448/448 - 0s - loss: 6.2896e-04 - val_loss: 6.5570e-06
Epoch 71/512
448/448 - 0s - loss: 6.4732e-04 - val_loss: 6.2697e-06
Epoch 72/512
448/448 - 0s - loss: 6.0930e-04 - val_loss: 6.1531e-06
Epoch 73/512
448/448 - 0s - loss: 6.1238e-04 - val_loss: 6.3004e-06
Epoch 74/512
448/448 - 0s - loss: 6.1997e-04 - val_loss: 6.0897e-06
Epoch 75/512
448/448 - 0s - loss: 5.9740e-04 - val_loss: 5.9562e-06
Epoch 76/512
448/448 - 0s - loss: 5.9486e-04 - val_loss: 5.9851e-06
Epoch 77/512
448/448 - 0s - loss: 5.9125e-04 - val_loss: 6.0646e-06
Epoch 78/512
448/448 - 0s - loss: 5.9805e-04 - val_loss: 5.8083e-06
Epoch 79/512
448/448 - 0s - loss: 5.7235e-04 - val_loss: 5.7190e-06
Epoch 80/512
448/448 - 0s - loss: 5.7108e-04 - val_loss: 5.8830e-06
Epoch 81/512
448/448 - 0s - loss: 5.8142e-04 - val_loss: 5.7354e-06
Epoch 82/512
448/448 - 0s - loss: 5.5895e-04 - val_loss: 5.6562e-06
Epoch 83/512
448/448 - 0s - loss: 5.5982e-04 - val_loss: 5.6161e-06
Epoch 84/512
448/448 - 0s - loss: 5.5438e-04 - val_loss: 5.5986e-06
Epoch 85/512
448/448 - 0s - loss: 5.5500e-04 - val_loss: 5.3703e-06
Epoch 86/512
448/448 - 0s - loss: 5.3272e-04 - val_loss: 5.4406e-06
Epoch 87/512
448/448 - 0s - loss: 5.4382e-04 - val_loss: 5.5401e-06
Epoch 88/512
448/448 - 0s - loss: 5.4449e-04 - val_loss: 5.2750e-06
Epoch 89/512
448/448 - 0s - loss: 5.2126e-04 - val_loss: 5.1537e-06
Epoch 90/512
448/448 - 0s - loss: 5.2119e-04 - val_loss: 5.2496e-06
Epoch 91/512
448/448 - 0s - loss: 5.2363e-04 - val_loss: 5.2860e-06
Epoch 92/512
448/448 - 0s - loss: 5.2020e-04 - val_loss: 5.1048e-06
Epoch 93/512
448/448 - 0s - loss: 5.0491e-04 - val_loss: 5.0181e-06
Epoch 94/512
448/448 - 0s - loss: 5.0361e-04 - val_loss: 5.0644e-06
Epoch 95/512
448/448 - 0s - loss: 5.0669e-04 - val_loss: 4.9766e-06
Epoch 96/512
448/448 - 0s - loss: 4.9116e-04 - val_loss: 4.9871e-06
Epoch 97/512
448/448 - 0s - loss: 4.9488e-04 - val_loss: 4.9698e-06
Epoch 98/512
448/448 - 0s - loss: 4.9002e-04 - val_loss: 4.8530e-06
Epoch 99/512
448/448 - 0s - loss: 4.8304e-04 - val_loss: 4.7672e-06
Epoch 100/512
448/448 - 0s - loss: 4.7655e-04 - val_loss: 4.7383e-06
Epoch 101/512
448/448 - 0s - loss: 4.7433e-04 - val_loss: 4.7957e-06
Epoch 102/512
448/448 - 0s - loss: 4.7636e-04 - val_loss: 4.6939e-06
Epoch 103/512
448/448 - 0s - loss: 4.6368e-04 - val_loss: 4.6266e-06
Epoch 104/512
448/448 - 0s - loss: 4.6026e-04 - val_loss: 4.6579e-06
Epoch 105/512
448/448 - 0s - loss: 4.6354e-04 - val_loss: 4.5325e-06
Epoch 106/512
448/448 - 0s - loss: 4.5024e-04 - val_loss: 4.4824e-06
Epoch 107/512
448/448 - 0s - loss: 4.4883e-04 - val_loss: 4.5299e-06
Epoch 108/512
448/448 - 0s - loss: 4.4898e-04 - val_loss: 4.4742e-06
Epoch 109/512
448/448 - 0s - loss: 4.4355e-04 - val_loss: 4.3432e-06
Epoch 110/512
448/448 - 0s - loss: 4.3402e-04 - val_loss: 4.3367e-06
Epoch 111/512
448/448 - 0s - loss: 4.3689e-04 - val_loss: 4.2938e-06
Epoch 112/512
448/448 - 0s - loss: 4.2886e-04 - val_loss: 4.2617e-06
Epoch 113/512
448/448 - 0s - loss: 4.2455e-04 - val_loss: 4.3062e-06
Epoch 114/512
448/448 - 0s - loss: 4.2925e-04 - val_loss: 4.1744e-06
Epoch 115/512
448/448 - 0s - loss: 4.1260e-04 - val_loss: 4.1385e-06
Epoch 116/512
448/448 - 0s - loss: 4.1377e-04 - val_loss: 4.2140e-06
Epoch 117/512
448/448 - 0s - loss: 4.1723e-04 - val_loss: 4.0934e-06
Epoch 118/512
448/448 - 0s - loss: 4.0357e-04 - val_loss: 4.0649e-06
Epoch 119/512
448/448 - 0s - loss: 4.0437e-04 - val_loss: 4.0600e-06
Epoch 120/512
448/448 - 0s - loss: 4.0388e-04 - val_loss: 3.9144e-06
Epoch 121/512
448/448 - 0s - loss: 3.9100e-04 - val_loss: 3.9084e-06
Epoch 122/512
448/448 - 0s - loss: 3.9410e-04 - val_loss: 3.9236e-06
Epoch 123/512
448/448 - 0s - loss: 3.8857e-04 - val_loss: 3.9564e-06
Epoch 124/512
448/448 - 0s - loss: 3.9216e-04 - val_loss: 3.8013e-06
Epoch 125/512
448/448 - 0s - loss: 3.7637e-04 - val_loss: 3.8070e-06
Epoch 126/512
448/448 - 0s - loss: 3.8129e-04 - val_loss: 3.7822e-06
Epoch 127/512
448/448 - 0s - loss: 3.7666e-04 - val_loss: 3.6973e-06
Epoch 128/512
448/448 - 0s - loss: 3.6951e-04 - val_loss: 3.7034e-06
Epoch 129/512
448/448 - 0s - loss: 3.7037e-04 - val_loss: 3.6855e-06
Epoch 130/512
448/448 - 0s - loss: 3.6516e-04 - val_loss: 3.6582e-06
Epoch 131/512
448/448 - 0s - loss: 3.6293e-04 - val_loss: 3.6257e-06
Epoch 132/512
448/448 - 0s - loss: 3.6188e-04 - val_loss: 3.4657e-06
Epoch 133/512
448/448 - 0s - loss: 3.4539e-04 - val_loss: 3.5741e-06
Epoch 134/512
448/448 - 0s - loss: 3.5900e-04 - val_loss: 3.5735e-06
Epoch 135/512
448/448 - 0s - loss: 3.5482e-04 - val_loss: 3.2662e-06
Epoch 136/512
448/448 - 0s - loss: 3.2786e-04 - val_loss: 3.4752e-06
Epoch 137/512
448/448 - 0s - loss: 3.5298e-04 - val_loss: 3.5109e-06
Epoch 138/512
448/448 - 0s - loss: 3.4436e-04 - val_loss: 3.2565e-06
Epoch 139/512
448/448 - 0s - loss: 3.2384e-04 - val_loss: 3.3236e-06
Epoch 140/512
448/448 - 0s - loss: 3.3701e-04 - val_loss: 3.3814e-06
Epoch 141/512
448/448 - 0s - loss: 3.3151e-04 - val_loss: 3.3146e-06
Epoch 142/512
448/448 - 0s - loss: 3.2922e-04 - val_loss: 3.1116e-06
Epoch 143/512
448/448 - 0s - loss: 3.1204e-04 - val_loss: 3.2389e-06
Epoch 144/512
448/448 - 0s - loss: 3.2484e-04 - val_loss: 3.3403e-06
Epoch 145/512
448/448 - 0s - loss: 3.2698e-04 - val_loss: 3.0659e-06
Epoch 146/512
448/448 - 0s - loss: 3.0274e-04 - val_loss: 3.0315e-06
Epoch 147/512
448/448 - 0s - loss: 3.0847e-04 - val_loss: 3.2011e-06
Epoch 148/512
448/448 - 0s - loss: 3.1821e-04 - val_loss: 3.0749e-06
Epoch 149/512
448/448 - 0s - loss: 3.0089e-04 - val_loss: 2.9451e-06
Epoch 150/512
448/448 - 0s - loss: 2.9580e-04 - val_loss: 3.0541e-06
Epoch 151/512
448/448 - 0s - loss: 3.0716e-04 - val_loss: 2.9618e-06
Epoch 152/512
448/448 - 0s - loss: 2.9338e-04 - val_loss: 2.8502e-06
Epoch 153/512
448/448 - 0s - loss: 2.8684e-04 - val_loss: 2.9759e-06
Epoch 154/512
448/448 - 0s - loss: 2.9761e-04 - val_loss: 2.9401e-06
Epoch 155/512
448/448 - 0s - loss: 2.8818e-04 - val_loss: 2.7999e-06
Epoch 156/512
448/448 - 0s - loss: 2.8157e-04 - val_loss: 2.7389e-06
Epoch 157/512
448/448 - 0s - loss: 2.7701e-04 - val_loss: 2.8452e-06
Epoch 158/512
448/448 - 0s - loss: 2.8454e-04 - val_loss: 2.7810e-06
Epoch 159/512
448/448 - 0s - loss: 2.7640e-04 - val_loss: 2.6755e-06
Epoch 160/512
448/448 - 0s - loss: 2.7013e-04 - val_loss: 2.6456e-06
Epoch 161/512
448/448 - 0s - loss: 2.6655e-04 - val_loss: 2.7550e-06
Epoch 162/512
448/448 - 0s - loss: 2.7501e-04 - val_loss: 2.6886e-06
Epoch 163/512
448/448 - 0s - loss: 2.6396e-04 - val_loss: 2.5760e-06
Epoch 164/512
448/448 - 0s - loss: 2.5757e-04 - val_loss: 2.6236e-06
Epoch 165/512
448/448 - 0s - loss: 2.6309e-04 - val_loss: 2.6082e-06
Epoch 166/512
448/448 - 0s - loss: 2.5942e-04 - val_loss: 2.4599e-06
Epoch 167/512
448/448 - 0s - loss: 2.4624e-04 - val_loss: 2.5272e-06
Epoch 168/512
448/448 - 0s - loss: 2.5345e-04 - val_loss: 2.5990e-06
Epoch 169/512
448/448 - 0s - loss: 2.5475e-04 - val_loss: 2.4484e-06
Epoch 170/512
448/448 - 0s - loss: 2.4240e-04 - val_loss: 2.3501e-06
Epoch 171/512
448/448 - 0s - loss: 2.3790e-04 - val_loss: 2.4565e-06
Epoch 172/512
448/448 - 0s - loss: 2.4571e-04 - val_loss: 2.4615e-06
Epoch 173/512
448/448 - 0s - loss: 2.4188e-04 - val_loss: 2.3176e-06
Epoch 174/512
448/448 - 0s - loss: 2.2910e-04 - val_loss: 2.3380e-06
Epoch 175/512
448/448 - 0s - loss: 2.3459e-04 - val_loss: 2.3603e-06
Epoch 176/512
448/448 - 0s - loss: 2.3437e-04 - val_loss: 2.2389e-06
Epoch 177/512
448/448 - 0s - loss: 2.2289e-04 - val_loss: 2.2346e-06
Epoch 178/512
448/448 - 0s - loss: 2.2460e-04 - val_loss: 2.2972e-06
Epoch 179/512
448/448 - 0s - loss: 2.2871e-04 - val_loss: 2.1917e-06
Epoch 180/512
448/448 - 0s - loss: 2.1613e-04 - val_loss: 2.1369e-06
Epoch 181/512
448/448 - 0s - loss: 2.1587e-04 - val_loss: 2.1828e-06
Epoch 182/512
448/448 - 0s - loss: 2.1841e-04 - val_loss: 2.1377e-06
Epoch 183/512
448/448 - 0s - loss: 2.1107e-04 - val_loss: 2.1254e-06
Epoch 184/512
448/448 - 0s - loss: 2.1085e-04 - val_loss: 2.1256e-06
Epoch 185/512
448/448 - 0s - loss: 2.1055e-04 - val_loss: 2.0387e-06
Epoch 186/512
448/448 - 0s - loss: 2.0475e-04 - val_loss: 1.9688e-06
Epoch 187/512
448/448 - 0s - loss: 1.9912e-04 - val_loss: 2.0445e-06
Epoch 188/512
448/448 - 0s - loss: 2.0639e-04 - val_loss: 1.9753e-06
Epoch 189/512
448/448 - 0s - loss: 1.9524e-04 - val_loss: 1.9618e-06
Epoch 190/512
448/448 - 0s - loss: 1.9717e-04 - val_loss: 1.9276e-06
Epoch 191/512
448/448 - 0s - loss: 1.9300e-04 - val_loss: 1.9029e-06
Epoch 192/512
448/448 - 0s - loss: 1.9112e-04 - val_loss: 1.8846e-06
Epoch 193/512
448/448 - 0s - loss: 1.8667e-04 - val_loss: 1.9310e-06
Epoch 194/512
448/448 - 0s - loss: 1.9168e-04 - val_loss: 1.8276e-06
Epoch 195/512
448/448 - 0s - loss: 1.8166e-04 - val_loss: 1.7665e-06
Epoch 196/512
448/448 - 0s - loss: 1.7895e-04 - val_loss: 1.8317e-06
Epoch 197/512
448/448 - 0s - loss: 1.8347e-04 - val_loss: 1.8018e-06
Epoch 198/512
448/448 - 0s - loss: 1.7888e-04 - val_loss: 1.6844e-06
Epoch 199/512
448/448 - 0s - loss: 1.6870e-04 - val_loss: 1.7452e-06
Epoch 200/512
448/448 - 0s - loss: 1.7597e-04 - val_loss: 1.7923e-06
Epoch 201/512
448/448 - 0s - loss: 1.7634e-04 - val_loss: 1.6172e-06
Epoch 202/512
448/448 - 0s - loss: 1.6053e-04 - val_loss: 1.6282e-06
Epoch 203/512
448/448 - 0s - loss: 1.6736e-04 - val_loss: 1.6822e-06
Epoch 204/512
448/448 - 0s - loss: 1.6782e-04 - val_loss: 1.5983e-06
Epoch 205/512
448/448 - 0s - loss: 1.5923e-04 - val_loss: 1.5571e-06
Epoch 206/512
448/448 - 0s - loss: 1.5778e-04 - val_loss: 1.5837e-06
Epoch 207/512
448/448 - 0s - loss: 1.5930e-04 - val_loss: 1.5768e-06
Epoch 208/512
448/448 - 0s - loss: 1.5761e-04 - val_loss: 1.4790e-06
Epoch 209/512
448/448 - 0s - loss: 1.4897e-04 - val_loss: 1.4813e-06
Epoch 210/512
448/448 - 0s - loss: 1.5072e-04 - val_loss: 1.5266e-06
Epoch 211/512
448/448 - 0s - loss: 1.5258e-04 - val_loss: 1.4464e-06
Epoch 212/512
448/448 - 0s - loss: 1.4350e-04 - val_loss: 1.4193e-06
Epoch 213/512
448/448 - 0s - loss: 1.4285e-04 - val_loss: 1.4806e-06
Epoch 214/512
448/448 - 0s - loss: 1.4801e-04 - val_loss: 1.3829e-06
Epoch 215/512
448/448 - 0s - loss: 1.3643e-04 - val_loss: 1.3456e-06
Epoch 216/512
448/448 - 0s - loss: 1.3764e-04 - val_loss: 1.3686e-06
Epoch 217/512
448/448 - 0s - loss: 1.3733e-04 - val_loss: 1.3559e-06
Epoch 218/512
448/448 - 0s - loss: 1.3540e-04 - val_loss: 1.2821e-06
Epoch 219/512
448/448 - 0s - loss: 1.2869e-04 - val_loss: 1.3091e-06
Epoch 220/512
448/448 - 0s - loss: 1.3241e-04 - val_loss: 1.2767e-06
Epoch 221/512
448/448 - 0s - loss: 1.2702e-04 - val_loss: 1.2501e-06
Epoch 222/512
448/448 - 0s - loss: 1.2549e-04 - val_loss: 1.2535e-06
Epoch 223/512
448/448 - 0s - loss: 1.2606e-04 - val_loss: 1.1920e-06
Epoch 224/512
448/448 - 0s - loss: 1.1916e-04 - val_loss: 1.1894e-06
Epoch 225/512
448/448 - 0s - loss: 1.2024e-04 - val_loss: 1.2236e-06
Epoch 226/512
448/448 - 0s - loss: 1.2166e-04 - val_loss: 1.1341e-06
Epoch 227/512
448/448 - 0s - loss: 1.1210e-04 - val_loss: 1.1195e-06
Epoch 228/512
448/448 - 0s - loss: 1.1409e-04 - val_loss: 1.1421e-06
Epoch 229/512
448/448 - 0s - loss: 1.1449e-04 - val_loss: 1.0884e-06
Epoch 230/512
448/448 - 0s - loss: 1.0816e-04 - val_loss: 1.0834e-06
Epoch 231/512
448/448 - 0s - loss: 1.0903e-04 - val_loss: 1.0831e-06
Epoch 232/512
448/448 - 0s - loss: 1.0840e-04 - val_loss: 1.0201e-06
Epoch 233/512
448/448 - 0s - loss: 1.0195e-04 - val_loss: 1.0199e-06
Epoch 234/512
448/448 - 0s - loss: 1.0328e-04 - val_loss: 1.0390e-06
Epoch 235/512
448/448 - 0s - loss: 1.0245e-04 - val_loss: 1.0083e-06
Epoch 236/512
448/448 - 0s - loss: 9.9666e-05 - val_loss: 9.4418e-07
Epoch 237/512
448/448 - 0s - loss: 9.4852e-05 - val_loss: 9.4635e-07
Epoch 238/512
448/448 - 0s - loss: 9.6567e-05 - val_loss: 9.4004e-07
Epoch 239/512
448/448 - 0s - loss: 9.4395e-05 - val_loss: 9.0499e-07
Epoch 240/512
448/448 - 0s - loss: 9.0653e-05 - val_loss: 9.0414e-07
Epoch 241/512
448/448 - 0s - loss: 9.0860e-05 - val_loss: 9.0978e-07
Epoch 242/512
448/448 - 0s - loss: 8.9999e-05 - val_loss: 8.5904e-07
Epoch 243/512
448/448 - 0s - loss: 8.5767e-05 - val_loss: 8.2966e-07
Epoch 244/512
448/448 - 0s - loss: 8.4463e-05 - val_loss: 8.3096e-07
Epoch 245/512
448/448 - 0s - loss: 8.4167e-05 - val_loss: 8.1830e-07
Epoch 246/512
448/448 - 0s - loss: 8.2583e-05 - val_loss: 7.7504e-07
Epoch 247/512
448/448 - 0s - loss: 7.8871e-05 - val_loss: 7.6242e-07
Epoch 248/512
448/448 - 0s - loss: 7.7716e-05 - val_loss: 7.8847e-07
Epoch 249/512
448/448 - 0s - loss: 7.9025e-05 - val_loss: 7.5386e-07
Epoch 250/512
448/448 - 0s - loss: 7.4074e-05 - val_loss: 7.4080e-07
Epoch 251/512
448/448 - 0s - loss: 7.4406e-05 - val_loss: 7.1728e-07
Epoch 252/512
448/448 - 0s - loss: 7.1835e-05 - val_loss: 6.9652e-07
Epoch 253/512
448/448 - 0s - loss: 7.0058e-05 - val_loss: 7.0086e-07
Epoch 254/512
448/448 - 0s - loss: 7.0292e-05 - val_loss: 6.7805e-07
Epoch 255/512
448/448 - 0s - loss: 6.7499e-05 - val_loss: 6.4977e-07
Epoch 256/512
448/448 - 0s - loss: 6.5557e-05 - val_loss: 6.4078e-07
Epoch 257/512
448/448 - 0s - loss: 6.4827e-05 - val_loss: 6.3603e-07
Epoch 258/512
448/448 - 0s - loss: 6.3977e-05 - val_loss: 6.0494e-07
Epoch 259/512
448/448 - 0s - loss: 6.0620e-05 - val_loss: 6.0508e-07
Epoch 260/512
448/448 - 0s - loss: 6.1106e-05 - val_loss: 5.9225e-07
Epoch 261/512
448/448 - 0s - loss: 5.9442e-05 - val_loss: 5.6184e-07
Epoch 262/512
448/448 - 0s - loss: 5.6772e-05 - val_loss: 5.5483e-07
Epoch 263/512
448/448 - 0s - loss: 5.6450e-05 - val_loss: 5.5699e-07
Epoch 264/512
448/448 - 0s - loss: 5.6193e-05 - val_loss: 5.2071e-07
Epoch 265/512
448/448 - 0s - loss: 5.2489e-05 - val_loss: 5.0782e-07
Epoch 266/512
448/448 - 0s - loss: 5.1906e-05 - val_loss: 5.2688e-07
Epoch 267/512
448/448 - 0s - loss: 5.2884e-05 - val_loss: 5.0068e-07
Epoch 268/512
448/448 - 0s - loss: 4.9390e-05 - val_loss: 4.6765e-07
Epoch 269/512
448/448 - 0s - loss: 4.7886e-05 - val_loss: 4.6177e-07
Epoch 270/512
448/448 - 0s - loss: 4.7196e-05 - val_loss: 4.7597e-07
Epoch 271/512
448/448 - 0s - loss: 4.7485e-05 - val_loss: 4.5679e-07
Epoch 272/512
448/448 - 0s - loss: 4.5274e-05 - val_loss: 4.2026e-07
Epoch 273/512
448/448 - 0s - loss: 4.2706e-05 - val_loss: 4.1867e-07
Epoch 274/512
448/448 - 0s - loss: 4.2838e-05 - val_loss: 4.3392e-07
Epoch 275/512
448/448 - 0s - loss: 4.3297e-05 - val_loss: 4.0062e-07
Epoch 276/512
448/448 - 0s - loss: 3.9719e-05 - val_loss: 3.7798e-07
Epoch 277/512
448/448 - 0s - loss: 3.8860e-05 - val_loss: 3.8462e-07
Epoch 278/512
448/448 - 0s - loss: 3.9303e-05 - val_loss: 3.7734e-07
Epoch 279/512
448/448 - 0s - loss: 3.7776e-05 - val_loss: 3.5449e-07
Epoch 280/512
448/448 - 0s - loss: 3.5685e-05 - val_loss: 3.4920e-07
Epoch 281/512
448/448 - 0s - loss: 3.5496e-05 - val_loss: 3.4623e-07
Epoch 282/512
448/448 - 0s - loss: 3.4875e-05 - val_loss: 3.2963e-07
Epoch 283/512
448/448 - 0s - loss: 3.3120e-05 - val_loss: 3.2272e-07
Epoch 284/512
448/448 - 0s - loss: 3.2633e-05 - val_loss: 3.1973e-07
Epoch 285/512
448/448 - 0s - loss: 3.2054e-05 - val_loss: 3.0708e-07
Epoch 286/512
448/448 - 0s - loss: 3.0955e-05 - val_loss: 2.8380e-07
Epoch 287/512
448/448 - 0s - loss: 2.8758e-05 - val_loss: 2.9133e-07
Epoch 288/512
448/448 - 0s - loss: 2.9912e-05 - val_loss: 2.8401e-07
Epoch 289/512
448/448 - 0s - loss: 2.8067e-05 - val_loss: 2.7153e-07
Epoch 290/512
448/448 - 0s - loss: 2.7473e-05 - val_loss: 2.6069e-07
Epoch 291/512
448/448 - 0s - loss: 2.6476e-05 - val_loss: 2.4892e-07
Epoch 292/512
448/448 - 0s - loss: 2.5367e-05 - val_loss: 2.5122e-07
Epoch 293/512
448/448 - 0s - loss: 2.5764e-05 - val_loss: 2.3867e-07
Epoch 294/512
448/448 - 0s - loss: 2.3958e-05 - val_loss: 2.2697e-07
Epoch 295/512
448/448 - 0s - loss: 2.3284e-05 - val_loss: 2.2443e-07
Epoch 296/512
448/448 - 0s - loss: 2.2902e-05 - val_loss: 2.2137e-07
Epoch 297/512
448/448 - 0s - loss: 2.2370e-05 - val_loss: 2.0839e-07
Epoch 298/512
448/448 - 0s - loss: 2.1184e-05 - val_loss: 1.9953e-07
Epoch 299/512
448/448 - 0s - loss: 2.0561e-05 - val_loss: 1.9915e-07
Epoch 300/512
448/448 - 0s - loss: 2.0162e-05 - val_loss: 1.9881e-07
Epoch 301/512
448/448 - 0s - loss: 2.0000e-05 - val_loss: 1.8356e-07
Epoch 302/512
448/448 - 0s - loss: 1.8451e-05 - val_loss: 1.7638e-07
Epoch 303/512
448/448 - 0s - loss: 1.7975e-05 - val_loss: 1.8274e-07
Epoch 304/512
448/448 - 0s - loss: 1.8587e-05 - val_loss: 1.6509e-07
Epoch 305/512
448/448 - 0s - loss: 1.6591e-05 - val_loss: 1.5265e-07
Epoch 306/512
448/448 - 0s - loss: 1.6018e-05 - val_loss: 1.6161e-07
Epoch 307/512
448/448 - 0s - loss: 1.6718e-05 - val_loss: 1.5672e-07
Epoch 308/512
448/448 - 0s - loss: 1.5652e-05 - val_loss: 1.3831e-07
Epoch 309/512
448/448 - 0s - loss: 1.4088e-05 - val_loss: 1.4342e-07
Epoch 310/512
448/448 - 0s - loss: 1.5041e-05 - val_loss: 1.4264e-07
Epoch 311/512
448/448 - 0s - loss: 1.4168e-05 - val_loss: 1.3139e-07
Epoch 312/512
448/448 - 0s - loss: 1.3263e-05 - val_loss: 1.2519e-07
Epoch 313/512
448/448 - 0s - loss: 1.2970e-05 - val_loss: 1.2443e-07
Epoch 314/512
448/448 - 0s - loss: 1.2753e-05 - val_loss: 1.1913e-07
Epoch 315/512
448/448 - 0s - loss: 1.2031e-05 - val_loss: 1.1636e-07
Epoch 316/512
448/448 - 0s - loss: 1.1906e-05 - val_loss: 1.1085e-07
Epoch 317/512
448/448 - 0s - loss: 1.1227e-05 - val_loss: 1.0731e-07
Epoch 318/512
448/448 - 0s - loss: 1.1029e-05 - val_loss: 1.0269e-07
Epoch 319/512
448/448 - 0s - loss: 1.0527e-05 - val_loss: 9.9793e-08
Epoch 320/512
448/448 - 0s - loss: 1.0193e-05 - val_loss: 9.5661e-08
Epoch 321/512
448/448 - 0s - loss: 9.7914e-06 - val_loss: 9.3033e-08
Epoch 322/512
448/448 - 0s - loss: 9.5452e-06 - val_loss: 8.9202e-08
Epoch 323/512
448/448 - 0s - loss: 9.0786e-06 - val_loss: 8.7411e-08
Epoch 324/512
448/448 - 0s - loss: 8.9214e-06 - val_loss: 8.3596e-08
Epoch 325/512
448/448 - 0s - loss: 8.5023e-06 - val_loss: 7.8757e-08
Epoch 326/512
448/448 - 0s - loss: 8.1042e-06 - val_loss: 7.6664e-08
Epoch 327/512
448/448 - 0s - loss: 7.8764e-06 - val_loss: 7.6580e-08
Epoch 328/512
448/448 - 0s - loss: 7.7976e-06 - val_loss: 7.0449e-08
Epoch 329/512
448/448 - 0s - loss: 7.2081e-06 - val_loss: 6.5724e-08
Epoch 330/512
448/448 - 0s - loss: 6.8711e-06 - val_loss: 6.7102e-08
Epoch 331/512
448/448 - 0s - loss: 6.9877e-06 - val_loss: 6.4351e-08
Epoch 332/512
448/448 - 0s - loss: 6.5495e-06 - val_loss: 5.9344e-08
Epoch 333/512
448/448 - 0s - loss: 6.0581e-06 - val_loss: 6.1655e-08
Epoch 334/512
448/448 - 0s - loss: 6.3894e-06 - val_loss: 5.6488e-08
Epoch 335/512
448/448 - 0s - loss: 5.6806e-06 - val_loss: 5.2827e-08
Epoch 336/512
448/448 - 0s - loss: 5.4836e-06 - val_loss: 5.4097e-08
Epoch 337/512
448/448 - 0s - loss: 5.5615e-06 - val_loss: 5.2483e-08
Epoch 338/512
448/448 - 0s - loss: 5.2956e-06 - val_loss: 4.6043e-08
Epoch 339/512
448/448 - 0s - loss: 4.7264e-06 - val_loss: 4.5985e-08
Epoch 340/512
448/448 - 0s - loss: 4.8162e-06 - val_loss: 4.8172e-08
Epoch 341/512
448/448 - 0s - loss: 4.8554e-06 - val_loss: 4.2995e-08
Epoch 342/512
448/448 - 0s - loss: 4.2854e-06 - val_loss: 3.9196e-08
Epoch 343/512
448/448 - 0s - loss: 4.1009e-06 - val_loss: 4.0686e-08
Epoch 344/512
448/448 - 0s - loss: 4.2278e-06 - val_loss: 4.0541e-08
Epoch 345/512
448/448 - 0s - loss: 4.0564e-06 - val_loss: 3.4778e-08
Epoch 346/512
448/448 - 0s - loss: 3.5363e-06 - val_loss: 3.4777e-08
Epoch 347/512
448/448 - 0s - loss: 3.6558e-06 - val_loss: 3.6433e-08
Epoch 348/512
448/448 - 0s - loss: 3.6740e-06 - val_loss: 3.1533e-08
Epoch 349/512
448/448 - 0s - loss: 3.1775e-06 - val_loss: 2.9484e-08
Epoch 350/512
448/448 - 0s - loss: 3.1267e-06 - val_loss: 3.1413e-08
Epoch 351/512
448/448 - 0s - loss: 3.2168e-06 - val_loss: 3.0248e-08
Epoch 352/512
448/448 - 0s - loss: 3.0171e-06 - val_loss: 2.5496e-08
Epoch 353/512
448/448 - 0s - loss: 2.6028e-06 - val_loss: 2.6210e-08
Epoch 354/512
448/448 - 0s - loss: 2.7895e-06 - val_loss: 2.7692e-08
Epoch 355/512
448/448 - 0s - loss: 2.7815e-06 - val_loss: 2.3206e-08
Epoch 356/512
448/448 - 0s - loss: 2.3169e-06 - val_loss: 2.2077e-08
Epoch 357/512
448/448 - 0s - loss: 2.3523e-06 - val_loss: 2.3687e-08
Epoch 358/512
448/448 - 0s - loss: 2.4416e-06 - val_loss: 2.1437e-08
Epoch 359/512
448/448 - 0s - loss: 2.1470e-06 - val_loss: 1.9230e-08
Epoch 360/512
448/448 - 0s - loss: 2.0178e-06 - val_loss: 1.9955e-08
Epoch 361/512
448/448 - 0s - loss: 2.0871e-06 - val_loss: 1.9886e-08
Epoch 362/512
448/448 - 0s - loss: 1.9925e-06 - val_loss: 1.7525e-08
Epoch 363/512
448/448 - 0s - loss: 1.7824e-06 - val_loss: 1.6810e-08
Epoch 364/512
448/448 - 0s - loss: 1.7674e-06 - val_loss: 1.7033e-08
Epoch 365/512
448/448 - 0s - loss: 1.7370e-06 - val_loss: 1.6239e-08
Epoch 366/512
448/448 - 0s - loss: 1.6396e-06 - val_loss: 1.4803e-08
Epoch 367/512
448/448 - 0s - loss: 1.5302e-06 - val_loss: 1.4018e-08
Epoch 368/512
448/448 - 0s - loss: 1.4702e-06 - val_loss: 1.4326e-08
Epoch 369/512
448/448 - 0s - loss: 1.4830e-06 - val_loss: 1.3166e-08
Epoch 370/512
448/448 - 0s - loss: 1.3440e-06 - val_loss: 1.1974e-08
Epoch 371/512
448/448 - 0s - loss: 1.2499e-06 - val_loss: 1.2530e-08
Epoch 372/512
448/448 - 0s - loss: 1.3074e-06 - val_loss: 1.2112e-08
Epoch 373/512
448/448 - 0s - loss: 1.2090e-06 - val_loss: 1.0676e-08
Epoch 374/512
448/448 - 0s - loss: 1.0979e-06 - val_loss: 1.0324e-08
Epoch 375/512
448/448 - 0s - loss: 1.1003e-06 - val_loss: 1.0342e-08
Epoch 376/512
448/448 - 0s - loss: 1.0662e-06 - val_loss: 9.6706e-09
Epoch 377/512
448/448 - 0s - loss: 9.8976e-07 - val_loss: 9.1649e-09
Epoch 378/512
448/448 - 0s - loss: 9.4897e-07 - val_loss: 8.9613e-09
Epoch 379/512
448/448 - 0s - loss: 9.2596e-07 - val_loss: 8.5262e-09
Epoch 380/512
448/448 - 0s - loss: 8.6919e-07 - val_loss: 7.8777e-09
Epoch 381/512
448/448 - 0s - loss: 8.2257e-07 - val_loss: 7.7183e-09
Epoch 382/512
448/448 - 0s - loss: 8.0294e-07 - val_loss: 7.5960e-09
Epoch 383/512
448/448 - 0s - loss: 7.7802e-07 - val_loss: 6.9456e-09
Epoch 384/512
448/448 - 0s - loss: 7.0869e-07 - val_loss: 6.6895e-09
Epoch 385/512
448/448 - 0s - loss: 7.0175e-07 - val_loss: 6.4209e-09
Epoch 386/512
448/448 - 0s - loss: 6.5879e-07 - val_loss: 6.2767e-09
Epoch 387/512
448/448 - 0s - loss: 6.4381e-07 - val_loss: 5.9876e-09
Epoch 388/512
448/448 - 0s - loss: 6.0837e-07 - val_loss: 5.5086e-09
Epoch 389/512
448/448 - 0s - loss: 5.6928e-07 - val_loss: 5.3292e-09
Epoch 390/512
448/448 - 0s - loss: 5.5690e-07 - val_loss: 5.1396e-09
Epoch 391/512
448/448 - 0s - loss: 5.2772e-07 - val_loss: 4.8460e-09
Epoch 392/512
448/448 - 0s - loss: 5.0245e-07 - val_loss: 4.6400e-09
Epoch 393/512
448/448 - 0s - loss: 4.8273e-07 - val_loss: 4.3823e-09
Epoch 394/512
448/448 - 0s - loss: 4.5526e-07 - val_loss: 4.2595e-09
Epoch 395/512
448/448 - 0s - loss: 4.4270e-07 - val_loss: 4.0713e-09
Epoch 396/512
448/448 - 0s - loss: 4.1828e-07 - val_loss: 3.8193e-09
Epoch 397/512
448/448 - 0s - loss: 3.9372e-07 - val_loss: 3.6450e-09
Epoch 398/512
448/448 - 0s - loss: 3.7952e-07 - val_loss: 3.5461e-09
Epoch 399/512
448/448 - 0s - loss: 3.6533e-07 - val_loss: 3.3625e-09
Epoch 400/512
448/448 - 0s - loss: 3.4764e-07 - val_loss: 3.1169e-09
Epoch 401/512
448/448 - 0s - loss: 3.2309e-07 - val_loss: 3.0858e-09
Epoch 402/512
448/448 - 0s - loss: 3.2231e-07 - val_loss: 2.8878e-09
Epoch 403/512
448/448 - 0s - loss: 2.9634e-07 - val_loss: 2.7131e-09
Epoch 404/512
448/448 - 0s - loss: 2.8446e-07 - val_loss: 2.5833e-09
Epoch 405/512
448/448 - 0s - loss: 2.7110e-07 - val_loss: 2.5165e-09
Epoch 406/512
448/448 - 0s - loss: 2.6249e-07 - val_loss: 2.4567e-09
Epoch 407/512
448/448 - 0s - loss: 2.5193e-07 - val_loss: 2.2344e-09
Epoch 408/512
448/448 - 0s - loss: 2.2988e-07 - val_loss: 2.1523e-09
Epoch 409/512
448/448 - 0s - loss: 2.2715e-07 - val_loss: 2.0753e-09
Epoch 410/512
448/448 - 0s - loss: 2.1519e-07 - val_loss: 1.9529e-09
Epoch 411/512
448/448 - 0s - loss: 2.0362e-07 - val_loss: 1.8627e-09
Epoch 412/512
448/448 - 0s - loss: 1.9645e-07 - val_loss: 1.7446e-09
Epoch 413/512
448/448 - 0s - loss: 1.8170e-07 - val_loss: 1.7370e-09
Epoch 414/512
448/448 - 0s - loss: 1.8131e-07 - val_loss: 1.6641e-09
Epoch 415/512
448/448 - 0s - loss: 1.6878e-07 - val_loss: 1.5435e-09
Epoch 416/512
448/448 - 0s - loss: 1.6048e-07 - val_loss: 1.4364e-09
Epoch 417/512
448/448 - 0s - loss: 1.4898e-07 - val_loss: 1.4338e-09
Epoch 418/512
448/448 - 0s - loss: 1.5010e-07 - val_loss: 1.3635e-09
Epoch 419/512
448/448 - 0s - loss: 1.3969e-07 - val_loss: 1.2111e-09
Epoch 420/512
448/448 - 0s - loss: 1.2748e-07 - val_loss: 1.1711e-09
Epoch 421/512
448/448 - 0s - loss: 1.2515e-07 - val_loss: 1.2066e-09
Epoch 422/512
448/448 - 0s - loss: 1.2641e-07 - val_loss: 1.0855e-09
Epoch 423/512
448/448 - 0s - loss: 1.1003e-07 - val_loss: 1.0122e-09
Epoch 424/512
448/448 - 0s - loss: 1.0743e-07 - val_loss: 1.0182e-09
Epoch 425/512
448/448 - 0s - loss: 1.0728e-07 - val_loss: 9.4512e-10
Epoch 426/512
448/448 - 0s - loss: 9.6938e-08 - val_loss: 8.8892e-10
Epoch 427/512
448/448 - 0s - loss: 9.3312e-08 - val_loss: 8.6643e-10
Epoch 428/512
448/448 - 0s - loss: 9.0983e-08 - val_loss: 8.2031e-10
Epoch 429/512
448/448 - 0s - loss: 8.5551e-08 - val_loss: 7.4549e-10
Epoch 430/512
448/448 - 0s - loss: 7.7872e-08 - val_loss: 7.5387e-10
Epoch 431/512
448/448 - 0s - loss: 7.8989e-08 - val_loss: 7.4665e-10
Epoch 432/512
448/448 - 0s - loss: 7.6131e-08 - val_loss: 6.5507e-10
Epoch 433/512
448/448 - 0s - loss: 6.7075e-08 - val_loss: 6.1929e-10
Epoch 434/512
448/448 - 0s - loss: 6.5909e-08 - val_loss: 6.3028e-10
Epoch 435/512
448/448 - 0s - loss: 6.5772e-08 - val_loss: 5.9007e-10
Epoch 436/512
448/448 - 0s - loss: 6.0438e-08 - val_loss: 5.2433e-10
Epoch 437/512
448/448 - 0s - loss: 5.4930e-08 - val_loss: 5.2887e-10
Epoch 438/512
448/448 - 0s - loss: 5.6170e-08 - val_loss: 5.1681e-10
Epoch 439/512
448/448 - 0s - loss: 5.3075e-08 - val_loss: 4.6836e-10
Epoch 440/512
448/448 - 0s - loss: 4.8515e-08 - val_loss: 4.3996e-10
Epoch 441/512
448/448 - 0s - loss: 4.6209e-08 - val_loss: 4.4672e-10
Epoch 442/512
448/448 - 0s - loss: 4.6832e-08 - val_loss: 4.1270e-10
Epoch 443/512
448/448 - 0s - loss: 4.2315e-08 - val_loss: 3.7289e-10
Epoch 444/512
448/448 - 0s - loss: 3.8999e-08 - val_loss: 3.7461e-10
Epoch 445/512
448/448 - 0s - loss: 3.9747e-08 - val_loss: 3.7005e-10
Epoch 446/512
448/448 - 0s - loss: 3.7888e-08 - val_loss: 3.3508e-10
Epoch 447/512
448/448 - 0s - loss: 3.4557e-08 - val_loss: 3.0452e-10
Epoch 448/512
448/448 - 0s - loss: 3.2102e-08 - val_loss: 3.1541e-10
Epoch 449/512
448/448 - 0s - loss: 3.3122e-08 - val_loss: 3.1093e-10
Epoch 450/512
448/448 - 0s - loss: 3.1337e-08 - val_loss: 2.6968e-10
Epoch 451/512
448/448 - 0s - loss: 2.7347e-08 - val_loss: 2.5773e-10
Epoch 452/512
448/448 - 0s - loss: 2.7495e-08 - val_loss: 2.5718e-10
Epoch 453/512
448/448 - 0s - loss: 2.6956e-08 - val_loss: 2.3802e-10
Epoch 454/512
448/448 - 0s - loss: 2.4684e-08 - val_loss: 2.2049e-10
Epoch 455/512
448/448 - 0s - loss: 2.3118e-08 - val_loss: 2.2081e-10
Epoch 456/512
448/448 - 0s - loss: 2.3150e-08 - val_loss: 2.1478e-10
Epoch 457/512
448/448 - 0s - loss: 2.2203e-08 - val_loss: 1.9181e-10
Epoch 458/512
448/448 - 0s - loss: 1.9988e-08 - val_loss: 1.7739e-10
Epoch 459/512
448/448 - 0s - loss: 1.8689e-08 - val_loss: 1.8462e-10
Epoch 460/512
448/448 - 0s - loss: 1.9520e-08 - val_loss: 1.7642e-10
Epoch 461/512
448/448 - 0s - loss: 1.8058e-08 - val_loss: 1.5649e-10
Epoch 462/512
448/448 - 0s - loss: 1.6234e-08 - val_loss: 1.5255e-10
Epoch 463/512
448/448 - 0s - loss: 1.6244e-08 - val_loss: 1.5031e-10
Epoch 464/512
448/448 - 0s - loss: 1.5583e-08 - val_loss: 1.4617e-10
Epoch 465/512
448/448 - 0s - loss: 1.5038e-08 - val_loss: 1.3345e-10
Epoch 466/512
448/448 - 0s - loss: 1.3667e-08 - val_loss: 1.2565e-10
Epoch 467/512
448/448 - 0s - loss: 1.3182e-08 - val_loss: 1.2394e-10
Epoch 468/512
448/448 - 0s - loss: 1.2967e-08 - val_loss: 1.1921e-10
Epoch 469/512
448/448 - 0s - loss: 1.2163e-08 - val_loss: 1.1398e-10
Epoch 470/512
448/448 - 0s - loss: 1.1791e-08 - val_loss: 1.0552e-10
Epoch 471/512
448/448 - 0s - loss: 1.0920e-08 - val_loss: 9.9385e-11
Epoch 472/512
448/448 - 0s - loss: 1.0375e-08 - val_loss: 9.7205e-11
Epoch 473/512
448/448 - 0s - loss: 1.0250e-08 - val_loss: 9.3477e-11
Epoch 474/512
448/448 - 0s - loss: 9.6376e-09 - val_loss: 8.8972e-11
Epoch 475/512
448/448 - 0s - loss: 9.1837e-09 - val_loss: 8.4193e-11
Epoch 476/512
448/448 - 0s - loss: 8.7812e-09 - val_loss: 8.0063e-11
Epoch 477/512
448/448 - 0s - loss: 8.3264e-09 - val_loss: 7.7774e-11
Epoch 478/512
448/448 - 0s - loss: 8.1609e-09 - val_loss: 7.3606e-11
Epoch 479/512
448/448 - 0s - loss: 7.5765e-09 - val_loss: 7.1219e-11
Epoch 480/512
448/448 - 0s - loss: 7.3599e-09 - val_loss: 6.9637e-11
Epoch 481/512
448/448 - 0s - loss: 7.1586e-09 - val_loss: 6.5643e-11
Epoch 482/512
448/448 - 0s - loss: 6.7426e-09 - val_loss: 6.1098e-11
Epoch 483/512
448/448 - 0s - loss: 6.3135e-09 - val_loss: 5.7205e-11
Epoch 484/512
448/448 - 0s - loss: 5.9748e-09 - val_loss: 5.7623e-11
Epoch 485/512
448/448 - 0s - loss: 6.0117e-09 - val_loss: 5.6722e-11
Epoch 486/512
448/448 - 0s - loss: 5.8122e-09 - val_loss: 5.3103e-11
Epoch 487/512
448/448 - 0s - loss: 5.3911e-09 - val_loss: 4.9749e-11
Epoch 488/512
448/448 - 0s - loss: 5.0530e-09 - val_loss: 4.8019e-11
Epoch 489/512
448/448 - 0s - loss: 4.9794e-09 - val_loss: 4.7108e-11
Epoch 490/512
448/448 - 0s - loss: 4.8147e-09 - val_loss: 4.5337e-11
Epoch 491/512
448/448 - 0s - loss: 4.6479e-09 - val_loss: 4.1664e-11
Epoch 492/512
448/448 - 0s - loss: 4.2513e-09 - val_loss: 4.0033e-11
Epoch 493/512
448/448 - 0s - loss: 4.1556e-09 - val_loss: 3.9240e-11
Epoch 494/512
448/448 - 0s - loss: 4.0484e-09 - val_loss: 3.8769e-11
Epoch 495/512
448/448 - 0s - loss: 4.0042e-09 - val_loss: 3.7028e-11
Epoch 496/512
448/448 - 0s - loss: 3.7697e-09 - val_loss: 3.4101e-11
Epoch 497/512
448/448 - 0s - loss: 3.5051e-09 - val_loss: 3.1645e-11
Epoch 498/512
448/448 - 0s - loss: 3.2889e-09 - val_loss: 3.2257e-11
Epoch 499/512
448/448 - 0s - loss: 3.3713e-09 - val_loss: 3.2190e-11
Epoch 500/512
448/448 - 0s - loss: 3.2902e-09 - val_loss: 3.0786e-11
Epoch 501/512
448/448 - 0s - loss: 3.1283e-09 - val_loss: 2.8162e-11
Epoch 502/512
448/448 - 0s - loss: 2.8850e-09 - val_loss: 2.6379e-11
Epoch 503/512
448/448 - 0s - loss: 2.7223e-09 - val_loss: 2.6490e-11
Epoch 504/512
448/448 - 0s - loss: 2.7483e-09 - val_loss: 2.6445e-11
Epoch 505/512
448/448 - 0s - loss: 2.7170e-09 - val_loss: 2.5347e-11
Epoch 506/512
448/448 - 0s - loss: 2.5754e-09 - val_loss: 2.3796e-11
Epoch 507/512
448/448 - 0s - loss: 2.4227e-09 - val_loss: 2.2748e-11
Epoch 508/512
448/448 - 0s - loss: 2.3441e-09 - val_loss: 2.1924e-11
Epoch 509/512
448/448 - 0s - loss: 2.2569e-09 - val_loss: 2.1577e-11
Epoch 510/512
448/448 - 0s - loss: 2.2132e-09 - val_loss: 2.0843e-11
Epoch 511/512
448/448 - 0s - loss: 2.1366e-09 - val_loss: 2.0293e-11
Epoch 512/512
448/448 - 0s - loss: 2.0540e-09 - val_loss: 1.9540e-11
2024-04-17 11:30:37.352247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8803e-09 - val_loss: 1.8460e-09
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9202e-09 - val_loss: 1.8746e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.9223e-09 - val_loss: 1.8014e-09
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8273e-09 - val_loss: 1.6870e-09
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.7105e-09 - val_loss: 1.6164e-09
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.6400e-09 - val_loss: 1.5588e-09
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5884e-09 - val_loss: 1.5140e-09
Epoch 8/512

Epoch 00008: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5482e-09 - val_loss: 1.4708e-09
Epoch 9/512

Epoch 00009: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5127e-09 - val_loss: 1.4384e-09
Epoch 10/512

Epoch 00010: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4638e-09 - val_loss: 1.3766e-09
Epoch 11/512

Epoch 00011: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3950e-09 - val_loss: 1.3103e-09
Epoch 12/512

Epoch 00012: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3392e-09 - val_loss: 1.2769e-09
Epoch 13/512

Epoch 00013: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3106e-09 - val_loss: 1.2543e-09
Epoch 14/512

Epoch 00014: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2773e-09 - val_loss: 1.2319e-09
Epoch 15/512

Epoch 00015: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2523e-09 - val_loss: 1.1923e-09
Epoch 16/512

Epoch 00016: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2117e-09 - val_loss: 1.1510e-09
Epoch 17/512

Epoch 00017: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1701e-09 - val_loss: 1.1136e-09
Epoch 18/512

Epoch 00018: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1307e-09 - val_loss: 1.0771e-09
Epoch 19/512

Epoch 00019: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0951e-09 - val_loss: 1.0439e-09
Epoch 20/512

Epoch 00020: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0596e-09 - val_loss: 1.0166e-09
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0356e-09 - val_loss: 1.0131e-09
Epoch 22/512

Epoch 00022: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0375e-09 - val_loss: 9.8478e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.9283e-10 - val_loss: 9.4133e-10
Epoch 24/512

Epoch 00024: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.4914e-10 - val_loss: 9.0311e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.1294e-10 - val_loss: 8.7136e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.8296e-10 - val_loss: 8.5378e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7271e-10 - val_loss: 8.5399e-10
Epoch 28/512

Epoch 00028: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.6506e-10 - val_loss: 8.3267e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.4715e-10 - val_loss: 8.0262e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.0928e-10 - val_loss: 7.7257e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.8882e-10 - val_loss: 7.6152e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.6832e-10 - val_loss: 7.3496e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.4426e-10 - val_loss: 6.9871e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.0958e-10 - val_loss: 6.9006e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.0261e-10 - val_loss: 6.8478e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.9391e-10 - val_loss: 6.6406e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.7813e-10 - val_loss: 6.5191e-10
Epoch 38/512

Epoch 00038: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.5883e-10 - val_loss: 6.3025e-10
Epoch 39/512

Epoch 00039: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.4036e-10 - val_loss: 6.2292e-10
Epoch 40/512

Epoch 00040: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.3242e-10 - val_loss: 6.1109e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.1514e-10 - val_loss: 5.8086e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.8427e-10 - val_loss: 5.6842e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.7637e-10 - val_loss: 5.5022e-10
Epoch 44/512

Epoch 00044: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.5903e-10 - val_loss: 5.4481e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.5016e-10 - val_loss: 5.3604e-10
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.4681e-10 - val_loss: 5.2617e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3705e-10 - val_loss: 5.2842e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.3221e-10 - val_loss: 5.1090e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.1915e-10 - val_loss: 4.9889e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.0634e-10 - val_loss: 4.8497e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.8555e-10 - val_loss: 4.6292e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.6392e-10 - val_loss: 4.3814e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4798e-10 - val_loss: 4.3866e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4798e-10 - val_loss: 4.4359e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5757e-10 - val_loss: 4.4001e-10
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.4596e-10 - val_loss: 4.3361e-10
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.3425e-10 - val_loss: 4.1525e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.2189e-10 - val_loss: 4.0789e-10
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.1162e-10 - val_loss: 3.9417e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9859e-10 - val_loss: 3.8412e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9344e-10 - val_loss: 3.7887e-10
Epoch 62/512

Epoch 00062: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.8225e-10 - val_loss: 3.7513e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.8251e-10 - val_loss: 3.7487e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.7721e-10 - val_loss: 3.6005e-10
Epoch 65/512

Epoch 00065: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5941e-10 - val_loss: 3.4960e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5267e-10 - val_loss: 3.3839e-10
Epoch 67/512

Epoch 00067: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.4087e-10 - val_loss: 3.3707e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4493e-10 - val_loss: 3.4036e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.4398e-10 - val_loss: 3.3034e-10
Epoch 70/512

Epoch 00070: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.3552e-10 - val_loss: 3.2915e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.3439e-10 - val_loss: 3.2375e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.2493e-10 - val_loss: 3.1302e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1961e-10 - val_loss: 3.1499e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.1522e-10 - val_loss: 2.9873e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.0406e-10 - val_loss: 2.9758e-10
Epoch 76/512

Epoch 00076: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.9531e-10 - val_loss: 2.8556e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9189e-10 - val_loss: 2.8875e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8854e-10 - val_loss: 2.7199e-10
Epoch 79/512

Epoch 00079: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.7509e-10 - val_loss: 2.6986e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7414e-10 - val_loss: 2.6992e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7606e-10 - val_loss: 2.7644e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.7749e-10 - val_loss: 2.6445e-10
Epoch 83/512

Epoch 00083: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.6777e-10 - val_loss: 2.6370e-10
Epoch 84/512

Epoch 00084: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.6372e-10 - val_loss: 2.5009e-10
Epoch 85/512

Epoch 00085: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.5105e-10 - val_loss: 2.4595e-10
Epoch 86/512

Epoch 00086: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4816e-10 - val_loss: 2.3582e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3942e-10 - val_loss: 2.4195e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5040e-10 - val_loss: 2.4972e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5179e-10 - val_loss: 2.4301e-10
Epoch 90/512

Epoch 00090: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4534e-10 - val_loss: 2.3236e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.3110e-10 - val_loss: 2.1886e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.2170e-10 - val_loss: 2.1511e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.1710e-10 - val_loss: 2.1036e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1553e-10 - val_loss: 2.1312e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1520e-10 - val_loss: 2.1296e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1577e-10 - val_loss: 2.1374e-10
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.1692e-10 - val_loss: 2.0669e-10
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0870e-10 - val_loss: 2.0490e-10
Epoch 99/512

Epoch 00099: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0635e-10 - val_loss: 2.0348e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0511e-10 - val_loss: 2.0100e-10
Epoch 101/512

Epoch 00101: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0395e-10 - val_loss: 2.0001e-10
Epoch 102/512

Epoch 00102: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0176e-10 - val_loss: 1.9728e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0012e-10 - val_loss: 1.9734e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9985e-10 - val_loss: 1.9901e-10
Epoch 105/512

Epoch 00105: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.9942e-10 - val_loss: 1.9231e-10
Epoch 106/512

Epoch 00106: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.9172e-10 - val_loss: 1.8130e-10
Epoch 107/512

Epoch 00107: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8038e-10 - val_loss: 1.6852e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6904e-10 - val_loss: 1.7035e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7587e-10 - val_loss: 1.7720e-10
Epoch 110/512

Epoch 00110: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.7610e-10 - val_loss: 1.6643e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6754e-10 - val_loss: 1.6891e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7297e-10 - val_loss: 1.7127e-10
Epoch 113/512

Epoch 00113: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.6832e-10 - val_loss: 1.5793e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5884e-10 - val_loss: 1.5889e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6210e-10 - val_loss: 1.5831e-10
Epoch 116/512

Epoch 00116: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5856e-10 - val_loss: 1.5396e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5562e-10 - val_loss: 1.5809e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6171e-10 - val_loss: 1.5809e-10
Epoch 119/512

Epoch 00119: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5620e-10 - val_loss: 1.4655e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4734e-10 - val_loss: 1.4672e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4943e-10 - val_loss: 1.4751e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4804e-10 - val_loss: 1.4298e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4603e-10 - val_loss: 1.5030e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5305e-10 - val_loss: 1.4645e-10
Epoch 125/512

Epoch 00125: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4631e-10 - val_loss: 1.4053e-10
Epoch 126/512

Epoch 00126: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4029e-10 - val_loss: 1.3948e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4416e-10 - val_loss: 1.4712e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.4530e-10 - val_loss: 1.3841e-10
Epoch 129/512

Epoch 00129: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3981e-10 - val_loss: 1.3748e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3944e-10 - val_loss: 1.3945e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3814e-10 - val_loss: 1.3352e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3294e-10 - val_loss: 1.2471e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2377e-10 - val_loss: 1.2066e-10
Epoch 134/512

Epoch 00134: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.2040e-10 - val_loss: 1.1691e-10
Epoch 135/512

Epoch 00135: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1767e-10 - val_loss: 1.1623e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1739e-10 - val_loss: 1.1813e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2006e-10 - val_loss: 1.2705e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2739e-10 - val_loss: 1.2423e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2517e-10 - val_loss: 1.2181e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2217e-10 - val_loss: 1.2090e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2423e-10 - val_loss: 1.2214e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2286e-10 - val_loss: 1.1790e-10
Epoch 143/512

Epoch 00143: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1815e-10 - val_loss: 1.1461e-10
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1550e-10 - val_loss: 1.1545e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1582e-10 - val_loss: 1.1375e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1381e-10 - val_loss: 1.0809e-10
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0973e-10 - val_loss: 1.1059e-10
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0999e-10 - val_loss: 1.0511e-10
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0461e-10 - val_loss: 1.0371e-10
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0633e-10 - val_loss: 1.0674e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0867e-10 - val_loss: 1.0607e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0616e-10 - val_loss: 1.0109e-10
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0101e-10 - val_loss: 1.0334e-10
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0416e-10 - val_loss: 1.0245e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0484e-10 - val_loss: 1.0497e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0621e-10 - val_loss: 1.0701e-10
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0842e-10 - val_loss: 1.0373e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0388e-10 - val_loss: 9.8962e-11
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.9003e-11 - val_loss: 9.6283e-11
Epoch 160/512

Epoch 00160: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.5869e-11 - val_loss: 9.2923e-11
Epoch 161/512

Epoch 00161: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.9880e-11 - val_loss: 8.7509e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0246e-11 - val_loss: 9.2650e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4094e-11 - val_loss: 9.5312e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6007e-11 - val_loss: 9.4819e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5400e-11 - val_loss: 8.9465e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9543e-11 - val_loss: 8.9214e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0974e-11 - val_loss: 9.1851e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2723e-11 - val_loss: 9.0614e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2241e-11 - val_loss: 9.2516e-11
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.9620e-11 - val_loss: 8.3846e-11
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.3661e-11 - val_loss: 8.2034e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2493e-11 - val_loss: 8.2796e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3724e-11 - val_loss: 8.3964e-11
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.5193e-11 - val_loss: 8.1936e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.0812e-11 - val_loss: 7.5666e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5958e-11 - val_loss: 7.7706e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9394e-11 - val_loss: 8.3592e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7042e-11 - val_loss: 8.9029e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9006e-11 - val_loss: 8.5543e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3841e-11 - val_loss: 7.8126e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8192e-11 - val_loss: 7.8442e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0179e-11 - val_loss: 8.0433e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0169e-11 - val_loss: 7.8411e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9440e-11 - val_loss: 7.7494e-11
Epoch 185/512

Epoch 00185: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.5792e-11 - val_loss: 6.9807e-11
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.7975e-11 - val_loss: 6.2976e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3224e-11 - val_loss: 6.5226e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8753e-11 - val_loss: 7.2510e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5502e-11 - val_loss: 7.6274e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7783e-11 - val_loss: 7.7418e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6396e-11 - val_loss: 7.2749e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3215e-11 - val_loss: 7.1623e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3360e-11 - val_loss: 7.4148e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6072e-11 - val_loss: 7.5313e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4662e-11 - val_loss: 7.1839e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9703e-11 - val_loss: 6.4189e-11
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.1921e-11 - val_loss: 5.6341e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.4997e-11 - val_loss: 5.2191e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3221e-11 - val_loss: 5.7299e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0446e-11 - val_loss: 6.5007e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7035e-11 - val_loss: 6.9772e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8339e-11 - val_loss: 6.2485e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1174e-11 - val_loss: 5.6758e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6715e-11 - val_loss: 5.5158e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6160e-11 - val_loss: 5.8830e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1270e-11 - val_loss: 6.4778e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6880e-11 - val_loss: 6.8152e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9375e-11 - val_loss: 6.5972e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4162e-11 - val_loss: 5.9435e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8201e-11 - val_loss: 5.5655e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.5584e-11 - val_loss: 5.1934e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2102e-11 - val_loss: 5.2807e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3879e-11 - val_loss: 5.6244e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8190e-11 - val_loss: 6.1674e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3257e-11 - val_loss: 6.0843e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0736e-11 - val_loss: 5.6317e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.4761e-11 - val_loss: 5.0486e-11
Epoch 218/512

Epoch 00218: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.0115e-11 - val_loss: 4.8278e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9284e-11 - val_loss: 5.1720e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3477e-11 - val_loss: 5.5705e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7534e-11 - val_loss: 6.1619e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2427e-11 - val_loss: 5.8868e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7192e-11 - val_loss: 5.1206e-11
Epoch 224/512

Epoch 00224: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.0161e-11 - val_loss: 4.7066e-11
Epoch 225/512

Epoch 00225: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.5987e-11 - val_loss: 4.2991e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.3198e-11 - val_loss: 4.2203e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4188e-11 - val_loss: 4.8713e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1558e-11 - val_loss: 5.6351e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.7634e-11 - val_loss: 5.7542e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6276e-11 - val_loss: 5.2351e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1082e-11 - val_loss: 4.7450e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7091e-11 - val_loss: 4.5355e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5271e-11 - val_loss: 4.4252e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.4010e-11 - val_loss: 4.2125e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1763e-11 - val_loss: 4.2782e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5305e-11 - val_loss: 5.0312e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1654e-11 - val_loss: 5.2825e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1735e-11 - val_loss: 4.9989e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9449e-11 - val_loss: 4.7427e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7905e-11 - val_loss: 4.7460e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7644e-11 - val_loss: 4.6894e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7405e-11 - val_loss: 4.6092e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5437e-11 - val_loss: 4.4127e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5827e-11 - val_loss: 4.9277e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1523e-11 - val_loss: 5.4646e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6316e-11 - val_loss: 5.7543e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5924e-11 - val_loss: 5.0858e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9393e-11 - val_loss: 4.5363e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4984e-11 - val_loss: 4.2779e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.2374e-11 - val_loss: 4.1628e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.1436e-11 - val_loss: 3.9294e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9473e-11 - val_loss: 4.0068e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0163e-11 - val_loss: 4.1981e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3621e-11 - val_loss: 4.6197e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9118e-11 - val_loss: 5.2003e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2602e-11 - val_loss: 4.9858e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7958e-11 - val_loss: 4.4021e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3481e-11 - val_loss: 4.1449e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0924e-11 - val_loss: 3.9904e-11
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9418e-11 - val_loss: 3.7648e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.7436e-11 - val_loss: 3.6773e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7655e-11 - val_loss: 3.8928e-11
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0250e-11 - val_loss: 4.3687e-11
Epoch 264/512

Epoch 00264: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6489e-11 - val_loss: 4.8980e-11
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.1119e-11 - val_loss: 5.3784e-11
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.3621e-11 - val_loss: 5.0180e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9109e-11 - val_loss: 4.5581e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5045e-11 - val_loss: 4.0889e-11
Epoch 269/512

Epoch 00269: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.9205e-11 - val_loss: 3.5494e-11
Epoch 270/512

Epoch 00270: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.4713e-11 - val_loss: 3.3577e-11
Epoch 271/512

Epoch 00271: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.3979e-11 - val_loss: 3.3326e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3844e-11 - val_loss: 3.5201e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6162e-11 - val_loss: 3.7554e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6729e-11 - val_loss: 3.6926e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9023e-11 - val_loss: 4.2865e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5143e-11 - val_loss: 4.8901e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0433e-11 - val_loss: 4.9836e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8219e-11 - val_loss: 4.2725e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0413e-11 - val_loss: 3.5098e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5297e-11 - val_loss: 3.4653e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5439e-11 - val_loss: 3.5949e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5588e-11 - val_loss: 3.3683e-11
Epoch 283/512

Epoch 00283: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.3480e-11 - val_loss: 3.2672e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3345e-11 - val_loss: 3.3657e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3721e-11 - val_loss: 3.4717e-11
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5454e-11 - val_loss: 3.7137e-11
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8196e-11 - val_loss: 4.0118e-11
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0786e-11 - val_loss: 3.8193e-11
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7629e-11 - val_loss: 3.6150e-11
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.5236e-11 - val_loss: 3.1083e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1458e-11 - val_loss: 3.1708e-11
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.1271e-11 - val_loss: 3.0634e-11
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.0515e-11 - val_loss: 2.9591e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0250e-11 - val_loss: 3.2314e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3342e-11 - val_loss: 3.4667e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4580e-11 - val_loss: 3.3789e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3424e-11 - val_loss: 3.1860e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2324e-11 - val_loss: 3.3331e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2913e-11 - val_loss: 3.2251e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4283e-11 - val_loss: 3.7681e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8479e-11 - val_loss: 3.8778e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7544e-11 - val_loss: 3.5385e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5009e-11 - val_loss: 3.3431e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2938e-11 - val_loss: 3.1369e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 3.0900e-11 - val_loss: 2.9107e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8708e-11 - val_loss: 2.7137e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7995e-11 - val_loss: 2.8595e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9054e-11 - val_loss: 2.8409e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8380e-11 - val_loss: 2.8246e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8948e-11 - val_loss: 3.0467e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1046e-11 - val_loss: 3.0641e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1195e-11 - val_loss: 3.1400e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1720e-11 - val_loss: 3.1360e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.2408e-11 - val_loss: 3.5090e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6033e-11 - val_loss: 3.6809e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5780e-11 - val_loss: 3.4039e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3400e-11 - val_loss: 3.1152e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0597e-11 - val_loss: 2.8946e-11
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.8446e-11 - val_loss: 2.6941e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.6432e-11 - val_loss: 2.5076e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5739e-11 - val_loss: 2.6004e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5744e-11 - val_loss: 2.6136e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6679e-11 - val_loss: 2.7761e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8101e-11 - val_loss: 2.8169e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8378e-11 - val_loss: 2.6871e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6603e-11 - val_loss: 2.7081e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7650e-11 - val_loss: 2.8654e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8938e-11 - val_loss: 2.8171e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8332e-11 - val_loss: 2.8092e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7555e-11 - val_loss: 2.6324e-11
Epoch 331/512

Epoch 00331: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.4768e-11 - val_loss: 2.0892e-11
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.9393e-11 - val_loss: 1.7085e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6913e-11 - val_loss: 1.7227e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7661e-11 - val_loss: 1.8492e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9656e-11 - val_loss: 2.1221e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2468e-11 - val_loss: 2.4179e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4912e-11 - val_loss: 2.6037e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6900e-11 - val_loss: 2.7111e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7750e-11 - val_loss: 2.8776e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9207e-11 - val_loss: 2.8795e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.9101e-11 - val_loss: 2.8075e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8316e-11 - val_loss: 2.8286e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7699e-11 - val_loss: 2.5908e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5606e-11 - val_loss: 2.5821e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6626e-11 - val_loss: 2.6787e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6494e-11 - val_loss: 2.5404e-11
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5574e-11 - val_loss: 2.5076e-11
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5681e-11 - val_loss: 2.6237e-11
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5524e-11 - val_loss: 2.1733e-11
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0260e-11 - val_loss: 1.8294e-11
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.8156e-11 - val_loss: 1.7018e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6881e-11 - val_loss: 1.7877e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8612e-11 - val_loss: 2.0249e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1428e-11 - val_loss: 2.2550e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3784e-11 - val_loss: 2.5436e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5459e-11 - val_loss: 2.5474e-11
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5729e-11 - val_loss: 2.5724e-11
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6356e-11 - val_loss: 2.7439e-11
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7098e-11 - val_loss: 2.6541e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6186e-11 - val_loss: 2.4749e-11
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4752e-11 - val_loss: 2.3747e-11
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3370e-11 - val_loss: 2.3272e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2905e-11 - val_loss: 2.2299e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2266e-11 - val_loss: 2.2866e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3544e-11 - val_loss: 2.3855e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3341e-11 - val_loss: 2.3098e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3019e-11 - val_loss: 2.3096e-11
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2369e-11 - val_loss: 2.1110e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1678e-11 - val_loss: 2.1939e-11
Epoch 370/512

Epoch 00370: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 2.0736e-11 - val_loss: 1.6820e-11
Epoch 371/512

Epoch 00371: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.5761e-11 - val_loss: 1.4044e-11
Epoch 372/512

Epoch 00372: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3873e-11 - val_loss: 1.3550e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3621e-11 - val_loss: 1.3428e-11
Epoch 374/512

Epoch 00374: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3000e-11 - val_loss: 1.2472e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3012e-11 - val_loss: 1.4192e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5034e-11 - val_loss: 1.6636e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8187e-11 - val_loss: 2.1327e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2469e-11 - val_loss: 2.4186e-11
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4666e-11 - val_loss: 2.4968e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5184e-11 - val_loss: 2.4458e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4464e-11 - val_loss: 2.4697e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5504e-11 - val_loss: 2.5675e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5134e-11 - val_loss: 2.3737e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3055e-11 - val_loss: 2.3536e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3914e-11 - val_loss: 2.2828e-11
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2080e-11 - val_loss: 2.0564e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0492e-11 - val_loss: 1.9989e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0618e-11 - val_loss: 2.0512e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0433e-11 - val_loss: 2.0853e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0930e-11 - val_loss: 1.9924e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0282e-11 - val_loss: 2.0409e-11
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0740e-11 - val_loss: 2.0853e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1442e-11 - val_loss: 2.1758e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1205e-11 - val_loss: 1.8753e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7258e-11 - val_loss: 1.4072e-11
Epoch 396/512

Epoch 00396: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.3055e-11 - val_loss: 1.1558e-11
Epoch 397/512

Epoch 00397: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.1199e-11 - val_loss: 1.0870e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1074e-11 - val_loss: 1.1012e-11
Epoch 399/512

Epoch 00399: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0486e-11 - val_loss: 1.0181e-11
Epoch 400/512

Epoch 00400: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0271e-11 - val_loss: 1.0090e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0098e-11 - val_loss: 9.7194e-12
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7829e-12 - val_loss: 1.0358e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0839e-11 - val_loss: 1.2853e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4223e-11 - val_loss: 1.6939e-11
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7904e-11 - val_loss: 1.9541e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0589e-11 - val_loss: 2.1505e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1482e-11 - val_loss: 2.2003e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2649e-11 - val_loss: 2.2604e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2619e-11 - val_loss: 2.1917e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1402e-11 - val_loss: 1.9838e-11
Epoch 411/512

Epoch 00411: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9620e-11 - val_loss: 1.9052e-11
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9379e-11 - val_loss: 1.8912e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9339e-11 - val_loss: 1.9148e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8929e-11 - val_loss: 1.8835e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8491e-11 - val_loss: 1.7682e-11
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7982e-11 - val_loss: 1.8043e-11
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8104e-11 - val_loss: 1.7340e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7155e-11 - val_loss: 1.6697e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6852e-11 - val_loss: 1.7215e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7641e-11 - val_loss: 1.8367e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8547e-11 - val_loss: 1.8333e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8569e-11 - val_loss: 1.8557e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7644e-11 - val_loss: 1.4119e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2931e-11 - val_loss: 1.1121e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0526e-11 - val_loss: 1.0216e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 1.0202e-11 - val_loss: 9.5499e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5726e-12 - val_loss: 9.6912e-12
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8424e-12 - val_loss: 1.0048e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0078e-11 - val_loss: 9.6123e-12
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6963e-12 - val_loss: 9.8965e-12
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8014e-12 - val_loss: 9.7032e-12
Epoch 432/512

Epoch 00432: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.4629e-12 - val_loss: 9.2351e-12
Epoch 433/512

Epoch 00433: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.1516e-12 - val_loss: 8.9754e-12
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3068e-12 - val_loss: 9.6658e-12
Epoch 435/512

Epoch 00435: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 9.5037e-12 - val_loss: 8.8612e-12
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0843e-12 - val_loss: 9.8843e-12
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0712e-11 - val_loss: 1.1695e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3005e-11 - val_loss: 1.5748e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6652e-11 - val_loss: 1.8322e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9198e-11 - val_loss: 2.0896e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1316e-11 - val_loss: 2.1532e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1334e-11 - val_loss: 2.1064e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0668e-11 - val_loss: 2.0276e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0352e-11 - val_loss: 1.9681e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9410e-11 - val_loss: 1.9611e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9618e-11 - val_loss: 1.8756e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8354e-11 - val_loss: 1.8932e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8740e-11 - val_loss: 1.8138e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8284e-11 - val_loss: 1.8514e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8355e-11 - val_loss: 1.8005e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7155e-11 - val_loss: 1.6179e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6332e-11 - val_loss: 1.6999e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6888e-11 - val_loss: 1.6069e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6120e-11 - val_loss: 1.6954e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7122e-11 - val_loss: 1.5742e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5584e-11 - val_loss: 1.5427e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5986e-11 - val_loss: 1.5991e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5919e-11 - val_loss: 1.4999e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4675e-11 - val_loss: 1.3139e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2406e-11 - val_loss: 1.0763e-11
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0199e-11 - val_loss: 9.0164e-12
Epoch 462/512

Epoch 00462: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 8.2219e-12 - val_loss: 7.2235e-12
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 7.1797e-12 - val_loss: 7.1177e-12
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.8969e-12 - val_loss: 6.4944e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6013e-12 - val_loss: 6.8490e-12
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8518e-12 - val_loss: 6.6051e-12
Epoch 467/512

Epoch 00467: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 6.1176e-12 - val_loss: 5.6870e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9761e-12 - val_loss: 6.3659e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2429e-12 - val_loss: 6.1967e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4453e-12 - val_loss: 6.8033e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8268e-12 - val_loss: 7.0687e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8656e-12 - val_loss: 6.8662e-12
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7324e-12 - val_loss: 6.3741e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3480e-12 - val_loss: 6.1293e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1874e-12 - val_loss: 6.0754e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3247e-12 - val_loss: 6.8210e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7586e-12 - val_loss: 6.5791e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1377e-12 - val_loss: 5.7034e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9784e-12 - val_loss: 6.2025e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2312e-12 - val_loss: 6.7780e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5749e-12 - val_loss: 8.9971e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0272e-11 - val_loss: 1.3162e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3906e-11 - val_loss: 1.5495e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6101e-11 - val_loss: 1.6840e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6962e-11 - val_loss: 1.6129e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5893e-11 - val_loss: 1.5340e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5050e-11 - val_loss: 1.4408e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4022e-11 - val_loss: 1.3606e-11
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3463e-11 - val_loss: 1.3160e-11
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3281e-11 - val_loss: 1.3561e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3450e-11 - val_loss: 1.3260e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3509e-11 - val_loss: 1.3610e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3650e-11 - val_loss: 1.3326e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3096e-11 - val_loss: 1.2232e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2260e-11 - val_loss: 1.2536e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2827e-11 - val_loss: 1.2834e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2653e-11 - val_loss: 1.2704e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2884e-11 - val_loss: 1.2932e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2597e-11 - val_loss: 1.2212e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2475e-11 - val_loss: 1.2875e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3246e-11 - val_loss: 1.4644e-11
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4585e-11 - val_loss: 1.4361e-11
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4379e-11 - val_loss: 1.3651e-11
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3259e-11 - val_loss: 1.2828e-11
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2831e-11 - val_loss: 1.2238e-11
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1974e-11 - val_loss: 1.1334e-11
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1437e-11 - val_loss: 1.0473e-11
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7776e-12 - val_loss: 8.2589e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8142e-12 - val_loss: 6.5585e-12
Epoch 510/512

Epoch 00510: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 5.9883e-12 - val_loss: 5.0757e-12
Epoch 511/512

Epoch 00511: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.9846e-12 - val_loss: 4.8150e-12
Epoch 512/512

Epoch 00512: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/addition_weights.h5
448/448 - 0s - loss: 4.4693e-12 - val_loss: 4.0097e-12
Train on 448 samples, validate on 448 samples
Epoch 1/512
448/448 - 1s - loss: 0.0457 - val_loss: 7.6897e-04
Epoch 2/512
448/448 - 0s - loss: 0.0021 - val_loss: 4.1079e-04
Epoch 3/512
448/448 - 0s - loss: 0.0010 - val_loss: 3.1965e-04
Epoch 4/512
448/448 - 0s - loss: 9.6788e-04 - val_loss: 2.1726e-04
Epoch 5/512
448/448 - 0s - loss: 0.0022 - val_loss: 1.4083e-04
Epoch 6/512
448/448 - 0s - loss: 9.5798e-04 - val_loss: 1.3593e-04
Epoch 7/512
448/448 - 0s - loss: 5.5040e-04 - val_loss: 1.0962e-04
Epoch 8/512
448/448 - 0s - loss: 8.1422e-04 - val_loss: 4.6229e-05
Epoch 9/512
448/448 - 0s - loss: 0.0017 - val_loss: 2.1992e-05
Epoch 10/512
448/448 - 0s - loss: 6.1776e-04 - val_loss: 2.0476e-05
Epoch 11/512
448/448 - 0s - loss: 5.2761e-04 - val_loss: 1.1026e-05
Epoch 12/512
448/448 - 0s - loss: 0.0012 - val_loss: 1.0515e-05
Epoch 13/512
448/448 - 0s - loss: 6.7696e-04 - val_loss: 1.2263e-05
Epoch 14/512
448/448 - 0s - loss: 3.9092e-04 - val_loss: 1.4281e-05
Epoch 15/512
448/448 - 0s - loss: 7.8053e-04 - val_loss: 1.9028e-05
Epoch 16/512
448/448 - 0s - loss: 6.5719e-04 - val_loss: 2.0454e-05
Epoch 17/512
448/448 - 0s - loss: 3.1836e-04 - val_loss: 2.0662e-05
Epoch 18/512
448/448 - 0s - loss: 5.1599e-04 - val_loss: 2.2056e-05
Epoch 19/512
448/448 - 0s - loss: 5.5314e-04 - val_loss: 2.2919e-05
Epoch 20/512
448/448 - 0s - loss: 2.7031e-04 - val_loss: 2.1262e-05
Epoch 21/512
448/448 - 0s - loss: 3.5870e-04 - val_loss: 2.1063e-05
Epoch 22/512
448/448 - 0s - loss: 4.2729e-04 - val_loss: 2.1236e-05
Epoch 23/512
448/448 - 0s - loss: 2.4033e-04 - val_loss: 1.8615e-05
Epoch 24/512
448/448 - 0s - loss: 2.5993e-04 - val_loss: 1.7435e-05
Epoch 25/512
448/448 - 0s - loss: 3.1158e-04 - val_loss: 1.7047e-05
Epoch 26/512
448/448 - 0s - loss: 2.1296e-04 - val_loss: 1.4516e-05
Epoch 27/512
448/448 - 0s - loss: 1.9558e-04 - val_loss: 1.2952e-05
Epoch 28/512
448/448 - 0s - loss: 2.2815e-04 - val_loss: 1.1902e-05
Epoch 29/512
448/448 - 0s - loss: 1.8740e-04 - val_loss: 9.9836e-06
Epoch 30/512
448/448 - 0s - loss: 1.5921e-04 - val_loss: 8.6045e-06
Epoch 31/512
448/448 - 0s - loss: 1.7610e-04 - val_loss: 7.7016e-06
Epoch 32/512
448/448 - 0s - loss: 1.5571e-04 - val_loss: 6.9214e-06
Epoch 33/512
448/448 - 0s - loss: 1.3641e-04 - val_loss: 6.3539e-06
Epoch 34/512
448/448 - 0s - loss: 1.4218e-04 - val_loss: 6.0877e-06
Epoch 35/512
448/448 - 0s - loss: 1.3005e-04 - val_loss: 5.9838e-06
Epoch 36/512
448/448 - 0s - loss: 1.2156e-04 - val_loss: 5.9052e-06
Epoch 37/512
448/448 - 0s - loss: 1.2067e-04 - val_loss: 5.8556e-06
Epoch 38/512
448/448 - 0s - loss: 1.1010e-04 - val_loss: 5.7552e-06
Epoch 39/512
448/448 - 0s - loss: 1.0927e-04 - val_loss: 5.5708e-06
Epoch 40/512
448/448 - 0s - loss: 1.0398e-04 - val_loss: 5.3173e-06
Epoch 41/512
448/448 - 0s - loss: 1.0061e-04 - val_loss: 4.9382e-06
Epoch 42/512
448/448 - 0s - loss: 9.5128e-05 - val_loss: 4.5145e-06
Epoch 43/512
448/448 - 0s - loss: 9.1941e-05 - val_loss: 4.2236e-06
Epoch 44/512
448/448 - 0s - loss: 9.2039e-05 - val_loss: 3.6557e-06
Epoch 45/512
448/448 - 0s - loss: 8.5752e-05 - val_loss: 3.4353e-06
Epoch 46/512
448/448 - 0s - loss: 8.3972e-05 - val_loss: 3.3246e-06
Epoch 47/512
448/448 - 0s - loss: 8.1835e-05 - val_loss: 3.2940e-06
Epoch 48/512
448/448 - 0s - loss: 7.9112e-05 - val_loss: 3.3359e-06
Epoch 49/512
448/448 - 0s - loss: 7.7862e-05 - val_loss: 2.9721e-06
Epoch 50/512
448/448 - 0s - loss: 7.4770e-05 - val_loss: 2.7473e-06
Epoch 51/512
448/448 - 0s - loss: 7.1907e-05 - val_loss: 2.5329e-06
Epoch 52/512
448/448 - 0s - loss: 7.1757e-05 - val_loss: 2.3935e-06
Epoch 53/512
448/448 - 0s - loss: 7.1016e-05 - val_loss: 2.2198e-06
Epoch 54/512
448/448 - 0s - loss: 6.5650e-05 - val_loss: 2.0496e-06
Epoch 55/512
448/448 - 0s - loss: 6.6667e-05 - val_loss: 1.9810e-06
Epoch 56/512
448/448 - 0s - loss: 6.5837e-05 - val_loss: 1.8205e-06
Epoch 57/512
448/448 - 0s - loss: 6.1826e-05 - val_loss: 1.6678e-06
Epoch 58/512
448/448 - 0s - loss: 6.1094e-05 - val_loss: 1.5683e-06
Epoch 59/512
448/448 - 0s - loss: 6.1621e-05 - val_loss: 1.4559e-06
Epoch 60/512
448/448 - 0s - loss: 5.9288e-05 - val_loss: 1.3360e-06
Epoch 61/512
448/448 - 0s - loss: 5.6542e-05 - val_loss: 1.2636e-06
Epoch 62/512
448/448 - 0s - loss: 5.7514e-05 - val_loss: 1.1939e-06
Epoch 63/512
448/448 - 0s - loss: 5.5230e-05 - val_loss: 1.0986e-06
Epoch 64/512
448/448 - 0s - loss: 5.3221e-05 - val_loss: 1.0503e-06
Epoch 65/512
448/448 - 0s - loss: 5.3778e-05 - val_loss: 9.8939e-07
Epoch 66/512
448/448 - 0s - loss: 5.2224e-05 - val_loss: 9.1699e-07
Epoch 67/512
448/448 - 0s - loss: 5.0644e-05 - val_loss: 8.5352e-07
Epoch 68/512
448/448 - 0s - loss: 5.0233e-05 - val_loss: 8.1476e-07
Epoch 69/512
448/448 - 0s - loss: 4.9427e-05 - val_loss: 7.6136e-07
Epoch 70/512
448/448 - 0s - loss: 4.7832e-05 - val_loss: 7.2798e-07
Epoch 71/512
448/448 - 0s - loss: 4.7702e-05 - val_loss: 6.8555e-07
Epoch 72/512
448/448 - 0s - loss: 4.6447e-05 - val_loss: 6.3793e-07
Epoch 73/512
448/448 - 0s - loss: 4.5941e-05 - val_loss: 6.0142e-07
Epoch 74/512
448/448 - 0s - loss: 4.4813e-05 - val_loss: 5.6567e-07
Epoch 75/512
448/448 - 0s - loss: 4.3509e-05 - val_loss: 5.3756e-07
Epoch 76/512
448/448 - 0s - loss: 4.3095e-05 - val_loss: 5.1861e-07
Epoch 77/512
448/448 - 0s - loss: 4.3792e-05 - val_loss: 4.8180e-07
Epoch 78/512
448/448 - 0s - loss: 4.0596e-05 - val_loss: 4.5342e-07
Epoch 79/512
448/448 - 0s - loss: 4.1166e-05 - val_loss: 4.4531e-07
Epoch 80/512
448/448 - 0s - loss: 4.0791e-05 - val_loss: 4.2514e-07
Epoch 81/512
448/448 - 0s - loss: 3.9440e-05 - val_loss: 3.9899e-07
Epoch 82/512
448/448 - 0s - loss: 3.9152e-05 - val_loss: 3.8178e-07
Epoch 83/512
448/448 - 0s - loss: 3.8588e-05 - val_loss: 3.5690e-07
Epoch 84/512
448/448 - 0s - loss: 3.7072e-05 - val_loss: 3.5272e-07
Epoch 85/512
448/448 - 0s - loss: 3.7215e-05 - val_loss: 3.4243e-07
Epoch 86/512
448/448 - 0s - loss: 3.7395e-05 - val_loss: 3.1650e-07
Epoch 87/512
448/448 - 0s - loss: 3.4990e-05 - val_loss: 3.0418e-07
Epoch 88/512
448/448 - 0s - loss: 3.4818e-05 - val_loss: 3.0530e-07
Epoch 89/512
448/448 - 0s - loss: 3.5425e-05 - val_loss: 2.9222e-07
Epoch 90/512
448/448 - 0s - loss: 3.4238e-05 - val_loss: 2.7513e-07
Epoch 91/512
448/448 - 0s - loss: 3.3026e-05 - val_loss: 2.6704e-07
Epoch 92/512
448/448 - 0s - loss: 3.3022e-05 - val_loss: 2.6308e-07
Epoch 93/512
448/448 - 0s - loss: 3.2627e-05 - val_loss: 2.4840e-07
Epoch 94/512
448/448 - 0s - loss: 3.1459e-05 - val_loss: 2.4192e-07
Epoch 95/512
448/448 - 0s - loss: 3.1677e-05 - val_loss: 2.3392e-07
Epoch 96/512
448/448 - 0s - loss: 3.0795e-05 - val_loss: 2.2560e-07
Epoch 97/512
448/448 - 0s - loss: 3.0376e-05 - val_loss: 2.1561e-07
Epoch 98/512
448/448 - 0s - loss: 2.9563e-05 - val_loss: 2.1275e-07
Epoch 99/512
448/448 - 0s - loss: 2.9670e-05 - val_loss: 2.0408e-07
Epoch 100/512
448/448 - 0s - loss: 2.8633e-05 - val_loss: 1.9909e-07
Epoch 101/512
448/448 - 0s - loss: 2.7999e-05 - val_loss: 1.9902e-07
Epoch 102/512
448/448 - 0s - loss: 2.8204e-05 - val_loss: 1.9173e-07
Epoch 103/512
448/448 - 0s - loss: 2.7144e-05 - val_loss: 1.8363e-07
Epoch 104/512
448/448 - 0s - loss: 2.6305e-05 - val_loss: 1.8512e-07
Epoch 105/512
448/448 - 0s - loss: 2.7068e-05 - val_loss: 1.8056e-07
Epoch 106/512
448/448 - 0s - loss: 2.5777e-05 - val_loss: 1.7300e-07
Epoch 107/512
448/448 - 0s - loss: 2.5117e-05 - val_loss: 1.7091e-07
Epoch 108/512
448/448 - 0s - loss: 2.5040e-05 - val_loss: 1.7171e-07
Epoch 109/512
448/448 - 0s - loss: 2.4799e-05 - val_loss: 1.6636e-07
Epoch 110/512
448/448 - 0s - loss: 2.4073e-05 - val_loss: 1.5983e-07
Epoch 111/512
448/448 - 0s - loss: 2.3494e-05 - val_loss: 1.5658e-07
Epoch 112/512
448/448 - 0s - loss: 2.2936e-05 - val_loss: 1.5737e-07
Epoch 113/512
448/448 - 0s - loss: 2.3191e-05 - val_loss: 1.5160e-07
Epoch 114/512
448/448 - 0s - loss: 2.2055e-05 - val_loss: 1.4828e-07
Epoch 115/512
448/448 - 0s - loss: 2.1989e-05 - val_loss: 1.4606e-07
Epoch 116/512
448/448 - 0s - loss: 2.1578e-05 - val_loss: 1.4134e-07
Epoch 117/512
448/448 - 0s - loss: 2.0952e-05 - val_loss: 1.3960e-07
Epoch 118/512
448/448 - 0s - loss: 2.0953e-05 - val_loss: 1.3652e-07
Epoch 119/512
448/448 - 0s - loss: 2.0428e-05 - val_loss: 1.2940e-07
Epoch 120/512
448/448 - 0s - loss: 1.9503e-05 - val_loss: 1.2805e-07
Epoch 121/512
448/448 - 0s - loss: 1.9640e-05 - val_loss: 1.2765e-07
Epoch 122/512
448/448 - 0s - loss: 1.9266e-05 - val_loss: 1.2382e-07
Epoch 123/512
448/448 - 0s - loss: 1.8591e-05 - val_loss: 1.2170e-07
Epoch 124/512
448/448 - 0s - loss: 1.8462e-05 - val_loss: 1.1871e-07
Epoch 125/512
448/448 - 0s - loss: 1.8072e-05 - val_loss: 1.1493e-07
Epoch 126/512
448/448 - 0s - loss: 1.7549e-05 - val_loss: 1.1176e-07
Epoch 127/512
448/448 - 0s - loss: 1.7425e-05 - val_loss: 1.0778e-07
Epoch 128/512
448/448 - 0s - loss: 1.6611e-05 - val_loss: 1.0673e-07
Epoch 129/512
448/448 - 0s - loss: 1.6778e-05 - val_loss: 1.0461e-07
Epoch 130/512
448/448 - 0s - loss: 1.6248e-05 - val_loss: 1.0074e-07
Epoch 131/512
448/448 - 0s - loss: 1.5726e-05 - val_loss: 9.8389e-08
Epoch 132/512
448/448 - 0s - loss: 1.5661e-05 - val_loss: 9.5702e-08
Epoch 133/512
448/448 - 0s - loss: 1.5049e-05 - val_loss: 9.4765e-08
Epoch 134/512
448/448 - 0s - loss: 1.5055e-05 - val_loss: 9.0751e-08
Epoch 135/512
448/448 - 0s - loss: 1.4366e-05 - val_loss: 8.8873e-08
Epoch 136/512
448/448 - 0s - loss: 1.4342e-05 - val_loss: 8.6353e-08
Epoch 137/512
448/448 - 0s - loss: 1.3886e-05 - val_loss: 8.2539e-08
Epoch 138/512
448/448 - 0s - loss: 1.3297e-05 - val_loss: 8.2889e-08
Epoch 139/512
448/448 - 0s - loss: 1.3594e-05 - val_loss: 8.0268e-08
Epoch 140/512
448/448 - 0s - loss: 1.3056e-05 - val_loss: 7.5030e-08
Epoch 141/512
448/448 - 0s - loss: 1.2299e-05 - val_loss: 7.4563e-08
Epoch 142/512
448/448 - 0s - loss: 1.2489e-05 - val_loss: 7.4707e-08
Epoch 143/512
448/448 - 0s - loss: 1.2312e-05 - val_loss: 7.0677e-08
Epoch 144/512
448/448 - 0s - loss: 1.1604e-05 - val_loss: 6.8662e-08
Epoch 145/512
448/448 - 0s - loss: 1.1477e-05 - val_loss: 6.7439e-08
Epoch 146/512
448/448 - 0s - loss: 1.1176e-05 - val_loss: 6.6868e-08
Epoch 147/512
448/448 - 0s - loss: 1.1151e-05 - val_loss: 6.3976e-08
Epoch 148/512
448/448 - 0s - loss: 1.0632e-05 - val_loss: 6.0890e-08
Epoch 149/512
448/448 - 0s - loss: 1.0234e-05 - val_loss: 6.0419e-08
Epoch 150/512
448/448 - 0s - loss: 1.0241e-05 - val_loss: 5.9775e-08
Epoch 151/512
448/448 - 0s - loss: 1.0074e-05 - val_loss: 5.6744e-08
Epoch 152/512
448/448 - 0s - loss: 9.4809e-06 - val_loss: 5.5211e-08
Epoch 153/512
448/448 - 0s - loss: 9.3262e-06 - val_loss: 5.5314e-08
Epoch 154/512
448/448 - 0s - loss: 9.3241e-06 - val_loss: 5.3751e-08
Epoch 155/512
448/448 - 0s - loss: 8.9752e-06 - val_loss: 5.0513e-08
Epoch 156/512
448/448 - 0s - loss: 8.4489e-06 - val_loss: 5.0253e-08
Epoch 157/512
448/448 - 0s - loss: 8.5038e-06 - val_loss: 5.0216e-08
Epoch 158/512
448/448 - 0s - loss: 8.4404e-06 - val_loss: 4.6602e-08
Epoch 159/512
448/448 - 0s - loss: 7.7037e-06 - val_loss: 4.6178e-08
Epoch 160/512
448/448 - 0s - loss: 7.8523e-06 - val_loss: 4.6049e-08
Epoch 161/512
448/448 - 0s - loss: 7.6959e-06 - val_loss: 4.3641e-08
Epoch 162/512
448/448 - 0s - loss: 7.2192e-06 - val_loss: 4.2554e-08
Epoch 163/512
448/448 - 0s - loss: 7.1136e-06 - val_loss: 4.2665e-08
Epoch 164/512
448/448 - 0s - loss: 7.1041e-06 - val_loss: 4.0781e-08
Epoch 165/512
448/448 - 0s - loss: 6.6850e-06 - val_loss: 3.8823e-08
Epoch 166/512
448/448 - 0s - loss: 6.4483e-06 - val_loss: 3.8888e-08
Epoch 167/512
448/448 - 0s - loss: 6.4407e-06 - val_loss: 3.8312e-08
Epoch 168/512
448/448 - 0s - loss: 6.2422e-06 - val_loss: 3.6510e-08
Epoch 169/512
448/448 - 0s - loss: 5.9093e-06 - val_loss: 3.5763e-08
Epoch 170/512
448/448 - 0s - loss: 5.8457e-06 - val_loss: 3.5265e-08
Epoch 171/512
448/448 - 0s - loss: 5.6832e-06 - val_loss: 3.3972e-08
Epoch 172/512
448/448 - 0s - loss: 5.4448e-06 - val_loss: 3.2926e-08
Epoch 173/512
448/448 - 0s - loss: 5.2801e-06 - val_loss: 3.2695e-08
Epoch 174/512
448/448 - 0s - loss: 5.2702e-06 - val_loss: 3.0916e-08
Epoch 175/512
448/448 - 0s - loss: 4.8654e-06 - val_loss: 3.0230e-08
Epoch 176/512
448/448 - 0s - loss: 4.8217e-06 - val_loss: 3.0477e-08
Epoch 177/512
448/448 - 0s - loss: 4.8119e-06 - val_loss: 2.9135e-08
Epoch 178/512
448/448 - 0s - loss: 4.4956e-06 - val_loss: 2.7830e-08
Epoch 179/512
448/448 - 0s - loss: 4.3599e-06 - val_loss: 2.7357e-08
Epoch 180/512
448/448 - 0s - loss: 4.2420e-06 - val_loss: 2.7366e-08
Epoch 181/512
448/448 - 0s - loss: 4.1932e-06 - val_loss: 2.6469e-08
Epoch 182/512
448/448 - 0s - loss: 3.9930e-06 - val_loss: 2.5274e-08
Epoch 183/512
448/448 - 0s - loss: 3.8133e-06 - val_loss: 2.4848e-08
Epoch 184/512
448/448 - 0s - loss: 3.7634e-06 - val_loss: 2.4212e-08
Epoch 185/512
448/448 - 0s - loss: 3.5765e-06 - val_loss: 2.3991e-08
Epoch 186/512
448/448 - 0s - loss: 3.5500e-06 - val_loss: 2.3199e-08
Epoch 187/512
448/448 - 0s - loss: 3.3642e-06 - val_loss: 2.2411e-08
Epoch 188/512
448/448 - 0s - loss: 3.2337e-06 - val_loss: 2.2205e-08
Epoch 189/512
448/448 - 0s - loss: 3.1961e-06 - val_loss: 2.1469e-08
Epoch 190/512
448/448 - 0s - loss: 3.0404e-06 - val_loss: 2.0563e-08
Epoch 191/512
448/448 - 0s - loss: 2.8972e-06 - val_loss: 2.0445e-08
Epoch 192/512
448/448 - 0s - loss: 2.8746e-06 - val_loss: 2.0157e-08
Epoch 193/512
448/448 - 0s - loss: 2.7752e-06 - val_loss: 1.9113e-08
Epoch 194/512
448/448 - 0s - loss: 2.5986e-06 - val_loss: 1.8587e-08
Epoch 195/512
448/448 - 0s - loss: 2.5390e-06 - val_loss: 1.8623e-08
Epoch 196/512
448/448 - 0s - loss: 2.4838e-06 - val_loss: 1.8398e-08
Epoch 197/512
448/448 - 0s - loss: 2.4101e-06 - val_loss: 1.7517e-08
Epoch 198/512
448/448 - 0s - loss: 2.2534e-06 - val_loss: 1.7133e-08
Epoch 199/512
448/448 - 0s - loss: 2.2282e-06 - val_loss: 1.6704e-08
Epoch 200/512
448/448 - 0s - loss: 2.1205e-06 - val_loss: 1.6319e-08
Epoch 201/512
448/448 - 0s - loss: 2.0544e-06 - val_loss: 1.6063e-08
Epoch 202/512
448/448 - 0s - loss: 1.9841e-06 - val_loss: 1.5688e-08
Epoch 203/512
448/448 - 0s - loss: 1.9252e-06 - val_loss: 1.5088e-08
Epoch 204/512
448/448 - 0s - loss: 1.8273e-06 - val_loss: 1.4587e-08
Epoch 205/512
448/448 - 0s - loss: 1.7552e-06 - val_loss: 1.4437e-08
Epoch 206/512
448/448 - 0s - loss: 1.7288e-06 - val_loss: 1.3944e-08
Epoch 207/512
448/448 - 0s - loss: 1.6124e-06 - val_loss: 1.3873e-08
Epoch 208/512
448/448 - 0s - loss: 1.6101e-06 - val_loss: 1.3420e-08
Epoch 209/512
448/448 - 0s - loss: 1.5104e-06 - val_loss: 1.2994e-08
Epoch 210/512
448/448 - 0s - loss: 1.4459e-06 - val_loss: 1.3044e-08
Epoch 211/512
448/448 - 0s - loss: 1.4500e-06 - val_loss: 1.2366e-08
Epoch 212/512
448/448 - 0s - loss: 1.3220e-06 - val_loss: 1.1965e-08
Epoch 213/512
448/448 - 0s - loss: 1.2794e-06 - val_loss: 1.2119e-08
Epoch 214/512
448/448 - 0s - loss: 1.2841e-06 - val_loss: 1.1955e-08
Epoch 215/512
448/448 - 0s - loss: 1.2360e-06 - val_loss: 1.1005e-08
Epoch 216/512
448/448 - 0s - loss: 1.0879e-06 - val_loss: 1.1059e-08
Epoch 217/512
448/448 - 0s - loss: 1.1334e-06 - val_loss: 1.1248e-08
Epoch 218/512
448/448 - 0s - loss: 1.1087e-06 - val_loss: 1.0524e-08
Epoch 219/512
448/448 - 0s - loss: 9.8831e-07 - val_loss: 1.0188e-08
Epoch 220/512
448/448 - 0s - loss: 9.7258e-07 - val_loss: 1.0360e-08
Epoch 221/512
448/448 - 0s - loss: 9.8589e-07 - val_loss: 9.9958e-09
Epoch 222/512
448/448 - 0s - loss: 9.0627e-07 - val_loss: 9.3983e-09
Epoch 223/512
448/448 - 0s - loss: 8.4071e-07 - val_loss: 9.3734e-09
Epoch 224/512
448/448 - 0s - loss: 8.4981e-07 - val_loss: 9.3804e-09
Epoch 225/512
448/448 - 0s - loss: 8.1894e-07 - val_loss: 9.0288e-09
Epoch 226/512
448/448 - 0s - loss: 7.6432e-07 - val_loss: 8.6562e-09
Epoch 227/512
448/448 - 0s - loss: 7.2290e-07 - val_loss: 8.6306e-09
Epoch 228/512
448/448 - 0s - loss: 7.2072e-07 - val_loss: 8.5466e-09
Epoch 229/512
448/448 - 0s - loss: 6.8405e-07 - val_loss: 8.3480e-09
Epoch 230/512
448/448 - 0s - loss: 6.6102e-07 - val_loss: 7.9561e-09
Epoch 231/512
448/448 - 0s - loss: 6.0653e-07 - val_loss: 7.8761e-09
Epoch 232/512
448/448 - 0s - loss: 6.0839e-07 - val_loss: 7.7866e-09
Epoch 233/512
448/448 - 0s - loss: 5.8441e-07 - val_loss: 7.4801e-09
Epoch 234/512
448/448 - 0s - loss: 5.4030e-07 - val_loss: 7.3303e-09
Epoch 235/512
448/448 - 0s - loss: 5.2936e-07 - val_loss: 7.2327e-09
Epoch 236/512
448/448 - 0s - loss: 5.1171e-07 - val_loss: 7.0215e-09
Epoch 237/512
448/448 - 0s - loss: 4.7992e-07 - val_loss: 6.8249e-09
Epoch 238/512
448/448 - 0s - loss: 4.5999e-07 - val_loss: 6.7221e-09
Epoch 239/512
448/448 - 0s - loss: 4.4468e-07 - val_loss: 6.6669e-09
Epoch 240/512
448/448 - 0s - loss: 4.3423e-07 - val_loss: 6.4455e-09
Epoch 241/512
448/448 - 0s - loss: 4.0017e-07 - val_loss: 6.3284e-09
Epoch 242/512
448/448 - 0s - loss: 3.9428e-07 - val_loss: 6.1682e-09
Epoch 243/512
448/448 - 0s - loss: 3.6950e-07 - val_loss: 6.0625e-09
Epoch 244/512
448/448 - 0s - loss: 3.5939e-07 - val_loss: 5.9316e-09
Epoch 245/512
448/448 - 0s - loss: 3.4150e-07 - val_loss: 5.7723e-09
Epoch 246/512
448/448 - 0s - loss: 3.2278e-07 - val_loss: 5.7236e-09
Epoch 247/512
448/448 - 0s - loss: 3.1852e-07 - val_loss: 5.5772e-09
Epoch 248/512
448/448 - 0s - loss: 2.9934e-07 - val_loss: 5.4116e-09
Epoch 249/512
448/448 - 0s - loss: 2.8240e-07 - val_loss: 5.3177e-09
Epoch 250/512
448/448 - 0s - loss: 2.7385e-07 - val_loss: 5.2787e-09
Epoch 251/512
448/448 - 0s - loss: 2.6656e-07 - val_loss: 5.1608e-09
Epoch 252/512
448/448 - 0s - loss: 2.5206e-07 - val_loss: 4.9819e-09
Epoch 253/512
448/448 - 0s - loss: 2.3262e-07 - val_loss: 4.9524e-09
Epoch 254/512
448/448 - 0s - loss: 2.3419e-07 - val_loss: 4.8704e-09
Epoch 255/512
448/448 - 0s - loss: 2.2178e-07 - val_loss: 4.6895e-09
Epoch 256/512
448/448 - 0s - loss: 2.0395e-07 - val_loss: 4.6179e-09
Epoch 257/512
448/448 - 0s - loss: 2.0196e-07 - val_loss: 4.5406e-09
Epoch 258/512
448/448 - 0s - loss: 1.9233e-07 - val_loss: 4.4327e-09
Epoch 259/512
448/448 - 0s - loss: 1.8113e-07 - val_loss: 4.3662e-09
Epoch 260/512
448/448 - 0s - loss: 1.7508e-07 - val_loss: 4.3041e-09
Epoch 261/512
448/448 - 0s - loss: 1.6977e-07 - val_loss: 4.1777e-09
Epoch 262/512
448/448 - 0s - loss: 1.5624e-07 - val_loss: 4.1171e-09
Epoch 263/512
448/448 - 0s - loss: 1.5355e-07 - val_loss: 4.0587e-09
Epoch 264/512
448/448 - 0s - loss: 1.4783e-07 - val_loss: 3.9575e-09
Epoch 265/512
448/448 - 0s - loss: 1.3767e-07 - val_loss: 3.8713e-09
Epoch 266/512
448/448 - 0s - loss: 1.3206e-07 - val_loss: 3.8303e-09
Epoch 267/512
448/448 - 0s - loss: 1.2879e-07 - val_loss: 3.7627e-09
Epoch 268/512
448/448 - 0s - loss: 1.2237e-07 - val_loss: 3.6597e-09
Epoch 269/512
448/448 - 0s - loss: 1.1492e-07 - val_loss: 3.5726e-09
Epoch 270/512
448/448 - 0s - loss: 1.0792e-07 - val_loss: 3.5807e-09
Epoch 271/512
448/448 - 0s - loss: 1.1018e-07 - val_loss: 3.5017e-09
Epoch 272/512
448/448 - 0s - loss: 1.0138e-07 - val_loss: 3.3814e-09
Epoch 273/512
448/448 - 0s - loss: 9.2679e-08 - val_loss: 3.3641e-09
Epoch 274/512
448/448 - 0s - loss: 9.4167e-08 - val_loss: 3.3477e-09
Epoch 275/512
448/448 - 0s - loss: 9.1251e-08 - val_loss: 3.2367e-09
Epoch 276/512
448/448 - 0s - loss: 8.2575e-08 - val_loss: 3.1439e-09
Epoch 277/512
448/448 - 0s - loss: 7.7587e-08 - val_loss: 3.1396e-09
Epoch 278/512
448/448 - 0s - loss: 7.8834e-08 - val_loss: 3.1204e-09
Epoch 279/512
448/448 - 0s - loss: 7.6141e-08 - val_loss: 3.0138e-09
Epoch 280/512
448/448 - 0s - loss: 6.7789e-08 - val_loss: 2.9556e-09
Epoch 281/512
448/448 - 0s - loss: 6.6323e-08 - val_loss: 2.9389e-09
Epoch 282/512
448/448 - 0s - loss: 6.5804e-08 - val_loss: 2.8816e-09
Epoch 283/512
448/448 - 0s - loss: 6.1010e-08 - val_loss: 2.8236e-09
Epoch 284/512
448/448 - 0s - loss: 5.8041e-08 - val_loss: 2.7734e-09
Epoch 285/512
448/448 - 0s - loss: 5.5513e-08 - val_loss: 2.7506e-09
Epoch 286/512
448/448 - 0s - loss: 5.4284e-08 - val_loss: 2.7060e-09
Epoch 287/512
448/448 - 0s - loss: 5.1082e-08 - val_loss: 2.6441e-09
Epoch 288/512
448/448 - 0s - loss: 4.8025e-08 - val_loss: 2.6101e-09
Epoch 289/512
448/448 - 0s - loss: 4.7015e-08 - val_loss: 2.5750e-09
Epoch 290/512
448/448 - 0s - loss: 4.4612e-08 - val_loss: 2.5432e-09
Epoch 291/512
448/448 - 0s - loss: 4.3206e-08 - val_loss: 2.4898e-09
Epoch 292/512
448/448 - 0s - loss: 3.9931e-08 - val_loss: 2.4599e-09
Epoch 293/512
448/448 - 0s - loss: 3.9437e-08 - val_loss: 2.4228e-09
Epoch 294/512
448/448 - 0s - loss: 3.7326e-08 - val_loss: 2.3768e-09
Epoch 295/512
448/448 - 0s - loss: 3.4756e-08 - val_loss: 2.3596e-09
Epoch 296/512
448/448 - 0s - loss: 3.4858e-08 - val_loss: 2.3323e-09
Epoch 297/512
448/448 - 0s - loss: 3.3098e-08 - val_loss: 2.2814e-09
Epoch 298/512
448/448 - 0s - loss: 3.0716e-08 - val_loss: 2.2467e-09
Epoch 299/512
448/448 - 0s - loss: 2.9465e-08 - val_loss: 2.2337e-09
Epoch 300/512
448/448 - 0s - loss: 2.9488e-08 - val_loss: 2.2002e-09
Epoch 301/512
448/448 - 0s - loss: 2.7645e-08 - val_loss: 2.1528e-09
Epoch 302/512
448/448 - 0s - loss: 2.5516e-08 - val_loss: 2.1258e-09
Epoch 303/512
448/448 - 0s - loss: 2.4921e-08 - val_loss: 2.1149e-09
Epoch 304/512
448/448 - 0s - loss: 2.4925e-08 - val_loss: 2.0806e-09
Epoch 305/512
448/448 - 0s - loss: 2.2999e-08 - val_loss: 2.0430e-09
Epoch 306/512
448/448 - 0s - loss: 2.1515e-08 - val_loss: 2.0246e-09
Epoch 307/512
448/448 - 0s - loss: 2.1512e-08 - val_loss: 2.0043e-09
Epoch 308/512
448/448 - 0s - loss: 2.0536e-08 - val_loss: 1.9792e-09
Epoch 309/512
448/448 - 0s - loss: 1.9646e-08 - val_loss: 1.9429e-09
Epoch 310/512
448/448 - 0s - loss: 1.8215e-08 - val_loss: 1.9247e-09
Epoch 311/512
448/448 - 0s - loss: 1.8080e-08 - val_loss: 1.9104e-09
Epoch 312/512
448/448 - 0s - loss: 1.7501e-08 - val_loss: 1.8852e-09
Epoch 313/512
448/448 - 0s - loss: 1.6671e-08 - val_loss: 1.8598e-09
Epoch 314/512
448/448 - 0s - loss: 1.5875e-08 - val_loss: 1.8318e-09
Epoch 315/512
448/448 - 0s - loss: 1.5174e-08 - val_loss: 1.8119e-09
Epoch 316/512
448/448 - 0s - loss: 1.4580e-08 - val_loss: 1.7947e-09
Epoch 317/512
448/448 - 0s - loss: 1.4350e-08 - val_loss: 1.7738e-09
Epoch 318/512
448/448 - 0s - loss: 1.3672e-08 - val_loss: 1.7483e-09
Epoch 319/512
448/448 - 0s - loss: 1.2937e-08 - val_loss: 1.7210e-09
Epoch 320/512
448/448 - 0s - loss: 1.2173e-08 - val_loss: 1.7053e-09
Epoch 321/512
448/448 - 0s - loss: 1.2014e-08 - val_loss: 1.6958e-09
Epoch 322/512
448/448 - 0s - loss: 1.1941e-08 - val_loss: 1.6773e-09
Epoch 323/512
448/448 - 0s - loss: 1.1365e-08 - val_loss: 1.6541e-09
Epoch 324/512
448/448 - 0s - loss: 1.0606e-08 - val_loss: 1.6291e-09
Epoch 325/512
448/448 - 0s - loss: 1.0045e-08 - val_loss: 1.6165e-09
Epoch 326/512
448/448 - 0s - loss: 1.0012e-08 - val_loss: 1.6108e-09
Epoch 327/512
448/448 - 0s - loss: 1.0111e-08 - val_loss: 1.5853e-09
Epoch 328/512
448/448 - 0s - loss: 9.1755e-09 - val_loss: 1.5630e-09
Epoch 329/512
448/448 - 0s - loss: 8.7846e-09 - val_loss: 1.5510e-09
Epoch 330/512
448/448 - 0s - loss: 8.6957e-09 - val_loss: 1.5367e-09
Epoch 331/512
448/448 - 0s - loss: 8.4277e-09 - val_loss: 1.5226e-09
Epoch 332/512
448/448 - 0s - loss: 8.2028e-09 - val_loss: 1.5070e-09
Epoch 333/512
448/448 - 0s - loss: 7.8434e-09 - val_loss: 1.4853e-09
Epoch 334/512
448/448 - 0s - loss: 7.3760e-09 - val_loss: 1.4656e-09
Epoch 335/512
448/448 - 0s - loss: 7.0711e-09 - val_loss: 1.4587e-09
Epoch 336/512
448/448 - 0s - loss: 7.1450e-09 - val_loss: 1.4508e-09
Epoch 337/512
448/448 - 0s - loss: 7.0884e-09 - val_loss: 1.4313e-09
Epoch 338/512
448/448 - 0s - loss: 6.5519e-09 - val_loss: 1.4151e-09
Epoch 339/512
448/448 - 0s - loss: 6.3561e-09 - val_loss: 1.3986e-09
Epoch 340/512
448/448 - 0s - loss: 6.0889e-09 - val_loss: 1.3885e-09
Epoch 341/512
448/448 - 0s - loss: 5.9529e-09 - val_loss: 1.3766e-09
Epoch 342/512
448/448 - 0s - loss: 5.8930e-09 - val_loss: 1.3626e-09
Epoch 343/512
448/448 - 0s - loss: 5.5641e-09 - val_loss: 1.3497e-09
Epoch 344/512
448/448 - 0s - loss: 5.4806e-09 - val_loss: 1.3398e-09
Epoch 345/512
448/448 - 0s - loss: 5.3821e-09 - val_loss: 1.3235e-09
Epoch 346/512
448/448 - 0s - loss: 5.0603e-09 - val_loss: 1.3089e-09
Epoch 347/512
448/448 - 0s - loss: 4.8813e-09 - val_loss: 1.2995e-09
Epoch 348/512
448/448 - 0s - loss: 4.8127e-09 - val_loss: 1.2898e-09
Epoch 349/512
448/448 - 0s - loss: 4.7302e-09 - val_loss: 1.2794e-09
Epoch 350/512
448/448 - 0s - loss: 4.6613e-09 - val_loss: 1.2666e-09
Epoch 351/512
448/448 - 0s - loss: 4.4011e-09 - val_loss: 1.2548e-09
Epoch 352/512
448/448 - 0s - loss: 4.3053e-09 - val_loss: 1.2443e-09
Epoch 353/512
448/448 - 0s - loss: 4.2429e-09 - val_loss: 1.2338e-09
Epoch 354/512
448/448 - 0s - loss: 4.0889e-09 - val_loss: 1.2240e-09
Epoch 355/512
448/448 - 0s - loss: 3.9989e-09 - val_loss: 1.2108e-09
Epoch 356/512
448/448 - 0s - loss: 3.8066e-09 - val_loss: 1.1991e-09
Epoch 357/512
448/448 - 0s - loss: 3.7403e-09 - val_loss: 1.1914e-09
Epoch 358/512
448/448 - 0s - loss: 3.7068e-09 - val_loss: 1.1828e-09
Epoch 359/512
448/448 - 0s - loss: 3.6733e-09 - val_loss: 1.1716e-09
Epoch 360/512
448/448 - 0s - loss: 3.4959e-09 - val_loss: 1.1605e-09
Epoch 361/512
448/448 - 0s - loss: 3.3455e-09 - val_loss: 1.1512e-09
Epoch 362/512
448/448 - 0s - loss: 3.2973e-09 - val_loss: 1.1425e-09
Epoch 363/512
448/448 - 0s - loss: 3.2632e-09 - val_loss: 1.1352e-09
Epoch 364/512
448/448 - 0s - loss: 3.1840e-09 - val_loss: 1.1251e-09
Epoch 365/512
448/448 - 0s - loss: 3.0908e-09 - val_loss: 1.1138e-09
Epoch 366/512
448/448 - 0s - loss: 2.9688e-09 - val_loss: 1.1062e-09
Epoch 367/512
448/448 - 0s - loss: 2.9355e-09 - val_loss: 1.0977e-09
Epoch 368/512
448/448 - 0s - loss: 2.9045e-09 - val_loss: 1.0869e-09
Epoch 369/512
448/448 - 0s - loss: 2.7597e-09 - val_loss: 1.0777e-09
Epoch 370/512
448/448 - 0s - loss: 2.7263e-09 - val_loss: 1.0737e-09
Epoch 371/512
448/448 - 0s - loss: 2.7446e-09 - val_loss: 1.0623e-09
Epoch 372/512
448/448 - 0s - loss: 2.5953e-09 - val_loss: 1.0552e-09
Epoch 373/512
448/448 - 0s - loss: 2.5418e-09 - val_loss: 1.0479e-09
Epoch 374/512
448/448 - 0s - loss: 2.5062e-09 - val_loss: 1.0394e-09
Epoch 375/512
448/448 - 0s - loss: 2.4558e-09 - val_loss: 1.0325e-09
Epoch 376/512
448/448 - 0s - loss: 2.4411e-09 - val_loss: 1.0234e-09
Epoch 377/512
448/448 - 0s - loss: 2.3327e-09 - val_loss: 1.0145e-09
Epoch 378/512
448/448 - 0s - loss: 2.2705e-09 - val_loss: 1.0089e-09
Epoch 379/512
448/448 - 0s - loss: 2.2701e-09 - val_loss: 1.0006e-09
Epoch 380/512
448/448 - 0s - loss: 2.1717e-09 - val_loss: 9.9214e-10
Epoch 381/512
448/448 - 0s - loss: 2.1292e-09 - val_loss: 9.8725e-10
Epoch 382/512
448/448 - 0s - loss: 2.1482e-09 - val_loss: 9.8228e-10
Epoch 383/512
448/448 - 0s - loss: 2.1512e-09 - val_loss: 9.7236e-10
Epoch 384/512
448/448 - 0s - loss: 2.0425e-09 - val_loss: 9.6380e-10
Epoch 385/512
448/448 - 0s - loss: 1.9283e-09 - val_loss: 9.5470e-10
Epoch 386/512
448/448 - 0s - loss: 1.8780e-09 - val_loss: 9.5088e-10
Epoch 387/512
448/448 - 0s - loss: 1.9466e-09 - val_loss: 9.4863e-10
Epoch 388/512
448/448 - 0s - loss: 1.9943e-09 - val_loss: 9.4099e-10
Epoch 389/512
448/448 - 0s - loss: 1.8846e-09 - val_loss: 9.3188e-10
Epoch 390/512
448/448 - 0s - loss: 1.7809e-09 - val_loss: 9.2536e-10
Epoch 391/512
448/448 - 0s - loss: 1.7622e-09 - val_loss: 9.2018e-10
Epoch 392/512
448/448 - 0s - loss: 1.7728e-09 - val_loss: 9.1414e-10
Epoch 393/512
448/448 - 0s - loss: 1.7188e-09 - val_loss: 9.0738e-10
Epoch 394/512
448/448 - 0s - loss: 1.6900e-09 - val_loss: 8.9970e-10
Epoch 395/512
448/448 - 0s - loss: 1.6423e-09 - val_loss: 8.9482e-10
Epoch 396/512
448/448 - 0s - loss: 1.6390e-09 - val_loss: 8.9002e-10
Epoch 397/512
448/448 - 0s - loss: 1.6195e-09 - val_loss: 8.8416e-10
Epoch 398/512
448/448 - 0s - loss: 1.5934e-09 - val_loss: 8.7813e-10
Epoch 399/512
448/448 - 0s - loss: 1.5559e-09 - val_loss: 8.7209e-10
Epoch 400/512
448/448 - 0s - loss: 1.5174e-09 - val_loss: 8.6829e-10
Epoch 401/512
448/448 - 0s - loss: 1.5306e-09 - val_loss: 8.6172e-10
Epoch 402/512
448/448 - 0s - loss: 1.4945e-09 - val_loss: 8.5614e-10
Epoch 403/512
448/448 - 0s - loss: 1.4599e-09 - val_loss: 8.4948e-10
Epoch 404/512
448/448 - 0s - loss: 1.4057e-09 - val_loss: 8.4470e-10
Epoch 405/512
448/448 - 0s - loss: 1.4015e-09 - val_loss: 8.4094e-10
Epoch 406/512
448/448 - 0s - loss: 1.3989e-09 - val_loss: 8.3519e-10
Epoch 407/512
448/448 - 0s - loss: 1.3844e-09 - val_loss: 8.3076e-10
Epoch 408/512
448/448 - 0s - loss: 1.3607e-09 - val_loss: 8.2471e-10
Epoch 409/512
448/448 - 0s - loss: 1.3283e-09 - val_loss: 8.2008e-10
Epoch 410/512
448/448 - 0s - loss: 1.3216e-09 - val_loss: 8.1463e-10
Epoch 411/512
448/448 - 0s - loss: 1.2873e-09 - val_loss: 8.0937e-10
Epoch 412/512
448/448 - 0s - loss: 1.2616e-09 - val_loss: 8.0332e-10
Epoch 413/512
448/448 - 0s - loss: 1.2199e-09 - val_loss: 7.9792e-10
Epoch 414/512
448/448 - 0s - loss: 1.2012e-09 - val_loss: 7.9483e-10
Epoch 415/512
448/448 - 0s - loss: 1.2235e-09 - val_loss: 7.9182e-10
Epoch 416/512
448/448 - 0s - loss: 1.2227e-09 - val_loss: 7.8775e-10
Epoch 417/512
448/448 - 0s - loss: 1.2064e-09 - val_loss: 7.8251e-10
Epoch 418/512
448/448 - 0s - loss: 1.1890e-09 - val_loss: 7.7697e-10
Epoch 419/512
448/448 - 0s - loss: 1.1612e-09 - val_loss: 7.7232e-10
Epoch 420/512
448/448 - 0s - loss: 1.1534e-09 - val_loss: 7.6687e-10
Epoch 421/512
448/448 - 0s - loss: 1.0928e-09 - val_loss: 7.6317e-10
Epoch 422/512
448/448 - 0s - loss: 1.0979e-09 - val_loss: 7.5952e-10
Epoch 423/512
448/448 - 0s - loss: 1.1050e-09 - val_loss: 7.5541e-10
Epoch 424/512
448/448 - 0s - loss: 1.0843e-09 - val_loss: 7.5116e-10
Epoch 425/512
448/448 - 0s - loss: 1.0653e-09 - val_loss: 7.4622e-10
Epoch 426/512
448/448 - 0s - loss: 1.0490e-09 - val_loss: 7.4257e-10
Epoch 427/512
448/448 - 0s - loss: 1.0457e-09 - val_loss: 7.3947e-10
Epoch 428/512
448/448 - 0s - loss: 1.0416e-09 - val_loss: 7.3394e-10
Epoch 429/512
448/448 - 0s - loss: 9.9785e-10 - val_loss: 7.2986e-10
Epoch 430/512
448/448 - 0s - loss: 9.8723e-10 - val_loss: 7.2564e-10
Epoch 431/512
448/448 - 0s - loss: 9.6366e-10 - val_loss: 7.2325e-10
Epoch 432/512
448/448 - 0s - loss: 9.8883e-10 - val_loss: 7.2084e-10
Epoch 433/512
448/448 - 0s - loss: 1.0094e-09 - val_loss: 7.1598e-10
Epoch 434/512
448/448 - 0s - loss: 9.7203e-10 - val_loss: 7.1217e-10
Epoch 435/512
448/448 - 0s - loss: 9.5150e-10 - val_loss: 7.0648e-10
Epoch 436/512
448/448 - 0s - loss: 9.1089e-10 - val_loss: 7.0203e-10
Epoch 437/512
448/448 - 0s - loss: 8.8835e-10 - val_loss: 6.9946e-10
Epoch 438/512
448/448 - 0s - loss: 9.0925e-10 - val_loss: 6.9702e-10
Epoch 439/512
448/448 - 0s - loss: 9.1815e-10 - val_loss: 6.9445e-10
Epoch 440/512
448/448 - 0s - loss: 9.3004e-10 - val_loss: 6.9105e-10
Epoch 441/512
448/448 - 0s - loss: 9.1104e-10 - val_loss: 6.8657e-10
Epoch 442/512
448/448 - 0s - loss: 8.7407e-10 - val_loss: 6.8144e-10
Epoch 443/512
448/448 - 0s - loss: 8.4019e-10 - val_loss: 6.7752e-10
Epoch 444/512
448/448 - 0s - loss: 8.2115e-10 - val_loss: 6.7518e-10
Epoch 445/512
448/448 - 0s - loss: 8.3279e-10 - val_loss: 6.7247e-10
Epoch 446/512
448/448 - 0s - loss: 8.3925e-10 - val_loss: 6.6888e-10
Epoch 447/512
448/448 - 0s - loss: 8.2987e-10 - val_loss: 6.6595e-10
Epoch 448/512
448/448 - 0s - loss: 8.2996e-10 - val_loss: 6.6312e-10
Epoch 449/512
448/448 - 0s - loss: 8.1741e-10 - val_loss: 6.5981e-10
Epoch 450/512
448/448 - 0s - loss: 8.0767e-10 - val_loss: 6.5600e-10
Epoch 451/512
448/448 - 0s - loss: 7.8223e-10 - val_loss: 6.5273e-10
Epoch 452/512
448/448 - 0s - loss: 7.8416e-10 - val_loss: 6.4941e-10
Epoch 453/512
448/448 - 0s - loss: 7.7848e-10 - val_loss: 6.4655e-10
Epoch 454/512
448/448 - 0s - loss: 7.6389e-10 - val_loss: 6.4381e-10
Epoch 455/512
448/448 - 0s - loss: 7.6608e-10 - val_loss: 6.4096e-10
Epoch 456/512
448/448 - 0s - loss: 7.6034e-10 - val_loss: 6.3762e-10
Epoch 457/512
448/448 - 0s - loss: 7.3928e-10 - val_loss: 6.3303e-10
Epoch 458/512
448/448 - 0s - loss: 7.2001e-10 - val_loss: 6.3072e-10
Epoch 459/512
448/448 - 0s - loss: 7.1194e-10 - val_loss: 6.2816e-10
Epoch 460/512
448/448 - 0s - loss: 7.3116e-10 - val_loss: 6.2597e-10
Epoch 461/512
448/448 - 0s - loss: 7.3559e-10 - val_loss: 6.2349e-10
Epoch 462/512
448/448 - 0s - loss: 7.2119e-10 - val_loss: 6.2053e-10
Epoch 463/512
448/448 - 0s - loss: 7.1975e-10 - val_loss: 6.1704e-10
Epoch 464/512
448/448 - 0s - loss: 6.9373e-10 - val_loss: 6.1428e-10
Epoch 465/512
448/448 - 0s - loss: 7.0356e-10 - val_loss: 6.1178e-10
Epoch 466/512
448/448 - 0s - loss: 6.9693e-10 - val_loss: 6.0908e-10
Epoch 467/512
448/448 - 0s - loss: 6.8331e-10 - val_loss: 6.0521e-10
Epoch 468/512
448/448 - 0s - loss: 6.6038e-10 - val_loss: 6.0247e-10
Epoch 469/512
448/448 - 0s - loss: 6.5133e-10 - val_loss: 5.9886e-10
Epoch 470/512
448/448 - 0s - loss: 6.4880e-10 - val_loss: 5.9853e-10
Epoch 471/512
448/448 - 0s - loss: 6.6602e-10 - val_loss: 5.9581e-10
Epoch 472/512
448/448 - 0s - loss: 6.6122e-10 - val_loss: 5.9338e-10
Epoch 473/512
448/448 - 0s - loss: 6.4667e-10 - val_loss: 5.9015e-10
Epoch 474/512
448/448 - 0s - loss: 6.4954e-10 - val_loss: 5.8688e-10
Epoch 475/512
448/448 - 0s - loss: 6.1487e-10 - val_loss: 5.8386e-10
Epoch 476/512
448/448 - 0s - loss: 6.0788e-10 - val_loss: 5.8206e-10
Epoch 477/512
448/448 - 0s - loss: 6.1799e-10 - val_loss: 5.7942e-10
Epoch 478/512
448/448 - 0s - loss: 6.1892e-10 - val_loss: 5.7826e-10
Epoch 479/512
448/448 - 0s - loss: 6.3122e-10 - val_loss: 5.7567e-10
Epoch 480/512
448/448 - 0s - loss: 6.3784e-10 - val_loss: 5.7346e-10
Epoch 481/512
448/448 - 0s - loss: 6.1164e-10 - val_loss: 5.6995e-10
Epoch 482/512
448/448 - 0s - loss: 5.9190e-10 - val_loss: 5.6680e-10
Epoch 483/512
448/448 - 0s - loss: 5.7906e-10 - val_loss: 5.6455e-10
Epoch 484/512
448/448 - 0s - loss: 5.8012e-10 - val_loss: 5.6249e-10
Epoch 485/512
448/448 - 0s - loss: 5.7700e-10 - val_loss: 5.6094e-10
Epoch 486/512
448/448 - 0s - loss: 5.8033e-10 - val_loss: 5.5833e-10
Epoch 487/512
448/448 - 0s - loss: 5.8152e-10 - val_loss: 5.5686e-10
Epoch 488/512
448/448 - 0s - loss: 5.8113e-10 - val_loss: 5.5383e-10
Epoch 489/512
448/448 - 0s - loss: 5.6371e-10 - val_loss: 5.5154e-10
Epoch 490/512
448/448 - 0s - loss: 5.6142e-10 - val_loss: 5.4913e-10
Epoch 491/512
448/448 - 0s - loss: 5.5116e-10 - val_loss: 5.4689e-10
Epoch 492/512
448/448 - 0s - loss: 5.5039e-10 - val_loss: 5.4492e-10
Epoch 493/512
448/448 - 0s - loss: 5.5086e-10 - val_loss: 5.4283e-10
Epoch 494/512
448/448 - 0s - loss: 5.4991e-10 - val_loss: 5.4089e-10
Epoch 495/512
448/448 - 0s - loss: 5.4191e-10 - val_loss: 5.3784e-10
Epoch 496/512
448/448 - 0s - loss: 5.3015e-10 - val_loss: 5.3602e-10
Epoch 497/512
448/448 - 0s - loss: 5.2834e-10 - val_loss: 5.3397e-10
Epoch 498/512
448/448 - 0s - loss: 5.2703e-10 - val_loss: 5.3261e-10
Epoch 499/512
448/448 - 0s - loss: 5.2930e-10 - val_loss: 5.3035e-10
Epoch 500/512
448/448 - 0s - loss: 5.2661e-10 - val_loss: 5.2791e-10
Epoch 501/512
448/448 - 0s - loss: 5.1874e-10 - val_loss: 5.2523e-10
Epoch 502/512
448/448 - 0s - loss: 5.0391e-10 - val_loss: 5.2295e-10
Epoch 503/512
448/448 - 0s - loss: 4.9959e-10 - val_loss: 5.2188e-10
Epoch 504/512
448/448 - 0s - loss: 5.0436e-10 - val_loss: 5.2013e-10
Epoch 505/512
448/448 - 0s - loss: 5.0771e-10 - val_loss: 5.1865e-10
Epoch 506/512
448/448 - 0s - loss: 5.1064e-10 - val_loss: 5.1624e-10
Epoch 507/512
448/448 - 0s - loss: 5.0398e-10 - val_loss: 5.1394e-10
Epoch 508/512
448/448 - 0s - loss: 5.0032e-10 - val_loss: 5.1164e-10
Epoch 509/512
448/448 - 0s - loss: 4.8520e-10 - val_loss: 5.0975e-10
Epoch 510/512
448/448 - 0s - loss: 4.7939e-10 - val_loss: 5.0753e-10
Epoch 511/512
448/448 - 0s - loss: 4.7496e-10 - val_loss: 5.0518e-10
Epoch 512/512
448/448 - 0s - loss: 4.6225e-10 - val_loss: 5.0350e-10
Train on 448 samples, validate on 448 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3059e-08 - val_loss: 2.7167e-08
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6192e-08 - val_loss: 1.7095e-09
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0288e-09 - val_loss: 3.6458e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3024e-10 - val_loss: 3.5022e-10
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9241e-10 - val_loss: 1.1336e-09
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3811e-09 - val_loss: 6.8026e-09
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3197e-09 - val_loss: 7.8544e-09
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.0975e-09 - val_loss: 2.6496e-09
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1973e-09 - val_loss: 1.5707e-09
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6876e-09 - val_loss: 2.1176e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7588e-09 - val_loss: 4.0745e-09
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.8213e-09 - val_loss: 4.8460e-09
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5870e-09 - val_loss: 3.2752e-09
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0163e-09 - val_loss: 2.3422e-09
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3765e-09 - val_loss: 2.3842e-09
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6576e-09 - val_loss: 3.0164e-09
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3257e-09 - val_loss: 3.3501e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.4035e-09 - val_loss: 2.9433e-09
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8902e-09 - val_loss: 2.4812e-09
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5148e-09 - val_loss: 2.3762e-09
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4937e-09 - val_loss: 2.5210e-09
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6678e-09 - val_loss: 2.6094e-09
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6885e-09 - val_loss: 2.4863e-09
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5294e-09 - val_loss: 2.3095e-09
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3574e-09 - val_loss: 2.2050e-09
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2750e-09 - val_loss: 2.2042e-09
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2871e-09 - val_loss: 2.2087e-09
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2763e-09 - val_loss: 2.1722e-09
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2158e-09 - val_loss: 2.0686e-09
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1102e-09 - val_loss: 1.9926e-09
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0462e-09 - val_loss: 1.9390e-09
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0006e-09 - val_loss: 1.8991e-09
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9614e-09 - val_loss: 1.8746e-09
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9319e-09 - val_loss: 1.8497e-09
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8985e-09 - val_loss: 1.7959e-09
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8332e-09 - val_loss: 1.7251e-09
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7698e-09 - val_loss: 1.7005e-09
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7375e-09 - val_loss: 1.6501e-09
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7001e-09 - val_loss: 1.6231e-09
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6686e-09 - val_loss: 1.5923e-09
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6344e-09 - val_loss: 1.5539e-09
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5925e-09 - val_loss: 1.5098e-09
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5477e-09 - val_loss: 1.4818e-09
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5187e-09 - val_loss: 1.4577e-09
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4948e-09 - val_loss: 1.4343e-09
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4699e-09 - val_loss: 1.3984e-09
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4277e-09 - val_loss: 1.3517e-09
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3821e-09 - val_loss: 1.3266e-09
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3670e-09 - val_loss: 1.3117e-09
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3450e-09 - val_loss: 1.2790e-09
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3063e-09 - val_loss: 1.2450e-09
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2828e-09 - val_loss: 1.2406e-09
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2757e-09 - val_loss: 1.2150e-09
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2434e-09 - val_loss: 1.1834e-09
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2102e-09 - val_loss: 1.1457e-09
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1675e-09 - val_loss: 1.1132e-09
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1405e-09 - val_loss: 1.1028e-09
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1400e-09 - val_loss: 1.1151e-09
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1448e-09 - val_loss: 1.0931e-09
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1167e-09 - val_loss: 1.0511e-09
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0667e-09 - val_loss: 1.0101e-09
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0342e-09 - val_loss: 1.0091e-09
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0364e-09 - val_loss: 1.0050e-09
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0277e-09 - val_loss: 9.8498e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0030e-09 - val_loss: 9.5741e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.7459e-10 - val_loss: 9.4095e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6316e-10 - val_loss: 9.2622e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4272e-10 - val_loss: 9.0942e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3329e-10 - val_loss: 9.0236e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1979e-10 - val_loss: 8.8834e-10
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0421e-10 - val_loss: 8.7351e-10
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8447e-10 - val_loss: 8.4630e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6048e-10 - val_loss: 8.3315e-10
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5238e-10 - val_loss: 8.2814e-10
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4485e-10 - val_loss: 8.1953e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.3898e-10 - val_loss: 8.0544e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1865e-10 - val_loss: 7.8525e-10
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9867e-10 - val_loss: 7.7342e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8553e-10 - val_loss: 7.5995e-10
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7825e-10 - val_loss: 7.4533e-10
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6256e-10 - val_loss: 7.3741e-10
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5121e-10 - val_loss: 7.3344e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4590e-10 - val_loss: 7.2297e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3336e-10 - val_loss: 7.0570e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1813e-10 - val_loss: 6.9939e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1435e-10 - val_loss: 6.7997e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8953e-10 - val_loss: 6.7110e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.8792e-10 - val_loss: 6.6815e-10
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7991e-10 - val_loss: 6.5942e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.7168e-10 - val_loss: 6.5200e-10
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6281e-10 - val_loss: 6.3797e-10
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.4834e-10 - val_loss: 6.2339e-10
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.3307e-10 - val_loss: 6.0924e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.2202e-10 - val_loss: 6.0811e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1946e-10 - val_loss: 6.0591e-10
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1526e-10 - val_loss: 6.0302e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.1412e-10 - val_loss: 5.8581e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.9287e-10 - val_loss: 5.7321e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8477e-10 - val_loss: 5.7247e-10
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.8422e-10 - val_loss: 5.6348e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.6980e-10 - val_loss: 5.4949e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5763e-10 - val_loss: 5.4295e-10
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5111e-10 - val_loss: 5.3823e-10
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.5307e-10 - val_loss: 5.4300e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4966e-10 - val_loss: 5.3622e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.4441e-10 - val_loss: 5.2429e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.2627e-10 - val_loss: 4.9796e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0690e-10 - val_loss: 4.9764e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0392e-10 - val_loss: 4.9295e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0314e-10 - val_loss: 4.9556e-10
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0878e-10 - val_loss: 5.0043e-10
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0990e-10 - val_loss: 4.9585e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
448/448 - 0s - loss: 5.0454e-10 - val_loss: 4.8861e-10
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.9419e-10 - val_loss: 4.7359e-10
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.7969e-10 - val_loss: 4.5905e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6530e-10 - val_loss: 4.5342e-10
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6077e-10 - val_loss: 4.4575e-10
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.5591e-10 - val_loss: 4.5240e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6162e-10 - val_loss: 4.5222e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6114e-10 - val_loss: 4.5460e-10
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.6030e-10 - val_loss: 4.4668e-10
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.4829e-10 - val_loss: 4.3131e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.3577e-10 - val_loss: 4.2127e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2480e-10 - val_loss: 4.1689e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.2058e-10 - val_loss: 4.1176e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1789e-10 - val_loss: 4.0508e-10
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1224e-10 - val_loss: 4.0894e-10
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1704e-10 - val_loss: 4.0598e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.1201e-10 - val_loss: 3.9902e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
448/448 - 0s - loss: 4.0353e-10 - val_loss: 3.9066e-10
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9554e-10 - val_loss: 3.8026e-10
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8479e-10 - val_loss: 3.7757e-10
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8660e-10 - val_loss: 3.8692e-10
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.9044e-10 - val_loss: 3.8247e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.8404e-10 - val_loss: 3.7221e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7942e-10 - val_loss: 3.7435e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7889e-10 - val_loss: 3.6908e-10
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.7482e-10 - val_loss: 3.6357e-10
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6748e-10 - val_loss: 3.5916e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.6389e-10 - val_loss: 3.5142e-10
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.5489e-10 - val_loss: 3.4822e-10
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5331e-10 - val_loss: 3.4932e-10
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.5503e-10 - val_loss: 3.4876e-10
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.5205e-10 - val_loss: 3.4354e-10
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.4835e-10 - val_loss: 3.3372e-10
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3730e-10 - val_loss: 3.3252e-10
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3612e-10 - val_loss: 3.2801e-10
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3371e-10 - val_loss: 3.3134e-10
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.3567e-10 - val_loss: 3.2842e-10
Epoch 150/512

Epoch 00150: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.3290e-10 - val_loss: 3.2455e-10
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.2768e-10 - val_loss: 3.1886e-10
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.2036e-10 - val_loss: 3.0704e-10
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1156e-10 - val_loss: 3.0585e-10
Epoch 154/512

Epoch 00154: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1110e-10 - val_loss: 3.0520e-10
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1095e-10 - val_loss: 3.1117e-10
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.1668e-10 - val_loss: 3.0972e-10
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.1223e-10 - val_loss: 3.0343e-10
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0759e-10 - val_loss: 2.9997e-10
Epoch 159/512

Epoch 00159: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0308e-10 - val_loss: 2.9814e-10
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0329e-10 - val_loss: 2.9986e-10
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0492e-10 - val_loss: 3.0106e-10
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
448/448 - 0s - loss: 3.0507e-10 - val_loss: 3.0079e-10
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 3.0044e-10 - val_loss: 2.9050e-10
Epoch 164/512

Epoch 00164: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.9155e-10 - val_loss: 2.8138e-10
Epoch 165/512

Epoch 00165: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.8261e-10 - val_loss: 2.7793e-10
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8218e-10 - val_loss: 2.7860e-10
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8293e-10 - val_loss: 2.8084e-10
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.8504e-10 - val_loss: 2.8036e-10
Epoch 169/512

Epoch 00169: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.8369e-10 - val_loss: 2.7516e-10
Epoch 170/512

Epoch 00170: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.7835e-10 - val_loss: 2.7251e-10
Epoch 171/512

Epoch 00171: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.7486e-10 - val_loss: 2.6747e-10
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.7136e-10 - val_loss: 2.6839e-10
Epoch 173/512

Epoch 00173: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6973e-10 - val_loss: 2.6284e-10
Epoch 174/512

Epoch 00174: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6511e-10 - val_loss: 2.6108e-10
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6398e-10 - val_loss: 2.5471e-10
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5901e-10 - val_loss: 2.5784e-10
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.6161e-10 - val_loss: 2.5755e-10
Epoch 178/512

Epoch 00178: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.6064e-10 - val_loss: 2.5262e-10
Epoch 179/512

Epoch 00179: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.5455e-10 - val_loss: 2.4703e-10
Epoch 180/512

Epoch 00180: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4791e-10 - val_loss: 2.4361e-10
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4671e-10 - val_loss: 2.4511e-10
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5014e-10 - val_loss: 2.4959e-10
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.5228e-10 - val_loss: 2.4657e-10
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4844e-10 - val_loss: 2.4544e-10
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.4809e-10 - val_loss: 2.4556e-10
Epoch 186/512

Epoch 00186: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4918e-10 - val_loss: 2.4345e-10
Epoch 187/512

Epoch 00187: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4597e-10 - val_loss: 2.3936e-10
Epoch 188/512

Epoch 00188: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.4054e-10 - val_loss: 2.3342e-10
Epoch 189/512

Epoch 00189: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.3504e-10 - val_loss: 2.2821e-10
Epoch 190/512

Epoch 00190: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2890e-10 - val_loss: 2.2451e-10
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2520e-10 - val_loss: 2.2202e-10
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2411e-10 - val_loss: 2.2314e-10
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2851e-10 - val_loss: 2.3450e-10
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3865e-10 - val_loss: 2.3729e-10
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.3954e-10 - val_loss: 2.3144e-10
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.3174e-10 - val_loss: 2.2141e-10
Epoch 197/512

Epoch 00197: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.2237e-10 - val_loss: 2.1716e-10
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2036e-10 - val_loss: 2.1803e-10
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2116e-10 - val_loss: 2.2069e-10
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.2219e-10 - val_loss: 2.1853e-10
Epoch 201/512

Epoch 00201: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1938e-10 - val_loss: 2.1365e-10
Epoch 202/512

Epoch 00202: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1511e-10 - val_loss: 2.0821e-10
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1073e-10 - val_loss: 2.0946e-10
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1163e-10 - val_loss: 2.0914e-10
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1147e-10 - val_loss: 2.0936e-10
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1104e-10 - val_loss: 2.0957e-10
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.1092e-10 - val_loss: 2.0644e-10
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0806e-10 - val_loss: 2.0644e-10
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.1017e-10 - val_loss: 2.0831e-10
Epoch 210/512

Epoch 00210: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0952e-10 - val_loss: 2.0541e-10
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 2.0591e-10 - val_loss: 1.9923e-10
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9856e-10 - val_loss: 1.9448e-10
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9638e-10 - val_loss: 1.9223e-10
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9347e-10 - val_loss: 1.9187e-10
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9365e-10 - val_loss: 1.9287e-10
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9730e-10 - val_loss: 2.0002e-10
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0299e-10 - val_loss: 2.0088e-10
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
448/448 - 0s - loss: 2.0175e-10 - val_loss: 1.9415e-10
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9587e-10 - val_loss: 1.9282e-10
Epoch 220/512

Epoch 00220: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9387e-10 - val_loss: 1.9091e-10
Epoch 221/512

Epoch 00221: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.9218e-10 - val_loss: 1.8774e-10
Epoch 222/512

Epoch 00222: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8830e-10 - val_loss: 1.8497e-10
Epoch 223/512

Epoch 00223: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8593e-10 - val_loss: 1.8166e-10
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8378e-10 - val_loss: 1.8191e-10
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8383e-10 - val_loss: 1.8251e-10
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8526e-10 - val_loss: 1.8435e-10
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8693e-10 - val_loss: 1.8773e-10
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.9091e-10 - val_loss: 1.8725e-10
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8804e-10 - val_loss: 1.8316e-10
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.8597e-10 - val_loss: 1.8305e-10
Epoch 231/512

Epoch 00231: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8347e-10 - val_loss: 1.7943e-10
Epoch 232/512

Epoch 00232: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.8018e-10 - val_loss: 1.7878e-10
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7938e-10 - val_loss: 1.7608e-10
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7703e-10 - val_loss: 1.7429e-10
Epoch 235/512

Epoch 00235: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7397e-10 - val_loss: 1.7271e-10
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7490e-10 - val_loss: 1.7010e-10
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7316e-10 - val_loss: 1.7604e-10
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7990e-10 - val_loss: 1.7780e-10
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7886e-10 - val_loss: 1.7755e-10
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.7615e-10 - val_loss: 1.7321e-10
Epoch 241/512

Epoch 00241: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.7411e-10 - val_loss: 1.6908e-10
Epoch 242/512

Epoch 00242: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6900e-10 - val_loss: 1.6348e-10
Epoch 243/512

Epoch 00243: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6507e-10 - val_loss: 1.6303e-10
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6436e-10 - val_loss: 1.6373e-10
Epoch 245/512

Epoch 00245: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6430e-10 - val_loss: 1.6054e-10
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6249e-10 - val_loss: 1.6428e-10
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6688e-10 - val_loss: 1.6323e-10
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6513e-10 - val_loss: 1.6510e-10
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6677e-10 - val_loss: 1.6359e-10
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6588e-10 - val_loss: 1.6240e-10
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6251e-10 - val_loss: 1.5933e-10
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.6132e-10 - val_loss: 1.5953e-10
Epoch 253/512

Epoch 00253: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.6027e-10 - val_loss: 1.5709e-10
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5867e-10 - val_loss: 1.5727e-10
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5889e-10 - val_loss: 1.5847e-10
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5964e-10 - val_loss: 1.5757e-10
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.5954e-10 - val_loss: 1.5914e-10
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5914e-10 - val_loss: 1.5660e-10
Epoch 259/512

Epoch 00259: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5738e-10 - val_loss: 1.5401e-10
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5481e-10 - val_loss: 1.5170e-10
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5168e-10 - val_loss: 1.5082e-10
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.5178e-10 - val_loss: 1.4806e-10
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4961e-10 - val_loss: 1.4815e-10
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4912e-10 - val_loss: 1.4717e-10
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4676e-10 - val_loss: 1.4386e-10
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4437e-10 - val_loss: 1.4333e-10
Epoch 267/512

Epoch 00267: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4342e-10 - val_loss: 1.3998e-10
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4048e-10 - val_loss: 1.4085e-10
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4201e-10 - val_loss: 1.4053e-10
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4286e-10 - val_loss: 1.4387e-10
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4530e-10 - val_loss: 1.4246e-10
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4432e-10 - val_loss: 1.4289e-10
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4367e-10 - val_loss: 1.4058e-10
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.4198e-10 - val_loss: 1.4081e-10
Epoch 275/512

Epoch 00275: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.4167e-10 - val_loss: 1.3716e-10
Epoch 276/512

Epoch 00276: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3728e-10 - val_loss: 1.3290e-10
Epoch 277/512

Epoch 00277: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3419e-10 - val_loss: 1.3239e-10
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3406e-10 - val_loss: 1.3274e-10
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3487e-10 - val_loss: 1.3655e-10
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3772e-10 - val_loss: 1.3785e-10
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3851e-10 - val_loss: 1.3600e-10
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3724e-10 - val_loss: 1.3669e-10
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3721e-10 - val_loss: 1.3580e-10
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3708e-10 - val_loss: 1.3518e-10
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3628e-10 - val_loss: 1.3422e-10
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3564e-10 - val_loss: 1.3469e-10
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3592e-10 - val_loss: 1.3288e-10
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3498e-10 - val_loss: 1.3292e-10
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.3454e-10 - val_loss: 1.3361e-10
Epoch 290/512

Epoch 00290: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3376e-10 - val_loss: 1.3024e-10
Epoch 291/512

Epoch 00291: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3103e-10 - val_loss: 1.2998e-10
Epoch 292/512

Epoch 00292: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.3097e-10 - val_loss: 1.2903e-10
Epoch 293/512

Epoch 00293: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2998e-10 - val_loss: 1.2865e-10
Epoch 294/512

Epoch 00294: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2994e-10 - val_loss: 1.2764e-10
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2992e-10 - val_loss: 1.2926e-10
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2959e-10 - val_loss: 1.2854e-10
Epoch 297/512

Epoch 00297: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2874e-10 - val_loss: 1.2654e-10
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2766e-10 - val_loss: 1.2711e-10
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2937e-10 - val_loss: 1.2927e-10
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2964e-10 - val_loss: 1.2594e-10
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2739e-10 - val_loss: 1.2681e-10
Epoch 302/512

Epoch 00302: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2666e-10 - val_loss: 1.2426e-10
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2554e-10 - val_loss: 1.2649e-10
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2628e-10 - val_loss: 1.2479e-10
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2551e-10 - val_loss: 1.2286e-10
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2430e-10 - val_loss: 1.2534e-10
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2570e-10 - val_loss: 1.2419e-10
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2483e-10 - val_loss: 1.2374e-10
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2401e-10 - val_loss: 1.2351e-10
Epoch 310/512

Epoch 00310: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2462e-10 - val_loss: 1.2248e-10
Epoch 311/512

Epoch 00311: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2377e-10 - val_loss: 1.2043e-10
Epoch 312/512

Epoch 00312: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2030e-10 - val_loss: 1.1906e-10
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1968e-10 - val_loss: 1.1940e-10
Epoch 314/512

Epoch 00314: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2029e-10 - val_loss: 1.1740e-10
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1798e-10 - val_loss: 1.1988e-10
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.2122e-10 - val_loss: 1.2039e-10
Epoch 317/512

Epoch 00317: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.2006e-10 - val_loss: 1.1543e-10
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1601e-10 - val_loss: 1.1554e-10
Epoch 319/512

Epoch 00319: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1646e-10 - val_loss: 1.1491e-10
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1556e-10 - val_loss: 1.1415e-10
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1477e-10 - val_loss: 1.1460e-10
Epoch 322/512

Epoch 00322: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1487e-10 - val_loss: 1.1213e-10
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1341e-10 - val_loss: 1.1352e-10
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1463e-10 - val_loss: 1.1457e-10
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1528e-10 - val_loss: 1.1459e-10
Epoch 326/512

Epoch 00326: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1480e-10 - val_loss: 1.1196e-10
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1364e-10 - val_loss: 1.1260e-10
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1374e-10 - val_loss: 1.1373e-10
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1493e-10 - val_loss: 1.1385e-10
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1511e-10 - val_loss: 1.1400e-10
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1446e-10 - val_loss: 1.1322e-10
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1333e-10 - val_loss: 1.1332e-10
Epoch 333/512

Epoch 00333: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1383e-10 - val_loss: 1.1124e-10
Epoch 334/512

Epoch 00334: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1201e-10 - val_loss: 1.1001e-10
Epoch 335/512

Epoch 00335: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1052e-10 - val_loss: 1.0948e-10
Epoch 336/512

Epoch 00336: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.1011e-10 - val_loss: 1.0910e-10
Epoch 337/512

Epoch 00337: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0995e-10 - val_loss: 1.0811e-10
Epoch 338/512

Epoch 00338: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0850e-10 - val_loss: 1.0778e-10
Epoch 339/512

Epoch 00339: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0814e-10 - val_loss: 1.0549e-10
Epoch 340/512

Epoch 00340: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0583e-10 - val_loss: 1.0402e-10
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0519e-10 - val_loss: 1.0575e-10
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0621e-10 - val_loss: 1.0547e-10
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0679e-10 - val_loss: 1.0661e-10
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0856e-10 - val_loss: 1.0868e-10
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0947e-10 - val_loss: 1.0932e-10
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0987e-10 - val_loss: 1.0973e-10
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1097e-10 - val_loss: 1.1056e-10
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.1169e-10 - val_loss: 1.1050e-10
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0873e-10 - val_loss: 1.0519e-10
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0435e-10 - val_loss: 1.0225e-10
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0148e-10 - val_loss: 9.9848e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0134e-10 - val_loss: 1.0082e-10
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0174e-10 - val_loss: 1.0082e-10
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0178e-10 - val_loss: 1.0274e-10
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0337e-10 - val_loss: 1.0260e-10
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0291e-10 - val_loss: 1.0272e-10
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0295e-10 - val_loss: 1.0319e-10
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0402e-10 - val_loss: 1.0260e-10
Epoch 359/512

Epoch 00359: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0252e-10 - val_loss: 9.9762e-11
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0079e-10 - val_loss: 1.0084e-10
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0186e-10 - val_loss: 1.0150e-10
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0224e-10 - val_loss: 1.0039e-10
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0070e-10 - val_loss: 1.0004e-10
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
448/448 - 0s - loss: 1.0086e-10 - val_loss: 1.0085e-10
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 1.0100e-10 - val_loss: 9.9028e-11
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.9546e-11 - val_loss: 9.7911e-11
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8614e-11 - val_loss: 9.8174e-11
Epoch 368/512

Epoch 00368: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.7913e-11 - val_loss: 9.5863e-11
Epoch 369/512

Epoch 00369: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.5638e-11 - val_loss: 9.5180e-11
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6751e-11 - val_loss: 9.7630e-11
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.9022e-11 - val_loss: 9.8974e-11
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.8622e-11 - val_loss: 9.5650e-11
Epoch 373/512

Epoch 00373: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.5240e-11 - val_loss: 9.3887e-11
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4610e-11 - val_loss: 9.5531e-11
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6001e-11 - val_loss: 9.5427e-11
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5763e-11 - val_loss: 9.4945e-11
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5911e-11 - val_loss: 9.5114e-11
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6165e-11 - val_loss: 9.4463e-11
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.4015e-11 - val_loss: 9.2127e-11
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3131e-11 - val_loss: 9.3443e-11
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3832e-11 - val_loss: 9.4695e-11
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5301e-11 - val_loss: 9.5326e-11
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.6295e-11 - val_loss: 9.5058e-11
Epoch 384/512

Epoch 00384: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.5234e-11 - val_loss: 9.4280e-11
Epoch 385/512

Epoch 00385: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.4564e-11 - val_loss: 9.2836e-11
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.2845e-11 - val_loss: 9.1092e-11
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2309e-11 - val_loss: 9.1799e-11
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1836e-11 - val_loss: 9.2230e-11
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2879e-11 - val_loss: 9.2026e-11
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3050e-11 - val_loss: 9.1834e-11
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2321e-11 - val_loss: 9.1783e-11
Epoch 392/512

Epoch 00392: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.1371e-11 - val_loss: 9.0092e-11
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0658e-11 - val_loss: 9.0930e-11
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1315e-11 - val_loss: 9.2262e-11
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.3683e-11 - val_loss: 9.3046e-11
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2830e-11 - val_loss: 9.1034e-11
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1010e-11 - val_loss: 9.0140e-11
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0798e-11 - val_loss: 9.0869e-11
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.2309e-11 - val_loss: 9.1884e-11
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1955e-11 - val_loss: 9.0215e-11
Epoch 401/512

Epoch 00401: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 9.0767e-11 - val_loss: 8.9157e-11
Epoch 402/512

Epoch 00402: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.9523e-11 - val_loss: 8.8599e-11
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.9346e-11 - val_loss: 8.9298e-11
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0188e-11 - val_loss: 8.9647e-11
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.8947e-11 - val_loss: 8.6812e-11
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.8222e-11 - val_loss: 8.9482e-11
Epoch 407/512

Epoch 00407: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0311e-11 - val_loss: 9.0290e-11
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1522e-11 - val_loss: 9.0228e-11
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.1242e-11 - val_loss: 9.0356e-11
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
448/448 - 0s - loss: 9.0194e-11 - val_loss: 8.8134e-11
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.8048e-11 - val_loss: 8.6096e-11
Epoch 412/512

Epoch 00412: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.5667e-11 - val_loss: 8.5350e-11
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5967e-11 - val_loss: 8.5549e-11
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6979e-11 - val_loss: 8.6661e-11
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.7263e-11 - val_loss: 8.5802e-11
Epoch 416/512

Epoch 00416: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.5891e-11 - val_loss: 8.3900e-11
Epoch 417/512

Epoch 00417: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.4198e-11 - val_loss: 8.3062e-11
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4367e-11 - val_loss: 8.4438e-11
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5238e-11 - val_loss: 8.5325e-11
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6307e-11 - val_loss: 8.5823e-11
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.6258e-11 - val_loss: 8.4596e-11
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4874e-11 - val_loss: 8.3721e-11
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4483e-11 - val_loss: 8.4336e-11
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.5216e-11 - val_loss: 8.4838e-11
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.4759e-11 - val_loss: 8.3214e-11
Epoch 426/512

Epoch 00426: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.3092e-11 - val_loss: 8.1448e-11
Epoch 427/512

Epoch 00427: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.1606e-11 - val_loss: 8.0224e-11
Epoch 428/512

Epoch 00428: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 8.0196e-11 - val_loss: 7.9066e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9644e-11 - val_loss: 7.9473e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0258e-11 - val_loss: 7.9691e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0550e-11 - val_loss: 8.0515e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1323e-11 - val_loss: 8.1727e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1863e-11 - val_loss: 8.1872e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2626e-11 - val_loss: 8.3108e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.2826e-11 - val_loss: 8.1091e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1485e-11 - val_loss: 8.0248e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0751e-11 - val_loss: 8.0339e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0740e-11 - val_loss: 8.0915e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1837e-11 - val_loss: 8.1159e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.1337e-11 - val_loss: 8.0032e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0669e-11 - val_loss: 7.9627e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9911e-11 - val_loss: 7.9191e-11
Epoch 443/512

Epoch 00443: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.8983e-11 - val_loss: 7.7251e-11
Epoch 444/512

Epoch 00444: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.7647e-11 - val_loss: 7.6926e-11
Epoch 445/512

Epoch 00445: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.7040e-11 - val_loss: 7.6908e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7878e-11 - val_loss: 7.7976e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9054e-11 - val_loss: 7.9199e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9651e-11 - val_loss: 7.8404e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8881e-11 - val_loss: 7.8572e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0099e-11 - val_loss: 8.0769e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
448/448 - 0s - loss: 8.0879e-11 - val_loss: 7.9210e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9476e-11 - val_loss: 7.8053e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7861e-11 - val_loss: 7.7046e-11
Epoch 454/512

Epoch 00454: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.7451e-11 - val_loss: 7.6244e-11
Epoch 455/512

Epoch 00455: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.6045e-11 - val_loss: 7.5994e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6681e-11 - val_loss: 7.7243e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7993e-11 - val_loss: 7.8031e-11
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.9232e-11 - val_loss: 7.8260e-11
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.8768e-11 - val_loss: 7.6799e-11
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7276e-11 - val_loss: 7.6027e-11
Epoch 461/512

Epoch 00461: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.6306e-11 - val_loss: 7.5160e-11
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5209e-11 - val_loss: 7.5761e-11
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6400e-11 - val_loss: 7.6441e-11
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7513e-11 - val_loss: 7.7077e-11
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7398e-11 - val_loss: 7.6648e-11
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7621e-11 - val_loss: 7.8134e-11
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7865e-11 - val_loss: 7.6794e-11
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.7286e-11 - val_loss: 7.6393e-11
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6878e-11 - val_loss: 7.7102e-11
Epoch 470/512

Epoch 00470: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.7271e-11 - val_loss: 7.5081e-11
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6010e-11 - val_loss: 7.5568e-11
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.5481e-11 - val_loss: 7.4596e-11
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.5076e-11 - val_loss: 7.4168e-11
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5250e-11 - val_loss: 7.5035e-11
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5420e-11 - val_loss: 7.5668e-11
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6150e-11 - val_loss: 7.5456e-11
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5623e-11 - val_loss: 7.5506e-11
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5356e-11 - val_loss: 7.4655e-11
Epoch 479/512

Epoch 00479: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.4696e-11 - val_loss: 7.3049e-11
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3052e-11 - val_loss: 7.3496e-11
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4067e-11 - val_loss: 7.4503e-11
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.5499e-11 - val_loss: 7.6125e-11
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6504e-11 - val_loss: 7.6552e-11
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.6877e-11 - val_loss: 7.4880e-11
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4621e-11 - val_loss: 7.3937e-11
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4187e-11 - val_loss: 7.4458e-11
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4937e-11 - val_loss: 7.3789e-11
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3975e-11 - val_loss: 7.3787e-11
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.3110e-11 - val_loss: 7.2148e-11
Epoch 490/512

Epoch 00490: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.1734e-11 - val_loss: 7.0291e-11
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0890e-11 - val_loss: 7.1231e-11
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.1258e-11 - val_loss: 7.0329e-11
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0859e-11 - val_loss: 7.1336e-11
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2212e-11 - val_loss: 7.2877e-11
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3752e-11 - val_loss: 7.4720e-11
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.4995e-11 - val_loss: 7.2950e-11
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3166e-11 - val_loss: 7.2171e-11
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2035e-11 - val_loss: 7.2050e-11
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.2932e-11 - val_loss: 7.2803e-11
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3628e-11 - val_loss: 7.3447e-11
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.3304e-11 - val_loss: 7.3599e-11
Epoch 502/512

Epoch 00502: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.2744e-11 - val_loss: 7.0165e-11
Epoch 503/512

Epoch 00503: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 7.0167e-11 - val_loss: 6.9513e-11
Epoch 504/512

Epoch 00504: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.9631e-11 - val_loss: 6.9291e-11
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.8354e-11 - val_loss: 6.7345e-11
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.7663e-11 - val_loss: 6.6544e-11
Epoch 507/512

Epoch 00507: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.6457e-11 - val_loss: 6.5499e-11
Epoch 508/512

Epoch 00508: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-curve-secp256k1/multiplication_weights.h5
448/448 - 0s - loss: 6.5536e-11 - val_loss: 6.4736e-11
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.5329e-11 - val_loss: 6.5684e-11
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.6528e-11 - val_loss: 6.7872e-11
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
448/448 - 0s - loss: 6.9123e-11 - val_loss: 7.0399e-11
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
448/448 - 0s - loss: 7.0964e-11 - val_loss: 6.8907e-11
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 9.304 | eve: 9.767 | bob: 9.196Epoch   0:   0% | abe: 9.276 | eve: 9.779 | bob: 9.175Epoch   0:   1% | abe: 9.270 | eve: 9.777 | bob: 9.176