WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-12 10:46:51.964055: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-12 10:46:52.088404: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:46:52.089201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:46:52.091815: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:46:52.094201: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:46:52.094848: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:46:52.098799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:46:52.101496: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:46:52.108629: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:46:52.118142: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:46:52.119314: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-12 10:46:52.139940: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-12 10:46:52.143435: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3d2a6c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:46:52.143762: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-12 10:46:52.651839: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x1627250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-12 10:46:52.651941: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-12 10:46:52.656379: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:89:00.0
2024-04-12 10:46:52.656520: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:46:52.656551: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-12 10:46:52.656578: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-12 10:46:52.656601: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-12 10:46:52.656624: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-12 10:46:52.656647: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-12 10:46:52.656671: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-12 10:46:52.672706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-12 10:46:52.673079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-12 10:46:52.681709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-12 10:46:52.681962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-12 10:46:52.682027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-12 10:46:52.701277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30593 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:89:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-12 10:46:58.270121: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.6300 - val_loss: 0.0038
Epoch 2/512
512/512 - 0s - loss: 0.2898 - val_loss: 0.0015
Epoch 3/512
512/512 - 0s - loss: 0.0928 - val_loss: 2.2971e-04
Epoch 4/512
512/512 - 0s - loss: 0.0108 - val_loss: 1.5214e-05
Epoch 5/512
512/512 - 0s - loss: 9.9590e-04 - val_loss: 6.0224e-06
Epoch 6/512
512/512 - 0s - loss: 5.1713e-04 - val_loss: 3.8830e-06
Epoch 7/512
512/512 - 0s - loss: 3.2414e-04 - val_loss: 2.2792e-06
Epoch 8/512
512/512 - 0s - loss: 1.8291e-04 - val_loss: 1.1803e-06
Epoch 9/512
512/512 - 0s - loss: 9.0325e-05 - val_loss: 5.2273e-07
Epoch 10/512
512/512 - 0s - loss: 3.7774e-05 - val_loss: 1.9003e-07
Epoch 11/512
512/512 - 0s - loss: 1.2818e-05 - val_loss: 5.3750e-08
Epoch 12/512
512/512 - 0s - loss: 3.3393e-06 - val_loss: 1.1024e-08
Epoch 13/512
512/512 - 0s - loss: 6.2213e-07 - val_loss: 1.5321e-09
Epoch 14/512
512/512 - 0s - loss: 1.1903e-07 - val_loss: 5.4905e-09
Epoch 15/512
512/512 - 0s - loss: 2.9634e-05 - val_loss: 6.7600e-06
Epoch 16/512
512/512 - 0s - loss: 0.0024 - val_loss: 9.0996e-06
Epoch 17/512
512/512 - 0s - loss: 3.9686e-04 - val_loss: 8.7962e-07
Epoch 18/512
512/512 - 0s - loss: 9.3548e-05 - val_loss: 1.7975e-06
Epoch 19/512
512/512 - 0s - loss: 5.5312e-04 - val_loss: 1.8025e-05
Epoch 20/512
512/512 - 0s - loss: 0.0016 - val_loss: 7.1661e-06
Epoch 21/512
512/512 - 0s - loss: 4.4813e-04 - val_loss: 2.7937e-06
Epoch 22/512
512/512 - 0s - loss: 3.6284e-04 - val_loss: 7.1452e-06
Epoch 23/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.1851e-05
Epoch 24/512
512/512 - 0s - loss: 8.5375e-04 - val_loss: 4.8939e-06
Epoch 25/512
512/512 - 0s - loss: 4.4703e-04 - val_loss: 5.1844e-06
Epoch 26/512
512/512 - 0s - loss: 6.7037e-04 - val_loss: 9.3607e-06
Epoch 27/512
512/512 - 0s - loss: 8.6772e-04 - val_loss: 6.8605e-06
Epoch 28/512
512/512 - 0s - loss: 5.7393e-04 - val_loss: 5.1617e-06
Epoch 29/512
512/512 - 0s - loss: 5.5856e-04 - val_loss: 6.8822e-06
Epoch 30/512
512/512 - 0s - loss: 7.2011e-04 - val_loss: 7.2416e-06
Epoch 31/512
512/512 - 0s - loss: 6.3798e-04 - val_loss: 5.5806e-06
Epoch 32/512
512/512 - 0s - loss: 5.4228e-04 - val_loss: 5.7892e-06
Epoch 33/512
512/512 - 0s - loss: 6.0658e-04 - val_loss: 6.5206e-06
Epoch 34/512
512/512 - 0s - loss: 6.2064e-04 - val_loss: 5.7170e-06
Epoch 35/512
512/512 - 0s - loss: 5.4351e-04 - val_loss: 5.3579e-06
Epoch 36/512
512/512 - 0s - loss: 5.4576e-04 - val_loss: 5.7730e-06
Epoch 37/512
512/512 - 0s - loss: 5.7243e-04 - val_loss: 5.6006e-06
Epoch 38/512
512/512 - 0s - loss: 5.3056e-04 - val_loss: 5.2729e-06
Epoch 39/512
512/512 - 0s - loss: 5.2519e-04 - val_loss: 5.1930e-06
Epoch 40/512
512/512 - 0s - loss: 5.1169e-04 - val_loss: 5.2407e-06
Epoch 41/512
512/512 - 0s - loss: 5.1589e-04 - val_loss: 5.0852e-06
Epoch 42/512
512/512 - 0s - loss: 4.9332e-04 - val_loss: 4.9476e-06
Epoch 43/512
512/512 - 0s - loss: 4.8818e-04 - val_loss: 4.8960e-06
Epoch 44/512
512/512 - 0s - loss: 4.7994e-04 - val_loss: 4.7850e-06
Epoch 45/512
512/512 - 0s - loss: 4.6850e-04 - val_loss: 4.6654e-06
Epoch 46/512
512/512 - 0s - loss: 4.6201e-04 - val_loss: 4.6072e-06
Epoch 47/512
512/512 - 0s - loss: 4.5150e-04 - val_loss: 4.5207e-06
Epoch 48/512
512/512 - 0s - loss: 4.4592e-04 - val_loss: 4.4401e-06
Epoch 49/512
512/512 - 0s - loss: 4.3768e-04 - val_loss: 4.3273e-06
Epoch 50/512
512/512 - 0s - loss: 4.2439e-04 - val_loss: 4.2466e-06
Epoch 51/512
512/512 - 0s - loss: 4.1911e-04 - val_loss: 4.2874e-06
Epoch 52/512
512/512 - 0s - loss: 4.2045e-04 - val_loss: 4.1268e-06
Epoch 53/512
512/512 - 0s - loss: 4.0329e-04 - val_loss: 3.9948e-06
Epoch 54/512
512/512 - 0s - loss: 3.9662e-04 - val_loss: 3.9596e-06
Epoch 55/512
512/512 - 0s - loss: 3.9020e-04 - val_loss: 3.9948e-06
Epoch 56/512
512/512 - 0s - loss: 3.9056e-04 - val_loss: 3.8857e-06
Epoch 57/512
512/512 - 0s - loss: 3.7957e-04 - val_loss: 3.6991e-06
Epoch 58/512
512/512 - 0s - loss: 3.6511e-04 - val_loss: 3.7138e-06
Epoch 59/512
512/512 - 0s - loss: 3.7132e-04 - val_loss: 3.6721e-06
Epoch 60/512
512/512 - 0s - loss: 3.5910e-04 - val_loss: 3.5566e-06
Epoch 61/512
512/512 - 0s - loss: 3.5193e-04 - val_loss: 3.5180e-06
Epoch 62/512
512/512 - 0s - loss: 3.4885e-04 - val_loss: 3.4494e-06
Epoch 63/512
512/512 - 0s - loss: 3.4231e-04 - val_loss: 3.3837e-06
Epoch 64/512
512/512 - 0s - loss: 3.3638e-04 - val_loss: 3.3151e-06
Epoch 65/512
512/512 - 0s - loss: 3.2907e-04 - val_loss: 3.2901e-06
Epoch 66/512
512/512 - 0s - loss: 3.2583e-04 - val_loss: 3.2567e-06
Epoch 67/512
512/512 - 0s - loss: 3.2200e-04 - val_loss: 3.1767e-06
Epoch 68/512
512/512 - 0s - loss: 3.1445e-04 - val_loss: 3.1073e-06
Epoch 69/512
512/512 - 0s - loss: 3.0911e-04 - val_loss: 3.0774e-06
Epoch 70/512
512/512 - 0s - loss: 3.0602e-04 - val_loss: 3.0407e-06
Epoch 71/512
512/512 - 0s - loss: 3.0254e-04 - val_loss: 2.9231e-06
Epoch 72/512
512/512 - 0s - loss: 2.9067e-04 - val_loss: 2.9249e-06
Epoch 73/512
512/512 - 0s - loss: 2.9028e-04 - val_loss: 2.9675e-06
Epoch 74/512
512/512 - 0s - loss: 2.9548e-04 - val_loss: 2.7701e-06
Epoch 75/512
512/512 - 0s - loss: 2.7422e-04 - val_loss: 2.7191e-06
Epoch 76/512
512/512 - 0s - loss: 2.7526e-04 - val_loss: 2.8088e-06
Epoch 77/512
512/512 - 0s - loss: 2.7962e-04 - val_loss: 2.7272e-06
Epoch 78/512
512/512 - 0s - loss: 2.6972e-04 - val_loss: 2.5648e-06
Epoch 79/512
512/512 - 0s - loss: 2.5851e-04 - val_loss: 2.5828e-06
Epoch 80/512
512/512 - 0s - loss: 2.5995e-04 - val_loss: 2.6945e-06
Epoch 81/512
512/512 - 0s - loss: 2.6477e-04 - val_loss: 2.5644e-06
Epoch 82/512
512/512 - 0s - loss: 2.4986e-04 - val_loss: 2.4298e-06
Epoch 83/512
512/512 - 0s - loss: 2.4540e-04 - val_loss: 2.4602e-06
Epoch 84/512
512/512 - 0s - loss: 2.4624e-04 - val_loss: 2.4890e-06
Epoch 85/512
512/512 - 0s - loss: 2.4518e-04 - val_loss: 2.3897e-06
Epoch 86/512
512/512 - 0s - loss: 2.3665e-04 - val_loss: 2.3011e-06
Epoch 87/512
512/512 - 0s - loss: 2.3204e-04 - val_loss: 2.3133e-06
Epoch 88/512
512/512 - 0s - loss: 2.3271e-04 - val_loss: 2.2899e-06
Epoch 89/512
512/512 - 0s - loss: 2.2578e-04 - val_loss: 2.2610e-06
Epoch 90/512
512/512 - 0s - loss: 2.2389e-04 - val_loss: 2.2480e-06
Epoch 91/512
512/512 - 0s - loss: 2.2233e-04 - val_loss: 2.1871e-06
Epoch 92/512
512/512 - 0s - loss: 2.1746e-04 - val_loss: 2.1065e-06
Epoch 93/512
512/512 - 0s - loss: 2.1256e-04 - val_loss: 2.0824e-06
Epoch 94/512
512/512 - 0s - loss: 2.0873e-04 - val_loss: 2.0998e-06
Epoch 95/512
512/512 - 0s - loss: 2.1096e-04 - val_loss: 2.0443e-06
Epoch 96/512
512/512 - 0s - loss: 2.0371e-04 - val_loss: 1.9597e-06
Epoch 97/512
512/512 - 0s - loss: 1.9629e-04 - val_loss: 2.0210e-06
Epoch 98/512
512/512 - 0s - loss: 2.0305e-04 - val_loss: 1.9963e-06
Epoch 99/512
512/512 - 0s - loss: 1.9617e-04 - val_loss: 1.8741e-06
Epoch 100/512
512/512 - 0s - loss: 1.8757e-04 - val_loss: 1.8748e-06
Epoch 101/512
512/512 - 0s - loss: 1.9100e-04 - val_loss: 1.8777e-06
Epoch 102/512
512/512 - 0s - loss: 1.8578e-04 - val_loss: 1.8433e-06
Epoch 103/512
512/512 - 0s - loss: 1.8317e-04 - val_loss: 1.8237e-06
Epoch 104/512
512/512 - 0s - loss: 1.8042e-04 - val_loss: 1.8025e-06
Epoch 105/512
512/512 - 0s - loss: 1.8015e-04 - val_loss: 1.7418e-06
Epoch 106/512
512/512 - 0s - loss: 1.7199e-04 - val_loss: 1.7201e-06
Epoch 107/512
512/512 - 0s - loss: 1.7290e-04 - val_loss: 1.7108e-06
Epoch 108/512
512/512 - 0s - loss: 1.7228e-04 - val_loss: 1.6222e-06
Epoch 109/512
512/512 - 0s - loss: 1.6168e-04 - val_loss: 1.6406e-06
Epoch 110/512
512/512 - 0s - loss: 1.6652e-04 - val_loss: 1.6578e-06
Epoch 111/512
512/512 - 0s - loss: 1.6469e-04 - val_loss: 1.5570e-06
Epoch 112/512
512/512 - 0s - loss: 1.5636e-04 - val_loss: 1.4979e-06
Epoch 113/512
512/512 - 0s - loss: 1.5399e-04 - val_loss: 1.5426e-06
Epoch 114/512
512/512 - 0s - loss: 1.5775e-04 - val_loss: 1.5101e-06
Epoch 115/512
512/512 - 0s - loss: 1.5024e-04 - val_loss: 1.4455e-06
Epoch 116/512
512/512 - 0s - loss: 1.4634e-04 - val_loss: 1.4645e-06
Epoch 117/512
512/512 - 0s - loss: 1.4769e-04 - val_loss: 1.4664e-06
Epoch 118/512
512/512 - 0s - loss: 1.4672e-04 - val_loss: 1.3931e-06
Epoch 119/512
512/512 - 0s - loss: 1.3840e-04 - val_loss: 1.3629e-06
Epoch 120/512
512/512 - 0s - loss: 1.3870e-04 - val_loss: 1.3878e-06
Epoch 121/512
512/512 - 0s - loss: 1.3954e-04 - val_loss: 1.3366e-06
Epoch 122/512
512/512 - 0s - loss: 1.3373e-04 - val_loss: 1.2854e-06
Epoch 123/512
512/512 - 0s - loss: 1.3004e-04 - val_loss: 1.3006e-06
Epoch 124/512
512/512 - 0s - loss: 1.3116e-04 - val_loss: 1.2968e-06
Epoch 125/512
512/512 - 0s - loss: 1.3023e-04 - val_loss: 1.2070e-06
Epoch 126/512
512/512 - 0s - loss: 1.2101e-04 - val_loss: 1.2101e-06
Epoch 127/512
512/512 - 0s - loss: 1.2369e-04 - val_loss: 1.2499e-06
Epoch 128/512
512/512 - 0s - loss: 1.2494e-04 - val_loss: 1.1772e-06
Epoch 129/512
512/512 - 0s - loss: 1.1667e-04 - val_loss: 1.1162e-06
Epoch 130/512
512/512 - 0s - loss: 1.1449e-04 - val_loss: 1.1508e-06
Epoch 131/512
512/512 - 0s - loss: 1.1754e-04 - val_loss: 1.1188e-06
Epoch 132/512
512/512 - 0s - loss: 1.1117e-04 - val_loss: 1.0780e-06
Epoch 133/512
512/512 - 0s - loss: 1.0982e-04 - val_loss: 1.0826e-06
Epoch 134/512
512/512 - 0s - loss: 1.0895e-04 - val_loss: 1.0649e-06
Epoch 135/512
512/512 - 0s - loss: 1.0653e-04 - val_loss: 1.0279e-06
Epoch 136/512
512/512 - 0s - loss: 1.0370e-04 - val_loss: 1.0025e-06
Epoch 137/512
512/512 - 0s - loss: 1.0213e-04 - val_loss: 9.8620e-07
Epoch 138/512
512/512 - 0s - loss: 1.0037e-04 - val_loss: 9.7051e-07
Epoch 139/512
512/512 - 0s - loss: 9.8136e-05 - val_loss: 9.5451e-07
Epoch 140/512
512/512 - 0s - loss: 9.5757e-05 - val_loss: 9.4602e-07
Epoch 141/512
512/512 - 0s - loss: 9.4785e-05 - val_loss: 9.2749e-07
Epoch 142/512
512/512 - 0s - loss: 9.3259e-05 - val_loss: 8.8817e-07
Epoch 143/512
512/512 - 0s - loss: 8.9322e-05 - val_loss: 8.7686e-07
Epoch 144/512
512/512 - 0s - loss: 8.9716e-05 - val_loss: 8.6374e-07
Epoch 145/512
512/512 - 0s - loss: 8.6489e-05 - val_loss: 8.4146e-07
Epoch 146/512
512/512 - 0s - loss: 8.5445e-05 - val_loss: 8.1861e-07
Epoch 147/512
512/512 - 0s - loss: 8.2463e-05 - val_loss: 8.1566e-07
Epoch 148/512
512/512 - 0s - loss: 8.2054e-05 - val_loss: 8.1111e-07
Epoch 149/512
512/512 - 0s - loss: 8.1786e-05 - val_loss: 7.5442e-07
Epoch 150/512
512/512 - 0s - loss: 7.5376e-05 - val_loss: 7.4551e-07
Epoch 151/512
512/512 - 0s - loss: 7.7111e-05 - val_loss: 7.5776e-07
Epoch 152/512
512/512 - 0s - loss: 7.6014e-05 - val_loss: 7.2300e-07
Epoch 153/512
512/512 - 0s - loss: 7.2194e-05 - val_loss: 6.9308e-07
Epoch 154/512
512/512 - 0s - loss: 7.0973e-05 - val_loss: 6.9166e-07
Epoch 155/512
512/512 - 0s - loss: 7.0200e-05 - val_loss: 6.8937e-07
Epoch 156/512
512/512 - 0s - loss: 6.9283e-05 - val_loss: 6.5294e-07
Epoch 157/512
512/512 - 0s - loss: 6.5701e-05 - val_loss: 6.2877e-07
Epoch 158/512
512/512 - 0s - loss: 6.4476e-05 - val_loss: 6.3796e-07
Epoch 159/512
512/512 - 0s - loss: 6.4701e-05 - val_loss: 6.2536e-07
Epoch 160/512
512/512 - 0s - loss: 6.2762e-05 - val_loss: 5.8093e-07
Epoch 161/512
512/512 - 0s - loss: 5.9079e-05 - val_loss: 5.7425e-07
Epoch 162/512
512/512 - 0s - loss: 5.9762e-05 - val_loss: 5.7231e-07
Epoch 163/512
512/512 - 0s - loss: 5.7899e-05 - val_loss: 5.5515e-07
Epoch 164/512
512/512 - 0s - loss: 5.5852e-05 - val_loss: 5.4885e-07
Epoch 165/512
512/512 - 0s - loss: 5.5375e-05 - val_loss: 5.3119e-07
Epoch 166/512
512/512 - 0s - loss: 5.3805e-05 - val_loss: 5.0785e-07
Epoch 167/512
512/512 - 0s - loss: 5.1549e-05 - val_loss: 4.9882e-07
Epoch 168/512
512/512 - 0s - loss: 5.0950e-05 - val_loss: 4.9833e-07
Epoch 169/512
512/512 - 0s - loss: 4.9786e-05 - val_loss: 4.8511e-07
Epoch 170/512
512/512 - 0s - loss: 4.8758e-05 - val_loss: 4.6320e-07
Epoch 171/512
512/512 - 0s - loss: 4.6908e-05 - val_loss: 4.4406e-07
Epoch 172/512
512/512 - 0s - loss: 4.5481e-05 - val_loss: 4.3601e-07
Epoch 173/512
512/512 - 0s - loss: 4.4582e-05 - val_loss: 4.3176e-07
Epoch 174/512
512/512 - 0s - loss: 4.3844e-05 - val_loss: 4.1803e-07
Epoch 175/512
512/512 - 0s - loss: 4.2469e-05 - val_loss: 3.9603e-07
Epoch 176/512
512/512 - 0s - loss: 4.0417e-05 - val_loss: 3.9147e-07
Epoch 177/512
512/512 - 0s - loss: 3.9885e-05 - val_loss: 3.9306e-07
Epoch 178/512
512/512 - 0s - loss: 4.0048e-05 - val_loss: 3.6519e-07
Epoch 179/512
512/512 - 0s - loss: 3.6942e-05 - val_loss: 3.4865e-07
Epoch 180/512
512/512 - 0s - loss: 3.6201e-05 - val_loss: 3.5778e-07
Epoch 181/512
512/512 - 0s - loss: 3.6534e-05 - val_loss: 3.4944e-07
Epoch 182/512
512/512 - 0s - loss: 3.4675e-05 - val_loss: 3.2914e-07
Epoch 183/512
512/512 - 0s - loss: 3.3199e-05 - val_loss: 3.2426e-07
Epoch 184/512
512/512 - 0s - loss: 3.3295e-05 - val_loss: 3.1290e-07
Epoch 185/512
512/512 - 0s - loss: 3.1565e-05 - val_loss: 2.9962e-07
Epoch 186/512
512/512 - 0s - loss: 3.0507e-05 - val_loss: 2.9643e-07
Epoch 187/512
512/512 - 0s - loss: 3.0495e-05 - val_loss: 2.8490e-07
Epoch 188/512
512/512 - 0s - loss: 2.8743e-05 - val_loss: 2.7103e-07
Epoch 189/512
512/512 - 0s - loss: 2.7865e-05 - val_loss: 2.6978e-07
Epoch 190/512
512/512 - 0s - loss: 2.7721e-05 - val_loss: 2.6027e-07
Epoch 191/512
512/512 - 0s - loss: 2.6474e-05 - val_loss: 2.4741e-07
Epoch 192/512
512/512 - 0s - loss: 2.5019e-05 - val_loss: 2.4775e-07
Epoch 193/512
512/512 - 0s - loss: 2.5468e-05 - val_loss: 2.4075e-07
Epoch 194/512
512/512 - 0s - loss: 2.4268e-05 - val_loss: 2.2163e-07
Epoch 195/512
512/512 - 0s - loss: 2.2777e-05 - val_loss: 2.1605e-07
Epoch 196/512
512/512 - 0s - loss: 2.2654e-05 - val_loss: 2.1825e-07
Epoch 197/512
512/512 - 0s - loss: 2.2072e-05 - val_loss: 2.1006e-07
Epoch 198/512
512/512 - 0s - loss: 2.1216e-05 - val_loss: 1.9981e-07
Epoch 199/512
512/512 - 0s - loss: 2.0596e-05 - val_loss: 1.8776e-07
Epoch 200/512
512/512 - 0s - loss: 1.9504e-05 - val_loss: 1.8255e-07
Epoch 201/512
512/512 - 0s - loss: 1.8944e-05 - val_loss: 1.8614e-07
Epoch 202/512
512/512 - 0s - loss: 1.9053e-05 - val_loss: 1.7823e-07
Epoch 203/512
512/512 - 0s - loss: 1.7958e-05 - val_loss: 1.6425e-07
Epoch 204/512
512/512 - 0s - loss: 1.6897e-05 - val_loss: 1.6260e-07
Epoch 205/512
512/512 - 0s - loss: 1.6952e-05 - val_loss: 1.6060e-07
Epoch 206/512
512/512 - 0s - loss: 1.6242e-05 - val_loss: 1.5216e-07
Epoch 207/512
512/512 - 0s - loss: 1.5504e-05 - val_loss: 1.4716e-07
Epoch 208/512
512/512 - 0s - loss: 1.5165e-05 - val_loss: 1.4241e-07
Epoch 209/512
512/512 - 0s - loss: 1.4488e-05 - val_loss: 1.3902e-07
Epoch 210/512
512/512 - 0s - loss: 1.4223e-05 - val_loss: 1.3214e-07
Epoch 211/512
512/512 - 0s - loss: 1.3520e-05 - val_loss: 1.2568e-07
Epoch 212/512
512/512 - 0s - loss: 1.2953e-05 - val_loss: 1.2369e-07
Epoch 213/512
512/512 - 0s - loss: 1.2798e-05 - val_loss: 1.2013e-07
Epoch 214/512
512/512 - 0s - loss: 1.2305e-05 - val_loss: 1.1191e-07
Epoch 215/512
512/512 - 0s - loss: 1.1520e-05 - val_loss: 1.0907e-07
Epoch 216/512
512/512 - 0s - loss: 1.1395e-05 - val_loss: 1.0842e-07
Epoch 217/512
512/512 - 0s - loss: 1.1023e-05 - val_loss: 1.0287e-07
Epoch 218/512
512/512 - 0s - loss: 1.0523e-05 - val_loss: 9.7360e-08
Epoch 219/512
512/512 - 0s - loss: 1.0081e-05 - val_loss: 9.3042e-08
Epoch 220/512
512/512 - 0s - loss: 9.6061e-06 - val_loss: 9.4147e-08
Epoch 221/512
512/512 - 0s - loss: 9.5818e-06 - val_loss: 9.1977e-08
Epoch 222/512
512/512 - 0s - loss: 9.2144e-06 - val_loss: 8.3596e-08
Epoch 223/512
512/512 - 0s - loss: 8.4881e-06 - val_loss: 8.1014e-08
Epoch 224/512
512/512 - 0s - loss: 8.4493e-06 - val_loss: 8.0569e-08
Epoch 225/512
512/512 - 0s - loss: 8.3213e-06 - val_loss: 7.3427e-08
Epoch 226/512
512/512 - 0s - loss: 7.5086e-06 - val_loss: 7.0809e-08
Epoch 227/512
512/512 - 0s - loss: 7.4549e-06 - val_loss: 7.1723e-08
Epoch 228/512
512/512 - 0s - loss: 7.3948e-06 - val_loss: 6.7586e-08
Epoch 229/512
512/512 - 0s - loss: 6.8650e-06 - val_loss: 6.3103e-08
Epoch 230/512
512/512 - 0s - loss: 6.5345e-06 - val_loss: 6.2566e-08
Epoch 231/512
512/512 - 0s - loss: 6.5441e-06 - val_loss: 5.9825e-08
Epoch 232/512
512/512 - 0s - loss: 6.0949e-06 - val_loss: 5.6271e-08
Epoch 233/512
512/512 - 0s - loss: 5.9211e-06 - val_loss: 5.3993e-08
Epoch 234/512
512/512 - 0s - loss: 5.6036e-06 - val_loss: 5.2960e-08
Epoch 235/512
512/512 - 0s - loss: 5.4664e-06 - val_loss: 5.1982e-08
Epoch 236/512
512/512 - 0s - loss: 5.3525e-06 - val_loss: 4.8005e-08
Epoch 237/512
512/512 - 0s - loss: 4.9194e-06 - val_loss: 4.5929e-08
Epoch 238/512
512/512 - 0s - loss: 4.7947e-06 - val_loss: 4.5821e-08
Epoch 239/512
512/512 - 0s - loss: 4.7578e-06 - val_loss: 4.3189e-08
Epoch 240/512
512/512 - 0s - loss: 4.4034e-06 - val_loss: 3.9949e-08
Epoch 241/512
512/512 - 0s - loss: 4.1733e-06 - val_loss: 3.9471e-08
Epoch 242/512
512/512 - 0s - loss: 4.1445e-06 - val_loss: 3.8956e-08
Epoch 243/512
512/512 - 0s - loss: 3.9855e-06 - val_loss: 3.6245e-08
Epoch 244/512
512/512 - 0s - loss: 3.7178e-06 - val_loss: 3.4225e-08
Epoch 245/512
512/512 - 0s - loss: 3.5828e-06 - val_loss: 3.3673e-08
Epoch 246/512
512/512 - 0s - loss: 3.4842e-06 - val_loss: 3.2434e-08
Epoch 247/512
512/512 - 0s - loss: 3.3415e-06 - val_loss: 3.0627e-08
Epoch 248/512
512/512 - 0s - loss: 3.1244e-06 - val_loss: 2.9561e-08
Epoch 249/512
512/512 - 0s - loss: 3.0830e-06 - val_loss: 2.8531e-08
Epoch 250/512
512/512 - 0s - loss: 2.9407e-06 - val_loss: 2.6878e-08
Epoch 251/512
512/512 - 0s - loss: 2.7852e-06 - val_loss: 2.5294e-08
Epoch 252/512
512/512 - 0s - loss: 2.6305e-06 - val_loss: 2.4897e-08
Epoch 253/512
512/512 - 0s - loss: 2.6295e-06 - val_loss: 2.3763e-08
Epoch 254/512
512/512 - 0s - loss: 2.4175e-06 - val_loss: 2.2509e-08
Epoch 255/512
512/512 - 0s - loss: 2.3517e-06 - val_loss: 2.1994e-08
Epoch 256/512
512/512 - 0s - loss: 2.2802e-06 - val_loss: 2.0690e-08
Epoch 257/512
512/512 - 0s - loss: 2.1516e-06 - val_loss: 1.9259e-08
Epoch 258/512
512/512 - 0s - loss: 1.9914e-06 - val_loss: 1.9407e-08
Epoch 259/512
512/512 - 0s - loss: 2.0512e-06 - val_loss: 1.8665e-08
Epoch 260/512
512/512 - 0s - loss: 1.8844e-06 - val_loss: 1.7014e-08
Epoch 261/512
512/512 - 0s - loss: 1.7628e-06 - val_loss: 1.6459e-08
Epoch 262/512
512/512 - 0s - loss: 1.7550e-06 - val_loss: 1.6059e-08
Epoch 263/512
512/512 - 0s - loss: 1.6532e-06 - val_loss: 1.5053e-08
Epoch 264/512
512/512 - 0s - loss: 1.5789e-06 - val_loss: 1.4089e-08
Epoch 265/512
512/512 - 0s - loss: 1.4787e-06 - val_loss: 1.3990e-08
Epoch 266/512
512/512 - 0s - loss: 1.4614e-06 - val_loss: 1.3750e-08
Epoch 267/512
512/512 - 0s - loss: 1.4138e-06 - val_loss: 1.2302e-08
Epoch 268/512
512/512 - 0s - loss: 1.2689e-06 - val_loss: 1.1757e-08
Epoch 269/512
512/512 - 0s - loss: 1.2464e-06 - val_loss: 1.2186e-08
Epoch 270/512
512/512 - 0s - loss: 1.2628e-06 - val_loss: 1.1150e-08
Epoch 271/512
512/512 - 0s - loss: 1.1230e-06 - val_loss: 1.0135e-08
Epoch 272/512
512/512 - 0s - loss: 1.0742e-06 - val_loss: 1.0157e-08
Epoch 273/512
512/512 - 0s - loss: 1.0719e-06 - val_loss: 9.8645e-09
Epoch 274/512
512/512 - 0s - loss: 1.0146e-06 - val_loss: 8.9604e-09
Epoch 275/512
512/512 - 0s - loss: 9.2911e-07 - val_loss: 8.5132e-09
Epoch 276/512
512/512 - 0s - loss: 8.9988e-07 - val_loss: 8.7522e-09
Epoch 277/512
512/512 - 0s - loss: 9.1424e-07 - val_loss: 8.0955e-09
Epoch 278/512
512/512 - 0s - loss: 8.1444e-07 - val_loss: 7.3512e-09
Epoch 279/512
512/512 - 0s - loss: 7.8456e-07 - val_loss: 7.2188e-09
Epoch 280/512
512/512 - 0s - loss: 7.6254e-07 - val_loss: 7.0525e-09
Epoch 281/512
512/512 - 0s - loss: 7.2603e-07 - val_loss: 6.6546e-09
Epoch 282/512
512/512 - 0s - loss: 6.8888e-07 - val_loss: 6.2273e-09
Epoch 283/512
512/512 - 0s - loss: 6.5491e-07 - val_loss: 5.9668e-09
Epoch 284/512
512/512 - 0s - loss: 6.2270e-07 - val_loss: 5.8196e-09
Epoch 285/512
512/512 - 0s - loss: 6.0613e-07 - val_loss: 5.5223e-09
Epoch 286/512
512/512 - 0s - loss: 5.7160e-07 - val_loss: 5.1785e-09
Epoch 287/512
512/512 - 0s - loss: 5.3659e-07 - val_loss: 5.0194e-09
Epoch 288/512
512/512 - 0s - loss: 5.2608e-07 - val_loss: 4.7488e-09
Epoch 289/512
512/512 - 0s - loss: 4.9745e-07 - val_loss: 4.3992e-09
Epoch 290/512
512/512 - 0s - loss: 4.6245e-07 - val_loss: 4.2760e-09
Epoch 291/512
512/512 - 0s - loss: 4.5538e-07 - val_loss: 4.1185e-09
Epoch 292/512
512/512 - 0s - loss: 4.2969e-07 - val_loss: 3.8715e-09
Epoch 293/512
512/512 - 0s - loss: 4.0417e-07 - val_loss: 3.6952e-09
Epoch 294/512
512/512 - 0s - loss: 3.9246e-07 - val_loss: 3.5457e-09
Epoch 295/512
512/512 - 0s - loss: 3.7220e-07 - val_loss: 3.3345e-09
Epoch 296/512
512/512 - 0s - loss: 3.4894e-07 - val_loss: 3.1862e-09
Epoch 297/512
512/512 - 0s - loss: 3.3639e-07 - val_loss: 3.1236e-09
Epoch 298/512
512/512 - 0s - loss: 3.2557e-07 - val_loss: 2.9306e-09
Epoch 299/512
512/512 - 0s - loss: 3.0231e-07 - val_loss: 2.7787e-09
Epoch 300/512
512/512 - 0s - loss: 2.9140e-07 - val_loss: 2.6917e-09
Epoch 301/512
512/512 - 0s - loss: 2.8050e-07 - val_loss: 2.5239e-09
Epoch 302/512
512/512 - 0s - loss: 2.6297e-07 - val_loss: 2.3735e-09
Epoch 303/512
512/512 - 0s - loss: 2.4828e-07 - val_loss: 2.3088e-09
Epoch 304/512
512/512 - 0s - loss: 2.4170e-07 - val_loss: 2.2272e-09
Epoch 305/512
512/512 - 0s - loss: 2.3145e-07 - val_loss: 2.0407e-09
Epoch 306/512
512/512 - 0s - loss: 2.1172e-07 - val_loss: 1.9701e-09
Epoch 307/512
512/512 - 0s - loss: 2.0969e-07 - val_loss: 1.9071e-09
Epoch 308/512
512/512 - 0s - loss: 1.9890e-07 - val_loss: 1.7462e-09
Epoch 309/512
512/512 - 0s - loss: 1.8235e-07 - val_loss: 1.7009e-09
Epoch 310/512
512/512 - 0s - loss: 1.8135e-07 - val_loss: 1.6532e-09
Epoch 311/512
512/512 - 0s - loss: 1.7336e-07 - val_loss: 1.4809e-09
Epoch 312/512
512/512 - 0s - loss: 1.5558e-07 - val_loss: 1.4348e-09
Epoch 313/512
512/512 - 0s - loss: 1.5407e-07 - val_loss: 1.4531e-09
Epoch 314/512
512/512 - 0s - loss: 1.5398e-07 - val_loss: 1.2869e-09
Epoch 315/512
512/512 - 0s - loss: 1.3226e-07 - val_loss: 1.2109e-09
Epoch 316/512
512/512 - 0s - loss: 1.3078e-07 - val_loss: 1.2561e-09
Epoch 317/512
512/512 - 0s - loss: 1.3326e-07 - val_loss: 1.1605e-09
Epoch 318/512
512/512 - 0s - loss: 1.1831e-07 - val_loss: 1.0286e-09
Epoch 319/512
512/512 - 0s - loss: 1.0895e-07 - val_loss: 1.0488e-09
Epoch 320/512
512/512 - 0s - loss: 1.1359e-07 - val_loss: 1.0230e-09
Epoch 321/512
512/512 - 0s - loss: 1.0448e-07 - val_loss: 9.1721e-10
Epoch 322/512
512/512 - 0s - loss: 9.5135e-08 - val_loss: 8.8787e-10
Epoch 323/512
512/512 - 0s - loss: 9.4126e-08 - val_loss: 8.7961e-10
Epoch 324/512
512/512 - 0s - loss: 9.2094e-08 - val_loss: 8.0076e-10
Epoch 325/512
512/512 - 0s - loss: 8.3149e-08 - val_loss: 7.3886e-10
Epoch 326/512
512/512 - 0s - loss: 7.9723e-08 - val_loss: 7.2685e-10
Epoch 327/512
512/512 - 0s - loss: 7.6666e-08 - val_loss: 7.1347e-10
Epoch 328/512
512/512 - 0s - loss: 7.4439e-08 - val_loss: 6.6868e-10
Epoch 329/512
512/512 - 0s - loss: 6.9705e-08 - val_loss: 6.1639e-10
Epoch 330/512
512/512 - 0s - loss: 6.4891e-08 - val_loss: 5.8632e-10
Epoch 331/512
512/512 - 0s - loss: 6.2705e-08 - val_loss: 5.7801e-10
Epoch 332/512
512/512 - 0s - loss: 6.0621e-08 - val_loss: 5.5498e-10
Epoch 333/512
512/512 - 0s - loss: 5.7623e-08 - val_loss: 5.0638e-10
Epoch 334/512
512/512 - 0s - loss: 5.3020e-08 - val_loss: 4.7851e-10
Epoch 335/512
512/512 - 0s - loss: 5.1387e-08 - val_loss: 4.7141e-10
Epoch 336/512
512/512 - 0s - loss: 4.9561e-08 - val_loss: 4.4869e-10
Epoch 337/512
512/512 - 0s - loss: 4.7096e-08 - val_loss: 4.1599e-10
Epoch 338/512
512/512 - 0s - loss: 4.3289e-08 - val_loss: 4.0100e-10
Epoch 339/512
512/512 - 0s - loss: 4.2713e-08 - val_loss: 3.9073e-10
Epoch 340/512
512/512 - 0s - loss: 4.1060e-08 - val_loss: 3.5948e-10
Epoch 341/512
512/512 - 0s - loss: 3.7448e-08 - val_loss: 3.3587e-10
Epoch 342/512
512/512 - 0s - loss: 3.5595e-08 - val_loss: 3.3692e-10
Epoch 343/512
512/512 - 0s - loss: 3.5467e-08 - val_loss: 3.2324e-10
Epoch 344/512
512/512 - 0s - loss: 3.3288e-08 - val_loss: 2.9532e-10
Epoch 345/512
512/512 - 0s - loss: 3.0744e-08 - val_loss: 2.7970e-10
Epoch 346/512
512/512 - 0s - loss: 2.9649e-08 - val_loss: 2.7524e-10
Epoch 347/512
512/512 - 0s - loss: 2.9037e-08 - val_loss: 2.5813e-10
Epoch 348/512
512/512 - 0s - loss: 2.6804e-08 - val_loss: 2.4127e-10
Epoch 349/512
512/512 - 0s - loss: 2.5501e-08 - val_loss: 2.3274e-10
Epoch 350/512
512/512 - 0s - loss: 2.4498e-08 - val_loss: 2.2751e-10
Epoch 351/512
512/512 - 0s - loss: 2.3592e-08 - val_loss: 2.1437e-10
Epoch 352/512
512/512 - 0s - loss: 2.2296e-08 - val_loss: 2.0007e-10
Epoch 353/512
512/512 - 0s - loss: 2.0885e-08 - val_loss: 1.9215e-10
Epoch 354/512
512/512 - 0s - loss: 2.0165e-08 - val_loss: 1.8552e-10
Epoch 355/512
512/512 - 0s - loss: 1.9441e-08 - val_loss: 1.7459e-10
Epoch 356/512
512/512 - 0s - loss: 1.8205e-08 - val_loss: 1.6406e-10
Epoch 357/512
512/512 - 0s - loss: 1.7194e-08 - val_loss: 1.5859e-10
Epoch 358/512
512/512 - 0s - loss: 1.6686e-08 - val_loss: 1.5260e-10
Epoch 359/512
512/512 - 0s - loss: 1.5888e-08 - val_loss: 1.4456e-10
Epoch 360/512
512/512 - 0s - loss: 1.5031e-08 - val_loss: 1.3861e-10
Epoch 361/512
512/512 - 0s - loss: 1.4548e-08 - val_loss: 1.2919e-10
Epoch 362/512
512/512 - 0s - loss: 1.3469e-08 - val_loss: 1.2378e-10
Epoch 363/512
512/512 - 0s - loss: 1.2889e-08 - val_loss: 1.2257e-10
Epoch 364/512
512/512 - 0s - loss: 1.2810e-08 - val_loss: 1.1679e-10
Epoch 365/512
512/512 - 0s - loss: 1.1994e-08 - val_loss: 1.0674e-10
Epoch 366/512
512/512 - 0s - loss: 1.1266e-08 - val_loss: 1.0020e-10
Epoch 367/512
512/512 - 0s - loss: 1.0549e-08 - val_loss: 9.8220e-11
Epoch 368/512
512/512 - 0s - loss: 1.0339e-08 - val_loss: 9.7757e-11
Epoch 369/512
512/512 - 0s - loss: 1.0112e-08 - val_loss: 9.2777e-11
Epoch 370/512
512/512 - 0s - loss: 9.5172e-09 - val_loss: 8.5069e-11
Epoch 371/512
512/512 - 0s - loss: 8.8894e-09 - val_loss: 7.9335e-11
Epoch 372/512
512/512 - 0s - loss: 8.2552e-09 - val_loss: 7.8960e-11
Epoch 373/512
512/512 - 0s - loss: 8.2647e-09 - val_loss: 7.9236e-11
Epoch 374/512
512/512 - 0s - loss: 8.2124e-09 - val_loss: 7.4115e-11
Epoch 375/512
512/512 - 0s - loss: 7.5009e-09 - val_loss: 6.6591e-11
Epoch 376/512
512/512 - 0s - loss: 6.8822e-09 - val_loss: 6.3804e-11
Epoch 377/512
512/512 - 0s - loss: 6.7408e-09 - val_loss: 6.4356e-11
Epoch 378/512
512/512 - 0s - loss: 6.6964e-09 - val_loss: 6.3056e-11
Epoch 379/512
512/512 - 0s - loss: 6.5081e-09 - val_loss: 5.8447e-11
Epoch 380/512
512/512 - 0s - loss: 5.9648e-09 - val_loss: 5.3657e-11
Epoch 381/512
512/512 - 0s - loss: 5.5665e-09 - val_loss: 5.1919e-11
Epoch 382/512
512/512 - 0s - loss: 5.4861e-09 - val_loss: 5.1572e-11
Epoch 383/512
512/512 - 0s - loss: 5.3934e-09 - val_loss: 4.9613e-11
Epoch 384/512
512/512 - 0s - loss: 5.1000e-09 - val_loss: 4.6127e-11
Epoch 385/512
512/512 - 0s - loss: 4.7559e-09 - val_loss: 4.4397e-11
Epoch 386/512
512/512 - 0s - loss: 4.6264e-09 - val_loss: 4.3536e-11
Epoch 387/512
512/512 - 0s - loss: 4.4883e-09 - val_loss: 4.1802e-11
Epoch 388/512
512/512 - 0s - loss: 4.3087e-09 - val_loss: 3.9921e-11
Epoch 389/512
512/512 - 0s - loss: 4.1307e-09 - val_loss: 3.7699e-11
Epoch 390/512
512/512 - 0s - loss: 3.9031e-09 - val_loss: 3.6169e-11
Epoch 391/512
512/512 - 0s - loss: 3.7876e-09 - val_loss: 3.5248e-11
Epoch 392/512
512/512 - 0s - loss: 3.6237e-09 - val_loss: 3.3865e-11
Epoch 393/512
512/512 - 0s - loss: 3.5121e-09 - val_loss: 3.3086e-11
Epoch 394/512
512/512 - 0s - loss: 3.3973e-09 - val_loss: 3.1553e-11
Epoch 395/512
512/512 - 0s - loss: 3.2269e-09 - val_loss: 2.9811e-11
Epoch 396/512
512/512 - 0s - loss: 3.0712e-09 - val_loss: 2.8399e-11
Epoch 397/512
512/512 - 0s - loss: 2.9222e-09 - val_loss: 2.7998e-11
Epoch 398/512
512/512 - 0s - loss: 2.9018e-09 - val_loss: 2.7252e-11
Epoch 399/512
512/512 - 0s - loss: 2.7942e-09 - val_loss: 2.6373e-11
Epoch 400/512
512/512 - 0s - loss: 2.6959e-09 - val_loss: 2.4974e-11
Epoch 401/512
512/512 - 0s - loss: 2.5714e-09 - val_loss: 2.4013e-11
Epoch 402/512
512/512 - 0s - loss: 2.4683e-09 - val_loss: 2.2889e-11
Epoch 403/512
512/512 - 0s - loss: 2.3518e-09 - val_loss: 2.2399e-11
Epoch 404/512
512/512 - 0s - loss: 2.3079e-09 - val_loss: 2.1966e-11
Epoch 405/512
512/512 - 0s - loss: 2.2694e-09 - val_loss: 2.0933e-11
Epoch 406/512
512/512 - 0s - loss: 2.1422e-09 - val_loss: 1.9999e-11
Epoch 407/512
512/512 - 0s - loss: 2.0484e-09 - val_loss: 1.9578e-11
Epoch 408/512
512/512 - 0s - loss: 1.9841e-09 - val_loss: 1.9226e-11
Epoch 409/512
512/512 - 0s - loss: 1.9715e-09 - val_loss: 1.8411e-11
Epoch 410/512
512/512 - 0s - loss: 1.8700e-09 - val_loss: 1.7645e-11
Epoch 411/512
512/512 - 0s - loss: 1.7865e-09 - val_loss: 1.6938e-11
Epoch 412/512
512/512 - 0s - loss: 1.7403e-09 - val_loss: 1.6536e-11
Epoch 413/512
512/512 - 0s - loss: 1.6972e-09 - val_loss: 1.6010e-11
Epoch 414/512
512/512 - 0s - loss: 1.6228e-09 - val_loss: 1.5529e-11
Epoch 415/512
512/512 - 0s - loss: 1.6179e-09 - val_loss: 1.4646e-11
Epoch 416/512
512/512 - 0s - loss: 1.4915e-09 - val_loss: 1.3962e-11
Epoch 417/512
512/512 - 0s - loss: 1.4153e-09 - val_loss: 1.4145e-11
Epoch 418/512
512/512 - 0s - loss: 1.4624e-09 - val_loss: 1.4167e-11
Epoch 419/512
512/512 - 0s - loss: 1.4391e-09 - val_loss: 1.3450e-11
Epoch 420/512
512/512 - 0s - loss: 1.3619e-09 - val_loss: 1.2721e-11
Epoch 421/512
512/512 - 0s - loss: 1.2706e-09 - val_loss: 1.2162e-11
Epoch 422/512
512/512 - 0s - loss: 1.2443e-09 - val_loss: 1.2132e-11
Epoch 423/512
512/512 - 0s - loss: 1.2314e-09 - val_loss: 1.1886e-11
Epoch 424/512
512/512 - 0s - loss: 1.1976e-09 - val_loss: 1.1580e-11
Epoch 425/512
512/512 - 0s - loss: 1.1663e-09 - val_loss: 1.1082e-11
Epoch 426/512
512/512 - 0s - loss: 1.1369e-09 - val_loss: 1.0718e-11
Epoch 427/512
512/512 - 0s - loss: 1.1097e-09 - val_loss: 1.0474e-11
Epoch 428/512
512/512 - 0s - loss: 1.0395e-09 - val_loss: 1.0048e-11
Epoch 429/512
512/512 - 0s - loss: 1.0323e-09 - val_loss: 9.6772e-12
Epoch 430/512
512/512 - 0s - loss: 9.9907e-10 - val_loss: 9.5593e-12
Epoch 431/512
512/512 - 0s - loss: 9.7346e-10 - val_loss: 9.3566e-12
Epoch 432/512
512/512 - 0s - loss: 9.5245e-10 - val_loss: 9.0177e-12
Epoch 433/512
512/512 - 0s - loss: 9.1937e-10 - val_loss: 8.7721e-12
Epoch 434/512
512/512 - 0s - loss: 8.9510e-10 - val_loss: 8.3460e-12
Epoch 435/512
512/512 - 0s - loss: 8.5042e-10 - val_loss: 8.2320e-12
Epoch 436/512
512/512 - 0s - loss: 8.3975e-10 - val_loss: 8.1587e-12
Epoch 437/512
512/512 - 0s - loss: 8.3142e-10 - val_loss: 8.1030e-12
Epoch 438/512
512/512 - 0s - loss: 8.1068e-10 - val_loss: 7.9771e-12
Epoch 439/512
512/512 - 0s - loss: 8.0416e-10 - val_loss: 7.7592e-12
Epoch 440/512
512/512 - 0s - loss: 7.8078e-10 - val_loss: 7.2533e-12
Epoch 441/512
512/512 - 0s - loss: 7.4245e-10 - val_loss: 6.9367e-12
Epoch 442/512
512/512 - 0s - loss: 6.9689e-10 - val_loss: 6.8200e-12
Epoch 443/512
512/512 - 0s - loss: 7.0578e-10 - val_loss: 6.8040e-12
Epoch 444/512
512/512 - 0s - loss: 6.9883e-10 - val_loss: 6.6684e-12
Epoch 445/512
512/512 - 0s - loss: 6.7763e-10 - val_loss: 6.3290e-12
Epoch 446/512
512/512 - 0s - loss: 6.5024e-10 - val_loss: 6.0771e-12
Epoch 447/512
512/512 - 0s - loss: 6.1302e-10 - val_loss: 5.9821e-12
Epoch 448/512
512/512 - 0s - loss: 6.0890e-10 - val_loss: 5.9509e-12
Epoch 449/512
512/512 - 0s - loss: 6.1171e-10 - val_loss: 5.9682e-12
Epoch 450/512
512/512 - 0s - loss: 6.0907e-10 - val_loss: 5.7359e-12
Epoch 451/512
512/512 - 0s - loss: 5.8611e-10 - val_loss: 5.5177e-12
Epoch 452/512
512/512 - 0s - loss: 5.6012e-10 - val_loss: 5.3848e-12
Epoch 453/512
512/512 - 0s - loss: 5.4321e-10 - val_loss: 5.1838e-12
Epoch 454/512
512/512 - 0s - loss: 5.2978e-10 - val_loss: 5.2231e-12
Epoch 455/512
512/512 - 0s - loss: 5.3287e-10 - val_loss: 5.0792e-12
Epoch 456/512
512/512 - 0s - loss: 5.1423e-10 - val_loss: 4.9618e-12
Epoch 457/512
512/512 - 0s - loss: 5.1379e-10 - val_loss: 4.9080e-12
Epoch 458/512
512/512 - 0s - loss: 5.0533e-10 - val_loss: 4.6952e-12
Epoch 459/512
512/512 - 0s - loss: 4.6932e-10 - val_loss: 4.3921e-12
Epoch 460/512
512/512 - 0s - loss: 4.4940e-10 - val_loss: 4.4296e-12
Epoch 461/512
512/512 - 0s - loss: 4.5022e-10 - val_loss: 4.4141e-12
Epoch 462/512
512/512 - 0s - loss: 4.4947e-10 - val_loss: 4.3763e-12
Epoch 463/512
512/512 - 0s - loss: 4.3705e-10 - val_loss: 4.3444e-12
Epoch 464/512
512/512 - 0s - loss: 4.5035e-10 - val_loss: 4.3503e-12
Epoch 465/512
512/512 - 0s - loss: 4.3773e-10 - val_loss: 4.1448e-12
Epoch 466/512
512/512 - 0s - loss: 4.2222e-10 - val_loss: 3.8618e-12
Epoch 467/512
512/512 - 0s - loss: 3.8644e-10 - val_loss: 3.6476e-12
Epoch 468/512
512/512 - 0s - loss: 3.7645e-10 - val_loss: 3.7723e-12
Epoch 469/512
512/512 - 0s - loss: 3.8324e-10 - val_loss: 3.8039e-12
Epoch 470/512
512/512 - 0s - loss: 3.8574e-10 - val_loss: 3.7269e-12
Epoch 471/512
512/512 - 0s - loss: 3.8353e-10 - val_loss: 3.6752e-12
Epoch 472/512
512/512 - 0s - loss: 3.6789e-10 - val_loss: 3.6427e-12
Epoch 473/512
512/512 - 0s - loss: 3.6327e-10 - val_loss: 3.4942e-12
Epoch 474/512
512/512 - 0s - loss: 3.4850e-10 - val_loss: 3.4322e-12
Epoch 475/512
512/512 - 0s - loss: 3.4878e-10 - val_loss: 3.3712e-12
Epoch 476/512
512/512 - 0s - loss: 3.3548e-10 - val_loss: 3.2524e-12
Epoch 477/512
512/512 - 0s - loss: 3.2614e-10 - val_loss: 3.1729e-12
Epoch 478/512
512/512 - 0s - loss: 3.2271e-10 - val_loss: 3.1525e-12
Epoch 479/512
512/512 - 0s - loss: 3.1709e-10 - val_loss: 3.0927e-12
Epoch 480/512
512/512 - 0s - loss: 3.1096e-10 - val_loss: 2.9845e-12
Epoch 481/512
512/512 - 0s - loss: 3.0193e-10 - val_loss: 2.9695e-12
Epoch 482/512
512/512 - 0s - loss: 2.9959e-10 - val_loss: 2.9429e-12
Epoch 483/512
512/512 - 0s - loss: 2.8891e-10 - val_loss: 2.8742e-12
Epoch 484/512
512/512 - 0s - loss: 2.8854e-10 - val_loss: 2.8266e-12
Epoch 485/512
512/512 - 0s - loss: 2.8516e-10 - val_loss: 2.8393e-12
Epoch 486/512
512/512 - 0s - loss: 2.8533e-10 - val_loss: 2.8514e-12
Epoch 487/512
512/512 - 0s - loss: 2.8773e-10 - val_loss: 2.7467e-12
Epoch 488/512
512/512 - 0s - loss: 2.7971e-10 - val_loss: 2.6259e-12
Epoch 489/512
512/512 - 0s - loss: 2.6214e-10 - val_loss: 2.5202e-12
Epoch 490/512
512/512 - 0s - loss: 2.5645e-10 - val_loss: 2.4771e-12
Epoch 491/512
512/512 - 0s - loss: 2.5185e-10 - val_loss: 2.4253e-12
Epoch 492/512
512/512 - 0s - loss: 2.5063e-10 - val_loss: 2.3829e-12
Epoch 493/512
512/512 - 0s - loss: 2.3526e-10 - val_loss: 2.2357e-12
Epoch 494/512
512/512 - 0s - loss: 2.2677e-10 - val_loss: 2.2449e-12
Epoch 495/512
512/512 - 0s - loss: 2.3115e-10 - val_loss: 2.3536e-12
Epoch 496/512
512/512 - 0s - loss: 2.3835e-10 - val_loss: 2.3705e-12
Epoch 497/512
512/512 - 0s - loss: 2.4110e-10 - val_loss: 2.3257e-12
Epoch 498/512
512/512 - 0s - loss: 2.3327e-10 - val_loss: 2.2753e-12
Epoch 499/512
512/512 - 0s - loss: 2.2876e-10 - val_loss: 2.1696e-12
Epoch 500/512
512/512 - 0s - loss: 2.1543e-10 - val_loss: 2.0068e-12
Epoch 501/512
512/512 - 0s - loss: 2.0084e-10 - val_loss: 1.9193e-12
Epoch 502/512
512/512 - 0s - loss: 1.9923e-10 - val_loss: 2.0065e-12
Epoch 503/512
512/512 - 0s - loss: 2.0801e-10 - val_loss: 2.0806e-12
Epoch 504/512
512/512 - 0s - loss: 2.1388e-10 - val_loss: 2.1129e-12
Epoch 505/512
512/512 - 0s - loss: 2.1607e-10 - val_loss: 2.1065e-12
Epoch 506/512
512/512 - 0s - loss: 2.0798e-10 - val_loss: 1.9780e-12
Epoch 507/512
512/512 - 0s - loss: 1.9399e-10 - val_loss: 1.7760e-12
Epoch 508/512
512/512 - 0s - loss: 1.7666e-10 - val_loss: 1.6861e-12
Epoch 509/512
512/512 - 0s - loss: 1.7085e-10 - val_loss: 1.7014e-12
Epoch 510/512
512/512 - 0s - loss: 1.7214e-10 - val_loss: 1.7497e-12
Epoch 511/512
512/512 - 0s - loss: 1.8330e-10 - val_loss: 1.9150e-12
Epoch 512/512
512/512 - 0s - loss: 2.0256e-10 - val_loss: 1.9828e-12
2024-04-12 10:47:18.338335: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.6903e-10 - val_loss: 1.3476e-10
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2273e-10 - val_loss: 1.0480e-10
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.0154e-10 - val_loss: 9.6103e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.4658e-11 - val_loss: 9.0802e-11
Epoch 5/512

Epoch 00005: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1745e-11 - val_loss: 9.2837e-11
Epoch 6/512

Epoch 00006: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6848e-11 - val_loss: 1.0123e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0532e-10 - val_loss: 1.0968e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1299e-10 - val_loss: 1.1570e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1743e-10 - val_loss: 1.1657e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1790e-10 - val_loss: 1.1574e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1482e-10 - val_loss: 1.1010e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1035e-10 - val_loss: 1.0619e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0713e-10 - val_loss: 1.0328e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0431e-10 - val_loss: 1.0367e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0493e-10 - val_loss: 1.0342e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0337e-10 - val_loss: 1.0130e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0198e-10 - val_loss: 1.0019e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0120e-10 - val_loss: 9.9101e-11
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0037e-10 - val_loss: 9.9859e-11
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0103e-10 - val_loss: 1.0003e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9059e-11 - val_loss: 9.6097e-11
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6459e-11 - val_loss: 9.3661e-11
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4880e-11 - val_loss: 9.3939e-11
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4710e-11 - val_loss: 9.3478e-11
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5784e-11 - val_loss: 9.8404e-11
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0372e-10 - val_loss: 1.0872e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1206e-10 - val_loss: 1.1252e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1185e-10 - val_loss: 1.0787e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0719e-10 - val_loss: 1.0198e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.6691e-11 - val_loss: 8.6879e-11
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.4088e-11 - val_loss: 7.9836e-11
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.8167e-11 - val_loss: 7.5008e-11
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5650e-11 - val_loss: 7.5252e-11
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6190e-11 - val_loss: 7.5814e-11
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7969e-11 - val_loss: 7.9366e-11
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1623e-11 - val_loss: 8.2906e-11
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4074e-11 - val_loss: 8.3166e-11
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3170e-11 - val_loss: 8.0272e-11
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0456e-11 - val_loss: 7.7878e-11
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8110e-11 - val_loss: 7.6786e-11
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7253e-11 - val_loss: 7.5724e-11
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6837e-11 - val_loss: 7.5722e-11
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.6017e-11 - val_loss: 7.3517e-11
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4383e-11 - val_loss: 7.4713e-11
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5570e-11 - val_loss: 7.4638e-11
Epoch 46/512

Epoch 00046: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.4208e-11 - val_loss: 7.1845e-11
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.2159e-11 - val_loss: 7.1395e-11
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1867e-11 - val_loss: 7.1746e-11
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.2298e-11 - val_loss: 7.1080e-11
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2819e-11 - val_loss: 7.3010e-11
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.3416e-11 - val_loss: 7.1024e-11
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.1105e-11 - val_loss: 7.0196e-11
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1140e-11 - val_loss: 7.0214e-11
Epoch 54/512

Epoch 00054: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.0674e-11 - val_loss: 6.8188e-11
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.7589e-11 - val_loss: 6.4979e-11
Epoch 56/512

Epoch 00056: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.3948e-11 - val_loss: 6.1133e-11
Epoch 57/512

Epoch 00057: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.1796e-11 - val_loss: 6.1106e-11
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.8183e-11 - val_loss: 5.1497e-11
Epoch 59/512

Epoch 00059: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.9207e-11 - val_loss: 4.4686e-11
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3197e-11 - val_loss: 4.1697e-11
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2135e-11 - val_loss: 4.2199e-11
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3451e-11 - val_loss: 4.7031e-11
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1629e-11 - val_loss: 5.8576e-11
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3583e-11 - val_loss: 6.8488e-11
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1115e-11 - val_loss: 7.1844e-11
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2079e-11 - val_loss: 6.9718e-11
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8800e-11 - val_loss: 6.6396e-11
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5732e-11 - val_loss: 6.4512e-11
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4177e-11 - val_loss: 6.2346e-11
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2332e-11 - val_loss: 6.0752e-11
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0339e-11 - val_loss: 5.8021e-11
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8481e-11 - val_loss: 5.7667e-11
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8642e-11 - val_loss: 5.8180e-11
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7765e-11 - val_loss: 5.5932e-11
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6342e-11 - val_loss: 5.5879e-11
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5451e-11 - val_loss: 5.4918e-11
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5289e-11 - val_loss: 5.4545e-11
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4867e-11 - val_loss: 5.3868e-11
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4972e-11 - val_loss: 5.5626e-11
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6297e-11 - val_loss: 5.5919e-11
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6322e-11 - val_loss: 5.5801e-11
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5390e-11 - val_loss: 5.3660e-11
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3680e-11 - val_loss: 5.3752e-11
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4471e-11 - val_loss: 5.4143e-11
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4608e-11 - val_loss: 5.4060e-11
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4854e-11 - val_loss: 5.3219e-11
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1859e-11 - val_loss: 4.9792e-11
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8293e-11 - val_loss: 4.5023e-11
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4721e-11 - val_loss: 4.4016e-11
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4155e-11 - val_loss: 4.3434e-11
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3589e-11 - val_loss: 4.3064e-11
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3573e-11 - val_loss: 4.4584e-11
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5863e-11 - val_loss: 4.6139e-11
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6246e-11 - val_loss: 4.3264e-11
Epoch 95/512

Epoch 00095: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.2293e-11 - val_loss: 3.9287e-11
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.7498e-11 - val_loss: 3.4900e-11
Epoch 97/512

Epoch 00097: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4413e-11 - val_loss: 3.2804e-11
Epoch 98/512

Epoch 00098: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.2206e-11 - val_loss: 3.0322e-11
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0778e-11 - val_loss: 3.2357e-11
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3259e-11 - val_loss: 3.3963e-11
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5702e-11 - val_loss: 3.7883e-11
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9142e-11 - val_loss: 3.9531e-11
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0447e-11 - val_loss: 4.0511e-11
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0231e-11 - val_loss: 4.0646e-11
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2358e-11 - val_loss: 4.4583e-11
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6826e-11 - val_loss: 4.8757e-11
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9738e-11 - val_loss: 5.0391e-11
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0404e-11 - val_loss: 4.8484e-11
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8426e-11 - val_loss: 4.7516e-11
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7280e-11 - val_loss: 4.6097e-11
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5965e-11 - val_loss: 4.5230e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4731e-11 - val_loss: 4.2880e-11
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3465e-11 - val_loss: 4.3692e-11
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3878e-11 - val_loss: 4.3253e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3312e-11 - val_loss: 4.2617e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2753e-11 - val_loss: 4.2163e-11
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1899e-11 - val_loss: 4.1412e-11
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1920e-11 - val_loss: 4.1945e-11
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2003e-11 - val_loss: 4.1391e-11
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1401e-11 - val_loss: 4.0349e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0896e-11 - val_loss: 4.0863e-11
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9815e-11 - val_loss: 3.7950e-11
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7072e-11 - val_loss: 3.6091e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6546e-11 - val_loss: 3.5822e-11
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5563e-11 - val_loss: 3.4879e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5568e-11 - val_loss: 3.5531e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6249e-11 - val_loss: 3.5793e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5923e-11 - val_loss: 3.5711e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5927e-11 - val_loss: 3.5564e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5384e-11 - val_loss: 3.4869e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4751e-11 - val_loss: 3.4313e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3960e-11 - val_loss: 3.2115e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2280e-11 - val_loss: 3.2234e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2607e-11 - val_loss: 3.3180e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3192e-11 - val_loss: 3.2480e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2575e-11 - val_loss: 3.1832e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2713e-11 - val_loss: 3.4237e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5428e-11 - val_loss: 3.6565e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4866e-11 - val_loss: 3.1390e-11
Epoch 140/512

Epoch 00140: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.0336e-11 - val_loss: 2.8438e-11
Epoch 141/512

Epoch 00141: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7753e-11 - val_loss: 2.6699e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7544e-11 - val_loss: 2.8373e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8299e-11 - val_loss: 2.7286e-11
Epoch 144/512

Epoch 00144: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7094e-11 - val_loss: 2.6575e-11
Epoch 145/512

Epoch 00145: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6507e-11 - val_loss: 2.6466e-11
Epoch 146/512

Epoch 00146: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6633e-11 - val_loss: 2.6198e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6406e-11 - val_loss: 2.6518e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7207e-11 - val_loss: 2.7214e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7957e-11 - val_loss: 2.8606e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8277e-11 - val_loss: 2.7217e-11
Epoch 151/512

Epoch 00151: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7052e-11 - val_loss: 2.5749e-11
Epoch 152/512

Epoch 00152: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.5861e-11 - val_loss: 2.5216e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5806e-11 - val_loss: 2.6744e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7499e-11 - val_loss: 2.8122e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8255e-11 - val_loss: 2.8530e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7961e-11 - val_loss: 2.6922e-11
Epoch 157/512

Epoch 00157: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6768e-11 - val_loss: 2.5216e-11
Epoch 158/512

Epoch 00158: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.4883e-11 - val_loss: 2.4282e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5009e-11 - val_loss: 2.6825e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8343e-11 - val_loss: 3.0161e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1053e-11 - val_loss: 3.1513e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1502e-11 - val_loss: 3.1363e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1346e-11 - val_loss: 3.0481e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0018e-11 - val_loss: 2.9482e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9723e-11 - val_loss: 2.9162e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9387e-11 - val_loss: 2.9513e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9187e-11 - val_loss: 2.8377e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8464e-11 - val_loss: 2.8030e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8209e-11 - val_loss: 2.8313e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9035e-11 - val_loss: 2.9574e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9618e-11 - val_loss: 2.8770e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8535e-11 - val_loss: 2.7848e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7899e-11 - val_loss: 2.7442e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7523e-11 - val_loss: 2.6993e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7365e-11 - val_loss: 2.7304e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7699e-11 - val_loss: 2.7430e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7689e-11 - val_loss: 2.7908e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7730e-11 - val_loss: 2.6980e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7595e-11 - val_loss: 2.7859e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7529e-11 - val_loss: 2.7227e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8259e-11 - val_loss: 2.9315e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9775e-11 - val_loss: 3.0284e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0896e-11 - val_loss: 3.0984e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1260e-11 - val_loss: 3.0519e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0348e-11 - val_loss: 3.0332e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0033e-11 - val_loss: 2.9451e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9922e-11 - val_loss: 2.9749e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9513e-11 - val_loss: 2.9025e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9018e-11 - val_loss: 2.8879e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9567e-11 - val_loss: 2.9643e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9306e-11 - val_loss: 2.8430e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8900e-11 - val_loss: 2.9409e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8119e-11 - val_loss: 2.4713e-11
Epoch 194/512

Epoch 00194: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.3056e-11 - val_loss: 2.0667e-11
Epoch 195/512

Epoch 00195: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.0001e-11 - val_loss: 1.8790e-11
Epoch 196/512

Epoch 00196: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.8251e-11 - val_loss: 1.7263e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7488e-11 - val_loss: 1.7925e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8116e-11 - val_loss: 1.7488e-11
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7258e-11 - val_loss: 1.7120e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7456e-11 - val_loss: 1.7990e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8364e-11 - val_loss: 1.8878e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9607e-11 - val_loss: 2.0087e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0266e-11 - val_loss: 1.9419e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8833e-11 - val_loss: 1.7690e-11
Epoch 205/512

Epoch 00205: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.7022e-11 - val_loss: 1.5934e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.5656e-11 - val_loss: 1.5061e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4668e-11 - val_loss: 1.4275e-11
Epoch 208/512

Epoch 00208: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.4100e-11 - val_loss: 1.3383e-11
Epoch 209/512

Epoch 00209: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.3050e-11 - val_loss: 1.2620e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2888e-11 - val_loss: 1.3208e-11
Epoch 211/512

Epoch 00211: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2916e-11 - val_loss: 1.2589e-11
Epoch 212/512

Epoch 00212: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2439e-11 - val_loss: 1.2463e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2575e-11 - val_loss: 1.2748e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2912e-11 - val_loss: 1.2683e-11
Epoch 215/512

Epoch 00215: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.2509e-11 - val_loss: 1.2270e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2471e-11 - val_loss: 1.2826e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3345e-11 - val_loss: 1.4034e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3982e-11 - val_loss: 1.4207e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4437e-11 - val_loss: 1.4619e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5054e-11 - val_loss: 1.4912e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4874e-11 - val_loss: 1.5019e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4955e-11 - val_loss: 1.4441e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4389e-11 - val_loss: 1.3731e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3680e-11 - val_loss: 1.3286e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3064e-11 - val_loss: 1.2559e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2400e-11 - val_loss: 1.2582e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2852e-11 - val_loss: 1.2894e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2569e-11 - val_loss: 1.3031e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4225e-11 - val_loss: 1.5655e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6617e-11 - val_loss: 1.8103e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9151e-11 - val_loss: 2.0682e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1792e-11 - val_loss: 2.3326e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4745e-11 - val_loss: 2.6149e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7187e-11 - val_loss: 2.8097e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8888e-11 - val_loss: 2.9453e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9199e-11 - val_loss: 2.8357e-11
Epoch 237/512

Epoch 00237: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8407e-11 - val_loss: 2.7752e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7177e-11 - val_loss: 2.5778e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5645e-11 - val_loss: 2.5528e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5337e-11 - val_loss: 2.4952e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5109e-11 - val_loss: 2.4382e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3737e-11 - val_loss: 2.3001e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3269e-11 - val_loss: 2.3137e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3790e-11 - val_loss: 2.4483e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4482e-11 - val_loss: 2.4499e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4489e-11 - val_loss: 2.4111e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3969e-11 - val_loss: 2.4022e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4172e-11 - val_loss: 2.3745e-11
Epoch 249/512

Epoch 00249: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3756e-11 - val_loss: 2.3573e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3297e-11 - val_loss: 2.3040e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3187e-11 - val_loss: 2.3085e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3219e-11 - val_loss: 2.2781e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3326e-11 - val_loss: 2.4241e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5149e-11 - val_loss: 2.5536e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4467e-11 - val_loss: 2.1819e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0971e-11 - val_loss: 1.9625e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9334e-11 - val_loss: 1.8375e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8097e-11 - val_loss: 1.5743e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4333e-11 - val_loss: 1.2382e-11
Epoch 260/512

Epoch 00260: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 1.1639e-11 - val_loss: 1.0437e-11
Epoch 261/512

Epoch 00261: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 9.8407e-12 - val_loss: 9.1067e-12
Epoch 262/512

Epoch 00262: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.9028e-12 - val_loss: 8.5524e-12
Epoch 263/512

Epoch 00263: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6024e-12 - val_loss: 8.5894e-12
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.4467e-12 - val_loss: 8.0973e-12
Epoch 265/512

Epoch 00265: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1196e-12 - val_loss: 8.2611e-12
Epoch 266/512

Epoch 00266: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4019e-12 - val_loss: 8.5963e-12
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5863e-12 - val_loss: 8.7228e-12
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5711e-12 - val_loss: 8.2162e-12
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2127e-12 - val_loss: 8.5174e-12
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7161e-12 - val_loss: 8.7728e-12
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0406e-12 - val_loss: 9.1882e-12
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2951e-12 - val_loss: 9.3789e-12
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5447e-12 - val_loss: 9.5078e-12
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2728e-12 - val_loss: 9.4789e-12
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6518e-12 - val_loss: 9.7098e-12
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4667e-12 - val_loss: 8.8495e-12
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8190e-12 - val_loss: 8.7215e-12
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0909e-12 - val_loss: 1.0021e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0071e-11 - val_loss: 1.0018e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9803e-12 - val_loss: 9.9665e-12
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0155e-11 - val_loss: 1.0273e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0413e-11 - val_loss: 1.0339e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0128e-11 - val_loss: 9.6704e-12
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6369e-12 - val_loss: 9.3413e-12
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7362e-12 - val_loss: 9.9418e-12
Epoch 286/512

Epoch 00286: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8855e-12 - val_loss: 9.6398e-12
Epoch 287/512

Epoch 00287: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4563e-12 - val_loss: 9.2206e-12
Epoch 288/512

Epoch 00288: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3065e-12 - val_loss: 9.3096e-12
Epoch 289/512

Epoch 00289: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3573e-12 - val_loss: 9.4895e-12
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7402e-12 - val_loss: 1.0308e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0786e-11 - val_loss: 1.1249e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1454e-11 - val_loss: 1.1790e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2294e-11 - val_loss: 1.2828e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3334e-11 - val_loss: 1.3975e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4415e-11 - val_loss: 1.4734e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4762e-11 - val_loss: 1.4923e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4662e-11 - val_loss: 1.3658e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3781e-11 - val_loss: 1.3849e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3615e-11 - val_loss: 1.2878e-11
Epoch 300/512

Epoch 00300: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2517e-11 - val_loss: 1.2270e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1998e-11 - val_loss: 1.1713e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1736e-11 - val_loss: 1.1429e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1595e-11 - val_loss: 1.1922e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1892e-11 - val_loss: 1.1761e-11
Epoch 305/512

Epoch 00305: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1576e-11 - val_loss: 1.1138e-11
Epoch 306/512

Epoch 00306: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1066e-11 - val_loss: 1.0510e-11
Epoch 307/512

Epoch 00307: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0607e-11 - val_loss: 1.0733e-11
Epoch 308/512

Epoch 00308: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0936e-11 - val_loss: 1.1290e-11
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1634e-11 - val_loss: 1.2904e-11
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4561e-11 - val_loss: 1.6975e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8043e-11 - val_loss: 1.9210e-11
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0026e-11 - val_loss: 2.1640e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2449e-11 - val_loss: 2.3417e-11
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3658e-11 - val_loss: 2.2875e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1619e-11 - val_loss: 1.9984e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9667e-11 - val_loss: 1.8951e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8285e-11 - val_loss: 1.6694e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6141e-11 - val_loss: 1.5677e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5322e-11 - val_loss: 1.5156e-11
Epoch 320/512

Epoch 00320: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4930e-11 - val_loss: 1.4818e-11
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5200e-11 - val_loss: 1.5609e-11
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5638e-11 - val_loss: 1.5457e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5240e-11 - val_loss: 1.4409e-11
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4402e-11 - val_loss: 1.4414e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4621e-11 - val_loss: 1.4401e-11
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4607e-11 - val_loss: 1.4917e-11
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5437e-11 - val_loss: 1.5839e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6231e-11 - val_loss: 1.6312e-11
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6164e-11 - val_loss: 1.5864e-11
Epoch 330/512

Epoch 00330: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5372e-11 - val_loss: 1.4411e-11
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4699e-11 - val_loss: 1.4756e-11
Epoch 332/512

Epoch 00332: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4938e-11 - val_loss: 1.5286e-11
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5542e-11 - val_loss: 1.5114e-11
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5070e-11 - val_loss: 1.4383e-11
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4408e-11 - val_loss: 1.4753e-11
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5121e-11 - val_loss: 1.5091e-11
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5260e-11 - val_loss: 1.5088e-11
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5174e-11 - val_loss: 1.5475e-11
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5679e-11 - val_loss: 1.5146e-11
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5308e-11 - val_loss: 1.5608e-11
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5599e-11 - val_loss: 1.5383e-11
Epoch 342/512

Epoch 00342: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5204e-11 - val_loss: 1.5556e-11
Epoch 343/512

Epoch 00343: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5489e-11 - val_loss: 1.5531e-11
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5803e-11 - val_loss: 1.5723e-11
Epoch 345/512

Epoch 00345: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4042e-11 - val_loss: 1.1485e-11
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0758e-11 - val_loss: 9.4206e-12
Epoch 347/512

Epoch 00347: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 8.7887e-12 - val_loss: 7.9695e-12
Epoch 348/512

Epoch 00348: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 7.6979e-12 - val_loss: 7.0544e-12
Epoch 349/512

Epoch 00349: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.7982e-12 - val_loss: 6.2921e-12
Epoch 350/512

Epoch 00350: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.2404e-12 - val_loss: 6.1954e-12
Epoch 351/512

Epoch 00351: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 6.0452e-12 - val_loss: 5.7643e-12
Epoch 352/512

Epoch 00352: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.7660e-12 - val_loss: 5.7509e-12
Epoch 353/512

Epoch 00353: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.6053e-12 - val_loss: 5.3201e-12
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4580e-12 - val_loss: 5.8212e-12
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3200e-12 - val_loss: 6.9599e-12
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9964e-12 - val_loss: 7.0373e-12
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6648e-12 - val_loss: 8.3811e-12
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5301e-12 - val_loss: 8.7346e-12
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9454e-12 - val_loss: 9.2840e-12
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5507e-12 - val_loss: 9.4559e-12
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4708e-12 - val_loss: 9.9279e-12
Epoch 362/512

Epoch 00362: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0418e-11 - val_loss: 1.0835e-11
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1091e-11 - val_loss: 1.1293e-11
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1150e-11 - val_loss: 1.0920e-11
Epoch 365/512

Epoch 00365: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0570e-11 - val_loss: 1.0224e-11
Epoch 366/512

Epoch 00366: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0223e-11 - val_loss: 9.9207e-12
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8847e-12 - val_loss: 9.6668e-12
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9024e-12 - val_loss: 1.0133e-11
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9977e-12 - val_loss: 9.6208e-12
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6439e-12 - val_loss: 9.5682e-12
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7390e-12 - val_loss: 9.6683e-12
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4160e-12 - val_loss: 9.2458e-12
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4162e-12 - val_loss: 9.5654e-12
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4579e-12 - val_loss: 8.8865e-12
Epoch 375/512

Epoch 00375: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4288e-12 - val_loss: 7.4410e-12
Epoch 376/512

Epoch 00376: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0375e-12 - val_loss: 6.3191e-12
Epoch 377/512

Epoch 00377: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9457e-12 - val_loss: 5.6826e-12
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6967e-12 - val_loss: 5.6952e-12
Epoch 379/512

Epoch 00379: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.6504e-12 - val_loss: 5.3128e-12
Epoch 380/512

Epoch 00380: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.2796e-12 - val_loss: 5.2553e-12
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3589e-12 - val_loss: 5.3444e-12
Epoch 382/512

Epoch 00382: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.2824e-12 - val_loss: 5.0796e-12
Epoch 383/512

Epoch 00383: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0734e-12 - val_loss: 5.0301e-12
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 5.0187e-12 - val_loss: 4.8520e-12
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.8105e-12 - val_loss: 4.6286e-12
Epoch 386/512

Epoch 00386: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.5899e-12 - val_loss: 4.4087e-12
Epoch 387/512

Epoch 00387: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4029e-12 - val_loss: 4.3777e-12
Epoch 388/512

Epoch 00388: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.3974e-12 - val_loss: 4.3733e-12
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4674e-12 - val_loss: 4.7131e-12
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9436e-12 - val_loss: 5.0678e-12
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3240e-12 - val_loss: 5.5396e-12
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6592e-12 - val_loss: 5.7881e-12
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8196e-12 - val_loss: 5.7882e-12
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7798e-12 - val_loss: 5.5818e-12
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4320e-12 - val_loss: 5.2785e-12
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3768e-12 - val_loss: 5.3580e-12
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2664e-12 - val_loss: 5.0197e-12
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9627e-12 - val_loss: 4.8210e-12
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8311e-12 - val_loss: 4.8067e-12
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7953e-12 - val_loss: 4.6155e-12
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5140e-12 - val_loss: 4.5516e-12
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5635e-12 - val_loss: 4.4098e-12
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3968e-12 - val_loss: 4.4997e-12
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4975e-12 - val_loss: 4.3943e-12
Epoch 405/512

Epoch 00405: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.4503e-12 - val_loss: 4.2764e-12
Epoch 406/512

Epoch 00406: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 4.0952e-12 - val_loss: 3.9564e-12
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9635e-12 - val_loss: 3.9498e-12
Epoch 408/512

Epoch 00408: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0439e-12 - val_loss: 4.0308e-12
Epoch 409/512

Epoch 00409: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0001e-12 - val_loss: 3.9606e-12
Epoch 410/512

Epoch 00410: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.9031e-12 - val_loss: 3.6228e-12
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6748e-12 - val_loss: 3.5790e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6000e-12 - val_loss: 3.7188e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6308e-12 - val_loss: 3.6761e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7550e-12 - val_loss: 3.6020e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6549e-12 - val_loss: 3.8623e-12
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9980e-12 - val_loss: 4.3228e-12
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4645e-12 - val_loss: 4.6168e-12
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7193e-12 - val_loss: 4.8465e-12
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8679e-12 - val_loss: 5.1120e-12
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4943e-12 - val_loss: 5.5866e-12
Epoch 421/512

Epoch 00421: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4436e-12 - val_loss: 5.2982e-12
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3157e-12 - val_loss: 5.2763e-12
Epoch 423/512

Epoch 00423: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2975e-12 - val_loss: 5.4080e-12
Epoch 424/512

Epoch 00424: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1639e-12 - val_loss: 5.5599e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2640e-12 - val_loss: 7.6894e-12
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6063e-12 - val_loss: 9.9852e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1242e-11 - val_loss: 1.3036e-11
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3868e-11 - val_loss: 1.4902e-11
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5942e-11 - val_loss: 1.7123e-11
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7588e-11 - val_loss: 1.7955e-11
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8152e-11 - val_loss: 1.8520e-11
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8314e-11 - val_loss: 1.7811e-11
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7775e-11 - val_loss: 1.7408e-11
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7511e-11 - val_loss: 1.7256e-11
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7557e-11 - val_loss: 1.7681e-11
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7266e-11 - val_loss: 1.6518e-11
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6344e-11 - val_loss: 1.6552e-11
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6901e-11 - val_loss: 1.6510e-11
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6749e-11 - val_loss: 1.7053e-11
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7403e-11 - val_loss: 1.7266e-11
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7628e-11 - val_loss: 1.7422e-11
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7120e-11 - val_loss: 1.6729e-11
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6924e-11 - val_loss: 1.6229e-11
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6444e-11 - val_loss: 1.6390e-11
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6451e-11 - val_loss: 1.6060e-11
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5662e-11 - val_loss: 1.5713e-11
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5888e-11 - val_loss: 1.5375e-11
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5494e-11 - val_loss: 1.5294e-11
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5310e-11 - val_loss: 1.5219e-11
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5212e-11 - val_loss: 1.5145e-11
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5343e-11 - val_loss: 1.5265e-11
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4911e-11 - val_loss: 1.4774e-11
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4716e-11 - val_loss: 1.4776e-11
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4755e-11 - val_loss: 1.4023e-11
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3473e-11 - val_loss: 1.2558e-11
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2377e-11 - val_loss: 1.2057e-11
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0798e-11 - val_loss: 8.9166e-12
Epoch 458/512

Epoch 00458: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0180e-12 - val_loss: 6.2699e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8737e-12 - val_loss: 5.1971e-12
Epoch 460/512

Epoch 00460: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8206e-12 - val_loss: 4.4334e-12
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1739e-12 - val_loss: 3.7975e-12
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7951e-12 - val_loss: 3.7800e-12
Epoch 463/512

Epoch 00463: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6834e-12 - val_loss: 3.6127e-12
Epoch 464/512

Epoch 00464: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6271e-12 - val_loss: 3.6277e-12
Epoch 465/512

Epoch 00465: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.6187e-12 - val_loss: 3.4832e-12
Epoch 466/512

Epoch 00466: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.4764e-12 - val_loss: 3.4501e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4608e-12 - val_loss: 3.4663e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4775e-12 - val_loss: 3.6985e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9372e-12 - val_loss: 4.0817e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2027e-12 - val_loss: 4.1266e-12
Epoch 471/512

Epoch 00471: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9888e-12 - val_loss: 3.8841e-12
Epoch 472/512

Epoch 00472: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8868e-12 - val_loss: 3.8781e-12
Epoch 473/512

Epoch 00473: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8801e-12 - val_loss: 3.8733e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8877e-12 - val_loss: 3.8708e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9554e-12 - val_loss: 4.0677e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0830e-12 - val_loss: 4.2059e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2735e-12 - val_loss: 4.2859e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3811e-12 - val_loss: 4.3762e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1830e-12 - val_loss: 3.8989e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8804e-12 - val_loss: 3.8549e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8964e-12 - val_loss: 4.0384e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9513e-12 - val_loss: 3.8694e-12
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9487e-12 - val_loss: 3.9288e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8165e-12 - val_loss: 3.8322e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7455e-12 - val_loss: 3.6706e-12
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5623e-12 - val_loss: 3.4646e-12
Epoch 487/512

Epoch 00487: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 3.1816e-12 - val_loss: 2.8233e-12
Epoch 488/512

Epoch 00488: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.7681e-12 - val_loss: 2.7234e-12
Epoch 489/512

Epoch 00489: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/addition_weights.h5
512/512 - 0s - loss: 2.6059e-12 - val_loss: 2.4235e-12
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6661e-12 - val_loss: 2.8870e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9130e-12 - val_loss: 2.9167e-12
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9863e-12 - val_loss: 3.0958e-12
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1063e-12 - val_loss: 3.1110e-12
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1149e-12 - val_loss: 3.1025e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1113e-12 - val_loss: 3.1000e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1064e-12 - val_loss: 3.1042e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1755e-12 - val_loss: 3.3829e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2959e-12 - val_loss: 2.9722e-12
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0493e-12 - val_loss: 3.0889e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8943e-12 - val_loss: 2.7414e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8051e-12 - val_loss: 2.7991e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8463e-12 - val_loss: 2.7613e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8298e-12 - val_loss: 2.8608e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9025e-12 - val_loss: 2.8135e-12
Epoch 505/512

Epoch 00505: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7786e-12 - val_loss: 2.7360e-12
Epoch 506/512

Epoch 00506: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6412e-12 - val_loss: 2.5660e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5679e-12 - val_loss: 2.6619e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6531e-12 - val_loss: 2.5811e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5790e-12 - val_loss: 2.5679e-12
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7977e-12 - val_loss: 3.0754e-12
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1072e-12 - val_loss: 3.1052e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2667e-12 - val_loss: 3.1985e-12
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.1138 - val_loss: 0.0465
Epoch 2/512
512/512 - 0s - loss: 0.0403 - val_loss: 0.0178
Epoch 3/512
512/512 - 0s - loss: 0.0351 - val_loss: 0.0182
Epoch 4/512
512/512 - 0s - loss: 0.0293 - val_loss: 0.0072
Epoch 5/512
512/512 - 0s - loss: 0.0237 - val_loss: 4.1627e-04
Epoch 6/512
512/512 - 0s - loss: 0.0174 - val_loss: 0.0026
Epoch 7/512
512/512 - 0s - loss: 0.0129 - val_loss: 0.0050
Epoch 8/512
512/512 - 0s - loss: 0.0156 - val_loss: 0.0046
Epoch 9/512
512/512 - 0s - loss: 0.0122 - val_loss: 0.0046
Epoch 10/512
512/512 - 0s - loss: 0.0115 - val_loss: 0.0063
Epoch 11/512
512/512 - 0s - loss: 0.0133 - val_loss: 0.0049
Epoch 12/512
512/512 - 0s - loss: 0.0095 - val_loss: 0.0051
Epoch 13/512
512/512 - 0s - loss: 0.0074 - val_loss: 0.0030
Epoch 14/512
512/512 - 0s - loss: 0.0073 - val_loss: 0.0032
Epoch 15/512
512/512 - 0s - loss: 0.0087 - val_loss: 0.0038
Epoch 16/512
512/512 - 0s - loss: 0.0062 - val_loss: 0.0029
Epoch 17/512
512/512 - 0s - loss: 0.0059 - val_loss: 0.0045
Epoch 18/512
512/512 - 0s - loss: 0.0070 - val_loss: 0.0058
Epoch 19/512
512/512 - 0s - loss: 0.0059 - val_loss: 0.0070
Epoch 20/512
512/512 - 0s - loss: 0.0057 - val_loss: 0.0072
Epoch 21/512
512/512 - 0s - loss: 0.0057 - val_loss: 0.0080
Epoch 22/512
512/512 - 0s - loss: 0.0050 - val_loss: 0.0080
Epoch 23/512
512/512 - 0s - loss: 0.0044 - val_loss: 0.0084
Epoch 24/512
512/512 - 0s - loss: 0.0042 - val_loss: 0.0071
Epoch 25/512
512/512 - 0s - loss: 0.0044 - val_loss: 0.0071
Epoch 26/512
512/512 - 0s - loss: 0.0041 - val_loss: 0.0052
Epoch 27/512
512/512 - 0s - loss: 0.0045 - val_loss: 0.0063
Epoch 28/512
512/512 - 0s - loss: 0.0037 - val_loss: 0.0069
Epoch 29/512
512/512 - 0s - loss: 0.0034 - val_loss: 0.0066
Epoch 30/512
512/512 - 0s - loss: 0.0028 - val_loss: 0.0066
Epoch 31/512
512/512 - 0s - loss: 0.0029 - val_loss: 0.0049
Epoch 32/512
512/512 - 0s - loss: 0.0035 - val_loss: 0.0051
Epoch 33/512
512/512 - 0s - loss: 0.0026 - val_loss: 0.0053
Epoch 34/512
512/512 - 0s - loss: 0.0023 - val_loss: 0.0048
Epoch 35/512
512/512 - 0s - loss: 0.0024 - val_loss: 0.0040
Epoch 36/512
512/512 - 0s - loss: 0.0026 - val_loss: 0.0041
Epoch 37/512
512/512 - 0s - loss: 0.0020 - val_loss: 0.0041
Epoch 38/512
512/512 - 0s - loss: 0.0021 - val_loss: 0.0034
Epoch 39/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0031
Epoch 40/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0022
Epoch 41/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0019
Epoch 42/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0015
Epoch 43/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0016
Epoch 44/512
512/512 - 0s - loss: 0.0013 - val_loss: 0.0016
Epoch 45/512
512/512 - 0s - loss: 0.0016 - val_loss: 0.0050
Epoch 46/512
512/512 - 0s - loss: 0.0024 - val_loss: 0.0035
Epoch 47/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0033
Epoch 48/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0034
Epoch 49/512
512/512 - 0s - loss: 0.0015 - val_loss: 0.0036
Epoch 50/512
512/512 - 0s - loss: 0.0014 - val_loss: 0.0030
Epoch 51/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0029
Epoch 52/512
512/512 - 0s - loss: 0.0011 - val_loss: 0.0029
Epoch 53/512
512/512 - 0s - loss: 0.0012 - val_loss: 0.0027
Epoch 54/512
512/512 - 0s - loss: 0.0010 - val_loss: 0.0024
Epoch 55/512
512/512 - 0s - loss: 8.9994e-04 - val_loss: 0.0021
Epoch 56/512
512/512 - 0s - loss: 7.3996e-04 - val_loss: 0.0018
Epoch 57/512
512/512 - 0s - loss: 7.6054e-04 - val_loss: 0.0018
Epoch 58/512
512/512 - 0s - loss: 7.9353e-04 - val_loss: 0.0016
Epoch 59/512
512/512 - 0s - loss: 6.9777e-04 - val_loss: 0.0014
Epoch 60/512
512/512 - 0s - loss: 6.2816e-04 - val_loss: 0.0013
Epoch 61/512
512/512 - 0s - loss: 6.1961e-04 - val_loss: 0.0012
Epoch 62/512
512/512 - 0s - loss: 5.7762e-04 - val_loss: 0.0010
Epoch 63/512
512/512 - 0s - loss: 4.7764e-04 - val_loss: 8.6636e-04
Epoch 64/512
512/512 - 0s - loss: 4.2752e-04 - val_loss: 7.8458e-04
Epoch 65/512
512/512 - 0s - loss: 4.3437e-04 - val_loss: 7.4320e-04
Epoch 66/512
512/512 - 0s - loss: 4.2370e-04 - val_loss: 6.1669e-04
Epoch 67/512
512/512 - 0s - loss: 3.3739e-04 - val_loss: 5.0951e-04
Epoch 68/512
512/512 - 0s - loss: 2.9643e-04 - val_loss: 4.3785e-04
Epoch 69/512
512/512 - 0s - loss: 2.7340e-04 - val_loss: 3.7100e-04
Epoch 70/512
512/512 - 0s - loss: 2.5205e-04 - val_loss: 3.1161e-04
Epoch 71/512
512/512 - 0s - loss: 2.1465e-04 - val_loss: 2.4748e-04
Epoch 72/512
512/512 - 0s - loss: 1.8141e-04 - val_loss: 1.9923e-04
Epoch 73/512
512/512 - 0s - loss: 1.5542e-04 - val_loss: 1.5664e-04
Epoch 74/512
512/512 - 0s - loss: 1.3350e-04 - val_loss: 1.2457e-04
Epoch 75/512
512/512 - 0s - loss: 1.1396e-04 - val_loss: 9.7422e-05
Epoch 76/512
512/512 - 0s - loss: 9.5110e-05 - val_loss: 7.4088e-05
Epoch 77/512
512/512 - 0s - loss: 7.6048e-05 - val_loss: 5.4810e-05
Epoch 78/512
512/512 - 0s - loss: 6.0375e-05 - val_loss: 4.2224e-05
Epoch 79/512
512/512 - 0s - loss: 4.9307e-05 - val_loss: 3.3507e-05
Epoch 80/512
512/512 - 0s - loss: 4.0540e-05 - val_loss: 2.5599e-05
Epoch 81/512
512/512 - 0s - loss: 3.0240e-05 - val_loss: 1.9724e-05
Epoch 82/512
512/512 - 0s - loss: 2.3340e-05 - val_loss: 1.5552e-05
Epoch 83/512
512/512 - 0s - loss: 1.9607e-05 - val_loss: 1.2307e-05
Epoch 84/512
512/512 - 0s - loss: 1.5078e-05 - val_loss: 1.0190e-05
Epoch 85/512
512/512 - 0s - loss: 1.1337e-05 - val_loss: 8.3751e-06
Epoch 86/512
512/512 - 0s - loss: 9.6221e-06 - val_loss: 6.6954e-06
Epoch 87/512
512/512 - 0s - loss: 8.0383e-06 - val_loss: 5.8119e-06
Epoch 88/512
512/512 - 0s - loss: 5.9530e-06 - val_loss: 5.1245e-06
Epoch 89/512
512/512 - 0s - loss: 4.9441e-06 - val_loss: 4.0946e-06
Epoch 90/512
512/512 - 0s - loss: 4.4813e-06 - val_loss: 3.4782e-06
Epoch 91/512
512/512 - 0s - loss: 3.4956e-06 - val_loss: 3.2849e-06
Epoch 92/512
512/512 - 0s - loss: 2.6498e-06 - val_loss: 2.8269e-06
Epoch 93/512
512/512 - 0s - loss: 2.3762e-06 - val_loss: 2.0880e-06
Epoch 94/512
512/512 - 0s - loss: 2.1549e-06 - val_loss: 2.0051e-06
Epoch 95/512
512/512 - 0s - loss: 1.5138e-06 - val_loss: 1.9539e-06
Epoch 96/512
512/512 - 0s - loss: 1.2436e-06 - val_loss: 1.5114e-06
Epoch 97/512
512/512 - 0s - loss: 1.2073e-06 - val_loss: 1.1311e-06
Epoch 98/512
512/512 - 0s - loss: 1.0158e-06 - val_loss: 1.1975e-06
Epoch 99/512
512/512 - 0s - loss: 7.3433e-07 - val_loss: 1.1026e-06
Epoch 100/512
512/512 - 0s - loss: 6.2369e-07 - val_loss: 8.4723e-07
Epoch 101/512
512/512 - 0s - loss: 6.3128e-07 - val_loss: 6.1530e-07
Epoch 102/512
512/512 - 0s - loss: 5.1856e-07 - val_loss: 6.7572e-07
Epoch 103/512
512/512 - 0s - loss: 3.6854e-07 - val_loss: 6.2334e-07
Epoch 104/512
512/512 - 0s - loss: 3.2743e-07 - val_loss: 4.4956e-07
Epoch 105/512
512/512 - 0s - loss: 3.5585e-07 - val_loss: 3.5491e-07
Epoch 106/512
512/512 - 0s - loss: 2.6974e-07 - val_loss: 4.2439e-07
Epoch 107/512
512/512 - 0s - loss: 1.9839e-07 - val_loss: 3.5618e-07
Epoch 108/512
512/512 - 0s - loss: 1.9288e-07 - val_loss: 2.1964e-07
Epoch 109/512
512/512 - 0s - loss: 2.0512e-07 - val_loss: 2.2963e-07
Epoch 110/512
512/512 - 0s - loss: 1.3717e-07 - val_loss: 2.3461e-07
Epoch 111/512
512/512 - 0s - loss: 1.1221e-07 - val_loss: 1.9112e-07
Epoch 112/512
512/512 - 0s - loss: 1.2390e-07 - val_loss: 1.1153e-07
Epoch 113/512
512/512 - 0s - loss: 1.1802e-07 - val_loss: 1.5082e-07
Epoch 114/512
512/512 - 0s - loss: 8.0800e-08 - val_loss: 1.4499e-07
Epoch 115/512
512/512 - 0s - loss: 7.5797e-08 - val_loss: 1.0039e-07
Epoch 116/512
512/512 - 0s - loss: 9.7672e-08 - val_loss: 6.0327e-08
Epoch 117/512
512/512 - 0s - loss: 9.8045e-08 - val_loss: 1.1448e-07
Epoch 118/512
512/512 - 0s - loss: 4.7534e-08 - val_loss: 1.1377e-07
Epoch 119/512
512/512 - 0s - loss: 4.0292e-08 - val_loss: 1.0592e-07
Epoch 120/512
512/512 - 0s - loss: 3.7894e-08 - val_loss: 5.9134e-08
Epoch 121/512
512/512 - 0s - loss: 8.3066e-08 - val_loss: 5.5065e-08
Epoch 122/512
512/512 - 0s - loss: 3.4609e-08 - val_loss: 7.6306e-08
Epoch 123/512
512/512 - 0s - loss: 2.7557e-08 - val_loss: 5.2475e-08
Epoch 124/512
512/512 - 0s - loss: 2.7098e-08 - val_loss: 4.0308e-08
Epoch 125/512
512/512 - 0s - loss: 3.2961e-08 - val_loss: 2.0345e-08
Epoch 126/512
512/512 - 0s - loss: 4.2727e-08 - val_loss: 3.4883e-08
Epoch 127/512
512/512 - 0s - loss: 2.1303e-08 - val_loss: 3.6798e-08
Epoch 128/512
512/512 - 0s - loss: 1.8903e-08 - val_loss: 3.4870e-08
Epoch 129/512
512/512 - 0s - loss: 2.0785e-08 - val_loss: 1.6102e-08
Epoch 130/512
512/512 - 0s - loss: 3.4741e-08 - val_loss: 1.9742e-08
Epoch 131/512
512/512 - 0s - loss: 1.7416e-08 - val_loss: 2.3304e-08
Epoch 132/512
512/512 - 0s - loss: 1.4581e-08 - val_loss: 2.0426e-08
Epoch 133/512
512/512 - 0s - loss: 1.4630e-08 - val_loss: 1.3839e-08
Epoch 134/512
512/512 - 0s - loss: 2.2220e-08 - val_loss: 9.3957e-09
Epoch 135/512
512/512 - 0s - loss: 1.7805e-08 - val_loss: 1.3435e-08
Epoch 136/512
512/512 - 0s - loss: 1.2368e-08 - val_loss: 1.2115e-08
Epoch 137/512
512/512 - 0s - loss: 1.1926e-08 - val_loss: 9.7739e-09
Epoch 138/512
512/512 - 0s - loss: 1.4604e-08 - val_loss: 7.0584e-09
Epoch 139/512
512/512 - 0s - loss: 1.5840e-08 - val_loss: 8.2141e-09
Epoch 140/512
512/512 - 0s - loss: 1.0997e-08 - val_loss: 8.4547e-09
Epoch 141/512
512/512 - 0s - loss: 9.9551e-09 - val_loss: 7.3442e-09
Epoch 142/512
512/512 - 0s - loss: 1.0775e-08 - val_loss: 5.6785e-09
Epoch 143/512
512/512 - 0s - loss: 1.2823e-08 - val_loss: 5.4504e-09
Epoch 144/512
512/512 - 0s - loss: 1.0367e-08 - val_loss: 5.8497e-09
Epoch 145/512
512/512 - 0s - loss: 8.9552e-09 - val_loss: 4.9906e-09
Epoch 146/512
512/512 - 0s - loss: 8.1194e-09 - val_loss: 4.8305e-09
Epoch 147/512
512/512 - 0s - loss: 9.0624e-09 - val_loss: 3.7940e-09
Epoch 148/512
512/512 - 0s - loss: 1.0756e-08 - val_loss: 3.8535e-09
Epoch 149/512
512/512 - 0s - loss: 8.4081e-09 - val_loss: 4.1092e-09
Epoch 150/512
512/512 - 0s - loss: 7.2961e-09 - val_loss: 3.8729e-09
Epoch 151/512
512/512 - 0s - loss: 7.2857e-09 - val_loss: 3.4530e-09
Epoch 152/512
512/512 - 0s - loss: 7.9069e-09 - val_loss: 3.0975e-09
Epoch 153/512
512/512 - 0s - loss: 7.9735e-09 - val_loss: 3.0618e-09
Epoch 154/512
512/512 - 0s - loss: 7.0807e-09 - val_loss: 3.0629e-09
Epoch 155/512
512/512 - 0s - loss: 6.5049e-09 - val_loss: 2.8865e-09
Epoch 156/512
512/512 - 0s - loss: 6.4679e-09 - val_loss: 2.6862e-09
Epoch 157/512
512/512 - 0s - loss: 6.7192e-09 - val_loss: 2.4948e-09
Epoch 158/512
512/512 - 0s - loss: 6.6426e-09 - val_loss: 2.4394e-09
Epoch 159/512
512/512 - 0s - loss: 6.0690e-09 - val_loss: 2.4154e-09
Epoch 160/512
512/512 - 0s - loss: 5.7259e-09 - val_loss: 2.3185e-09
Epoch 161/512
512/512 - 0s - loss: 5.7115e-09 - val_loss: 2.1786e-09
Epoch 162/512
512/512 - 0s - loss: 5.7959e-09 - val_loss: 2.0904e-09
Epoch 163/512
512/512 - 0s - loss: 5.7538e-09 - val_loss: 2.0878e-09
Epoch 164/512
512/512 - 0s - loss: 5.4505e-09 - val_loss: 2.0419e-09
Epoch 165/512
512/512 - 0s - loss: 5.1500e-09 - val_loss: 1.9777e-09
Epoch 166/512
512/512 - 0s - loss: 5.0560e-09 - val_loss: 1.8892e-09
Epoch 167/512
512/512 - 0s - loss: 5.0459e-09 - val_loss: 1.8185e-09
Epoch 168/512
512/512 - 0s - loss: 5.0031e-09 - val_loss: 1.7582e-09
Epoch 169/512
512/512 - 0s - loss: 4.9201e-09 - val_loss: 1.7174e-09
Epoch 170/512
512/512 - 0s - loss: 4.6741e-09 - val_loss: 1.6806e-09
Epoch 171/512
512/512 - 0s - loss: 4.5689e-09 - val_loss: 1.6262e-09
Epoch 172/512
512/512 - 0s - loss: 4.5337e-09 - val_loss: 1.5778e-09
Epoch 173/512
512/512 - 0s - loss: 4.5084e-09 - val_loss: 1.5410e-09
Epoch 174/512
512/512 - 0s - loss: 4.3598e-09 - val_loss: 1.5035e-09
Epoch 175/512
512/512 - 0s - loss: 4.2494e-09 - val_loss: 1.4673e-09
Epoch 176/512
512/512 - 0s - loss: 4.1125e-09 - val_loss: 1.4365e-09
Epoch 177/512
512/512 - 0s - loss: 4.0516e-09 - val_loss: 1.3867e-09
Epoch 178/512
512/512 - 0s - loss: 4.0851e-09 - val_loss: 1.3460e-09
Epoch 179/512
512/512 - 0s - loss: 4.0912e-09 - val_loss: 1.3110e-09
Epoch 180/512
512/512 - 0s - loss: 3.9460e-09 - val_loss: 1.3074e-09
Epoch 181/512
512/512 - 0s - loss: 3.7684e-09 - val_loss: 1.2883e-09
Epoch 182/512
512/512 - 0s - loss: 3.6745e-09 - val_loss: 1.2623e-09
Epoch 183/512
512/512 - 0s - loss: 3.6826e-09 - val_loss: 1.2192e-09
Epoch 184/512
512/512 - 0s - loss: 3.7073e-09 - val_loss: 1.1863e-09
Epoch 185/512
512/512 - 0s - loss: 3.6836e-09 - val_loss: 1.1689e-09
Epoch 186/512
512/512 - 0s - loss: 3.5628e-09 - val_loss: 1.1511e-09
Epoch 187/512
512/512 - 0s - loss: 3.4586e-09 - val_loss: 1.1376e-09
Epoch 188/512
512/512 - 0s - loss: 3.3836e-09 - val_loss: 1.1131e-09
Epoch 189/512
512/512 - 0s - loss: 3.3825e-09 - val_loss: 1.0818e-09
Epoch 190/512
512/512 - 0s - loss: 3.3980e-09 - val_loss: 1.0563e-09
Epoch 191/512
512/512 - 0s - loss: 3.3465e-09 - val_loss: 1.0498e-09
Epoch 192/512
512/512 - 0s - loss: 3.2283e-09 - val_loss: 1.0426e-09
Epoch 193/512
512/512 - 0s - loss: 3.1539e-09 - val_loss: 1.0184e-09
Epoch 194/512
512/512 - 0s - loss: 3.1529e-09 - val_loss: 9.9709e-10
Epoch 195/512
512/512 - 0s - loss: 3.1231e-09 - val_loss: 9.8411e-10
Epoch 196/512
512/512 - 0s - loss: 3.0917e-09 - val_loss: 9.6337e-10
Epoch 197/512
512/512 - 0s - loss: 3.0565e-09 - val_loss: 9.4905e-10
Epoch 198/512
512/512 - 0s - loss: 3.0110e-09 - val_loss: 9.3445e-10
Epoch 199/512
512/512 - 0s - loss: 2.9887e-09 - val_loss: 9.1565e-10
Epoch 200/512
512/512 - 0s - loss: 2.9655e-09 - val_loss: 9.0762e-10
Epoch 201/512
512/512 - 0s - loss: 2.8972e-09 - val_loss: 9.0014e-10
Epoch 202/512
512/512 - 0s - loss: 2.8358e-09 - val_loss: 8.8347e-10
Epoch 203/512
512/512 - 0s - loss: 2.8152e-09 - val_loss: 8.7158e-10
Epoch 204/512
512/512 - 0s - loss: 2.7836e-09 - val_loss: 8.5967e-10
Epoch 205/512
512/512 - 0s - loss: 2.7815e-09 - val_loss: 8.4206e-10
Epoch 206/512
512/512 - 0s - loss: 2.7676e-09 - val_loss: 8.3101e-10
Epoch 207/512
512/512 - 0s - loss: 2.6986e-09 - val_loss: 8.2767e-10
Epoch 208/512
512/512 - 0s - loss: 2.6632e-09 - val_loss: 8.1299e-10
Epoch 209/512
512/512 - 0s - loss: 2.6342e-09 - val_loss: 8.0510e-10
Epoch 210/512
512/512 - 0s - loss: 2.6129e-09 - val_loss: 7.9083e-10
Epoch 211/512
512/512 - 0s - loss: 2.6023e-09 - val_loss: 7.8103e-10
Epoch 212/512
512/512 - 0s - loss: 2.5724e-09 - val_loss: 7.7224e-10
Epoch 213/512
512/512 - 0s - loss: 2.5534e-09 - val_loss: 7.6628e-10
Epoch 214/512
512/512 - 0s - loss: 2.4859e-09 - val_loss: 7.6005e-10
Epoch 215/512
512/512 - 0s - loss: 2.4462e-09 - val_loss: 7.5094e-10
Epoch 216/512
512/512 - 0s - loss: 2.4377e-09 - val_loss: 7.4240e-10
Epoch 217/512
512/512 - 0s - loss: 2.4409e-09 - val_loss: 7.3010e-10
Epoch 218/512
512/512 - 0s - loss: 2.4050e-09 - val_loss: 7.2513e-10
Epoch 219/512
512/512 - 0s - loss: 2.3730e-09 - val_loss: 7.1529e-10
Epoch 220/512
512/512 - 0s - loss: 2.3660e-09 - val_loss: 7.0518e-10
Epoch 221/512
512/512 - 0s - loss: 2.3568e-09 - val_loss: 6.9839e-10
Epoch 222/512
512/512 - 0s - loss: 2.3308e-09 - val_loss: 6.9132e-10
Epoch 223/512
512/512 - 0s - loss: 2.3196e-09 - val_loss: 6.8733e-10
Epoch 224/512
512/512 - 0s - loss: 2.2648e-09 - val_loss: 6.8115e-10
Epoch 225/512
512/512 - 0s - loss: 2.2516e-09 - val_loss: 6.7192e-10
Epoch 226/512
512/512 - 0s - loss: 2.2495e-09 - val_loss: 6.6379e-10
Epoch 227/512
512/512 - 0s - loss: 2.2231e-09 - val_loss: 6.5949e-10
Epoch 228/512
512/512 - 0s - loss: 2.1961e-09 - val_loss: 6.5403e-10
Epoch 229/512
512/512 - 0s - loss: 2.1810e-09 - val_loss: 6.4439e-10
Epoch 230/512
512/512 - 0s - loss: 2.1618e-09 - val_loss: 6.3917e-10
Epoch 231/512
512/512 - 0s - loss: 2.1300e-09 - val_loss: 6.3663e-10
Epoch 232/512
512/512 - 0s - loss: 2.1028e-09 - val_loss: 6.3031e-10
Epoch 233/512
512/512 - 0s - loss: 2.1056e-09 - val_loss: 6.1915e-10
Epoch 234/512
512/512 - 0s - loss: 2.1137e-09 - val_loss: 6.1547e-10
Epoch 235/512
512/512 - 0s - loss: 2.0716e-09 - val_loss: 6.1052e-10
Epoch 236/512
512/512 - 0s - loss: 2.0414e-09 - val_loss: 6.0811e-10
Epoch 237/512
512/512 - 0s - loss: 2.0261e-09 - val_loss: 6.0315e-10
Epoch 238/512
512/512 - 0s - loss: 2.0192e-09 - val_loss: 5.9391e-10
Epoch 239/512
512/512 - 0s - loss: 2.0212e-09 - val_loss: 5.8783e-10
Epoch 240/512
512/512 - 0s - loss: 2.0315e-09 - val_loss: 5.7924e-10
Epoch 241/512
512/512 - 0s - loss: 2.0168e-09 - val_loss: 5.7801e-10
Epoch 242/512
512/512 - 0s - loss: 1.9655e-09 - val_loss: 5.7573e-10
Epoch 243/512
512/512 - 0s - loss: 1.9372e-09 - val_loss: 5.7310e-10
Epoch 244/512
512/512 - 0s - loss: 1.9124e-09 - val_loss: 5.6771e-10
Epoch 245/512
512/512 - 0s - loss: 1.9026e-09 - val_loss: 5.6086e-10
Epoch 246/512
512/512 - 0s - loss: 1.8996e-09 - val_loss: 5.5579e-10
Epoch 247/512
512/512 - 0s - loss: 1.9198e-09 - val_loss: 5.4769e-10
Epoch 248/512
512/512 - 0s - loss: 1.9223e-09 - val_loss: 5.4403e-10
Epoch 249/512
512/512 - 0s - loss: 1.8839e-09 - val_loss: 5.4308e-10
Epoch 250/512
512/512 - 0s - loss: 1.8402e-09 - val_loss: 5.4207e-10
Epoch 251/512
512/512 - 0s - loss: 1.8207e-09 - val_loss: 5.3797e-10
Epoch 252/512
512/512 - 0s - loss: 1.8193e-09 - val_loss: 5.3072e-10
Epoch 253/512
512/512 - 0s - loss: 1.8257e-09 - val_loss: 5.2444e-10
Epoch 254/512
512/512 - 0s - loss: 1.8120e-09 - val_loss: 5.2392e-10
Epoch 255/512
512/512 - 0s - loss: 1.7961e-09 - val_loss: 5.1879e-10
Epoch 256/512
512/512 - 0s - loss: 1.7758e-09 - val_loss: 5.1701e-10
Epoch 257/512
512/512 - 0s - loss: 1.7483e-09 - val_loss: 5.1469e-10
Epoch 258/512
512/512 - 0s - loss: 1.7411e-09 - val_loss: 5.0847e-10
Epoch 259/512
512/512 - 0s - loss: 1.7314e-09 - val_loss: 5.0485e-10
Epoch 260/512
512/512 - 0s - loss: 1.7170e-09 - val_loss: 5.0251e-10
Epoch 261/512
512/512 - 0s - loss: 1.7114e-09 - val_loss: 4.9616e-10
Epoch 262/512
512/512 - 0s - loss: 1.7175e-09 - val_loss: 4.9210e-10
Epoch 263/512
512/512 - 0s - loss: 1.7187e-09 - val_loss: 4.8912e-10
Epoch 264/512
512/512 - 0s - loss: 1.6910e-09 - val_loss: 4.8679e-10
Epoch 265/512
512/512 - 0s - loss: 1.6736e-09 - val_loss: 4.8326e-10
Epoch 266/512
512/512 - 0s - loss: 1.6608e-09 - val_loss: 4.7982e-10
Epoch 267/512
512/512 - 0s - loss: 1.6541e-09 - val_loss: 4.7568e-10
Epoch 268/512
512/512 - 0s - loss: 1.6424e-09 - val_loss: 4.7411e-10
Epoch 269/512
512/512 - 0s - loss: 1.6294e-09 - val_loss: 4.7176e-10
Epoch 270/512
512/512 - 0s - loss: 1.6272e-09 - val_loss: 4.6652e-10
Epoch 271/512
512/512 - 0s - loss: 1.6155e-09 - val_loss: 4.6509e-10
Epoch 272/512
512/512 - 0s - loss: 1.6092e-09 - val_loss: 4.6105e-10
Epoch 273/512
512/512 - 0s - loss: 1.6053e-09 - val_loss: 4.5922e-10
Epoch 274/512
512/512 - 0s - loss: 1.5843e-09 - val_loss: 4.5735e-10
Epoch 275/512
512/512 - 0s - loss: 1.5643e-09 - val_loss: 4.5359e-10
Epoch 276/512
512/512 - 0s - loss: 1.5522e-09 - val_loss: 4.5230e-10
Epoch 277/512
512/512 - 0s - loss: 1.5518e-09 - val_loss: 4.4657e-10
Epoch 278/512
512/512 - 0s - loss: 1.5635e-09 - val_loss: 4.4415e-10
Epoch 279/512
512/512 - 0s - loss: 1.5521e-09 - val_loss: 4.4092e-10
Epoch 280/512
512/512 - 0s - loss: 1.5364e-09 - val_loss: 4.3837e-10
Epoch 281/512
512/512 - 0s - loss: 1.5254e-09 - val_loss: 4.3829e-10
Epoch 282/512
512/512 - 0s - loss: 1.4841e-09 - val_loss: 4.3933e-10
Epoch 283/512
512/512 - 0s - loss: 1.4723e-09 - val_loss: 4.3515e-10
Epoch 284/512
512/512 - 0s - loss: 1.4788e-09 - val_loss: 4.2905e-10
Epoch 285/512
512/512 - 0s - loss: 1.4964e-09 - val_loss: 4.2500e-10
Epoch 286/512
512/512 - 0s - loss: 1.4978e-09 - val_loss: 4.2075e-10
Epoch 287/512
512/512 - 0s - loss: 1.4861e-09 - val_loss: 4.2198e-10
Epoch 288/512
512/512 - 0s - loss: 1.4582e-09 - val_loss: 4.2099e-10
Epoch 289/512
512/512 - 0s - loss: 1.4455e-09 - val_loss: 4.1829e-10
Epoch 290/512
512/512 - 0s - loss: 1.4447e-09 - val_loss: 4.1440e-10
Epoch 291/512
512/512 - 0s - loss: 1.4353e-09 - val_loss: 4.1289e-10
Epoch 292/512
512/512 - 0s - loss: 1.4323e-09 - val_loss: 4.0979e-10
Epoch 293/512
512/512 - 0s - loss: 1.4291e-09 - val_loss: 4.0794e-10
Epoch 294/512
512/512 - 0s - loss: 1.4206e-09 - val_loss: 4.0478e-10
Epoch 295/512
512/512 - 0s - loss: 1.4127e-09 - val_loss: 4.0226e-10
Epoch 296/512
512/512 - 0s - loss: 1.4099e-09 - val_loss: 3.9998e-10
Epoch 297/512
512/512 - 0s - loss: 1.4037e-09 - val_loss: 3.9860e-10
Epoch 298/512
512/512 - 0s - loss: 1.3809e-09 - val_loss: 3.9854e-10
Epoch 299/512
512/512 - 0s - loss: 1.3696e-09 - val_loss: 3.9663e-10
Epoch 300/512
512/512 - 0s - loss: 1.3678e-09 - val_loss: 3.9256e-10
Epoch 301/512
512/512 - 0s - loss: 1.3577e-09 - val_loss: 3.9264e-10
Epoch 302/512
512/512 - 0s - loss: 1.3477e-09 - val_loss: 3.9008e-10
Epoch 303/512
512/512 - 0s - loss: 1.3488e-09 - val_loss: 3.8534e-10
Epoch 304/512
512/512 - 0s - loss: 1.3540e-09 - val_loss: 3.8204e-10
Epoch 305/512
512/512 - 0s - loss: 1.3494e-09 - val_loss: 3.8197e-10
Epoch 306/512
512/512 - 0s - loss: 1.3351e-09 - val_loss: 3.8061e-10
Epoch 307/512
512/512 - 0s - loss: 1.3287e-09 - val_loss: 3.7794e-10
Epoch 308/512
512/512 - 0s - loss: 1.3225e-09 - val_loss: 3.7653e-10
Epoch 309/512
512/512 - 0s - loss: 1.3097e-09 - val_loss: 3.7485e-10
Epoch 310/512
512/512 - 0s - loss: 1.3000e-09 - val_loss: 3.7348e-10
Epoch 311/512
512/512 - 0s - loss: 1.3009e-09 - val_loss: 3.7096e-10
Epoch 312/512
512/512 - 0s - loss: 1.2974e-09 - val_loss: 3.6916e-10
Epoch 313/512
512/512 - 0s - loss: 1.2796e-09 - val_loss: 3.6765e-10
Epoch 314/512
512/512 - 0s - loss: 1.2768e-09 - val_loss: 3.6549e-10
Epoch 315/512
512/512 - 0s - loss: 1.2828e-09 - val_loss: 3.6253e-10
Epoch 316/512
512/512 - 0s - loss: 1.2779e-09 - val_loss: 3.6092e-10
Epoch 317/512
512/512 - 0s - loss: 1.2716e-09 - val_loss: 3.5969e-10
Epoch 318/512
512/512 - 0s - loss: 1.2735e-09 - val_loss: 3.5583e-10
Epoch 319/512
512/512 - 0s - loss: 1.2645e-09 - val_loss: 3.5610e-10
Epoch 320/512
512/512 - 0s - loss: 1.2569e-09 - val_loss: 3.5379e-10
Epoch 321/512
512/512 - 0s - loss: 1.2478e-09 - val_loss: 3.5240e-10
Epoch 322/512
512/512 - 0s - loss: 1.2351e-09 - val_loss: 3.5415e-10
Epoch 323/512
512/512 - 0s - loss: 1.2160e-09 - val_loss: 3.5214e-10
Epoch 324/512
512/512 - 0s - loss: 1.2131e-09 - val_loss: 3.4939e-10
Epoch 325/512
512/512 - 0s - loss: 1.2174e-09 - val_loss: 3.4816e-10
Epoch 326/512
512/512 - 0s - loss: 1.2142e-09 - val_loss: 3.4446e-10
Epoch 327/512
512/512 - 0s - loss: 1.2197e-09 - val_loss: 3.4315e-10
Epoch 328/512
512/512 - 0s - loss: 1.2085e-09 - val_loss: 3.4338e-10
Epoch 329/512
512/512 - 0s - loss: 1.1874e-09 - val_loss: 3.4217e-10
Epoch 330/512
512/512 - 0s - loss: 1.1845e-09 - val_loss: 3.4005e-10
Epoch 331/512
512/512 - 0s - loss: 1.1816e-09 - val_loss: 3.3858e-10
Epoch 332/512
512/512 - 0s - loss: 1.1775e-09 - val_loss: 3.3581e-10
Epoch 333/512
512/512 - 0s - loss: 1.1890e-09 - val_loss: 3.3197e-10
Epoch 334/512
512/512 - 0s - loss: 1.1927e-09 - val_loss: 3.3163e-10
Epoch 335/512
512/512 - 0s - loss: 1.1832e-09 - val_loss: 3.3067e-10
Epoch 336/512
512/512 - 0s - loss: 1.1609e-09 - val_loss: 3.3154e-10
Epoch 337/512
512/512 - 0s - loss: 1.1450e-09 - val_loss: 3.3186e-10
Epoch 338/512
512/512 - 0s - loss: 1.1361e-09 - val_loss: 3.2871e-10
Epoch 339/512
512/512 - 0s - loss: 1.1393e-09 - val_loss: 3.2686e-10
Epoch 340/512
512/512 - 0s - loss: 1.1401e-09 - val_loss: 3.2405e-10
Epoch 341/512
512/512 - 0s - loss: 1.1481e-09 - val_loss: 3.2037e-10
Epoch 342/512
512/512 - 0s - loss: 1.1485e-09 - val_loss: 3.2080e-10
Epoch 343/512
512/512 - 0s - loss: 1.1291e-09 - val_loss: 3.2048e-10
Epoch 344/512
512/512 - 0s - loss: 1.1159e-09 - val_loss: 3.2144e-10
Epoch 345/512
512/512 - 0s - loss: 1.0992e-09 - val_loss: 3.1960e-10
Epoch 346/512
512/512 - 0s - loss: 1.1029e-09 - val_loss: 3.1745e-10
Epoch 347/512
512/512 - 0s - loss: 1.1117e-09 - val_loss: 3.1318e-10
Epoch 348/512
512/512 - 0s - loss: 1.1254e-09 - val_loss: 3.1160e-10
Epoch 349/512
512/512 - 0s - loss: 1.1115e-09 - val_loss: 3.1097e-10
Epoch 350/512
512/512 - 0s - loss: 1.0987e-09 - val_loss: 3.1168e-10
Epoch 351/512
512/512 - 0s - loss: 1.0920e-09 - val_loss: 3.0934e-10
Epoch 352/512
512/512 - 0s - loss: 1.0907e-09 - val_loss: 3.0854e-10
Epoch 353/512
512/512 - 0s - loss: 1.0851e-09 - val_loss: 3.0755e-10
Epoch 354/512
512/512 - 0s - loss: 1.0742e-09 - val_loss: 3.0733e-10
Epoch 355/512
512/512 - 0s - loss: 1.0750e-09 - val_loss: 3.0426e-10
Epoch 356/512
512/512 - 0s - loss: 1.0791e-09 - val_loss: 3.0269e-10
Epoch 357/512
512/512 - 0s - loss: 1.0811e-09 - val_loss: 3.0100e-10
Epoch 358/512
512/512 - 0s - loss: 1.0776e-09 - val_loss: 2.9990e-10
Epoch 359/512
512/512 - 0s - loss: 1.0673e-09 - val_loss: 3.0000e-10
Epoch 360/512
512/512 - 0s - loss: 1.0605e-09 - val_loss: 2.9777e-10
Epoch 361/512
512/512 - 0s - loss: 1.0535e-09 - val_loss: 2.9784e-10
Epoch 362/512
512/512 - 0s - loss: 1.0479e-09 - val_loss: 2.9603e-10
Epoch 363/512
512/512 - 0s - loss: 1.0356e-09 - val_loss: 2.9716e-10
Epoch 364/512
512/512 - 0s - loss: 1.0264e-09 - val_loss: 2.9562e-10
Epoch 365/512
512/512 - 0s - loss: 1.0295e-09 - val_loss: 2.9208e-10
Epoch 366/512
512/512 - 0s - loss: 1.0452e-09 - val_loss: 2.8981e-10
Epoch 367/512
512/512 - 0s - loss: 1.0339e-09 - val_loss: 2.9123e-10
Epoch 368/512
512/512 - 0s - loss: 1.0180e-09 - val_loss: 2.9024e-10
Epoch 369/512
512/512 - 0s - loss: 1.0187e-09 - val_loss: 2.8855e-10
Epoch 370/512
512/512 - 0s - loss: 1.0156e-09 - val_loss: 2.8726e-10
Epoch 371/512
512/512 - 0s - loss: 1.0094e-09 - val_loss: 2.8748e-10
Epoch 372/512
512/512 - 0s - loss: 1.0119e-09 - val_loss: 2.8361e-10
Epoch 373/512
512/512 - 0s - loss: 1.0229e-09 - val_loss: 2.8334e-10
Epoch 374/512
512/512 - 0s - loss: 1.0114e-09 - val_loss: 2.8213e-10
Epoch 375/512
512/512 - 0s - loss: 9.9758e-10 - val_loss: 2.8401e-10
Epoch 376/512
512/512 - 0s - loss: 9.8024e-10 - val_loss: 2.8393e-10
Epoch 377/512
512/512 - 0s - loss: 9.7711e-10 - val_loss: 2.8182e-10
Epoch 378/512
512/512 - 0s - loss: 9.8196e-10 - val_loss: 2.7938e-10
Epoch 379/512
512/512 - 0s - loss: 9.9003e-10 - val_loss: 2.7698e-10
Epoch 380/512
512/512 - 0s - loss: 9.8994e-10 - val_loss: 2.7600e-10
Epoch 381/512
512/512 - 0s - loss: 9.8289e-10 - val_loss: 2.7502e-10
Epoch 382/512
512/512 - 0s - loss: 9.8647e-10 - val_loss: 2.7393e-10
Epoch 383/512
512/512 - 0s - loss: 9.7470e-10 - val_loss: 2.7399e-10
Epoch 384/512
512/512 - 0s - loss: 9.6219e-10 - val_loss: 2.7563e-10
Epoch 385/512
512/512 - 0s - loss: 9.5186e-10 - val_loss: 2.7335e-10
Epoch 386/512
512/512 - 0s - loss: 9.5763e-10 - val_loss: 2.7114e-10
Epoch 387/512
512/512 - 0s - loss: 9.5552e-10 - val_loss: 2.7031e-10
Epoch 388/512
512/512 - 0s - loss: 9.5771e-10 - val_loss: 2.6905e-10
Epoch 389/512
512/512 - 0s - loss: 9.5691e-10 - val_loss: 2.6738e-10
Epoch 390/512
512/512 - 0s - loss: 9.5148e-10 - val_loss: 2.6662e-10
Epoch 391/512
512/512 - 0s - loss: 9.5127e-10 - val_loss: 2.6641e-10
Epoch 392/512
512/512 - 0s - loss: 9.4363e-10 - val_loss: 2.6538e-10
Epoch 393/512
512/512 - 0s - loss: 9.4130e-10 - val_loss: 2.6474e-10
Epoch 394/512
512/512 - 0s - loss: 9.3730e-10 - val_loss: 2.6350e-10
Epoch 395/512
512/512 - 0s - loss: 9.2943e-10 - val_loss: 2.6279e-10
Epoch 396/512
512/512 - 0s - loss: 9.3276e-10 - val_loss: 2.6109e-10
Epoch 397/512
512/512 - 0s - loss: 9.3155e-10 - val_loss: 2.6115e-10
Epoch 398/512
512/512 - 0s - loss: 9.2408e-10 - val_loss: 2.6027e-10
Epoch 399/512
512/512 - 0s - loss: 9.1412e-10 - val_loss: 2.6020e-10
Epoch 400/512
512/512 - 0s - loss: 9.0341e-10 - val_loss: 2.5989e-10
Epoch 401/512
512/512 - 0s - loss: 9.0155e-10 - val_loss: 2.5905e-10
Epoch 402/512
512/512 - 0s - loss: 9.0092e-10 - val_loss: 2.5767e-10
Epoch 403/512
512/512 - 0s - loss: 8.9903e-10 - val_loss: 2.5674e-10
Epoch 404/512
512/512 - 0s - loss: 8.9541e-10 - val_loss: 2.5693e-10
Epoch 405/512
512/512 - 0s - loss: 8.9316e-10 - val_loss: 2.5397e-10
Epoch 406/512
512/512 - 0s - loss: 8.9928e-10 - val_loss: 2.5283e-10
Epoch 407/512
512/512 - 0s - loss: 9.0091e-10 - val_loss: 2.5128e-10
Epoch 408/512
512/512 - 0s - loss: 9.0152e-10 - val_loss: 2.4993e-10
Epoch 409/512
512/512 - 0s - loss: 8.9941e-10 - val_loss: 2.5097e-10
Epoch 410/512
512/512 - 0s - loss: 8.9258e-10 - val_loss: 2.4903e-10
Epoch 411/512
512/512 - 0s - loss: 8.9496e-10 - val_loss: 2.4833e-10
Epoch 412/512
512/512 - 0s - loss: 8.8236e-10 - val_loss: 2.4922e-10
Epoch 413/512
512/512 - 0s - loss: 8.7899e-10 - val_loss: 2.4774e-10
Epoch 414/512
512/512 - 0s - loss: 8.7300e-10 - val_loss: 2.4759e-10
Epoch 415/512
512/512 - 0s - loss: 8.6489e-10 - val_loss: 2.4734e-10
Epoch 416/512
512/512 - 0s - loss: 8.6596e-10 - val_loss: 2.4600e-10
Epoch 417/512
512/512 - 0s - loss: 8.6100e-10 - val_loss: 2.4467e-10
Epoch 418/512
512/512 - 0s - loss: 8.6627e-10 - val_loss: 2.4314e-10
Epoch 419/512
512/512 - 0s - loss: 8.6741e-10 - val_loss: 2.4237e-10
Epoch 420/512
512/512 - 0s - loss: 8.6152e-10 - val_loss: 2.4240e-10
Epoch 421/512
512/512 - 0s - loss: 8.5498e-10 - val_loss: 2.4168e-10
Epoch 422/512
512/512 - 0s - loss: 8.5066e-10 - val_loss: 2.4117e-10
Epoch 423/512
512/512 - 0s - loss: 8.5140e-10 - val_loss: 2.3982e-10
Epoch 424/512
512/512 - 0s - loss: 8.4894e-10 - val_loss: 2.3937e-10
Epoch 425/512
512/512 - 0s - loss: 8.4146e-10 - val_loss: 2.3949e-10
Epoch 426/512
512/512 - 0s - loss: 8.3880e-10 - val_loss: 2.3873e-10
Epoch 427/512
512/512 - 0s - loss: 8.3210e-10 - val_loss: 2.3855e-10
Epoch 428/512
512/512 - 0s - loss: 8.3133e-10 - val_loss: 2.3693e-10
Epoch 429/512
512/512 - 0s - loss: 8.3611e-10 - val_loss: 2.3474e-10
Epoch 430/512
512/512 - 0s - loss: 8.4252e-10 - val_loss: 2.3392e-10
Epoch 431/512
512/512 - 0s - loss: 8.4328e-10 - val_loss: 2.3334e-10
Epoch 432/512
512/512 - 0s - loss: 8.3060e-10 - val_loss: 2.3466e-10
Epoch 433/512
512/512 - 0s - loss: 8.1692e-10 - val_loss: 2.3416e-10
Epoch 434/512
512/512 - 0s - loss: 8.1702e-10 - val_loss: 2.3289e-10
Epoch 435/512
512/512 - 0s - loss: 8.1479e-10 - val_loss: 2.3224e-10
Epoch 436/512
512/512 - 0s - loss: 8.1453e-10 - val_loss: 2.3086e-10
Epoch 437/512
512/512 - 0s - loss: 8.1331e-10 - val_loss: 2.3026e-10
Epoch 438/512
512/512 - 0s - loss: 8.1578e-10 - val_loss: 2.2901e-10
Epoch 439/512
512/512 - 0s - loss: 8.1879e-10 - val_loss: 2.2735e-10
Epoch 440/512
512/512 - 0s - loss: 8.1957e-10 - val_loss: 2.2701e-10
Epoch 441/512
512/512 - 0s - loss: 8.1083e-10 - val_loss: 2.2796e-10
Epoch 442/512
512/512 - 0s - loss: 8.0649e-10 - val_loss: 2.2611e-10
Epoch 443/512
512/512 - 0s - loss: 8.0061e-10 - val_loss: 2.2624e-10
Epoch 444/512
512/512 - 0s - loss: 7.9676e-10 - val_loss: 2.2547e-10
Epoch 445/512
512/512 - 0s - loss: 7.9773e-10 - val_loss: 2.2480e-10
Epoch 446/512
512/512 - 0s - loss: 7.9489e-10 - val_loss: 2.2493e-10
Epoch 447/512
512/512 - 0s - loss: 7.9366e-10 - val_loss: 2.2363e-10
Epoch 448/512
512/512 - 0s - loss: 7.8879e-10 - val_loss: 2.2353e-10
Epoch 449/512
512/512 - 0s - loss: 7.8795e-10 - val_loss: 2.2242e-10
Epoch 450/512
512/512 - 0s - loss: 7.8529e-10 - val_loss: 2.2175e-10
Epoch 451/512
512/512 - 0s - loss: 7.8477e-10 - val_loss: 2.2020e-10
Epoch 452/512
512/512 - 0s - loss: 7.8181e-10 - val_loss: 2.2156e-10
Epoch 453/512
512/512 - 0s - loss: 7.7346e-10 - val_loss: 2.2047e-10
Epoch 454/512
512/512 - 0s - loss: 7.6988e-10 - val_loss: 2.2006e-10
Epoch 455/512
512/512 - 0s - loss: 7.7014e-10 - val_loss: 2.1873e-10
Epoch 456/512
512/512 - 0s - loss: 7.7564e-10 - val_loss: 2.1737e-10
Epoch 457/512
512/512 - 0s - loss: 7.7493e-10 - val_loss: 2.1698e-10
Epoch 458/512
512/512 - 0s - loss: 7.7130e-10 - val_loss: 2.1576e-10
Epoch 459/512
512/512 - 0s - loss: 7.7145e-10 - val_loss: 2.1571e-10
Epoch 460/512
512/512 - 0s - loss: 7.6877e-10 - val_loss: 2.1500e-10
Epoch 461/512
512/512 - 0s - loss: 7.7114e-10 - val_loss: 2.1382e-10
Epoch 462/512
512/512 - 0s - loss: 7.6978e-10 - val_loss: 2.1377e-10
Epoch 463/512
512/512 - 0s - loss: 7.6109e-10 - val_loss: 2.1347e-10
Epoch 464/512
512/512 - 0s - loss: 7.5986e-10 - val_loss: 2.1371e-10
Epoch 465/512
512/512 - 0s - loss: 7.4798e-10 - val_loss: 2.1364e-10
Epoch 466/512
512/512 - 0s - loss: 7.4383e-10 - val_loss: 2.1343e-10
Epoch 467/512
512/512 - 0s - loss: 7.4207e-10 - val_loss: 2.1263e-10
Epoch 468/512
512/512 - 0s - loss: 7.4408e-10 - val_loss: 2.1077e-10
Epoch 469/512
512/512 - 0s - loss: 7.5176e-10 - val_loss: 2.0947e-10
Epoch 470/512
512/512 - 0s - loss: 7.4921e-10 - val_loss: 2.0917e-10
Epoch 471/512
512/512 - 0s - loss: 7.4489e-10 - val_loss: 2.0930e-10
Epoch 472/512
512/512 - 0s - loss: 7.3579e-10 - val_loss: 2.0909e-10
Epoch 473/512
512/512 - 0s - loss: 7.3485e-10 - val_loss: 2.0889e-10
Epoch 474/512
512/512 - 0s - loss: 7.2982e-10 - val_loss: 2.0841e-10
Epoch 475/512
512/512 - 0s - loss: 7.2466e-10 - val_loss: 2.0805e-10
Epoch 476/512
512/512 - 0s - loss: 7.1892e-10 - val_loss: 2.0734e-10
Epoch 477/512
512/512 - 0s - loss: 7.2214e-10 - val_loss: 2.0698e-10
Epoch 478/512
512/512 - 0s - loss: 7.2008e-10 - val_loss: 2.0611e-10
Epoch 479/512
512/512 - 0s - loss: 7.2158e-10 - val_loss: 2.0529e-10
Epoch 480/512
512/512 - 0s - loss: 7.2413e-10 - val_loss: 2.0379e-10
Epoch 481/512
512/512 - 0s - loss: 7.3224e-10 - val_loss: 2.0202e-10
Epoch 482/512
512/512 - 0s - loss: 7.3500e-10 - val_loss: 2.0271e-10
Epoch 483/512
512/512 - 0s - loss: 7.2011e-10 - val_loss: 2.0325e-10
Epoch 484/512
512/512 - 0s - loss: 7.0793e-10 - val_loss: 2.0342e-10
Epoch 485/512
512/512 - 0s - loss: 7.1216e-10 - val_loss: 2.0178e-10
Epoch 486/512
512/512 - 0s - loss: 7.1424e-10 - val_loss: 2.0091e-10
Epoch 487/512
512/512 - 0s - loss: 7.1030e-10 - val_loss: 2.0104e-10
Epoch 488/512
512/512 - 0s - loss: 7.0572e-10 - val_loss: 2.0166e-10
Epoch 489/512
512/512 - 0s - loss: 7.0295e-10 - val_loss: 1.9937e-10
Epoch 490/512
512/512 - 0s - loss: 7.0322e-10 - val_loss: 1.9971e-10
Epoch 491/512
512/512 - 0s - loss: 7.0203e-10 - val_loss: 1.9890e-10
Epoch 492/512
512/512 - 0s - loss: 7.0606e-10 - val_loss: 1.9756e-10
Epoch 493/512
512/512 - 0s - loss: 7.0069e-10 - val_loss: 1.9741e-10
Epoch 494/512
512/512 - 0s - loss: 6.9831e-10 - val_loss: 1.9723e-10
Epoch 495/512
512/512 - 0s - loss: 6.9470e-10 - val_loss: 1.9679e-10
Epoch 496/512
512/512 - 0s - loss: 6.9365e-10 - val_loss: 1.9669e-10
Epoch 497/512
512/512 - 0s - loss: 6.8658e-10 - val_loss: 1.9652e-10
Epoch 498/512
512/512 - 0s - loss: 6.8804e-10 - val_loss: 1.9525e-10
Epoch 499/512
512/512 - 0s - loss: 6.9097e-10 - val_loss: 1.9403e-10
Epoch 500/512
512/512 - 0s - loss: 6.9428e-10 - val_loss: 1.9338e-10
Epoch 501/512
512/512 - 0s - loss: 6.9008e-10 - val_loss: 1.9361e-10
Epoch 502/512
512/512 - 0s - loss: 6.8526e-10 - val_loss: 1.9321e-10
Epoch 503/512
512/512 - 0s - loss: 6.8368e-10 - val_loss: 1.9258e-10
Epoch 504/512
512/512 - 0s - loss: 6.8172e-10 - val_loss: 1.9229e-10
Epoch 505/512
512/512 - 0s - loss: 6.8013e-10 - val_loss: 1.9220e-10
Epoch 506/512
512/512 - 0s - loss: 6.7414e-10 - val_loss: 1.9211e-10
Epoch 507/512
512/512 - 0s - loss: 6.7259e-10 - val_loss: 1.9129e-10
Epoch 508/512
512/512 - 0s - loss: 6.7576e-10 - val_loss: 1.8974e-10
Epoch 509/512
512/512 - 0s - loss: 6.8062e-10 - val_loss: 1.8864e-10
Epoch 510/512
512/512 - 0s - loss: 6.8137e-10 - val_loss: 1.8852e-10
Epoch 511/512
512/512 - 0s - loss: 6.7272e-10 - val_loss: 1.8843e-10
Epoch 512/512
512/512 - 0s - loss: 6.6957e-10 - val_loss: 1.8900e-10
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5780e-08 - val_loss: 1.0705e-09
Epoch 2/512

Epoch 00002: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 3.1048e-10 - val_loss: 1.7553e-11
Epoch 3/512

Epoch 00003: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.7086e-11 - val_loss: 1.6782e-11
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.6516e-11 - val_loss: 1.6315e-11
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.6025e-11 - val_loss: 1.5896e-11
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5582e-11 - val_loss: 1.5436e-11
Epoch 7/512

Epoch 00007: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5160e-11 - val_loss: 1.5062e-11
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5200e-11 - val_loss: 1.7448e-11
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0264e-11 - val_loss: 2.3137e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8657e-09 - val_loss: 5.7535e-09
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6717e-09 - val_loss: 4.9580e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4437e-10 - val_loss: 6.2815e-11
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9004e-11 - val_loss: 3.7922e-11
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9076e-11 - val_loss: 4.7152e-11
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9253e-11 - val_loss: 1.4698e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4230e-10 - val_loss: 9.6132e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5012e-09 - val_loss: 1.5357e-09
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0760e-09 - val_loss: 4.2413e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9539e-10 - val_loss: 1.6623e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5123e-10 - val_loss: 1.4705e-10
Epoch 21/512

Epoch 00021: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7390e-10 - val_loss: 2.4805e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4960e-10 - val_loss: 5.4903e-10
Epoch 23/512

Epoch 00023: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6542e-10 - val_loss: 7.1111e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3742e-10 - val_loss: 4.5041e-10
Epoch 25/512

Epoch 00025: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7480e-10 - val_loss: 2.6759e-10
Epoch 26/512

Epoch 00026: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4461e-10 - val_loss: 2.1988e-10
Epoch 27/512

Epoch 00027: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2914e-10 - val_loss: 2.5263e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8474e-10 - val_loss: 3.3669e-10
Epoch 29/512

Epoch 00029: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6875e-10 - val_loss: 3.8771e-10
Epoch 30/512

Epoch 00030: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8427e-10 - val_loss: 3.4911e-10
Epoch 31/512

Epoch 00031: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2757e-10 - val_loss: 2.7780e-10
Epoch 32/512

Epoch 00032: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6217e-10 - val_loss: 2.3366e-10
Epoch 33/512

Epoch 00033: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2948e-10 - val_loss: 2.2304e-10
Epoch 34/512

Epoch 00034: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2965e-10 - val_loss: 2.3617e-10
Epoch 35/512

Epoch 00035: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4537e-10 - val_loss: 2.5160e-10
Epoch 36/512

Epoch 00036: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5747e-10 - val_loss: 2.5530e-10
Epoch 37/512

Epoch 00037: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5418e-10 - val_loss: 2.4064e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3492e-10 - val_loss: 2.1876e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1389e-10 - val_loss: 2.0084e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9973e-10 - val_loss: 1.9337e-10
Epoch 41/512

Epoch 00041: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9453e-10 - val_loss: 1.9367e-10
Epoch 42/512

Epoch 00042: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9566e-10 - val_loss: 1.9383e-10
Epoch 43/512

Epoch 00043: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9534e-10 - val_loss: 1.9207e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9010e-10 - val_loss: 1.8230e-10
Epoch 45/512

Epoch 00045: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8185e-10 - val_loss: 1.7729e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7660e-10 - val_loss: 1.7073e-10
Epoch 47/512

Epoch 00047: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6896e-10 - val_loss: 1.6356e-10
Epoch 48/512

Epoch 00048: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6395e-10 - val_loss: 1.6084e-10
Epoch 49/512

Epoch 00049: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6158e-10 - val_loss: 1.5857e-10
Epoch 50/512

Epoch 00050: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5896e-10 - val_loss: 1.5504e-10
Epoch 51/512

Epoch 00051: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5566e-10 - val_loss: 1.5284e-10
Epoch 52/512

Epoch 00052: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5195e-10 - val_loss: 1.4756e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4665e-10 - val_loss: 1.4173e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4007e-10 - val_loss: 1.3597e-10
Epoch 55/512

Epoch 00055: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3466e-10 - val_loss: 1.3059e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3026e-10 - val_loss: 1.2791e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2791e-10 - val_loss: 1.2684e-10
Epoch 58/512

Epoch 00058: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2693e-10 - val_loss: 1.2555e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2534e-10 - val_loss: 1.2358e-10
Epoch 60/512

Epoch 00060: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2346e-10 - val_loss: 1.2114e-10
Epoch 61/512

Epoch 00061: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2086e-10 - val_loss: 1.1912e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1900e-10 - val_loss: 1.1601e-10
Epoch 63/512

Epoch 00063: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1455e-10 - val_loss: 1.1141e-10
Epoch 64/512

Epoch 00064: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1047e-10 - val_loss: 1.0689e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0707e-10 - val_loss: 1.0529e-10
Epoch 66/512

Epoch 00066: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0509e-10 - val_loss: 1.0379e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0378e-10 - val_loss: 1.0228e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0238e-10 - val_loss: 1.0135e-10
Epoch 69/512

Epoch 00069: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0061e-10 - val_loss: 9.8392e-11
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7659e-11 - val_loss: 9.6652e-11
Epoch 71/512

Epoch 00071: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6711e-11 - val_loss: 9.5433e-11
Epoch 72/512

Epoch 00072: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5355e-11 - val_loss: 9.3830e-11
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4677e-11 - val_loss: 9.3970e-11
Epoch 74/512

Epoch 00074: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3290e-11 - val_loss: 9.1678e-11
Epoch 75/512

Epoch 00075: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.0895e-11 - val_loss: 8.8081e-11
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7032e-11 - val_loss: 8.5885e-11
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4813e-11 - val_loss: 8.3032e-11
Epoch 78/512

Epoch 00078: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2136e-11 - val_loss: 8.0042e-11
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0109e-11 - val_loss: 8.0189e-11
Epoch 80/512

Epoch 00080: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0695e-11 - val_loss: 8.0475e-11
Epoch 81/512

Epoch 00081: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9423e-11 - val_loss: 7.8151e-11
Epoch 82/512

Epoch 00082: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7778e-11 - val_loss: 7.7627e-11
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7906e-11 - val_loss: 7.7532e-11
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6938e-11 - val_loss: 7.5959e-11
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6023e-11 - val_loss: 7.5095e-11
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4300e-11 - val_loss: 7.2579e-11
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1677e-11 - val_loss: 7.0004e-11
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9653e-11 - val_loss: 6.9252e-11
Epoch 89/512

Epoch 00089: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8691e-11 - val_loss: 6.7997e-11
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7740e-11 - val_loss: 6.6848e-11
Epoch 91/512

Epoch 00091: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6455e-11 - val_loss: 6.5858e-11
Epoch 92/512

Epoch 00092: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6290e-11 - val_loss: 6.6679e-11
Epoch 93/512

Epoch 00093: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6235e-11 - val_loss: 6.5305e-11
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4971e-11 - val_loss: 6.4335e-11
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3762e-11 - val_loss: 6.2867e-11
Epoch 96/512

Epoch 00096: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2827e-11 - val_loss: 6.2155e-11
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1935e-11 - val_loss: 6.0947e-11
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0467e-11 - val_loss: 5.9800e-11
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9721e-11 - val_loss: 5.9428e-11
Epoch 100/512

Epoch 00100: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9277e-11 - val_loss: 5.8127e-11
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8312e-11 - val_loss: 5.7928e-11
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7944e-11 - val_loss: 5.7071e-11
Epoch 103/512

Epoch 00103: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6537e-11 - val_loss: 5.5672e-11
Epoch 104/512

Epoch 00104: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5812e-11 - val_loss: 5.5430e-11
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5329e-11 - val_loss: 5.4741e-11
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4128e-11 - val_loss: 5.3865e-11
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3856e-11 - val_loss: 5.2443e-11
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.2473e-11 - val_loss: 5.1926e-11
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1729e-11 - val_loss: 5.1209e-11
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0485e-11 - val_loss: 4.9565e-11
Epoch 111/512

Epoch 00111: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9363e-11 - val_loss: 4.9105e-11
Epoch 112/512

Epoch 00112: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8603e-11 - val_loss: 4.8276e-11
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8003e-11 - val_loss: 4.7523e-11
Epoch 114/512

Epoch 00114: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7306e-11 - val_loss: 4.7303e-11
Epoch 115/512

Epoch 00115: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7074e-11 - val_loss: 4.6758e-11
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6914e-11 - val_loss: 4.7208e-11
Epoch 117/512

Epoch 00117: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7075e-11 - val_loss: 4.6595e-11
Epoch 118/512

Epoch 00118: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6524e-11 - val_loss: 4.6401e-11
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5958e-11 - val_loss: 4.5214e-11
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4840e-11 - val_loss: 4.4202e-11
Epoch 121/512

Epoch 00121: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4056e-11 - val_loss: 4.4371e-11
Epoch 122/512

Epoch 00122: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4328e-11 - val_loss: 4.3633e-11
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3576e-11 - val_loss: 4.3306e-11
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2698e-11 - val_loss: 4.1749e-11
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0787e-11 - val_loss: 4.0001e-11
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0340e-11 - val_loss: 4.0299e-11
Epoch 127/512

Epoch 00127: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0503e-11 - val_loss: 4.0863e-11
Epoch 128/512

Epoch 00128: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0798e-11 - val_loss: 4.0936e-11
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.0100e-11 - val_loss: 3.9636e-11
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9706e-11 - val_loss: 3.9433e-11
Epoch 131/512

Epoch 00131: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9334e-11 - val_loss: 3.9437e-11
Epoch 132/512

Epoch 00132: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9178e-11 - val_loss: 3.9186e-11
Epoch 133/512

Epoch 00133: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9318e-11 - val_loss: 3.9154e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8600e-11 - val_loss: 3.7881e-11
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6949e-11 - val_loss: 3.5997e-11
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6247e-11 - val_loss: 3.5851e-11
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5827e-11 - val_loss: 3.5851e-11
Epoch 138/512

Epoch 00138: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5525e-11 - val_loss: 3.5095e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4908e-11 - val_loss: 3.4446e-11
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3992e-11 - val_loss: 3.3839e-11
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3471e-11 - val_loss: 3.3107e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3635e-11 - val_loss: 3.4682e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4493e-11 - val_loss: 3.4254e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4636e-11 - val_loss: 3.4892e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5039e-11 - val_loss: 3.5084e-11
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4935e-11 - val_loss: 3.4672e-11
Epoch 147/512

Epoch 00147: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4119e-11 - val_loss: 3.3462e-11
Epoch 148/512

Epoch 00148: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2964e-11 - val_loss: 3.2408e-11
Epoch 149/512

Epoch 00149: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2264e-11 - val_loss: 3.2027e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2130e-11 - val_loss: 3.2782e-11
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2542e-11 - val_loss: 3.2113e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.1571e-11 - val_loss: 3.0972e-11
Epoch 153/512

Epoch 00153: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0806e-11 - val_loss: 3.0458e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.0126e-11 - val_loss: 3.0084e-11
Epoch 155/512

Epoch 00155: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9886e-11 - val_loss: 2.9825e-11
Epoch 156/512

Epoch 00156: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9306e-11 - val_loss: 2.9453e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8834e-11 - val_loss: 2.8613e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8962e-11 - val_loss: 2.9607e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.9165e-11 - val_loss: 2.8866e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8420e-11 - val_loss: 2.8069e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7860e-11 - val_loss: 2.7851e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7859e-11 - val_loss: 2.8614e-11
Epoch 163/512

Epoch 00163: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8338e-11 - val_loss: 2.8093e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7519e-11 - val_loss: 2.7428e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7161e-11 - val_loss: 2.6942e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6559e-11 - val_loss: 2.6184e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6159e-11 - val_loss: 2.6163e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5807e-11 - val_loss: 2.5385e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5352e-11 - val_loss: 2.5754e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5601e-11 - val_loss: 2.5871e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6031e-11 - val_loss: 2.6407e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6568e-11 - val_loss: 2.6544e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.6215e-11 - val_loss: 2.5681e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5270e-11 - val_loss: 2.5084e-11
Epoch 175/512

Epoch 00175: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4572e-11 - val_loss: 2.4197e-11
Epoch 176/512

Epoch 00176: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3721e-11 - val_loss: 2.3550e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3433e-11 - val_loss: 2.3493e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3059e-11 - val_loss: 2.2878e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2652e-11 - val_loss: 2.2807e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2927e-11 - val_loss: 2.3724e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3655e-11 - val_loss: 2.4010e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4308e-11 - val_loss: 2.4512e-11
Epoch 183/512

Epoch 00183: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4039e-11 - val_loss: 2.4103e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3642e-11 - val_loss: 2.3596e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.3217e-11 - val_loss: 2.3172e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2758e-11 - val_loss: 2.2273e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2015e-11 - val_loss: 2.1974e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1915e-11 - val_loss: 2.2371e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2004e-11 - val_loss: 2.1931e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1866e-11 - val_loss: 2.1920e-11
Epoch 191/512

Epoch 00191: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1905e-11 - val_loss: 2.2119e-11
Epoch 192/512

Epoch 00192: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1759e-11 - val_loss: 2.1754e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1238e-11 - val_loss: 2.1211e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1071e-11 - val_loss: 2.0937e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0416e-11 - val_loss: 2.0429e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9967e-11 - val_loss: 1.9843e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9457e-11 - val_loss: 1.9036e-11
Epoch 198/512

Epoch 00198: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8541e-11 - val_loss: 1.8451e-11
Epoch 199/512

Epoch 00199: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8583e-11 - val_loss: 1.9433e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9546e-11 - val_loss: 1.9483e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9648e-11 - val_loss: 2.0405e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0758e-11 - val_loss: 2.1445e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1288e-11 - val_loss: 2.1496e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0987e-11 - val_loss: 2.0548e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0194e-11 - val_loss: 2.0165e-11
Epoch 206/512

Epoch 00206: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9798e-11 - val_loss: 1.9557e-11
Epoch 207/512

Epoch 00207: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9343e-11 - val_loss: 1.9124e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8586e-11 - val_loss: 1.8427e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8484e-11 - val_loss: 1.8669e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8364e-11 - val_loss: 1.8178e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7874e-11 - val_loss: 1.8223e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8146e-11 - val_loss: 1.8516e-11
Epoch 213/512

Epoch 00213: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8532e-11 - val_loss: 1.8617e-11
Epoch 214/512

Epoch 00214: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8260e-11 - val_loss: 1.8343e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8228e-11 - val_loss: 1.8480e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8354e-11 - val_loss: 1.8368e-11
Epoch 217/512

Epoch 00217: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7989e-11 - val_loss: 1.7861e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7454e-11 - val_loss: 1.7437e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7267e-11 - val_loss: 1.7236e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7089e-11 - val_loss: 1.6879e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6438e-11 - val_loss: 1.6381e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6176e-11 - val_loss: 1.6038e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5933e-11 - val_loss: 1.6449e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6216e-11 - val_loss: 1.6358e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6498e-11 - val_loss: 1.7127e-11
Epoch 226/512

Epoch 00226: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7247e-11 - val_loss: 1.7678e-11
Epoch 227/512

Epoch 00227: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7778e-11 - val_loss: 1.8269e-11
Epoch 228/512

Epoch 00228: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8214e-11 - val_loss: 1.8293e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7791e-11 - val_loss: 1.7576e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7327e-11 - val_loss: 1.7151e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6602e-11 - val_loss: 1.6472e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6008e-11 - val_loss: 1.5341e-11
Epoch 233/512

Epoch 00233: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.5070e-11 - val_loss: 1.5029e-11
Epoch 234/512

Epoch 00234: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4613e-11 - val_loss: 1.4714e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4727e-11 - val_loss: 1.5121e-11
Epoch 236/512

Epoch 00236: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4719e-11 - val_loss: 1.4638e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4426e-11 - val_loss: 1.4352e-11
Epoch 238/512

Epoch 00238: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4559e-11 - val_loss: 1.5321e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5393e-11 - val_loss: 1.5536e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5367e-11 - val_loss: 1.5441e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5131e-11 - val_loss: 1.5159e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5210e-11 - val_loss: 1.5568e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5410e-11 - val_loss: 1.5461e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5552e-11 - val_loss: 1.5662e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5339e-11 - val_loss: 1.5122e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4758e-11 - val_loss: 1.4489e-11
Epoch 247/512

Epoch 00247: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4175e-11 - val_loss: 1.4343e-11
Epoch 248/512

Epoch 00248: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.4073e-11 - val_loss: 1.4077e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3718e-11 - val_loss: 1.3717e-11
Epoch 250/512

Epoch 00250: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3516e-11 - val_loss: 1.3578e-11
Epoch 251/512

Epoch 00251: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3326e-11 - val_loss: 1.3415e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3440e-11 - val_loss: 1.4008e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4348e-11 - val_loss: 1.5057e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4775e-11 - val_loss: 1.4831e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4708e-11 - val_loss: 1.4833e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4699e-11 - val_loss: 1.4950e-11
Epoch 257/512

Epoch 00257: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4807e-11 - val_loss: 1.4972e-11
Epoch 258/512

Epoch 00258: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4893e-11 - val_loss: 1.5194e-11
Epoch 259/512

Epoch 00259: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4899e-11 - val_loss: 1.4476e-11
Epoch 260/512

Epoch 00260: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4122e-11 - val_loss: 1.4109e-11
Epoch 261/512

Epoch 00261: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3659e-11 - val_loss: 1.3433e-11
Epoch 262/512

Epoch 00262: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3283e-11 - val_loss: 1.3491e-11
Epoch 263/512

Epoch 00263: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.3203e-11 - val_loss: 1.3059e-11
Epoch 264/512

Epoch 00264: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2891e-11 - val_loss: 1.3041e-11
Epoch 265/512

Epoch 00265: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2648e-11 - val_loss: 1.2472e-11
Epoch 266/512

Epoch 00266: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.2250e-11 - val_loss: 1.2186e-11
Epoch 267/512

Epoch 00267: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2173e-11 - val_loss: 1.2511e-11
Epoch 268/512

Epoch 00268: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2566e-11 - val_loss: 1.2843e-11
Epoch 269/512

Epoch 00269: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2630e-11 - val_loss: 1.2693e-11
Epoch 270/512

Epoch 00270: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2719e-11 - val_loss: 1.3134e-11
Epoch 271/512

Epoch 00271: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2812e-11 - val_loss: 1.2621e-11
Epoch 272/512

Epoch 00272: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2544e-11 - val_loss: 1.2792e-11
Epoch 273/512

Epoch 00273: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2863e-11 - val_loss: 1.3329e-11
Epoch 274/512

Epoch 00274: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3334e-11 - val_loss: 1.3773e-11
Epoch 275/512

Epoch 00275: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3464e-11 - val_loss: 1.3361e-11
Epoch 276/512

Epoch 00276: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3341e-11 - val_loss: 1.3585e-11
Epoch 277/512

Epoch 00277: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3416e-11 - val_loss: 1.3407e-11
Epoch 278/512

Epoch 00278: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3388e-11 - val_loss: 1.3887e-11
Epoch 279/512

Epoch 00279: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3848e-11 - val_loss: 1.4088e-11
Epoch 280/512

Epoch 00280: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3693e-11 - val_loss: 1.3698e-11
Epoch 281/512

Epoch 00281: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3278e-11 - val_loss: 1.3047e-11
Epoch 282/512

Epoch 00282: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2656e-11 - val_loss: 1.2654e-11
Epoch 283/512

Epoch 00283: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2309e-11 - val_loss: 1.2352e-11
Epoch 284/512

Epoch 00284: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2314e-11 - val_loss: 1.2588e-11
Epoch 285/512

Epoch 00285: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2439e-11 - val_loss: 1.2335e-11
Epoch 286/512

Epoch 00286: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1977e-11 - val_loss: 1.1967e-11
Epoch 287/512

Epoch 00287: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1745e-11 - val_loss: 1.1826e-11
Epoch 288/512

Epoch 00288: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1519e-11 - val_loss: 1.1634e-11
Epoch 289/512

Epoch 00289: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1377e-11 - val_loss: 1.1564e-11
Epoch 290/512

Epoch 00290: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1596e-11 - val_loss: 1.2065e-11
Epoch 291/512

Epoch 00291: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2110e-11 - val_loss: 1.2292e-11
Epoch 292/512

Epoch 00292: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2122e-11 - val_loss: 1.1940e-11
Epoch 293/512

Epoch 00293: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1822e-11 - val_loss: 1.1854e-11
Epoch 294/512

Epoch 00294: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1632e-11 - val_loss: 1.1614e-11
Epoch 295/512

Epoch 00295: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1443e-11 - val_loss: 1.1575e-11
Epoch 296/512

Epoch 00296: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1602e-11 - val_loss: 1.1981e-11
Epoch 297/512

Epoch 00297: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1839e-11 - val_loss: 1.1915e-11
Epoch 298/512

Epoch 00298: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1791e-11 - val_loss: 1.1876e-11
Epoch 299/512

Epoch 00299: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1652e-11 - val_loss: 1.1694e-11
Epoch 300/512

Epoch 00300: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1418e-11 - val_loss: 1.1386e-11
Epoch 301/512

Epoch 00301: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1277e-11 - val_loss: 1.1423e-11
Epoch 302/512

Epoch 00302: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1335e-11 - val_loss: 1.1640e-11
Epoch 303/512

Epoch 00303: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1596e-11 - val_loss: 1.1789e-11
Epoch 304/512

Epoch 00304: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1590e-11 - val_loss: 1.1419e-11
Epoch 305/512

Epoch 00305: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.1130e-11 - val_loss: 1.0974e-11
Epoch 306/512

Epoch 00306: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0612e-11 - val_loss: 1.0262e-11
Epoch 307/512

Epoch 00307: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 1.0067e-11 - val_loss: 1.0149e-11
Epoch 308/512

Epoch 00308: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.9054e-12 - val_loss: 9.8549e-12
Epoch 309/512

Epoch 00309: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6606e-12 - val_loss: 9.8817e-12
Epoch 310/512

Epoch 00310: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7955e-12 - val_loss: 1.0064e-11
Epoch 311/512

Epoch 00311: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7699e-12 - val_loss: 9.8612e-12
Epoch 312/512

Epoch 00312: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8226e-12 - val_loss: 1.0087e-11
Epoch 313/512

Epoch 00313: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8547e-12 - val_loss: 9.8585e-12
Epoch 314/512

Epoch 00314: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7755e-12 - val_loss: 1.0132e-11
Epoch 315/512

Epoch 00315: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0013e-11 - val_loss: 1.0231e-11
Epoch 316/512

Epoch 00316: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0282e-11 - val_loss: 1.0591e-11
Epoch 317/512

Epoch 00317: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0692e-11 - val_loss: 1.1113e-11
Epoch 318/512

Epoch 00318: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0641e-11 - val_loss: 1.0455e-11
Epoch 319/512

Epoch 00319: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0163e-11 - val_loss: 1.0002e-11
Epoch 320/512

Epoch 00320: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.6997e-12 - val_loss: 9.6995e-12
Epoch 321/512

Epoch 00321: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6781e-12 - val_loss: 9.9886e-12
Epoch 322/512

Epoch 00322: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9099e-12 - val_loss: 1.0035e-11
Epoch 323/512

Epoch 00323: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8022e-12 - val_loss: 9.9162e-12
Epoch 324/512

Epoch 00324: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9283e-12 - val_loss: 1.0230e-11
Epoch 325/512

Epoch 00325: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0012e-11 - val_loss: 9.9296e-12
Epoch 326/512

Epoch 00326: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7734e-12 - val_loss: 9.9458e-12
Epoch 327/512

Epoch 00327: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9083e-12 - val_loss: 1.0009e-11
Epoch 328/512

Epoch 00328: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8440e-12 - val_loss: 9.8308e-12
Epoch 329/512

Epoch 00329: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7554e-12 - val_loss: 9.8653e-12
Epoch 330/512

Epoch 00330: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.5347e-12 - val_loss: 9.3808e-12
Epoch 331/512

Epoch 00331: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2834e-12 - val_loss: 9.4954e-12
Epoch 332/512

Epoch 00332: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.3484e-12 - val_loss: 9.2709e-12
Epoch 333/512

Epoch 00333: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1409e-12 - val_loss: 9.2723e-12
Epoch 334/512

Epoch 00334: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2475e-12 - val_loss: 9.5042e-12
Epoch 335/512

Epoch 00335: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3179e-12 - val_loss: 9.3307e-12
Epoch 336/512

Epoch 00336: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2457e-12 - val_loss: 9.5724e-12
Epoch 337/512

Epoch 00337: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4118e-12 - val_loss: 9.4589e-12
Epoch 338/512

Epoch 00338: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.2400e-12 - val_loss: 9.4389e-12
Epoch 339/512

Epoch 00339: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4306e-12 - val_loss: 9.6529e-12
Epoch 340/512

Epoch 00340: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.5279e-12 - val_loss: 9.6352e-12
Epoch 341/512

Epoch 00341: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3583e-12 - val_loss: 9.4340e-12
Epoch 342/512

Epoch 00342: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 9.2288e-12 - val_loss: 9.2355e-12
Epoch 343/512

Epoch 00343: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.9076e-12 - val_loss: 8.8768e-12
Epoch 344/512

Epoch 00344: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7305e-12 - val_loss: 9.0272e-12
Epoch 345/512

Epoch 00345: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.8268e-12 - val_loss: 8.8284e-12
Epoch 346/512

Epoch 00346: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7136e-12 - val_loss: 8.9894e-12
Epoch 347/512

Epoch 00347: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8740e-12 - val_loss: 8.9863e-12
Epoch 348/512

Epoch 00348: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7219e-12 - val_loss: 8.8591e-12
Epoch 349/512

Epoch 00349: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8057e-12 - val_loss: 9.3266e-12
Epoch 350/512

Epoch 00350: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4825e-12 - val_loss: 9.8395e-12
Epoch 351/512

Epoch 00351: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8709e-12 - val_loss: 1.0205e-11
Epoch 352/512

Epoch 00352: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0213e-11 - val_loss: 1.0619e-11
Epoch 353/512

Epoch 00353: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0542e-11 - val_loss: 1.0589e-11
Epoch 354/512

Epoch 00354: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0243e-11 - val_loss: 1.0274e-11
Epoch 355/512

Epoch 00355: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0151e-11 - val_loss: 1.0177e-11
Epoch 356/512

Epoch 00356: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.8269e-12 - val_loss: 9.7467e-12
Epoch 357/512

Epoch 00357: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6563e-12 - val_loss: 9.8311e-12
Epoch 358/512

Epoch 00358: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6768e-12 - val_loss: 9.5742e-12
Epoch 359/512

Epoch 00359: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.3236e-12 - val_loss: 9.1500e-12
Epoch 360/512

Epoch 00360: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.8920e-12 - val_loss: 9.1005e-12
Epoch 361/512

Epoch 00361: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9062e-12 - val_loss: 8.8996e-12
Epoch 362/512

Epoch 00362: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.6860e-12 - val_loss: 8.7001e-12
Epoch 363/512

Epoch 00363: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4706e-12 - val_loss: 8.8456e-12
Epoch 364/512

Epoch 00364: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7017e-12 - val_loss: 8.7072e-12
Epoch 365/512

Epoch 00365: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.4979e-12 - val_loss: 8.4913e-12
Epoch 366/512

Epoch 00366: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.2463e-12 - val_loss: 8.3673e-12
Epoch 367/512

Epoch 00367: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.3378e-12 - val_loss: 8.5892e-12
Epoch 368/512

Epoch 00368: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5063e-12 - val_loss: 8.6822e-12
Epoch 369/512

Epoch 00369: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6082e-12 - val_loss: 8.7950e-12
Epoch 370/512

Epoch 00370: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6040e-12 - val_loss: 8.7890e-12
Epoch 371/512

Epoch 00371: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6198e-12 - val_loss: 8.7028e-12
Epoch 372/512

Epoch 00372: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6295e-12 - val_loss: 8.7759e-12
Epoch 373/512

Epoch 00373: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6483e-12 - val_loss: 8.7460e-12
Epoch 374/512

Epoch 00374: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5805e-12 - val_loss: 8.7037e-12
Epoch 375/512

Epoch 00375: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 8.2901e-12 - val_loss: 8.2463e-12
Epoch 376/512

Epoch 00376: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.9302e-12 - val_loss: 7.9281e-12
Epoch 377/512

Epoch 00377: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.6792e-12 - val_loss: 7.6613e-12
Epoch 378/512

Epoch 00378: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5376e-12 - val_loss: 7.7238e-12
Epoch 379/512

Epoch 00379: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5591e-12 - val_loss: 7.6931e-12
Epoch 380/512

Epoch 00380: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5481e-12 - val_loss: 7.7132e-12
Epoch 381/512

Epoch 00381: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5844e-12 - val_loss: 7.8417e-12
Epoch 382/512

Epoch 00382: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7573e-12 - val_loss: 7.9525e-12
Epoch 383/512

Epoch 00383: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7031e-12 - val_loss: 7.7074e-12
Epoch 384/512

Epoch 00384: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.5107e-12 - val_loss: 7.5217e-12
Epoch 385/512

Epoch 00385: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.3405e-12 - val_loss: 7.3628e-12
Epoch 386/512

Epoch 00386: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2248e-12 - val_loss: 7.4660e-12
Epoch 387/512

Epoch 00387: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4531e-12 - val_loss: 7.8740e-12
Epoch 388/512

Epoch 00388: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8293e-12 - val_loss: 8.1101e-12
Epoch 389/512

Epoch 00389: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9976e-12 - val_loss: 8.3742e-12
Epoch 390/512

Epoch 00390: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2402e-12 - val_loss: 8.4833e-12
Epoch 391/512

Epoch 00391: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4050e-12 - val_loss: 8.5970e-12
Epoch 392/512

Epoch 00392: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6567e-12 - val_loss: 9.0695e-12
Epoch 393/512

Epoch 00393: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9303e-12 - val_loss: 8.9792e-12
Epoch 394/512

Epoch 00394: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.7682e-12 - val_loss: 8.7875e-12
Epoch 395/512

Epoch 00395: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5882e-12 - val_loss: 8.6049e-12
Epoch 396/512

Epoch 00396: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2872e-12 - val_loss: 8.2073e-12
Epoch 397/512

Epoch 00397: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9056e-12 - val_loss: 7.7954e-12
Epoch 398/512

Epoch 00398: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5850e-12 - val_loss: 7.7648e-12
Epoch 399/512

Epoch 00399: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5228e-12 - val_loss: 7.6701e-12
Epoch 400/512

Epoch 00400: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5992e-12 - val_loss: 7.8022e-12
Epoch 401/512

Epoch 00401: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5917e-12 - val_loss: 7.6395e-12
Epoch 402/512

Epoch 00402: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5204e-12 - val_loss: 7.6970e-12
Epoch 403/512

Epoch 00403: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6321e-12 - val_loss: 7.8431e-12
Epoch 404/512

Epoch 00404: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7321e-12 - val_loss: 7.8999e-12
Epoch 405/512

Epoch 00405: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7360e-12 - val_loss: 7.8863e-12
Epoch 406/512

Epoch 00406: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5751e-12 - val_loss: 7.3807e-12
Epoch 407/512

Epoch 00407: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 7.1129e-12 - val_loss: 7.0783e-12
Epoch 408/512

Epoch 00408: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.8501e-12 - val_loss: 6.9532e-12
Epoch 409/512

Epoch 00409: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.7900e-12 - val_loss: 6.9370e-12
Epoch 410/512

Epoch 00410: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7982e-12 - val_loss: 6.9449e-12
Epoch 411/512

Epoch 00411: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.7960e-12 - val_loss: 6.8818e-12
Epoch 412/512

Epoch 00412: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7691e-12 - val_loss: 7.0003e-12
Epoch 413/512

Epoch 00413: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9907e-12 - val_loss: 7.2053e-12
Epoch 414/512

Epoch 00414: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0958e-12 - val_loss: 7.1844e-12
Epoch 415/512

Epoch 00415: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0627e-12 - val_loss: 7.2860e-12
Epoch 416/512

Epoch 00416: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2195e-12 - val_loss: 7.4683e-12
Epoch 417/512

Epoch 00417: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3979e-12 - val_loss: 7.5136e-12
Epoch 418/512

Epoch 00418: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2854e-12 - val_loss: 7.2702e-12
Epoch 419/512

Epoch 00419: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0084e-12 - val_loss: 7.0313e-12
Epoch 420/512

Epoch 00420: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8806e-12 - val_loss: 7.0777e-12
Epoch 421/512

Epoch 00421: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.8402e-12 - val_loss: 6.8810e-12
Epoch 422/512

Epoch 00422: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7097e-12 - val_loss: 6.9289e-12
Epoch 423/512

Epoch 00423: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.7622e-12 - val_loss: 6.8778e-12
Epoch 424/512

Epoch 00424: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.7587e-12 - val_loss: 6.8059e-12
Epoch 425/512

Epoch 00425: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7089e-12 - val_loss: 6.9155e-12
Epoch 426/512

Epoch 00426: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8123e-12 - val_loss: 7.0451e-12
Epoch 427/512

Epoch 00427: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8398e-12 - val_loss: 7.0814e-12
Epoch 428/512

Epoch 00428: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9278e-12 - val_loss: 7.1142e-12
Epoch 429/512

Epoch 00429: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9210e-12 - val_loss: 7.0352e-12
Epoch 430/512

Epoch 00430: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8722e-12 - val_loss: 7.0049e-12
Epoch 431/512

Epoch 00431: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8422e-12 - val_loss: 6.8737e-12
Epoch 432/512

Epoch 00432: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8226e-12 - val_loss: 7.1160e-12
Epoch 433/512

Epoch 00433: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0068e-12 - val_loss: 7.1748e-12
Epoch 434/512

Epoch 00434: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1162e-12 - val_loss: 7.4437e-12
Epoch 435/512

Epoch 00435: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2449e-12 - val_loss: 7.2972e-12
Epoch 436/512

Epoch 00436: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2020e-12 - val_loss: 7.3554e-12
Epoch 437/512

Epoch 00437: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2324e-12 - val_loss: 7.3369e-12
Epoch 438/512

Epoch 00438: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0791e-12 - val_loss: 7.0777e-12
Epoch 439/512

Epoch 00439: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9375e-12 - val_loss: 7.1255e-12
Epoch 440/512

Epoch 00440: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0098e-12 - val_loss: 7.3466e-12
Epoch 441/512

Epoch 00441: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3891e-12 - val_loss: 7.7259e-12
Epoch 442/512

Epoch 00442: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6869e-12 - val_loss: 7.9814e-12
Epoch 443/512

Epoch 00443: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7543e-12 - val_loss: 7.7990e-12
Epoch 444/512

Epoch 00444: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6104e-12 - val_loss: 7.6953e-12
Epoch 445/512

Epoch 00445: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5920e-12 - val_loss: 7.7926e-12
Epoch 446/512

Epoch 00446: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6935e-12 - val_loss: 7.8792e-12
Epoch 447/512

Epoch 00447: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7151e-12 - val_loss: 7.7720e-12
Epoch 448/512

Epoch 00448: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5155e-12 - val_loss: 7.4618e-12
Epoch 449/512

Epoch 00449: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2946e-12 - val_loss: 7.2905e-12
Epoch 450/512

Epoch 00450: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0327e-12 - val_loss: 6.9949e-12
Epoch 451/512

Epoch 00451: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9416e-12 - val_loss: 7.0972e-12
Epoch 452/512

Epoch 00452: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9983e-12 - val_loss: 7.1748e-12
Epoch 453/512

Epoch 00453: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9792e-12 - val_loss: 7.0890e-12
Epoch 454/512

Epoch 00454: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0399e-12 - val_loss: 7.2911e-12
Epoch 455/512

Epoch 00455: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1631e-12 - val_loss: 7.2100e-12
Epoch 456/512

Epoch 00456: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0042e-12 - val_loss: 6.9883e-12
Epoch 457/512

Epoch 00457: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8112e-12 - val_loss: 6.8876e-12
Epoch 458/512

Epoch 00458: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.6293e-12 - val_loss: 6.6024e-12
Epoch 459/512

Epoch 00459: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5372e-12 - val_loss: 6.6973e-12
Epoch 460/512

Epoch 00460: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.5007e-12 - val_loss: 6.5754e-12
Epoch 461/512

Epoch 00461: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4717e-12 - val_loss: 6.6940e-12
Epoch 462/512

Epoch 00462: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6210e-12 - val_loss: 6.7650e-12
Epoch 463/512

Epoch 00463: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.5437e-12 - val_loss: 6.5473e-12
Epoch 464/512

Epoch 00464: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.3783e-12 - val_loss: 6.3724e-12
Epoch 465/512

Epoch 00465: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2550e-12 - val_loss: 6.4904e-12
Epoch 466/512

Epoch 00466: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4448e-12 - val_loss: 6.6742e-12
Epoch 467/512

Epoch 00467: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5668e-12 - val_loss: 6.7600e-12
Epoch 468/512

Epoch 00468: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5888e-12 - val_loss: 6.7040e-12
Epoch 469/512

Epoch 00469: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4649e-12 - val_loss: 6.5039e-12
Epoch 470/512

Epoch 00470: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3650e-12 - val_loss: 6.4812e-12
Epoch 471/512

Epoch 00471: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 6.2028e-12 - val_loss: 6.1532e-12
Epoch 472/512

Epoch 00472: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.8884e-12 - val_loss: 5.8144e-12
Epoch 473/512

Epoch 00473: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.5944e-12 - val_loss: 5.7162e-12
Epoch 474/512

Epoch 00474: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6157e-12 - val_loss: 5.8315e-12
Epoch 475/512

Epoch 00475: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8051e-12 - val_loss: 6.1000e-12
Epoch 476/512

Epoch 00476: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0086e-12 - val_loss: 6.1939e-12
Epoch 477/512

Epoch 00477: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1696e-12 - val_loss: 6.5281e-12
Epoch 478/512

Epoch 00478: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4791e-12 - val_loss: 6.7670e-12
Epoch 479/512

Epoch 00479: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6608e-12 - val_loss: 6.8344e-12
Epoch 480/512

Epoch 00480: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6904e-12 - val_loss: 6.8489e-12
Epoch 481/512

Epoch 00481: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7590e-12 - val_loss: 6.9230e-12
Epoch 482/512

Epoch 00482: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8311e-12 - val_loss: 6.9839e-12
Epoch 483/512

Epoch 00483: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8495e-12 - val_loss: 6.9409e-12
Epoch 484/512

Epoch 00484: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8493e-12 - val_loss: 6.9027e-12
Epoch 485/512

Epoch 00485: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7915e-12 - val_loss: 6.8127e-12
Epoch 486/512

Epoch 00486: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5079e-12 - val_loss: 6.4548e-12
Epoch 487/512

Epoch 00487: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2119e-12 - val_loss: 6.1149e-12
Epoch 488/512

Epoch 00488: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8788e-12 - val_loss: 5.8040e-12
Epoch 489/512

Epoch 00489: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6125e-12 - val_loss: 5.8206e-12
Epoch 490/512

Epoch 00490: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7619e-12 - val_loss: 6.0828e-12
Epoch 491/512

Epoch 00491: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9946e-12 - val_loss: 6.1011e-12
Epoch 492/512

Epoch 00492: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9399e-12 - val_loss: 6.0966e-12
Epoch 493/512

Epoch 00493: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0471e-12 - val_loss: 6.2865e-12
Epoch 494/512

Epoch 00494: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1953e-12 - val_loss: 6.4287e-12
Epoch 495/512

Epoch 00495: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2917e-12 - val_loss: 6.4320e-12
Epoch 496/512

Epoch 00496: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2739e-12 - val_loss: 6.4055e-12
Epoch 497/512

Epoch 00497: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3156e-12 - val_loss: 6.6252e-12
Epoch 498/512

Epoch 00498: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6055e-12 - val_loss: 6.9089e-12
Epoch 499/512

Epoch 00499: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8009e-12 - val_loss: 7.0197e-12
Epoch 500/512

Epoch 00500: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9344e-12 - val_loss: 6.8396e-12
Epoch 501/512

Epoch 00501: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4281e-12 - val_loss: 6.3490e-12
Epoch 502/512

Epoch 00502: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1379e-12 - val_loss: 6.2335e-12
Epoch 503/512

Epoch 00503: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0820e-12 - val_loss: 6.1229e-12
Epoch 504/512

Epoch 00504: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8487e-12 - val_loss: 5.7646e-12
Epoch 505/512

Epoch 00505: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.5032e-12 - val_loss: 5.5158e-12
Epoch 506/512

Epoch 00506: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-ma-rate-0.3-cuvre-secp224r1/multiplication_weights.h5
512/512 - 0s - loss: 5.2988e-12 - val_loss: 5.4161e-12
Epoch 507/512

Epoch 00507: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3158e-12 - val_loss: 5.5362e-12
Epoch 508/512

Epoch 00508: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5180e-12 - val_loss: 5.7719e-12
Epoch 509/512

Epoch 00509: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.6682e-12 - val_loss: 5.8645e-12
Epoch 510/512

Epoch 00510: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7561e-12 - val_loss: 5.9812e-12
Epoch 511/512

Epoch 00511: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9567e-12 - val_loss: 6.1924e-12
Epoch 512/512

Epoch 00512: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0682e-12 - val_loss: 6.1672e-12
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?
Epoch   0:   0% | abe: 10.275 | eve: 9.794 | bob: 10.163Epoch   0:   0% | abe: 10.222 | eve: 9.786 | bob: 10.110Epoch   0:   1% | abe: 10.168 | eve: 9.783 | bob: 10.056Epoch   0:   2% | abe: 10.145 | eve: 9.793 | bob: 10.033Epoch   0:   3% | abe: 10.094 | eve: 9.792 | bob: 9.982Epoch   0:   3% | abe: 10.063 | eve: 9.791 | bob: 9.950Epoch   0:   4% | abe: 10.016 | eve: 9.794 | bob: 9.903Epoch   0:   5% | abe: 9.984 | eve: 9.790 | bob: 9.872Epoch   0:   6% | abe: 9.954 | eve: 9.796 | bob: 9.842Epoch   0:   7% | abe: 9.919 | eve: 9.805 | bob: 9.809Epoch   0:   7% | abe: 9.887 | eve: 9.809 | bob: 9.780Epoch   0:   8% | abe: 9.854 | eve: 9.808 | bob: 9.750Epoch   0:   9% | abe: 9.817 | eve: 9.817 | bob: 9.716Epoch   0:  10% | abe: 9.787 | eve: 9.818 | bob: 9.689Epoch   0:  10% | abe: 9.759 | eve: 9.826 | bob: 9.665Epoch   0:  11% | abe: 9.732 | eve: 9.822 | bob: 9.642Epoch   0:  12% | abe: 9.708 | eve: 9.822 | bob: 9.621Epoch   0:  13% | abe: 9.689 | eve: 9.822 | bob: 9.605Epoch   0:  14% | abe: 9.671 | eve: 9.822 | bob: 9.590Epoch   0:  14% | abe: 9.649 | eve: 9.821 | bob: 9.570Epoch   0:  15% | abe: 9.633 | eve: 9.821 | bob: 9.557Epoch   0:  16% | abe: 9.613 | eve: 9.823 | bob: 9.538Epoch   0:  17% | abe: 9.593 | eve: 9.821 | bob: 9.520Epoch   0:  17% | abe: 9.576 | eve: 9.820 | bob: 9.504Epoch   0:  18% | abe: 9.562 | eve: 9.819 | bob: 9.491Epoch   0:  19% | abe: 9.548 | eve: 9.822 | bob: 9.478Epoch   0:  20% | abe: 9.534 | eve: 9.823 | bob: 9.466Epoch   0:  21% | abe: 9.520 | eve: 9.825 | bob: 9.452Epoch   0:  21% | abe: 9.508 | eve: 9.824 | bob: 9.441Epoch   0:  22% | abe: 9.496 | eve: 9.825 | bob: 9.430Epoch   0:  23% | abe: 9.483 | eve: 9.829 | bob: 9.418Epoch   0:  24% | abe: 9.472 | eve: 9.829 | bob: 9.408Epoch   0:  25% | abe: 9.463 | eve: 9.831 | bob: 9.400Epoch   0:  25% | abe: 9.454 | eve: 9.834 | bob: 9.391Epoch   0:  26% | abe: 9.442 | eve: 9.835 | bob: 9.380Epoch   0:  27% | abe: 9.433 | eve: 9.834 | bob: 9.371Epoch   0:  28% | abe: 9.424 | eve: 9.835 | bob: 9.362Epoch   0:  28% | abe: 9.417 | eve: 9.835 | bob: 9.355Epoch   0:  29% | abe: 9.408 | eve: 9.838 | bob: 9.347Epoch   0:  30% | abe: 9.399 | eve: 9.839 | bob: 9.339Epoch   0:  31% | abe: 9.392 | eve: 9.841 | bob: 9.332Epoch   0:  32% | abe: 9.385 | eve: 9.840 | bob: 9.325Epoch   0:  32% | abe: 9.380 | eve: 9.843 | bob: 9.320Epoch   0:  33% | abe: 9.373 | eve: 9.844 | bob: 9.314Epoch   0:  34% | abe: 9.366 | eve: 9.845 | bob: 9.306Epoch   0:  35% | abe: 9.359 | eve: 9.846 | bob: 9.300Epoch   0:  35% | abe: 9.353 | eve: 9.848 | bob: 9.294Epoch   0:  36% | abe: 9.347 | eve: 9.848 | bob: 9.288Epoch   0:  37% | abe: 9.342 | eve: 9.851 | bob: 9.283Epoch   0:  38% | abe: 9.337 | eve: 9.850 | bob: 9.279Epoch   0:  39% | abe: 9.332 | eve: 9.851 | bob: 9.273Epoch   0:  39% | abe: 9.328 | eve: 9.853 | bob: 9.270Epoch   0:  40% | abe: 9.324 | eve: 9.854 | bob: 9.266Epoch   0:  41% | abe: 9.320 | eve: 9.854 | bob: 9.262Epoch   0:  42% | abe: 9.316 | eve: 9.855 | bob: 9.258Epoch   0:  42% | abe: 9.313 | eve: 9.857 | bob: 9.255Epoch   0:  43% | abe: 9.308 | eve: 9.857 | bob: 9.251Epoch   0:  44% | abe: 9.304 | eve: 9.857 | bob: 9.247Epoch   0:  45% | abe: 9.300 | eve: 9.858 | bob: 9.243Epoch   0:  46% | abe: 9.296 | eve: 9.860 | bob: 9.239Epoch   0:  46% | abe: 9.293 | eve: 9.861 | bob: 9.236Epoch   0:  47% | abe: 9.289 | eve: 9.862 | bob: 9.232Epoch   0:  48% | abe: 9.286 | eve: 9.863 | bob: 9.230Epoch   0:  49% | abe: 9.283 | eve: 9.864 | bob: 9.227Epoch   0:  50% | abe: 9.280 | eve: 9.865 | bob: 9.223Epoch   0:  50% | abe: 9.277 | eve: 9.866 | bob: 9.220Epoch   0:  51% | abe: 9.273 | eve: 9.867 | bob: 9.217Epoch   0:  52% | abe: 9.270 | eve: 9.867 | bob: 9.214Epoch   0:  53% | abe: 9.268 | eve: 9.866 | bob: 9.211Epoch   0:  53% | abe: 9.265 | eve: 9.869 | bob: 9.209Epoch   0:  54% | abe: 9.263 | eve: 9.869 | bob: 9.207Epoch   0:  55% | abe: 9.260 | eve: 9.872 | bob: 9.204Epoch   0:  56% | abe: 9.257 | eve: 9.873 | bob: 9.201Epoch   0:  57% | abe: 9.254 | eve: 9.874 | bob: 9.198Epoch   0:  57% | abe: 9.252 | eve: 9.875 | bob: 9.195Epoch   0:  58% | abe: 9.249 | eve: 9.876 | bob: 9.193Epoch   0:  59% | abe: 9.247 | eve: 9.877 | bob: 9.190Epoch   0:  60% | abe: 9.244 | eve: 9.878 | bob: 9.188Epoch   0:  60% | abe: 9.243 | eve: 9.879 | bob: 9.187Epoch   0:  61% | abe: 9.241 | eve: 9.880 | bob: 9.185Epoch   0:  62% | abe: 9.239 | eve: 9.880 | bob: 9.183Epoch   0:  63% | abe: 9.237 | eve: 9.881 | bob: 9.181Epoch   0:  64% | abe: 9.235 | eve: 9.881 | bob: 9.179Epoch   0:  64% | abe: 9.233 | eve: 9.881 | bob: 9.177Epoch   0:  65% | abe: 9.231 | eve: 9.883 | bob: 9.175Epoch   0:  66% | abe: 9.229 | eve: 9.883 | bob: 9.173Epoch   0:  67% | abe: 9.227 | eve: 9.884 | bob: 9.171Epoch   0:  67% | abe: 9.225 | eve: 9.885 | bob: 9.169Epoch   0:  68% | abe: 9.224 | eve: 9.887 | bob: 9.168Epoch   0:  69% | abe: 9.222 | eve: 9.887 | bob: 9.166Epoch   0:  70% | abe: 9.221 | eve: 9.889 | bob: 9.165Epoch   0:  71% | abe: 9.219 | eve: 9.889 | bob: 9.163Epoch   0:  71% | abe: 9.217 | eve: 9.890 | bob: 9.162Epoch   0:  72% | abe: 9.215 | eve: 9.892 | bob: 9.159Epoch   0:  73% | abe: 9.213 | eve: 9.893 | bob: 9.158Epoch   0:  74% | abe: 9.212 | eve: 9.894 | bob: 9.156Epoch   0:  75% | abe: 9.211 | eve: 9.895 | bob: 9.155Epoch   0:  75% | abe: 9.209 | eve: 9.896 | bob: 9.153Epoch   0:  76% | abe: 9.207 | eve: 9.897 | bob: 9.152Epoch   0:  77% | abe: 9.206 | eve: 9.898 | bob: 9.150Epoch   0:  78% | abe: 9.205 | eve: 9.899 | bob: 9.149Epoch   0:  78% | abe: 9.204 | eve: 9.900 | bob: 9.148Epoch   0:  79% | abe: 9.202 | eve: 9.901 | bob: 9.146Epoch   0:  80% | abe: 9.200 | eve: 9.901 | bob: 9.145Epoch   0:  81% | abe: 9.199 | eve: 9.902 | bob: 9.143Epoch   0:  82% | abe: 9.198 | eve: 9.902 | bob: 9.142Epoch   0:  82% | abe: 9.197 | eve: 9.904 | bob: 9.141Epoch   0:  83% | abe: 9.196 | eve: 9.904 | bob: 9.140Epoch   0:  84% | abe: 9.195 | eve: 9.905 | bob: 9.139Epoch   0:  85% | abe: 9.194 | eve: 9.906 | bob: 9.138Epoch   0:  85% | abe: 9.193 | eve: 9.906 | bob: 9.137Epoch   0:  86% | abe: 9.191 | eve: 9.908 | bob: 9.136Epoch   0:  87% | abe: 9.190 | eve: 9.909 | bob: 9.134Epoch   0:  88% | abe: 9.190 | eve: 9.910 | bob: 9.134Epoch   0:  89% | abe: 9.189 | eve: 9.911 | bob: 9.133Epoch   0:  89% | abe: 9.188 | eve: 9.912 | bob: 9.133Epoch   0:  90% | abe: 9.187 | eve: 9.913 | bob: 9.132Epoch   0:  91% | abe: 9.186 | eve: 9.913 | bob: 9.131Epoch   0:  92% | abe: 9.185 | eve: 9.914 | bob: 9.129Epoch   0:  92% | abe: 9.184 | eve: 9.914 | bob: 9.128Epoch   0:  93% | abe: 9.183 | eve: 9.915 | bob: 9.128Epoch   0:  94% | abe: 9.182 | eve: 9.916 | bob: 9.127Epoch   0:  95% | abe: 9.182 | eve: 9.916 | bob: 9.126Epoch   0:  96% | abe: 9.181 | eve: 9.917 | bob: 9.126Epoch   0:  96% | abe: 9.180 | eve: 9.917 | bob: 9.124Epoch   0:  97% | abe: 9.179 | eve: 9.918 | bob: 9.124Epoch   0:  98% | abe: 9.178 | eve: 9.919 | bob: 9.123Epoch   0:  99% | abe: 9.177 | eve: 9.920 | bob: 9.122
New best Bob loss 9.121570067383573 at epoch 0
Epoch   1:   0% | abe: 9.043 | eve: 10.072 | bob: 8.989Epoch   1:   0% | abe: 9.074 | eve: 10.015 | bob: 9.020Epoch   1:   1% | abe: 9.075 | eve: 10.033 | bob: 9.020Epoch   1:   2% | abe: 9.085 | eve: 10.025 | bob: 9.031Epoch   1:   3% | abe: 9.083 | eve: 10.015 | bob: 9.030Epoch   1:   3% | abe: 9.075 | eve: 10.016 | bob: 9.021Epoch   1:   4% | abe: 9.078 | eve: 10.007 | bob: 9.025Epoch   1:   5% | abe: 9.070 | eve: 10.003 | bob: 9.016Epoch   1:   6% | abe: 9.064 | eve: 9.999 | bob: 9.011Epoch   1:   7% | abe: 9.065 | eve: 9.987 | bob: 9.011Epoch   1:   7% | abe: 9.065 | eve: 9.986 | bob: 9.012Epoch   1:   8% | abe: 9.065 | eve: 9.994 | bob: 9.011Epoch   1:   9% | abe: 9.064 | eve: 9.992 | bob: 9.010Epoch   1:  10% | abe: 9.061 | eve: 9.991 | bob: 9.007Epoch   1:  10% | abe: 9.065 | eve: 9.992 | bob: 9.011Epoch   1:  11% | abe: 9.066 | eve: 9.994 | bob: 9.012Epoch   1:  12% | abe: 9.069 | eve: 9.994 | bob: 9.015Epoch   1:  13% | abe: 9.066 | eve: 9.994 | bob: 9.012Epoch   1:  14% | abe: 9.063 | eve: 9.992 | bob: 9.009Epoch   1:  14% | abe: 9.062 | eve: 9.995 | bob: 9.009Epoch   1:  15% | abe: 9.063 | eve: 9.995 | bob: 9.009Epoch   1:  16% | abe: 9.062 | eve: 9.995 | bob: 9.008Epoch   1:  17% | abe: 9.064 | eve: 9.995 | bob: 9.010Epoch   1:  17% | abe: 9.066 | eve: 9.996 | bob: 9.012Epoch   1:  18% | abe: 9.065 | eve: 9.999 | bob: 9.011Epoch   1:  19% | abe: 9.063 | eve: 9.999 | bob: 9.009Epoch   1:  20% | abe: 9.063 | eve: 9.998 | bob: 9.009Epoch   1:  21% | abe: 9.063 | eve: 9.998 | bob: 9.009Epoch   1:  21% | abe: 9.065 | eve: 9.999 | bob: 9.011Epoch   1:  22% | abe: 9.064 | eve: 10.001 | bob: 9.010Epoch   1:  23% | abe: 9.063 | eve: 10.002 | bob: 9.009Epoch   1:  24% | abe: 9.065 | eve: 10.001 | bob: 9.011Epoch   1:  25% | abe: 9.063 | eve: 10.002 | bob: 9.008Epoch   1:  25% | abe: 9.061 | eve: 10.006 | bob: 9.007Epoch   1:  26% | abe: 9.061 | eve: 10.007 | bob: 9.007Epoch   1:  27% | abe: 9.062 | eve: 10.008 | bob: 9.007Epoch   1:  28% | abe: 9.063 | eve: 10.010 | bob: 9.008Epoch   1:  28% | abe: 9.063 | eve: 10.011 | bob: 9.009Epoch   1:  29% | abe: 9.063 | eve: 10.012 | bob: 9.009Epoch   1:  30% | abe: 9.063 | eve: 10.013 | bob: 9.008Epoch   1:  31% | abe: 9.062 | eve: 10.016 | bob: 9.007Epoch   1:  32% | abe: 9.061 | eve: 10.015 | bob: 9.007Epoch   1:  32% | abe: 9.061 | eve: 10.018 | bob: 9.007Epoch   1:  33% | abe: 9.063 | eve: 10.016 | bob: 9.009Epoch   1:  34% | abe: 9.062 | eve: 10.018 | bob: 9.008Epoch   1:  35% | abe: 9.062 | eve: 10.018 | bob: 9.008Epoch   1:  35% | abe: 9.063 | eve: 10.019 | bob: 9.008Epoch   1:  36% | abe: 9.064 | eve: 10.019 | bob: 9.009Epoch   1:  37% | abe: 9.063 | eve: 10.019 | bob: 9.009Epoch   1:  38% | abe: 9.064 | eve: 10.020 | bob: 9.010Epoch   1:  39% | abe: 9.065 | eve: 10.022 | bob: 9.011Epoch   1:  39% | abe: 9.066 | eve: 10.022 | bob: 9.011Epoch   1:  40% | abe: 9.066 | eve: 10.024 | bob: 9.012Epoch   1:  41% | abe: 9.065 | eve: 10.023 | bob: 9.010Epoch   1:  42% | abe: 9.064 | eve: 10.024 | bob: 9.010Epoch   1:  42% | abe: 9.064 | eve: 10.025 | bob: 9.010Epoch   1:  43% | abe: 9.064 | eve: 10.025 | bob: 9.010Epoch   1:  44% | abe: 9.064 | eve: 10.025 | bob: 9.010Epoch   1:  45% | abe: 9.065 | eve: 10.026 | bob: 9.010Epoch   1:  46% | abe: 9.066 | eve: 10.027 | bob: 9.011Epoch   1:  46% | abe: 9.065 | eve: 10.027 | bob: 9.011Epoch   1:  47% | abe: 9.065 | eve: 10.029 | bob: 9.011Epoch   1:  48% | abe: 9.066 | eve: 10.030 | bob: 9.012Epoch   1:  49% | abe: 9.065 | eve: 10.030 | bob: 9.011Epoch   1:  50% | abe: 9.065 | eve: 10.031 | bob: 9.011Epoch   1:  50% | abe: 9.064 | eve: 10.033 | bob: 9.010Epoch   1:  51% | abe: 9.064 | eve: 10.033 | bob: 9.010Epoch   1:  52% | abe: 9.065 | eve: 10.033 | bob: 9.011Epoch   1:  53% | abe: 9.065 | eve: 10.033 | bob: 9.010Epoch   1:  53% | abe: 9.064 | eve: 10.033 | bob: 9.010Epoch   1:  54% | abe: 9.064 | eve: 10.033 | bob: 9.009Epoch   1:  55% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  56% | abe: 9.063 | eve: 10.033 | bob: 9.009Epoch   1:  57% | abe: 9.064 | eve: 10.035 | bob: 9.009Epoch   1:  57% | abe: 9.064 | eve: 10.034 | bob: 9.010Epoch   1:  58% | abe: 9.064 | eve: 10.034 | bob: 9.009Epoch   1:  59% | abe: 9.064 | eve: 10.035 | bob: 9.010Epoch   1:  60% | abe: 9.064 | eve: 10.034 | bob: 9.010Epoch   1:  60% | abe: 9.064 | eve: 10.033 | bob: 9.010Epoch   1:  61% | abe: 9.064 | eve: 10.033 | bob: 9.010Epoch   1:  62% | abe: 9.064 | eve: 10.033 | bob: 9.009Epoch   1:  63% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  64% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  64% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  65% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  66% | abe: 9.063 | eve: 10.034 | bob: 9.009Epoch   1:  67% | abe: 9.064 | eve: 10.034 | bob: 9.009Epoch   1:  67% | abe: 9.063 | eve: 10.035 | bob: 9.009Epoch   1:  68% | abe: 9.063 | eve: 10.035 | bob: 9.008Epoch   1:  69% | abe: 9.063 | eve: 10.036 | bob: 9.008Epoch   1:  70% | abe: 9.062 | eve: 10.036 | bob: 9.008Epoch   1:  71% | abe: 9.062 | eve: 10.035 | bob: 9.007Epoch   1:  71% | abe: 9.063 | eve: 10.036 | bob: 9.008Epoch   1:  72% | abe: 9.062 | eve: 10.036 | bob: 9.007Epoch   1:  73% | abe: 9.062 | eve: 10.036 | bob: 9.007Epoch   1:  74% | abe: 9.062 | eve: 10.037 | bob: 9.007Epoch   1:  75% | abe: 9.062 | eve: 10.037 | bob: 9.007Epoch   1:  75% | abe: 9.062 | eve: 10.037 | bob: 9.007Epoch   1:  76% | abe: 9.062 | eve: 10.038 | bob: 9.007Epoch   1:  77% | abe: 9.062 | eve: 10.039 | bob: 9.007Epoch   1:  78% | abe: 9.062 | eve: 10.038 | bob: 9.007Epoch   1:  78% | abe: 9.062 | eve: 10.038 | bob: 9.008Epoch   1:  79% | abe: 9.063 | eve: 10.038 | bob: 9.008Epoch   1:  80% | abe: 9.063 | eve: 10.038 | bob: 9.008Epoch   1:  81% | abe: 9.063 | eve: 10.039 | bob: 9.008Epoch   1:  82% | abe: 9.062 | eve: 10.039 | bob: 9.008Epoch   1:  82% | abe: 9.062 | eve: 10.039 | bob: 9.008Epoch   1:  83% | abe: 9.062 | eve: 10.039 | bob: 9.008Epoch   1:  84% | abe: 9.062 | eve: 10.039 | bob: 9.007Epoch   1:  85% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  85% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  86% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  87% | abe: 9.061 | eve: 10.040 | bob: 9.006Epoch   1:  88% | abe: 9.060 | eve: 10.039 | bob: 9.006Epoch   1:  89% | abe: 9.061 | eve: 10.040 | bob: 9.006Epoch   1:  89% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  90% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  91% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  92% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  92% | abe: 9.061 | eve: 10.038 | bob: 9.007Epoch   1:  93% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  94% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  95% | abe: 9.061 | eve: 10.039 | bob: 9.006Epoch   1:  96% | abe: 9.061 | eve: 10.040 | bob: 9.007Epoch   1:  96% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  97% | abe: 9.060 | eve: 10.039 | bob: 9.006Epoch   1:  98% | abe: 9.061 | eve: 10.039 | bob: 9.007Epoch   1:  99% | abe: 9.061 | eve: 10.039 | bob: 9.007
New best Bob loss 9.006625865825072 at epoch 1
Epoch   2:   0% | abe: 9.012 | eve: 10.033 | bob: 8.960Epoch   2:   0% | abe: 9.040 | eve: 10.062 | bob: 8.988Epoch   2:   1% | abe: 9.038 | eve: 10.037 | bob: 8.985Epoch   2:   2% | abe: 9.040 | eve: 10.033 | bob: 8.988Epoch   2:   3% | abe: 9.038 | eve: 10.039 | bob: 8.986Epoch   2:   3% | abe: 9.040 | eve: 10.041 | bob: 8.988Epoch   2:   4% | abe: 9.044 | eve: 10.041 | bob: 8.992Epoch   2:   5% | abe: 9.043 | eve: 10.035 | bob: 8.992Epoch   2:   6% | abe: 9.047 | eve: 10.041 | bob: 8.996Epoch   2:   7% | abe: 9.049 | eve: 10.040 | bob: 8.998Epoch   2:   7% | abe: 9.054 | eve: 10.044 | bob: 9.003Epoch   2:   8% | abe: 9.057 | eve: 10.050 | bob: 9.006Epoch   2:   9% | abe: 9.054 | eve: 10.047 | bob: 9.002Epoch   2:  10% | abe: 9.053 | eve: 10.044 | bob: 9.001Epoch   2:  10% | abe: 9.055 | eve: 10.046 | bob: 9.003Epoch   2:  11% | abe: 9.058 | eve: 10.050 | bob: 9.007Epoch   2:  12% | abe: 9.055 | eve: 10.052 | bob: 9.004Epoch   2:  13% | abe: 9.052 | eve: 10.054 | bob: 9.001Epoch   2:  14% | abe: 9.055 | eve: 10.056 | bob: 9.003Epoch   2:  14% | abe: 9.056 | eve: 10.059 | bob: 9.004Epoch   2:  15% | abe: 9.053 | eve: 10.058 | bob: 9.002Epoch   2:  16% | abe: 9.050 | eve: 10.057 | bob: 8.999Epoch   2:  17% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  17% | abe: 9.051 | eve: 10.057 | bob: 9.000Epoch   2:  18% | abe: 9.052 | eve: 10.057 | bob: 9.001Epoch   2:  19% | abe: 9.053 | eve: 10.055 | bob: 9.001Epoch   2:  20% | abe: 9.052 | eve: 10.055 | bob: 9.000Epoch   2:  21% | abe: 9.051 | eve: 10.056 | bob: 9.000Epoch   2:  21% | abe: 9.051 | eve: 10.056 | bob: 9.000Epoch   2:  22% | abe: 9.052 | eve: 10.052 | bob: 9.000Epoch   2:  23% | abe: 9.051 | eve: 10.052 | bob: 9.000Epoch   2:  24% | abe: 9.052 | eve: 10.052 | bob: 9.000Epoch   2:  25% | abe: 9.052 | eve: 10.054 | bob: 9.000Epoch   2:  25% | abe: 9.052 | eve: 10.054 | bob: 9.001Epoch   2:  26% | abe: 9.052 | eve: 10.054 | bob: 9.000Epoch   2:  27% | abe: 9.052 | eve: 10.053 | bob: 9.000Epoch   2:  28% | abe: 9.053 | eve: 10.053 | bob: 9.001Epoch   2:  28% | abe: 9.053 | eve: 10.054 | bob: 9.001Epoch   2:  29% | abe: 9.052 | eve: 10.054 | bob: 9.001Epoch   2:  30% | abe: 9.052 | eve: 10.054 | bob: 9.000Epoch   2:  31% | abe: 9.052 | eve: 10.053 | bob: 9.000Epoch   2:  32% | abe: 9.051 | eve: 10.054 | bob: 9.000Epoch   2:  32% | abe: 9.051 | eve: 10.054 | bob: 8.999Epoch   2:  33% | abe: 9.050 | eve: 10.054 | bob: 8.998Epoch   2:  34% | abe: 9.050 | eve: 10.054 | bob: 8.999Epoch   2:  35% | abe: 9.050 | eve: 10.054 | bob: 8.998Epoch   2:  35% | abe: 9.050 | eve: 10.054 | bob: 8.999Epoch   2:  36% | abe: 9.051 | eve: 10.053 | bob: 9.000Epoch   2:  37% | abe: 9.052 | eve: 10.054 | bob: 9.000Epoch   2:  38% | abe: 9.052 | eve: 10.056 | bob: 9.000Epoch   2:  39% | abe: 9.052 | eve: 10.056 | bob: 9.000Epoch   2:  39% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  40% | abe: 9.050 | eve: 10.058 | bob: 8.999Epoch   2:  41% | abe: 9.051 | eve: 10.059 | bob: 9.000Epoch   2:  42% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  42% | abe: 9.052 | eve: 10.058 | bob: 9.001Epoch   2:  43% | abe: 9.053 | eve: 10.059 | bob: 9.001Epoch   2:  44% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  45% | abe: 9.052 | eve: 10.058 | bob: 9.000Epoch   2:  46% | abe: 9.051 | eve: 10.057 | bob: 9.000Epoch   2:  46% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  47% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  48% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  49% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  50% | abe: 9.051 | eve: 10.057 | bob: 8.999Epoch   2:  50% | abe: 9.051 | eve: 10.056 | bob: 8.999Epoch   2:  51% | abe: 9.051 | eve: 10.056 | bob: 8.999Epoch   2:  52% | abe: 9.051 | eve: 10.056 | bob: 8.999Epoch   2:  53% | abe: 9.051 | eve: 10.056 | bob: 8.999Epoch   2:  53% | abe: 9.052 | eve: 10.056 | bob: 9.000Epoch   2:  54% | abe: 9.052 | eve: 10.057 | bob: 9.000Epoch   2:  55% | abe: 9.052 | eve: 10.056 | bob: 9.001Epoch   2:  56% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  57% | abe: 9.052 | eve: 10.058 | bob: 9.001Epoch   2:  57% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  58% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  59% | abe: 9.051 | eve: 10.058 | bob: 9.000Epoch   2:  60% | abe: 9.051 | eve: 10.059 | bob: 9.000Epoch   2:  60% | abe: 9.052 | eve: 10.059 | bob: 9.001Epoch   2:  61% | abe: 9.051 | eve: 10.059 | bob: 9.000Epoch   2:  62% | abe: 9.051 | eve: 10.059 | bob: 9.000Epoch   2:  63% | abe: 9.051 | eve: 10.059 | bob: 9.000Epoch   2:  64% | abe: 9.051 | eve: 10.059 | bob: 8.999Epoch   2:  64% | abe: 9.050 | eve: 10.058 | bob: 8.999Epoch   2:  65% | abe: 9.051 | eve: 10.057 | bob: 9.000Epoch   2:  66% | abe: 9.051 | eve: 10.057 | bob: 9.000Epoch   2:  67% | abe: 9.051 | eve: 10.058 | bob: 9.000