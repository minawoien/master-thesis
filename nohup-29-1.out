WARNING:tensorflow:From /home/stud/minawoi/miniconda3/envs/conda-venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.
Instructions for updating:
If using Keras pass *_constraint arguments to layers.
2024-04-11 15:48:30.783982: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2024-04-11 15:48:30.858670: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:09:00.0
2024-04-11 15:48:30.859613: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 15:48:30.862168: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 15:48:30.864079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 15:48:30.864946: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 15:48:30.867880: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 15:48:30.870744: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 15:48:30.876832: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 15:48:30.886341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 15:48:30.886898: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
2024-04-11 15:48:30.898270: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199835000 Hz
2024-04-11 15:48:30.901180: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f0d670 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2024-04-11 15:48:30.901237: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2024-04-11 15:48:31.122876: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x48d6d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2024-04-11 15:48:31.123048: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2024-04-11 15:48:31.131465: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:09:00.0
2024-04-11 15:48:31.131570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 15:48:31.131617: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
2024-04-11 15:48:31.131653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0
2024-04-11 15:48:31.131688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0
2024-04-11 15:48:31.131724: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0
2024-04-11 15:48:31.131785: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0
2024-04-11 15:48:31.131853: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2024-04-11 15:48:31.138913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0
2024-04-11 15:48:31.139034: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0
2024-04-11 15:48:31.143102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:
2024-04-11 15:48:31.143148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 
2024-04-11 15:48:31.143167: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N 
2024-04-11 15:48:31.149983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 25692 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:09:00.0, compute capability: 7.0)
WARNING:tensorflow:Output bob missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob.
WARNING:tensorflow:Output bob_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to bob_1.
WARNING:tensorflow:Output eve missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve.
WARNING:tensorflow:Output eve_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to eve_1.
2024-04-11 15:48:34.845397: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0
Train on 512 samples, validate on 512 samples
Epoch 1/512
512/512 - 1s - loss: 0.3975 - val_loss: 8.4153e-04
Epoch 2/512
512/512 - 0s - loss: 0.0399 - val_loss: 5.2591e-05
Epoch 3/512
512/512 - 0s - loss: 0.0029 - val_loss: 1.3641e-05
Epoch 4/512
512/512 - 0s - loss: 0.0012 - val_loss: 9.6781e-06
Epoch 5/512
512/512 - 0s - loss: 8.5057e-04 - val_loss: 6.6622e-06
Epoch 6/512
512/512 - 0s - loss: 5.6999e-04 - val_loss: 4.2252e-06
Epoch 7/512
512/512 - 0s - loss: 3.4991e-04 - val_loss: 2.4214e-06
Epoch 8/512
512/512 - 0s - loss: 1.9278e-04 - val_loss: 1.2228e-06
Epoch 9/512
512/512 - 0s - loss: 9.2791e-05 - val_loss: 5.2669e-07
Epoch 10/512
512/512 - 0s - loss: 3.7702e-05 - val_loss: 1.8522e-07
Epoch 11/512
512/512 - 0s - loss: 1.2358e-05 - val_loss: 5.0315e-08
Epoch 12/512
512/512 - 0s - loss: 3.1592e-06 - val_loss: 1.8982e-08
Epoch 13/512
512/512 - 0s - loss: 4.5623e-05 - val_loss: 1.0170e-05
Epoch 14/512
512/512 - 0s - loss: 0.0046 - val_loss: 1.4626e-05
Epoch 15/512
512/512 - 0s - loss: 6.2625e-04 - val_loss: 1.0005e-06
Epoch 16/512
512/512 - 0s - loss: 9.7019e-05 - val_loss: 1.6546e-06
Epoch 17/512
512/512 - 0s - loss: 5.5994e-04 - val_loss: 2.2779e-05
Epoch 18/512
512/512 - 0s - loss: 0.0029 - val_loss: 1.3219e-05
Epoch 19/512
512/512 - 0s - loss: 8.1352e-04 - val_loss: 3.8839e-06
Epoch 20/512
512/512 - 0s - loss: 4.7893e-04 - val_loss: 8.6695e-06
Epoch 21/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.9710e-05
Epoch 22/512
512/512 - 0s - loss: 0.0016 - val_loss: 8.2024e-06
Epoch 23/512
512/512 - 0s - loss: 7.4437e-04 - val_loss: 7.2931e-06
Epoch 24/512
512/512 - 0s - loss: 9.8876e-04 - val_loss: 1.3643e-05
Epoch 25/512
512/512 - 0s - loss: 0.0015 - val_loss: 1.1592e-05
Epoch 26/512
512/512 - 0s - loss: 0.0010 - val_loss: 8.0372e-06
Epoch 27/512
512/512 - 0s - loss: 8.8268e-04 - val_loss: 1.0248e-05
Epoch 28/512
512/512 - 0s - loss: 0.0012 - val_loss: 1.1862e-05
Epoch 29/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.0008e-06
Epoch 30/512
512/512 - 0s - loss: 8.9982e-04 - val_loss: 8.9912e-06
Epoch 31/512
512/512 - 0s - loss: 0.0010 - val_loss: 1.0700e-05
Epoch 32/512
512/512 - 0s - loss: 0.0011 - val_loss: 9.6425e-06
Epoch 33/512
512/512 - 0s - loss: 9.4361e-04 - val_loss: 8.6429e-06
Epoch 34/512
512/512 - 0s - loss: 9.1681e-04 - val_loss: 9.4024e-06
Epoch 35/512
512/512 - 0s - loss: 9.9551e-04 - val_loss: 9.4935e-06
Epoch 36/512
512/512 - 0s - loss: 9.4970e-04 - val_loss: 8.6076e-06
Epoch 37/512
512/512 - 0s - loss: 8.8553e-04 - val_loss: 8.6635e-06
Epoch 38/512
512/512 - 0s - loss: 9.0692e-04 - val_loss: 9.2026e-06
Epoch 39/512
512/512 - 0s - loss: 9.4306e-04 - val_loss: 8.4286e-06
Epoch 40/512
512/512 - 0s - loss: 8.4573e-04 - val_loss: 8.1791e-06
Epoch 41/512
512/512 - 0s - loss: 8.6358e-04 - val_loss: 8.5905e-06
Epoch 42/512
512/512 - 0s - loss: 8.7972e-04 - val_loss: 8.4309e-06
Epoch 43/512
512/512 - 0s - loss: 8.4732e-04 - val_loss: 7.9826e-06
Epoch 44/512
512/512 - 0s - loss: 8.1607e-04 - val_loss: 8.0118e-06
Epoch 45/512
512/512 - 0s - loss: 8.2652e-04 - val_loss: 8.0423e-06
Epoch 46/512
512/512 - 0s - loss: 8.2287e-04 - val_loss: 7.7495e-06
Epoch 47/512
512/512 - 0s - loss: 7.8555e-04 - val_loss: 7.6418e-06
Epoch 48/512
512/512 - 0s - loss: 7.8770e-04 - val_loss: 7.7410e-06
Epoch 49/512
512/512 - 0s - loss: 7.8979e-04 - val_loss: 7.4864e-06
Epoch 50/512
512/512 - 0s - loss: 7.5330e-04 - val_loss: 7.3372e-06
Epoch 51/512
512/512 - 0s - loss: 7.5100e-04 - val_loss: 7.5343e-06
Epoch 52/512
512/512 - 0s - loss: 7.6227e-04 - val_loss: 7.2883e-06
Epoch 53/512
512/512 - 0s - loss: 7.3029e-04 - val_loss: 7.0348e-06
Epoch 54/512
512/512 - 0s - loss: 7.1734e-04 - val_loss: 7.0633e-06
Epoch 55/512
512/512 - 0s - loss: 7.1451e-04 - val_loss: 7.1740e-06
Epoch 56/512
512/512 - 0s - loss: 7.1814e-04 - val_loss: 6.9726e-06
Epoch 57/512
512/512 - 0s - loss: 6.9635e-04 - val_loss: 6.6637e-06
Epoch 58/512
512/512 - 0s - loss: 6.7540e-04 - val_loss: 6.7459e-06
Epoch 59/512
512/512 - 0s - loss: 6.8850e-04 - val_loss: 6.6930e-06
Epoch 60/512
512/512 - 0s - loss: 6.6821e-04 - val_loss: 6.5305e-06
Epoch 61/512
512/512 - 0s - loss: 6.5821e-04 - val_loss: 6.4911e-06
Epoch 62/512
512/512 - 0s - loss: 6.5439e-04 - val_loss: 6.3688e-06
Epoch 63/512
512/512 - 0s - loss: 6.4349e-04 - val_loss: 6.2842e-06
Epoch 64/512
512/512 - 0s - loss: 6.3520e-04 - val_loss: 6.1910e-06
Epoch 65/512
512/512 - 0s - loss: 6.2470e-04 - val_loss: 6.1582e-06
Epoch 66/512
512/512 - 0s - loss: 6.1811e-04 - val_loss: 6.1409e-06
Epoch 67/512
512/512 - 0s - loss: 6.1577e-04 - val_loss: 5.9997e-06
Epoch 68/512
512/512 - 0s - loss: 6.0156e-04 - val_loss: 5.8649e-06
Epoch 69/512
512/512 - 0s - loss: 5.9200e-04 - val_loss: 5.8495e-06
Epoch 70/512
512/512 - 0s - loss: 5.8873e-04 - val_loss: 5.8311e-06
Epoch 71/512
512/512 - 0s - loss: 5.8540e-04 - val_loss: 5.6226e-06
Epoch 72/512
512/512 - 0s - loss: 5.6387e-04 - val_loss: 5.5966e-06
Epoch 73/512
512/512 - 0s - loss: 5.6217e-04 - val_loss: 5.6897e-06
Epoch 74/512
512/512 - 0s - loss: 5.7351e-04 - val_loss: 5.3821e-06
Epoch 75/512
512/512 - 0s - loss: 5.3675e-04 - val_loss: 5.3049e-06
Epoch 76/512
512/512 - 0s - loss: 5.3868e-04 - val_loss: 5.4698e-06
Epoch 77/512
512/512 - 0s - loss: 5.4701e-04 - val_loss: 5.3390e-06
Epoch 78/512
512/512 - 0s - loss: 5.3094e-04 - val_loss: 5.0434e-06
Epoch 79/512
512/512 - 0s - loss: 5.0932e-04 - val_loss: 5.0791e-06
Epoch 80/512
512/512 - 0s - loss: 5.1420e-04 - val_loss: 5.2898e-06
Epoch 81/512
512/512 - 0s - loss: 5.2225e-04 - val_loss: 5.0518e-06
Epoch 82/512
512/512 - 0s - loss: 4.9507e-04 - val_loss: 4.8319e-06
Epoch 83/512
512/512 - 0s - loss: 4.8831e-04 - val_loss: 4.8985e-06
Epoch 84/512
512/512 - 0s - loss: 4.9035e-04 - val_loss: 4.9377e-06
Epoch 85/512
512/512 - 0s - loss: 4.8925e-04 - val_loss: 4.7271e-06
Epoch 86/512
512/512 - 0s - loss: 4.6788e-04 - val_loss: 4.6229e-06
Epoch 87/512
512/512 - 0s - loss: 4.6604e-04 - val_loss: 4.6965e-06
Epoch 88/512
512/512 - 0s - loss: 4.6808e-04 - val_loss: 4.5943e-06
Epoch 89/512
512/512 - 0s - loss: 4.5080e-04 - val_loss: 4.5259e-06
Epoch 90/512
512/512 - 0s - loss: 4.4872e-04 - val_loss: 4.5244e-06
Epoch 91/512
512/512 - 0s - loss: 4.4684e-04 - val_loss: 4.4202e-06
Epoch 92/512
512/512 - 0s - loss: 4.3637e-04 - val_loss: 4.2755e-06
Epoch 93/512
512/512 - 0s - loss: 4.2885e-04 - val_loss: 4.2270e-06
Epoch 94/512
512/512 - 0s - loss: 4.2067e-04 - val_loss: 4.2391e-06
Epoch 95/512
512/512 - 0s - loss: 4.2349e-04 - val_loss: 4.1506e-06
Epoch 96/512
512/512 - 0s - loss: 4.1202e-04 - val_loss: 3.9989e-06
Epoch 97/512
512/512 - 0s - loss: 3.9610e-04 - val_loss: 4.1032e-06
Epoch 98/512
512/512 - 0s - loss: 4.0940e-04 - val_loss: 4.0604e-06
Epoch 99/512
512/512 - 0s - loss: 3.9608e-04 - val_loss: 3.8219e-06
Epoch 100/512
512/512 - 0s - loss: 3.7981e-04 - val_loss: 3.8075e-06
Epoch 101/512
512/512 - 0s - loss: 3.8356e-04 - val_loss: 3.8248e-06
Epoch 102/512
512/512 - 0s - loss: 3.7693e-04 - val_loss: 3.7635e-06
Epoch 103/512
512/512 - 0s - loss: 3.6757e-04 - val_loss: 3.7438e-06
Epoch 104/512
512/512 - 0s - loss: 3.6761e-04 - val_loss: 3.6659e-06
Epoch 105/512
512/512 - 0s - loss: 3.6091e-04 - val_loss: 3.5334e-06
Epoch 106/512
512/512 - 0s - loss: 3.4653e-04 - val_loss: 3.5394e-06
Epoch 107/512
512/512 - 0s - loss: 3.5027e-04 - val_loss: 3.5210e-06
Epoch 108/512
512/512 - 0s - loss: 3.4775e-04 - val_loss: 3.3042e-06
Epoch 109/512
512/512 - 0s - loss: 3.2370e-04 - val_loss: 3.3520e-06
Epoch 110/512
512/512 - 0s - loss: 3.3580e-04 - val_loss: 3.4085e-06
Epoch 111/512
512/512 - 0s - loss: 3.3214e-04 - val_loss: 3.1690e-06
Epoch 112/512
512/512 - 0s - loss: 3.1310e-04 - val_loss: 3.0448e-06
Epoch 113/512
512/512 - 0s - loss: 3.0868e-04 - val_loss: 3.1433e-06
Epoch 114/512
512/512 - 0s - loss: 3.1579e-04 - val_loss: 3.0923e-06
Epoch 115/512
512/512 - 0s - loss: 3.0141e-04 - val_loss: 2.9556e-06
Epoch 116/512
512/512 - 0s - loss: 2.9299e-04 - val_loss: 2.9780e-06
Epoch 117/512
512/512 - 0s - loss: 2.9476e-04 - val_loss: 2.9663e-06
Epoch 118/512
512/512 - 0s - loss: 2.8987e-04 - val_loss: 2.8596e-06
Epoch 119/512
512/512 - 0s - loss: 2.7862e-04 - val_loss: 2.7923e-06
Epoch 120/512
512/512 - 0s - loss: 2.7643e-04 - val_loss: 2.7758e-06
Epoch 121/512
512/512 - 0s - loss: 2.7298e-04 - val_loss: 2.7085e-06
Epoch 122/512
512/512 - 0s - loss: 2.6720e-04 - val_loss: 2.6194e-06
Epoch 123/512
512/512 - 0s - loss: 2.5746e-04 - val_loss: 2.6099e-06
Epoch 124/512
512/512 - 0s - loss: 2.5763e-04 - val_loss: 2.5910e-06
Epoch 125/512
512/512 - 0s - loss: 2.5429e-04 - val_loss: 2.4644e-06
Epoch 126/512
512/512 - 0s - loss: 2.4194e-04 - val_loss: 2.4296e-06
Epoch 127/512
512/512 - 0s - loss: 2.3984e-04 - val_loss: 2.4810e-06
Epoch 128/512
512/512 - 0s - loss: 2.4305e-04 - val_loss: 2.3769e-06
Epoch 129/512
512/512 - 0s - loss: 2.2896e-04 - val_loss: 2.2559e-06
Epoch 130/512
512/512 - 0s - loss: 2.2361e-04 - val_loss: 2.2850e-06
Epoch 131/512
512/512 - 0s - loss: 2.2570e-04 - val_loss: 2.2381e-06
Epoch 132/512
512/512 - 0s - loss: 2.1671e-04 - val_loss: 2.1600e-06
Epoch 133/512
512/512 - 0s - loss: 2.1236e-04 - val_loss: 2.1368e-06
Epoch 134/512
512/512 - 0s - loss: 2.0878e-04 - val_loss: 2.1009e-06
Epoch 135/512
512/512 - 0s - loss: 2.0461e-04 - val_loss: 2.0435e-06
Epoch 136/512
512/512 - 0s - loss: 1.9918e-04 - val_loss: 1.9883e-06
Epoch 137/512
512/512 - 0s - loss: 1.9526e-04 - val_loss: 1.9347e-06
Epoch 138/512
512/512 - 0s - loss: 1.8979e-04 - val_loss: 1.9094e-06
Epoch 139/512
512/512 - 0s - loss: 1.8701e-04 - val_loss: 1.8856e-06
Epoch 140/512
512/512 - 0s - loss: 1.8218e-04 - val_loss: 1.8400e-06
Epoch 141/512
512/512 - 0s - loss: 1.7881e-04 - val_loss: 1.7833e-06
Epoch 142/512
512/512 - 0s - loss: 1.7435e-04 - val_loss: 1.7126e-06
Epoch 143/512
512/512 - 0s - loss: 1.6775e-04 - val_loss: 1.7029e-06
Epoch 144/512
512/512 - 0s - loss: 1.6739e-04 - val_loss: 1.6847e-06
Epoch 145/512
512/512 - 0s - loss: 1.6256e-04 - val_loss: 1.6158e-06
Epoch 146/512
512/512 - 0s - loss: 1.5742e-04 - val_loss: 1.5637e-06
Epoch 147/512
512/512 - 0s - loss: 1.5290e-04 - val_loss: 1.5604e-06
Epoch 148/512
512/512 - 0s - loss: 1.5158e-04 - val_loss: 1.5460e-06
Epoch 149/512
512/512 - 0s - loss: 1.4925e-04 - val_loss: 1.4561e-06
Epoch 150/512
512/512 - 0s - loss: 1.3998e-04 - val_loss: 1.4188e-06
Epoch 151/512
512/512 - 0s - loss: 1.4027e-04 - val_loss: 1.4132e-06
Epoch 152/512
512/512 - 0s - loss: 1.3711e-04 - val_loss: 1.3600e-06
Epoch 153/512
512/512 - 0s - loss: 1.3165e-04 - val_loss: 1.3181e-06
Epoch 154/512
512/512 - 0s - loss: 1.2881e-04 - val_loss: 1.2971e-06
Epoch 155/512
512/512 - 0s - loss: 1.2579e-04 - val_loss: 1.2828e-06
Epoch 156/512
512/512 - 0s - loss: 1.2416e-04 - val_loss: 1.2197e-06
Epoch 157/512
512/512 - 0s - loss: 1.1776e-04 - val_loss: 1.1703e-06
Epoch 158/512
512/512 - 0s - loss: 1.1485e-04 - val_loss: 1.1791e-06
Epoch 159/512
512/512 - 0s - loss: 1.1464e-04 - val_loss: 1.1569e-06
Epoch 160/512
512/512 - 0s - loss: 1.1114e-04 - val_loss: 1.0725e-06
Epoch 161/512
512/512 - 0s - loss: 1.0401e-04 - val_loss: 1.0585e-06
Epoch 162/512
512/512 - 0s - loss: 1.0496e-04 - val_loss: 1.0569e-06
Epoch 163/512
512/512 - 0s - loss: 1.0176e-04 - val_loss: 1.0125e-06
Epoch 164/512
512/512 - 0s - loss: 9.6830e-05 - val_loss: 9.9172e-07
Epoch 165/512
512/512 - 0s - loss: 9.6493e-05 - val_loss: 9.6196e-07
Epoch 166/512
512/512 - 0s - loss: 9.2939e-05 - val_loss: 9.1698e-07
Epoch 167/512
512/512 - 0s - loss: 8.8876e-05 - val_loss: 9.0479e-07
Epoch 168/512
512/512 - 0s - loss: 8.7602e-05 - val_loss: 9.0198e-07
Epoch 169/512
512/512 - 0s - loss: 8.5342e-05 - val_loss: 8.7161e-07
Epoch 170/512
512/512 - 0s - loss: 8.3248e-05 - val_loss: 8.2154e-07
Epoch 171/512
512/512 - 0s - loss: 7.8853e-05 - val_loss: 7.9619e-07
Epoch 172/512
512/512 - 0s - loss: 7.7656e-05 - val_loss: 7.8539e-07
Epoch 173/512
512/512 - 0s - loss: 7.5791e-05 - val_loss: 7.5807e-07
Epoch 174/512
512/512 - 0s - loss: 7.2885e-05 - val_loss: 7.2960e-07
Epoch 175/512
512/512 - 0s - loss: 7.0708e-05 - val_loss: 7.0795e-07
Epoch 176/512
512/512 - 0s - loss: 6.8662e-05 - val_loss: 6.8727e-07
Epoch 177/512
512/512 - 0s - loss: 6.5818e-05 - val_loss: 6.7677e-07
Epoch 178/512
512/512 - 0s - loss: 6.5778e-05 - val_loss: 6.4572e-07
Epoch 179/512
512/512 - 0s - loss: 6.1592e-05 - val_loss: 6.1519e-07
Epoch 180/512
512/512 - 0s - loss: 5.9837e-05 - val_loss: 6.1710e-07
Epoch 181/512
512/512 - 0s - loss: 5.9215e-05 - val_loss: 6.0296e-07
Epoch 182/512
512/512 - 0s - loss: 5.6815e-05 - val_loss: 5.7607e-07
Epoch 183/512
512/512 - 0s - loss: 5.4816e-05 - val_loss: 5.5293e-07
Epoch 184/512
512/512 - 0s - loss: 5.3123e-05 - val_loss: 5.3448e-07
Epoch 185/512
512/512 - 0s - loss: 5.1273e-05 - val_loss: 5.1879e-07
Epoch 186/512
512/512 - 0s - loss: 4.9571e-05 - val_loss: 5.0554e-07
Epoch 187/512
512/512 - 0s - loss: 4.8547e-05 - val_loss: 4.8431e-07
Epoch 188/512
512/512 - 0s - loss: 4.6108e-05 - val_loss: 4.6450e-07
Epoch 189/512
512/512 - 0s - loss: 4.4729e-05 - val_loss: 4.5722e-07
Epoch 190/512
512/512 - 0s - loss: 4.3936e-05 - val_loss: 4.3884e-07
Epoch 191/512
512/512 - 0s - loss: 4.1830e-05 - val_loss: 4.1958e-07
Epoch 192/512
512/512 - 0s - loss: 3.9977e-05 - val_loss: 4.1656e-07
Epoch 193/512
512/512 - 0s - loss: 3.9790e-05 - val_loss: 4.0320e-07
Epoch 194/512
512/512 - 0s - loss: 3.8101e-05 - val_loss: 3.7477e-07
Epoch 195/512
512/512 - 0s - loss: 3.5951e-05 - val_loss: 3.6292e-07
Epoch 196/512
512/512 - 0s - loss: 3.5193e-05 - val_loss: 3.6209e-07
Epoch 197/512
512/512 - 0s - loss: 3.4260e-05 - val_loss: 3.4955e-07
Epoch 198/512
512/512 - 0s - loss: 3.3139e-05 - val_loss: 3.2848e-07
Epoch 199/512
512/512 - 0s - loss: 3.1560e-05 - val_loss: 3.1248e-07
Epoch 200/512
512/512 - 0s - loss: 3.0329e-05 - val_loss: 3.0416e-07
Epoch 201/512
512/512 - 0s - loss: 2.9225e-05 - val_loss: 3.0417e-07
Epoch 202/512
512/512 - 0s - loss: 2.8997e-05 - val_loss: 2.9095e-07
Epoch 203/512
512/512 - 0s - loss: 2.7438e-05 - val_loss: 2.7070e-07
Epoch 204/512
512/512 - 0s - loss: 2.5862e-05 - val_loss: 2.6789e-07
Epoch 205/512
512/512 - 0s - loss: 2.5902e-05 - val_loss: 2.6097e-07
Epoch 206/512
512/512 - 0s - loss: 2.4549e-05 - val_loss: 2.4453e-07
Epoch 207/512
512/512 - 0s - loss: 2.3286e-05 - val_loss: 2.4014e-07
Epoch 208/512
512/512 - 0s - loss: 2.3028e-05 - val_loss: 2.3277e-07
Epoch 209/512
512/512 - 0s - loss: 2.1811e-05 - val_loss: 2.2424e-07
Epoch 210/512
512/512 - 0s - loss: 2.1323e-05 - val_loss: 2.1313e-07
Epoch 211/512
512/512 - 0s - loss: 2.0214e-05 - val_loss: 2.0305e-07
Epoch 212/512
512/512 - 0s - loss: 1.9363e-05 - val_loss: 2.0060e-07
Epoch 213/512
512/512 - 0s - loss: 1.9138e-05 - val_loss: 1.9303e-07
Epoch 214/512
512/512 - 0s - loss: 1.8195e-05 - val_loss: 1.7976e-07
Epoch 215/512
512/512 - 0s - loss: 1.7139e-05 - val_loss: 1.7490e-07
Epoch 216/512
512/512 - 0s - loss: 1.6754e-05 - val_loss: 1.7450e-07
Epoch 217/512
512/512 - 0s - loss: 1.6419e-05 - val_loss: 1.6479e-07
Epoch 218/512
512/512 - 0s - loss: 1.5360e-05 - val_loss: 1.5474e-07
Epoch 219/512
512/512 - 0s - loss: 1.4844e-05 - val_loss: 1.5022e-07
Epoch 220/512
512/512 - 0s - loss: 1.4208e-05 - val_loss: 1.4956e-07
Epoch 221/512
512/512 - 0s - loss: 1.4007e-05 - val_loss: 1.4303e-07
Epoch 222/512
512/512 - 0s - loss: 1.3256e-05 - val_loss: 1.3315e-07
Epoch 223/512
512/512 - 0s - loss: 1.2520e-05 - val_loss: 1.2965e-07
Epoch 224/512
512/512 - 0s - loss: 1.2349e-05 - val_loss: 1.2579e-07
Epoch 225/512
512/512 - 0s - loss: 1.1871e-05 - val_loss: 1.1641e-07
Epoch 226/512
512/512 - 0s - loss: 1.0985e-05 - val_loss: 1.1360e-07
Epoch 227/512
512/512 - 0s - loss: 1.0905e-05 - val_loss: 1.1136e-07
Epoch 228/512
512/512 - 0s - loss: 1.0503e-05 - val_loss: 1.0444e-07
Epoch 229/512
512/512 - 0s - loss: 9.8269e-06 - val_loss: 1.0148e-07
Epoch 230/512
512/512 - 0s - loss: 9.5699e-06 - val_loss: 9.9477e-08
Epoch 231/512
512/512 - 0s - loss: 9.3635e-06 - val_loss: 9.2710e-08
Epoch 232/512
512/512 - 0s - loss: 8.6637e-06 - val_loss: 8.7921e-08
Epoch 233/512
512/512 - 0s - loss: 8.4519e-06 - val_loss: 8.6058e-08
Epoch 234/512
512/512 - 0s - loss: 8.1056e-06 - val_loss: 8.3233e-08
Epoch 235/512
512/512 - 0s - loss: 7.7995e-06 - val_loss: 7.9578e-08
Epoch 236/512
512/512 - 0s - loss: 7.4871e-06 - val_loss: 7.4877e-08
Epoch 237/512
512/512 - 0s - loss: 7.0640e-06 - val_loss: 7.2846e-08
Epoch 238/512
512/512 - 0s - loss: 6.8874e-06 - val_loss: 7.0829e-08
Epoch 239/512
512/512 - 0s - loss: 6.6294e-06 - val_loss: 6.6532e-08
Epoch 240/512
512/512 - 0s - loss: 6.2265e-06 - val_loss: 6.3452e-08
Epoch 241/512
512/512 - 0s - loss: 6.0053e-06 - val_loss: 6.2187e-08
Epoch 242/512
512/512 - 0s - loss: 5.8257e-06 - val_loss: 5.9293e-08
Epoch 243/512
512/512 - 0s - loss: 5.5355e-06 - val_loss: 5.5857e-08
Epoch 244/512
512/512 - 0s - loss: 5.2568e-06 - val_loss: 5.4058e-08
Epoch 245/512
512/512 - 0s - loss: 5.0915e-06 - val_loss: 5.2327e-08
Epoch 246/512
512/512 - 0s - loss: 4.8884e-06 - val_loss: 4.9638e-08
Epoch 247/512
512/512 - 0s - loss: 4.6218e-06 - val_loss: 4.7524e-08
Epoch 248/512
512/512 - 0s - loss: 4.4235e-06 - val_loss: 4.6366e-08
Epoch 249/512
512/512 - 0s - loss: 4.3456e-06 - val_loss: 4.3743e-08
Epoch 250/512
512/512 - 0s - loss: 4.0385e-06 - val_loss: 4.1245e-08
Epoch 251/512
512/512 - 0s - loss: 3.9105e-06 - val_loss: 3.9567e-08
Epoch 252/512
512/512 - 0s - loss: 3.7075e-06 - val_loss: 3.8371e-08
Epoch 253/512
512/512 - 0s - loss: 3.6294e-06 - val_loss: 3.6394e-08
Epoch 254/512
512/512 - 0s - loss: 3.3520e-06 - val_loss: 3.4935e-08
Epoch 255/512
512/512 - 0s - loss: 3.2989e-06 - val_loss: 3.4192e-08
Epoch 256/512
512/512 - 0s - loss: 3.1732e-06 - val_loss: 3.1556e-08
Epoch 257/512
512/512 - 0s - loss: 2.9517e-06 - val_loss: 2.9700e-08
Epoch 258/512
512/512 - 0s - loss: 2.7923e-06 - val_loss: 3.0147e-08
Epoch 259/512
512/512 - 0s - loss: 2.8363e-06 - val_loss: 2.8846e-08
Epoch 260/512
512/512 - 0s - loss: 2.5978e-06 - val_loss: 2.6156e-08
Epoch 261/512
512/512 - 0s - loss: 2.4501e-06 - val_loss: 2.5520e-08
Epoch 262/512
512/512 - 0s - loss: 2.4244e-06 - val_loss: 2.4764e-08
Epoch 263/512
512/512 - 0s - loss: 2.2832e-06 - val_loss: 2.3128e-08
Epoch 264/512
512/512 - 0s - loss: 2.1827e-06 - val_loss: 2.1538e-08
Epoch 265/512
512/512 - 0s - loss: 2.0262e-06 - val_loss: 2.1484e-08
Epoch 266/512
512/512 - 0s - loss: 2.0152e-06 - val_loss: 2.1407e-08
Epoch 267/512
512/512 - 0s - loss: 1.9650e-06 - val_loss: 1.9004e-08
Epoch 268/512
512/512 - 0s - loss: 1.7419e-06 - val_loss: 1.7778e-08
Epoch 269/512
512/512 - 0s - loss: 1.7078e-06 - val_loss: 1.8596e-08
Epoch 270/512
512/512 - 0s - loss: 1.7242e-06 - val_loss: 1.7366e-08
Epoch 271/512
512/512 - 0s - loss: 1.5636e-06 - val_loss: 1.5659e-08
Epoch 272/512
512/512 - 0s - loss: 1.4731e-06 - val_loss: 1.5482e-08
Epoch 273/512
512/512 - 0s - loss: 1.4572e-06 - val_loss: 1.5115e-08
Epoch 274/512
512/512 - 0s - loss: 1.3921e-06 - val_loss: 1.3966e-08
Epoch 275/512
512/512 - 0s - loss: 1.2945e-06 - val_loss: 1.3097e-08
Epoch 276/512
512/512 - 0s - loss: 1.2250e-06 - val_loss: 1.3192e-08
Epoch 277/512
512/512 - 0s - loss: 1.2292e-06 - val_loss: 1.2534e-08
Epoch 278/512
512/512 - 0s - loss: 1.1331e-06 - val_loss: 1.1574e-08
Epoch 279/512
512/512 - 0s - loss: 1.0813e-06 - val_loss: 1.1068e-08
Epoch 280/512
512/512 - 0s - loss: 1.0299e-06 - val_loss: 1.0700e-08
Epoch 281/512
512/512 - 0s - loss: 9.8949e-07 - val_loss: 1.0383e-08
Epoch 282/512
512/512 - 0s - loss: 9.5884e-07 - val_loss: 9.5759e-09
Epoch 283/512
512/512 - 0s - loss: 8.8342e-07 - val_loss: 9.1039e-09
Epoch 284/512
512/512 - 0s - loss: 8.5048e-07 - val_loss: 8.9898e-09
Epoch 285/512
512/512 - 0s - loss: 8.2758e-07 - val_loss: 8.5768e-09
Epoch 286/512
512/512 - 0s - loss: 7.8182e-07 - val_loss: 8.0231e-09
Epoch 287/512
512/512 - 0s - loss: 7.3268e-07 - val_loss: 7.6763e-09
Epoch 288/512
512/512 - 0s - loss: 7.1491e-07 - val_loss: 7.2773e-09
Epoch 289/512
512/512 - 0s - loss: 6.7553e-07 - val_loss: 6.8177e-09
Epoch 290/512
512/512 - 0s - loss: 6.3656e-07 - val_loss: 6.5515e-09
Epoch 291/512
512/512 - 0s - loss: 6.1387e-07 - val_loss: 6.3556e-09
Epoch 292/512
512/512 - 0s - loss: 5.8735e-07 - val_loss: 6.0313e-09
Epoch 293/512
512/512 - 0s - loss: 5.5687e-07 - val_loss: 5.6368e-09
Epoch 294/512
512/512 - 0s - loss: 5.2605e-07 - val_loss: 5.4086e-09
Epoch 295/512
512/512 - 0s - loss: 5.0556e-07 - val_loss: 5.1993e-09
Epoch 296/512
512/512 - 0s - loss: 4.8181e-07 - val_loss: 4.9722e-09
Epoch 297/512
512/512 - 0s - loss: 4.5846e-07 - val_loss: 4.7032e-09
Epoch 298/512
512/512 - 0s - loss: 4.3356e-07 - val_loss: 4.5227e-09
Epoch 299/512
512/512 - 0s - loss: 4.1474e-07 - val_loss: 4.3589e-09
Epoch 300/512
512/512 - 0s - loss: 3.9885e-07 - val_loss: 4.1468e-09
Epoch 301/512
512/512 - 0s - loss: 3.7740e-07 - val_loss: 3.8818e-09
Epoch 302/512
512/512 - 0s - loss: 3.5698e-07 - val_loss: 3.7145e-09
Epoch 303/512
512/512 - 0s - loss: 3.4101e-07 - val_loss: 3.5598e-09
Epoch 304/512
512/512 - 0s - loss: 3.2725e-07 - val_loss: 3.4055e-09
Epoch 305/512
512/512 - 0s - loss: 3.1011e-07 - val_loss: 3.1858e-09
Epoch 306/512
512/512 - 0s - loss: 2.9148e-07 - val_loss: 3.0954e-09
Epoch 307/512
512/512 - 0s - loss: 2.8562e-07 - val_loss: 2.9181e-09
Epoch 308/512
512/512 - 0s - loss: 2.6705e-07 - val_loss: 2.6844e-09
Epoch 309/512
512/512 - 0s - loss: 2.4722e-07 - val_loss: 2.6574e-09
Epoch 310/512
512/512 - 0s - loss: 2.4774e-07 - val_loss: 2.5668e-09
Epoch 311/512
512/512 - 0s - loss: 2.3488e-07 - val_loss: 2.2721e-09
Epoch 312/512
512/512 - 0s - loss: 2.1011e-07 - val_loss: 2.2137e-09
Epoch 313/512
512/512 - 0s - loss: 2.0910e-07 - val_loss: 2.2455e-09
Epoch 314/512
512/512 - 0s - loss: 2.0820e-07 - val_loss: 2.0134e-09
Epoch 315/512
512/512 - 0s - loss: 1.8176e-07 - val_loss: 1.8721e-09
Epoch 316/512
512/512 - 0s - loss: 1.7629e-07 - val_loss: 1.9198e-09
Epoch 317/512
512/512 - 0s - loss: 1.7845e-07 - val_loss: 1.8196e-09
Epoch 318/512
512/512 - 0s - loss: 1.6345e-07 - val_loss: 1.5977e-09
Epoch 319/512
512/512 - 0s - loss: 1.4729e-07 - val_loss: 1.5979e-09
Epoch 320/512
512/512 - 0s - loss: 1.5103e-07 - val_loss: 1.5984e-09
Epoch 321/512
512/512 - 0s - loss: 1.4331e-07 - val_loss: 1.4411e-09
Epoch 322/512
512/512 - 0s - loss: 1.2953e-07 - val_loss: 1.3656e-09
Epoch 323/512
512/512 - 0s - loss: 1.2707e-07 - val_loss: 1.3531e-09
Epoch 324/512
512/512 - 0s - loss: 1.2405e-07 - val_loss: 1.2545e-09
Epoch 325/512
512/512 - 0s - loss: 1.1376e-07 - val_loss: 1.1485e-09
Epoch 326/512
512/512 - 0s - loss: 1.0783e-07 - val_loss: 1.1275e-09
Epoch 327/512
512/512 - 0s - loss: 1.0417e-07 - val_loss: 1.1100e-09
Epoch 328/512
512/512 - 0s - loss: 1.0086e-07 - val_loss: 1.0333e-09
Epoch 329/512
512/512 - 0s - loss: 9.4045e-08 - val_loss: 9.5339e-10
Epoch 330/512
512/512 - 0s - loss: 8.8469e-08 - val_loss: 9.2226e-10
Epoch 331/512
512/512 - 0s - loss: 8.5478e-08 - val_loss: 8.9598e-10
Epoch 332/512
512/512 - 0s - loss: 8.1703e-08 - val_loss: 8.4752e-10
Epoch 333/512
512/512 - 0s - loss: 7.7489e-08 - val_loss: 7.9269e-10
Epoch 334/512
512/512 - 0s - loss: 7.3041e-08 - val_loss: 7.5532e-10
Epoch 335/512
512/512 - 0s - loss: 7.0254e-08 - val_loss: 7.1992e-10
Epoch 336/512
512/512 - 0s - loss: 6.6017e-08 - val_loss: 6.9374e-10
Epoch 337/512
512/512 - 0s - loss: 6.4259e-08 - val_loss: 6.5859e-10
Epoch 338/512
512/512 - 0s - loss: 5.9548e-08 - val_loss: 6.2436e-10
Epoch 339/512
512/512 - 0s - loss: 5.7615e-08 - val_loss: 6.0177e-10
Epoch 340/512
512/512 - 0s - loss: 5.5378e-08 - val_loss: 5.6267e-10
Epoch 341/512
512/512 - 0s - loss: 5.1518e-08 - val_loss: 5.3211e-10
Epoch 342/512
512/512 - 0s - loss: 4.8982e-08 - val_loss: 5.1802e-10
Epoch 343/512
512/512 - 0s - loss: 4.7669e-08 - val_loss: 4.9965e-10
Epoch 344/512
512/512 - 0s - loss: 4.5165e-08 - val_loss: 4.6915e-10
Epoch 345/512
512/512 - 0s - loss: 4.2843e-08 - val_loss: 4.4361e-10
Epoch 346/512
512/512 - 0s - loss: 4.0595e-08 - val_loss: 4.1961e-10
Epoch 347/512
512/512 - 0s - loss: 3.8596e-08 - val_loss: 4.0157e-10
Epoch 348/512
512/512 - 0s - loss: 3.7127e-08 - val_loss: 3.8502e-10
Epoch 349/512
512/512 - 0s - loss: 3.5423e-08 - val_loss: 3.6310e-10
Epoch 350/512
512/512 - 0s - loss: 3.3128e-08 - val_loss: 3.4863e-10
Epoch 351/512
512/512 - 0s - loss: 3.1973e-08 - val_loss: 3.3888e-10
Epoch 352/512
512/512 - 0s - loss: 3.1013e-08 - val_loss: 3.1741e-10
Epoch 353/512
512/512 - 0s - loss: 2.8853e-08 - val_loss: 3.0130e-10
Epoch 354/512
512/512 - 0s - loss: 2.7649e-08 - val_loss: 2.9182e-10
Epoch 355/512
512/512 - 0s - loss: 2.6739e-08 - val_loss: 2.7539e-10
Epoch 356/512
512/512 - 0s - loss: 2.5205e-08 - val_loss: 2.5881e-10
Epoch 357/512
512/512 - 0s - loss: 2.3778e-08 - val_loss: 2.5018e-10
Epoch 358/512
512/512 - 0s - loss: 2.3030e-08 - val_loss: 2.4187e-10
Epoch 359/512
512/512 - 0s - loss: 2.2065e-08 - val_loss: 2.2816e-10
Epoch 360/512
512/512 - 0s - loss: 2.0782e-08 - val_loss: 2.1449e-10
Epoch 361/512
512/512 - 0s - loss: 1.9915e-08 - val_loss: 2.0673e-10
Epoch 362/512
512/512 - 0s - loss: 1.9033e-08 - val_loss: 1.9791e-10
Epoch 363/512
512/512 - 0s - loss: 1.8010e-08 - val_loss: 1.9133e-10
Epoch 364/512
512/512 - 0s - loss: 1.7607e-08 - val_loss: 1.8328e-10
Epoch 365/512
512/512 - 0s - loss: 1.6778e-08 - val_loss: 1.7192e-10
Epoch 366/512
512/512 - 0s - loss: 1.5873e-08 - val_loss: 1.5948e-10
Epoch 367/512
512/512 - 0s - loss: 1.4765e-08 - val_loss: 1.5602e-10
Epoch 368/512
512/512 - 0s - loss: 1.4521e-08 - val_loss: 1.5475e-10
Epoch 369/512
512/512 - 0s - loss: 1.4142e-08 - val_loss: 1.4672e-10
Epoch 370/512
512/512 - 0s - loss: 1.3373e-08 - val_loss: 1.3618e-10
Epoch 371/512
512/512 - 0s - loss: 1.2673e-08 - val_loss: 1.2729e-10
Epoch 372/512
512/512 - 0s - loss: 1.1718e-08 - val_loss: 1.2552e-10
Epoch 373/512
512/512 - 0s - loss: 1.1661e-08 - val_loss: 1.2563e-10
Epoch 374/512
512/512 - 0s - loss: 1.1544e-08 - val_loss: 1.1812e-10
Epoch 375/512
512/512 - 0s - loss: 1.0710e-08 - val_loss: 1.0742e-10
Epoch 376/512
512/512 - 0s - loss: 9.9616e-09 - val_loss: 1.0349e-10
Epoch 377/512
512/512 - 0s - loss: 9.7198e-09 - val_loss: 1.0317e-10
Epoch 378/512
512/512 - 0s - loss: 9.5012e-09 - val_loss: 1.0012e-10
Epoch 379/512
512/512 - 0s - loss: 9.1948e-09 - val_loss: 9.2648e-11
Epoch 380/512
512/512 - 0s - loss: 8.5105e-09 - val_loss: 8.7265e-11
Epoch 381/512
512/512 - 0s - loss: 8.1652e-09 - val_loss: 8.5250e-11
Epoch 382/512
512/512 - 0s - loss: 8.0009e-09 - val_loss: 8.3413e-11
Epoch 383/512
512/512 - 0s - loss: 7.7607e-09 - val_loss: 7.9619e-11
Epoch 384/512
512/512 - 0s - loss: 7.3943e-09 - val_loss: 7.4443e-11
Epoch 385/512
512/512 - 0s - loss: 6.8960e-09 - val_loss: 7.1321e-11
Epoch 386/512
512/512 - 0s - loss: 6.7247e-09 - val_loss: 7.0635e-11
Epoch 387/512
512/512 - 0s - loss: 6.5909e-09 - val_loss: 6.8673e-11
Epoch 388/512
512/512 - 0s - loss: 6.3359e-09 - val_loss: 6.5003e-11
Epoch 389/512
512/512 - 0s - loss: 6.0399e-09 - val_loss: 6.1779e-11
Epoch 390/512
512/512 - 0s - loss: 5.7532e-09 - val_loss: 5.9409e-11
Epoch 391/512
512/512 - 0s - loss: 5.5605e-09 - val_loss: 5.7231e-11
Epoch 392/512
512/512 - 0s - loss: 5.3285e-09 - val_loss: 5.6000e-11
Epoch 393/512
512/512 - 0s - loss: 5.2260e-09 - val_loss: 5.4104e-11
Epoch 394/512
512/512 - 0s - loss: 5.0227e-09 - val_loss: 5.1403e-11
Epoch 395/512
512/512 - 0s - loss: 4.8083e-09 - val_loss: 4.9561e-11
Epoch 396/512
512/512 - 0s - loss: 4.6589e-09 - val_loss: 4.7630e-11
Epoch 397/512
512/512 - 0s - loss: 4.4302e-09 - val_loss: 4.6029e-11
Epoch 398/512
512/512 - 0s - loss: 4.3267e-09 - val_loss: 4.4819e-11
Epoch 399/512
512/512 - 0s - loss: 4.1949e-09 - val_loss: 4.2708e-11
Epoch 400/512
512/512 - 0s - loss: 3.9915e-09 - val_loss: 4.1340e-11
Epoch 401/512
512/512 - 0s - loss: 3.8644e-09 - val_loss: 3.9982e-11
Epoch 402/512
512/512 - 0s - loss: 3.7666e-09 - val_loss: 3.8949e-11
Epoch 403/512
512/512 - 0s - loss: 3.6491e-09 - val_loss: 3.7517e-11
Epoch 404/512
512/512 - 0s - loss: 3.5055e-09 - val_loss: 3.6063e-11
Epoch 405/512
512/512 - 0s - loss: 3.3907e-09 - val_loss: 3.4652e-11
Epoch 406/512
512/512 - 0s - loss: 3.2762e-09 - val_loss: 3.3670e-11
Epoch 407/512
512/512 - 0s - loss: 3.1364e-09 - val_loss: 3.2706e-11
Epoch 408/512
512/512 - 0s - loss: 3.0748e-09 - val_loss: 3.2241e-11
Epoch 409/512
512/512 - 0s - loss: 3.0322e-09 - val_loss: 3.0987e-11
Epoch 410/512
512/512 - 0s - loss: 2.8982e-09 - val_loss: 2.9895e-11
Epoch 411/512
512/512 - 0s - loss: 2.7861e-09 - val_loss: 2.8770e-11
Epoch 412/512
512/512 - 0s - loss: 2.6723e-09 - val_loss: 2.7560e-11
Epoch 413/512
512/512 - 0s - loss: 2.5988e-09 - val_loss: 2.6932e-11
Epoch 414/512
512/512 - 0s - loss: 2.5518e-09 - val_loss: 2.6587e-11
Epoch 415/512
512/512 - 0s - loss: 2.5276e-09 - val_loss: 2.5386e-11
Epoch 416/512
512/512 - 0s - loss: 2.3676e-09 - val_loss: 2.3778e-11
Epoch 417/512
512/512 - 0s - loss: 2.2119e-09 - val_loss: 2.3550e-11
Epoch 418/512
512/512 - 0s - loss: 2.2487e-09 - val_loss: 2.3812e-11
Epoch 419/512
512/512 - 0s - loss: 2.2491e-09 - val_loss: 2.2622e-11
Epoch 420/512
512/512 - 0s - loss: 2.1197e-09 - val_loss: 2.1536e-11
Epoch 421/512
512/512 - 0s - loss: 2.0173e-09 - val_loss: 2.0978e-11
Epoch 422/512
512/512 - 0s - loss: 1.9691e-09 - val_loss: 2.0681e-11
Epoch 423/512
512/512 - 0s - loss: 1.9473e-09 - val_loss: 2.0496e-11
Epoch 424/512
512/512 - 0s - loss: 1.9220e-09 - val_loss: 1.9709e-11
Epoch 425/512
512/512 - 0s - loss: 1.8464e-09 - val_loss: 1.8777e-11
Epoch 426/512
512/512 - 0s - loss: 1.7841e-09 - val_loss: 1.8107e-11
Epoch 427/512
512/512 - 0s - loss: 1.7204e-09 - val_loss: 1.7545e-11
Epoch 428/512
512/512 - 0s - loss: 1.6535e-09 - val_loss: 1.7407e-11
Epoch 429/512
512/512 - 0s - loss: 1.6943e-09 - val_loss: 1.7257e-11
Epoch 430/512
512/512 - 0s - loss: 1.6350e-09 - val_loss: 1.6383e-11
Epoch 431/512
512/512 - 0s - loss: 1.5572e-09 - val_loss: 1.5939e-11
Epoch 432/512
512/512 - 0s - loss: 1.5038e-09 - val_loss: 1.5378e-11
Epoch 433/512
512/512 - 0s - loss: 1.4751e-09 - val_loss: 1.4920e-11
Epoch 434/512
512/512 - 0s - loss: 1.4329e-09 - val_loss: 1.4754e-11
Epoch 435/512
512/512 - 0s - loss: 1.3992e-09 - val_loss: 1.4577e-11
Epoch 436/512
512/512 - 0s - loss: 1.3884e-09 - val_loss: 1.4345e-11
Epoch 437/512
512/512 - 0s - loss: 1.3545e-09 - val_loss: 1.4017e-11
Epoch 438/512
512/512 - 0s - loss: 1.3017e-09 - val_loss: 1.3428e-11
Epoch 439/512
512/512 - 0s - loss: 1.2690e-09 - val_loss: 1.3204e-11
Epoch 440/512
512/512 - 0s - loss: 1.2615e-09 - val_loss: 1.2819e-11
Epoch 441/512
512/512 - 0s - loss: 1.2326e-09 - val_loss: 1.2514e-11
Epoch 442/512
512/512 - 0s - loss: 1.1844e-09 - val_loss: 1.2143e-11
Epoch 443/512
512/512 - 0s - loss: 1.1655e-09 - val_loss: 1.1864e-11
Epoch 444/512
512/512 - 0s - loss: 1.1285e-09 - val_loss: 1.1278e-11
Epoch 445/512
512/512 - 0s - loss: 1.0845e-09 - val_loss: 1.0905e-11
Epoch 446/512
512/512 - 0s - loss: 1.0611e-09 - val_loss: 1.0982e-11
Epoch 447/512
512/512 - 0s - loss: 1.0628e-09 - val_loss: 1.1119e-11
Epoch 448/512
512/512 - 0s - loss: 1.0560e-09 - val_loss: 1.0782e-11
Epoch 449/512
512/512 - 0s - loss: 1.0213e-09 - val_loss: 1.0396e-11
Epoch 450/512
512/512 - 0s - loss: 9.9411e-10 - val_loss: 9.8964e-12
Epoch 451/512
512/512 - 0s - loss: 9.5075e-10 - val_loss: 9.5524e-12
Epoch 452/512
512/512 - 0s - loss: 9.1589e-10 - val_loss: 9.4392e-12
Epoch 453/512
512/512 - 0s - loss: 9.1152e-10 - val_loss: 9.3909e-12
Epoch 454/512
512/512 - 0s - loss: 9.0091e-10 - val_loss: 9.3560e-12
Epoch 455/512
512/512 - 0s - loss: 9.0409e-10 - val_loss: 9.2940e-12
Epoch 456/512
512/512 - 0s - loss: 8.8809e-10 - val_loss: 8.7325e-12
Epoch 457/512
512/512 - 0s - loss: 8.3967e-10 - val_loss: 8.4893e-12
Epoch 458/512
512/512 - 0s - loss: 8.2430e-10 - val_loss: 8.3240e-12
Epoch 459/512
512/512 - 0s - loss: 8.0740e-10 - val_loss: 8.3369e-12
Epoch 460/512
512/512 - 0s - loss: 8.0598e-10 - val_loss: 8.2487e-12
Epoch 461/512
512/512 - 0s - loss: 7.8195e-10 - val_loss: 8.0847e-12
Epoch 462/512
512/512 - 0s - loss: 7.7915e-10 - val_loss: 7.8746e-12
Epoch 463/512
512/512 - 0s - loss: 7.4179e-10 - val_loss: 7.6279e-12
Epoch 464/512
512/512 - 0s - loss: 7.3522e-10 - val_loss: 7.4789e-12
Epoch 465/512
512/512 - 0s - loss: 7.1921e-10 - val_loss: 7.3668e-12
Epoch 466/512
512/512 - 0s - loss: 7.1770e-10 - val_loss: 7.1788e-12
Epoch 467/512
512/512 - 0s - loss: 6.8509e-10 - val_loss: 6.8919e-12
Epoch 468/512
512/512 - 0s - loss: 6.6896e-10 - val_loss: 6.9090e-12
Epoch 469/512
512/512 - 0s - loss: 6.6945e-10 - val_loss: 6.9249e-12
Epoch 470/512
512/512 - 0s - loss: 6.6178e-10 - val_loss: 6.7105e-12
Epoch 471/512
512/512 - 0s - loss: 6.5661e-10 - val_loss: 6.6741e-12
Epoch 472/512
512/512 - 0s - loss: 6.3628e-10 - val_loss: 6.5792e-12
Epoch 473/512
512/512 - 0s - loss: 6.2814e-10 - val_loss: 6.4634e-12
Epoch 474/512
512/512 - 0s - loss: 6.1432e-10 - val_loss: 6.2777e-12
Epoch 475/512
512/512 - 0s - loss: 6.0346e-10 - val_loss: 6.0953e-12
Epoch 476/512
512/512 - 0s - loss: 5.8555e-10 - val_loss: 6.0821e-12
Epoch 477/512
512/512 - 0s - loss: 5.8916e-10 - val_loss: 5.9808e-12
Epoch 478/512
512/512 - 0s - loss: 5.7574e-10 - val_loss: 5.8574e-12
Epoch 479/512
512/512 - 0s - loss: 5.5672e-10 - val_loss: 5.5366e-12
Epoch 480/512
512/512 - 0s - loss: 5.3363e-10 - val_loss: 5.3397e-12
Epoch 481/512
512/512 - 0s - loss: 5.1809e-10 - val_loss: 5.3439e-12
Epoch 482/512
512/512 - 0s - loss: 5.2374e-10 - val_loss: 5.4456e-12
Epoch 483/512
512/512 - 0s - loss: 5.1700e-10 - val_loss: 5.3680e-12
Epoch 484/512
512/512 - 0s - loss: 5.1671e-10 - val_loss: 5.2875e-12
Epoch 485/512
512/512 - 0s - loss: 5.1044e-10 - val_loss: 5.2278e-12
Epoch 486/512
512/512 - 0s - loss: 4.9828e-10 - val_loss: 5.1489e-12
Epoch 487/512
512/512 - 0s - loss: 4.9694e-10 - val_loss: 5.0018e-12
Epoch 488/512
512/512 - 0s - loss: 4.8591e-10 - val_loss: 4.7865e-12
Epoch 489/512
512/512 - 0s - loss: 4.5839e-10 - val_loss: 4.6046e-12
Epoch 490/512
512/512 - 0s - loss: 4.4532e-10 - val_loss: 4.6030e-12
Epoch 491/512
512/512 - 0s - loss: 4.5353e-10 - val_loss: 4.5607e-12
Epoch 492/512
512/512 - 0s - loss: 4.4701e-10 - val_loss: 4.4619e-12
Epoch 493/512
512/512 - 0s - loss: 4.3596e-10 - val_loss: 4.4474e-12
Epoch 494/512
512/512 - 0s - loss: 4.3099e-10 - val_loss: 4.3741e-12
Epoch 495/512
512/512 - 0s - loss: 4.2271e-10 - val_loss: 4.3000e-12
Epoch 496/512
512/512 - 0s - loss: 4.1637e-10 - val_loss: 4.3126e-12
Epoch 497/512
512/512 - 0s - loss: 4.2406e-10 - val_loss: 4.2758e-12
Epoch 498/512
512/512 - 0s - loss: 4.0877e-10 - val_loss: 4.0635e-12
Epoch 499/512
512/512 - 0s - loss: 3.9502e-10 - val_loss: 3.9828e-12
Epoch 500/512
512/512 - 0s - loss: 3.8452e-10 - val_loss: 3.8807e-12
Epoch 501/512
512/512 - 0s - loss: 3.7932e-10 - val_loss: 3.8300e-12
Epoch 502/512
512/512 - 0s - loss: 3.7867e-10 - val_loss: 3.8130e-12
Epoch 503/512
512/512 - 0s - loss: 3.7536e-10 - val_loss: 3.8684e-12
Epoch 504/512
512/512 - 0s - loss: 3.7375e-10 - val_loss: 3.7212e-12
Epoch 505/512
512/512 - 0s - loss: 3.6495e-10 - val_loss: 3.7207e-12
Epoch 506/512
512/512 - 0s - loss: 3.5918e-10 - val_loss: 3.7369e-12
Epoch 507/512
512/512 - 0s - loss: 3.6312e-10 - val_loss: 3.6344e-12
Epoch 508/512
512/512 - 0s - loss: 3.5596e-10 - val_loss: 3.6139e-12
Epoch 509/512
512/512 - 0s - loss: 3.5123e-10 - val_loss: 3.5872e-12
Epoch 510/512
512/512 - 0s - loss: 3.4368e-10 - val_loss: 3.4597e-12
Epoch 511/512
512/512 - 0s - loss: 3.3508e-10 - val_loss: 3.3920e-12
Epoch 512/512
512/512 - 0s - loss: 3.3176e-10 - val_loss: 3.2779e-12
2024-04-11 15:49:02.682292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Train on 512 samples, validate on 512 samples
Epoch 1/512

Epoch 00001: val_loss improved from inf to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3066e-10 - val_loss: 6.2330e-10
Epoch 2/512

Epoch 00002: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7646e-10 - val_loss: 7.6527e-10
Epoch 3/512

Epoch 00003: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0905e-10 - val_loss: 6.3849e-10
Epoch 4/512

Epoch 00004: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6101e-10 - val_loss: 4.8151e-10
Epoch 5/512

Epoch 00005: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2824e-10 - val_loss: 3.8736e-10
Epoch 6/512

Epoch 00006: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.6269e-10 - val_loss: 3.5854e-10
Epoch 7/512

Epoch 00007: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5694e-10 - val_loss: 3.9054e-10
Epoch 8/512

Epoch 00008: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9442e-10 - val_loss: 4.2918e-10
Epoch 9/512

Epoch 00009: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.3211e-10 - val_loss: 4.6926e-10
Epoch 10/512

Epoch 00010: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.6130e-10 - val_loss: 4.7661e-10
Epoch 11/512

Epoch 00011: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.5414e-10 - val_loss: 4.4556e-10
Epoch 12/512

Epoch 00012: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2244e-10 - val_loss: 4.1408e-10
Epoch 13/512

Epoch 00013: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.9567e-10 - val_loss: 3.8855e-10
Epoch 14/512

Epoch 00014: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7529e-10 - val_loss: 3.7689e-10
Epoch 15/512

Epoch 00015: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.6459e-10 - val_loss: 3.7282e-10
Epoch 16/512

Epoch 00016: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7017e-10 - val_loss: 3.8389e-10
Epoch 17/512

Epoch 00017: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8035e-10 - val_loss: 3.9707e-10
Epoch 18/512

Epoch 00018: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.8568e-10 - val_loss: 3.9131e-10
Epoch 19/512

Epoch 00019: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.7657e-10 - val_loss: 3.7255e-10
Epoch 20/512

Epoch 00020: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.5815e-10 - val_loss: 3.6262e-10
Epoch 21/512

Epoch 00021: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.4689e-10 - val_loss: 3.4779e-10
Epoch 22/512

Epoch 00022: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.4125e-10 - val_loss: 3.4918e-10
Epoch 23/512

Epoch 00023: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3854e-10 - val_loss: 3.3999e-10
Epoch 24/512

Epoch 00024: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.3196e-10 - val_loss: 3.4336e-10
Epoch 25/512

Epoch 00025: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3458e-10 - val_loss: 3.3963e-10
Epoch 26/512

Epoch 00026: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.3041e-10 - val_loss: 3.3138e-10
Epoch 27/512

Epoch 00027: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.2062e-10 - val_loss: 3.2903e-10
Epoch 28/512

Epoch 00028: val_loss did not improve from 0.00000
512/512 - 0s - loss: 3.2314e-10 - val_loss: 3.2924e-10
Epoch 29/512

Epoch 00029: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1804e-10 - val_loss: 3.2518e-10
Epoch 30/512

Epoch 00030: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.1516e-10 - val_loss: 3.1558e-10
Epoch 31/512

Epoch 00031: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0917e-10 - val_loss: 3.1526e-10
Epoch 32/512

Epoch 00032: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.0525e-10 - val_loss: 3.0523e-10
Epoch 33/512

Epoch 00033: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9789e-10 - val_loss: 3.0142e-10
Epoch 34/512

Epoch 00034: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.9224e-10 - val_loss: 2.9370e-10
Epoch 35/512

Epoch 00035: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.8294e-10 - val_loss: 2.8694e-10
Epoch 36/512

Epoch 00036: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7907e-10 - val_loss: 2.8463e-10
Epoch 37/512

Epoch 00037: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7860e-10 - val_loss: 2.8384e-10
Epoch 38/512

Epoch 00038: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.7839e-10 - val_loss: 2.8712e-10
Epoch 39/512

Epoch 00039: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8074e-10 - val_loss: 2.8622e-10
Epoch 40/512

Epoch 00040: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.8083e-10 - val_loss: 2.8735e-10
Epoch 41/512

Epoch 00041: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.7624e-10 - val_loss: 2.7623e-10
Epoch 42/512

Epoch 00042: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6931e-10 - val_loss: 2.7398e-10
Epoch 43/512

Epoch 00043: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.6351e-10 - val_loss: 2.5807e-10
Epoch 44/512

Epoch 00044: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.5311e-10 - val_loss: 2.5888e-10
Epoch 45/512

Epoch 00045: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5207e-10 - val_loss: 2.5233e-10
Epoch 46/512

Epoch 00046: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.4939e-10 - val_loss: 2.5822e-10
Epoch 47/512

Epoch 00047: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.5144e-10 - val_loss: 2.5200e-10
Epoch 48/512

Epoch 00048: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4554e-10 - val_loss: 2.5189e-10
Epoch 49/512

Epoch 00049: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4693e-10 - val_loss: 2.4875e-10
Epoch 50/512

Epoch 00050: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.4067e-10 - val_loss: 2.4095e-10
Epoch 51/512

Epoch 00051: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.3412e-10 - val_loss: 2.3545e-10
Epoch 52/512

Epoch 00052: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2896e-10 - val_loss: 2.3233e-10
Epoch 53/512

Epoch 00053: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2743e-10 - val_loss: 2.3311e-10
Epoch 54/512

Epoch 00054: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2781e-10 - val_loss: 2.3283e-10
Epoch 55/512

Epoch 00055: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.2643e-10 - val_loss: 2.2480e-10
Epoch 56/512

Epoch 00056: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2112e-10 - val_loss: 2.2580e-10
Epoch 57/512

Epoch 00057: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.2267e-10 - val_loss: 2.2580e-10
Epoch 58/512

Epoch 00058: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1941e-10 - val_loss: 2.2102e-10
Epoch 59/512

Epoch 00059: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.1730e-10 - val_loss: 2.2359e-10
Epoch 60/512

Epoch 00060: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.1695e-10 - val_loss: 2.1549e-10
Epoch 61/512

Epoch 00061: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0696e-10 - val_loss: 2.0910e-10
Epoch 62/512

Epoch 00062: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0712e-10 - val_loss: 2.1540e-10
Epoch 63/512

Epoch 00063: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0913e-10 - val_loss: 2.0608e-10
Epoch 64/512

Epoch 00064: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 2.0210e-10 - val_loss: 2.0559e-10
Epoch 65/512

Epoch 00065: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0334e-10 - val_loss: 2.0682e-10
Epoch 66/512

Epoch 00066: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9942e-10 - val_loss: 1.9822e-10
Epoch 67/512

Epoch 00067: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.9699e-10 - val_loss: 2.0545e-10
Epoch 68/512

Epoch 00068: val_loss did not improve from 0.00000
512/512 - 0s - loss: 2.0116e-10 - val_loss: 2.0113e-10
Epoch 69/512

Epoch 00069: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.9328e-10 - val_loss: 1.9148e-10
Epoch 70/512

Epoch 00070: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8998e-10 - val_loss: 1.9267e-10
Epoch 71/512

Epoch 00071: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8786e-10 - val_loss: 1.8904e-10
Epoch 72/512

Epoch 00072: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8318e-10 - val_loss: 1.8365e-10
Epoch 73/512

Epoch 00073: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.8508e-10 - val_loss: 1.8970e-10
Epoch 74/512

Epoch 00074: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.8323e-10 - val_loss: 1.8252e-10
Epoch 75/512

Epoch 00075: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7709e-10 - val_loss: 1.7956e-10
Epoch 76/512

Epoch 00076: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7960e-10 - val_loss: 1.8491e-10
Epoch 77/512

Epoch 00077: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7921e-10 - val_loss: 1.7980e-10
Epoch 78/512

Epoch 00078: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7536e-10 - val_loss: 1.7878e-10
Epoch 79/512

Epoch 00079: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.7776e-10 - val_loss: 1.8104e-10
Epoch 80/512

Epoch 00080: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.7473e-10 - val_loss: 1.7223e-10
Epoch 81/512

Epoch 00081: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6863e-10 - val_loss: 1.6933e-10
Epoch 82/512

Epoch 00082: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.6423e-10 - val_loss: 1.6103e-10
Epoch 83/512

Epoch 00083: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5824e-10 - val_loss: 1.6278e-10
Epoch 84/512

Epoch 00084: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6292e-10 - val_loss: 1.6978e-10
Epoch 85/512

Epoch 00085: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6798e-10 - val_loss: 1.6986e-10
Epoch 86/512

Epoch 00086: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6508e-10 - val_loss: 1.6687e-10
Epoch 87/512

Epoch 00087: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6370e-10 - val_loss: 1.6663e-10
Epoch 88/512

Epoch 00088: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.6434e-10 - val_loss: 1.6470e-10
Epoch 89/512

Epoch 00089: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5797e-10 - val_loss: 1.5999e-10
Epoch 90/512

Epoch 00090: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5657e-10 - val_loss: 1.6002e-10
Epoch 91/512

Epoch 00091: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5759e-10 - val_loss: 1.5713e-10
Epoch 92/512

Epoch 00092: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.5166e-10 - val_loss: 1.4858e-10
Epoch 93/512

Epoch 00093: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4286e-10 - val_loss: 1.4670e-10
Epoch 94/512

Epoch 00094: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4685e-10 - val_loss: 1.5387e-10
Epoch 95/512

Epoch 00095: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.5021e-10 - val_loss: 1.4742e-10
Epoch 96/512

Epoch 00096: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4349e-10 - val_loss: 1.4395e-10
Epoch 97/512

Epoch 00097: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4098e-10 - val_loss: 1.4682e-10
Epoch 98/512

Epoch 00098: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4675e-10 - val_loss: 1.5140e-10
Epoch 99/512

Epoch 00099: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4580e-10 - val_loss: 1.4405e-10
Epoch 100/512

Epoch 00100: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.4100e-10 - val_loss: 1.4131e-10
Epoch 101/512

Epoch 00101: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4083e-10 - val_loss: 1.4775e-10
Epoch 102/512

Epoch 00102: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.4398e-10 - val_loss: 1.4233e-10
Epoch 103/512

Epoch 00103: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3749e-10 - val_loss: 1.3560e-10
Epoch 104/512

Epoch 00104: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.3035e-10 - val_loss: 1.3029e-10
Epoch 105/512

Epoch 00105: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2943e-10 - val_loss: 1.3436e-10
Epoch 106/512

Epoch 00106: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3235e-10 - val_loss: 1.3508e-10
Epoch 107/512

Epoch 00107: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3429e-10 - val_loss: 1.3606e-10
Epoch 108/512

Epoch 00108: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3272e-10 - val_loss: 1.3422e-10
Epoch 109/512

Epoch 00109: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3197e-10 - val_loss: 1.3599e-10
Epoch 110/512

Epoch 00110: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.3246e-10 - val_loss: 1.3113e-10
Epoch 111/512

Epoch 00111: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2736e-10 - val_loss: 1.2868e-10
Epoch 112/512

Epoch 00112: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2645e-10 - val_loss: 1.2798e-10
Epoch 113/512

Epoch 00113: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2697e-10 - val_loss: 1.3066e-10
Epoch 114/512

Epoch 00114: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2697e-10 - val_loss: 1.2715e-10
Epoch 115/512

Epoch 00115: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2463e-10 - val_loss: 1.2673e-10
Epoch 116/512

Epoch 00116: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.2562e-10 - val_loss: 1.2832e-10
Epoch 117/512

Epoch 00117: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.2340e-10 - val_loss: 1.2028e-10
Epoch 118/512

Epoch 00118: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1629e-10 - val_loss: 1.1812e-10
Epoch 119/512

Epoch 00119: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1698e-10 - val_loss: 1.2040e-10
Epoch 120/512

Epoch 00120: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1861e-10 - val_loss: 1.1917e-10
Epoch 121/512

Epoch 00121: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1807e-10 - val_loss: 1.1807e-10
Epoch 122/512

Epoch 00122: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1349e-10 - val_loss: 1.0988e-10
Epoch 123/512

Epoch 00123: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0750e-10 - val_loss: 1.1263e-10
Epoch 124/512

Epoch 00124: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1292e-10 - val_loss: 1.1736e-10
Epoch 125/512

Epoch 00125: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1678e-10 - val_loss: 1.2059e-10
Epoch 126/512

Epoch 00126: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1940e-10 - val_loss: 1.1998e-10
Epoch 127/512

Epoch 00127: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.1332e-10 - val_loss: 1.0848e-10
Epoch 128/512

Epoch 00128: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0489e-10 - val_loss: 1.0735e-10
Epoch 129/512

Epoch 00129: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0696e-10 - val_loss: 1.1136e-10
Epoch 130/512

Epoch 00130: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.1192e-10 - val_loss: 1.1260e-10
Epoch 131/512

Epoch 00131: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0869e-10 - val_loss: 1.0482e-10
Epoch 132/512

Epoch 00132: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 1.0124e-10 - val_loss: 1.0092e-10
Epoch 133/512

Epoch 00133: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.8405e-11 - val_loss: 9.8396e-11
Epoch 134/512

Epoch 00134: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0119e-10 - val_loss: 1.0761e-10
Epoch 135/512

Epoch 00135: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0832e-10 - val_loss: 1.1002e-10
Epoch 136/512

Epoch 00136: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0514e-10 - val_loss: 1.0337e-10
Epoch 137/512

Epoch 00137: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9734e-11 - val_loss: 9.9954e-11
Epoch 138/512

Epoch 00138: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.6654e-11 - val_loss: 9.4868e-11
Epoch 139/512

Epoch 00139: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.7979e-11 - val_loss: 1.0568e-10
Epoch 140/512

Epoch 00140: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0612e-10 - val_loss: 1.0816e-10
Epoch 141/512

Epoch 00141: val_loss did not improve from 0.00000
512/512 - 0s - loss: 1.0356e-10 - val_loss: 9.9832e-11
Epoch 142/512

Epoch 00142: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.6014e-11 - val_loss: 9.6518e-11
Epoch 143/512

Epoch 00143: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4963e-11 - val_loss: 9.6015e-11
Epoch 144/512

Epoch 00144: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4444e-11 - val_loss: 9.8882e-11
Epoch 145/512

Epoch 00145: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9787e-11 - val_loss: 1.0287e-10
Epoch 146/512

Epoch 00146: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9130e-11 - val_loss: 9.6742e-11
Epoch 147/512

Epoch 00147: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.4286e-11 - val_loss: 9.3628e-11
Epoch 148/512

Epoch 00148: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.3101e-11 - val_loss: 9.3451e-11
Epoch 149/512

Epoch 00149: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 9.1183e-11 - val_loss: 9.1324e-11
Epoch 150/512

Epoch 00150: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4611e-11 - val_loss: 1.0202e-10
Epoch 151/512

Epoch 00151: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.9059e-11 - val_loss: 9.7166e-11
Epoch 152/512

Epoch 00152: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.4561e-11 - val_loss: 9.3241e-11
Epoch 153/512

Epoch 00153: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.9612e-11 - val_loss: 8.8197e-11
Epoch 154/512

Epoch 00154: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.6766e-11 - val_loss: 8.8304e-11
Epoch 155/512

Epoch 00155: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 8.6049e-11 - val_loss: 8.3437e-11
Epoch 156/512

Epoch 00156: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.9009e-11 - val_loss: 7.6538e-11
Epoch 157/512

Epoch 00157: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5701e-11 - val_loss: 7.9060e-11
Epoch 158/512

Epoch 00158: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9966e-11 - val_loss: 8.3669e-11
Epoch 159/512

Epoch 00159: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5245e-11 - val_loss: 9.0749e-11
Epoch 160/512

Epoch 00160: val_loss did not improve from 0.00000
512/512 - 0s - loss: 9.1373e-11 - val_loss: 9.3094e-11
Epoch 161/512

Epoch 00161: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.9453e-11 - val_loss: 8.6460e-11
Epoch 162/512

Epoch 00162: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.1607e-11 - val_loss: 7.7862e-11
Epoch 163/512

Epoch 00163: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.4578e-11 - val_loss: 7.2825e-11
Epoch 164/512

Epoch 00164: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.4752e-11 - val_loss: 8.0083e-11
Epoch 165/512

Epoch 00165: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0466e-11 - val_loss: 8.5034e-11
Epoch 166/512

Epoch 00166: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4867e-11 - val_loss: 8.5551e-11
Epoch 167/512

Epoch 00167: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.5059e-11 - val_loss: 8.7086e-11
Epoch 168/512

Epoch 00168: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.4433e-11 - val_loss: 8.0354e-11
Epoch 169/512

Epoch 00169: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6169e-11 - val_loss: 7.3292e-11
Epoch 170/512

Epoch 00170: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1917e-11 - val_loss: 7.4223e-11
Epoch 171/512

Epoch 00171: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.5023e-11 - val_loss: 7.8426e-11
Epoch 172/512

Epoch 00172: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9571e-11 - val_loss: 8.3417e-11
Epoch 173/512

Epoch 00173: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.2476e-11 - val_loss: 8.3597e-11
Epoch 174/512

Epoch 00174: val_loss did not improve from 0.00000
512/512 - 0s - loss: 8.0622e-11 - val_loss: 7.6227e-11
Epoch 175/512

Epoch 00175: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 7.2469e-11 - val_loss: 7.1292e-11
Epoch 176/512

Epoch 00176: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.9431e-11 - val_loss: 6.7789e-11
Epoch 177/512

Epoch 00177: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6400e-11 - val_loss: 6.8787e-11
Epoch 178/512

Epoch 00178: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.1847e-11 - val_loss: 7.6791e-11
Epoch 179/512

Epoch 00179: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7319e-11 - val_loss: 7.9381e-11
Epoch 180/512

Epoch 00180: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.8567e-11 - val_loss: 8.0393e-11
Epoch 181/512

Epoch 00181: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.9419e-11 - val_loss: 7.6448e-11
Epoch 182/512

Epoch 00182: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2532e-11 - val_loss: 6.7978e-11
Epoch 183/512

Epoch 00183: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.5668e-11 - val_loss: 6.4142e-11
Epoch 184/512

Epoch 00184: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3642e-11 - val_loss: 6.4907e-11
Epoch 185/512

Epoch 00185: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.6523e-11 - val_loss: 7.1205e-11
Epoch 186/512

Epoch 00186: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2049e-11 - val_loss: 7.6642e-11
Epoch 187/512

Epoch 00187: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6740e-11 - val_loss: 7.8661e-11
Epoch 188/512

Epoch 00188: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.7684e-11 - val_loss: 7.5519e-11
Epoch 189/512

Epoch 00189: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2047e-11 - val_loss: 6.9902e-11
Epoch 190/512

Epoch 00190: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.7606e-11 - val_loss: 6.4936e-11
Epoch 191/512

Epoch 00191: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.2636e-11 - val_loss: 6.2746e-11
Epoch 192/512

Epoch 00192: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.1667e-11 - val_loss: 6.2113e-11
Epoch 193/512

Epoch 00193: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.4098e-11 - val_loss: 6.8962e-11
Epoch 194/512

Epoch 00194: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0776e-11 - val_loss: 7.5118e-11
Epoch 195/512

Epoch 00195: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.6301e-11 - val_loss: 7.6472e-11
Epoch 196/512

Epoch 00196: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.3174e-11 - val_loss: 7.0531e-11
Epoch 197/512

Epoch 00197: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.8125e-11 - val_loss: 6.5717e-11
Epoch 198/512

Epoch 00198: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.3043e-11 - val_loss: 6.2113e-11
Epoch 199/512

Epoch 00199: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0593e-11 - val_loss: 5.9333e-11
Epoch 200/512

Epoch 00200: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.9669e-11 - val_loss: 6.4451e-11
Epoch 201/512

Epoch 00201: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5133e-11 - val_loss: 6.9745e-11
Epoch 202/512

Epoch 00202: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.0767e-11 - val_loss: 7.2039e-11
Epoch 203/512

Epoch 00203: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9014e-11 - val_loss: 6.6414e-11
Epoch 204/512

Epoch 00204: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.3838e-11 - val_loss: 6.2703e-11
Epoch 205/512

Epoch 00205: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.1268e-11 - val_loss: 6.1777e-11
Epoch 206/512

Epoch 00206: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 6.0244e-11 - val_loss: 5.9176e-11
Epoch 207/512

Epoch 00207: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.8089e-11 - val_loss: 5.7428e-11
Epoch 208/512

Epoch 00208: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8529e-11 - val_loss: 6.3806e-11
Epoch 209/512

Epoch 00209: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.5803e-11 - val_loss: 7.1503e-11
Epoch 210/512

Epoch 00210: val_loss did not improve from 0.00000
512/512 - 0s - loss: 7.2743e-11 - val_loss: 7.3141e-11
Epoch 211/512

Epoch 00211: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.9850e-11 - val_loss: 6.6647e-11
Epoch 212/512

Epoch 00212: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.2927e-11 - val_loss: 5.8214e-11
Epoch 213/512

Epoch 00213: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.6181e-11 - val_loss: 5.6626e-11
Epoch 214/512

Epoch 00214: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.5965e-11 - val_loss: 5.4916e-11
Epoch 215/512

Epoch 00215: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4134e-11 - val_loss: 5.5600e-11
Epoch 216/512

Epoch 00216: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5797e-11 - val_loss: 5.6643e-11
Epoch 217/512

Epoch 00217: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.4474e-11 - val_loss: 5.2073e-11
Epoch 218/512

Epoch 00218: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0808e-11 - val_loss: 5.3298e-11
Epoch 219/512

Epoch 00219: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3898e-11 - val_loss: 5.7544e-11
Epoch 220/512

Epoch 00220: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8269e-11 - val_loss: 5.9662e-11
Epoch 221/512

Epoch 00221: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7765e-11 - val_loss: 5.8051e-11
Epoch 222/512

Epoch 00222: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8103e-11 - val_loss: 5.9952e-11
Epoch 223/512

Epoch 00223: val_loss did not improve from 0.00000
512/512 - 0s - loss: 6.0123e-11 - val_loss: 6.0500e-11
Epoch 224/512

Epoch 00224: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8477e-11 - val_loss: 5.7762e-11
Epoch 225/512

Epoch 00225: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7067e-11 - val_loss: 5.4963e-11
Epoch 226/512

Epoch 00226: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 5.1831e-11 - val_loss: 4.9692e-11
Epoch 227/512

Epoch 00227: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.8117e-11 - val_loss: 4.5968e-11
Epoch 228/512

Epoch 00228: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.3822e-11 - val_loss: 4.2389e-11
Epoch 229/512

Epoch 00229: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1904e-11 - val_loss: 4.5480e-11
Epoch 230/512

Epoch 00230: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8068e-11 - val_loss: 5.2502e-11
Epoch 231/512

Epoch 00231: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.3675e-11 - val_loss: 5.7922e-11
Epoch 232/512

Epoch 00232: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8807e-11 - val_loss: 6.0123e-11
Epoch 233/512

Epoch 00233: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8250e-11 - val_loss: 5.7812e-11
Epoch 234/512

Epoch 00234: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7693e-11 - val_loss: 5.9282e-11
Epoch 235/512

Epoch 00235: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.7280e-11 - val_loss: 5.4079e-11
Epoch 236/512

Epoch 00236: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.0512e-11 - val_loss: 4.6401e-11
Epoch 237/512

Epoch 00237: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.4056e-11 - val_loss: 4.2085e-11
Epoch 238/512

Epoch 00238: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1241e-11 - val_loss: 4.1772e-11
Epoch 239/512

Epoch 00239: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1521e-11 - val_loss: 4.5563e-11
Epoch 240/512

Epoch 00240: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8357e-11 - val_loss: 5.3663e-11
Epoch 241/512

Epoch 00241: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4995e-11 - val_loss: 5.9292e-11
Epoch 242/512

Epoch 00242: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.8334e-11 - val_loss: 5.7493e-11
Epoch 243/512

Epoch 00243: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.5208e-11 - val_loss: 5.3283e-11
Epoch 244/512

Epoch 00244: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1856e-11 - val_loss: 5.0462e-11
Epoch 245/512

Epoch 00245: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.8141e-11 - val_loss: 4.6532e-11
Epoch 246/512

Epoch 00246: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.4970e-11 - val_loss: 4.2945e-11
Epoch 247/512

Epoch 00247: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.1877e-11 - val_loss: 4.2189e-11
Epoch 248/512

Epoch 00248: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2182e-11 - val_loss: 4.3366e-11
Epoch 249/512

Epoch 00249: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.1938e-11 - val_loss: 4.0764e-11
Epoch 250/512

Epoch 00250: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.2292e-11 - val_loss: 4.6429e-11
Epoch 251/512

Epoch 00251: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.7689e-11 - val_loss: 5.0361e-11
Epoch 252/512

Epoch 00252: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.1684e-11 - val_loss: 5.4477e-11
Epoch 253/512

Epoch 00253: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4194e-11 - val_loss: 5.4915e-11
Epoch 254/512

Epoch 00254: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4761e-11 - val_loss: 5.5248e-11
Epoch 255/512

Epoch 00255: val_loss did not improve from 0.00000
512/512 - 0s - loss: 5.4669e-11 - val_loss: 5.2745e-11
Epoch 256/512

Epoch 00256: val_loss did not improve from 0.00000
512/512 - 0s - loss: 4.9364e-11 - val_loss: 4.5652e-11
Epoch 257/512

Epoch 00257: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 4.2530e-11 - val_loss: 3.9858e-11
Epoch 258/512

Epoch 00258: val_loss improved from 0.00000 to 0.00000, saving model to weights/weights-multiplication-and-addition-in-different-models-operation-29/multiplication_addition_weights.h5
512/512 - 0s - loss: 3.9039e-11 - val_loss: 3.8480e-11
